#!/usr/bin/env python3
"""
æµ‹è¯• arXiv è¦†ç›–ç‡ï¼šéšæœºé‡‡æ · 100 ç¯‡è®ºæ–‡ï¼Œçœ‹èƒ½åœ¨ arXiv ä¸Šæ‰¾åˆ°å¤šå°‘
"""
import json
import random
import time
from pathlib import Path
import requests
import xml.etree.ElementTree as ET
from urllib.parse import urlencode
import sys

# æ·»åŠ çˆ¶ç›®å½•åˆ°è·¯å¾„ï¼Œä»¥ä¾¿å¯¼å…¥profile_reader
sys.path.append(str(Path(__file__).parent.parent))
from crawling_profiles.profile_reader import collect_all_papers


def search_arxiv(title, author=None):
    """
    ä½¿ç”¨ arXiv API æœç´¢è®ºæ–‡
    API æ–‡æ¡£: https://arxiv.org/help/api/user-manual
    """
    base_url = "http://export.arxiv.org/api/query"
    
    # æ„å»ºæŸ¥è¯¢å­—ç¬¦ä¸²
    # arXiv æŸ¥è¯¢è¯­æ³•: ti:title, au:author, all:all fields
    if author:
        # å°è¯•ç”¨æ ‡é¢˜å’Œä½œè€…æœç´¢
        query = f'ti:"{title}" AND au:"{author}"'
    else:
        query = f'ti:"{title}"'
    
    params = {
        'search_query': query,
        'start': 0,
        'max_results': 3,  # æœ€å¤šè¿”å›3ä¸ªç»“æœ
        'sortBy': 'relevance',
        'sortOrder': 'descending'
    }
    
    headers = {
        "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36"
    }
    
    try:
        response = requests.get(base_url, params=params, headers=headers, timeout=10)
        
        if response.status_code == 200:
            # è§£æ XML å“åº”
            root = ET.fromstring(response.content)
            
            # arXiv API ä½¿ç”¨ Atom å‘½åç©ºé—´
            ns = {'atom': 'http://www.w3.org/2005/Atom',
                  'arxiv': 'http://arxiv.org/schemas/atom'}
            
            entries = root.findall('atom:entry', ns)
            
            if not entries:
                return None
            
            results = []
            for entry in entries:
                # æå– arXiv ID
                arxiv_id = None
                id_text = entry.find('atom:id', ns).text if entry.find('atom:id', ns) is not None else None
                if id_text:
                    # arXiv ID æ ¼å¼: http://arxiv.org/abs/1234.5678v1
                    arxiv_id = id_text.split('/')[-1].split('v')[0]
                
                # æå–æ ‡é¢˜
                title_elem = entry.find('atom:title', ns)
                arxiv_title = title_elem.text.strip() if title_elem is not None else None
                
                # æå–ä½œè€…
                authors = []
                for author_elem in entry.findall('atom:author', ns):
                    name_elem = author_elem.find('atom:name', ns)
                    if name_elem is not None:
                        authors.append(name_elem.text.strip())
                
                # æå–æ‘˜è¦
                summary_elem = entry.find('atom:summary', ns)
                abstract = summary_elem.text.strip() if summary_elem is not None else None
                
                # æå–å‘å¸ƒæ—¥æœŸ
                published_elem = entry.find('atom:published', ns)
                published = published_elem.text.strip() if published_elem is not None else None
                year = None
                if published:
                    try:
                        year = int(published.split('-')[0])
                    except:
                        pass
                
                # æå–åˆ†ç±»
                categories = []
                for category_elem in entry.findall('atom:category', ns):
                    term = category_elem.get('term')
                    if term:
                        categories.append(term)
                
                # æ„å»º PDF URLï¼ˆarXiv æ‰€æœ‰è®ºæ–‡éƒ½æœ‰ PDFï¼‰
                pdf_url = None
                if arxiv_id:
                    pdf_url = f"https://arxiv.org/pdf/{arxiv_id}.pdf"
                
                # æ„å»º arXiv é¡µé¢ URL
                arxiv_url = None
                if arxiv_id:
                    arxiv_url = f"https://arxiv.org/abs/{arxiv_id}"
                
                result = {
                    'arxiv_id': arxiv_id,
                    'title': arxiv_title,
                    'authors': authors,
                    'year': year,
                    'published': published,
                    'categories': categories,
                    'abstract': abstract,
                    'url': arxiv_url,
                    'pdf_url': pdf_url  # arXiv æ‰€æœ‰è®ºæ–‡éƒ½æœ‰ PDF
                }
                results.append(result)
            
            return results
            
        elif response.status_code == 429:
            print(f"  âš ï¸  Rate limit exceeded, waiting...")
            time.sleep(60)  # ç­‰å¾…1åˆ†é’Ÿ
            return search_arxiv(title, author)  # é‡è¯•
        else:
            print(f"  âš ï¸  HTTP {response.status_code}")
            return None
            
    except ET.ParseError as e:
        print(f"  âš ï¸  XML Parse Error: {e}")
        return None
    except Exception as e:
        print(f"  âš ï¸  Error: {e}")
        return None


# collect_all_papers å‡½æ•°å·²ç§»è‡³ profile_reader.pyï¼Œç°åœ¨ç›´æ¥å¯¼å…¥ä½¿ç”¨


def test_arxiv_coverage(all_papers, sample_size=100):
    """æµ‹è¯• arXiv è¦†ç›–ç‡"""
    # éšæœºæ‰“ä¹±å¹¶é‡‡æ ·
    random.shuffle(all_papers)
    sampled_papers = all_papers[:sample_size]
    
    print(f"\néšæœºé‡‡æ · {len(sampled_papers)} ç¯‡è®ºæ–‡è¿›è¡Œæµ‹è¯•\n")
    print("=" * 80)
    
    found_count = 0
    not_found_count = 0
    pdf_available_count = 0  # æœ‰ PDF URL çš„æ•°é‡ï¼ˆarXiv æ‰€æœ‰è®ºæ–‡éƒ½æœ‰ PDFï¼‰
    
    results = []
    
    for idx, paper in enumerate(sampled_papers, 1):
        title = paper['title']
        author = paper['author_name']
        
        print(f"[{idx}/{len(sampled_papers)}] æœç´¢: {title[:60]}...")
        print(f"  ä½œè€…: {author}")
        
        arxiv_results = search_arxiv(title, author=author)
        
        result = {
            'paper': paper,
            'arxiv_found': arxiv_results is not None,
            'arxiv_results': arxiv_results
        }
        results.append(result)
        
        if arxiv_results:
            found_count += 1
            print(f"  âœ… æ‰¾åˆ° {len(arxiv_results)} ä¸ªç»“æœ")
            if arxiv_results:
                print(f"  arXiv ID: {arxiv_results[0].get('arxiv_id', 'N/A')}")
                print(f"  arXiv URL: {arxiv_results[0].get('url', 'N/A')}")
                pdf_url = arxiv_results[0].get('pdf_url')
                if pdf_url:
                    pdf_available_count += 1
                    print(f"  ğŸ“„ PDF URL: {pdf_url}")
                else:
                    print(f"  âš ï¸  No PDF URL available")
                # æ˜¾ç¤ºåˆ†ç±»
                categories = arxiv_results[0].get('categories', [])
                if categories:
                    print(f"  åˆ†ç±»: {', '.join(categories[:3])}")  # åªæ˜¾ç¤ºå‰3ä¸ªåˆ†ç±»
        else:
            not_found_count += 1
            print(f"  âŒ æœªæ‰¾åˆ°")
        
        print()
        
        # arXiv API å»ºè®®: æ¯æ¬¡è¯·æ±‚é—´éš” 3 ç§’
        time.sleep(3.1)
    
    # ç»Ÿè®¡ç»“æœ
    print("=" * 80)
    print("æµ‹è¯•å®Œæˆï¼")
    print("=" * 80)
    print(f"é‡‡æ ·è®ºæ–‡æ•°: {len(sampled_papers)}")
    print(f"æ‰¾åˆ°è®ºæ–‡: {found_count} ç¯‡ ({found_count/len(sampled_papers)*100:.1f}%)")
    print(f"æœªæ‰¾åˆ°: {not_found_count} ç¯‡ ({not_found_count/len(sampled_papers)*100:.1f}%)")
    print(f"æœ‰ PDF URL: {pdf_available_count} ç¯‡ ({pdf_available_count/len(sampled_papers)*100:.1f}%)")
    print(f"\narXiv è¦†ç›–ç‡: {found_count/len(sampled_papers)*100:.1f}%")
    print(f"PDF URL è¦†ç›–ç‡: {pdf_available_count/len(sampled_papers)*100:.1f}%")
    
    return results, pdf_available_count


def save_results(results, pdf_available_count, output_file):
    """ä¿å­˜æµ‹è¯•ç»“æœ"""
    # æå–æœªæ‰¾åˆ°çš„è®ºæ–‡ä¿¡æ¯
    not_found_papers = []
    for r in results:
        if not r['arxiv_found']:
            not_found_papers.append({
                'title': r['paper'].get('title', ''),
                'author_name': r['paper'].get('author_name', ''),
                'venue': r['paper'].get('venue', ''),
                'year': r['paper'].get('year', ''),
                'citations': r['paper'].get('citations', 0)
            })
    
    summary = {
        'total_tested': len(results),
        'found': sum(1 for r in results if r['arxiv_found']),
        'not_found': sum(1 for r in results if not r['arxiv_found']),
        'pdf_available': pdf_available_count,
        'coverage_rate': sum(1 for r in results if r['arxiv_found']) / len(results) * 100,
        'pdf_coverage_rate': pdf_available_count / len(results) * 100,
        'not_found_papers': not_found_papers,  # æœªæ‰¾åˆ°çš„è®ºæ–‡åˆ—è¡¨
        'detailed_results': results
    }
    
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(summary, f, indent=2, ensure_ascii=False)
    
    print(f"\nç»“æœå·²ä¿å­˜åˆ°: {output_file}")
    
    # å•ç‹¬ä¿å­˜æœªæ‰¾åˆ°çš„è®ºæ–‡åˆ°å¦ä¸€ä¸ªæ–‡ä»¶ï¼Œæ–¹ä¾¿æŸ¥çœ‹
    if not_found_papers:
        not_found_file = output_file.replace('.json', '_not_found.json')
        with open(not_found_file, 'w', encoding='utf-8') as f:
            json.dump({
                'total_not_found': len(not_found_papers),
                'not_found_papers': not_found_papers
            }, f, indent=2, ensure_ascii=False)
        print(f"æœªæ‰¾åˆ°çš„è®ºæ–‡åˆ—è¡¨å·²ä¿å­˜åˆ°: {not_found_file}")
        print(f"å…± {len(not_found_papers)} ç¯‡è®ºæ–‡æœªåœ¨ arXiv ä¸Šæ‰¾åˆ°")


if __name__ == "__main__":
    # æ”¯æŒä»ç›®å½•æˆ–zipæ–‡ä»¶è¯»å–
    # å¦‚æœzipæ–‡ä»¶å­˜åœ¨ï¼Œä¼˜å…ˆä½¿ç”¨zipæ–‡ä»¶ï¼ˆæ›´èŠ‚çœç©ºé—´ï¼‰
    profiles_zip = "/Users/yuhli/Desktop/citation/crawling_profiles/all_author_profiles.zip"
    profiles_directory = "/Users/yuhli/Desktop/citation/crawling_profiles/all_author_profiles"
    
    # è‡ªåŠ¨é€‰æ‹©ï¼šä¼˜å…ˆä½¿ç”¨zipæ–‡ä»¶ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™ä½¿ç”¨ç›®å½•
    if Path(profiles_zip).exists():
        profiles_source = profiles_zip
        print(f"ä½¿ç”¨zipæ–‡ä»¶: {profiles_zip}\n")
    else:
        profiles_source = profiles_directory
        print(f"ä½¿ç”¨ç›®å½•: {profiles_directory}\n")
    
    output_filepath = "/Users/yuhli/Desktop/citation/crawling_papers/arxiv_coverage_test.json"
    
    # æ­¥éª¤1: æ”¶é›†100ä¸ªä½œè€…çš„æ‰€æœ‰è®ºæ–‡
    all_papers = collect_all_papers(profiles_source, num_authors=100)
    
    # æ­¥éª¤2: éšæœºé‡‡æ ·100ç¯‡è®ºæ–‡æµ‹è¯•
    results, pdf_available_count = test_arxiv_coverage(all_papers, sample_size=100)
    
    # æ­¥éª¤3: ä¿å­˜ç»“æœ
    save_results(results, pdf_available_count, output_filepath)

