#!/usr/bin/env python3
"""
æµ‹è¯• Semantic Scholar è¦†ç›–ç‡ï¼šéšæœºé‡‡æ · 100 ç¯‡è®ºæ–‡ï¼Œçœ‹èƒ½æ‰¾åˆ°å¤šå°‘
"""
import json
import random
import time
from pathlib import Path
import requests
import sys

# æ·»åŠ çˆ¶ç›®å½•åˆ°è·¯å¾„ï¼Œä»¥ä¾¿å¯¼å…¥profile_reader
sys.path.append(str(Path(__file__).parent.parent))
from crawling_profiles.profile_reader import collect_all_papers


def search_semantic_scholar(title, author=None):
    """
    ä½¿ç”¨ Semantic Scholar API æœç´¢è®ºæ–‡
    API æ–‡æ¡£: https://api.semanticscholar.org/
    """
    base_url = "https://api.semanticscholar.org/graph/v1/paper/search"
    
    # API Key (1 request per second)
    API_KEY = "78q8LRUz2IZgHoDiPvH42MVb0vEmR7p4mpiXZ0Ej"
    
    # æ„å»ºæŸ¥è¯¢
    if author:
        query = f'{title} {author}'
    else:
        query = title
    
    params = {
        'query': query,
        'limit': 3,  # æœ€å¤šè¿”å›3ä¸ªç»“æœ
        'fields': 'paperId,title,authors,year,venue,citationCount,abstract,url,publicationTypes,openAccessPdf,externalIds'
    }
    
    headers = {
        "x-api-key": API_KEY,
        "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36"
    }
    
    try:
        response = requests.get(base_url, params=params, headers=headers, timeout=10)
        
        if response.status_code == 200:
            data = response.json()
            papers = data.get('data', [])
            
            if not papers:
                return None
            
            results = []
            for paper in papers:
                # è·å– PDF URL
                pdf_url = None
                external_ids = paper.get('externalIds', {})
                
                # ä¼˜å…ˆä½¿ç”¨å¼€æ”¾è·å– PDF
                if paper.get('openAccessPdf') and paper.get('openAccessPdf', {}).get('url'):
                    pdf_url = paper.get('openAccessPdf', {}).get('url')
                # å¦‚æœæœ‰ arXiv IDï¼Œæ„å»º arXiv PDF URL
                elif external_ids and external_ids.get('ArXiv'):
                    arxiv_id = external_ids.get('ArXiv')
                    pdf_url = f"https://arxiv.org/pdf/{arxiv_id}.pdf"
                
                result = {
                    'paper_id': paper.get('paperId'),
                    'title': paper.get('title'),
                    'authors': [a.get('name') for a in paper.get('authors', [])],
                    'year': paper.get('year'),
                    'venue': paper.get('venue'),
                    'citation_count': paper.get('citationCount'),
                    'abstract': paper.get('abstract'),
                    'url': paper.get('url'),
                    'publication_types': paper.get('publicationTypes', []),
                    'pdf_url': pdf_url,  # å¯ä»¥ wget çš„ PDF URL
                    'external_ids': external_ids,  # DOI, arXiv ID, PubMed ID ç­‰
                    'open_access': paper.get('openAccessPdf')
                }
                results.append(result)
            
            return results
            
        elif response.status_code == 429:
            print(f"  âš ï¸  Rate limit exceeded, waiting...")
            time.sleep(60)  # ç­‰å¾…1åˆ†é’Ÿ
            return search_semantic_scholar(title, author)  # é‡è¯•
        else:
            print(f"  âš ï¸  HTTP {response.status_code}")
            return None
            
    except Exception as e:
        print(f"  âš ï¸  Error: {e}")
        return None


# collect_all_papers å‡½æ•°å·²ç§»è‡³ profile_reader.pyï¼Œç°åœ¨ç›´æ¥å¯¼å…¥ä½¿ç”¨


def test_semantic_scholar_coverage(all_papers, sample_size=100):
    """æµ‹è¯• Semantic Scholar è¦†ç›–ç‡"""
    # éšæœºæ‰“ä¹±å¹¶é‡‡æ ·
    random.shuffle(all_papers)
    sampled_papers = all_papers[:sample_size]
    
    print(f"\néšæœºé‡‡æ · {len(sampled_papers)} ç¯‡è®ºæ–‡è¿›è¡Œæµ‹è¯•\n")
    print("=" * 80)
    
    found_count = 0
    not_found_count = 0
    pdf_available_count = 0  # æœ‰ PDF URL çš„æ•°é‡
    
    results = []
    
    for idx, paper in enumerate(sampled_papers, 1):
        title = paper['title']
        author = paper['author_name']
        
        print(f"[{idx}/{len(sampled_papers)}] æœç´¢: {title[:60]}...")
        print(f"  ä½œè€…: {author}")
        
        s2_results = search_semantic_scholar(title, author=author)
        
        result = {
            'paper': paper,
            's2_found': s2_results is not None,
            's2_results': s2_results
        }
        results.append(result)
        
        if s2_results:
            found_count += 1
            print(f"  âœ… æ‰¾åˆ° {len(s2_results)} ä¸ªç»“æœ")
            if s2_results:
                print(f"  S2 Paper ID: {s2_results[0].get('paper_id', 'N/A')}")
                print(f"  S2 URL: {s2_results[0].get('url', 'N/A')}")
                pdf_url = s2_results[0].get('pdf_url')
                if pdf_url:
                    pdf_available_count += 1
                    print(f"  ğŸ“„ PDF URL: {pdf_url}")
                else:
                    print(f"  âš ï¸  No PDF URL available")
        else:
            not_found_count += 1
            print(f"  âŒ æœªæ‰¾åˆ°")
        
        print()
        
        # Semantic Scholar API é™åˆ¶ (with API key): 1 request per second
        time.sleep(1.1)
    
    # ç»Ÿè®¡ç»“æœ
    print("=" * 80)
    print("æµ‹è¯•å®Œæˆï¼")
    print("=" * 80)
    print(f"é‡‡æ ·è®ºæ–‡æ•°: {len(sampled_papers)}")
    print(f"æ‰¾åˆ°è®ºæ–‡: {found_count} ç¯‡ ({found_count/len(sampled_papers)*100:.1f}%)")
    print(f"æœªæ‰¾åˆ°: {not_found_count} ç¯‡ ({not_found_count/len(sampled_papers)*100:.1f}%)")
    print(f"æœ‰ PDF URL: {pdf_available_count} ç¯‡ ({pdf_available_count/len(sampled_papers)*100:.1f}%)")
    print(f"\nSemantic Scholar è¦†ç›–ç‡: {found_count/len(sampled_papers)*100:.1f}%")
    print(f"PDF URL è¦†ç›–ç‡: {pdf_available_count/len(sampled_papers)*100:.1f}%")
    
    return results, pdf_available_count


def save_results(results, pdf_available_count, output_file):
    """ä¿å­˜æµ‹è¯•ç»“æœ"""
    # æå–æœªæ‰¾åˆ°çš„è®ºæ–‡ä¿¡æ¯
    not_found_papers = []
    for r in results:
        if not r['s2_found']:
            not_found_papers.append({
                'title': r['paper'].get('title', ''),
                'author_name': r['paper'].get('author_name', ''),
                'venue': r['paper'].get('venue', ''),
                'year': r['paper'].get('year', ''),
                'citations': r['paper'].get('citations', 0)
            })
    
    summary = {
        'total_tested': len(results),
        'found': sum(1 for r in results if r['s2_found']),
        'not_found': sum(1 for r in results if not r['s2_found']),
        'pdf_available': pdf_available_count,
        'coverage_rate': sum(1 for r in results if r['s2_found']) / len(results) * 100,
        'pdf_coverage_rate': pdf_available_count / len(results) * 100,
        'not_found_papers': not_found_papers,  # æœªæ‰¾åˆ°çš„è®ºæ–‡åˆ—è¡¨
        'detailed_results': results
    }
    
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(summary, f, indent=2, ensure_ascii=False)
    
    print(f"\nç»“æœå·²ä¿å­˜åˆ°: {output_file}")
    
    # å•ç‹¬ä¿å­˜æœªæ‰¾åˆ°çš„è®ºæ–‡åˆ°å¦ä¸€ä¸ªæ–‡ä»¶ï¼Œæ–¹ä¾¿æŸ¥çœ‹
    if not_found_papers:
        not_found_file = output_file.replace('.json', '_not_found.json')
        with open(not_found_file, 'w', encoding='utf-8') as f:
            json.dump({
                'total_not_found': len(not_found_papers),
                'not_found_papers': not_found_papers
            }, f, indent=2, ensure_ascii=False)
        print(f"æœªæ‰¾åˆ°çš„è®ºæ–‡åˆ—è¡¨å·²ä¿å­˜åˆ°: {not_found_file}")
        print(f"å…± {len(not_found_papers)} ç¯‡è®ºæ–‡æœªåœ¨ Semantic Scholar ä¸Šæ‰¾åˆ°")


if __name__ == "__main__":
    # æ”¯æŒä»ç›®å½•æˆ–zipæ–‡ä»¶è¯»å–
    # å¦‚æœzipæ–‡ä»¶å­˜åœ¨ï¼Œä¼˜å…ˆä½¿ç”¨zipæ–‡ä»¶ï¼ˆæ›´èŠ‚çœç©ºé—´ï¼‰
    profiles_zip = "/Users/yuhli/Desktop/citation/crawling_profiles/all_author_profiles.zip"
    profiles_directory = "/Users/yuhli/Desktop/citation/crawling_profiles/all_author_profiles"
    
    # è‡ªåŠ¨é€‰æ‹©ï¼šä¼˜å…ˆä½¿ç”¨zipæ–‡ä»¶ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™ä½¿ç”¨ç›®å½•
    if Path(profiles_zip).exists():
        profiles_source = profiles_zip
        print(f"ä½¿ç”¨zipæ–‡ä»¶: {profiles_zip}\n")
    else:
        profiles_source = profiles_directory
        print(f"ä½¿ç”¨ç›®å½•: {profiles_directory}\n")
    
    output_filepath = "/Users/yuhli/Desktop/citation/crawling_papers/semantic_scholar_coverage_test.json"
    
    # æ­¥éª¤1: æ”¶é›†100ä¸ªä½œè€…çš„æ‰€æœ‰è®ºæ–‡
    all_papers = collect_all_papers(profiles_source, num_authors=100)
    
    # æ­¥éª¤2: éšæœºé‡‡æ ·100ç¯‡è®ºæ–‡æµ‹è¯•
    results, pdf_available_count = test_semantic_scholar_coverage(all_papers, sample_size=100)
    
    # æ­¥éª¤3: ä¿å­˜ç»“æœ
    save_results(results, pdf_available_count, output_filepath)

