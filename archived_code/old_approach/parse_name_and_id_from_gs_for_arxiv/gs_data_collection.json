[
  {
    "arxiv_id": "2310.12092",
    "title": "HSTR-Net: Reference Based Video Super-resolution with Dual Cameras",
    "year": 2023,
    "published": "2023-10-18T16:37:01Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "High-spatio-temporal resolution (HSTR) video recording plays a crucial role in enhancing various imagery tasks that require fine-detailed information. State-of-the-art cameras provide this required high frame-rate and high spatial resolution together, albeit at a high cost. To alleviate this issue, this paper proposes a dual camera system for the generation of HSTR video using reference-based super-resolution (RefSR). One camera captures high spatial resolution low frame rate (HSLF) video while ",
    "arxiv_url": "https://arxiv.org/abs/2310.12092v2",
    "pdf_url": "https://arxiv.org/pdf/2310.12092v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2310.12092",
    "arxiv_authors": [
      "H. Umut Suluhan",
      "Abdullah Enes Doruk",
      "Hasan F. Ates",
      "Bahadir K. Gunturk"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=HSTR-Net%3A+Reference+Based+Video+Super-resolution+with+Dual+Cameras+H.+Umut+Suluhan+Abdullah+Enes+Doruk+Hasan+F.+Ates+Bahadir+K.+Gunturk",
    "gs_search_success": false,
    "gs_authors": null,
    "citation_count": null,
    "error_type": "multiple_matches"
  },
  {
    "arxiv_id": "2405.13659",
    "title": "EgoChoir: Capturing 3D Human-Object Interaction Regions from Egocentric Views",
    "year": 2024,
    "published": "2024-05-22T14:03:48Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Understanding egocentric human-object interaction (HOI) is a fundamental aspect of human-centric perception, facilitating applications like AR/VR and embodied AI. For the egocentric HOI, in addition to perceiving semantics e.g., ''what'' interaction is occurring, capturing ''where'' the interaction specifically manifests in 3D space is also crucial, which links the perception and operation. Existing methods primarily leverage observations of HOI to capture interaction regions from an exocentric ",
    "arxiv_url": "https://arxiv.org/abs/2405.13659v2",
    "pdf_url": "https://arxiv.org/pdf/2405.13659v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2405.13659",
    "arxiv_authors": [
      "Yuhang Yang",
      "Wei Zhai",
      "Chengfeng Wang",
      "Chengjun Yu",
      "Yang Cao",
      "Zheng-Jun Zha"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=EgoChoir%3A+Capturing+3D+Human-Object+Interaction+Regions+from+Egocentric+Views+Yuhang+Yang+Wei+Zhai+Chengfeng+Wang+Chengjun+Yu+Yang+Cao",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "Y Yang",
        "id": "x3aClGEAAAAJ"
      },
      {
        "name": "W Zhai",
        "id": "gDnBC1gAAAAJ"
      },
      {
        "name": "C Wang",
        "id": null
      },
      {
        "name": "C Yu",
        "id": null
      },
      {
        "name": "Y Cao",
        "id": "K7rTHNcAAAAJ"
      },
      {
        "name": "ZJ ZhaAdvances in Neural Information Processing Systems",
        "id": null
      }
    ],
    "citation_count": 19
  },
  {
    "arxiv_id": "2501.15572",
    "title": "Comparative clinical evaluation of \"memory-efficient\" synthetic 3d generative adversarial networks (gan) head-to-head to state of art: results on computed tomography of the chest",
    "year": 2025,
    "published": "2025-01-26T15:57:44Z",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "abstract": "Generative Adversarial Networks (GANs) are increasingly used to generate synthetic medical images, addressing the critical shortage of annotated data for training Artificial Intelligence systems. This study introduces CRF-GAN, a novel memory-efficient GAN architecture that enhances structural consistency in 3D medical image synthesis. Integrating Conditional Random Fields within a two-step generation process allows CRF-GAN improving spatial coherence while maintaining high-resolution image quali",
    "arxiv_url": "https://arxiv.org/abs/2501.15572v3",
    "pdf_url": "https://arxiv.org/pdf/2501.15572v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2501.15572",
    "arxiv_authors": [
      "Mahshid Shiri",
      "Chandra Bortolotto",
      "Alessandro Bruno",
      "Alessio Consonni",
      "Daniela Maria Grasso",
      "Leonardo Brizzi",
      "Daniele Loiacono",
      "Lorenzo Preda"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Comparative+clinical+evaluation+of+%22memory-efficient%22+synthetic+3d+generative+adversarial+networks+%28gan%29+head-to-head+to+state+of+art%3A+results+on+computed+tomography+of+the+chest+Mahshid+Shiri+Chandra+Bortolotto+Alessandro+Bruno+Alessio+Consonni+Daniela+Maria+Grasso",
    "gs_search_success": false,
    "gs_authors": null,
    "citation_count": null,
    "error_type": "multiple_matches"
  },
  {
    "arxiv_id": "2402.03769",
    "title": "AttackNet: Enhancing Biometric Security via Tailored Convolutional Neural Network Architectures for Liveness Detection",
    "year": 2024,
    "published": "2024-02-06T07:22:50Z",
    "categories": [
      "cs.CV",
      "cs.CR"
    ],
    "abstract": "Biometric security is the cornerstone of modern identity verification and authentication systems, where the integrity and reliability of biometric samples is of paramount importance. This paper introduces AttackNet, a bespoke Convolutional Neural Network architecture, meticulously designed to combat spoofing threats in biometric systems. Rooted in deep learning methodologies, this model offers a layered defense mechanism, seamlessly transitioning from low-level feature extraction to high-level p",
    "arxiv_url": "https://arxiv.org/abs/2402.03769v2",
    "pdf_url": "https://arxiv.org/pdf/2402.03769v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2402.03769",
    "arxiv_authors": [
      "Oleksandr Kuznetsov",
      "Dmytro Zakharov",
      "Emanuele Frontoni",
      "Andrea Maranesi"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=AttackNet%3A+Enhancing+Biometric+Security+via+Tailored+Convolutional+Neural+Network+Architectures+for+Liveness+Detection+Oleksandr+Kuznetsov+Dmytro+Zakharov+Emanuele+Frontoni+Andrea+Maranesi",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "O Kuznetsov",
        "id": "DUI-bncAAAAJ"
      },
      {
        "name": "D Zakharov",
        "id": "WL-8aoAAAAAJ"
      },
      {
        "name": "E Frontoni",
        "id": "Vgi8nAcAAAAJ"
      }
    ],
    "citation_count": 12
  },
  {
    "arxiv_id": "2408.02813",
    "title": "Mitigating Malicious Attacks in Federated Learning via Confidence-aware Defense",
    "year": 2024,
    "published": "2024-08-05T20:27:45Z",
    "categories": [
      "cs.LG",
      "cs.CR",
      "cs.CV",
      "cs.DC"
    ],
    "abstract": "Federated Learning (FL) is a distributed machine learning diagram that enables multiple clients to collaboratively train a global model without sharing their private local data. However, FL systems are vulnerable to attacks that are happening in malicious clients through data poisoning and model poisoning, which can deteriorate the performance of aggregated global model. Existing defense methods typically focus on mitigating specific types of poisoning and are often ineffective against unseen ty",
    "arxiv_url": "https://arxiv.org/abs/2408.02813v2",
    "pdf_url": "https://arxiv.org/pdf/2408.02813v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2408.02813",
    "arxiv_authors": [
      "Qilei Li",
      "Ahmed M. Abdelmoniem"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Mitigating+Malicious+Attacks+in+Federated+Learning+via+Confidence-aware+Defense+Qilei+Li+Ahmed+M.+Abdelmoniem",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "Q Li",
        "id": "BIUlY6AAAAAJ"
      },
      {
        "name": "AM Abdelmoniem -",
        "id": null
      }
    ],
    "citation_count": 1
  },
  {
    "arxiv_id": "2412.14579",
    "title": "GSRender: Deduplicated Occupancy Prediction via Weakly Supervised 3D Gaussian Splatting",
    "year": 2024,
    "published": "2024-12-19T06:57:37Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "3D occupancy perception is gaining increasing attention due to its capability to offer detailed and precise environment representations. Previous weakly-supervised NeRF methods balance efficiency and accuracy, with mIoU varying by 5-10 points due to sampling count along camera rays. Recently, real-time Gaussian splatting has gained widespread popularity in 3D reconstruction, and the occupancy prediction task can also be viewed as a reconstruction task. Consequently, we propose GSRender, which na",
    "arxiv_url": "https://arxiv.org/abs/2412.14579v1",
    "pdf_url": "https://arxiv.org/pdf/2412.14579v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2412.14579",
    "arxiv_authors": [
      "Qianpu Sun",
      "Changyong Shu",
      "Sifan Zhou",
      "Zichen Yu",
      "Yan Chen",
      "Dawei Yang",
      "Yuan Chun"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=GSRender%3A+Deduplicated+Occupancy+Prediction+via+Weakly+Supervised+3D+Gaussian+Splatting+Qianpu+Sun+Changyong+Shu+Sifan+Zhou+Zichen+Yu+Yan+Chen",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "Q Sun",
        "id": "cVjaTlMAAAAJ"
      },
      {
        "name": "C Shu",
        "id": "aJIJUoAAAAAJ"
      },
      {
        "name": "S Zhou",
        "id": "kSdqoi0AAAAJ"
      },
      {
        "name": "Z Yu",
        "id": null
      },
      {
        "name": "Y Chen",
        "id": "5Dsu0HwAAAAJ"
      },
      {
        "name": "D Yang",
        "id": "j2ANma0AAAAJ"
      },
      {
        "name": "Y Chun",
        "id": null
      }
    ],
    "citation_count": 8
  },
  {
    "arxiv_id": "2311.17366",
    "title": "Generative Hierarchical Temporal Transformer for Hand Pose and Action Modeling",
    "year": 2023,
    "published": "2023-11-29T05:28:39Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "We present a novel unified framework that concurrently tackles recognition and future prediction for human hand pose and action modeling. Previous works generally provide isolated solutions for either recognition or prediction, which not only increases the complexity of integration in practical applications, but more importantly, cannot exploit the synergy of both sides and suffer suboptimal performances in their respective domains. To address this problem, we propose a generative Transformer VA",
    "arxiv_url": "https://arxiv.org/abs/2311.17366v3",
    "pdf_url": "https://arxiv.org/pdf/2311.17366v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2311.17366",
    "arxiv_authors": [
      "Yilin Wen",
      "Hao Pan",
      "Takehiko Ohkawa",
      "Lei Yang",
      "Jia Pan",
      "Yoichi Sato",
      "Taku Komura",
      "Wenping Wang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Generative+Hierarchical+Temporal+Transformer+for+Hand+Pose+and+Action+Modeling+Yilin+Wen+Hao+Pan+Takehiko+Ohkawa+Lei+Yang+Jia+Pan",
    "gs_search_success": false,
    "gs_authors": null,
    "citation_count": null,
    "error_type": "fetch_failed"
  },
  {
    "arxiv_id": "2406.16583",
    "title": "Personalized federated learning based on feature fusion",
    "year": 2024,
    "published": "2024-06-24T12:16:51Z",
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "abstract": "Federated learning enables distributed clients to collaborate on training while storing their data locally to protect client privacy. However, due to the heterogeneity of data, models, and devices, the final global model may need to perform better for tasks on each client. Communication bottlenecks, data heterogeneity, and model heterogeneity have been common challenges in federated learning. In this work, we considered a label distribution skew problem, a type of data heterogeneity easily overl",
    "arxiv_url": "https://arxiv.org/abs/2406.16583v1",
    "pdf_url": "https://arxiv.org/pdf/2406.16583v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2406.16583",
    "arxiv_authors": [
      "Wolong Xing",
      "Zhenkui Shi",
      "Hongyan Peng",
      "Xiantao Hu",
      "Xianxian Li"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Personalized+federated+learning+based+on+feature+fusion+Wolong+Xing+Zhenkui+Shi+Hongyan+Peng+Xiantao+Hu+Xianxian+Li",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "W Xing",
        "id": null
      },
      {
        "name": "Z Shi",
        "id": "C3YlCgIAAAAJ"
      },
      {
        "name": "H Peng",
        "id": null
      },
      {
        "name": "X Hu",
        "id": "ZlUB72cAAAAJ"
      },
      {
        "name": "Y Zheng",
        "id": "B6QzwQMAAAAJ"
      },
      {
        "name": "X Li2024 27th International",
        "id": null
      }
    ],
    "citation_count": 4
  },
  {
    "arxiv_id": "2309.01674",
    "title": "Prompt me a Dataset: An investigation of text-image prompting for historical image dataset creation using foundation models",
    "year": 2023,
    "published": "2023-09-04T15:37:03Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.DL"
    ],
    "abstract": "In this paper, we present a pipeline for image extraction from historical documents using foundation models, and evaluate text-image prompts and their effectiveness on humanities datasets of varying levels of complexity. The motivation for this approach stems from the high interest of historians in visual elements printed alongside historical texts on the one hand, and from the relative lack of well-annotated datasets within the humanities when compared to other domains. We propose a sequential ",
    "arxiv_url": "https://arxiv.org/abs/2309.01674v1",
    "pdf_url": "https://arxiv.org/pdf/2309.01674v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2309.01674",
    "arxiv_authors": [
      "Hassan El-Hajj",
      "Matteo Valleriani"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Prompt+me+a+Dataset%3A+An+investigation+of+text-image+prompting+for+historical+image+dataset+creation+using+foundation+models+Hassan+El-Hajj+Matteo+Valleriani",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "H El-Hajj",
        "id": null
      },
      {
        "name": "M Valleriani - International",
        "id": null
      }
    ],
    "citation_count": 3
  },
  {
    "arxiv_id": "2504.04704",
    "title": "LagKV: Lag-Relative Information of the KV Cache Tells Which Tokens Are Important",
    "year": 2025,
    "published": "2025-04-07T03:22:15Z",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.CV"
    ],
    "abstract": "The increasing size of the Key-Value (KV) cache during the Large Language Models long-context inference is the main obstacle for its balance between the deployment cost and task accuracy. To reduce the KV cache size in such scenarios, most previous efforts leveraged on the attention weight to evict non-critical cache tokens. But there is a trade-off in those methods, they usually require major modification of the inference infrastructure and significant computation overhead. Based on the fact th",
    "arxiv_url": "https://arxiv.org/abs/2504.04704v2",
    "pdf_url": "https://arxiv.org/pdf/2504.04704v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2504.04704",
    "arxiv_authors": [
      "Manlai Liang",
      "JiaMing Zhang",
      "Xiong Li",
      "Jinlong Li"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=LagKV%3A+Lag-Relative+Information+of+the+KV+Cache+Tells+Which+Tokens+Are+Important+Manlai+Liang+JiaMing+Zhang+Xiong+Li+Jinlong+Li",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "M Liang",
        "id": "O-gqcrYAAAAJ"
      },
      {
        "name": "JM Zhang",
        "id": null
      },
      {
        "name": "X Li",
        "id": null
      },
      {
        "name": "J Li -",
        "id": null
      }
    ],
    "citation_count": null
  },
  {
    "arxiv_id": "2308.12535",
    "title": "SCP: Spherical-Coordinate-based Learned Point Cloud Compression",
    "year": 2023,
    "published": "2023-08-24T03:44:05Z",
    "categories": [
      "cs.CV",
      "eess.IV"
    ],
    "abstract": "In recent years, the task of learned point cloud compression has gained prominence. An important type of point cloud, the spinning LiDAR point cloud, is generated by spinning LiDAR on vehicles. This process results in numerous circular shapes and azimuthal angle invariance features within the point clouds. However, these two features have been largely overlooked by previous methodologies. In this paper, we introduce a model-agnostic method called Spherical-Coordinate-based learned Point cloud co",
    "arxiv_url": "https://arxiv.org/abs/2308.12535v3",
    "pdf_url": "https://arxiv.org/pdf/2308.12535v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2308.12535",
    "arxiv_authors": [
      "Ao Luo",
      "Linxin Song",
      "Keisuke Nonaka",
      "Kyohei Unno",
      "Heming Sun",
      "Masayuki Goto",
      "Jiro Katto"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=SCP%3A+Spherical-Coordinate-based+Learned+Point+Cloud+Compression+Ao+Luo+Linxin+Song+Keisuke+Nonaka+Kyohei+Unno+Heming+Sun",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "A Luo",
        "id": "B0Us5zcAAAAJ"
      },
      {
        "name": "L Song",
        "id": "IjqXzSwAAAAJ"
      },
      {
        "name": "K Nonaka",
        "id": "XcFO3hAAAAAJ"
      },
      {
        "name": "K Unno",
        "id": "s6rHQNsAAAAJ"
      },
      {
        "name": "H Sun",
        "id": "LtkiCFcAAAAJ"
      },
      {
        "name": "M Goto",
        "id": "bthD8VEAAAAJ"
      },
      {
        "name": "J Katto",
        "id": "sUBzrjUAAAAJ"
      }
    ],
    "citation_count": 19
  },
  {
    "arxiv_id": "2402.09270",
    "title": "Fast Window-Based Event Denoising with Spatiotemporal Correlation Enhancement",
    "year": 2024,
    "published": "2024-02-14T15:56:42Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Previous deep learning-based event denoising methods mostly suffer from poor interpretability and difficulty in real-time processing due to their complex architecture designs. In this paper, we propose window-based event denoising, which simultaneously deals with a stack of events while existing element-based denoising focuses on one event each time. Besides, we give the theoretical analysis based on probability distributions in both temporal and spatial domains to improve interpretability. In t",
    "arxiv_url": "https://arxiv.org/abs/2402.09270v1",
    "pdf_url": "https://arxiv.org/pdf/2402.09270v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2402.09270",
    "arxiv_authors": [
      "Huachen Fang",
      "Jinjian Wu",
      "Qibin Hou",
      "Weisheng Dong",
      "Guangming Shi"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Fast+Window-Based+Event+Denoising+with+Spatiotemporal+Correlation+Enhancement+Huachen+Fang+Jinjian+Wu+Qibin+Hou+Weisheng+Dong+Guangming+Shi",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "H Fang",
        "id": "FAdGKQ0AAAAJ"
      },
      {
        "name": "J Wu",
        "id": "QsSBP2UAAAAJ"
      },
      {
        "name": "Q Hou",
        "id": "fF8OFV8AAAAJ"
      },
      {
        "name": "W Dong",
        "id": "-g58LsoAAAAJ"
      },
      {
        "name": "G ShiIEEE Transactions on Pattern Analysis and Machine Intelligence",
        "id": null
      }
    ],
    "citation_count": 2
  },
  {
    "arxiv_id": "2301.12378",
    "title": "Towards Inference Efficient Deep Ensemble Learning",
    "year": 2023,
    "published": "2023-01-29T06:48:53Z",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "abstract": "Ensemble methods can deliver surprising performance gains but also bring significantly higher computational costs, e.g., can be up to 2048X in large-scale ensemble tasks. However, we found that the majority of computations in ensemble methods are redundant. For instance, over 77% of samples in CIFAR-100 dataset can be correctly classified with only a single ResNet-18 model, which indicates that only around 23% of the samples need an ensemble of extra models. To this end, we propose an inference ",
    "arxiv_url": "https://arxiv.org/abs/2301.12378v1",
    "pdf_url": "https://arxiv.org/pdf/2301.12378v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2301.12378",
    "arxiv_authors": [
      "Ziyue Li",
      "Kan Ren",
      "Yifan Yang",
      "Xinyang Jiang",
      "Yuqing Yang",
      "Dongsheng Li"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Towards+Inference+Efficient+Deep+Ensemble+Learning+Ziyue+Li+Kan+Ren+Yifan+Yang+Xinyang+Jiang+Yuqing+Yang",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "Z Li",
        "id": "NQVzCSkAAAAJ"
      },
      {
        "name": "K Ren",
        "id": "USnQVWgAAAAJ"
      },
      {
        "name": "Y Yang",
        "id": "4BtNQAEAAAAJ"
      },
      {
        "name": "X Jiang",
        "id": "JiTfWVMAAAAJ"
      },
      {
        "name": "Y Yang",
        "id": "4BtNQAEAAAAJ"
      },
      {
        "name": "D Li -",
        "id": null
      }
    ],
    "citation_count": 17
  },
  {
    "arxiv_id": "2411.14869",
    "title": "BIP3D: Bridging 2D Images and 3D Perception for Embodied Intelligence",
    "year": 2024,
    "published": "2024-11-22T11:35:42Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "abstract": "In embodied intelligence systems, a key component is 3D perception algorithm, which enables agents to understand their surrounding environments. Previous algorithms primarily rely on point cloud, which, despite offering precise geometric information, still constrain perception performance due to inherent sparsity, noise, and data scarcity. In this work, we introduce a novel image-centric 3D perception model, BIP3D, which leverages expressive image features with explicit 3D position encoding to o",
    "arxiv_url": "https://arxiv.org/abs/2411.14869v2",
    "pdf_url": "https://arxiv.org/pdf/2411.14869v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2411.14869",
    "arxiv_authors": [
      "Xuewu Lin",
      "Tianwei Lin",
      "Lichao Huang",
      "Hongyu Xie",
      "Zhizhong Su"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=BIP3D%3A+Bridging+2D+Images+and+3D+Perception+for+Embodied+Intelligence+Xuewu+Lin+Tianwei+Lin+Lichao+Huang+Hongyu+Xie+Zhizhong+Su",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "X Lin",
        "id": "pfXQwcQAAAAJ"
      },
      {
        "name": "T Lin",
        "id": "9oQiDgsAAAAJ"
      },
      {
        "name": "L Huang",
        "id": "F2e_jZMAAAAJ"
      },
      {
        "name": "H Xie",
        "id": null
      },
      {
        "name": "Z Su",
        "id": null
      }
    ],
    "citation_count": 5
  },
  {
    "arxiv_id": "2312.11038",
    "title": "UniChest: Conquer-and-Divide Pre-training for Multi-Source Chest X-Ray Classification",
    "year": 2023,
    "published": "2023-12-18T09:16:48Z",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "Vision-Language Pre-training (VLP) that utilizes the multi-modal information to promote the training efficiency and effectiveness, has achieved great success in vision recognition of natural domains and shown promise in medical imaging diagnosis for the Chest X-Rays (CXRs). However, current works mainly pay attention to the exploration on single dataset of CXRs, which locks the potential of this powerful paradigm on larger hybrid of multi-source CXRs datasets. We identify that although blending ",
    "arxiv_url": "https://arxiv.org/abs/2312.11038v2",
    "pdf_url": "https://arxiv.org/pdf/2312.11038v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2312.11038",
    "arxiv_authors": [
      "Tianjie Dai",
      "Ruipeng Zhang",
      "Feng Hong",
      "Jiangchao Yao",
      "Ya Zhang",
      "Yanfeng Wang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=UniChest%3A+Conquer-and-Divide+Pre-training+for+Multi-Source+Chest+X-Ray+Classification+Tianjie+Dai+Ruipeng+Zhang+Feng+Hong+Jiangchao+Yao+Ya+Zhang",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "T Dai",
        "id": "RXXJRTYAAAAJ"
      },
      {
        "name": "R Zhang",
        "id": "m7RzJxwAAAAJ"
      },
      {
        "name": "F Hong",
        "id": "DCTAaNQAAAAJ"
      },
      {
        "name": "J Yao",
        "id": "w8oDh9QAAAAJ"
      },
      {
        "name": "Y Zhang",
        "id": "pbjw9sMAAAAJ"
      },
      {
        "name": "Y WangIEEE Transactions on Medical Imaging",
        "id": null
      }
    ],
    "citation_count": 32
  },
  {
    "arxiv_id": "2403.15944",
    "title": "Adaptive Super Resolution For One-Shot Talking-Head Generation",
    "year": 2024,
    "published": "2024-03-23T22:14:38Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "eess.IV"
    ],
    "abstract": "The one-shot talking-head generation learns to synthesize a talking-head video with one source portrait image under the driving of same or different identity video. Usually these methods require plane-based pixel transformations via Jacobin matrices or facial image warps for novel poses generation. The constraints of using a single image source and pixel displacements often compromise the clarity of the synthesized images. Some methods try to improve the quality of synthesized videos by introduc",
    "arxiv_url": "https://arxiv.org/abs/2403.15944v1",
    "pdf_url": "https://arxiv.org/pdf/2403.15944v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2403.15944",
    "arxiv_authors": [
      "Luchuan Song",
      "Pinxin Liu",
      "Guojun Yin",
      "Chenliang Xu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Adaptive+Super+Resolution+For+One-Shot+Talking-Head+Generation+Luchuan+Song+Pinxin+Liu+Guojun+Yin+Chenliang+Xu",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "L Song",
        "id": "UVsyxOoAAAAJ"
      },
      {
        "name": "P Liu",
        "id": "ZJQldrQAAAAJ"
      },
      {
        "name": "G Yin",
        "id": "0yKrHJ0AAAAJ"
      },
      {
        "name": "C Xu - ICASSP",
        "id": null
      }
    ],
    "citation_count": 19
  },
  {
    "arxiv_id": "2307.10601",
    "title": "SCA-PVNet: Self-and-Cross Attention Based Aggregation of Point Cloud and Multi-View for 3D Object Retrieval",
    "year": 2023,
    "published": "2023-07-20T05:46:32Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "To address 3D object retrieval, substantial efforts have been made to generate highly discriminative descriptors of 3D objects represented by a single modality, e.g., voxels, point clouds or multi-view images. It is promising to leverage the complementary information from multi-modality representations of 3D objects to further improve retrieval performance. However, multi-modality 3D object retrieval is rarely developed and analyzed on large-scale datasets. In this paper, we propose self-and-cro",
    "arxiv_url": "https://arxiv.org/abs/2307.10601v2",
    "pdf_url": "https://arxiv.org/pdf/2307.10601v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2307.10601",
    "arxiv_authors": [
      "Dongyun Lin",
      "Yi Cheng",
      "Aiyuan Guo",
      "Shangbo Mao",
      "Yiqun Li"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=SCA-PVNet%3A+Self-and-Cross+Attention+Based+Aggregation+of+Point+Cloud+and+Multi-View+for+3D+Object+Retrieval+Dongyun+Lin+Yi+Cheng+Aiyuan+Guo+Shangbo+Mao+Yiqun+Li",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "D Lin",
        "id": "p4abc84AAAAJ"
      },
      {
        "name": "Y Cheng",
        "id": "OmyNx3IAAAAJ"
      },
      {
        "name": "A Guo",
        "id": null
      },
      {
        "name": "S Mao",
        "id": null
      },
      {
        "name": "Y Li - Knowledge-Based Systems",
        "id": null
      }
    ],
    "citation_count": 22
  },
  {
    "arxiv_id": "2406.13762",
    "title": "Unveiling the Hidden Structure of Self-Attention via Kernel Principal Component Analysis",
    "year": 2024,
    "published": "2024-06-19T18:22:32Z",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.CV",
      "stat.ML"
    ],
    "abstract": "The remarkable success of transformers in sequence modeling tasks, spanning various applications in natural language processing and computer vision, is attributed to the critical role of self-attention. Similar to the development of most deep learning models, the construction of these attention mechanisms relies on heuristics and experience. In our work, we derive self-attention from kernel principal component analysis (kernel PCA) and show that self-attention projects its query vectors onto the",
    "arxiv_url": "https://arxiv.org/abs/2406.13762v2",
    "pdf_url": "https://arxiv.org/pdf/2406.13762v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2406.13762",
    "arxiv_authors": [
      "Rachel S. Y. Teo",
      "Tan M. Nguyen"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Unveiling+the+Hidden+Structure+of+Self-Attention+via+Kernel+Principal+Component+Analysis+Rachel+S.+Y.+Teo+Tan+M.+Nguyen",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "RSY Teo",
        "id": "TQ1jGsYAAAAJ"
      },
      {
        "name": "T Nguyen - Advances in Neural Information",
        "id": null
      }
    ],
    "citation_count": 8
  },
  {
    "arxiv_id": "2308.06622",
    "title": "DFM-X: Augmentation by Leveraging Prior Knowledge of Shortcut Learning",
    "year": 2023,
    "published": "2023-08-12T17:39:10Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Neural networks are prone to learn easy solutions from superficial statistics in the data, namely shortcut learning, which impairs generalization and robustness of models. We propose a data augmentation strategy, named DFM-X, that leverages knowledge about frequency shortcuts, encoded in Dominant Frequencies Maps computed for image classification models. We randomly select X% training images of certain classes for augmentation, and process them by retaining the frequencies included in the DFMs o",
    "arxiv_url": "https://arxiv.org/abs/2308.06622v1",
    "pdf_url": "https://arxiv.org/pdf/2308.06622v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2308.06622",
    "arxiv_authors": [
      "Shunxin Wang",
      "Christoph Brune",
      "Raymond Veldhuis",
      "Nicola Strisciuglio"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=DFM-X%3A+Augmentation+by+Leveraging+Prior+Knowledge+of+Shortcut+Learning+Shunxin+Wang+Christoph+Brune+Raymond+Veldhuis+Nicola+Strisciuglio",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "S Wang",
        "id": "G-ibdsIAAAAJ"
      },
      {
        "name": "C Brune",
        "id": "QkD3WhsAAAAJ"
      },
      {
        "name": "R Veldhuis",
        "id": "7BpMrY0AAAAJ"
      },
      {
        "name": "N Strisciuglio",
        "id": "7cgpfGYAAAAJ"
      }
    ],
    "citation_count": 10
  },
  {
    "arxiv_id": "2505.12911",
    "title": "HiERO: understanding the hierarchy of human behavior enhances reasoning on egocentric videos",
    "year": 2025,
    "published": "2025-05-19T09:47:41Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Human activities are particularly complex and variable, and this makes challenging for deep learning models to reason about them. However, we note that such variability does have an underlying structure, composed of a hierarchy of patterns of related actions. We argue that such structure can emerge naturally from unscripted videos of human activities, and can be leveraged to better reason about their content. We present HiERO, a weakly-supervised method to enrich video segments features with the",
    "arxiv_url": "https://arxiv.org/abs/2505.12911v1",
    "pdf_url": "https://arxiv.org/pdf/2505.12911v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2505.12911",
    "arxiv_authors": [
      "Simone Alberto Peirone",
      "Francesca Pistilli",
      "Giuseppe Averta"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=HiERO%3A+understanding+the+hierarchy+of+human+behavior+enhances+reasoning+on+egocentric+videos+Simone+Alberto+Peirone+Francesca+Pistilli+Giuseppe+Averta",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "SA Peirone",
        "id": "yV8C7pwAAAAJ"
      },
      {
        "name": "F Pistilli",
        "id": "7MJdvzYAAAAJ"
      },
      {
        "name": "G Averta -",
        "id": null
      }
    ],
    "citation_count": null
  },
  {
    "arxiv_id": "2405.04589",
    "title": "A Novel Wide-Area Multiobject Detection System with High-Probability Region Searching",
    "year": 2024,
    "published": "2024-05-07T18:06:40Z",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "abstract": "In recent years, wide-area visual surveillance systems have been widely applied in various industrial and transportation scenarios. These systems, however, face significant challenges when implementing multi-object detection due to conflicts arising from the need for high-resolution imaging, efficient object searching, and accurate localization. To address these challenges, this paper presents a hybrid system that incorporates a wide-angle camera, a high-speed search camera, and a galvano-mirror",
    "arxiv_url": "https://arxiv.org/abs/2405.04589v1",
    "pdf_url": "https://arxiv.org/pdf/2405.04589v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2405.04589",
    "arxiv_authors": [
      "Xianlei Long",
      "Hui Zhao",
      "Chao Chen",
      "Fuqiang Gu",
      "Qingyi Gu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=A+Novel+Wide-Area+Multiobject+Detection+System+with+High-Probability+Region+Searching+Xianlei+Long+Hui+Zhao+Chao+Chen+Fuqiang+Gu+Qingyi+Gu",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "X Long",
        "id": "8JOf6s8AAAAJ"
      },
      {
        "name": "H Zhao",
        "id": "6luJjFQAAAAJ"
      },
      {
        "name": "C Chen",
        "id": null
      },
      {
        "name": "F Gu",
        "id": "dRBfNTkAAAAJ"
      },
      {
        "name": "Q Gu2024 IEEE International",
        "id": null
      }
    ],
    "citation_count": 9
  },
  {
    "arxiv_id": "2310.17261",
    "title": "Attribute Based Interpretable Evaluation Metrics for Generative Models",
    "year": 2023,
    "published": "2023-10-26T09:25:09Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "When the training dataset comprises a 1:1 proportion of dogs to cats, a generative model that produces 1:1 dogs and cats better resembles the training species distribution than another model with 3:1 dogs and cats. Can we capture this phenomenon using existing metrics? Unfortunately, we cannot, because these metrics do not provide any interpretability beyond \"diversity\". In this context, we propose a new evaluation protocol that measures the divergence of a set of generated images from the train",
    "arxiv_url": "https://arxiv.org/abs/2310.17261v3",
    "pdf_url": "https://arxiv.org/pdf/2310.17261v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2310.17261",
    "arxiv_authors": [
      "Dongkyun Kim",
      "Mingi Kwon",
      "Youngjung Uh"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Attribute+Based+Interpretable+Evaluation+Metrics+for+Generative+Models+Dongkyun+Kim+Mingi+Kwon+Youngjung+Uh",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "D Kim",
        "id": null
      },
      {
        "name": "M Kwon",
        "id": "W8vK8BwAAAAJ"
      },
      {
        "name": "Y Uh -",
        "id": null
      }
    ],
    "citation_count": 4
  },
  {
    "arxiv_id": "2401.12456",
    "title": "Exploration and Improvement of Nerf-based 3D Scene Editing Techniques",
    "year": 2024,
    "published": "2024-01-23T02:53:06Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR"
    ],
    "abstract": "NeRF's high-quality scene synthesis capability was quickly accepted by scholars in the years after it was proposed, and significant progress has been made in 3D scene representation and synthesis. However, the high computational cost limits intuitive and efficient editing of scenes, making NeRF's development in the scene editing field facing many challenges. This paper reviews the preliminary explorations of scholars on NeRF in the scene or object editing field in recent years, mainly changing t",
    "arxiv_url": "https://arxiv.org/abs/2401.12456v1",
    "pdf_url": "https://arxiv.org/pdf/2401.12456v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2401.12456",
    "arxiv_authors": [
      "Shun Fang",
      "Ming Cui",
      "Xing Feng",
      "Yanan Zhang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Exploration+and+Improvement+of+Nerf-based+3D+Scene+Editing+Techniques+Shun+Fang+Ming+Cui+Xing+Feng+Yanan+Zhang",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "S Fang",
        "id": null
      },
      {
        "name": "M Cui",
        "id": null
      },
      {
        "name": "X Feng",
        "id": null
      },
      {
        "name": "Y Zhang -",
        "id": null
      }
    ],
    "citation_count": 2
  },
  {
    "arxiv_id": "2504.12689",
    "title": "HSS-IAD: A Heterogeneous Same-Sort Industrial Anomaly Detection Dataset",
    "year": 2025,
    "published": "2025-04-17T06:31:26Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Multi-class Unsupervised Anomaly Detection algorithms (MUAD) are receiving increasing attention due to their relatively low deployment costs and improved training efficiency. However, the real-world effectiveness of MUAD methods is questioned due to limitations in current Industrial Anomaly Detection (IAD) datasets. These datasets contain numerous classes that are unlikely to be produced by the same factory and fail to cover multiple structures or appearances. Additionally, the defects do not re",
    "arxiv_url": "https://arxiv.org/abs/2504.12689v1",
    "pdf_url": "https://arxiv.org/pdf/2504.12689v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2504.12689",
    "arxiv_authors": [
      "Qishan Wang",
      "Shuyong Gao",
      "Junjie Hu",
      "Jiawen Yu",
      "Xuan Tong",
      "You Li",
      "Wenqiang Zhang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=HSS-IAD%3A+A+Heterogeneous+Same-Sort+Industrial+Anomaly+Detection+Dataset+Qishan+Wang+Shuyong+Gao+Junjie+Hu+Jiawen+Yu+Xuan+Tong",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "Q Wang",
        "id": "r3A6OFAAAAAJ"
      },
      {
        "name": "S Gao",
        "id": "gzuiMkcAAAAJ"
      },
      {
        "name": "J Hu",
        "id": null
      },
      {
        "name": "J Yu",
        "id": null
      },
      {
        "name": "X Tong",
        "id": null
      },
      {
        "name": "Y Li",
        "id": null
      },
      {
        "name": "W Zhang",
        "id": null
      }
    ],
    "citation_count": 1
  },
  {
    "arxiv_id": "2406.08231",
    "title": "Using Deep Convolutional Neural Networks to Detect Rendered Glitches in Video Games",
    "year": 2024,
    "published": "2024-06-12T13:59:45Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "In this paper, we present a method using Deep Convolutional Neural Networks (DCNNs) to detect common glitches in video games. The problem setting consists of an image (800x800 RGB) as input to be classified into one of five defined classes, normal image, or one of four different kinds of glitches (stretched, low resolution, missing and placeholder textures). Using a supervised approach, we train a ShuffleNetV2 using generated data. This work focuses on detecting texture graphical anomalies achie",
    "arxiv_url": "https://arxiv.org/abs/2406.08231v1",
    "pdf_url": "https://arxiv.org/pdf/2406.08231v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2406.08231",
    "arxiv_authors": [
      "Carlos Garcia Ling",
      "Konrad Tollmar",
      "Linus Gisslen"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Using+Deep+Convolutional+Neural+Networks+to+Detect+Rendered+Glitches+in+Video+Games+Carlos+Garcia+Ling+Konrad+Tollmar+Linus+Gisslen",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "C Ling",
        "id": null
      },
      {
        "name": "K Tollmar",
        "id": "5MTY7-wAAAAJ"
      },
      {
        "name": "L Gissl√©n -",
        "id": null
      }
    ],
    "citation_count": 25
  },
  {
    "arxiv_id": "2303.13779",
    "title": "Exploiting Unlabelled Photos for Stronger Fine-Grained SBIR",
    "year": 2023,
    "published": "2023-03-24T03:34:33Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "This paper advances the fine-grained sketch-based image retrieval (FG-SBIR) literature by putting forward a strong baseline that overshoots prior state-of-the-arts by ~11%. This is not via complicated design though, but by addressing two critical issues facing the community (i) the gold standard triplet loss does not enforce holistic latent space geometry, and (ii) there are never enough sketches to train a high accuracy model. For the former, we propose a simple modification to the standard tri",
    "arxiv_url": "https://arxiv.org/abs/2303.13779v1",
    "pdf_url": "https://arxiv.org/pdf/2303.13779v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2303.13779",
    "arxiv_authors": [
      "Aneeshan Sain",
      "Ayan Kumar Bhunia",
      "Subhadeep Koley",
      "Pinaki Nath Chowdhury",
      "Soumitri Chattopadhyay",
      "Tao Xiang",
      "Yi-Zhe Song"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Exploiting+Unlabelled+Photos+for+Stronger+Fine-Grained+SBIR+Aneeshan+Sain+Ayan+Kumar+Bhunia+Subhadeep+Koley+Pinaki+Nath+Chowdhury+Soumitri+Chattopadhyay",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "A Sain",
        "id": "_QWFBvoAAAAJ"
      },
      {
        "name": "AK Bhunia",
        "id": "gjslbzsAAAAJ"
      },
      {
        "name": "S Koley",
        "id": "-mOrpz8AAAAJ"
      },
      {
        "name": "PN Chowdhury",
        "id": "HE2nfp0AAAAJ"
      },
      {
        "name": "S Chattopadhyay",
        "id": "AyMx6O4AAAAJ"
      },
      {
        "name": "T Xiang",
        "id": "MeS5d4gAAAAJ"
      },
      {
        "name": "YZ Song",
        "id": "irZFP_AAAAAJ"
      }
    ],
    "citation_count": 33
  },
  {
    "arxiv_id": "2306.10531",
    "title": "GenPose: Generative Category-level Object Pose Estimation via Diffusion Models",
    "year": 2023,
    "published": "2023-06-18T11:45:42Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Object pose estimation plays a vital role in embodied AI and computer vision, enabling intelligent agents to comprehend and interact with their surroundings. Despite the practicality of category-level pose estimation, current approaches encounter challenges with partially observed point clouds, known as the multihypothesis issue. In this study, we propose a novel solution by reframing categorylevel object pose estimation as conditional generative modeling, departing from traditional point-to-poi",
    "arxiv_url": "https://arxiv.org/abs/2306.10531v3",
    "pdf_url": "https://arxiv.org/pdf/2306.10531v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2306.10531",
    "arxiv_authors": [
      "Jiyao Zhang",
      "Mingdong Wu",
      "Hao Dong"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=GenPose%3A+Generative+Category-level+Object+Pose+Estimation+via+Diffusion+Models+Jiyao+Zhang+Mingdong+Wu+Hao+Dong",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "J Zhang",
        "id": "nf1Q7P4AAAAJ"
      },
      {
        "name": "M Wu",
        "id": "MPfBNuIAAAAJ"
      },
      {
        "name": "H Dong - Advances in Neural Information",
        "id": null
      }
    ],
    "citation_count": 21
  },
  {
    "arxiv_id": "2308.10402",
    "title": "Simple Baselines for Interactive Video Retrieval with Questions and Answers",
    "year": 2023,
    "published": "2023-08-21T00:32:19Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.HC"
    ],
    "abstract": "To date, the majority of video retrieval systems have been optimized for a \"single-shot\" scenario in which the user submits a query in isolation, ignoring previous interactions with the system. Recently, there has been renewed interest in interactive systems to enhance retrieval, but existing approaches are complex and deliver limited gains in performance. In this work, we revisit this topic and propose several simple yet effective baselines for interactive video retrieval via question-answering",
    "arxiv_url": "https://arxiv.org/abs/2308.10402v1",
    "pdf_url": "https://arxiv.org/pdf/2308.10402v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2308.10402",
    "arxiv_authors": [
      "Kaiqu Liang",
      "Samuel Albanie"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Simple+Baselines+for+Interactive+Video+Retrieval+with+Questions+and+Answers+Kaiqu+Liang+Samuel+Albanie",
    "gs_search_success": false,
    "gs_authors": null,
    "citation_count": null,
    "error_type": "fetch_failed"
  },
  {
    "arxiv_id": "2407.09524",
    "title": "Geometric Understanding of Discriminability and Transferability for Visual Domain Adaptation",
    "year": 2024,
    "published": "2024-06-24T13:31:08Z",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "To overcome the restriction of identical distribution assumption, invariant representation learning for unsupervised domain adaptation (UDA) has made significant advances in computer vision and pattern recognition communities. In UDA scenario, the training and test data belong to different domains while the task model is learned to be invariant. Recently, empirical connections between transferability and discriminability have received increasing attention, which is the key to understanding the i",
    "arxiv_url": "https://arxiv.org/abs/2407.09524v1",
    "pdf_url": "https://arxiv.org/pdf/2407.09524v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2407.09524",
    "arxiv_authors": [
      "You-Wei Luo",
      "Chuan-Xian Ren",
      "Xiao-Lin Xu",
      "Qingshan Liu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Geometric+Understanding+of+Discriminability+and+Transferability+for+Visual+Domain+Adaptation+You-Wei+Luo+Chuan-Xian+Ren+Xiao-Lin+Xu+Qingshan+Liu",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "YW Luo",
        "id": "n9xRWGsAAAAJ"
      },
      {
        "name": "CX Ren",
        "id": "nWsPNkQAAAAJ"
      },
      {
        "name": "XL Xu",
        "id": null
      },
      {
        "name": "Q Liu - IEEE Transactions on Pattern",
        "id": null
      }
    ],
    "citation_count": 6
  },
  {
    "arxiv_id": "2407.15798",
    "title": "Robust Facial Reactions Generation: An Emotion-Aware Framework with Modality Compensation",
    "year": 2024,
    "published": "2024-07-22T17:00:02Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "The objective of the Multiple Appropriate Facial Reaction Generation (MAFRG) task is to produce contextually appropriate and diverse listener facial behavioural responses based on the multimodal behavioural data of the conversational partner (i.e., the speaker). Current methodologies typically assume continuous availability of speech and facial modality data, neglecting real-world scenarios where these data may be intermittently unavailable, which often results in model failures. Furthermore, de",
    "arxiv_url": "https://arxiv.org/abs/2407.15798v2",
    "pdf_url": "https://arxiv.org/pdf/2407.15798v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2407.15798",
    "arxiv_authors": [
      "Guanyu Hu",
      "Jie Wei",
      "Siyang Song",
      "Dimitrios Kollias",
      "Xinyu Yang",
      "Zhonglin Sun",
      "Odysseus Kaloidas"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Robust+Facial+Reactions+Generation%3A+An+Emotion-Aware+Framework+with+Modality+Compensation+Guanyu+Hu+Jie+Wei+Siyang+Song+Dimitrios+Kollias+Xinyu+Yang",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "G Hu",
        "id": "AxO-zOEAAAAJ"
      },
      {
        "name": "J Wei",
        "id": "mJIoa64AAAAJ"
      },
      {
        "name": "S Song",
        "id": "ZKSL1IcAAAAJ"
      },
      {
        "name": "D Kollias",
        "id": "360Gmc0AAAAJ"
      },
      {
        "name": "X Yang",
        "id": null
      },
      {
        "name": "Z Sun",
        "id": "-w2Y9dQAAAAJ"
      },
      {
        "name": "O Kaloidas2024 IEEE International Joint",
        "id": null
      }
    ],
    "citation_count": 3
  },
  {
    "arxiv_id": "2502.10724",
    "title": "Semantics-aware Test-time Adaptation for 3D Human Pose Estimation",
    "year": 2025,
    "published": "2025-02-15T08:27:18Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "This work highlights a semantics misalignment in 3D human pose estimation. For the task of test-time adaptation, the misalignment manifests as overly smoothed and unguided predictions. The smoothing settles predictions towards some average pose. Furthermore, when there are occlusions or truncations, the adaptation becomes fully unguided. To this end, we pioneer the integration of a semantics-aware motion prior for the test-time adaptation of 3D pose estimation. We leverage video understanding an",
    "arxiv_url": "https://arxiv.org/abs/2502.10724v2",
    "pdf_url": "https://arxiv.org/pdf/2502.10724v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2502.10724",
    "arxiv_authors": [
      "Qiuxia Lin",
      "Rongyu Chen",
      "Kerui Gu",
      "Angela Yao"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Semantics-aware+Test-time+Adaptation+for+3D+Human+Pose+Estimation+Qiuxia+Lin+Rongyu+Chen+Kerui+Gu+Angela+Yao",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "Q Lin",
        "id": "fUCbVmcAAAAJ"
      },
      {
        "name": "R Chen",
        "id": "gP_jm9UAAAAJ"
      },
      {
        "name": "K Gu",
        "id": "if-RXSEAAAAJ"
      },
      {
        "name": "A Yao -",
        "id": null
      }
    ],
    "citation_count": 2
  },
  {
    "arxiv_id": "2302.05105",
    "title": "Text recognition on images using pre-trained CNN",
    "year": 2023,
    "published": "2023-02-10T08:09:51Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "A text on an image often stores important information and directly carries high level semantics, makes it as important source of information and become a very active research topic. Many studies have shown that the use of CNN-based neural networks is quite effective and accurate for image classification which is the basis of text recognition. It can also be more enhanced by using transfer learning from pre-trained model trained on ImageNet dataset as an initial weight. In this research, the reco",
    "arxiv_url": "https://arxiv.org/abs/2302.05105v1",
    "pdf_url": "https://arxiv.org/pdf/2302.05105v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2302.05105",
    "arxiv_authors": [
      "Afgani Fajar Rizky",
      "Novanto Yudistira",
      "Edy Santoso"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Text+recognition+on+images+using+pre-trained+CNN+Afgani+Fajar+Rizky+Novanto+Yudistira+Edy+Santoso",
    "gs_search_success": false,
    "gs_authors": null,
    "citation_count": null,
    "error_type": "multiple_matches"
  },
  {
    "arxiv_id": "2412.12596",
    "title": "OpenViewer: Openness-Aware Multi-View Learning",
    "year": 2024,
    "published": "2024-12-17T06:54:54Z",
    "categories": [
      "cs.CV",
      "stat.AP",
      "stat.ML"
    ],
    "abstract": "Multi-view learning methods leverage multiple data sources to enhance perception by mining correlations across views, typically relying on predefined categories. However, deploying these models in real-world scenarios presents two primary openness challenges. 1) Lack of Interpretability: The integration mechanisms of multi-view data in existing black-box models remain poorly explained; 2) Insufficient Generalization: Most models are not adapted to multi-view scenarios involving unknown categorie",
    "arxiv_url": "https://arxiv.org/abs/2412.12596v1",
    "pdf_url": "https://arxiv.org/pdf/2412.12596v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2412.12596",
    "arxiv_authors": [
      "Shide Du",
      "Zihan Fang",
      "Yanchao Tan",
      "Changwei Wang",
      "Shiping Wang",
      "Wenzhong Guo"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=OpenViewer%3A+Openness-Aware+Multi-View+Learning+Shide+Du+Zihan+Fang+Yanchao+Tan+Changwei+Wang+Shiping+Wang",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "S Du",
        "id": "H9ru_e0AAAAJ"
      },
      {
        "name": "Z Fang",
        "id": null
      },
      {
        "name": "Y Tan",
        "id": "NQWuK9UAAAAJ"
      },
      {
        "name": "C Wang",
        "id": "DnJKQI8AAAAJ"
      },
      {
        "name": "S Wang",
        "id": "_Yhc9TwAAAAJ"
      },
      {
        "name": "W Guo",
        "id": null
      }
    ],
    "citation_count": 9
  },
  {
    "arxiv_id": "2307.00368",
    "title": "Minimizing Energy Consumption of Deep Learning Models by Energy-Aware Training",
    "year": 2023,
    "published": "2023-07-01T15:44:01Z",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "abstract": "Deep learning models undergo a significant increase in the number of parameters they possess, leading to the execution of a larger number of operations during inference. This expansion significantly contributes to higher energy consumption and prediction latency. In this work, we propose EAT, a gradient-based algorithm that aims to reduce energy consumption during model training. To this end, we leverage a differentiable approximation of the $\\ell_0$ norm, and use it as a sparse penalty over the",
    "arxiv_url": "https://arxiv.org/abs/2307.00368v1",
    "pdf_url": "https://arxiv.org/pdf/2307.00368v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2307.00368",
    "arxiv_authors": [
      "Dario Lazzaro",
      "Antonio Emanuele Cin√†",
      "Maura Pintor",
      "Ambra Demontis",
      "Battista Biggio",
      "Fabio Roli",
      "Marcello Pelillo"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Minimizing+Energy+Consumption+of+Deep+Learning+Models+by+Energy-Aware+Training+Dario+Lazzaro+Antonio+Emanuele+Cin%C3%A0+Maura+Pintor+Ambra+Demontis+Battista+Biggio",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "D Lazzaro",
        "id": null
      },
      {
        "name": "AE Cin√†",
        "id": "Qtj8Lb8AAAAJ"
      },
      {
        "name": "M Pintor",
        "id": "Tu45bY4AAAAJ"
      },
      {
        "name": "A Demontis",
        "id": "n_GuF3EAAAAJ"
      },
      {
        "name": "B Biggio",
        "id": "OoUIOYwAAAAJ"
      },
      {
        "name": "F Roli",
        "id": "sCypmFAAAAAJ"
      },
      {
        "name": "M PelilloInternational",
        "id": null
      }
    ],
    "citation_count": 23
  },
  {
    "arxiv_id": "2503.23353",
    "title": "Object Isolated Attention for Consistent Story Visualization",
    "year": 2025,
    "published": "2025-03-30T08:16:52Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Open-ended story visualization is a challenging task that involves generating coherent image sequences from a given storyline. One of the main difficulties is maintaining character consistency while creating natural and contextually fitting scenes--an area where many existing methods struggle. In this paper, we propose an enhanced Transformer module that uses separate self attention and cross attention mechanisms, leveraging prior knowledge from pre-trained diffusion models to ensure logical sce",
    "arxiv_url": "https://arxiv.org/abs/2503.23353v1",
    "pdf_url": "https://arxiv.org/pdf/2503.23353v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2503.23353",
    "arxiv_authors": [
      "Xiangyang Luo",
      "Junhao Cheng",
      "Yifan Xie",
      "Xin Zhang",
      "Tao Feng",
      "Zhou Liu",
      "Fei Ma",
      "Fei Yu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Object+Isolated+Attention+for+Consistent+Story+Visualization+Xiangyang+Luo+Junhao+Cheng+Yifan+Xie+Xin+Zhang+Tao+Feng",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "X Luo",
        "id": "L-3A3JgAAAAJ"
      },
      {
        "name": "J Cheng",
        "id": "NBIllU8AAAAJ"
      },
      {
        "name": "Y Xie",
        "id": "t2X8PpsAAAAJ"
      },
      {
        "name": "X Zhang",
        "id": null
      },
      {
        "name": "T Feng",
        "id": null
      },
      {
        "name": "Z Liu",
        "id": "SOvUdosAAAAJ"
      },
      {
        "name": "F Ma",
        "id": "RJOEAMYAAAAJ"
      },
      {
        "name": "F Yu",
        "id": null
      }
    ],
    "citation_count": 3
  },
  {
    "arxiv_id": "2412.12031",
    "title": "RepFace: Refining Closed-Set Noise with Progressive Label Correction for Face Recognition",
    "year": 2024,
    "published": "2024-12-16T17:57:33Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Face recognition has made remarkable strides, driven by the expanding scale of datasets, advancements in various backbone and discriminative losses. However, face recognition performance is heavily affected by the label noise, especially closed-set noise. While numerous studies have focused on handling label noise, addressing closed-set noise still poses challenges. This paper identifies this challenge as training isn't robust to noise at the early-stage training, and necessitating an appropriat",
    "arxiv_url": "https://arxiv.org/abs/2412.12031v1",
    "pdf_url": "https://arxiv.org/pdf/2412.12031v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2412.12031",
    "arxiv_authors": [
      "Jie Zhang",
      "Xun Gong",
      "Zhonglin Sun"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=RepFace%3A+Refining+Closed-Set+Noise+with+Progressive+Label+Correction+for+Face+Recognition+Jie+Zhang+Xun+Gong+Zhonglin+Sun",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "J Zhang",
        "id": null
      },
      {
        "name": "X Gong",
        "id": "7PqgUw4AAAAJ"
      },
      {
        "name": "Z Sun -",
        "id": null
      }
    ],
    "citation_count": null
  },
  {
    "arxiv_id": "2302.05087",
    "title": "Generalized Video Anomaly Event Detection: Systematic Taxonomy and Comparison of Deep Models",
    "year": 2023,
    "published": "2023-02-10T07:11:37Z",
    "categories": [
      "cs.CV",
      "cs.MM"
    ],
    "abstract": "Video Anomaly Detection (VAD) serves as a pivotal technology in the intelligent surveillance systems, enabling the temporal or spatial identification of anomalous events within videos. While existing reviews predominantly concentrate on conventional unsupervised methods, they often overlook the emergence of weakly-supervised and fully-unsupervised approaches. To address this gap, this survey extends the conventional scope of VAD beyond unsupervised methods, encompassing a broader spectrum termed",
    "arxiv_url": "https://arxiv.org/abs/2302.05087v3",
    "pdf_url": "https://arxiv.org/pdf/2302.05087v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2302.05087",
    "arxiv_authors": [
      "Yang Liu",
      "Dingkang Yang",
      "Yan Wang",
      "Jing Liu",
      "Jun Liu",
      "Azzedine Boukerche",
      "Peng Sun",
      "Liang Song"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Generalized+Video+Anomaly+Event+Detection%3A+Systematic+Taxonomy+and+Comparison+of+Deep+Models+Yang+Liu+Dingkang+Yang+Yan+Wang+Jing+Liu+Jun+Liu",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "Y Liu",
        "id": "kA7yH9QAAAAJ"
      },
      {
        "name": "D Yang",
        "id": "jvlDhkcAAAAJ"
      },
      {
        "name": "Y Wang",
        "id": "RQSDgFkAAAAJ"
      },
      {
        "name": "J Liu",
        "id": "Q5Ild8UAAAAJ"
      },
      {
        "name": "J Liu",
        "id": "Q5Ild8UAAAAJ"
      },
      {
        "name": "A Boukerche",
        "id": "RMo9P4oAAAAJ"
      },
      {
        "name": "P Sun",
        "id": "sLt7XScAAAAJ"
      }
    ],
    "citation_count": 151
  },
  {
    "arxiv_id": "2406.11831",
    "title": "Exploring the Role of Large Language Models in Prompt Encoding for Diffusion Models",
    "year": 2024,
    "published": "2024-06-17T17:59:43Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Large language models (LLMs) based on decoder-only transformers have demonstrated superior text understanding capabilities compared to CLIP and T5-series models. However, the paradigm for utilizing current advanced LLMs in text-to-image diffusion models remains to be explored. We observed an unusual phenomenon: directly using a large language model as the prompt encoder significantly degrades the prompt-following ability in image generation. We identified two main obstacles behind this issue. On",
    "arxiv_url": "https://arxiv.org/abs/2406.11831v3",
    "pdf_url": "https://arxiv.org/pdf/2406.11831v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2406.11831",
    "arxiv_authors": [
      "Bingqi Ma",
      "Zhuofan Zong",
      "Guanglu Song",
      "Hongsheng Li",
      "Yu Liu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Exploring+the+Role+of+Large+Language+Models+in+Prompt+Encoding+for+Diffusion+Models+Bingqi+Ma+Zhuofan+Zong+Guanglu+Song+Hongsheng+Li+Yu+Liu",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "B Ma",
        "id": "rcWQWCoAAAAJ"
      },
      {
        "name": "Z Zong",
        "id": "vls0YhoAAAAJ"
      },
      {
        "name": "G Song",
        "id": "Bd3v08QAAAAJ"
      },
      {
        "name": "H Li",
        "id": "BN2Ze-QAAAAJ"
      },
      {
        "name": "Y Liu -",
        "id": null
      }
    ],
    "citation_count": 37
  },
  {
    "arxiv_id": "2502.01524",
    "title": "Efficiently Integrate Large Language Models with Visual Perception: A Survey from the Training Paradigm Perspective",
    "year": 2025,
    "published": "2025-02-03T17:01:59Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "abstract": "The integration of vision-language modalities has been a significant focus in multimodal learning, traditionally relying on Vision-Language Pretrained Models. However, with the advent of Large Language Models (LLMs), there has been a notable shift towards incorporating LLMs with vision modalities. Following this, the training paradigms for incorporating vision modalities into LLMs have evolved. Initially, the approach was to integrate the modalities through pretraining the modality integrator, n",
    "arxiv_url": "https://arxiv.org/abs/2502.01524v1",
    "pdf_url": "https://arxiv.org/pdf/2502.01524v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2502.01524",
    "arxiv_authors": [
      "Xiaorui Ma",
      "Haoran Xie",
      "S. Joe Qin"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Efficiently+Integrate+Large+Language+Models+with+Visual+Perception%3A+A+Survey+from+the+Training+Paradigm+Perspective+Xiaorui+Ma+Haoran+Xie+S.+Joe+Qin",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "X Ma",
        "id": "KHFek2kAAAAJ"
      },
      {
        "name": "H Xie",
        "id": "O4lGUj8AAAAJ"
      },
      {
        "name": "SJ Qin - Information Fusion",
        "id": null
      }
    ],
    "citation_count": 5
  },
  {
    "arxiv_id": "2312.06699",
    "title": "Leveraging Generative Language Models for Weakly Supervised Sentence Component Analysis in Video-Language Joint Learning",
    "year": 2023,
    "published": "2023-12-10T02:03:51Z",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "A thorough comprehension of textual data is a fundamental element in multi-modal video analysis tasks. However, recent works have shown that the current models do not achieve a comprehensive understanding of the textual data during the training for the target downstream tasks. Orthogonal to the previous approaches to this limitation, we postulate that understanding the significance of the sentence components according to the target task can potentially enhance the performance of the models. Henc",
    "arxiv_url": "https://arxiv.org/abs/2312.06699v1",
    "pdf_url": "https://arxiv.org/pdf/2312.06699v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2312.06699",
    "arxiv_authors": [
      "Zaber Ibn Abdul Hakim",
      "Najibul Haque Sarker",
      "Rahul Pratap Singh",
      "Bishmoy Paul",
      "Ali Dabouei",
      "Min Xu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Leveraging+Generative+Language+Models+for+Weakly+Supervised+Sentence+Component+Analysis+in+Video-Language+Joint+Learning+Zaber+Ibn+Abdul+Hakim+Najibul+Haque+Sarker+Rahul+Pratap+Singh+Bishmoy+Paul+Ali+Dabouei",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "ZIA Hakim",
        "id": "fcg9BRAAAAAJ"
      },
      {
        "name": "NH Sarker",
        "id": "MHPDy_AAAAAJ"
      },
      {
        "name": "RP Singh",
        "id": null
      },
      {
        "name": "B Paul",
        "id": "jRWGiboAAAAJ"
      },
      {
        "name": "A Dabouei",
        "id": "t5FcQqUAAAAJ"
      },
      {
        "name": "M Xu",
        "id": "Y3Cqt0cAAAAJ"
      }
    ],
    "citation_count": 4
  },
  {
    "arxiv_id": "2502.09993",
    "title": "Navigating Label Ambiguity for Facial Expression Recognition in the Wild",
    "year": 2025,
    "published": "2025-02-14T08:24:38Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Facial expression recognition (FER) remains a challenging task due to label ambiguity caused by the subjective nature of facial expressions and noisy samples. Additionally, class imbalance, which is common in real-world datasets, further complicates FER. Although many studies have shown impressive improvements, they typically address only one of these issues, leading to suboptimal results. To tackle both challenges simultaneously, we propose a novel framework called Navigating Label Ambiguity (N",
    "arxiv_url": "https://arxiv.org/abs/2502.09993v1",
    "pdf_url": "https://arxiv.org/pdf/2502.09993v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2502.09993",
    "arxiv_authors": [
      "JunGyu Lee",
      "Yeji Choi",
      "Haksub Kim",
      "Ig-Jae Kim",
      "Gi Pyo Nam"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Navigating+Label+Ambiguity+for+Facial+Expression+Recognition+in+the+Wild+JunGyu+Lee+Yeji+Choi+Haksub+Kim+Ig-Jae+Kim+Gi+Pyo+Nam",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "JG Lee",
        "id": null
      },
      {
        "name": "Y Choi",
        "id": "IWvP0A4AAAAJ"
      },
      {
        "name": "H Kim",
        "id": "F5z0unQAAAAJ"
      },
      {
        "name": "IJ Kim",
        "id": "mkpLT20AAAAJ"
      },
      {
        "name": "GP Nam -",
        "id": null
      }
    ],
    "citation_count": 2
  },
  {
    "arxiv_id": "2312.13252",
    "title": "Zero-Shot Metric Depth with a Field-of-View Conditioned Diffusion Model",
    "year": 2023,
    "published": "2023-12-20T18:27:47Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "While methods for monocular depth estimation have made significant strides on standard benchmarks, zero-shot metric depth estimation remains unsolved. Challenges include the joint modeling of indoor and outdoor scenes, which often exhibit significantly different distributions of RGB and depth, and the depth-scale ambiguity due to unknown camera intrinsics. Recent work has proposed specialized multi-head architectures for jointly modeling indoor and outdoor scenes. In contrast, we advocate a gene",
    "arxiv_url": "https://arxiv.org/abs/2312.13252v1",
    "pdf_url": "https://arxiv.org/pdf/2312.13252v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2312.13252",
    "arxiv_authors": [
      "Saurabh Saxena",
      "Junhwa Hur",
      "Charles Herrmann",
      "Deqing Sun",
      "David J. Fleet"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Zero-Shot+Metric+Depth+with+a+Field-of-View+Conditioned+Diffusion+Model+Saurabh+Saxena+Junhwa+Hur+Charles+Herrmann+Deqing+Sun+David+J.+Fleet",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "S Saxena",
        "id": "WTz38osAAAAJ"
      },
      {
        "name": "J Hur",
        "id": "z4dNJdkAAAAJ"
      },
      {
        "name": "C Herrmann",
        "id": "LQvi5XAAAAAJ"
      },
      {
        "name": "D Sun",
        "id": "t4rgICIAAAAJ"
      },
      {
        "name": "DJ Fleet",
        "id": "njOmQFsAAAAJ"
      }
    ],
    "citation_count": 29
  },
  {
    "arxiv_id": "2308.03183",
    "title": "Photorealistic and Identity-Preserving Image-Based Emotion Manipulation with Latent Diffusion Models",
    "year": 2023,
    "published": "2023-08-06T18:28:26Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "In this paper, we investigate the emotion manipulation capabilities of diffusion models with \"in-the-wild\" images, a rather unexplored application area relative to the vast and rapidly growing literature for image-to-image translation tasks. Our proposed method encapsulates several pieces of prior work, with the most important being Latent Diffusion models and text-driven manipulation with CLIP latents. We conduct extensive qualitative and quantitative evaluations on AffectNet, demonstrating the",
    "arxiv_url": "https://arxiv.org/abs/2308.03183v1",
    "pdf_url": "https://arxiv.org/pdf/2308.03183v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2308.03183",
    "arxiv_authors": [
      "Ioannis Pikoulis",
      "Panagiotis P. Filntisis",
      "Petros Maragos"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Photorealistic+and+Identity-Preserving+Image-Based+Emotion+Manipulation+with+Latent+Diffusion+Models+Ioannis+Pikoulis+Panagiotis+P.+Filntisis+Petros+Maragos",
    "gs_search_success": false,
    "gs_authors": null,
    "citation_count": null,
    "error_type": "fetch_failed"
  },
  {
    "arxiv_id": "2502.20100",
    "title": "Generative augmentations for improved cardiac ultrasound segmentation using diffusion models",
    "year": 2025,
    "published": "2025-02-27T13:57:14Z",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "abstract": "One of the main challenges in current research on segmentation in cardiac ultrasound is the lack of large and varied labeled datasets and the differences in annotation conventions between datasets. This makes it difficult to design robust segmentation models that generalize well to external datasets. This work utilizes diffusion models to create generative augmentations that can significantly improve diversity of the dataset and thus the generalisability of segmentation models without the need f",
    "arxiv_url": "https://arxiv.org/abs/2502.20100v1",
    "pdf_url": "https://arxiv.org/pdf/2502.20100v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2502.20100",
    "arxiv_authors": [
      "Gilles Van De Vyver",
      "Aksel Try Lenz",
      "Erik Smistad",
      "Sindre Hellum Olaisen",
      "Bj√∏rnar Grenne",
      "Espen Holte",
      "H√•avard Dalen",
      "Lasse L√∏vstakken"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Generative+augmentations+for+improved+cardiac+ultrasound+segmentation+using+diffusion+models+Gilles+Van+De+Vyver+Aksel+Try+Lenz+Erik+Smistad+Sindre+Hellum+Olaisen+Bj%C3%B8rnar+Grenne",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "G Van De Vyver",
        "id": "_3BVcxUAAAAJ"
      },
      {
        "name": "AT Lenz",
        "id": null
      },
      {
        "name": "E Smistad",
        "id": "bSPPQaIAAAAJ"
      },
      {
        "name": "SH Olaisen",
        "id": null
      },
      {
        "name": "B Grenne",
        "id": null
      },
      {
        "name": "E Holte",
        "id": null
      },
      {
        "name": "H Dalen",
        "id": null
      },
      {
        "name": "L L√∏vstakkenScientific Reports",
        "id": null
      }
    ],
    "citation_count": null
  },
  {
    "arxiv_id": "2405.02363",
    "title": "LLM as Dataset Analyst: Subpopulation Structure Discovery with Large Language Model",
    "year": 2024,
    "published": "2024-05-03T05:09:54Z",
    "categories": [
      "cs.CV",
      "cs.CL"
    ],
    "abstract": "The distribution of subpopulations is an important property hidden within a dataset. Uncovering and analyzing the subpopulation distribution within datasets provides a comprehensive understanding of the datasets, standing as a powerful tool beneficial to various downstream tasks, including Dataset Subpopulation Organization, Subpopulation Shift, and Slice Discovery. Despite its importance, there has been no work that systematically explores the subpopulation distribution of datasets to our knowl",
    "arxiv_url": "https://arxiv.org/abs/2405.02363v2",
    "pdf_url": "https://arxiv.org/pdf/2405.02363v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2405.02363",
    "arxiv_authors": [
      "Yulin Luo",
      "Ruichuan An",
      "Bocheng Zou",
      "Yiming Tang",
      "Jiaming Liu",
      "Shanghang Zhang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=LLM+as+Dataset+Analyst%3A+Subpopulation+Structure+Discovery+with+Large+Language+Model+Yulin+Luo+Ruichuan+An+Bocheng+Zou+Yiming+Tang+Jiaming+Liu",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "Y Luo",
        "id": "SgeV4NkAAAAJ"
      },
      {
        "name": "R An",
        "id": "R5iSLPQAAAAJ"
      },
      {
        "name": "B Zou",
        "id": "orGwqXIAAAAJ"
      },
      {
        "name": "Y Tang",
        "id": "0WYAYQ8AAAAJ"
      },
      {
        "name": "J Liu",
        "id": "cPki5sUAAAAJ"
      },
      {
        "name": "S Zhang - European",
        "id": null
      }
    ],
    "citation_count": 9
  },
  {
    "arxiv_id": "2405.07178",
    "title": "Hologram: Realtime Holographic Overlays via LiDAR Augmented Reconstruction",
    "year": 2024,
    "published": "2024-05-12T06:35:10Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Guided by the hologram technology of the infamous Star Wars franchise, I present an application that creates real-time holographic overlays using LiDAR augmented 3D reconstruction. Prior attempts involve SLAM or NeRFs which either require highly calibrated scenes, incur steep computation costs, or fail to render dynamic scenes. I propose 3 high-fidelity reconstruction tools that can run on a portable device, such as a iPhone 14 Pro, which can allow for metric accurate facial reconstructions. My ",
    "arxiv_url": "https://arxiv.org/abs/2405.07178v1",
    "pdf_url": "https://arxiv.org/pdf/2405.07178v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2405.07178",
    "arxiv_authors": [
      "Ekansh Agrawal"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Hologram%3A+Realtime+Holographic+Overlays+via+LiDAR+Augmented+Reconstruction+Ekansh+Agrawal",
    "gs_search_success": false,
    "gs_authors": null,
    "citation_count": null,
    "error_type": "fetch_failed"
  },
  {
    "arxiv_id": "2406.13763",
    "title": "Through the Theory of Mind's Eye: Reading Minds with Multimodal Video Large Language Models",
    "year": 2024,
    "published": "2024-06-19T18:24:31Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Can large multimodal models have a human-like ability for emotional and social reasoning, and if so, how does it work? Recent research has discovered emergent theory-of-mind (ToM) reasoning capabilities in large language models (LLMs). LLMs can reason about people's mental states by solving various text-based ToM tasks that ask questions about the actors' ToM (e.g., human belief, desire, intention). However, human reasoning in the wild is often grounded in dynamic scenes across time. Thus, we co",
    "arxiv_url": "https://arxiv.org/abs/2406.13763v2",
    "pdf_url": "https://arxiv.org/pdf/2406.13763v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2406.13763",
    "arxiv_authors": [
      "Zhawnen Chen",
      "Tianchun Wang",
      "Yizhou Wang",
      "Michal Kosinski",
      "Xiang Zhang",
      "Yun Fu",
      "Sheng Li"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Through+the+Theory+of+Mind%27s+Eye%3A+Reading+Minds+with+Multimodal+Video+Large+Language+Models+Zhawnen+Chen+Tianchun+Wang+Yizhou+Wang+Michal+Kosinski+Xiang+Zhang",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "Z Chen",
        "id": "_ZKWfJ0AAAAJ"
      },
      {
        "name": "T Wang",
        "id": "8su8b60AAAAJ"
      },
      {
        "name": "Y Wang",
        "id": "H4kqV1MAAAAJ"
      },
      {
        "name": "M Kosinski",
        "id": "01-XV0YAAAAJ"
      },
      {
        "name": "X Zhang",
        "id": null
      },
      {
        "name": "Y Fu",
        "id": "h-JEcQ8AAAAJ"
      },
      {
        "name": "S Li",
        "id": "DEncVcYAAAAJ"
      }
    ],
    "citation_count": 9
  },
  {
    "arxiv_id": "2411.14205",
    "title": "Is this Generated Person Existed in Real-world? Fine-grained Detecting and Calibrating Abnormal Human-body",
    "year": 2024,
    "published": "2024-11-21T15:13:38Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Recent improvements in visual synthesis have significantly enhanced the depiction of generated human photos, which are pivotal due to their wide applicability and demand. Nonetheless, the existing text-to-image or text-to-video models often generate low-quality human photos that might differ considerably from real-world body structures, referred to as \"abnormal human bodies\". Such abnormalities, typically deemed unacceptable, pose considerable challenges in the detection and repair of them withi",
    "arxiv_url": "https://arxiv.org/abs/2411.14205v1",
    "pdf_url": "https://arxiv.org/pdf/2411.14205v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2411.14205",
    "arxiv_authors": [
      "Zeqing Wang",
      "Qingyang Ma",
      "Wentao Wan",
      "Haojie Li",
      "Keze Wang",
      "Yonghong Tian"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Is+this+Generated+Person+Existed+in+Real-world%3F+Fine-grained+Detecting+and+Calibrating+Abnormal+Human-body+Zeqing+Wang+Qingyang+Ma+Wentao+Wan+Haojie+Li+Keze+Wang",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "Z Wang",
        "id": "K5TojPQAAAAJ"
      },
      {
        "name": "Q Ma",
        "id": "Zhzi3ZcAAAAJ"
      },
      {
        "name": "W Wan",
        "id": "xYGwtHkAAAAJ"
      },
      {
        "name": "H Li",
        "id": null
      },
      {
        "name": "K Wang",
        "id": "Qirk2fYAAAAJ"
      },
      {
        "name": "Y Tian",
        "id": null
      }
    ],
    "citation_count": 8
  },
  {
    "arxiv_id": "2304.12624",
    "title": "Shape-Net: Room Layout Estimation from Panoramic Images Robust to Occlusion using Knowledge Distillation with 3D Shapes as Additional Inputs",
    "year": 2023,
    "published": "2023-04-25T07:45:43Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Estimating the layout of a room from a single-shot panoramic image is important in virtual/augmented reality and furniture layout simulation. This involves identifying three-dimensional (3D) geometry, such as the location of corners and boundaries, and performing 3D reconstruction. However, occlusion is a common issue that can negatively impact room layout estimation, and this has not been thoroughly studied to date. It is possible to obtain 3D shape information of rooms as drawings of buildings",
    "arxiv_url": "https://arxiv.org/abs/2304.12624v1",
    "pdf_url": "https://arxiv.org/pdf/2304.12624v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2304.12624",
    "arxiv_authors": [
      "Mizuki Tabata",
      "Kana Kurata",
      "Junichiro Tamamatsu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Shape-Net%3A+Room+Layout+Estimation+from+Panoramic+Images+Robust+to+Occlusion+using+Knowledge+Distillation+with+3D+Shapes+as+Additional+Inputs+Mizuki+Tabata+Kana+Kurata+Junichiro+Tamamatsu",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "M Tabata",
        "id": "E3jp28AAAAAJ"
      },
      {
        "name": "K Kurata",
        "id": null
      },
      {
        "name": "J Tamamatsu",
        "id": null
      }
    ],
    "citation_count": 11
  },
  {
    "arxiv_id": "2404.13541",
    "title": "Generalizable Novel-View Synthesis using a Stereo Camera",
    "year": 2024,
    "published": "2024-04-21T05:39:44Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "In this paper, we propose the first generalizable view synthesis approach that specifically targets multi-view stereo-camera images. Since recent stereo matching has demonstrated accurate geometry prediction, we introduce stereo matching into novel-view synthesis for high-quality geometry reconstruction. To this end, this paper proposes a novel framework, dubbed StereoNeRF, which integrates stereo matching into a NeRF-based generalizable view synthesis approach. StereoNeRF is equipped with three",
    "arxiv_url": "https://arxiv.org/abs/2404.13541v1",
    "pdf_url": "https://arxiv.org/pdf/2404.13541v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2404.13541",
    "arxiv_authors": [
      "Haechan Lee",
      "Wonjoon Jin",
      "Seung-Hwan Baek",
      "Sunghyun Cho"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Generalizable+Novel-View+Synthesis+using+a+Stereo+Camera+Haechan+Lee+Wonjoon+Jin+Seung-Hwan+Baek+Sunghyun+Cho",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "H Lee",
        "id": null
      },
      {
        "name": "W Jin",
        "id": "6zuOYBwAAAAJ"
      },
      {
        "name": "SH Baek",
        "id": "IX93v14AAAAJ"
      },
      {
        "name": "S Cho -",
        "id": null
      }
    ],
    "citation_count": 7
  },
  {
    "arxiv_id": "2308.04798",
    "title": "Enhancing Mobile Privacy and Security: A Face Skin Patch-Based Anti-Spoofing Approach",
    "year": 2023,
    "published": "2023-08-09T08:36:13Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "As Facial Recognition System(FRS) is widely applied in areas such as access control and mobile payments due to its convenience and high accuracy. The security of facial recognition is also highly regarded. The Face anti-spoofing system(FAS) for face recognition is an important component used to enhance the security of face recognition systems. Traditional FAS used images containing identity information to detect spoofing traces, however there is a risk of privacy leakage during the transmission ",
    "arxiv_url": "https://arxiv.org/abs/2308.04798v1",
    "pdf_url": "https://arxiv.org/pdf/2308.04798v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2308.04798",
    "arxiv_authors": [
      "Qiushi Guo"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Enhancing+Mobile+Privacy+and+Security%3A+A+Face+Skin+Patch-Based+Anti-Spoofing+Approach+Qiushi+Guo",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "Q Guo",
        "id": "mrRCeSMAAAAJ"
      },
      {
        "name": "Y Chen",
        "id": null
      },
      {
        "name": "S Liao -",
        "id": null
      }
    ],
    "citation_count": 2
  },
  {
    "arxiv_id": "2407.20987",
    "title": "PIXELMOD: Improving Soft Moderation of Visual Misleading Information on Twitter",
    "year": 2024,
    "published": "2024-07-30T17:21:32Z",
    "categories": [
      "cs.CV",
      "cs.CY"
    ],
    "abstract": "Images are a powerful and immediate vehicle to carry misleading or outright false messages, yet identifying image-based misinformation at scale poses unique challenges. In this paper, we present PIXELMOD, a system that leverages perceptual hashes, vector databases, and optical character recognition (OCR) to efficiently identify images that are candidates to receive soft moderation labels on Twitter. We show that PIXELMOD outperforms existing image similarity approaches when applied to soft moder",
    "arxiv_url": "https://arxiv.org/abs/2407.20987v1",
    "pdf_url": "https://arxiv.org/pdf/2407.20987v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2407.20987",
    "arxiv_authors": [
      "Pujan Paudel",
      "Chen Ling",
      "Jeremy Blackburn",
      "Gianluca Stringhini"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=PIXELMOD%3A+Improving+Soft+Moderation+of+Visual+Misleading+Information+on+Twitter+Pujan+Paudel+Chen+Ling+Jeremy+Blackburn+Gianluca+Stringhini",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "P Paudel",
        "id": "8K4IiBwAAAAJ"
      },
      {
        "name": "C Ling",
        "id": "i9KCc1cAAAAJ"
      },
      {
        "name": "J Blackburn",
        "id": "W_ApnIUAAAAJ"
      },
      {
        "name": "G Stringhini - 33rd USENIX Security",
        "id": null
      }
    ],
    "citation_count": 2
  },
  {
    "arxiv_id": "2505.19218",
    "title": "Advancing Video Self-Supervised Learning via Image Foundation Models",
    "year": 2025,
    "published": "2025-05-25T16:25:39Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "In the past decade, image foundation models (IFMs) have achieved unprecedented progress. However, the potential of directly using IFMs for video self-supervised representation learning has largely been overlooked. In this study, we propose an advancing video self-supervised learning (AdViSe) approach, aimed at significantly reducing the training overhead of video representation models using pre-trained IFMs. Specifically, we first introduce temporal modeling modules (ResNet3D) to IFMs, construct",
    "arxiv_url": "https://arxiv.org/abs/2505.19218v1",
    "pdf_url": "https://arxiv.org/pdf/2505.19218v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2505.19218",
    "arxiv_authors": [
      "Jingwei Wu",
      "Zhewei Huang",
      "Chang Liu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Advancing+Video+Self-Supervised+Learning+via+Image+Foundation+Models+Jingwei+Wu+Zhewei+Huang+Chang+Liu",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "J Wu",
        "id": null
      },
      {
        "name": "Z Huang",
        "id": "zJEkaG8AAAAJ"
      },
      {
        "name": "C Liu - Pattern Recognition Letters",
        "id": null
      }
    ],
    "citation_count": null
  },
  {
    "arxiv_id": "2303.03761",
    "title": "Graph Neural Networks in Vision-Language Image Understanding: A Survey",
    "year": 2023,
    "published": "2023-03-07T09:56:23Z",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "2D image understanding is a complex problem within computer vision, but it holds the key to providing human-level scene comprehension. It goes further than identifying the objects in an image, and instead, it attempts to understand the scene. Solutions to this problem form the underpinning of a range of tasks, including image captioning, visual question answering (VQA), and image retrieval. Graphs provide a natural way to represent the relational arrangement between objects in an image, and thus",
    "arxiv_url": "https://arxiv.org/abs/2303.03761v2",
    "pdf_url": "https://arxiv.org/pdf/2303.03761v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2303.03761",
    "arxiv_authors": [
      "Henry Senior",
      "Gregory Slabaugh",
      "Shanxin Yuan",
      "Luca Rossi"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Graph+Neural+Networks+in+Vision-Language+Image+Understanding%3A+A+Survey+Henry+Senior+Gregory+Slabaugh+Shanxin+Yuan+Luca+Rossi",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "H Senior",
        "id": "tL5MtX0AAAAJ"
      },
      {
        "name": "G Slabaugh",
        "id": "oUK2gu8AAAAJ"
      },
      {
        "name": "S Yuan",
        "id": "htlcuX4AAAAJ"
      }
    ],
    "citation_count": 37
  },
  {
    "arxiv_id": "2503.00266",
    "title": "Flow Matching for Medical Image Synthesis: Bridging the Gap Between Speed and Quality",
    "year": 2025,
    "published": "2025-03-01T00:49:47Z",
    "categories": [
      "cs.CV",
      "eess.IV"
    ],
    "abstract": "Deep learning models have emerged as a powerful tool for various medical applications. However, their success depends on large, high-quality datasets that are challenging to obtain due to privacy concerns and costly annotation. Generative models, such as diffusion models, offer a potential solution by synthesizing medical images, but their practical adoption is hindered by long inference times. In this paper, we propose the use of an optimal transport flow matching approach to accelerate image g",
    "arxiv_url": "https://arxiv.org/abs/2503.00266v1",
    "pdf_url": "https://arxiv.org/pdf/2503.00266v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2503.00266",
    "arxiv_authors": [
      "Milad Yazdani",
      "Yasamin Medghalchi",
      "Pooria Ashrafian",
      "Ilker Hacihaliloglu",
      "Dena Shahriari"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Flow+Matching+for+Medical+Image+Synthesis%3A+Bridging+the+Gap+Between+Speed+and+Quality+Milad+Yazdani+Yasamin+Medghalchi+Pooria+Ashrafian+Ilker+Hacihaliloglu+Dena+Shahriari",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "M Yazdani",
        "id": "L6nBGM8AAAAJ"
      },
      {
        "name": "Y Medghalchi",
        "id": "ascG1x0AAAAJ"
      },
      {
        "name": "P Ashrafian",
        "id": "Rpp_1QsAAAAJ"
      },
      {
        "name": "I Hacihaliloglu",
        "id": "dA7G64kAAAAJ"
      },
      {
        "name": "D ShahriariInternational",
        "id": null
      }
    ],
    "citation_count": 6
  },
  {
    "arxiv_id": "2504.11286",
    "title": "Lightweight Medical Image Restoration via Integrating Reliable Lesion-Semantic Driven Prior",
    "year": 2025,
    "published": "2025-04-15T15:26:28Z",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "abstract": "Medical image restoration tasks aim to recover high-quality images from degraded observations, exhibiting emergent desires in many clinical scenarios, such as low-dose CT image denoising, MRI super-resolution, and MRI artifact removal. Despite the success achieved by existing deep learning-based restoration methods with sophisticated modules, they struggle with rendering computationally-efficient reconstruction results. Moreover, they usually ignore the reliability of the restoration results, wh",
    "arxiv_url": "https://arxiv.org/abs/2504.11286v2",
    "pdf_url": "https://arxiv.org/pdf/2504.11286v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2504.11286",
    "arxiv_authors": [
      "Pengcheng Zheng",
      "Kecheng Chen",
      "Jiaxin Huang",
      "Bohao Chen",
      "Ju Liu",
      "Yazhou Ren",
      "Xiaorong Pu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Lightweight+Medical+Image+Restoration+via+Integrating+Reliable+Lesion-Semantic+Driven+Prior+Pengcheng+Zheng+Kecheng+Chen+Jiaxin+Huang+Bohao+Chen+Ju+Liu",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "P Zheng",
        "id": "ndtvzgsAAAAJ"
      },
      {
        "name": "K Chen",
        "id": "xE3hzToAAAAJ"
      },
      {
        "name": "J Huang",
        "id": null
      },
      {
        "name": "B Chen",
        "id": null
      },
      {
        "name": "J Liu",
        "id": null
      },
      {
        "name": "Y Ren",
        "id": "M7Ocw0YAAAAJ"
      },
      {
        "name": "X Pu",
        "id": null
      }
    ],
    "citation_count": 1
  },
  {
    "arxiv_id": "2310.03890",
    "title": "Accelerated Neural Network Training with Rooted Logistic Objectives",
    "year": 2023,
    "published": "2023-10-05T20:49:48Z",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "abstract": "Many neural networks deployed in the real world scenarios are trained using cross entropy based loss functions. From the optimization perspective, it is known that the behavior of first order methods such as gradient descent crucially depend on the separability of datasets. In fact, even in the most simplest case of binary classification, the rate of convergence depends on two factors: (1) condition number of data matrix, and (2) separability of the dataset. With no further pre-processing techni",
    "arxiv_url": "https://arxiv.org/abs/2310.03890v1",
    "pdf_url": "https://arxiv.org/pdf/2310.03890v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2310.03890",
    "arxiv_authors": [
      "Zhu Wang",
      "Praveen Raj Veluswami",
      "Harsh Mishra",
      "Sathya N. Ravi"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Accelerated+Neural+Network+Training+with+Rooted+Logistic+Objectives+Zhu+Wang+Praveen+Raj+Veluswami+Harsh+Mishra+Sathya+N.+Ravi",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "Z Wang",
        "id": "mMyQX4oAAAAJ"
      },
      {
        "name": "PR Veluswami",
        "id": null
      },
      {
        "name": "H Mishra",
        "id": "Z2K6oOwAAAAJ"
      },
      {
        "name": "SN Ravi -",
        "id": null
      }
    ],
    "citation_count": null
  },
  {
    "arxiv_id": "2302.02755",
    "title": "Fine-Grained Action Detection with RGB and Pose Information using Two Stream Convolutional Networks",
    "year": 2023,
    "published": "2023-02-06T13:05:55Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "cs.MM"
    ],
    "abstract": "As participants of the MediaEval 2022 Sport Task, we propose a two-stream network approach for the classification and detection of table tennis strokes. Each stream is a succession of 3D Convolutional Neural Network (CNN) blocks using attention mechanisms. Each stream processes different 4D inputs. Our method utilizes raw RGB data and pose information computed from MMPose toolbox. The pose information is treated as an image by applying the pose either on a black background or on the original RGB",
    "arxiv_url": "https://arxiv.org/abs/2302.02755v1",
    "pdf_url": "https://arxiv.org/pdf/2302.02755v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2302.02755",
    "arxiv_authors": [
      "Leonard Hacker",
      "Finn Bartels",
      "Pierre-Etienne Martin"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Fine-Grained+Action+Detection+with+RGB+and+Pose+Information+using+Two+Stream+Convolutional+Networks+Leonard+Hacker+Finn+Bartels+Pierre-Etienne+Martin",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "L Hacker",
        "id": null
      },
      {
        "name": "F Bartels",
        "id": null
      },
      {
        "name": "PE Martin -",
        "id": null
      }
    ],
    "citation_count": 7
  },
  {
    "arxiv_id": "2303.08695",
    "title": "RefiNeRF: Modelling dynamic neural radiance fields with inconsistent or missing camera parameters",
    "year": 2023,
    "published": "2023-03-15T15:27:18Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Novel view synthesis (NVS) is a challenging task in computer vision that involves synthesizing new views of a scene from a limited set of input images. Neural Radiance Fields (NeRF) have emerged as a powerful approach to address this problem, but they require accurate knowledge of camera \\textit{intrinsic} and \\textit{extrinsic} parameters. Traditionally, structure-from-motion (SfM) and multi-view stereo (MVS) approaches have been used to extract camera parameters, but these methods can be unrel",
    "arxiv_url": "https://arxiv.org/abs/2303.08695v1",
    "pdf_url": "https://arxiv.org/pdf/2303.08695v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2303.08695",
    "arxiv_authors": [
      "Shuja Khalid",
      "Frank Rudzicz"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=RefiNeRF%3A+Modelling+dynamic+neural+radiance+fields+with+inconsistent+or+missing+camera+parameters+Shuja+Khalid+Frank+Rudzicz",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "S Khalid",
        "id": "1Fa69AYAAAAJ"
      },
      {
        "name": "F Rudzicz -",
        "id": null
      }
    ],
    "citation_count": 5
  },
  {
    "arxiv_id": "2403.10242",
    "title": "GeoGS3D: Single-view 3D Reconstruction via Geometric-aware Diffusion Model and Gaussian Splatting",
    "year": 2024,
    "published": "2024-03-15T12:24:36Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "We introduce GeoGS3D, a novel two-stage framework for reconstructing detailed 3D objects from single-view images. Inspired by the success of pre-trained 2D diffusion models, our method incorporates an orthogonal plane decomposition mechanism to extract 3D geometric features from the 2D input, facilitating the generation of multi-view consistent images. During the following Gaussian Splatting, these images are fused with epipolar attention, fully utilizing the geometric correlations across views.",
    "arxiv_url": "https://arxiv.org/abs/2403.10242v2",
    "pdf_url": "https://arxiv.org/pdf/2403.10242v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2403.10242",
    "arxiv_authors": [
      "Qijun Feng",
      "Zhen Xing",
      "Zuxuan Wu",
      "Yu-Gang Jiang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=GeoGS3D%3A+Single-view+3D+Reconstruction+via+Geometric-aware+Diffusion+Model+and+Gaussian+Splatting+Qijun+Feng+Zhen+Xing+Zuxuan+Wu+Yu-Gang+Jiang",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "Q Feng",
        "id": "wQ-Pfa0AAAAJ"
      },
      {
        "name": "Z Xing",
        "id": "yuiXa5EAAAAJ"
      },
      {
        "name": "Z Wu",
        "id": "7t12hVkAAAAJ"
      },
      {
        "name": "YG Jiang -",
        "id": null
      }
    ],
    "citation_count": 2
  },
  {
    "arxiv_id": "2407.15707",
    "title": "Predicting the Best of N Visual Trackers",
    "year": 2024,
    "published": "2024-07-22T15:17:09Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "eess.IV"
    ],
    "abstract": "We observe that the performance of SOTA visual trackers surprisingly strongly varies across different video attributes and datasets. No single tracker remains the best performer across all tracking attributes and datasets. To bridge this gap, for a given video sequence, we predict the \"Best of the N Trackers\", called the BofN meta-tracker. At its core, a Tracking Performance Prediction Network (TP2N) selects a predicted best performing visual tracker for the given video sequence using only a few",
    "arxiv_url": "https://arxiv.org/abs/2407.15707v1",
    "pdf_url": "https://arxiv.org/pdf/2407.15707v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2407.15707",
    "arxiv_authors": [
      "Basit Alawode",
      "Sajid Javed",
      "Arif Mahmood",
      "Jiri Matas"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Predicting+the+Best+of+N+Visual+Trackers+Basit+Alawode+Sajid+Javed+Arif+Mahmood+Jiri+Matas",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "B Alawode",
        "id": "DCppi6gAAAAJ"
      },
      {
        "name": "S Javed",
        "id": "6qvbEhUAAAAJ"
      },
      {
        "name": "A Mahmood",
        "id": "_e6yGs4AAAAJ"
      },
      {
        "name": "J Matas -",
        "id": null
      }
    ],
    "citation_count": null
  },
  {
    "arxiv_id": "2502.11338",
    "title": "WRT-SAM: Foundation Model-Driven Segmentation for Generalized Weld Radiographic Testing",
    "year": 2025,
    "published": "2025-02-17T01:31:36Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Radiographic testing is a fundamental non-destructive evaluation technique for identifying weld defects and assessing quality in industrial applications due to its high-resolution imaging capabilities. Over the past decade, deep learning techniques have significantly advanced weld defect identification in radiographic images. However, conventional approaches, which rely on training small-scale, task-specific models on single-scenario datasets, exhibit poor cross-scenario generalization. Recently",
    "arxiv_url": "https://arxiv.org/abs/2502.11338v1",
    "pdf_url": "https://arxiv.org/pdf/2502.11338v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2502.11338",
    "arxiv_authors": [
      "Yunyi Zhou",
      "Kun Shi",
      "Gang Hao"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=WRT-SAM%3A+Foundation+Model-Driven+Segmentation+for+Generalized+Weld+Radiographic+Testing+Yunyi+Zhou+Kun+Shi+Gang+Hao",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "Y Zhou",
        "id": null
      },
      {
        "name": "K Shi",
        "id": null
      },
      {
        "name": "G Hao -",
        "id": null
      }
    ],
    "citation_count": 2
  },
  {
    "arxiv_id": "2405.08419",
    "title": "WaterMamba: Visual State Space Model for Underwater Image Enhancement",
    "year": 2024,
    "published": "2024-05-14T08:26:29Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Underwater imaging often suffers from low quality due to factors affecting light propagation and absorption in water. To improve image quality, some underwater image enhancement (UIE) methods based on convolutional neural networks (CNN) and Transformer have been proposed. However, CNN-based UIE methods are limited in modeling long-range dependencies, and Transformer-based methods involve a large number of parameters and complex self-attention mechanisms, posing efficiency challenges. Considering",
    "arxiv_url": "https://arxiv.org/abs/2405.08419v1",
    "pdf_url": "https://arxiv.org/pdf/2405.08419v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2405.08419",
    "arxiv_authors": [
      "Meisheng Guan",
      "Haiyong Xu",
      "Gangyi Jiang",
      "Mei Yu",
      "Yeyao Chen",
      "Ting Luo",
      "Yang Song"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=WaterMamba%3A+Visual+State+Space+Model+for+Underwater+Image+Enhancement+Meisheng+Guan+Haiyong+Xu+Gangyi+Jiang+Mei+Yu+Yeyao+Chen",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "M Guan",
        "id": null
      },
      {
        "name": "H Xu",
        "id": "yJ3S724AAAAJ"
      },
      {
        "name": "G Jiang",
        "id": null
      },
      {
        "name": "M Yu",
        "id": null
      },
      {
        "name": "Y Chen",
        "id": null
      },
      {
        "name": "T Luo",
        "id": null
      },
      {
        "name": "Y Song",
        "id": null
      }
    ],
    "citation_count": 17
  },
  {
    "arxiv_id": "2305.12653",
    "title": "Estimating Discrete Total Curvature with Per Triangle Normal Variation",
    "year": 2023,
    "published": "2023-05-22T02:52:29Z",
    "categories": [
      "cs.GR",
      "cs.CG",
      "cs.CV"
    ],
    "abstract": "We introduce a novel approach for measuring the total curvature at every triangle of a discrete surface. This method takes advantage of the relationship between per triangle total curvature and the Dirichlet energy of the Gauss map. This new tool can be used on both triangle meshes and point clouds and has numerous applications. In this study, we demonstrate the effectiveness of our technique by using it for feature-aware mesh decimation, and show that it outperforms existing curvature-estimatio",
    "arxiv_url": "https://arxiv.org/abs/2305.12653v2",
    "pdf_url": "https://arxiv.org/pdf/2305.12653v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2305.12653",
    "arxiv_authors": [
      "Crane He Chen"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Estimating+Discrete+Total+Curvature+with+Per+Triangle+Normal+Variation+Crane+He+Chen",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "CH Chen - ACM SIGGRAPH",
        "id": null
      }
    ],
    "citation_count": 4
  },
  {
    "arxiv_id": "2312.00316",
    "title": "Improving Efficiency of DNN-based Relocalization Module for Autonomous Driving with Server-side Computing",
    "year": 2023,
    "published": "2023-12-01T03:16:10Z",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "abstract": "In this work, we present a novel framework for camera relocation in autonomous vehicles, leveraging deep neural networks (DNN). While existing literature offers various DNN-based camera relocation methods, their deployment is hindered by their high computational demands during inference. In contrast, our approach addresses this challenge through edge cloud collaboration. Specifically, we strategically offload certain modules of the neural network to the server and evaluate the inference time of ",
    "arxiv_url": "https://arxiv.org/abs/2312.00316v1",
    "pdf_url": "https://arxiv.org/pdf/2312.00316v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2312.00316",
    "arxiv_authors": [
      "Dengbo Li",
      "Jieren Cheng",
      "Boyi Liu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Improving+Efficiency+of+DNN-based+Relocalization+Module+for+Autonomous+Driving+with+Server-side+Computing+Dengbo+Li+Jieren+Cheng+Boyi+Liu",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "D Li",
        "id": null
      },
      {
        "name": "H Zhang",
        "id": "I_r3lY4AAAAJ"
      },
      {
        "name": "J Cheng",
        "id": "tP9z__UAAAAJ"
      },
      {
        "name": "B Liu -",
        "id": null
      }
    ],
    "citation_count": 3
  },
  {
    "arxiv_id": "2405.03055",
    "title": "Multi-hop graph transformer network for 3D human pose estimation",
    "year": 2024,
    "published": "2024-05-05T21:29:20Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Accurate 3D human pose estimation is a challenging task due to occlusion and depth ambiguity. In this paper, we introduce a multi-hop graph transformer network designed for 2D-to-3D human pose estimation in videos by leveraging the strengths of multi-head self-attention and multi-hop graph convolutional networks with disentangled neighborhoods to capture spatio-temporal dependencies and handle long-range interactions. The proposed network architecture consists of a graph attention block composed",
    "arxiv_url": "https://arxiv.org/abs/2405.03055v1",
    "pdf_url": "https://arxiv.org/pdf/2405.03055v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2405.03055",
    "arxiv_authors": [
      "Zaedul Islam",
      "A. Ben Hamza"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Multi-hop+graph+transformer+network+for+3D+human+pose+estimation+Zaedul+Islam+A.+Ben+Hamza",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "Z Islam",
        "id": "I_3IJZsAAAAJ"
      },
      {
        "name": "AB Hamza -",
        "id": null
      }
    ],
    "citation_count": 15
  },
  {
    "arxiv_id": "2410.10929",
    "title": "ASTM :Autonomous Smart Traffic Management System Using Artificial Intelligence CNN and LSTM",
    "year": 2024,
    "published": "2024-10-14T16:35:27Z",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "abstract": "In the modern world, the development of Artificial Intelligence (AI) has contributed to improvements in various areas, including automation, computer vision, fraud detection, and more. AI can be leveraged to enhance the efficiency of Autonomous Smart Traffic Management (ASTM) systems and reduce traffic congestion rates. This paper presents an Autonomous Smart Traffic Management (STM) system that uses AI to improve traffic flow rates. The system employs the YOLO V5 Convolutional Neural Network to",
    "arxiv_url": "https://arxiv.org/abs/2410.10929v7",
    "pdf_url": "https://arxiv.org/pdf/2410.10929v7",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2410.10929",
    "arxiv_authors": [
      "Christofel Rio Goenawan"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=ASTM+%3AAutonomous+Smart+Traffic+Management+System+Using+Artificial+Intelligence+CNN+and+LSTM+Christofel+Rio+Goenawan",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "CR Goenawan -",
        "id": null
      }
    ],
    "citation_count": 15
  },
  {
    "arxiv_id": "2411.01683",
    "title": "ROAD-Waymo: Action Awareness at Scale for Autonomous Driving",
    "year": 2024,
    "published": "2024-11-03T20:46:50Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Autonomous Vehicle (AV) perception systems require more than simply seeing, via e.g., object detection or scene segmentation. They need a holistic understanding of what is happening within the scene for safe interaction with other road users. Few datasets exist for the purpose of developing and training algorithms to comprehend the actions of other road users. This paper presents ROAD-Waymo, an extensive dataset for the development and benchmarking of techniques for agent, action, location and e",
    "arxiv_url": "https://arxiv.org/abs/2411.01683v2",
    "pdf_url": "https://arxiv.org/pdf/2411.01683v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2411.01683",
    "arxiv_authors": [
      "Salman Khan",
      "Izzeddin Teeti",
      "Reza Javanmard Alitappeh",
      "Mihaela C. Stoian",
      "Eleonora Giunchiglia",
      "Gurkirt Singh",
      "Andrew Bradley",
      "Fabio Cuzzolin"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=ROAD-Waymo%3A+Action+Awareness+at+Scale+for+Autonomous+Driving+Salman+Khan+Izzeddin+Teeti+Reza+Javanmard+Alitappeh+Mihaela+C.+Stoian+Eleonora+Giunchiglia",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "S Khan",
        "id": "0kXugtIAAAAJ"
      },
      {
        "name": "I Teeti",
        "id": "pkPhmcsAAAAJ"
      },
      {
        "name": "RJ Alitappeh",
        "id": "8hfecoMAAAAJ"
      },
      {
        "name": "MC Stoian",
        "id": "B_48apwAAAAJ"
      },
      {
        "name": "E Giunchiglia",
        "id": "HAgGqScAAAAJ"
      },
      {
        "name": "G Singh",
        "id": "w8XHUMIAAAAJ"
      },
      {
        "name": "A Bradley",
        "id": "D828shcAAAAJ"
      },
      {
        "name": "F Cuzzolin",
        "id": "T8LkBTYAAAAJ"
      }
    ],
    "citation_count": 5
  },
  {
    "arxiv_id": "2401.06550",
    "title": "Multimodal Urban Areas of Interest Generation via Remote Sensing Imagery and Geographical Prior",
    "year": 2024,
    "published": "2024-01-12T12:54:30Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Urban area-of-interest (AOI) refers to an integrated urban functional zone with defined polygonal boundaries. The rapid development of urban commerce has led to increasing demands for highly accurate and timely AOI data. However, existing research primarily focuses on coarse-grained functional zones for urban planning or regional economic analysis, and often neglects the expiration of AOI in the real world. They fail to fulfill the precision demands of Mobile Internet Online-to-Offline (O2O) bus",
    "arxiv_url": "https://arxiv.org/abs/2401.06550v3",
    "pdf_url": "https://arxiv.org/pdf/2401.06550v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2401.06550",
    "arxiv_authors": [
      "Chuanji Shi",
      "Yingying Zhang",
      "Jiaotuan Wang",
      "Xin Guo",
      "Qiqi Zhu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Multimodal+Urban+Areas+of+Interest+Generation+via+Remote+Sensing+Imagery+and+Geographical+Prior+Chuanji+Shi+Yingying+Zhang+Jiaotuan+Wang+Xin+Guo+Qiqi+Zhu",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "C Shi",
        "id": null
      },
      {
        "name": "Y Zhang",
        "id": "nOjOl4EAAAAJ"
      },
      {
        "name": "J Wang",
        "id": null
      },
      {
        "name": "X Guo",
        "id": "d2eWV9wAAAAJ"
      },
      {
        "name": "Q Zhu - International",
        "id": null
      }
    ],
    "citation_count": 2
  },
  {
    "arxiv_id": "2406.17319",
    "title": "DMF-Net: Image-Guided Point Cloud Completion with Dual-Channel Modality Fusion and Shape-Aware Upsampling Transformer",
    "year": 2024,
    "published": "2024-06-25T07:08:19Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "In this paper we study the task of a single-view image-guided point cloud completion. Existing methods have got promising results by fusing the information of image into point cloud explicitly or implicitly. However, given that the image has global shape information and the partial point cloud has rich local details, We believe that both modalities need to be given equal attention when performing modality fusion. To this end, we propose a novel dual-channel modality fusion network for image-guid",
    "arxiv_url": "https://arxiv.org/abs/2406.17319v1",
    "pdf_url": "https://arxiv.org/pdf/2406.17319v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2406.17319",
    "arxiv_authors": [
      "Aihua Mao",
      "Yuxuan Tang",
      "Jiangtao Huang",
      "Ying He"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=DMF-Net%3A+Image-Guided+Point+Cloud+Completion+with+Dual-Channel+Modality+Fusion+and+Shape-Aware+Upsampling+Transformer+Aihua+Mao+Yuxuan+Tang+Jiangtao+Huang+Ying+He",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "A Mao",
        "id": null
      },
      {
        "name": "Y Tang",
        "id": null
      },
      {
        "name": "J Huang",
        "id": null
      },
      {
        "name": "Y He -",
        "id": null
      }
    ],
    "citation_count": 7
  },
  {
    "arxiv_id": "2503.18225",
    "title": "DeLoRA: Decoupling Angles and Strength in Low-rank Adaptation",
    "year": 2025,
    "published": "2025-03-23T22:00:56Z",
    "categories": [
      "cs.LG",
      "cs.CL",
      "cs.CV"
    ],
    "abstract": "Parameter-Efficient FineTuning (PEFT) methods have recently gained significant popularity thanks to the widespread availability of large-scale pretrained models. These methods allow for quick adaptation to downstream tasks with minimal computational cost. However, popular finetuning methods such as LoRA exhibit limited robustness when it comes to hyperparameter choices or extended training regimes, preventing optimal out-of-the-box performance. In contrast, bounded approaches, such as ETHER, pro",
    "arxiv_url": "https://arxiv.org/abs/2503.18225v2",
    "pdf_url": "https://arxiv.org/pdf/2503.18225v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2503.18225",
    "arxiv_authors": [
      "Massimo Bini",
      "Leander Girrbach",
      "Zeynep Akata"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=DeLoRA%3A+Decoupling+Angles+and+Strength+in+Low-rank+Adaptation+Massimo+Bini+Leander+Girrbach+Zeynep+Akata",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "M Bini",
        "id": "qNYKJoIAAAAJ"
      },
      {
        "name": "L Girrbach",
        "id": "xk6HHiAAAAAJ"
      },
      {
        "name": "Z Akata - The Thirteenth International",
        "id": null
      }
    ],
    "citation_count": 1
  },
  {
    "arxiv_id": "2407.11743",
    "title": "OAM-TCD: A globally diverse dataset of high-resolution tree cover maps",
    "year": 2024,
    "published": "2024-07-16T14:11:29Z",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "Accurately quantifying tree cover is an important metric for ecosystem monitoring and for assessing progress in restored sites. Recent works have shown that deep learning-based segmentation algorithms are capable of accurately mapping trees at country and continental scales using high-resolution aerial and satellite imagery. Mapping at high (ideally sub-meter) resolution is necessary to identify individual trees, however there are few open-access datasets containing instance level annotations an",
    "arxiv_url": "https://arxiv.org/abs/2407.11743v1",
    "pdf_url": "https://arxiv.org/pdf/2407.11743v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2407.11743",
    "arxiv_authors": [
      "Josh Veitch-Michaelis",
      "Andrew Cottam",
      "Daniella Schweizer",
      "Eben N. Broadbent",
      "David Dao",
      "Ce Zhang",
      "Angelica Almeyda Zambrano",
      "Simeon Max"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=OAM-TCD%3A+A+globally+diverse+dataset+of+high-resolution+tree+cover+maps+Josh+Veitch-Michaelis+Andrew+Cottam+Daniella+Schweizer+Eben+N.+Broadbent+David+Dao",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "J Veitch-Michaelis",
        "id": "Wb1ZLRAAAAAJ"
      },
      {
        "name": "A Cottam",
        "id": "tKXjQ78AAAAJ"
      },
      {
        "name": "D Schweizer",
        "id": null
      },
      {
        "name": "E Broadbent",
        "id": "uPkV21MAAAAJ"
      },
      {
        "name": "D Dao",
        "id": "XHeNA_8AAAAJ"
      },
      {
        "name": "C Zhang",
        "id": "GkXqbmMAAAAJ"
      }
    ],
    "citation_count": 10
  },
  {
    "arxiv_id": "2504.06752",
    "title": "Compass Control: Multi Object Orientation Control for Text-to-Image Generation",
    "year": 2025,
    "published": "2025-04-09T10:15:15Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Existing approaches for controlling text-to-image diffusion models, while powerful, do not allow for explicit 3D object-centric control, such as precise control of object orientation. In this work, we address the problem of multi-object orientation control in text-to-image diffusion models. This enables the generation of diverse multi-object scenes with precise orientation control for each object. The key idea is to condition the diffusion model with a set of orientation-aware \\textbf{compass} t",
    "arxiv_url": "https://arxiv.org/abs/2504.06752v2",
    "pdf_url": "https://arxiv.org/pdf/2504.06752v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2504.06752",
    "arxiv_authors": [
      "Rishubh Parihar",
      "Vaibhav Agrawal",
      "Sachidanand VS",
      "R. Venkatesh Babu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Compass+Control%3A+Multi+Object+Orientation+Control+for+Text-to-Image+Generation+Rishubh+Parihar+Vaibhav+Agrawal+Sachidanand+VS+R.+Venkatesh+Babu",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "R Parihar",
        "id": "RaRoJFYAAAAJ"
      },
      {
        "name": "V Agrawal",
        "id": "3tFzDucAAAAJ"
      },
      {
        "name": "S VS",
        "id": "Hup_9BkAAAAJ"
      },
      {
        "name": "VB Radhakrishnan",
        "id": null
      }
    ],
    "citation_count": 3
  },
  {
    "arxiv_id": "2305.04719",
    "title": "Learning to Generate Poetic Chinese Landscape Painting with Calligraphy",
    "year": 2023,
    "published": "2023-05-08T14:10:10Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "In this paper, we present a novel system (denoted as Polaca) to generate poetic Chinese landscape painting with calligraphy. Unlike previous single image-to-image painting generation, Polaca takes the classic poetry as input and outputs the artistic landscape painting image with the corresponding calligraphy. It is equipped with three different modules to complete the whole piece of landscape painting artwork: the first one is a text-to-image module to generate landscape painting image, the seco",
    "arxiv_url": "https://arxiv.org/abs/2305.04719v1",
    "pdf_url": "https://arxiv.org/pdf/2305.04719v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2305.04719",
    "arxiv_authors": [
      "Shaozu Yuan",
      "Aijun Dai",
      "Zhiling Yan",
      "Ruixue Liu",
      "Meng Chen",
      "Baoyang Chen",
      "Zhijie Qiu",
      "Xiaodong He"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Learning+to+Generate+Poetic+Chinese+Landscape+Painting+with+Calligraphy+Shaozu+Yuan+Aijun+Dai+Zhiling+Yan+Ruixue+Liu+Meng+Chen",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "S Yuan",
        "id": "k_5RuqgAAAAJ"
      },
      {
        "name": "A Dai",
        "id": "S7CMvgoAAAAJ"
      },
      {
        "name": "Z Yan",
        "id": "zL4fjh8AAAAJ"
      },
      {
        "name": "R Liu",
        "id": "Vd3QqXUAAAAJ"
      },
      {
        "name": "M Chen",
        "id": "oFyp8_0AAAAJ"
      },
      {
        "name": "B Chen",
        "id": "lNN5lBgAAAAJ"
      },
      {
        "name": "Z Qiu",
        "id": null
      },
      {
        "name": "X He",
        "id": "W5WbqgoAAAAJ"
      }
    ],
    "citation_count": 14
  },
  {
    "arxiv_id": "2312.08766",
    "title": "A Dual Convolutional Neural Network Pipeline for Melanoma Diagnostics and Prognostics",
    "year": 2023,
    "published": "2023-12-14T09:28:50Z",
    "categories": [
      "eess.IV",
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "Melanoma is a type of cancer that begins in the cells controlling the pigment of the skin, and it is often referred to as the most dangerous skin cancer. Diagnosing melanoma can be time-consuming, and a recent increase in melanoma incidents indicates a growing demand for a more efficient diagnostic process. This paper presents a pipeline for melanoma diagnostics, leveraging two convolutional neural networks, a diagnosis, and a prognosis model. The diagnostic model is responsible for localizing m",
    "arxiv_url": "https://arxiv.org/abs/2312.08766v1",
    "pdf_url": "https://arxiv.org/pdf/2312.08766v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2312.08766",
    "arxiv_authors": [
      "Marie B√∏-Sande",
      "Edvin Benjaminsen",
      "Neel Kanwal",
      "Saul Fuster",
      "Helga Hardardottir",
      "Ingrid Lundal",
      "Emiel A. M. Janssen",
      "Kjersti Engan"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=A+Dual+Convolutional+Neural+Network+Pipeline+for+Melanoma+Diagnostics+and+Prognostics+Marie+B%C3%B8-Sande+Edvin+Benjaminsen+Neel+Kanwal+Saul+Fuster+Helga+Hardardottir",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "M B√∏-Sande",
        "id": null
      },
      {
        "name": "E Benjaminsen",
        "id": null
      },
      {
        "name": "N Kanwal",
        "id": "KvTZ7TYAAAAJ"
      },
      {
        "name": "S Fuster",
        "id": "rUXUCWQAAAAJ"
      },
      {
        "name": "H Hardardottir",
        "id": null
      },
      {
        "name": "I Lundal",
        "id": null
      },
      {
        "name": "EAM Janssen",
        "id": "F4ETXP0AAAAJ"
      }
    ],
    "citation_count": null
  },
  {
    "arxiv_id": "2405.07516",
    "title": "Support-Query Prototype Fusion Network for Few-shot Medical Image Segmentation",
    "year": 2024,
    "published": "2024-05-13T07:31:16Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "In recent years, deep learning based on Convolutional Neural Networks (CNNs) has achieved remarkable success in many applications. However, their heavy reliance on extensive labeled data and limited generalization ability to unseen classes pose challenges to their suitability for medical image processing tasks. Few-shot learning, which utilizes a small amount of labeled data to generalize to unseen classes, has emerged as a critical research area, attracting substantial attention. Currently, mos",
    "arxiv_url": "https://arxiv.org/abs/2405.07516v1",
    "pdf_url": "https://arxiv.org/pdf/2405.07516v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2405.07516",
    "arxiv_authors": [
      "Xiaoxiao Wu",
      "Zhenguo Gao",
      "Xiaowei Chen",
      "Yakai Wang",
      "Shulei Qu",
      "Na Li"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Support-Query+Prototype+Fusion+Network+for+Few-shot+Medical+Image+Segmentation+Xiaoxiao+Wu+Zhenguo+Gao+Xiaowei+Chen+Yakai+Wang+Shulei+Qu",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "X Wu",
        "id": null
      },
      {
        "name": "Z Gao",
        "id": "UxFPpOsAAAAJ"
      },
      {
        "name": "X Chen",
        "id": null
      },
      {
        "name": "Y Wang",
        "id": null
      },
      {
        "name": "S Qu",
        "id": null
      },
      {
        "name": "N Li -",
        "id": null
      }
    ],
    "citation_count": 2
  },
  {
    "arxiv_id": "2305.01611",
    "title": "AutoColor: Learned Light Power Control for Multi-Color Holograms",
    "year": 2023,
    "published": "2023-05-02T17:14:03Z",
    "categories": [
      "cs.CV",
      "cs.LG",
      "eess.IV"
    ],
    "abstract": "Multi-color holograms rely on simultaneous illumination from multiple light sources. These multi-color holograms could utilize light sources better than conventional single-color holograms and can improve the dynamic range of holographic displays. In this letter, we introduce AutoColor , the first learned method for estimating the optimal light source powers required for illuminating multi-color holograms. For this purpose, we establish the first multi-color hologram dataset using synthetic imag",
    "arxiv_url": "https://arxiv.org/abs/2305.01611v2",
    "pdf_url": "https://arxiv.org/pdf/2305.01611v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2305.01611",
    "arxiv_authors": [
      "Yicheng Zhan",
      "Koray Kavaklƒ±",
      "Hakan Urey",
      "Qi Sun",
      "Kaan Ak≈üit"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=AutoColor%3A+Learned+Light+Power+Control+for+Multi-Color+Holograms+Yicheng+Zhan+Koray+Kavakl%C4%B1+Hakan+Urey+Qi+Sun+Kaan+Ak%C5%9Fit",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "Y Zhan",
        "id": "x2ptSYUAAAAJ"
      },
      {
        "name": "K Kavaklƒ±",
        "id": "rn6XtO4AAAAJ"
      },
      {
        "name": "H Urey",
        "id": "4z4L9HMAAAAJ"
      },
      {
        "name": "Q Sun",
        "id": "oN7gaqMAAAAJ"
      },
      {
        "name": "K Ak≈üitOptical Architectures for Displays and Sensing in Augmented",
        "id": null
      }
    ],
    "citation_count": 6
  },
  {
    "arxiv_id": "2412.10224",
    "title": "SPT: Sequence Prompt Transformer for Interactive Image Segmentation",
    "year": 2024,
    "published": "2024-12-13T15:49:18Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Interactive segmentation aims to extract objects of interest from an image based on user-provided clicks. In real-world applications, there is often a need to segment a series of images featuring the same target object. However, existing methods typically process one image at a time, failing to consider the sequential nature of the images. To overcome this limitation, we propose a novel method called Sequence Prompt Transformer (SPT), the first to utilize sequential image information for interac",
    "arxiv_url": "https://arxiv.org/abs/2412.10224v1",
    "pdf_url": "https://arxiv.org/pdf/2412.10224v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2412.10224",
    "arxiv_authors": [
      "Senlin Cheng",
      "Haopeng Sun"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=SPT%3A+Sequence+Prompt+Transformer+for+Interactive+Image+Segmentation+Senlin+Cheng+Haopeng+Sun",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "S Cheng",
        "id": null
      },
      {
        "name": "H Sun",
        "id": null
      },
      {
        "name": "T Xie",
        "id": "QIRNDL0AAAAJ"
      },
      {
        "name": "H Zhao",
        "id": null
      },
      {
        "name": "Y Chen",
        "id": null
      },
      {
        "name": "B Xu",
        "id": null
      },
      {
        "name": "XB LiICASSP",
        "id": null
      }
    ],
    "citation_count": 18
  },
  {
    "arxiv_id": "2412.13081",
    "title": "Prompt Augmentation for Self-supervised Text-guided Image Manipulation",
    "year": 2024,
    "published": "2024-12-17T16:54:05Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Text-guided image editing finds applications in various creative and practical fields. While recent studies in image generation have advanced the field, they often struggle with the dual challenges of coherent image transformation and context preservation. In response, our work introduces prompt augmentation, a method amplifying a single input prompt into several target prompts, strengthening textual context and enabling localised image editing. Specifically, we use the augmented prompts to deli",
    "arxiv_url": "https://arxiv.org/abs/2412.13081v1",
    "pdf_url": "https://arxiv.org/pdf/2412.13081v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2412.13081",
    "arxiv_authors": [
      "Rumeysa Bodur",
      "Binod Bhattarai",
      "Tae-Kyun Kim"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Prompt+Augmentation+for+Self-supervised+Text-guided+Image+Manipulation+Rumeysa+Bodur+Binod+Bhattarai+Tae-Kyun+Kim",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "R Bodur",
        "id": "8sljREAAAAAJ"
      },
      {
        "name": "B Bhattarai",
        "id": "PDEi58sAAAAJ"
      },
      {
        "name": "TK Kim -",
        "id": null
      }
    ],
    "citation_count": 4
  },
  {
    "arxiv_id": "2408.05475",
    "title": "Cross-view image geo-localization with Panorama-BEV Co-Retrieval Network",
    "year": 2024,
    "published": "2024-08-10T08:03:58Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Cross-view geolocalization identifies the geographic location of street view images by matching them with a georeferenced satellite database. Significant challenges arise due to the drastic appearance and geometry differences between views. In this paper, we propose a new approach for cross-view image geo-localization, i.e., the Panorama-BEV Co-Retrieval Network. Specifically, by utilizing the ground plane assumption and geometric relations, we convert street view panorama images into the BEV vi",
    "arxiv_url": "https://arxiv.org/abs/2408.05475v1",
    "pdf_url": "https://arxiv.org/pdf/2408.05475v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2408.05475",
    "arxiv_authors": [
      "Junyan Ye",
      "Zhutao Lv",
      "Weijia Li",
      "Jinhua Yu",
      "Haote Yang",
      "Huaping Zhong",
      "Conghui He"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Cross-view+image+geo-localization+with+Panorama-BEV+Co-Retrieval+Network+Junyan+Ye+Zhutao+Lv+Weijia+Li+Jinhua+Yu+Haote+Yang",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "J Ye",
        "id": "6IbGkd4AAAAJ"
      },
      {
        "name": "Z Lv",
        "id": "aGJ7T4YAAAAJ"
      },
      {
        "name": "W Li",
        "id": "R6Rnh9IAAAAJ"
      },
      {
        "name": "J Yu",
        "id": "radsfXwAAAAJ"
      },
      {
        "name": "H Yang",
        "id": "9p8R9GYAAAAJ"
      },
      {
        "name": "H Zhong",
        "id": "AqmmngsAAAAJ"
      },
      {
        "name": "C HeEuropean",
        "id": null
      }
    ],
    "citation_count": 23
  },
  {
    "arxiv_id": "2306.11249",
    "title": "OpenSTL: A Comprehensive Benchmark of Spatio-Temporal Predictive Learning",
    "year": 2023,
    "published": "2023-06-20T03:02:14Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Spatio-temporal predictive learning is a learning paradigm that enables models to learn spatial and temporal patterns by predicting future frames from given past frames in an unsupervised manner. Despite remarkable progress in recent years, a lack of systematic understanding persists due to the diverse settings, complex implementation, and difficult reproducibility. Without standardization, comparisons can be unfair and insights inconclusive. To address this dilemma, we propose OpenSTL, a compre",
    "arxiv_url": "https://arxiv.org/abs/2306.11249v2",
    "pdf_url": "https://arxiv.org/pdf/2306.11249v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2306.11249",
    "arxiv_authors": [
      "Cheng Tan",
      "Siyuan Li",
      "Zhangyang Gao",
      "Wenfei Guan",
      "Zedong Wang",
      "Zicheng Liu",
      "Lirong Wu",
      "Stan Z. Li"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=OpenSTL%3A+A+Comprehensive+Benchmark+of+Spatio-Temporal+Predictive+Learning+Cheng+Tan+Siyuan+Li+Zhangyang+Gao+Wenfei+Guan+Zedong+Wang",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "C Tan",
        "id": "6kTV6aMAAAAJ"
      },
      {
        "name": "S Li",
        "id": "SKTQTXwAAAAJ"
      },
      {
        "name": "Z Gao",
        "id": "4SclT-QAAAAJ"
      },
      {
        "name": "W Guan",
        "id": null
      },
      {
        "name": "Z Wang",
        "id": "CEJ4pugAAAAJ"
      },
      {
        "name": "Z Liu",
        "id": "bkALdvsAAAAJ"
      },
      {
        "name": "L Wu",
        "id": "Tk7TrCoAAAAJ"
      },
      {
        "name": "SZ LiAdvances in Neural Information Processing Systems",
        "id": null
      }
    ],
    "citation_count": 108
  },
  {
    "arxiv_id": "2405.11582",
    "title": "SLAB: Efficient Transformers with Simplified Linear Attention and Progressive Re-parameterized Batch Normalization",
    "year": 2024,
    "published": "2024-05-19T15:22:25Z",
    "categories": [
      "cs.CV",
      "cs.CL"
    ],
    "abstract": "Transformers have become foundational architectures for both natural language and computer vision tasks. However, the high computational cost makes it quite challenging to deploy on resource-constraint devices. This paper investigates the computational bottleneck modules of efficient transformer, i.e., normalization layers and attention modules. LayerNorm is commonly used in transformer architectures but is not computational friendly due to statistic calculation during inference. However, replac",
    "arxiv_url": "https://arxiv.org/abs/2405.11582v2",
    "pdf_url": "https://arxiv.org/pdf/2405.11582v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2405.11582",
    "arxiv_authors": [
      "Jialong Guo",
      "Xinghao Chen",
      "Yehui Tang",
      "Yunhe Wang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=SLAB%3A+Efficient+Transformers+with+Simplified+Linear+Attention+and+Progressive+Re-parameterized+Batch+Normalization+Jialong+Guo+Xinghao+Chen+Yehui+Tang+Yunhe+Wang",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "J Guo",
        "id": null
      },
      {
        "name": "X Chen",
        "id": "tuGWUVIAAAAJ"
      },
      {
        "name": "Y Tang",
        "id": "TkSZQ6gAAAAJ"
      },
      {
        "name": "Y Wang -",
        "id": null
      }
    ],
    "citation_count": 42
  },
  {
    "arxiv_id": "2401.16304",
    "title": "Regressing Transformers for Data-efficient Visual Place Recognition",
    "year": 2024,
    "published": "2024-01-29T17:04:32Z",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "Visual place recognition is a critical task in computer vision, especially for localization and navigation systems. Existing methods often rely on contrastive learning: image descriptors are trained to have small distance for similar images and larger distance for dissimilar ones in a latent space. However, this approach struggles to ensure accurate distance-based image similarity representation, particularly when training with binary pairwise labels, and complex re-ranking strategies are requir",
    "arxiv_url": "https://arxiv.org/abs/2401.16304v1",
    "pdf_url": "https://arxiv.org/pdf/2401.16304v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2401.16304",
    "arxiv_authors": [
      "Mar√≠a Leyva-Vallina",
      "Nicola Strisciuglio",
      "Nicolai Petkov"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Regressing+Transformers+for+Data-efficient+Visual+Place+Recognition+Mar%C3%ADa+Leyva-Vallina+Nicola+Strisciuglio+Nicolai+Petkov",
    "gs_search_success": false,
    "gs_authors": null,
    "citation_count": null,
    "error_type": "fetch_failed"
  },
  {
    "arxiv_id": "2502.05964",
    "title": "Revisiting Gradient-based Uncertainty for Monocular Depth Estimation",
    "year": 2025,
    "published": "2025-02-09T17:21:41Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Monocular depth estimation, similar to other image-based tasks, is prone to erroneous predictions due to ambiguities in the image, for example, caused by dynamic objects or shadows. For this reason, pixel-wise uncertainty assessment is required for safety-critical applications to highlight the areas where the prediction is unreliable. We address this in a post hoc manner and introduce gradient-based uncertainty estimation for already trained depth estimation models. To extract gradients without ",
    "arxiv_url": "https://arxiv.org/abs/2502.05964v1",
    "pdf_url": "https://arxiv.org/pdf/2502.05964v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2502.05964",
    "arxiv_authors": [
      "Julia Hornauer",
      "Amir El-Ghoussani",
      "Vasileios Belagiannis"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Revisiting+Gradient-based+Uncertainty+for+Monocular+Depth+Estimation+Julia+Hornauer+Amir+El-Ghoussani+Vasileios+Belagiannis",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "J Hornauer",
        "id": "vgem-pUAAAAJ"
      },
      {
        "name": "V Belagiannis - European",
        "id": null
      }
    ],
    "citation_count": 4
  },
  {
    "arxiv_id": "2505.06918",
    "title": "Uni-AIMS: AI-Powered Microscopy Image Analysis",
    "year": 2025,
    "published": "2025-05-11T09:35:53Z",
    "categories": [
      "eess.IV",
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "This paper presents a systematic solution for the intelligent recognition and automatic analysis of microscopy images. We developed a data engine that generates high-quality annotated datasets through a combination of the collection of diverse microscopy images from experiments, synthetic data generation and a human-in-the-loop annotation process. To address the unique challenges of microscopy images, we propose a segmentation model capable of robustly detecting both small and large objects. The",
    "arxiv_url": "https://arxiv.org/abs/2505.06918v2",
    "pdf_url": "https://arxiv.org/pdf/2505.06918v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2505.06918",
    "arxiv_authors": [
      "Yanhui Hong",
      "Nan Wang",
      "Zhiyi Xia",
      "Haoyi Tao",
      "Xi Fang",
      "Yiming Li",
      "Jiankun Wang",
      "Peng Jin",
      "Xiaochen Cai",
      "Shengyu Li",
      "Ziqi Chen",
      "Zezhong Zhang",
      "Guolin Ke",
      "Linfeng Zhang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Uni-AIMS%3A+AI-Powered+Microscopy+Image+Analysis+Yanhui+Hong+Nan+Wang+Zhiyi+Xia+Haoyi+Tao+Xi+Fang",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "Y Hong",
        "id": null
      },
      {
        "name": "N Wang",
        "id": null
      },
      {
        "name": "Z Xia",
        "id": null
      },
      {
        "name": "H Tao",
        "id": null
      },
      {
        "name": "X Fang",
        "id": "f1IADCIAAAAJ"
      },
      {
        "name": "Y Li",
        "id": null
      },
      {
        "name": "J Wang",
        "id": null
      },
      {
        "name": "P Jin",
        "id": null
      },
      {
        "name": "X Cai",
        "id": "Ja-DPZcAAAAJ"
      },
      {
        "name": "S Li",
        "id": null
      },
      {
        "name": "Z Chen",
        "id": null
      },
      {
        "name": "Z Zhang",
        "id": null
      }
    ],
    "citation_count": null
  },
  {
    "arxiv_id": "2307.05317",
    "title": "Automatic Generation of Semantic Parts for Face Image Synthesis",
    "year": 2023,
    "published": "2023-07-11T15:01:42Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Semantic image synthesis (SIS) refers to the problem of generating realistic imagery given a semantic segmentation mask that defines the spatial layout of object classes. Most of the approaches in the literature, other than the quality of the generated images, put effort in finding solutions to increase the generation diversity in terms of style i.e. texture. However, they all neglect a different feature, which is the possibility of manipulating the layout provided by the mask. Currently, the on",
    "arxiv_url": "https://arxiv.org/abs/2307.05317v1",
    "pdf_url": "https://arxiv.org/pdf/2307.05317v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2307.05317",
    "arxiv_authors": [
      "Tomaso Fontanini",
      "Claudio Ferrari",
      "Massimo Bertozzi",
      "Andrea Prati"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Automatic+Generation+of+Semantic+Parts+for+Face+Image+Synthesis+Tomaso+Fontanini+Claudio+Ferrari+Massimo+Bertozzi+Andrea+Prati",
    "gs_search_success": false,
    "gs_authors": null,
    "citation_count": null,
    "error_type": "multiple_matches"
  },
  {
    "arxiv_id": "2308.07770",
    "title": "Multi-scale Promoted Self-adjusting Correlation Learning for Facial Action Unit Detection",
    "year": 2023,
    "published": "2023-08-15T13:43:48Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Facial Action Unit (AU) detection is a crucial task in affective computing and social robotics as it helps to identify emotions expressed through facial expressions. Anatomically, there are innumerable correlations between AUs, which contain rich information and are vital for AU detection. Previous methods used fixed AU correlations based on expert experience or statistical rules on specific benchmarks, but it is challenging to comprehensively reflect complex correlations between AUs via hand-cr",
    "arxiv_url": "https://arxiv.org/abs/2308.07770v1",
    "pdf_url": "https://arxiv.org/pdf/2308.07770v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2308.07770",
    "arxiv_authors": [
      "Xin Liu",
      "Kaishen Yuan",
      "Xuesong Niu",
      "Jingang Shi",
      "Zitong Yu",
      "Huanjing Yue",
      "Jingyu Yang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Multi-scale+Promoted+Self-adjusting+Correlation+Learning+for+Facial+Action+Unit+Detection+Xin+Liu+Kaishen+Yuan+Xuesong+Niu+Jingang+Shi+Zitong+Yu",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "X Liu",
        "id": "xHkC5U0AAAAJ"
      },
      {
        "name": "K Yuan",
        "id": "LiIKEyMAAAAJ"
      },
      {
        "name": "X Niu",
        "id": "iuPSV-0AAAAJ"
      },
      {
        "name": "J Shi",
        "id": "N2ftCz4AAAAJ"
      },
      {
        "name": "Z Yu",
        "id": "ziHejLwAAAAJ"
      },
      {
        "name": "H Yue",
        "id": "1umAObUAAAAJ"
      }
    ],
    "citation_count": 33
  },
  {
    "arxiv_id": "2504.08384",
    "title": "Towards Efficient and Robust Moment Retrieval System: A Unified Framework for Multi-Granularity Models and Temporal Reranking",
    "year": 2025,
    "published": "2025-04-11T09:36:46Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Long-form video understanding presents significant challenges for interactive retrieval systems, as conventional methods struggle to process extensive video content efficiently. Existing approaches often rely on single models, inefficient storage, unstable temporal search, and context-agnostic reranking, limiting their effectiveness. This paper presents a novel framework to enhance interactive video retrieval through four key innovations: (1) an ensemble search strategy that integrates coarse-gr",
    "arxiv_url": "https://arxiv.org/abs/2504.08384v1",
    "pdf_url": "https://arxiv.org/pdf/2504.08384v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2504.08384",
    "arxiv_authors": [
      "Huu-Loc Tran",
      "Tinh-Anh Nguyen-Nhu",
      "Huu-Phong Phan-Nguyen",
      "Tien-Huy Nguyen",
      "Nhat-Minh Nguyen-Dich",
      "Anh Dao",
      "Huy-Duc Do",
      "Quan Nguyen",
      "Hoang M. Le",
      "Quang-Vinh Dinh"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Towards+Efficient+and+Robust+Moment+Retrieval+System%3A+A+Unified+Framework+for+Multi-Granularity+Models+and+Temporal+Reranking+Huu-Loc+Tran+Tinh-Anh+Nguyen-Nhu+Huu-Phong+Phan-Nguyen+Tien-Huy+Nguyen+Nhat-Minh+Nguyen-Dich",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "HL Tran",
        "id": "1xlRrhMAAAAJ"
      },
      {
        "name": "TA Nguyen-Nhu",
        "id": null
      },
      {
        "name": "HP Phan-Nguyen",
        "id": "ZzNbKiIAAAAJ"
      },
      {
        "name": "TH Nguyen",
        "id": "-MRppsEAAAAJ"
      },
      {
        "name": "NM Nguyen-Dich",
        "id": null
      },
      {
        "name": "A Dao",
        "id": "EOD0QkMAAAAJ"
      },
      {
        "name": "HD Do",
        "id": null
      }
    ],
    "citation_count": 3
  },
  {
    "arxiv_id": "2308.09908",
    "title": "LEGO: Learning and Graph-Optimized Modular Tracker for Online Multi-Object Tracking with Point Clouds",
    "year": 2023,
    "published": "2023-08-19T05:15:02Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Online multi-object tracking (MOT) plays a pivotal role in autonomous systems. The state-of-the-art approaches usually employ a tracking-by-detection method, and data association plays a critical role. This paper proposes a learning and graph-optimized (LEGO) modular tracker to improve data association performance in the existing literature. The proposed LEGO tracker integrates graph optimization and self-attention mechanisms, which efficiently formulate the association score map, facilitating t",
    "arxiv_url": "https://arxiv.org/abs/2308.09908v6",
    "pdf_url": "https://arxiv.org/pdf/2308.09908v6",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2308.09908",
    "arxiv_authors": [
      "Zhenrong Zhang",
      "Jianan Liu",
      "Yuxuan Xia",
      "Tao Huang",
      "Qing-Long Han",
      "Hongbin Liu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=LEGO%3A+Learning+and+Graph-Optimized+Modular+Tracker+for+Online+Multi-Object+Tracking+with+Point+Clouds+Zhenrong+Zhang+Jianan+Liu+Yuxuan+Xia+Tao+Huang+Qing-Long+Han",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "Z Zhang",
        "id": "B9xqJfoAAAAJ"
      },
      {
        "name": "J Liu",
        "id": null
      },
      {
        "name": "Y Xia",
        "id": "ONkHTSwAAAAJ"
      },
      {
        "name": "T Huang",
        "id": "iNkMsdoAAAAJ"
      },
      {
        "name": "QL Han",
        "id": "j0pyptAAAAAJ"
      },
      {
        "name": "H LiuIEEE Transactions on Circuits and Systems for Video Technology",
        "id": null
      }
    ],
    "citation_count": 20
  },
  {
    "arxiv_id": "2309.09887",
    "title": "On Model Explanations with Transferable Neural Pathways",
    "year": 2023,
    "published": "2023-09-18T15:50:38Z",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "Neural pathways as model explanations consist of a sparse set of neurons that provide the same level of prediction performance as the whole model. Existing methods primarily focus on accuracy and sparsity but the generated pathways may offer limited interpretability thus fall short in explaining the model behavior. In this paper, we suggest two interpretability criteria of neural pathways: (i) same-class neural pathways should primarily consist of class-relevant neurons; (ii) each instance's neu",
    "arxiv_url": "https://arxiv.org/abs/2309.09887v1",
    "pdf_url": "https://arxiv.org/pdf/2309.09887v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2309.09887",
    "arxiv_authors": [
      "Xinmiao Lin",
      "Wentao Bao",
      "Qi Yu",
      "Yu Kong"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=On+Model+Explanations+with+Transferable+Neural+Pathways+Xinmiao+Lin+Wentao+Bao+Qi+Yu+Yu+Kong",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "X Lin",
        "id": "08i0pL0AAAAJ"
      },
      {
        "name": "W Bao",
        "id": "fBcEItYAAAAJ"
      },
      {
        "name": "Q Yu",
        "id": "L3gWdfEAAAAJ"
      },
      {
        "name": "Y Kong -",
        "id": null
      }
    ],
    "citation_count": null
  },
  {
    "arxiv_id": "2307.10275",
    "title": "Survey on Controlable Image Synthesis with Deep Learning",
    "year": 2023,
    "published": "2023-07-18T07:02:51Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Image synthesis has attracted emerging research interests in academic and industry communities. Deep learning technologies especially the generative models greatly inspired controllable image synthesis approaches and applications, which aim to generate particular visual contents with latent prompts. In order to further investigate low-level controllable image synthesis problem which is crucial for fine image rendering and editing tasks, we present a survey of some recent works on 3D controllable",
    "arxiv_url": "https://arxiv.org/abs/2307.10275v1",
    "pdf_url": "https://arxiv.org/pdf/2307.10275v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2307.10275",
    "arxiv_authors": [
      "Shixiong Zhang",
      "Jiao Li",
      "Lu Yang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Survey+on+Controlable+Image+Synthesis+with+Deep+Learning+Shixiong+Zhang+Jiao+Li+Lu+Yang",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "S Zhang",
        "id": null
      },
      {
        "name": "J Li",
        "id": null
      },
      {
        "name": "L Yang -",
        "id": null
      }
    ],
    "citation_count": 3
  },
  {
    "arxiv_id": "2301.06961",
    "title": "Composite Deep Network with Feature Weighting for Improved Delineation of COVID Infection in Lung CT",
    "year": 2023,
    "published": "2023-01-17T15:36:27Z",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "abstract": "An early effective screening and grading of COVID-19 has become imperative towards optimizing the limited available resources of the medical facilities. An automated segmentation of the infected volumes in lung CT is expected to significantly aid in the diagnosis and care of patients. However, an accurate demarcation of lesions remains problematic due to their irregular structure and location(s) within the lung. A novel deep learning architecture, Composite Deep network with Feature Weighting (C",
    "arxiv_url": "https://arxiv.org/abs/2301.06961v2",
    "pdf_url": "https://arxiv.org/pdf/2301.06961v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2301.06961",
    "arxiv_authors": [
      "Pallabi Dutta",
      "Sushmita Mitra"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Composite+Deep+Network+with+Feature+Weighting+for+Improved+Delineation+of+COVID+Infection+in+Lung+CT+Pallabi+Dutta+Sushmita+Mitra",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "P Dutta",
        "id": "4J683WEAAAAJ"
      },
      {
        "name": "S Mitra -",
        "id": null
      }
    ],
    "citation_count": 1
  },
  {
    "arxiv_id": "2308.10488",
    "title": "Enhancing Medical Image Segmentation: Optimizing Cross-Entropy Weights and Post-Processing with Autoencoders",
    "year": 2023,
    "published": "2023-08-21T06:09:00Z",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "abstract": "The task of medical image segmentation presents unique challenges, necessitating both localized and holistic semantic understanding to accurately delineate areas of interest, such as critical tissues or aberrant features. This complexity is heightened in medical image segmentation due to the high degree of inter-class similarities, intra-class variations, and possible image obfuscation. The segmentation task further diversifies when considering the study of histopathology slides for autoimmune d",
    "arxiv_url": "https://arxiv.org/abs/2308.10488v1",
    "pdf_url": "https://arxiv.org/pdf/2308.10488v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2308.10488",
    "arxiv_authors": [
      "Pranav Singh",
      "Luoyao Chen",
      "Mei Chen",
      "Jinqian Pan",
      "Raviteja Chukkapalli",
      "Shravan Chaudhari",
      "Jacopo Cirrone"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Enhancing+Medical+Image+Segmentation%3A+Optimizing+Cross-Entropy+Weights+and+Post-Processing+with+Autoencoders+Pranav+Singh+Luoyao+Chen+Mei+Chen+Jinqian+Pan+Raviteja+Chukkapalli",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "P Singh",
        "id": "aUPq9a0AAAAJ"
      },
      {
        "name": "L Chen",
        "id": "xGueaRUAAAAJ"
      },
      {
        "name": "M Chen",
        "id": null
      },
      {
        "name": "J Pan",
        "id": "6jJqKrsAAAAJ"
      },
      {
        "name": "R Chukkapalli",
        "id": "XC99HvAAAAAJ"
      },
      {
        "name": "S Chaudhari",
        "id": "TxzHQTUAAAAJ"
      },
      {
        "name": "J Cirrone",
        "id": "DF9nXUYAAAAJ"
      }
    ],
    "citation_count": 6
  },
  {
    "arxiv_id": "2401.05334",
    "title": "URHand: Universal Relightable Hands",
    "year": 2024,
    "published": "2024-01-10T18:59:51Z",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "abstract": "Existing photorealistic relightable hand models require extensive identity-specific observations in different views, poses, and illuminations, and face challenges in generalizing to natural illuminations and novel identities. To bridge this gap, we present URHand, the first universal relightable hand model that generalizes across viewpoints, poses, illuminations, and identities. Our model allows few-shot personalization using images captured with a mobile phone, and is ready to be photorealistic",
    "arxiv_url": "https://arxiv.org/abs/2401.05334v1",
    "pdf_url": "https://arxiv.org/pdf/2401.05334v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2401.05334",
    "arxiv_authors": [
      "Zhaoxi Chen",
      "Gyeongsik Moon",
      "Kaiwen Guo",
      "Chen Cao",
      "Stanislav Pidhorskyi",
      "Tomas Simon",
      "Rohan Joshi",
      "Yuan Dong",
      "Yichen Xu",
      "Bernardo Pires",
      "He Wen",
      "Lucas Evans",
      "Bo Peng",
      "Julia Buffalini",
      "Autumn Trimble",
      "Kevyn McPhail",
      "Melissa Schoeller",
      "Shoou-I Yu",
      "Javier Romero",
      "Michael Zollh√∂fer",
      "Yaser Sheikh",
      "Ziwei Liu",
      "Shunsuke Saito"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=URHand%3A+Universal+Relightable+Hands+Zhaoxi+Chen+Gyeongsik+Moon+Kaiwen+Guo+Chen+Cao+Stanislav+Pidhorskyi",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "Z Chen",
        "id": "HsV0WbwAAAAJ"
      },
      {
        "name": "G Moon",
        "id": "2f2D258AAAAJ"
      },
      {
        "name": "K Guo",
        "id": "N1tM8l8AAAAJ"
      },
      {
        "name": "C Cao",
        "id": "fijNHGAAAAAJ"
      },
      {
        "name": "S Pidhorskyi",
        "id": "u4hkhOQAAAAJ"
      },
      {
        "name": "T Simon",
        "id": "7aabHgsAAAAJ"
      },
      {
        "name": "R Joshi",
        "id": null
      },
      {
        "name": "Y Dong",
        "id": "MHwprqAAAAAJ"
      },
      {
        "name": "Y Xu",
        "id": "nNpyhdsAAAAJ"
      },
      {
        "name": "B Pires",
        "id": "IN4BBvIAAAAJ"
      }
    ],
    "citation_count": 20
  },
  {
    "arxiv_id": "2311.17552",
    "title": "An Efficient Illumination Invariant Tiger Detection Framework for Wildlife Surveillance",
    "year": 2023,
    "published": "2023-11-29T11:35:54Z",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "Tiger conservation necessitates the strategic deployment of multifaceted initiatives encompassing the preservation of ecological habitats, anti-poaching measures, and community involvement for sustainable growth in the tiger population. With the advent of artificial intelligence, tiger surveillance can be automated using object detection. In this paper, an accurate illumination invariant framework is proposed based on EnlightenGAN and YOLOv8 for tiger detection. The fine-tuned YOLOv8 model achie",
    "arxiv_url": "https://arxiv.org/abs/2311.17552v2",
    "pdf_url": "https://arxiv.org/pdf/2311.17552v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2311.17552",
    "arxiv_authors": [
      "Gaurav Pendharkar",
      "A. Ancy Micheal",
      "Jason Misquitta",
      "Ranjeesh Kaippada"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=An+Efficient+Illumination+Invariant+Tiger+Detection+Framework+for+Wildlife+Surveillance+Gaurav+Pendharkar+A.+Ancy+Micheal+Jason+Misquitta+Ranjeesh+Kaippada",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "G Pendharkar",
        "id": null
      },
      {
        "name": "AA Micheal",
        "id": "8nHyjLYAAAAJ"
      },
      {
        "name": "J Misquitta",
        "id": null
      },
      {
        "name": "R KaippadaInternational",
        "id": null
      }
    ],
    "citation_count": 1
  },
  {
    "arxiv_id": "2405.07369",
    "title": "Incorporating Anatomical Awareness for Enhanced Generalizability and Progression Prediction in Deep Learning-Based Radiographic Sacroiliitis Detection",
    "year": 2024,
    "published": "2024-05-12T20:02:25Z",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "Purpose: To examine whether incorporating anatomical awareness into a deep learning model can improve generalizability and enable prediction of disease progression.   Methods: This retrospective multicenter study included conventional pelvic radiographs of 4 different patient cohorts focusing on axial spondyloarthritis (axSpA) collected at university and community hospitals. The first cohort, which consisted of 1483 radiographs, was split into training (n=1261) and validation (n=222) sets. The o",
    "arxiv_url": "https://arxiv.org/abs/2405.07369v1",
    "pdf_url": "https://arxiv.org/pdf/2405.07369v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2405.07369",
    "arxiv_authors": [
      "Felix J. Dorfner",
      "Janis L. Vahldiek",
      "Leonhard Donle",
      "Andrei Zhukov",
      "Lina Xu",
      "Hartmut H√§ntze",
      "Marcus R. Makowski",
      "Hugo J. W. L. Aerts",
      "Fabian Proft",
      "Valeria Rios Rodriguez",
      "Judith Rademacher",
      "Mikhail Protopopov",
      "Hildrun Haibel",
      "Torsten Diekhoff",
      "Murat Torgutalp",
      "Lisa C. Adams",
      "Denis Poddubnyy",
      "Keno K. Bressem"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Incorporating+Anatomical+Awareness+for+Enhanced+Generalizability+and+Progression+Prediction+in+Deep+Learning-Based+Radiographic+Sacroiliitis+Detection+Felix+J.+Dorfner+Janis+L.+Vahldiek+Leonhard+Donle+Andrei+Zhukov+Lina+Xu",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "FJ Dorfner",
        "id": "CnNPqZgAAAAJ"
      },
      {
        "name": "JL Vahldiek",
        "id": null
      },
      {
        "name": "L Donle",
        "id": "GzGySNsAAAAJ"
      },
      {
        "name": "A Zhukov",
        "id": null
      },
      {
        "name": "L Xu",
        "id": null
      },
      {
        "name": "H H√§ntze",
        "id": null
      },
      {
        "name": "MR Makowski",
        "id": null
      },
      {
        "name": "HJWL Aerts",
        "id": "v7G4QvIAAAAJ"
      }
    ],
    "citation_count": 3
  },
  {
    "arxiv_id": "2303.13062",
    "title": "SIEDOB: Semantic Image Editing by Disentangling Object and Background",
    "year": 2023,
    "published": "2023-03-23T06:17:23Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Semantic image editing provides users with a flexible tool to modify a given image guided by a corresponding segmentation map. In this task, the features of the foreground objects and the backgrounds are quite different. However, all previous methods handle backgrounds and objects as a whole using a monolithic model. Consequently, they remain limited in processing content-rich images and suffer from generating unrealistic objects and texture-inconsistent backgrounds. To address this issue, we pr",
    "arxiv_url": "https://arxiv.org/abs/2303.13062v1",
    "pdf_url": "https://arxiv.org/pdf/2303.13062v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2303.13062",
    "arxiv_authors": [
      "Wuyang Luo",
      "Su Yang",
      "Xinjian Zhang",
      "Weishan Zhang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=SIEDOB%3A+Semantic+Image+Editing+by+Disentangling+Object+and+Background+Wuyang+Luo+Su+Yang+Xinjian+Zhang+Weishan+Zhang",
    "gs_search_success": false,
    "gs_authors": null,
    "citation_count": null,
    "error_type": "fetch_failed"
  },
  {
    "arxiv_id": "2402.19308",
    "title": "Loss-Free Machine Unlearning",
    "year": 2024,
    "published": "2024-02-29T16:15:34Z",
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "abstract": "We present a machine unlearning approach that is both retraining- and label-free. Most existing machine unlearning approaches require a model to be fine-tuned to remove information while preserving performance. This is computationally expensive and necessitates the storage of the whole dataset for the lifetime of the model. Retraining-free approaches often utilise Fisher information, which is derived from the loss and requires labelled data which may not be available. Thus, we present an extensi",
    "arxiv_url": "https://arxiv.org/abs/2402.19308v1",
    "pdf_url": "https://arxiv.org/pdf/2402.19308v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2402.19308",
    "arxiv_authors": [
      "Jack Foster",
      "Stefan Schoepf",
      "Alexandra Brintrup"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Loss-Free+Machine+Unlearning+Jack+Foster+Stefan+Schoepf+Alexandra+Brintrup",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "J Foster",
        "id": "7m8cBAoAAAAJ"
      },
      {
        "name": "S Schoepf",
        "id": "GTvLmf0AAAAJ"
      },
      {
        "name": "A Brintrup -",
        "id": null
      }
    ],
    "citation_count": 8
  },
  {
    "arxiv_id": "2407.16822",
    "title": "Integrating Clinical Knowledge Graphs and Gradient-Based Neural Systems for Enhanced Melanoma Diagnosis via the 7-Point Checklist",
    "year": 2024,
    "published": "2024-07-23T20:27:16Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "The 7-point checklist (7PCL) is a widely used diagnostic tool in dermoscopy for identifying malignant melanoma by assigning point values to seven specific attributes. However, the traditional 7PCL is limited to distinguishing between malignant melanoma and melanocytic Nevi, and falls short in scenarios where multiple skin diseases with appearances similar to melanoma coexist. To address this limitation, we propose a novel diagnostic framework that integrates a clinical knowledge-based topologica",
    "arxiv_url": "https://arxiv.org/abs/2407.16822v3",
    "pdf_url": "https://arxiv.org/pdf/2407.16822v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2407.16822",
    "arxiv_authors": [
      "Yuheng Wang",
      "Tianze Yu",
      "Jiayue Cai",
      "Sunil Kalia",
      "Harvey Lui",
      "Z. Jane Wang",
      "Tim K. Lee"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Integrating+Clinical+Knowledge+Graphs+and+Gradient-Based+Neural+Systems+for+Enhanced+Melanoma+Diagnosis+via+the+7-Point+Checklist+Yuheng+Wang+Tianze+Yu+Jiayue+Cai+Sunil+Kalia+Harvey+Lui",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "Y Wang",
        "id": "_ZYcQFUAAAAJ"
      },
      {
        "name": "T Yu",
        "id": "-wRGM1QAAAAJ"
      },
      {
        "name": "J Cai",
        "id": null
      },
      {
        "name": "S Kalia",
        "id": null
      },
      {
        "name": "H Lui",
        "id": null
      },
      {
        "name": "ZJ Wang",
        "id": "W75uTm8AAAAJ"
      },
      {
        "name": "TK LeeIEEE Transactions on Neural Networks and Learning Systems",
        "id": null
      }
    ],
    "citation_count": null
  },
  {
    "arxiv_id": "2301.07836",
    "title": "Masked Autoencoding Does Not Help Natural Language Supervision at Scale",
    "year": 2023,
    "published": "2023-01-19T01:05:18Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Self supervision and natural language supervision have emerged as two exciting ways to train general purpose image encoders which excel at a variety of downstream tasks. Recent works such as M3AE and SLIP have suggested that these approaches can be effectively combined, but most notably their results use small pre-training datasets (<50M samples) and don't effectively reflect the large-scale regime (>100M examples) that is commonly used for these approaches. Here we investigate whether a similar",
    "arxiv_url": "https://arxiv.org/abs/2301.07836v4",
    "pdf_url": "https://arxiv.org/pdf/2301.07836v4",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2301.07836",
    "arxiv_authors": [
      "Floris Weers",
      "Vaishaal Shankar",
      "Angelos Katharopoulos",
      "Yinfei Yang",
      "Tom Gunter"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Masked+Autoencoding+Does+Not+Help+Natural+Language+Supervision+at+Scale+Floris+Weers+Vaishaal+Shankar+Angelos+Katharopoulos+Yinfei+Yang+Tom+Gunter",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "F Weers",
        "id": "hc_S-hIAAAAJ"
      },
      {
        "name": "V Shankar",
        "id": "cUkE7OgAAAAJ"
      },
      {
        "name": "A Katharopoulos",
        "id": "CNSO4uIAAAAJ"
      },
      {
        "name": "Y Yang",
        "id": "kvDbu90AAAAJ"
      },
      {
        "name": "T Gunter",
        "id": "091Onx0AAAAJ"
      }
    ],
    "citation_count": 18
  },
  {
    "arxiv_id": "2501.14198",
    "title": "Sparse Mixture-of-Experts for Non-Uniform Noise Reduction in MRI Images",
    "year": 2025,
    "published": "2025-01-24T03:04:44Z",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "abstract": "Magnetic Resonance Imaging (MRI) is an essential diagnostic tool in clinical settings but its utility is often hindered by noise artifacts introduced during the imaging process. Effective denoising is critical for enhancing image quality while preserving anatomical structures. However traditional denoising methods which typically assume uniform noise distributions struggle to handle the non-uniform noise commonly present in MRI images. In this paper we introduce a novel approach leveraging a spa",
    "arxiv_url": "https://arxiv.org/abs/2501.14198v2",
    "pdf_url": "https://arxiv.org/pdf/2501.14198v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2501.14198",
    "arxiv_authors": [
      "Zeyun Deng",
      "Joseph Campbell"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Sparse+Mixture-of-Experts+for+Non-Uniform+Noise+Reduction+in+MRI+Images+Zeyun+Deng+Joseph+Campbell",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "Z Deng",
        "id": "zSkMp_AAAAAJ"
      },
      {
        "name": "J Campbell -",
        "id": null
      }
    ],
    "citation_count": 3
  },
  {
    "arxiv_id": "2502.14214",
    "title": "Asymmetric Co-Training for Source-Free Few-Shot Domain Adaptation",
    "year": 2025,
    "published": "2025-02-20T02:58:45Z",
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "abstract": "Source-free unsupervised domain adaptation (SFUDA) has gained significant attention as an alternative to traditional unsupervised domain adaptation (UDA), which relies on the constant availability of labeled source data. However, SFUDA approaches come with inherent limitations that are frequently overlooked. These challenges include performance degradation when the unlabeled target data fails to meet critical assumptions, such as having a closed-set label distribution identical to that of the so",
    "arxiv_url": "https://arxiv.org/abs/2502.14214v1",
    "pdf_url": "https://arxiv.org/pdf/2502.14214v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2502.14214",
    "arxiv_authors": [
      "Gengxu Li",
      "Yuan Wu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Asymmetric+Co-Training+for+Source-Free+Few-Shot+Domain+Adaptation+Gengxu+Li+Yuan+Wu",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "G Li",
        "id": "yMb2FqYAAAAJ"
      },
      {
        "name": "Y Wu -",
        "id": null
      }
    ],
    "citation_count": 1
  },
  {
    "arxiv_id": "2307.04100",
    "title": "Visible and infrared self-supervised fusion trained on a single example",
    "year": 2023,
    "published": "2023-07-09T05:25:46Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Multispectral imaging is an important task of image processing and computer vision, which is especially relevant to applications such as dehazing or object detection. With the development of the RGBT (RGB & Thermal) sensor, the problem of visible (RGB) to Near Infrared (NIR) image fusion has become particularly timely. Indeed, while visible images see color, but suffer from noise, haze, and clouds, the NIR channel captures a clearer picture. The proposed approach fuses these two channels by trai",
    "arxiv_url": "https://arxiv.org/abs/2307.04100v2",
    "pdf_url": "https://arxiv.org/pdf/2307.04100v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2307.04100",
    "arxiv_authors": [
      "Nati Ofir",
      "Jean-Christophe Nebel"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Visible+and+infrared+self-supervised+fusion+trained+on+a+single+example+Nati+Ofir+Jean-Christophe+Nebel",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "N Ofir",
        "id": "Auz08a8AAAAJ"
      },
      {
        "name": "JC Nebel -",
        "id": null
      }
    ],
    "citation_count": null
  },
  {
    "arxiv_id": "2503.06700",
    "title": "MemorySAM: Memorize Modalities and Semantics with Segment Anything Model 2 for Multi-modal Semantic Segmentation",
    "year": 2025,
    "published": "2025-03-09T17:33:15Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Research has focused on Multi-Modal Semantic Segmentation (MMSS), where pixel-wise predictions are derived from multiple visual modalities captured by diverse sensors. Recently, the large vision model, Segment Anything Model 2 (SAM2), has shown strong zero-shot segmentation performance on both images and videos. When extending SAM2 to MMSS, two issues arise: 1. How can SAM2 be adapted to multi-modal data? 2. How can SAM2 better understand semantics? Inspired by cross-frame correlation in videos,",
    "arxiv_url": "https://arxiv.org/abs/2503.06700v2",
    "pdf_url": "https://arxiv.org/pdf/2503.06700v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2503.06700",
    "arxiv_authors": [
      "Chenfei Liao",
      "Xu Zheng",
      "Yuanhuiyi Lyu",
      "Haiwei Xue",
      "Yihong Cao",
      "Jiawen Wang",
      "Kailun Yang",
      "Xuming Hu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=MemorySAM%3A+Memorize+Modalities+and+Semantics+with+Segment+Anything+Model+2+for+Multi-modal+Semantic+Segmentation+Chenfei+Liao+Xu+Zheng+Yuanhuiyi+Lyu+Haiwei+Xue+Yihong+Cao",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "C Liao",
        "id": "2ZlT5o0AAAAJ"
      },
      {
        "name": "X Zheng",
        "id": "Ii1c51QAAAAJ"
      },
      {
        "name": "Y Lyu",
        "id": "6_lvla4AAAAJ"
      },
      {
        "name": "H Xue",
        "id": null
      },
      {
        "name": "Y Cao",
        "id": "j3364z8AAAAJ"
      },
      {
        "name": "J Wang",
        "id": null
      },
      {
        "name": "K Yang",
        "id": "pKFqWhgAAAAJ"
      },
      {
        "name": "X Hu",
        "id": "dbBKbXoAAAAJ"
      }
    ],
    "citation_count": 10
  },
  {
    "arxiv_id": "2307.00574",
    "title": "Bidirectional Temporal Diffusion Model for Temporally Consistent Human Animation",
    "year": 2023,
    "published": "2023-07-02T13:57:45Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "We introduce a method to generate temporally coherent human animation from a single image, a video, or a random noise. This problem has been formulated as modeling of an auto-regressive generation, i.e., to regress past frames to decode future frames. However, such unidirectional generation is highly prone to motion drifting over time, generating unrealistic human animation with significant artifacts such as appearance distortion. We claim that bidirectional temporal modeling enforces temporal c",
    "arxiv_url": "https://arxiv.org/abs/2307.00574v5",
    "pdf_url": "https://arxiv.org/pdf/2307.00574v5",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2307.00574",
    "arxiv_authors": [
      "Tserendorj Adiya",
      "Jae Shin Yoon",
      "Jungeun Lee",
      "Sanghun Kim",
      "Hwasup Lim"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Bidirectional+Temporal+Diffusion+Model+for+Temporally+Consistent+Human+Animation+Tserendorj+Adiya+Jae+Shin+Yoon+Jungeun+Lee+Sanghun+Kim+Hwasup+Lim",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "T Adiya",
        "id": "Q3n62X8AAAAJ"
      },
      {
        "name": "JS Yoon",
        "id": "q7nNPyYAAAAJ"
      },
      {
        "name": "J Lee",
        "id": null
      },
      {
        "name": "S Kim",
        "id": null
      },
      {
        "name": "H Lim -",
        "id": null
      }
    ],
    "citation_count": 3
  },
  {
    "arxiv_id": "2410.23413",
    "title": "EchoFM: Foundation Model for Generalizable Echocardiogram Analysis",
    "year": 2024,
    "published": "2024-10-30T19:32:02Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Foundation models have recently gained significant attention because of their generalizability and adaptability across multiple tasks and data distributions. Although medical foundation models have emerged, solutions for cardiac imaging, especially echocardiography videos, are still unexplored. In this paper, we introduce EchoFM, a foundation model specifically designed to represent and analyze echocardiography videos. In EchoFM, we propose a self-supervised learning framework that captures both",
    "arxiv_url": "https://arxiv.org/abs/2410.23413v2",
    "pdf_url": "https://arxiv.org/pdf/2410.23413v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2410.23413",
    "arxiv_authors": [
      "Sekeun Kim",
      "Pengfei Jin",
      "Sifan Song",
      "Cheng Chen",
      "Yiwei Li",
      "Hui Ren",
      "Xiang Li",
      "Tianming Liu",
      "Quanzheng Li"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=EchoFM%3A+Foundation+Model+for+Generalizable+Echocardiogram+Analysis+Sekeun+Kim+Pengfei+Jin+Sifan+Song+Cheng+Chen+Yiwei+Li",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "S Kim",
        "id": "tU6pacYAAAAJ"
      },
      {
        "name": "P Jin",
        "id": "4S5I1TwAAAAJ"
      },
      {
        "name": "S Song",
        "id": "A-SP7VYAAAAJ"
      },
      {
        "name": "C Chen",
        "id": "bRe3FlcAAAAJ"
      },
      {
        "name": "Y Li",
        "id": "sfEPWiAAAAAJ"
      },
      {
        "name": "H Ren",
        "id": "-8N8iIYAAAAJ"
      },
      {
        "name": "X Li",
        "id": "MjkwwiQAAAAJ"
      },
      {
        "name": "T Liu",
        "id": "92RPXm0AAAAJ"
      },
      {
        "name": "Q LiIEEE transactions on medical imaging",
        "id": null
      }
    ],
    "citation_count": 6
  },
  {
    "arxiv_id": "2307.02869",
    "title": "MomentDiff: Generative Video Moment Retrieval from Random to Real",
    "year": 2023,
    "published": "2023-07-06T09:12:13Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Video moment retrieval pursues an efficient and generalized solution to identify the specific temporal segments within an untrimmed video that correspond to a given language description. To achieve this goal, we provide a generative diffusion-based framework called MomentDiff, which simulates a typical human retrieval process from random browsing to gradual localization. Specifically, we first diffuse the real span to random noise, and learn to denoise the random noise to the original span with ",
    "arxiv_url": "https://arxiv.org/abs/2307.02869v2",
    "pdf_url": "https://arxiv.org/pdf/2307.02869v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2307.02869",
    "arxiv_authors": [
      "Pandeng Li",
      "Chen-Wei Xie",
      "Hongtao Xie",
      "Liming Zhao",
      "Lei Zhang",
      "Yun Zheng",
      "Deli Zhao",
      "Yongdong Zhang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=MomentDiff%3A+Generative+Video+Moment+Retrieval+from+Random+to+Real+Pandeng+Li+Chen-Wei+Xie+Hongtao+Xie+Liming+Zhao+Lei+Zhang",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "P Li",
        "id": "Moy-4-0AAAAJ"
      },
      {
        "name": "CW Xie",
        "id": "UHCDCRMAAAAJ"
      },
      {
        "name": "H Xie",
        "id": null
      },
      {
        "name": "L Zhao",
        "id": "noI8iqIAAAAJ"
      },
      {
        "name": "L Zhang",
        "id": "E8ya4h4AAAAJ"
      },
      {
        "name": "Y Zheng",
        "id": "-hFpScAAAAAJ"
      },
      {
        "name": "D Zhao",
        "id": "7LhjCn0AAAAJ"
      },
      {
        "name": "Y ZhangAdvances in neural information processing systems",
        "id": null
      }
    ],
    "citation_count": 107
  },
  {
    "arxiv_id": "2408.13809",
    "title": "On the Robustness of Kolmogorov-Arnold Networks: An Adversarial Perspective",
    "year": 2024,
    "published": "2024-08-25T11:10:15Z",
    "categories": [
      "cs.CV",
      "cs.CR"
    ],
    "abstract": "Kolmogorov-Arnold Networks (KANs) have recently emerged as a novel approach to function approximation, demonstrating remarkable potential in various domains. Despite their theoretical promise, the robustness of KANs under adversarial conditions has yet to be thoroughly examined. In this paper we explore the adversarial robustness of KANs, with a particular focus on image classification tasks. We assess the performance of KANs against standard white box and black-box adversarial attacks, comparin",
    "arxiv_url": "https://arxiv.org/abs/2408.13809v3",
    "pdf_url": "https://arxiv.org/pdf/2408.13809v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2408.13809",
    "arxiv_authors": [
      "Tal Alter",
      "Raz Lapid",
      "Moshe Sipper"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=On+the+Robustness+of+Kolmogorov-Arnold+Networks%3A+An+Adversarial+Perspective+Tal+Alter+Raz+Lapid+Moshe+Sipper",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "T Alter",
        "id": null
      },
      {
        "name": "R Lapid",
        "id": "ZvgO_hkAAAAJ"
      },
      {
        "name": "M Sipper -",
        "id": null
      }
    ],
    "citation_count": 14
  },
  {
    "arxiv_id": "2308.14066",
    "title": "Bi-Modality Medical Image Synthesis Using Semi-Supervised Sequential Generative Adversarial Networks",
    "year": 2023,
    "published": "2023-08-27T10:39:33Z",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "abstract": "In this paper, we propose a bi-modality medical image synthesis approach based on sequential generative adversarial network (GAN) and semi-supervised learning. Our approach consists of two generative modules that synthesize images of the two modalities in a sequential order. A method for measuring the synthesis complexity is proposed to automatically determine the synthesis order in our sequential GAN. Images of the modality with a lower complexity are synthesized first, and the counterparts wit",
    "arxiv_url": "https://arxiv.org/abs/2308.14066v2",
    "pdf_url": "https://arxiv.org/pdf/2308.14066v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2308.14066",
    "arxiv_authors": [
      "Xin Yang",
      "Yi Lin",
      "Zhiwei Wang",
      "Xin Li",
      "Kwang-Ting Cheng"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Bi-Modality+Medical+Image+Synthesis+Using+Semi-Supervised+Sequential+Generative+Adversarial+Networks+Xin+Yang+Yi+Lin+Zhiwei+Wang+Xin+Li+Kwang-Ting+Cheng",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "X Yang",
        "id": "FrQLlCYAAAAJ"
      },
      {
        "name": "Y Lin",
        "id": "JaSNrNEAAAAJ"
      },
      {
        "name": "Z Wang",
        "id": "LwQcmgYAAAAJ"
      },
      {
        "name": "X Li",
        "id": null
      },
      {
        "name": "KT ChengIEEE",
        "id": null
      }
    ],
    "citation_count": 77
  },
  {
    "arxiv_id": "2405.08209",
    "title": "Who's in and who's out? A case study of multimodal CLIP-filtering in DataComp",
    "year": 2024,
    "published": "2024-05-13T21:53:06Z",
    "categories": [
      "cs.CY",
      "cs.CL",
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "As training datasets become increasingly drawn from unstructured, uncontrolled environments such as the web, researchers and industry practitioners have increasingly relied upon data filtering techniques to \"filter out the noise\" of web-scraped data. While datasets have been widely shown to reflect the biases and values of their creators, in this paper we contribute to an emerging body of research that assesses the filters used to create these datasets. We show that image-text data filtering als",
    "arxiv_url": "https://arxiv.org/abs/2405.08209v2",
    "pdf_url": "https://arxiv.org/pdf/2405.08209v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2405.08209",
    "arxiv_authors": [
      "Rachel Hong",
      "William Agnew",
      "Tadayoshi Kohno",
      "Jamie Morgenstern"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Who%27s+in+and+who%27s+out%3F+A+case+study+of+multimodal+CLIP-filtering+in+DataComp+Rachel+Hong+William+Agnew+Tadayoshi+Kohno+Jamie+Morgenstern",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "R Hong",
        "id": "wsmZmYsAAAAJ"
      },
      {
        "name": "W Agnew",
        "id": "_B-tud4AAAAJ"
      },
      {
        "name": "T Kohno",
        "id": "s_YDrrgAAAAJ"
      },
      {
        "name": "J Morgenstern",
        "id": "t0ZfFH8AAAAJ"
      }
    ],
    "citation_count": 27
  },
  {
    "arxiv_id": "2412.06224",
    "title": "Uni-NaVid: A Video-based Vision-Language-Action Model for Unifying Embodied Navigation Tasks",
    "year": 2024,
    "published": "2024-12-09T05:55:55Z",
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "abstract": "A practical navigation agent must be capable of handling a wide range of interaction demands, such as following instructions, searching objects, answering questions, tracking people, and more. Existing models for embodied navigation fall short of serving as practical generalists in the real world, as they are often constrained by specific task configurations or pre-defined maps with discretized waypoints. In this work, we present Uni-NaVid, the first video-based vision-language-action (VLA) mode",
    "arxiv_url": "https://arxiv.org/abs/2412.06224v2",
    "pdf_url": "https://arxiv.org/pdf/2412.06224v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2412.06224",
    "arxiv_authors": [
      "Jiazhao Zhang",
      "Kunyu Wang",
      "Shaoan Wang",
      "Minghan Li",
      "Haoran Liu",
      "Songlin Wei",
      "Zhongyuan Wang",
      "Zhizheng Zhang",
      "He Wang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Uni-NaVid%3A+A+Video-based+Vision-Language-Action+Model+for+Unifying+Embodied+Navigation+Tasks+Jiazhao+Zhang+Kunyu+Wang+Shaoan+Wang+Minghan+Li+Haoran+Liu",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "J Zhang",
        "id": "zOZgpXwAAAAJ"
      },
      {
        "name": "K Wang",
        "id": "y4U2xvMAAAAJ"
      },
      {
        "name": "S Wang",
        "id": "m3FCQrkAAAAJ"
      },
      {
        "name": "M Li",
        "id": null
      },
      {
        "name": "H Liu",
        "id": null
      },
      {
        "name": "S Wei",
        "id": "jmtAxTgAAAAJ"
      },
      {
        "name": "Z Wang",
        "id": "4XVJrRAAAAAJ"
      },
      {
        "name": "Z Zhang",
        "id": "X7M0I8kAAAAJ"
      },
      {
        "name": "H Wang",
        "id": "roCAWkoAAAAJ"
      }
    ],
    "citation_count": 55
  },
  {
    "arxiv_id": "2310.15624",
    "title": "GUPNet++: Geometry Uncertainty Propagation Network for Monocular 3D Object Detection",
    "year": 2023,
    "published": "2023-10-24T08:45:15Z",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "Geometry plays a significant role in monocular 3D object detection. It can be used to estimate object depth by using the perspective projection between object's physical size and 2D projection in the image plane, which can introduce mathematical priors into deep models. However, this projection process also introduces error amplification, where the error of the estimated height is amplified and reflected into the projected depth. It leads to unreliable depth inferences and also impairs training ",
    "arxiv_url": "https://arxiv.org/abs/2310.15624v2",
    "pdf_url": "https://arxiv.org/pdf/2310.15624v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2310.15624",
    "arxiv_authors": [
      "Yan Lu",
      "Xinzhu Ma",
      "Lei Yang",
      "Tianzhu Zhang",
      "Yating Liu",
      "Qi Chu",
      "Tong He",
      "Yonghui Li",
      "Wanli Ouyang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=GUPNet%2B%2B%3A+Geometry+Uncertainty+Propagation+Network+for+Monocular+3D+Object+Detection+Yan+Lu+Xinzhu+Ma+Lei+Yang+Tianzhu+Zhang+Yating+Liu",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "Y Lu",
        "id": "YbbHudwAAAAJ"
      },
      {
        "name": "X Ma",
        "id": "8PuKa_8AAAAJ"
      },
      {
        "name": "L Yang",
        "id": "jZH2IPYAAAAJ"
      },
      {
        "name": "T Zhang",
        "id": null
      },
      {
        "name": "Y Liu",
        "id": null
      },
      {
        "name": "Q Chu",
        "id": "JZjOMdsAAAAJ"
      },
      {
        "name": "T He",
        "id": "kWADCMUAAAAJ"
      },
      {
        "name": "Y Li",
        "id": null
      },
      {
        "name": "W OuyangIEEE Transactions on Pattern Analysis and Machine Intelligence",
        "id": null
      }
    ],
    "citation_count": 14
  },
  {
    "arxiv_id": "2303.13069",
    "title": "Human Guided Ground-truth Generation for Realistic Image Super-resolution",
    "year": 2023,
    "published": "2023-03-23T06:53:14Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "How to generate the ground-truth (GT) image is a critical issue for training realistic image super-resolution (Real-ISR) models. Existing methods mostly take a set of high-resolution (HR) images as GTs and apply various degradations to simulate their low-resolution (LR) counterparts. Though great progress has been achieved, such an LR-HR pair generation scheme has several limitations. First, the perceptual quality of HR images may not be high enough, limiting the quality of Real-ISR outputs. Sec",
    "arxiv_url": "https://arxiv.org/abs/2303.13069v1",
    "pdf_url": "https://arxiv.org/pdf/2303.13069v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2303.13069",
    "arxiv_authors": [
      "Du Chen",
      "Jie Liang",
      "Xindong Zhang",
      "Ming Liu",
      "Hui Zeng",
      "Lei Zhang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Human+Guided+Ground-truth+Generation+for+Realistic+Image+Super-resolution+Du+Chen+Jie+Liang+Xindong+Zhang+Ming+Liu+Hui+Zeng",
    "gs_search_success": false,
    "gs_authors": null,
    "citation_count": null,
    "error_type": "fetch_failed"
  },
  {
    "arxiv_id": "2306.06048",
    "title": "How Does Fine-Tuning Impact Out-of-Distribution Detection for Vision-Language Models?",
    "year": 2023,
    "published": "2023-06-09T17:16:50Z",
    "categories": [
      "cs.CV",
      "cs.CY",
      "cs.LG"
    ],
    "abstract": "Recent large vision-language models such as CLIP have shown remarkable out-of-distribution (OOD) detection and generalization performance. However, their zero-shot in-distribution (ID) accuracy is often limited for downstream datasets. Recent CLIP-based fine-tuning methods such as prompt learning have demonstrated significant improvements in ID classification and OOD generalization where OOD labels are available. Nonetheless, it remains unclear whether the model is reliable to semantic shifts wi",
    "arxiv_url": "https://arxiv.org/abs/2306.06048v3",
    "pdf_url": "https://arxiv.org/pdf/2306.06048v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2306.06048",
    "arxiv_authors": [
      "Yifei Ming",
      "Yixuan Li"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=How+Does+Fine-Tuning+Impact+Out-of-Distribution+Detection+for+Vision-Language+Models%3F+Yifei+Ming+Yixuan+Li",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "Y Ming",
        "id": "Dh_4cyQAAAAJ"
      },
      {
        "name": "Y Li - International",
        "id": null
      }
    ],
    "citation_count": 65
  },
  {
    "arxiv_id": "2309.05346",
    "title": "Learning Geometric Representations of Objects via Interaction",
    "year": 2023,
    "published": "2023-09-11T09:45:22Z",
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "abstract": "We address the problem of learning representations from observations of a scene involving an agent and an external object the agent interacts with. To this end, we propose a representation learning framework extracting the location in physical space of both the agent and the object from unstructured observations of arbitrary nature. Our framework relies on the actions performed by the agent as the only source of supervision, while assuming that the object is displaced by the agent via unknown dy",
    "arxiv_url": "https://arxiv.org/abs/2309.05346v1",
    "pdf_url": "https://arxiv.org/pdf/2309.05346v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2309.05346",
    "arxiv_authors": [
      "Alfredo Reichlin",
      "Giovanni Luca Marchetti",
      "Hang Yin",
      "Anastasiia Varava",
      "Danica Kragic"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Learning+Geometric+Representations+of+Objects+via+Interaction+Alfredo+Reichlin+Giovanni+Luca+Marchetti+Hang+Yin+Anastasiia+Varava+Danica+Kragic",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "A Reichlin",
        "id": "PE0RmDUAAAAJ"
      },
      {
        "name": "GL Marchetti",
        "id": "ePYa2qAAAAAJ"
      },
      {
        "name": "H Yin",
        "id": "7VW7URUAAAAJ"
      },
      {
        "name": "A Varava",
        "id": "zbl1MvgAAAAJ"
      },
      {
        "name": "D KragicJoint European",
        "id": null
      }
    ],
    "citation_count": null
  },
  {
    "arxiv_id": "2404.00380",
    "title": "DHR: Dual Features-Driven Hierarchical Rebalancing in Inter- and Intra-Class Regions for Weakly-Supervised Semantic Segmentation",
    "year": 2024,
    "published": "2024-03-30T14:35:31Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Weakly-supervised semantic segmentation (WSS) ensures high-quality segmentation with limited data and excels when employed as input seed masks for large-scale vision models such as Segment Anything. However, WSS faces challenges related to minor classes since those are overlooked in images with adjacent multiple classes, a limitation originating from the overfitting of traditional expansion methods like Random Walk. We first address this by employing unsupervised and weakly-supervised feature ma",
    "arxiv_url": "https://arxiv.org/abs/2404.00380v2",
    "pdf_url": "https://arxiv.org/pdf/2404.00380v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2404.00380",
    "arxiv_authors": [
      "Sanghyun Jo",
      "Fei Pan",
      "In-Jae Yu",
      "Kyungsu Kim"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=DHR%3A+Dual+Features-Driven+Hierarchical+Rebalancing+in+Inter-+and+Intra-Class+Regions+for+Weakly-Supervised+Semantic+Segmentation+Sanghyun+Jo+Fei+Pan+In-Jae+Yu+Kyungsu+Kim",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "S Jo",
        "id": "xgP6q2YAAAAJ"
      },
      {
        "name": "F Pan",
        "id": "VGE3DlYAAAAJ"
      },
      {
        "name": "IJ Yu",
        "id": "Q5qZWJoAAAAJ"
      },
      {
        "name": "K Kim - European",
        "id": null
      }
    ],
    "citation_count": 3
  },
  {
    "arxiv_id": "2410.09747",
    "title": "t-READi: Transformer-Powered Robust and Efficient Multimodal Inference for Autonomous Driving",
    "year": 2024,
    "published": "2024-10-13T06:53:58Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.DC",
      "cs.LG",
      "cs.RO"
    ],
    "abstract": "Given the wide adoption of multimodal sensors (e.g., camera, lidar, radar) by autonomous vehicles (AVs), deep analytics to fuse their outputs for a robust perception become imperative. However, existing fusion methods often make two assumptions rarely holding in practice: i) similar data distributions for all inputs and ii) constant availability for all sensors. Because, for example, lidars have various resolutions and failures of radars may occur, such variability often results in significant p",
    "arxiv_url": "https://arxiv.org/abs/2410.09747v3",
    "pdf_url": "https://arxiv.org/pdf/2410.09747v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2410.09747",
    "arxiv_authors": [
      "Pengfei Hu",
      "Yuhang Qian",
      "Tianyue Zheng",
      "Ang Li",
      "Zhe Chen",
      "Yue Gao",
      "Xiuzhen Cheng",
      "Jun Luo"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=t-READi%3A+Transformer-Powered+Robust+and+Efficient+Multimodal+Inference+for+Autonomous+Driving+Pengfei+Hu+Yuhang+Qian+Tianyue+Zheng+Ang+Li+Zhe+Chen",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "P Hu",
        "id": "CMddi7oAAAAJ"
      },
      {
        "name": "Y Qian",
        "id": null
      },
      {
        "name": "T Zheng",
        "id": "A-YvDJgAAAAJ"
      },
      {
        "name": "A Li",
        "id": "JVKSaWIAAAAJ"
      },
      {
        "name": "Z Chen",
        "id": "hhl5-78AAAAJ"
      },
      {
        "name": "Y Gao",
        "id": "VCFRT_sAAAAJ"
      },
      {
        "name": "X Cheng",
        "id": "O1yGhH0AAAAJ"
      }
    ],
    "citation_count": 6
  },
  {
    "arxiv_id": "2412.18619",
    "title": "Next Token Prediction Towards Multimodal Intelligence: A Comprehensive Survey",
    "year": 2024,
    "published": "2024-12-16T05:02:25Z",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV",
      "cs.LG",
      "cs.MM",
      "eess.AS"
    ],
    "abstract": "Building on the foundations of language modeling in natural language processing, Next Token Prediction (NTP) has evolved into a versatile training objective for machine learning tasks across various modalities, achieving considerable success. As Large Language Models (LLMs) have advanced to unify understanding and generation tasks within the textual modality, recent research has shown that tasks from different modalities can also be effectively encapsulated within the NTP framework, transforming",
    "arxiv_url": "https://arxiv.org/abs/2412.18619v2",
    "pdf_url": "https://arxiv.org/pdf/2412.18619v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2412.18619",
    "arxiv_authors": [
      "Liang Chen",
      "Zekun Wang",
      "Shuhuai Ren",
      "Lei Li",
      "Haozhe Zhao",
      "Yunshui Li",
      "Zefan Cai",
      "Hongcheng Guo",
      "Lei Zhang",
      "Yizhe Xiong",
      "Yichi Zhang",
      "Ruoyu Wu",
      "Qingxiu Dong",
      "Ge Zhang",
      "Jian Yang",
      "Lingwei Meng",
      "Shujie Hu",
      "Yulong Chen",
      "Junyang Lin",
      "Shuai Bai",
      "Andreas Vlachos",
      "Xu Tan",
      "Minjia Zhang",
      "Wen Xiao",
      "Aaron Yee",
      "Tianyu Liu",
      "Baobao Chang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Next+Token+Prediction+Towards+Multimodal+Intelligence%3A+A+Comprehensive+Survey+Liang+Chen+Zekun+Wang+Shuhuai+Ren+Lei+Li+Haozhe+Zhao",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "L Chen",
        "id": "lMKPaTYAAAAJ"
      },
      {
        "name": "Z Wang",
        "id": "g-AOtlYAAAAJ"
      },
      {
        "name": "S Ren",
        "id": "3X8yS-cAAAAJ"
      },
      {
        "name": "L Li",
        "id": "1B0l7U8AAAAJ"
      },
      {
        "name": "H Zhao",
        "id": "skIXywUAAAAJ"
      },
      {
        "name": "Y Li",
        "id": "juR8ZS4AAAAJ"
      },
      {
        "name": "Z Cai",
        "id": "eyIrttAAAAAJ"
      },
      {
        "name": "H Guo",
        "id": "eynbo4cAAAAJ"
      },
      {
        "name": "L Zhang",
        "id": "K8di4JwAAAAJ"
      },
      {
        "name": "Y Xiong",
        "id": "HptpkCYAAAAJ"
      },
      {
        "name": "Y Zhang",
        "id": "qcdWjbgAAAAJ"
      },
      {
        "name": "R Wu",
        "id": null
      }
    ],
    "citation_count": 29
  },
  {
    "arxiv_id": "2402.19463",
    "title": "SeMoLi: What Moves Together Belongs Together",
    "year": 2024,
    "published": "2024-02-29T18:54:53Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "We tackle semi-supervised object detection based on motion cues. Recent results suggest that heuristic-based clustering methods in conjunction with object trackers can be used to pseudo-label instances of moving objects and use these as supervisory signals to train 3D object detectors in Lidar data without manual supervision. We re-think this approach and suggest that both, object detection, as well as motion-inspired pseudo-labeling, can be tackled in a data-driven manner. We leverage recent ad",
    "arxiv_url": "https://arxiv.org/abs/2402.19463v2",
    "pdf_url": "https://arxiv.org/pdf/2402.19463v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2402.19463",
    "arxiv_authors": [
      "Jenny Seidenschwarz",
      "Aljo≈°a O≈°ep",
      "Francesco Ferroni",
      "Simon Lucey",
      "Laura Leal-Taix√©"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=SeMoLi%3A+What+Moves+Together+Belongs+Together+Jenny+Seidenschwarz+Aljo%C5%A1a+O%C5%A1ep+Francesco+Ferroni+Simon+Lucey+Laura+Leal-Taix%C3%A9",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "J Seidenschwarz",
        "id": "7AiuDocAAAAJ"
      },
      {
        "name": "A Osep",
        "id": "X7EN55cAAAAJ"
      },
      {
        "name": "F Ferroni",
        "id": "gNgfYKcAAAAJ"
      },
      {
        "name": "S Lucey",
        "id": "vmAe35UAAAAJ"
      },
      {
        "name": "L Leal-Taix√©",
        "id": "tT2TC-UAAAAJ"
      }
    ],
    "citation_count": 10
  },
  {
    "arxiv_id": "2309.03576",
    "title": "DropPos: Pre-Training Vision Transformers by Reconstructing Dropped Positions",
    "year": 2023,
    "published": "2023-09-07T09:12:02Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "As it is empirically observed that Vision Transformers (ViTs) are quite insensitive to the order of input tokens, the need for an appropriate self-supervised pretext task that enhances the location awareness of ViTs is becoming evident. To address this, we present DropPos, a novel pretext task designed to reconstruct Dropped Positions. The formulation of DropPos is simple: we first drop a large random subset of positional embeddings and then the model classifies the actual position for each non-",
    "arxiv_url": "https://arxiv.org/abs/2309.03576v2",
    "pdf_url": "https://arxiv.org/pdf/2309.03576v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2309.03576",
    "arxiv_authors": [
      "Haochen Wang",
      "Junsong Fan",
      "Yuxi Wang",
      "Kaiyou Song",
      "Tong Wang",
      "Zhaoxiang Zhang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=DropPos%3A+Pre-Training+Vision+Transformers+by+Reconstructing+Dropped+Positions+Haochen+Wang+Junsong+Fan+Yuxi+Wang+Kaiyou+Song+Tong+Wang",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "H Wang",
        "id": "oNlpTdcAAAAJ"
      },
      {
        "name": "J Fan",
        "id": "AfK4UcUAAAAJ"
      },
      {
        "name": "Y Wang",
        "id": "waLCodcAAAAJ"
      },
      {
        "name": "K Song",
        "id": "VLqzM1wAAAAJ"
      },
      {
        "name": "T Wang",
        "id": null
      },
      {
        "name": "ZX ZHANGAdvances in Neural Information Processing Systems",
        "id": null
      }
    ],
    "citation_count": 29
  },
  {
    "arxiv_id": "2307.04129",
    "title": "Cross-modal Orthogonal High-rank Augmentation for RGB-Event Transformer-trackers",
    "year": 2023,
    "published": "2023-07-09T08:58:47Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "This paper addresses the problem of cross-modal object tracking from RGB videos and event data. Rather than constructing a complex cross-modal fusion network, we explore the great potential of a pre-trained vision Transformer (ViT). Particularly, we delicately investigate plug-and-play training augmentations that encourage the ViT to bridge the vast distribution gap between the two modalities, enabling comprehensive cross-modal information interaction and thus enhancing its ability. Specifically",
    "arxiv_url": "https://arxiv.org/abs/2307.04129v2",
    "pdf_url": "https://arxiv.org/pdf/2307.04129v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2307.04129",
    "arxiv_authors": [
      "Zhiyu Zhu",
      "Junhui Hou",
      "Dapeng Oliver Wu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Cross-modal+Orthogonal+High-rank+Augmentation+for+RGB-Event+Transformer-trackers+Zhiyu+Zhu+Junhui+Hou+Dapeng+Oliver+Wu",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "Z Zhu",
        "id": "d1L0KkoAAAAJ"
      },
      {
        "name": "J Hou",
        "id": "j6eefhwAAAAJ"
      },
      {
        "name": "DO Wu -",
        "id": null
      }
    ],
    "citation_count": 56
  },
  {
    "arxiv_id": "2408.13585",
    "title": "FLEURS-ASL: Including American Sign Language in Massively Multilingual Multitask Evaluation",
    "year": 2024,
    "published": "2024-08-24T13:59:41Z",
    "categories": [
      "cs.CL",
      "cs.CV"
    ],
    "abstract": "Sign language translation has historically been peripheral to mainstream machine translation research. In order to help converge the fields, we introduce FLEURS-ASL, an extension of the multiway parallel benchmarks FLORES (for text) and FLEURS (for speech) to support their first sign language (as video), American Sign Language, translated by 5 Certified Deaf Interpreters. FLEURS-ASL can be used to evaluate a variety of tasks -- primarily sentence- and discourse-level translation -- between ASL a",
    "arxiv_url": "https://arxiv.org/abs/2408.13585v1",
    "pdf_url": "https://arxiv.org/pdf/2408.13585v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2408.13585",
    "arxiv_authors": [
      "Garrett Tanzer"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=FLEURS-ASL%3A+Including+American+Sign+Language+in+Massively+Multilingual+Multitask+Evaluation+Garrett+Tanzer",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "G Tanzer -",
        "id": null
      }
    ],
    "citation_count": 8
  },
  {
    "arxiv_id": "2409.18556",
    "title": "CodeSCAN: ScreenCast ANalysis for Video Programming Tutorials",
    "year": 2024,
    "published": "2024-09-27T08:53:17Z",
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "abstract": "Programming tutorials in the form of coding screencasts play a crucial role in programming education, serving both novices and experienced developers. However, the video format of these tutorials presents a challenge due to the difficulty of searching for and within videos. Addressing the absence of large-scale and diverse datasets for screencast analysis, we introduce the CodeSCAN dataset. It comprises 12,000 screenshots captured from the Visual Studio Code environment during development, featu",
    "arxiv_url": "https://arxiv.org/abs/2409.18556v1",
    "pdf_url": "https://arxiv.org/pdf/2409.18556v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2409.18556",
    "arxiv_authors": [
      "Alexander Naumann",
      "Felix Hertlein",
      "Jacqueline H√∂llig",
      "Lucas Cazzonelli",
      "Steffen Thoma"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=CodeSCAN%3A+ScreenCast+ANalysis+for+Video+Programming+Tutorials+Alexander+Naumann+Felix+Hertlein+Jacqueline+H%C3%B6llig+Lucas+Cazzonelli+Steffen+Thoma",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "A Naumann",
        "id": "wCcE6DAAAAAJ"
      },
      {
        "name": "F Hertlein",
        "id": "HLS2VW0AAAAJ"
      },
      {
        "name": "J H√∂llig",
        "id": "kFuXA4YAAAAJ"
      },
      {
        "name": "L Cazzonelli",
        "id": "aQvJ7VcAAAAJ"
      },
      {
        "name": "S Thoma",
        "id": null
      }
    ],
    "citation_count": 1
  },
  {
    "arxiv_id": "2502.00079",
    "title": "Advanced Assessment of Stroke in Retinal Fundus Imaging with Deep Multi-view Learning",
    "year": 2025,
    "published": "2025-01-31T14:10:04Z",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "abstract": "Stroke is globally a major cause of mortality and morbidity, and hence accurate and rapid diagnosis of stroke is valuable. Retinal fundus imaging reveals the known markers of elevated stroke risk in the eyes, which are retinal venular widening, arteriolar narrowing, and increased tortuosity. In contrast to other imaging techniques used for stroke diagnosis, the acquisition of fundus images is easy, non-invasive, fast, and inexpensive. Therefore, in this study, we propose a multi-view stroke netw",
    "arxiv_url": "https://arxiv.org/abs/2502.00079v1",
    "pdf_url": "https://arxiv.org/pdf/2502.00079v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2502.00079",
    "arxiv_authors": [
      "Aysen Degerli",
      "Mika Hilvo",
      "Juha Pajula",
      "Petri Huhtinen",
      "Pekka J√§k√§l√§"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Advanced+Assessment+of+Stroke+in+Retinal+Fundus+Imaging+with+Deep+Multi-view+Learning+Aysen+Degerli+Mika+Hilvo+Juha+Pajula+Petri+Huhtinen+Pekka+J%C3%A4k%C3%A4l%C3%A4",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "A Degerli",
        "id": "U1SUwJQAAAAJ"
      },
      {
        "name": "M Hilvo",
        "id": null
      },
      {
        "name": "J Pajula",
        "id": null
      },
      {
        "name": "P Huhtinen",
        "id": null
      },
      {
        "name": "P J√§k√§l√§",
        "id": null
      }
    ],
    "citation_count": null
  },
  {
    "arxiv_id": "2408.12466",
    "title": "WCEbleedGen: A wireless capsule endoscopy dataset and its benchmarking for automatic bleeding classification, detection, and segmentation",
    "year": 2024,
    "published": "2024-08-22T15:06:50Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "abstract": "Computer-based analysis of Wireless Capsule Endoscopy (WCE) is crucial. However, a medically annotated WCE dataset for training and evaluation of automatic classification, detection, and segmentation of bleeding and non-bleeding frames is currently lacking. The present work focused on development of a medically annotated WCE dataset called WCEbleedGen for automatic classification, detection, and segmentation of bleeding and non-bleeding frames. It comprises 2,618 WCE bleeding and non-bleeding fr",
    "arxiv_url": "https://arxiv.org/abs/2408.12466v1",
    "pdf_url": "https://arxiv.org/pdf/2408.12466v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2408.12466",
    "arxiv_authors": [
      "Palak Handa",
      "Manas Dhir",
      "Amirreza Mahbod",
      "Florian Schwarzhans",
      "Ramona Woitek",
      "Nidhi Goel",
      "Deepak Gunjan"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=WCEbleedGen%3A+A+wireless+capsule+endoscopy+dataset+and+its+benchmarking+for+automatic+bleeding+classification%2C+detection%2C+and+segmentation+Palak+Handa+Manas+Dhir+Amirreza+Mahbod+Florian+Schwarzhans+Ramona+Woitek",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "P Handa",
        "id": "mbjj4WsAAAAJ"
      },
      {
        "name": "M Dhir",
        "id": null
      },
      {
        "name": "A Mahbod",
        "id": "yJgcjbMAAAAJ"
      },
      {
        "name": "F Schwarzhans",
        "id": "GkUE8qkAAAAJ"
      },
      {
        "name": "R Woitek",
        "id": null
      },
      {
        "name": "N Goel",
        "id": "gDpJuRwAAAAJ"
      },
      {
        "name": "D Gunjan",
        "id": null
      }
    ],
    "citation_count": 9
  },
  {
    "arxiv_id": "2402.07370",
    "title": "SelfSwapper: Self-Supervised Face Swapping via Shape Agnostic Masked AutoEncoder",
    "year": 2024,
    "published": "2024-02-12T02:01:53Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Face swapping has gained significant attention for its varied applications. Most previous face swapping approaches have relied on the seesaw game training scheme, also known as the target-oriented approach. However, this often leads to instability in model training and results in undesired samples with blended identities due to the target identity leakage problem. Source-oriented methods achieve more stable training with self-reconstruction objective but often fail to accurately reflect target i",
    "arxiv_url": "https://arxiv.org/abs/2402.07370v2",
    "pdf_url": "https://arxiv.org/pdf/2402.07370v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2402.07370",
    "arxiv_authors": [
      "Jaeseong Lee",
      "Junha Hyung",
      "Sohyun Jeong",
      "Jaegul Choo"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=SelfSwapper%3A+Self-Supervised+Face+Swapping+via+Shape+Agnostic+Masked+AutoEncoder+Jaeseong+Lee+Junha+Hyung+Sohyun+Jeong+Jaegul+Choo",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "J Lee",
        "id": "vWkHKrMAAAAJ"
      },
      {
        "name": "J Hyung",
        "id": "DsaV5aQAAAAJ"
      },
      {
        "name": "S Jung",
        "id": null
      },
      {
        "name": "J Choo - European",
        "id": null
      }
    ],
    "citation_count": 3
  },
  {
    "arxiv_id": "2303.13269",
    "title": "Disguise without Disruption: Utility-Preserving Face De-Identification",
    "year": 2023,
    "published": "2023-03-23T13:50:46Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "With the rise of cameras and smart sensors, humanity generates an exponential amount of data. This valuable information, including underrepresented cases like AI in medical settings, can fuel new deep-learning tools. However, data scientists must prioritize ensuring privacy for individuals in these untapped datasets, especially for images or videos with faces, which are prime targets for identification methods. Proposed solutions to de-identify such images often compromise non-identifying facial",
    "arxiv_url": "https://arxiv.org/abs/2303.13269v2",
    "pdf_url": "https://arxiv.org/pdf/2303.13269v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2303.13269",
    "arxiv_authors": [
      "Zikui Cai",
      "Zhongpai Gao",
      "Benjamin Planche",
      "Meng Zheng",
      "Terrence Chen",
      "M. Salman Asif",
      "Ziyan Wu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Disguise+without+Disruption%3A+Utility-Preserving+Face+De-Identification+Zikui+Cai+Zhongpai+Gao+Benjamin+Planche+Meng+Zheng+Terrence+Chen",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "Z Cai",
        "id": "-SrU69AAAAAJ"
      },
      {
        "name": "Z Gao",
        "id": "jbjHT1YAAAAJ"
      },
      {
        "name": "B Planche",
        "id": "cP3ahiAAAAAJ"
      },
      {
        "name": "M Zheng",
        "id": "1D5PfMgAAAAJ"
      },
      {
        "name": "T Chen",
        "id": "S2BT6ogAAAAJ"
      },
      {
        "name": "MS Asif",
        "id": "Dl0puDcAAAAJ"
      },
      {
        "name": "Z Wu",
        "id": "CkPUb-4AAAAJ"
      }
    ],
    "citation_count": 23
  },
  {
    "arxiv_id": "2504.12542",
    "title": "Post-Hurricane Debris Segmentation Using Fine-Tuned Foundational Vision Models",
    "year": 2025,
    "published": "2025-04-17T00:08:50Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Timely and accurate detection of hurricane debris is critical for effective disaster response and community resilience. While post-disaster aerial imagery is readily available, robust debris segmentation solutions applicable across multiple disaster regions remain limited. Developing a generalized solution is challenging due to varying environmental and imaging conditions that alter debris' visual signatures across different regions, further compounded by the scarcity of training data. This stud",
    "arxiv_url": "https://arxiv.org/abs/2504.12542v2",
    "pdf_url": "https://arxiv.org/pdf/2504.12542v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2504.12542",
    "arxiv_authors": [
      "Kooshan Amini",
      "Yuhao Liu",
      "Jamie Ellen Padgett",
      "Guha Balakrishnan",
      "Ashok Veeraraghavan"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Post-Hurricane+Debris+Segmentation+Using+Fine-Tuned+Foundational+Vision+Models+Kooshan+Amini+Yuhao+Liu+Jamie+Ellen+Padgett+Guha+Balakrishnan+Ashok+Veeraraghavan",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "K Amini",
        "id": "IeIQM1kAAAAJ"
      },
      {
        "name": "Y Liu",
        "id": "2eI0k5gAAAAJ"
      },
      {
        "name": "JE Padgett",
        "id": "aHuQagUAAAAJ"
      },
      {
        "name": "G Balakrishnan",
        "id": "8rZyuc8AAAAJ"
      },
      {
        "name": "A Veeraraghavan",
        "id": "tI-oUmsAAAAJ"
      }
    ],
    "citation_count": null
  },
  {
    "arxiv_id": "2310.01926",
    "title": "DARTH: Holistic Test-time Adaptation for Multiple Object Tracking",
    "year": 2023,
    "published": "2023-10-03T10:10:42Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Multiple object tracking (MOT) is a fundamental component of perception systems for autonomous driving, and its robustness to unseen conditions is a requirement to avoid life-critical failures. Despite the urge of safety in driving systems, no solution to the MOT adaptation problem to domain shift in test-time conditions has ever been proposed. However, the nature of a MOT system is manifold - requiring object detection and instance association - and adapting all its components is non-trivial. I",
    "arxiv_url": "https://arxiv.org/abs/2310.01926v1",
    "pdf_url": "https://arxiv.org/pdf/2310.01926v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2310.01926",
    "arxiv_authors": [
      "Mattia Segu",
      "Bernt Schiele",
      "Fisher Yu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=DARTH%3A+Holistic+Test-time+Adaptation+for+Multiple+Object+Tracking+Mattia+Segu+Bernt+Schiele+Fisher+Yu",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "M Segu",
        "id": "dwX7yWkAAAAJ"
      },
      {
        "name": "B Schiele",
        "id": "z76PBfYAAAAJ"
      },
      {
        "name": "F Yu -",
        "id": null
      }
    ],
    "citation_count": 19
  },
  {
    "arxiv_id": "2412.04472",
    "title": "Stereo Anywhere: Robust Zero-Shot Deep Stereo Matching Even Where Either Stereo or Mono Fail",
    "year": 2024,
    "published": "2024-12-05T18:59:58Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "We introduce Stereo Anywhere, a novel stereo-matching framework that combines geometric constraints with robust priors from monocular depth Vision Foundation Models (VFMs). By elegantly coupling these complementary worlds through a dual-branch architecture, we seamlessly integrate stereo matching with learned contextual cues. Following this design, our framework introduces novel cost volume fusion mechanisms that effectively handle critical challenges such as textureless regions, occlusions, and",
    "arxiv_url": "https://arxiv.org/abs/2412.04472v2",
    "pdf_url": "https://arxiv.org/pdf/2412.04472v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2412.04472",
    "arxiv_authors": [
      "Luca Bartolomei",
      "Fabio Tosi",
      "Matteo Poggi",
      "Stefano Mattoccia"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Stereo+Anywhere%3A+Robust+Zero-Shot+Deep+Stereo+Matching+Even+Where+Either+Stereo+or+Mono+Fail+Luca+Bartolomei+Fabio+Tosi+Matteo+Poggi+Stefano+Mattoccia",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "L Bartolomei",
        "id": "kCDBYqUAAAAJ"
      },
      {
        "name": "F Tosi",
        "id": "5-UOaQkAAAAJ"
      },
      {
        "name": "M Poggi",
        "id": "bve0VwgAAAAJ"
      },
      {
        "name": "S Mattoccia",
        "id": "P954CG8AAAAJ"
      }
    ],
    "citation_count": 31
  },
  {
    "arxiv_id": "2412.09919",
    "title": "B-VLLM: A Vision Large Language Model with Balanced Spatio-Temporal Tokens",
    "year": 2024,
    "published": "2024-12-13T07:13:40Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Recently, Vision Large Language Models (VLLMs) integrated with vision encoders have shown promising performance in vision understanding. The key of VLLMs is to encode visual content into sequences of visual tokens, enabling VLLMs to simultaneously process both visual and textual content. However, understanding videos, especially long videos, remain a challenge to VLLMs as the number of visual tokens grows rapidly when encoding videos, resulting in the risk of exceeding the context window of VLLM",
    "arxiv_url": "https://arxiv.org/abs/2412.09919v2",
    "pdf_url": "https://arxiv.org/pdf/2412.09919v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2412.09919",
    "arxiv_authors": [
      "Zhuqiang Lu",
      "Zhenfei Yin",
      "Mengwei He",
      "Zhihui Wang",
      "Zicheng Liu",
      "Zhiyong Wang",
      "Kun Hu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=B-VLLM%3A+A+Vision+Large+Language+Model+with+Balanced+Spatio-Temporal+Tokens+Zhuqiang+Lu+Zhenfei+Yin+Mengwei+He+Zhihui+Wang+Zicheng+Liu",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "Z Lu",
        "id": "gqqMGsMAAAAJ"
      },
      {
        "name": "Z Yin",
        "id": null
      },
      {
        "name": "M He",
        "id": null
      },
      {
        "name": "Z Wang",
        "id": "Sqou_P0AAAAJ"
      },
      {
        "name": "Z Liu",
        "id": null
      },
      {
        "name": "Z Wang",
        "id": "Sqou_P0AAAAJ"
      },
      {
        "name": "K Hu",
        "id": "_UiH_xgAAAAJ"
      }
    ],
    "citation_count": 2
  },
  {
    "arxiv_id": "2304.04016",
    "title": "Arithmetic Intensity Balancing Convolution for Hardware-aware Efficient Block Design",
    "year": 2023,
    "published": "2023-04-08T14:06:54Z",
    "categories": [
      "cs.LG",
      "cs.AR",
      "cs.CV"
    ],
    "abstract": "As deep learning advances, edge devices and lightweight neural networks are becoming more important. To reduce latency in the AI accelerator, it's essential to not only reduce FLOPs but also enhance hardware performance. We proposed an arithmetic intensity balancing convolution (ABConv) to address the issue of the overall intensity being limited by the small weight arithmetic intensity for convolution with a small spatial size. ABConv increased the maximum bound of overall arithmetic intensity a",
    "arxiv_url": "https://arxiv.org/abs/2304.04016v1",
    "pdf_url": "https://arxiv.org/pdf/2304.04016v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2304.04016",
    "arxiv_authors": [
      "Shinkook Choi",
      "Junkyeong Choi"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Arithmetic+Intensity+Balancing+Convolution+for+Hardware-aware+Efficient+Block+Design+Shinkook+Choi+Junkyeong+Choi",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "S Choi",
        "id": "85kIotQAAAAJ"
      },
      {
        "name": "J Choi -",
        "id": null
      }
    ],
    "citation_count": 2
  },
  {
    "arxiv_id": "2311.15668",
    "title": "Deformation-Guided Unsupervised Non-Rigid Shape Matching",
    "year": 2023,
    "published": "2023-11-27T09:55:55Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "We present an unsupervised data-driven approach for non-rigid shape matching. Shape matching identifies correspondences between two shapes and is a fundamental step in many computer vision and graphics applications. Our approach is designed to be particularly robust when matching shapes digitized using 3D scanners that contain fine geometric detail and suffer from different types of noise including topological noise caused by the coalescence of spatially close surface regions. We build on two st",
    "arxiv_url": "https://arxiv.org/abs/2311.15668v1",
    "pdf_url": "https://arxiv.org/pdf/2311.15668v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2311.15668",
    "arxiv_authors": [
      "Aymen Merrouche",
      "Joao Regateiro",
      "Stefanie Wuhrer",
      "Edmond Boyer"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Deformation-Guided+Unsupervised+Non-Rigid+Shape+Matching+Aymen+Merrouche+Joao+Regateiro+Stefanie+Wuhrer+Edmond+Boyer",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "A Merrouche",
        "id": "l0GO32YAAAAJ"
      },
      {
        "name": "J Regateiro",
        "id": "-bafXF0AAAAJ"
      },
      {
        "name": "S Wuhrer",
        "id": "Wy1l2jsAAAAJ"
      },
      {
        "name": "E Boyer",
        "id": "bxcnXn8AAAAJ"
      }
    ],
    "citation_count": 3
  },
  {
    "arxiv_id": "2404.01727",
    "title": "Generalizing 6-DoF Grasp Detection via Domain Prior Knowledge",
    "year": 2024,
    "published": "2024-04-02T08:33:21Z",
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "abstract": "We focus on the generalization ability of the 6-DoF grasp detection method in this paper. While learning-based grasp detection methods can predict grasp poses for unseen objects using the grasp distribution learned from the training set, they often exhibit a significant performance drop when encountering objects with diverse shapes and structures. To enhance the grasp detection methods' generalization ability, we incorporate domain prior knowledge of robotic grasping, enabling better adaptation ",
    "arxiv_url": "https://arxiv.org/abs/2404.01727v1",
    "pdf_url": "https://arxiv.org/pdf/2404.01727v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2404.01727",
    "arxiv_authors": [
      "Haoxiang Ma",
      "Modi Shi",
      "Boyang Gao",
      "Di Huang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Generalizing+6-DoF+Grasp+Detection+via+Domain+Prior+Knowledge+Haoxiang+Ma+Modi+Shi+Boyang+Gao+Di+Huang",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "H Ma",
        "id": "RC0U_o0AAAAJ"
      },
      {
        "name": "M Shi",
        "id": "e8e7k58AAAAJ"
      },
      {
        "name": "B Gao",
        "id": null
      },
      {
        "name": "D Huang -",
        "id": null
      }
    ],
    "citation_count": 11
  },
  {
    "arxiv_id": "2408.11805",
    "title": "ACE: A Cross-Platform Visual-Exoskeletons System for Low-Cost Dexterous Teleoperation",
    "year": 2024,
    "published": "2024-08-21T17:48:31Z",
    "categories": [
      "cs.RO",
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "Learning from demonstrations has shown to be an effective approach to robotic manipulation, especially with the recently collected large-scale robot data with teleoperation systems. Building an efficient teleoperation system across diverse robot platforms has become more crucial than ever. However, there is a notable lack of cost-effective and user-friendly teleoperation systems for different end-effectors, e.g., anthropomorphic robot hands and grippers, that can operate across multiple platform",
    "arxiv_url": "https://arxiv.org/abs/2408.11805v1",
    "pdf_url": "https://arxiv.org/pdf/2408.11805v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2408.11805",
    "arxiv_authors": [
      "Shiqi Yang",
      "Minghuan Liu",
      "Yuzhe Qin",
      "Runyu Ding",
      "Jialong Li",
      "Xuxin Cheng",
      "Ruihan Yang",
      "Sha Yi",
      "Xiaolong Wang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=ACE%3A+A+Cross-Platform+Visual-Exoskeletons+System+for+Low-Cost+Dexterous+Teleoperation+Shiqi+Yang+Minghuan+Liu+Yuzhe+Qin+Runyu+Ding+Jialong+Li",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "S Yang -",
        "id": null
      }
    ],
    "citation_count": 69
  },
  {
    "arxiv_id": "2310.20083",
    "title": "Facial asymmetry: A Computer Vision based behaviometric index for assessment during a face-to-face interview",
    "year": 2023,
    "published": "2023-10-30T23:42:08Z",
    "categories": [
      "cs.CV",
      "cs.CY"
    ],
    "abstract": "Choosing the right person for the right job makes the personnel interview process a cognitively demanding task. Psychometric tests, followed by an interview, have often been used to aid the process although such mechanisms have their limitations. While psychometric tests suffer from faking or social desirability of responses, the interview process depends on the way the responses are analyzed by the interviewers. We propose the use of behaviometry as an assistive tool to facilitate an objective ",
    "arxiv_url": "https://arxiv.org/abs/2310.20083v1",
    "pdf_url": "https://arxiv.org/pdf/2310.20083v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2310.20083",
    "arxiv_authors": [
      "Shuvam Keshari",
      "Tanusree Dutta",
      "Raju Mullick",
      "Ashish Rathor",
      "Priyadarshi Patnaik"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Facial+asymmetry%3A+A+Computer+Vision+based+behaviometric+index+for+assessment+during+a+face-to-face+interview+Shuvam+Keshari+Tanusree+Dutta+Raju+Mullick+Ashish+Rathor+Priyadarshi+Patnaik",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "S Keshari",
        "id": "LVdUDqIAAAAJ"
      },
      {
        "name": "T Dutta",
        "id": "RoHtbRUAAAAJ"
      },
      {
        "name": "R Mullick",
        "id": null
      },
      {
        "name": "A Rathor",
        "id": null
      },
      {
        "name": "P Patnaik",
        "id": "et5Z9HwAAAAJ"
      }
    ],
    "citation_count": null
  },
  {
    "arxiv_id": "2401.12164",
    "title": "Semi-supervised segmentation of land cover images using nonlinear canonical correlation analysis with multiple features and t-SNE",
    "year": 2024,
    "published": "2024-01-22T17:56:07Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Image segmentation is a clustering task whereby each pixel is assigned a cluster label. Remote sensing data usually consists of multiple bands of spectral images in which there exist semantically meaningful land cover subregions, co-registered with other source data such as LIDAR (LIght Detection And Ranging) data, where available. This suggests that, in order to account for spatial correlation between pixels, a feature vector associated with each pixel may be a vectorized tensor representing th",
    "arxiv_url": "https://arxiv.org/abs/2401.12164v1",
    "pdf_url": "https://arxiv.org/pdf/2401.12164v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2401.12164",
    "arxiv_authors": [
      "Hong Wei",
      "James Xiao",
      "Yichao Zhang",
      "Xia Hong"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Semi-supervised+segmentation+of+land+cover+images+using+nonlinear+canonical+correlation+analysis+with+multiple+features+and+t-SNE+Hong+Wei+James+Xiao+Yichao+Zhang+Xia+Hong",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "H Wei",
        "id": "EvdNshIAAAAJ"
      },
      {
        "name": "J Xiao",
        "id": "tMaofHIAAAAJ"
      },
      {
        "name": "Y Zhang",
        "id": null
      },
      {
        "name": "X Hong -",
        "id": null
      }
    ],
    "citation_count": null
  },
  {
    "arxiv_id": "2505.02325",
    "title": "TeDA: Boosting Vision-Lanuage Models for Zero-Shot 3D Object Retrieval via Testing-time Distribution Alignment",
    "year": 2025,
    "published": "2025-05-05T02:47:07Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Learning discriminative 3D representations that generalize well to unknown testing categories is an emerging requirement for many real-world 3D applications. Existing well-established methods often struggle to attain this goal due to insufficient 3D training data from broader concepts. Meanwhile, pre-trained large vision-language models (e.g., CLIP) have shown remarkable zero-shot generalization capabilities. Yet, they are limited in extracting suitable 3D representations due to substantial gaps",
    "arxiv_url": "https://arxiv.org/abs/2505.02325v1",
    "pdf_url": "https://arxiv.org/pdf/2505.02325v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2505.02325",
    "arxiv_authors": [
      "Zhichuan Wang",
      "Yang Zhou",
      "Jinhai Xiang",
      "Yulong Wang",
      "Xinwei He"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=TeDA%3A+Boosting+Vision-Lanuage+Models+for+Zero-Shot+3D+Object+Retrieval+via+Testing-time+Distribution+Alignment+Zhichuan+Wang+Yang+Zhou+Jinhai+Xiang+Yulong+Wang+Xinwei+He",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "Z Wang",
        "id": "Qvrnv1gAAAAJ"
      },
      {
        "name": "Y Zhou",
        "id": "9oXvA2IAAAAJ"
      },
      {
        "name": "J Xiang",
        "id": "z4YTksIAAAAJ"
      },
      {
        "name": "Y Wang",
        "id": null
      },
      {
        "name": "X He -",
        "id": null
      }
    ],
    "citation_count": 1
  },
  {
    "arxiv_id": "2407.17035",
    "title": "Q-Ground: Image Quality Grounding with Large Multi-modality Models",
    "year": 2024,
    "published": "2024-07-24T06:42:46Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Recent advances of large multi-modality models (LMM) have greatly improved the ability of image quality assessment (IQA) method to evaluate and explain the quality of visual content. However, these advancements are mostly focused on overall quality assessment, and the detailed examination of local quality, which is crucial for comprehensive visual understanding, is still largely unexplored. In this work, we introduce Q-Ground, the first framework aimed at tackling fine-scale visual quality groun",
    "arxiv_url": "https://arxiv.org/abs/2407.17035v1",
    "pdf_url": "https://arxiv.org/pdf/2407.17035v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2407.17035",
    "arxiv_authors": [
      "Chaofeng Chen",
      "Sensen Yang",
      "Haoning Wu",
      "Liang Liao",
      "Zicheng Zhang",
      "Annan Wang",
      "Wenxiu Sun",
      "Qiong Yan",
      "Weisi Lin"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Q-Ground%3A+Image+Quality+Grounding+with+Large+Multi-modality+Models+Chaofeng+Chen+Sensen+Yang+Haoning+Wu+Liang+Liao+Zicheng+Zhang",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "C Chen",
        "id": "lxiqnI0AAAAJ"
      },
      {
        "name": "S Yang",
        "id": null
      },
      {
        "name": "H Wu",
        "id": "wth-VbMAAAAJ"
      },
      {
        "name": "L Liao",
        "id": "kqTUHSIAAAAJ"
      },
      {
        "name": "Z Zhang",
        "id": "QICTEckAAAAJ"
      },
      {
        "name": "A Wang",
        "id": "vZn0gL8AAAAJ"
      },
      {
        "name": "W Sun",
        "id": "X9lE6O4AAAAJ"
      },
      {
        "name": "Q Yan",
        "id": "uT9CtPYAAAAJ"
      },
      {
        "name": "W Lin",
        "id": "D_S41X4AAAAJ"
      }
    ],
    "citation_count": 26
  },
  {
    "arxiv_id": "2311.15751",
    "title": "PyNanospacing: TEM image processing tool for strain analysis and visualization",
    "year": 2023,
    "published": "2023-11-27T12:08:46Z",
    "categories": [
      "cond-mat.mtrl-sci",
      "cs.CV"
    ],
    "abstract": "The diverse spectrum of material characteristics including band gap, mechanical moduli, color, phonon and electronic density of states, along with catalytic and surface properties are intricately intertwined with the atomic structure and the corresponding interatomic bond-lengths. This interconnection extends to the manifestation of interplanar spacings within a crystalline lattice. Analysis of these interplanar spacings and the comprehension of any deviations, whether it be lattice compression ",
    "arxiv_url": "https://arxiv.org/abs/2311.15751v1",
    "pdf_url": "https://arxiv.org/pdf/2311.15751v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2311.15751",
    "arxiv_authors": [
      "Mehmet Ali Sarsil",
      "Mubashir Mansoor",
      "Mert Saracoglu",
      "Servet Timur",
      "Mustafa Urgen",
      "Onur Ergen"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=PyNanospacing%3A+TEM+image+processing+tool+for+strain+analysis+and+visualization+Mehmet+Ali+Sarsil+Mubashir+Mansoor+Mert+Saracoglu+Servet+Timur+Mustafa+Urgen",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "MA Sarsil",
        "id": "Qm2Pv3gAAAAJ"
      },
      {
        "name": "M Mansoor",
        "id": "LsEUpaUAAAAJ"
      },
      {
        "name": "M Saracoglu",
        "id": "X1ea7owAAAAJ"
      },
      {
        "name": "S Timur",
        "id": "56GefAsAAAAJ"
      },
      {
        "name": "M Urgen",
        "id": "3TEIpuUAAAAJ"
      },
      {
        "name": "O Ergen",
        "id": "NR5OmGQAAAAJ"
      }
    ],
    "citation_count": null
  },
  {
    "arxiv_id": "2403.10931",
    "title": "Towards Collective Intelligence: Uncertainty-aware SAM Adaptation for Ambiguous Medical Image Segmentation",
    "year": 2024,
    "published": "2024-03-16T14:11:54Z",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "abstract": "Collective intelligence from multiple medical experts consistently surpasses individual expertise in clinical diagnosis, particularly for ambiguous medical image segmentation tasks involving unclear tissue boundaries or pathological variations. The Segment Anything Model (SAM), a powerful vision foundation model originally designed for natural image segmentation, has shown remarkable potential when adapted to medical image segmentation tasks. However, existing SAM adaptation methods follow a sin",
    "arxiv_url": "https://arxiv.org/abs/2403.10931v3",
    "pdf_url": "https://arxiv.org/pdf/2403.10931v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2403.10931",
    "arxiv_authors": [
      "Mingzhou Jiang",
      "Jiaying Zhou",
      "Junde Wu",
      "Tianyang Wang",
      "Yueming Jin",
      "Min Xu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Towards+Collective+Intelligence%3A+Uncertainty-aware+SAM+Adaptation+for+Ambiguous+Medical+Image+Segmentation+Mingzhou+Jiang+Jiaying+Zhou+Junde+Wu+Tianyang+Wang+Yueming+Jin",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "M Jiang",
        "id": null
      },
      {
        "name": "J Zhou",
        "id": "8vcWZJQAAAAJ"
      },
      {
        "name": "J Wu",
        "id": "FZSKG-AAAAAJ"
      },
      {
        "name": "T Wang",
        "id": "QbTV0r0AAAAJ"
      },
      {
        "name": "Y Jin",
        "id": "s_kbB4oAAAAJ"
      },
      {
        "name": "M Xu -",
        "id": null
      }
    ],
    "citation_count": 6
  },
  {
    "arxiv_id": "2407.14735",
    "title": "ECRTime: Ensemble Integration of Classification and Retrieval for Time Series Classification",
    "year": 2024,
    "published": "2024-07-20T03:17:23Z",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "abstract": "Deep learning-based methods for Time Series Classification (TSC) typically utilize deep networks to extract features, which are then processed through a combination of a Fully Connected (FC) layer and a SoftMax function. However, we have observed the phenomenon of inter-class similarity and intra-class inconsistency in the datasets from the UCR archive and further analyzed how this phenomenon adversely affects the \"FC+SoftMax\" paradigm. To address the issue, we introduce ECR, which, for the firs",
    "arxiv_url": "https://arxiv.org/abs/2407.14735v1",
    "pdf_url": "https://arxiv.org/pdf/2407.14735v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2407.14735",
    "arxiv_authors": [
      "Fan Zhao",
      "You Chen"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=ECRTime%3A+Ensemble+Integration+of+Classification+and+Retrieval+for+Time+Series+Classification+Fan+Zhao+You+Chen",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "F Zhao",
        "id": null
      },
      {
        "name": "Y Chen -",
        "id": null
      }
    ],
    "citation_count": null
  },
  {
    "arxiv_id": "2501.07711",
    "title": "Pedestrian Trajectory Prediction Based on Social Interactions Learning With Random Weights",
    "year": 2025,
    "published": "2025-01-13T21:45:01Z",
    "categories": [
      "cs.CV",
      "cs.MM"
    ],
    "abstract": "Pedestrian trajectory prediction is a critical technology in the evolution of self-driving cars toward complete artificial intelligence. Over recent years, focusing on the trajectories of pedestrians to model their social interactions has surged with great interest in more accurate trajectory predictions. However, existing methods for modeling pedestrian social interactions rely on pre-defined rules, struggling to capture non-explicit social interactions. In this work, we propose a novel framewo",
    "arxiv_url": "https://arxiv.org/abs/2501.07711v1",
    "pdf_url": "https://arxiv.org/pdf/2501.07711v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2501.07711",
    "arxiv_authors": [
      "Jiajia Xie",
      "Sheng Zhang",
      "Beihao Xia",
      "Zhu Xiao",
      "Hongbo Jiang",
      "Siwang Zhou",
      "Zheng Qin",
      "Hongyang Chen"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Pedestrian+Trajectory+Prediction+Based+on+Social+Interactions+Learning+With+Random+Weights+Jiajia+Xie+Sheng+Zhang+Beihao+Xia+Zhu+Xiao+Hongbo+Jiang",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "J Xie",
        "id": "9GBN3bYAAAAJ"
      },
      {
        "name": "S Zhang",
        "id": "axYcwt4AAAAJ"
      },
      {
        "name": "B Xia",
        "id": "Lx_iqH8AAAAJ"
      },
      {
        "name": "Z Xiao",
        "id": "8SVEEHYAAAAJ"
      },
      {
        "name": "H Jiang",
        "id": "m-bu8JgAAAAJ"
      },
      {
        "name": "S Zhou",
        "id": "PQPUL7cAAAAJ"
      },
      {
        "name": "Z Qin",
        "id": "IfSLVLMAAAAJ"
      },
      {
        "name": "H ChenIEEE Transactions on Multimedia",
        "id": null
      }
    ],
    "citation_count": 23
  },
  {
    "arxiv_id": "2310.06836",
    "title": "A General Protocol to Probe Large Vision Models for 3D Physical Understanding",
    "year": 2023,
    "published": "2023-10-10T17:59:28Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Our objective in this paper is to probe large vision models to determine to what extent they 'understand' different physical properties of the 3D scene depicted in an image. To this end, we make the following contributions: (i) We introduce a general and lightweight protocol to evaluate whether features of an off-the-shelf large vision model encode a number of physical 'properties' of the 3D scene, by training discriminative classifiers on the features for these properties. The probes are applie",
    "arxiv_url": "https://arxiv.org/abs/2310.06836v3",
    "pdf_url": "https://arxiv.org/pdf/2310.06836v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2310.06836",
    "arxiv_authors": [
      "Guanqi Zhan",
      "Chuanxia Zheng",
      "Weidi Xie",
      "Andrew Zisserman"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=A+General+Protocol+to+Probe+Large+Vision+Models+for+3D+Physical+Understanding+Guanqi+Zhan+Chuanxia+Zheng+Weidi+Xie+Andrew+Zisserman",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "G Zhan",
        "id": "f_m4WJIAAAAJ"
      },
      {
        "name": "C Zheng",
        "id": "mvpE6bIAAAAJ"
      },
      {
        "name": "W Xie",
        "id": "Vtrqj4gAAAAJ"
      },
      {
        "name": "A ZissermanAdvances in Neural Information Processing Systems",
        "id": null
      }
    ],
    "citation_count": 13
  },
  {
    "arxiv_id": "2407.09550",
    "title": "CAPM: Fast and Robust Verification on Maxpool-based CNN via Dual Network",
    "year": 2024,
    "published": "2024-06-27T14:43:06Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "abstract": "This study uses CAPM (Convex Adversarial Polytope for Maxpool-based CNN) to improve the verified bound for general purpose maxpool-based convolutional neural networks (CNNs) under bounded norm adversarial perturbations. The maxpool function is decomposed as a series of ReLU functions to extend the convex relaxation technique to maxpool functions, by which the verified bound can be efficiently computed through a dual network. The experimental results demonstrate that this technique allows the sta",
    "arxiv_url": "https://arxiv.org/abs/2407.09550v3",
    "pdf_url": "https://arxiv.org/pdf/2407.09550v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2407.09550",
    "arxiv_authors": [
      "Jia-Hau Bai",
      "Chi-Ting Liu",
      "Yu Wang",
      "Fu-Chieh Chang",
      "Pei-Yuan Wu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=CAPM%3A+Fast+and+Robust+Verification+on+Maxpool-based+CNN+via+Dual+Network+Jia-Hau+Bai+Chi-Ting+Liu+Yu+Wang+Fu-Chieh+Chang+Pei-Yuan+Wu",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "JH Bai",
        "id": null
      },
      {
        "name": "CT Liu",
        "id": "mK7iwcQAAAAJ"
      },
      {
        "name": "Y Wang",
        "id": null
      },
      {
        "name": "FC Chang",
        "id": "UUdfw9wAAAAJ"
      },
      {
        "name": "PY Wu -",
        "id": null
      }
    ],
    "citation_count": 1
  },
  {
    "arxiv_id": "2311.16512",
    "title": "CoSeR: Bridging Image and Language for Cognitive Super-Resolution",
    "year": 2023,
    "published": "2023-11-27T16:33:29Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Existing super-resolution (SR) models primarily focus on restoring local texture details, often neglecting the global semantic information within the scene. This oversight can lead to the omission of crucial semantic details or the introduction of inaccurate textures during the recovery process. In our work, we introduce the Cognitive Super-Resolution (CoSeR) framework, empowering SR models with the capacity to comprehend low-resolution images. We achieve this by marrying image appearance and la",
    "arxiv_url": "https://arxiv.org/abs/2311.16512v4",
    "pdf_url": "https://arxiv.org/pdf/2311.16512v4",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2311.16512",
    "arxiv_authors": [
      "Haoze Sun",
      "Wenbo Li",
      "Jianzhuang Liu",
      "Haoyu Chen",
      "Renjing Pei",
      "Xueyi Zou",
      "Youliang Yan",
      "Yujiu Yang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=CoSeR%3A+Bridging+Image+and+Language+for+Cognitive+Super-Resolution+Haoze+Sun+Wenbo+Li+Jianzhuang+Liu+Haoyu+Chen+Renjing+Pei",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "H Sun",
        "id": "aB2KirIAAAAJ"
      },
      {
        "name": "W Li",
        "id": "foGn_TIAAAAJ"
      },
      {
        "name": "J Liu",
        "id": "sKauaAwAAAAJ"
      },
      {
        "name": "H Chen",
        "id": "KWbcBucAAAAJ"
      },
      {
        "name": "R Pei",
        "id": null
      },
      {
        "name": "X Zou",
        "id": "0ua28KoAAAAJ"
      },
      {
        "name": "Y Yan",
        "id": "JPUwfAMAAAAJ"
      },
      {
        "name": "Y Yang",
        "id": "4gH3sxsAAAAJ"
      }
    ],
    "citation_count": 81
  },
  {
    "arxiv_id": "2501.08900",
    "title": "Enhanced Multi-Scale Cross-Attention for Person Image Generation",
    "year": 2025,
    "published": "2025-01-15T16:08:25Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "In this paper, we propose a novel cross-attention-based generative adversarial network (GAN) for the challenging person image generation task. Cross-attention is a novel and intuitive multi-modal fusion method in which an attention/correlation matrix is calculated between two feature maps of different modalities. Specifically, we propose the novel XingGAN (or CrossingGAN), which consists of two generation branches that capture the person's appearance and shape, respectively. Moreover, we propose",
    "arxiv_url": "https://arxiv.org/abs/2501.08900v1",
    "pdf_url": "https://arxiv.org/pdf/2501.08900v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2501.08900",
    "arxiv_authors": [
      "Hao Tang",
      "Ling Shao",
      "Nicu Sebe",
      "Luc Van Gool"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Enhanced+Multi-Scale+Cross-Attention+for+Person+Image+Generation+Hao+Tang+Ling+Shao+Nicu+Sebe+Luc+Van+Gool",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "H Tang",
        "id": "9zJkeEMAAAAJ"
      },
      {
        "name": "L Shao",
        "id": "z84rLjoAAAAJ"
      },
      {
        "name": "N Sebe",
        "id": "stFCYOAAAAAJ"
      },
      {
        "name": "L Van GoolIEEE Transactions on Pattern Analysis and Machine Intelligence",
        "id": null
      }
    ],
    "citation_count": 3
  },
  {
    "arxiv_id": "2302.01721",
    "title": "TEXTure: Text-Guided Texturing of 3D Shapes",
    "year": 2023,
    "published": "2023-02-03T13:18:45Z",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "abstract": "In this paper, we present TEXTure, a novel method for text-guided generation, editing, and transfer of textures for 3D shapes. Leveraging a pretrained depth-to-image diffusion model, TEXTure applies an iterative scheme that paints a 3D model from different viewpoints. Yet, while depth-to-image models can create plausible textures from a single viewpoint, the stochastic nature of the generation process can cause many inconsistencies when texturing an entire 3D object. To tackle these problems, we",
    "arxiv_url": "https://arxiv.org/abs/2302.01721v1",
    "pdf_url": "https://arxiv.org/pdf/2302.01721v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2302.01721",
    "arxiv_authors": [
      "Elad Richardson",
      "Gal Metzer",
      "Yuval Alaluf",
      "Raja Giryes",
      "Daniel Cohen-Or"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=TEXTure%3A+Text-Guided+Texturing+of+3D+Shapes+Elad+Richardson+Gal+Metzer+Yuval+Alaluf+Raja+Giryes+Daniel+Cohen-Or",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "E Richardson",
        "id": "9npMV2kAAAAJ"
      },
      {
        "name": "G Metzer",
        "id": "QVeYd9YAAAAJ"
      },
      {
        "name": "Y Alaluf",
        "id": "uvaPP80AAAAJ"
      },
      {
        "name": "R Giryes",
        "id": "9aQUYVQAAAAJ"
      },
      {
        "name": "D Cohen-OrACM SIGGRAPH",
        "id": null
      }
    ],
    "citation_count": 337
  },
  {
    "arxiv_id": "2406.14086",
    "title": "Seg-LSTM: Performance of xLSTM for Semantic Segmentation of Remotely Sensed Images",
    "year": 2024,
    "published": "2024-06-20T08:01:28Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "abstract": "Recent advancements in autoregressive networks with linear complexity have driven significant research progress, demonstrating exceptional performance in large language models. A representative model is the Extended Long Short-Term Memory (xLSTM), which incorporates gating mechanisms and memory structures, performing comparably to Transformer architectures in long-sequence language tasks. Autoregressive networks such as xLSTM can utilize image serialization to extend their application to visual ",
    "arxiv_url": "https://arxiv.org/abs/2406.14086v1",
    "pdf_url": "https://arxiv.org/pdf/2406.14086v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2406.14086",
    "arxiv_authors": [
      "Qinfeng Zhu",
      "Yuanzhi Cai",
      "Lei Fan"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Seg-LSTM%3A+Performance+of+xLSTM+for+Semantic+Segmentation+of+Remotely+Sensed+Images+Qinfeng+Zhu+Yuanzhi+Cai+Lei+Fan",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "Q Zhu",
        "id": "gG81gcUAAAAJ"
      },
      {
        "name": "Y Cai",
        "id": "4JaOK1sAAAAJ"
      },
      {
        "name": "L Fan -",
        "id": null
      }
    ],
    "citation_count": 13
  },
  {
    "arxiv_id": "2311.04052",
    "title": "Generative AIBIM: An automatic and intelligent structural design pipeline integrating BIM and generative AI",
    "year": 2023,
    "published": "2023-11-07T15:05:19Z",
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "abstract": "AI-based structural design represents a transformative approach that addresses the inefficiencies inherent in traditional structural design practices. This paper innovates the existing AI-based design frameworks from four aspects and proposes Generative AIBIM: an intelligent design pipeline that integrates BIM and generative AI. First, the proposed pipeline not only broadens the application scope of BIM, which aligns with BIM's growing relevance in civil engineering, but also marks a significant",
    "arxiv_url": "https://arxiv.org/abs/2311.04052v2",
    "pdf_url": "https://arxiv.org/pdf/2311.04052v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2311.04052",
    "arxiv_authors": [
      "Zhili He",
      "Yu-Hsing Wang",
      "Jian Zhang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Generative+AIBIM%3A+An+automatic+and+intelligent+structural+design+pipeline+integrating+BIM+and+generative+AI+Zhili+He+Yu-Hsing+Wang+Jian+Zhang",
    "gs_search_success": false,
    "gs_authors": null,
    "citation_count": null,
    "error_type": "multiple_matches"
  },
  {
    "arxiv_id": "2406.05757",
    "title": "Vision Mamba: Cutting-Edge Classification of Alzheimer's Disease with 3D MRI Scans",
    "year": 2024,
    "published": "2024-06-09T12:23:22Z",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "Classifying 3D MRI images for early detection of Alzheimer's disease is a critical task in medical imaging. Traditional approaches using Convolutional Neural Networks (CNNs) and Transformers face significant challenges in this domain. CNNs, while effective in capturing local spatial features, struggle with long-range dependencies and often require extensive computational resources for high-resolution 3D data. Transformers, on the other hand, excel in capturing global context but suffer from quad",
    "arxiv_url": "https://arxiv.org/abs/2406.05757v1",
    "pdf_url": "https://arxiv.org/pdf/2406.05757v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2406.05757",
    "arxiv_authors": [
      "Muthukumar K A",
      "Amit Gurung",
      "Priya Ranjan"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Vision+Mamba%3A+Cutting-Edge+Classification+of+Alzheimer%27s+Disease+with+3D+MRI+Scans+Muthukumar+K+A+Amit+Gurung+Priya+Ranjan",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "A Gurung",
        "id": "gDyKuzYAAAAJ"
      },
      {
        "name": "P Ranjan -",
        "id": null
      }
    ],
    "citation_count": 4
  },
  {
    "arxiv_id": "2402.14098",
    "title": "Intriguing Properties of Modern GANs",
    "year": 2024,
    "published": "2024-02-21T19:48:11Z",
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "abstract": "Modern GANs achieve remarkable performance in terms of generating realistic and diverse samples. This has led many to believe that ``GANs capture the training data manifold''. In this work we show that this interpretation is wrong. We empirically show that the manifold learned by modern GANs does not fit the training distribution: specifically the manifold does not pass through the training examples and passes closer to out-of-distribution images than to in-distribution images. We also investiga",
    "arxiv_url": "https://arxiv.org/abs/2402.14098v1",
    "pdf_url": "https://arxiv.org/pdf/2402.14098v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2402.14098",
    "arxiv_authors": [
      "Roy Friedman",
      "Yair Weiss"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Intriguing+Properties+of+Modern+GANs+Roy+Friedman+Yair+Weiss",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "R Friedman",
        "id": "qhJgyt4AAAAJ"
      },
      {
        "name": "Y Weiss -",
        "id": null
      }
    ],
    "citation_count": null
  },
  {
    "arxiv_id": "2404.11236",
    "title": "ONOT: a High-Quality ICAO-compliant Synthetic Mugshot Dataset",
    "year": 2024,
    "published": "2024-04-17T10:38:51Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Nowadays, state-of-the-art AI-based generative models represent a viable solution to overcome privacy issues and biases in the collection of datasets containing personal information, such as faces. Following this intuition, in this paper we introduce ONOT, a synthetic dataset specifically focused on the generation of high-quality faces in adherence to the requirements of the ISO/IEC 39794-5 standards that, following the guidelines of the International Civil Aviation Organization (ICAO), defines ",
    "arxiv_url": "https://arxiv.org/abs/2404.11236v1",
    "pdf_url": "https://arxiv.org/pdf/2404.11236v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2404.11236",
    "arxiv_authors": [
      "Nicol√≤ Di Domenico",
      "Guido Borghi",
      "Annalisa Franco",
      "Davide Maltoni"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=ONOT%3A+a+High-Quality+ICAO-compliant+Synthetic+Mugshot+Dataset+Nicol%C3%B2+Di+Domenico+Guido+Borghi+Annalisa+Franco+Davide+Maltoni",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "N Di Domenico",
        "id": "DEPDfiAAAAAJ"
      },
      {
        "name": "G Borghi",
        "id": "81ovlBIAAAAJ"
      },
      {
        "name": "A Franco",
        "id": "thns9agAAAAJ"
      },
      {
        "name": "D Maltoni2024 IEEE 18th International",
        "id": null
      }
    ],
    "citation_count": 16
  },
  {
    "arxiv_id": "2304.11375",
    "title": "Unsupervised CD in satellite image time series by contrastive learning and feature tracking",
    "year": 2023,
    "published": "2023-04-22T11:19:19Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "While unsupervised change detection using contrastive learning has been significantly improved the performance of literature techniques, at present, it only focuses on the bi-temporal change detection scenario. Previous state-of-the-art models for image time-series change detection often use features obtained by learning for clustering or training a model from scratch using pseudo labels tailored to each scene. However, these approaches fail to exploit the spatial-temporal information of image t",
    "arxiv_url": "https://arxiv.org/abs/2304.11375v1",
    "pdf_url": "https://arxiv.org/pdf/2304.11375v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2304.11375",
    "arxiv_authors": [
      "Yuxing Chen",
      "Lorenzo Bruzzone"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Unsupervised+CD+in+satellite+image+time+series+by+contrastive+learning+and+feature+tracking+Yuxing+Chen+Lorenzo+Bruzzone",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "Y Chen",
        "id": "U_UBuVUAAAAJ"
      },
      {
        "name": "L Bruzzone - IEEE Transactions on Geoscience and",
        "id": null
      }
    ],
    "citation_count": 9
  },
  {
    "arxiv_id": "2502.16493",
    "title": "Trunk-branch Contrastive Network with Multi-view Deformable Aggregation for Multi-view Action Recognition",
    "year": 2025,
    "published": "2025-02-23T08:10:20Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Multi-view action recognition aims to identify actions in a given multi-view scene. Traditional studies initially extracted refined features from each view, followed by implemented paired interaction and integration, but they potentially overlooked the critical local features in each view. When observing objects from multiple perspectives, individuals typically form a comprehensive impression and subsequently fill in specific details. Drawing inspiration from this cognitive process, we propose a",
    "arxiv_url": "https://arxiv.org/abs/2502.16493v1",
    "pdf_url": "https://arxiv.org/pdf/2502.16493v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2502.16493",
    "arxiv_authors": [
      "Yingyuan Yang",
      "Guoyuan Liang",
      "Can Wang",
      "Xiaojun Wu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Trunk-branch+Contrastive+Network+with+Multi-view+Deformable+Aggregation+for+Multi-view+Action+Recognition+Yingyuan+Yang+Guoyuan+Liang+Can+Wang+Xiaojun+Wu",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "Y Yang",
        "id": "JvZ_r8EAAAAJ"
      },
      {
        "name": "G Liang",
        "id": "KaoieMMAAAAJ"
      },
      {
        "name": "C Wang",
        "id": null
      },
      {
        "name": "X Wu - Pattern Recognition",
        "id": null
      }
    ],
    "citation_count": 1
  },
  {
    "arxiv_id": "2306.00003",
    "title": "Detecting Heart Disease from Multi-View Ultrasound Images via Supervised Attention Multiple Instance Learning",
    "year": 2023,
    "published": "2023-05-25T18:22:12Z",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "Aortic stenosis (AS) is a degenerative valve condition that causes substantial morbidity and mortality. This condition is under-diagnosed and under-treated. In clinical practice, AS is diagnosed with expert review of transthoracic echocardiography, which produces dozens of ultrasound images of the heart. Only some of these views show the aortic valve. To automate screening for AS, deep networks must learn to mimic a human expert's ability to identify views of the aortic valve then aggregate acro",
    "arxiv_url": "https://arxiv.org/abs/2306.00003v3",
    "pdf_url": "https://arxiv.org/pdf/2306.00003v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2306.00003",
    "arxiv_authors": [
      "Zhe Huang",
      "Benjamin S. Wessler",
      "Michael C. Hughes"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Detecting+Heart+Disease+from+Multi-View+Ultrasound+Images+via+Supervised+Attention+Multiple+Instance+Learning+Zhe+Huang+Benjamin+S.+Wessler+Michael+C.+Hughes",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "Z Huang",
        "id": "txh41pQAAAAJ"
      },
      {
        "name": "BS Wessler",
        "id": "oAv0uvYAAAAJ"
      },
      {
        "name": "MC HughesMachine Learning for Healthcare",
        "id": null
      }
    ],
    "citation_count": 14
  },
  {
    "arxiv_id": "2404.06389",
    "title": "Raster Forge: Interactive Raster Manipulation Library and GUI for Python",
    "year": 2024,
    "published": "2024-04-09T15:31:48Z",
    "categories": [
      "eess.IV",
      "cs.CV",
      "cs.CY",
      "cs.MS"
    ],
    "abstract": "Raster Forge is a Python library and graphical user interface for raster data manipulation and analysis. The tool is focused on remote sensing applications, particularly in wildfire management. It allows users to import, visualize, and process raster layers for tasks such as image compositing or topographical analysis. For wildfire management, it generates fuel maps using predefined models. Its impact extends from disaster management to hydrological modeling, agriculture, and environmental monit",
    "arxiv_url": "https://arxiv.org/abs/2404.06389v2",
    "pdf_url": "https://arxiv.org/pdf/2404.06389v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2404.06389",
    "arxiv_authors": [
      "Afonso Oliveira",
      "Nuno Fachada",
      "Jo√£o P. Matos-Carvalho"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Raster+Forge%3A+Interactive+Raster+Manipulation+Library+and+GUI+for+Python+Afonso+Oliveira+Nuno+Fachada+Jo%C3%A3o+P.+Matos-Carvalho",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "A Oliveira",
        "id": "OlTBFHUAAAAJ"
      },
      {
        "name": "N Fachada",
        "id": "MaZgRdIAAAAJ"
      },
      {
        "name": "JP Matos-Carvalho - Software Impacts",
        "id": null
      }
    ],
    "citation_count": 2
  },
  {
    "arxiv_id": "2412.05337",
    "title": "ACT-Bench: Towards Action Controllable World Models for Autonomous Driving",
    "year": 2024,
    "published": "2024-12-06T01:06:28Z",
    "categories": [
      "cs.CV",
      "cs.LG",
      "cs.RO"
    ],
    "abstract": "World models have emerged as promising neural simulators for autonomous driving, with the potential to supplement scarce real-world data and enable closed-loop evaluations. However, current research primarily evaluates these models based on visual realism or downstream task performance, with limited focus on fidelity to specific action instructions - a crucial property for generating targeted simulation scenes. Although some studies address action fidelity, their evaluations rely on closed-sourc",
    "arxiv_url": "https://arxiv.org/abs/2412.05337v1",
    "pdf_url": "https://arxiv.org/pdf/2412.05337v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2412.05337",
    "arxiv_authors": [
      "Hidehisa Arai",
      "Keishi Ishihara",
      "Tsubasa Takahashi",
      "Yu Yamaguchi"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=ACT-Bench%3A+Towards+Action+Controllable+World+Models+for+Autonomous+Driving+Hidehisa+Arai+Keishi+Ishihara+Tsubasa+Takahashi+Yu+Yamaguchi",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "H Arai",
        "id": "i0-55k8AAAAJ"
      },
      {
        "name": "K Ishihara",
        "id": "hqJEn9AAAAAJ"
      },
      {
        "name": "T Takahashi",
        "id": "s-jrZ94AAAAJ"
      },
      {
        "name": "Y Yamaguchi",
        "id": "ITN0TLcAAAAJ"
      }
    ],
    "citation_count": 6
  },
  {
    "arxiv_id": "2502.10214",
    "title": "Mapping bathymetry of inland water bodies on the North Slope of Alaska with Landsat using Random Forest",
    "year": 2025,
    "published": "2025-02-14T15:08:37Z",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "The North Slope of Alaska is dominated by small waterbodies that provide critical ecosystem services for local population and wildlife. Detailed information on the depth of the waterbodies is scarce due to the challenges with collecting such information. In this work we have trained a machine learning (Random Forest Regressor) model to predict depth from multispectral Landsat data in waterbodies across the North Slope of Alaska. The greatest challenge is the scarcity of in situ data, which is ex",
    "arxiv_url": "https://arxiv.org/abs/2502.10214v1",
    "pdf_url": "https://arxiv.org/pdf/2502.10214v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2502.10214",
    "arxiv_authors": [
      "Mark L. Carroll",
      "Margaret R. Wooten",
      "Claire E. Simpson",
      "Caleb S. Spradlin",
      "Melanie J. Frost",
      "Mariana Blanco-Rojas",
      "Zachary W. Williams",
      "Jordan A. Caraballo-Vega",
      "Christopher S. R. Neigh"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Mapping+bathymetry+of+inland+water+bodies+on+the+North+Slope+of+Alaska+with+Landsat+using+Random+Forest+Mark+L.+Carroll+Margaret+R.+Wooten+Claire+E.+Simpson+Caleb+S.+Spradlin+Melanie+J.+Frost",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "ML Carroll",
        "id": "Hnp-SlQAAAAJ"
      },
      {
        "name": "MR Wooten",
        "id": "G8rnHfsAAAAJ"
      },
      {
        "name": "CE Simpson",
        "id": "GwB_5o8AAAAJ"
      },
      {
        "name": "CS Spradlin",
        "id": "_V5HfIIAAAAJ"
      },
      {
        "name": "MJ Frost",
        "id": "x0K_RkUAAAAJ"
      },
      {
        "name": "M Blanco-Rojas",
        "id": "9Vqi2cQAAAAJ"
      },
      {
        "name": "ZW Williams",
        "id": null
      }
    ],
    "citation_count": 1
  },
  {
    "arxiv_id": "2503.02117",
    "title": "Parabolic Continual Learning",
    "year": 2025,
    "published": "2025-03-03T22:59:13Z",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "abstract": "Regularizing continual learning techniques is important for anticipating algorithmic behavior under new realizations of data. We introduce a new approach to continual learning by imposing the properties of a parabolic partial differential equation (PDE) to regularize the expected behavior of the loss over time. This class of parabolic PDEs has a number of favorable properties that allow us to analyze the error incurred through forgetting and the error induced through generalization. Specifically",
    "arxiv_url": "https://arxiv.org/abs/2503.02117v1",
    "pdf_url": "https://arxiv.org/pdf/2503.02117v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2503.02117",
    "arxiv_authors": [
      "Haoming Yang",
      "Ali Hasan",
      "Vahid Tarokh"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Parabolic+Continual+Learning+Haoming+Yang+Ali+Hasan+Vahid+Tarokh",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "H Yang",
        "id": "uz7goREAAAAJ"
      },
      {
        "name": "A Hasan",
        "id": "4De_LnYAAAAJ"
      },
      {
        "name": "V Tarokh -",
        "id": null
      }
    ],
    "citation_count": 1
  },
  {
    "arxiv_id": "2308.04771",
    "title": "SUnAA: Sparse Unmixing using Archetypal Analysis",
    "year": 2023,
    "published": "2023-08-09T07:58:33Z",
    "categories": [
      "cs.CV",
      "cs.LG",
      "eess.IV"
    ],
    "abstract": "This paper introduces a new sparse unmixing technique using archetypal analysis (SUnAA). First, we design a new model based on archetypal analysis. We assume that the endmembers of interest are a convex combination of endmembers provided by a spectral library and that the number of endmembers of interest is known. Then, we propose a minimization problem. Unlike most conventional sparse unmixing methods, here the minimization problem is non-convex. We minimize the optimization objective iterative",
    "arxiv_url": "https://arxiv.org/abs/2308.04771v1",
    "pdf_url": "https://arxiv.org/pdf/2308.04771v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2308.04771",
    "arxiv_authors": [
      "Behnood Rasti",
      "Alexandre Zouaoui",
      "Julien Mairal",
      "Jocelyn Chanussot"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=SUnAA%3A+Sparse+Unmixing+using+Archetypal+Analysis+Behnood+Rasti+Alexandre+Zouaoui+Julien+Mairal+Jocelyn+Chanussot",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "B Rasti",
        "id": "hA_Xi6MAAAAJ"
      },
      {
        "name": "A Zouaoui",
        "id": "k0fFobUAAAAJ"
      },
      {
        "name": "J Mairal",
        "id": "Bx9WGD6lBFEC"
      },
      {
        "name": "J ChanussotIEEE Geoscience and Remote Sensing Letters",
        "id": null
      }
    ],
    "citation_count": 7
  },
  {
    "arxiv_id": "2501.19035",
    "title": "SynthmanticLiDAR: A Synthetic Dataset for Semantic Segmentation on LiDAR Imaging",
    "year": 2025,
    "published": "2025-01-31T11:09:10Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Semantic segmentation on LiDAR imaging is increasingly gaining attention, as it can provide useful knowledge for perception systems and potential for autonomous driving. However, collecting and labeling real LiDAR data is an expensive and time-consuming task. While datasets such as SemanticKITTI have been manually collected and labeled, the introduction of simulation tools such as CARLA, has enabled the creation of synthetic datasets on demand.   In this work, we present a modified CARLA simulat",
    "arxiv_url": "https://arxiv.org/abs/2501.19035v1",
    "pdf_url": "https://arxiv.org/pdf/2501.19035v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2501.19035",
    "arxiv_authors": [
      "Javier Montalvo",
      "Pablo Carballeira",
      "√Ålvaro Garc√≠a-Mart√≠n"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=SynthmanticLiDAR%3A+A+Synthetic+Dataset+for+Semantic+Segmentation+on+LiDAR+Imaging+Javier+Montalvo+Pablo+Carballeira+%C3%81lvaro+Garc%C3%ADa-Mart%C3%ADn",
    "gs_search_success": false,
    "gs_authors": null,
    "citation_count": null,
    "error_type": "multiple_matches"
  },
  {
    "arxiv_id": "2501.09167",
    "title": "Embodied Scene Understanding for Vision Language Models via MetaVQA",
    "year": 2025,
    "published": "2025-01-15T21:36:19Z",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "abstract": "Vision Language Models (VLMs) demonstrate significant potential as embodied AI agents for various mobility applications. However, a standardized, closed-loop benchmark for evaluating their spatial reasoning and sequential decision-making capabilities is lacking. To address this, we present MetaVQA: a comprehensive benchmark designed to assess and enhance VLMs' understanding of spatial relationships and scene dynamics through Visual Question Answering (VQA) and closed-loop simulations. MetaVQA le",
    "arxiv_url": "https://arxiv.org/abs/2501.09167v1",
    "pdf_url": "https://arxiv.org/pdf/2501.09167v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2501.09167",
    "arxiv_authors": [
      "Weizhen Wang",
      "Chenda Duan",
      "Zhenghao Peng",
      "Yuxin Liu",
      "Bolei Zhou"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Embodied+Scene+Understanding+for+Vision+Language+Models+via+MetaVQA+Weizhen+Wang+Chenda+Duan+Zhenghao+Peng+Yuxin+Liu+Bolei+Zhou",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "W Wang",
        "id": "BepoGRMAAAAJ"
      },
      {
        "name": "C Duan",
        "id": "DooYOyoAAAAJ"
      },
      {
        "name": "Z Peng",
        "id": "JZ8ws6IAAAAJ"
      },
      {
        "name": "Y Liu",
        "id": "ZQoOjaIAAAAJ"
      },
      {
        "name": "B Zhou",
        "id": "9D4aG8AAAAAJ"
      }
    ],
    "citation_count": 10
  },
  {
    "arxiv_id": "2505.18060",
    "title": "Semantic Correspondence: Unified Benchmarking and a Strong Baseline",
    "year": 2025,
    "published": "2025-05-23T16:07:16Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Establishing semantic correspondence is a challenging task in computer vision, aiming to match keypoints with the same semantic information across different images. Benefiting from the rapid development of deep learning, remarkable progress has been made over the past decade. However, a comprehensive review and analysis of this task remains absent. In this paper, we present the first extensive survey of semantic correspondence methods. We first propose a taxonomy to classify existing methods bas",
    "arxiv_url": "https://arxiv.org/abs/2505.18060v3",
    "pdf_url": "https://arxiv.org/pdf/2505.18060v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2505.18060",
    "arxiv_authors": [
      "Kaiyan Zhang",
      "Xinghui Li",
      "Jingyi Lu",
      "Kai Han"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Semantic+Correspondence%3A+Unified+Benchmarking+and+a+Strong+Baseline+Kaiyan+Zhang+Xinghui+Li+Jingyi+Lu+Kai+Han",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "K Zhang",
        "id": "ef255KYAAAAJ"
      },
      {
        "name": "X Li",
        "id": "XLlgbBoAAAAJ"
      },
      {
        "name": "J Lu",
        "id": null
      },
      {
        "name": "K Han -",
        "id": null
      }
    ],
    "citation_count": 3
  },
  {
    "arxiv_id": "2409.10921",
    "title": "KALE: An Artwork Image Captioning System Augmented with Heterogeneous Graph",
    "year": 2024,
    "published": "2024-09-17T06:39:18Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Exploring the narratives conveyed by fine-art paintings is a challenge in image captioning, where the goal is to generate descriptions that not only precisely represent the visual content but also offer a in-depth interpretation of the artwork's meaning. The task is particularly complex for artwork images due to their diverse interpretations and varied aesthetic principles across different artistic schools and styles. In response to this, we present KALE Knowledge-Augmented vision-Language model",
    "arxiv_url": "https://arxiv.org/abs/2409.10921v1",
    "pdf_url": "https://arxiv.org/pdf/2409.10921v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2409.10921",
    "arxiv_authors": [
      "Yanbei Jiang",
      "Krista A. Ehinger",
      "Jey Han Lau"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=KALE%3A+An+Artwork+Image+Captioning+System+Augmented+with+Heterogeneous+Graph+Yanbei+Jiang+Krista+A.+Ehinger+Jey+Han+Lau",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "Y Jiang",
        "id": "-JHMVhQAAAAJ"
      },
      {
        "name": "KA Ehinger",
        "id": "EdGfpdcAAAAJ"
      },
      {
        "name": "JH Lau -",
        "id": null
      }
    ],
    "citation_count": 8
  },
  {
    "arxiv_id": "2412.00091",
    "title": "Graph Canvas for Controllable 3D Scene Generation",
    "year": 2024,
    "published": "2024-11-27T12:41:23Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR"
    ],
    "abstract": "Spatial intelligence is foundational to AI systems that interact with the physical world, particularly in 3D scene generation and spatial comprehension. Current methodologies for 3D scene generation often rely heavily on predefined datasets, and struggle to adapt dynamically to changing spatial relationships. In this paper, we introduce GraphCanvas3D, a programmable, extensible, and adaptable framework for controllable 3D scene generation. Leveraging in-context learning, GraphCanvas3D enables dy",
    "arxiv_url": "https://arxiv.org/abs/2412.00091v2",
    "pdf_url": "https://arxiv.org/pdf/2412.00091v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2412.00091",
    "arxiv_authors": [
      "Libin Liu",
      "Shen Chen",
      "Sen Jia",
      "Jingzhe Shi",
      "Zhongyu Jiang",
      "Can Jin",
      "Wu Zongkai",
      "Jenq-Neng Hwang",
      "Lei Li"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Graph+Canvas+for+Controllable+3D+Scene+Generation+Libin+Liu+Shen+Chen+Sen+Jia+Jingzhe+Shi+Zhongyu+Jiang",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "L Liu",
        "id": null
      },
      {
        "name": "S Chen",
        "id": "IbztFUkAAAAJ"
      },
      {
        "name": "S Jia",
        "id": null
      },
      {
        "name": "J Shi",
        "id": "x_4IcIoAAAAJ"
      },
      {
        "name": "Z Jiang",
        "id": "RCCe8ZgAAAAJ"
      },
      {
        "name": "C Jin",
        "id": "RK-8dz0AAAAJ"
      },
      {
        "name": "W Zongkai",
        "id": null
      },
      {
        "name": "JN Hwang",
        "id": "4OAHy-kAAAAJ"
      },
      {
        "name": "L Li",
        "id": "DOyVxx0AAAAJ"
      }
    ],
    "citation_count": 8
  },
  {
    "arxiv_id": "2306.06360",
    "title": "3D reconstruction using Structure for Motion",
    "year": 2023,
    "published": "2023-06-10T06:39:28Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "cs.RO"
    ],
    "abstract": "We are working towards 3D reconstruction of indoor spaces using a pair of HDR cameras in a stereo vision configuration mounted on an indoor mobile floor robot that captures various textures and spatial features as 2D images and this data is simultaneously utilized as a feed to our algorithm which will allow us to visualize the depth map.",
    "arxiv_url": "https://arxiv.org/abs/2306.06360v1",
    "pdf_url": "https://arxiv.org/pdf/2306.06360v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2306.06360",
    "arxiv_authors": [
      "Kshitij Karnawat",
      "Hritvik Choudhari",
      "Abhimanyu Saxena",
      "Mudit Singal",
      "Raajith Gadam"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=3D+reconstruction+using+Structure+for+Motion+Kshitij+Karnawat+Hritvik+Choudhari+Abhimanyu+Saxena+Mudit+Singal+Raajith+Gadam",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "K Karnawat",
        "id": "fWxBiT4AAAAJ"
      },
      {
        "name": "H Choudhari",
        "id": null
      },
      {
        "name": "A Saxena",
        "id": null
      },
      {
        "name": "M Singal",
        "id": null
      },
      {
        "name": "R Gadam",
        "id": null
      }
    ],
    "citation_count": null
  },
  {
    "arxiv_id": "2411.04533",
    "title": "Neural Fingerprints for Adversarial Attack Detection",
    "year": 2024,
    "published": "2024-11-07T08:43:42Z",
    "categories": [
      "cs.CV",
      "cs.LG",
      "stat.AP"
    ],
    "abstract": "Deep learning models for image classification have become standard tools in recent years. A well known vulnerability of these models is their susceptibility to adversarial examples. These are generated by slightly altering an image of a certain class in a way that is imperceptible to humans but causes the model to classify it wrongly as another class. Many algorithms have been proposed to address this problem, falling generally into one of two categories: (i) building robust classifiers (ii) dir",
    "arxiv_url": "https://arxiv.org/abs/2411.04533v1",
    "pdf_url": "https://arxiv.org/pdf/2411.04533v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2411.04533",
    "arxiv_authors": [
      "Haim Fisher",
      "Moni Shahar",
      "Yehezkel S. Resheff"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Neural+Fingerprints+for+Adversarial+Attack+Detection+Haim+Fisher+Moni+Shahar+Yehezkel+S.+Resheff",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "H Fisher",
        "id": null
      },
      {
        "name": "M Shahar",
        "id": "BS_3BAgAAAAJ"
      },
      {
        "name": "YS Resheff -",
        "id": null
      }
    ],
    "citation_count": 1
  },
  {
    "arxiv_id": "2312.07052",
    "title": "Adjustable Robust Transformer for High Myopia Screening in Optical Coherence Tomography",
    "year": 2023,
    "published": "2023-12-12T08:08:39Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Myopia is a manifestation of visual impairment caused by an excessively elongated eyeball. Image data is critical material for studying high myopia and pathological myopia. Measurements of spherical equivalent and axial length are the gold standards for identifying high myopia, but the available image data for matching them is scarce. In addition, the criteria for defining high myopia vary from study to study, and therefore the inclusion of samples in automated screening efforts requires an appr",
    "arxiv_url": "https://arxiv.org/abs/2312.07052v1",
    "pdf_url": "https://arxiv.org/pdf/2312.07052v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2312.07052",
    "arxiv_authors": [
      "Xiao Ma",
      "Zetian Zhang",
      "Zexuan Ji",
      "Kun Huang",
      "Na Su",
      "Songtao Yuan",
      "Qiang Chen"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Adjustable+Robust+Transformer+for+High+Myopia+Screening+in+Optical+Coherence+Tomography+Xiao+Ma+Zetian+Zhang+Zexuan+Ji+Kun+Huang+Na+Su",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "X Ma",
        "id": null
      },
      {
        "name": "Z Zhang",
        "id": "4C5WoWgAAAAJ"
      },
      {
        "name": "Z Ji",
        "id": null
      },
      {
        "name": "K Huang",
        "id": "zbDWCSgAAAAJ"
      },
      {
        "name": "N Su",
        "id": null
      },
      {
        "name": "S Yuan",
        "id": null
      },
      {
        "name": "Q ChenInternational",
        "id": null
      }
    ],
    "citation_count": 2
  },
  {
    "arxiv_id": "2504.13580",
    "title": "Leveraging Automatic CAD Annotations for Supervised Learning in 3D Scene Understanding",
    "year": 2025,
    "published": "2025-04-18T09:33:45Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "High-level 3D scene understanding is essential in many applications. However, the challenges of generating accurate 3D annotations make development of deep learning models difficult. We turn to recent advancements in automatic retrieval of synthetic CAD models, and show that data generated by such methods can be used as high-quality ground truth for training supervised deep learning models. More exactly, we employ a pipeline akin to the one previously used to automatically annotate objects in Sc",
    "arxiv_url": "https://arxiv.org/abs/2504.13580v4",
    "pdf_url": "https://arxiv.org/pdf/2504.13580v4",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2504.13580",
    "arxiv_authors": [
      "Yuchen Rao",
      "Stefan Ainetter",
      "Sinisa Stekovic",
      "Vincent Lepetit",
      "Friedrich Fraundorfer"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Leveraging+Automatic+CAD+Annotations+for+Supervised+Learning+in+3D+Scene+Understanding+Yuchen+Rao+Stefan+Ainetter+Sinisa+Stekovic+Vincent+Lepetit+Friedrich+Fraundorfer",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "Y Rao",
        "id": "PcW1oL4AAAAJ"
      },
      {
        "name": "S Ainetter",
        "id": "VWNkvIwAAAAJ"
      },
      {
        "name": "S Stekovic",
        "id": "T0qGNQYAAAAJ"
      },
      {
        "name": "V Lepetit",
        "id": "h0a5q3QAAAAJ"
      },
      {
        "name": "F Fraundorfer",
        "id": "M0boL5kAAAAJ"
      }
    ],
    "citation_count": null
  },
  {
    "arxiv_id": "2308.02213",
    "title": "Balanced Classification: A Unified Framework for Long-Tailed Object Detection",
    "year": 2023,
    "published": "2023-08-04T09:11:07Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Conventional detectors suffer from performance degradation when dealing with long-tailed data due to a classification bias towards the majority head categories. In this paper, we contend that the learning bias originates from two factors: 1) the unequal competition arising from the imbalanced distribution of foreground categories, and 2) the lack of sample diversity in tail categories. To tackle these issues, we introduce a unified framework called BAlanced CLassification (BACL), which enables a",
    "arxiv_url": "https://arxiv.org/abs/2308.02213v1",
    "pdf_url": "https://arxiv.org/pdf/2308.02213v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2308.02213",
    "arxiv_authors": [
      "Tianhao Qi",
      "Hongtao Xie",
      "Pandeng Li",
      "Jiannan Ge",
      "Yongdong Zhang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Balanced+Classification%3A+A+Unified+Framework+for+Long-Tailed+Object+Detection+Tianhao+Qi+Hongtao+Xie+Pandeng+Li+Jiannan+Ge+Yongdong+Zhang",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "T Qi",
        "id": "eU_veu0AAAAJ"
      },
      {
        "name": "H Xie",
        "id": null
      },
      {
        "name": "P Li",
        "id": "Moy-4-0AAAAJ"
      },
      {
        "name": "J Ge",
        "id": "Y6xHVekAAAAJ"
      },
      {
        "name": "Y Zhang - IEEE Transactions on",
        "id": null
      }
    ],
    "citation_count": 25
  },
  {
    "arxiv_id": "2503.00242",
    "title": "Boundary-Emphasized Weight Maps for Distal Airway Segmentation",
    "year": 2025,
    "published": "2025-02-28T23:11:13Z",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "abstract": "Automated airway segmentation from lung CT scans is vital for diagnosing and monitoring pulmonary diseases. Despite advancements, challenges like leakage, breakage, and class imbalance persist, particularly in capturing small airways and preserving topology. We propose the Boundary-Emphasized Loss (BEL), which enhances boundary preservation using a boundary-based weight map and an adaptive weight refinement strategy. Unlike centerline-based approaches, BEL prioritizes boundary voxels to reduce m",
    "arxiv_url": "https://arxiv.org/abs/2503.00242v1",
    "pdf_url": "https://arxiv.org/pdf/2503.00242v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2503.00242",
    "arxiv_authors": [
      "Ali Keshavarzi",
      "Elsa Angelini"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Boundary-Emphasized+Weight+Maps+for+Distal+Airway+Segmentation+Ali+Keshavarzi+Elsa+Angelini",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "A Keshavarzi",
        "id": "GBaZhSQAAAAJ"
      },
      {
        "name": "E Angelini -",
        "id": null
      }
    ],
    "citation_count": null
  },
  {
    "arxiv_id": "2407.10389",
    "title": "Boost Your NeRF: A Model-Agnostic Mixture of Experts Framework for High Quality and Efficient Rendering",
    "year": 2024,
    "published": "2024-07-15T01:58:54Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Since the introduction of NeRFs, considerable attention has been focused on improving their training and inference times, leading to the development of Fast-NeRFs models. Despite demonstrating impressive rendering speed and quality, the rapid convergence of such models poses challenges for further improving reconstruction quality. Common strategies to improve rendering quality involves augmenting model parameters or increasing the number of sampled points. However, these computationally intensiv",
    "arxiv_url": "https://arxiv.org/abs/2407.10389v3",
    "pdf_url": "https://arxiv.org/pdf/2407.10389v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2407.10389",
    "arxiv_authors": [
      "Francesco Di Sario",
      "Riccardo Renzulli",
      "Enzo Tartaglione",
      "Marco Grangetto"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Boost+Your+NeRF%3A+A+Model-Agnostic+Mixture+of+Experts+Framework+for+High+Quality+and+Efficient+Rendering+Francesco+Di+Sario+Riccardo+Renzulli+Enzo+Tartaglione+Marco+Grangetto",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "F Di Sario",
        "id": "wjOMl_8AAAAJ"
      },
      {
        "name": "R Renzulli",
        "id": "JlAby_oAAAAJ"
      },
      {
        "name": "E Tartaglione",
        "id": "uKuvN64AAAAJ"
      },
      {
        "name": "M GrangettoEuropean",
        "id": null
      }
    ],
    "citation_count": 6
  },
  {
    "arxiv_id": "2407.12214",
    "title": "VideoClusterNet: Self-Supervised and Adaptive Face Clustering For Videos",
    "year": 2024,
    "published": "2024-07-16T23:34:55Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "With the rise of digital media content production, the need for analyzing movies and TV series episodes to locate the main cast of characters precisely is gaining importance.Specifically, Video Face Clustering aims to group together detected video face tracks with common facial identities. This problem is very challenging due to the large range of pose, expression, appearance, and lighting variations of a given face across video frames. Generic pre-trained Face Identification (ID) models fail to",
    "arxiv_url": "https://arxiv.org/abs/2407.12214v2",
    "pdf_url": "https://arxiv.org/pdf/2407.12214v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2407.12214",
    "arxiv_authors": [
      "Devesh Walawalkar",
      "Pablo Garrido"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=VideoClusterNet%3A+Self-Supervised+and+Adaptive+Face+Clustering+For+Videos+Devesh+Walawalkar+Pablo+Garrido",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "D Walawalkar",
        "id": "MpHIZfEAAAAJ"
      },
      {
        "name": "P Garrido - European",
        "id": null
      }
    ],
    "citation_count": 2
  },
  {
    "arxiv_id": "2404.13550",
    "title": "Pointsoup: High-Performance and Extremely Low-Decoding-Latency Learned Geometry Codec for Large-Scale Point Cloud Scenes",
    "year": 2024,
    "published": "2024-04-21T06:31:29Z",
    "categories": [
      "cs.CV",
      "eess.IV"
    ],
    "abstract": "Despite considerable progress being achieved in point cloud geometry compression, there still remains a challenge in effectively compressing large-scale scenes with sparse surfaces. Another key challenge lies in reducing decoding latency, a crucial requirement in real-world application. In this paper, we propose Pointsoup, an efficient learning-based geometry codec that attains high-performance and extremely low-decoding-latency simultaneously. Inspired by conventional Trisoup codec, a point mod",
    "arxiv_url": "https://arxiv.org/abs/2404.13550v1",
    "pdf_url": "https://arxiv.org/pdf/2404.13550v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2404.13550",
    "arxiv_authors": [
      "Kang You",
      "Kai Liu",
      "Li Yu",
      "Pan Gao",
      "Dandan Ding"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Pointsoup%3A+High-Performance+and+Extremely+Low-Decoding-Latency+Learned+Geometry+Codec+for+Large-Scale+Point+Cloud+Scenes+Kang+You+Kai+Liu+Li+Yu+Pan+Gao+Dandan+Ding",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "K You",
        "id": "JLH972sAAAAJ"
      },
      {
        "name": "K Liu",
        "id": null
      },
      {
        "name": "L Yu",
        "id": "HSzWxk4AAAAJ"
      },
      {
        "name": "P Gao",
        "id": "vxHerj4AAAAJ"
      },
      {
        "name": "D Ding -",
        "id": null
      }
    ],
    "citation_count": 10
  },
  {
    "arxiv_id": "2407.17267",
    "title": "M4: Multi-Proxy Multi-Gate Mixture of Experts Network for Multiple Instance Learning in Histopathology Image Analysis",
    "year": 2024,
    "published": "2024-07-24T13:30:46Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Multiple instance learning (MIL) has been successfully applied for whole slide images (WSIs) analysis in computational pathology, enabling a wide range of prediction tasks from tumor subtyping to inferring genetic mutations and multi-omics biomarkers. However, existing MIL methods predominantly focus on single-task learning, resulting in not only overall low efficiency but also the overlook of inter-task relatedness. To address these issues, we proposed an adapted architecture of Multi-gate Mixt",
    "arxiv_url": "https://arxiv.org/abs/2407.17267v1",
    "pdf_url": "https://arxiv.org/pdf/2407.17267v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2407.17267",
    "arxiv_authors": [
      "Junyu Li",
      "Ye Zhang",
      "Wen Shu",
      "Xiaobing Feng",
      "Yingchun Wang",
      "Pengju Yan",
      "Xiaolin Li",
      "Chulin Sha",
      "Min He"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=M4%3A+Multi-Proxy+Multi-Gate+Mixture+of+Experts+Network+for+Multiple+Instance+Learning+in+Histopathology+Image+Analysis+Junyu+Li+Ye+Zhang+Wen+Shu+Xiaobing+Feng+Yingchun+Wang",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "J Li",
        "id": null
      },
      {
        "name": "Y Zhang",
        "id": "FrQoW4AAAAAJ"
      },
      {
        "name": "W Shu",
        "id": null
      },
      {
        "name": "X Feng",
        "id": "sVZepFQAAAAJ"
      },
      {
        "name": "Y Wang",
        "id": null
      },
      {
        "name": "P Yan",
        "id": "f7DsE-UAAAAJ"
      },
      {
        "name": "X Li",
        "id": null
      },
      {
        "name": "C Sha",
        "id": "w-fWJ-YAAAAJ"
      },
      {
        "name": "M HeMedical Image Analysis",
        "id": null
      }
    ],
    "citation_count": 4
  },
  {
    "arxiv_id": "2308.00228",
    "title": "Using Scene and Semantic Features for Multi-modal Emotion Recognition",
    "year": 2023,
    "published": "2023-08-01T01:54:55Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Automatic emotion recognition is a hot topic with a wide range of applications. Much work has been done in the area of automatic emotion recognition in recent years. The focus has been mainly on using the characteristics of a person such as speech, facial expression and pose for this purpose. However, the processing of scene and semantic features for emotion recognition has had limited exploration. In this paper, we propose to use combined scene and semantic features, along with personal feature",
    "arxiv_url": "https://arxiv.org/abs/2308.00228v1",
    "pdf_url": "https://arxiv.org/pdf/2308.00228v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2308.00228",
    "arxiv_authors": [
      "Zhifeng Wang",
      "Ramesh Sankaranarayana"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Using+Scene+and+Semantic+Features+for+Multi-modal+Emotion+Recognition+Zhifeng+Wang+Ramesh+Sankaranarayana",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "Z Wang",
        "id": "Uh7R-acAAAAJ"
      },
      {
        "name": "R Sankaranarayana -",
        "id": null
      }
    ],
    "citation_count": 1
  },
  {
    "arxiv_id": "2411.06911",
    "title": "Gaussian Process Emulators for Few-Shot Segmentation in Cardiac MRI",
    "year": 2024,
    "published": "2024-11-11T12:13:58Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "abstract": "Segmentation of cardiac magnetic resonance images (MRI) is crucial for the analysis and assessment of cardiac function, helping to diagnose and treat various cardiovascular diseases. Most recent techniques rely on deep learning and usually require an extensive amount of labeled data. To overcome this problem, few-shot learning has the capability of reducing data dependency on labeled data. In this work, we introduce a new method that merges few-shot learning with a U-Net architecture and Gaussia",
    "arxiv_url": "https://arxiv.org/abs/2411.06911v2",
    "pdf_url": "https://arxiv.org/pdf/2411.06911v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2411.06911",
    "arxiv_authors": [
      "Bruno Viti",
      "Franz Thaler",
      "Kathrin Lisa Kapper",
      "Martin Urschler",
      "Martin Holler",
      "Elias Karabelas"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Gaussian+Process+Emulators+for+Few-Shot+Segmentation+in+Cardiac+MRI+Bruno+Viti+Franz+Thaler+Kathrin+Lisa+Kapper+Martin+Urschler+Martin+Holler",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "B Viti",
        "id": "JTJM8FsAAAAJ"
      },
      {
        "name": "F Thaler",
        "id": "xYcMlrIAAAAJ"
      },
      {
        "name": "KL Kapper",
        "id": "RVcC6oYAAAAJ"
      },
      {
        "name": "M Urschler",
        "id": "HKLMeBIAAAAJ"
      },
      {
        "name": "M Holler",
        "id": "0dmnG6QAAAAJ"
      }
    ],
    "citation_count": null
  },
  {
    "arxiv_id": "2304.02135",
    "title": "FREDOM: Fairness Domain Adaptation Approach to Semantic Scene Understanding",
    "year": 2023,
    "published": "2023-04-04T21:35:10Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Although Domain Adaptation in Semantic Scene Segmentation has shown impressive improvement in recent years, the fairness concerns in the domain adaptation have yet to be well defined and addressed. In addition, fairness is one of the most critical aspects when deploying the segmentation models into human-related real-world applications, e.g., autonomous driving, as any unfair predictions could influence human safety. In this paper, we propose a novel Fairness Domain Adaptation (FREDOM) approach ",
    "arxiv_url": "https://arxiv.org/abs/2304.02135v1",
    "pdf_url": "https://arxiv.org/pdf/2304.02135v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2304.02135",
    "arxiv_authors": [
      "Thanh-Dat Truong",
      "Ngan Le",
      "Bhiksha Raj",
      "Jackson Cothren",
      "Khoa Luu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=FREDOM%3A+Fairness+Domain+Adaptation+Approach+to+Semantic+Scene+Understanding+Thanh-Dat+Truong+Ngan+Le+Bhiksha+Raj+Jackson+Cothren+Khoa+Luu",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "TD Truong",
        "id": "qrmxykkAAAAJ"
      },
      {
        "name": "N Le",
        "id": "8ck0k_UAAAAJ"
      },
      {
        "name": "B Raj",
        "id": "IWcGY98AAAAJ"
      },
      {
        "name": "J Cothren",
        "id": "_WB9fo4AAAAJ"
      },
      {
        "name": "K Luu",
        "id": "JPAl8-gAAAAJ"
      }
    ],
    "citation_count": 50
  },
  {
    "arxiv_id": "2409.18218",
    "title": "Learning to Drive via Asymmetric Self-Play",
    "year": 2024,
    "published": "2024-09-26T18:55:38Z",
    "categories": [
      "cs.RO",
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "Large-scale data is crucial for learning realistic and capable driving policies. However, it can be impractical to rely on scaling datasets with real data alone. The majority of driving data is uninteresting, and deliberately collecting new long-tail scenarios is expensive and unsafe. We propose asymmetric self-play to scale beyond real data with additional challenging, solvable, and realistic synthetic scenarios. Our approach pairs a teacher that learns to generate scenarios it can solve but th",
    "arxiv_url": "https://arxiv.org/abs/2409.18218v1",
    "pdf_url": "https://arxiv.org/pdf/2409.18218v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2409.18218",
    "arxiv_authors": [
      "Chris Zhang",
      "Sourav Biswas",
      "Kelvin Wong",
      "Kion Fallah",
      "Lunjun Zhang",
      "Dian Chen",
      "Sergio Casas",
      "Raquel Urtasun"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Learning+to+Drive+via+Asymmetric+Self-Play+Chris+Zhang+Sourav+Biswas+Kelvin+Wong+Kion+Fallah+Lunjun+Zhang",
    "gs_search_success": false,
    "gs_authors": null,
    "citation_count": null,
    "error_type": "multiple_matches"
  },
  {
    "arxiv_id": "2302.01133",
    "title": "SceneScape: Text-Driven Consistent Scene Generation",
    "year": 2023,
    "published": "2023-02-02T14:47:19Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "We present a method for text-driven perpetual view generation -- synthesizing long-term videos of various scenes solely, given an input text prompt describing the scene and camera poses. We introduce a novel framework that generates such videos in an online fashion by combining the generative power of a pre-trained text-to-image model with the geometric priors learned by a pre-trained monocular depth prediction model. To tackle the pivotal challenge of achieving 3D consistency, i.e., synthesizin",
    "arxiv_url": "https://arxiv.org/abs/2302.01133v2",
    "pdf_url": "https://arxiv.org/pdf/2302.01133v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2302.01133",
    "arxiv_authors": [
      "Rafail Fridman",
      "Amit Abecasis",
      "Yoni Kasten",
      "Tali Dekel"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=SceneScape%3A+Text-Driven+Consistent+Scene+Generation+Rafail+Fridman+Amit+Abecasis+Yoni+Kasten+Tali+Dekel",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "R Fridman",
        "id": "qBPARd8AAAAJ"
      },
      {
        "name": "A Abecasis",
        "id": "AUpDHsgAAAAJ"
      },
      {
        "name": "Y Kasten",
        "id": "kc4-e8oAAAAJ"
      },
      {
        "name": "T DekelAdvances in Neural Information Processing Systems",
        "id": null
      }
    ],
    "citation_count": 145
  },
  {
    "arxiv_id": "2311.06845",
    "title": "Sampler Scheduler for Diffusion Models",
    "year": 2023,
    "published": "2023-11-12T13:35:25Z",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "Diffusion modeling (DM) has high-quality generative performance, and the sampling problem is an important part of the DM performance. Thanks to efficient differential equation solvers, the sampling speed can be reduced while higher sampling quality is guaranteed. However, currently, there is a contradiction in samplers for diffusion-based generative models: the mainstream sampler choices are diverse, each with its own characteristics in terms of performance. However, only a single sampler algori",
    "arxiv_url": "https://arxiv.org/abs/2311.06845v1",
    "pdf_url": "https://arxiv.org/pdf/2311.06845v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2311.06845",
    "arxiv_authors": [
      "Zitong Cheng"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Sampler+Scheduler+for+Diffusion+Models+Zitong+Cheng",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "Z Cheng -",
        "id": null
      }
    ],
    "citation_count": 2
  },
  {
    "arxiv_id": "2312.00937",
    "title": "Zero-Shot Video Question Answering with Procedural Programs",
    "year": 2023,
    "published": "2023-12-01T21:34:10Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "We propose to answer zero-shot questions about videos by generating short procedural programs that derive a final answer from solving a sequence of visual subtasks. We present Procedural Video Querying (ProViQ), which uses a large language model to generate such programs from an input question and an API of visual modules in the prompt, then executes them to obtain the output. Recent similar procedural approaches have proven successful for image question answering, but videos remain challenging:",
    "arxiv_url": "https://arxiv.org/abs/2312.00937v1",
    "pdf_url": "https://arxiv.org/pdf/2312.00937v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2312.00937",
    "arxiv_authors": [
      "Rohan Choudhury",
      "Koichiro Niinuma",
      "Kris M. Kitani",
      "L√°szl√≥ A. Jeni"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Zero-Shot+Video+Question+Answering+with+Procedural+Programs+Rohan+Choudhury+Koichiro+Niinuma+Kris+M.+Kitani+L%C3%A1szl%C3%B3+A.+Jeni",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "R Choudhury",
        "id": "G01350MAAAAJ"
      },
      {
        "name": "K Niinuma",
        "id": "AFaeUrYAAAAJ"
      },
      {
        "name": "KM Kitani",
        "id": "yv3sH74AAAAJ"
      },
      {
        "name": "LA Jeni -",
        "id": null
      }
    ],
    "citation_count": 39
  },
  {
    "arxiv_id": "2301.11422",
    "title": "RMSim: Controlled Respiratory Motion Simulation on Static Patient Scans",
    "year": 2023,
    "published": "2023-01-26T21:20:14Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "This work aims to generate realistic anatomical deformations from static patient scans. Specifically, we present a method to generate these deformations/augmentations via deep learning driven respiratory motion simulation that provides the ground truth for validating deformable image registration (DIR) algorithms and driving more accurate deep learning based DIR. We present a novel 3D Seq2Seq deep learning respiratory motion simulator (RMSim) that learns from 4D-CT images and predicts future bre",
    "arxiv_url": "https://arxiv.org/abs/2301.11422v1",
    "pdf_url": "https://arxiv.org/pdf/2301.11422v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2301.11422",
    "arxiv_authors": [
      "Donghoon Lee",
      "Ellen Yorke",
      "Masoud Zarepisheh",
      "Saad Nadeem",
      "Yu-Chi Hu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=RMSim%3A+Controlled+Respiratory+Motion+Simulation+on+Static+Patient+Scans+Donghoon+Lee+Ellen+Yorke+Masoud+Zarepisheh+Saad+Nadeem+Yu-Chi+Hu",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "D Lee",
        "id": "b9uHmt0AAAAJ"
      },
      {
        "name": "E Yorke",
        "id": null
      },
      {
        "name": "M Zarepisheh",
        "id": "DuytjrMAAAAJ"
      },
      {
        "name": "S Nadeem",
        "id": "F-RwTMgAAAAJ"
      },
      {
        "name": "YC HuPhysics in Medicine & Biology",
        "id": null
      }
    ],
    "citation_count": 5
  },
  {
    "arxiv_id": "2505.13232",
    "title": "StarFT: Robust Fine-tuning of Zero-shot Models via Spuriosity Alignment",
    "year": 2025,
    "published": "2025-05-19T15:15:35Z",
    "categories": [
      "cs.AI",
      "cs.CV"
    ],
    "abstract": "Learning robust representations from data often requires scale, which has led to the success of recent zero-shot models such as CLIP. However, the obtained robustness can easily be deteriorated when these models are fine-tuned on other downstream tasks (e.g., of smaller scales). Previous works often interpret this phenomenon in the context of domain shift, developing fine-tuning methods that aim to preserve the original domain as much as possible. However, in a different context, fine-tuned mode",
    "arxiv_url": "https://arxiv.org/abs/2505.13232v3",
    "pdf_url": "https://arxiv.org/pdf/2505.13232v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2505.13232",
    "arxiv_authors": [
      "Younghyun Kim",
      "Jongheon Jeong",
      "Sangkyung Kwak",
      "Kyungmin Lee",
      "Juho Lee",
      "Jinwoo Shin"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=StarFT%3A+Robust+Fine-tuning+of+Zero-shot+Models+via+Spuriosity+Alignment+Younghyun+Kim+Jongheon+Jeong+Sangkyung+Kwak+Kyungmin+Lee+Juho+Lee",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "Y Kim",
        "id": "Bk6pxR0AAAAJ"
      },
      {
        "name": "J Jeong",
        "id": "mZB2qfcAAAAJ"
      },
      {
        "name": "S Kwak",
        "id": "THl093MAAAAJ"
      },
      {
        "name": "K Lee",
        "id": null
      },
      {
        "name": "J Lee",
        "id": null
      },
      {
        "name": "J Shin -",
        "id": null
      }
    ],
    "citation_count": null
  },
  {
    "arxiv_id": "2406.17115",
    "title": "Evaluating the Quality of Hallucination Benchmarks for Large Vision-Language Models",
    "year": 2024,
    "published": "2024-06-24T20:08:07Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Despite the rapid progress and outstanding performance of Large Vision-Language Models (LVLMs) in recent years, LVLMs have been plagued by the issue of hallucination, i.e., LVLMs tend to generate responses that are inconsistent with the corresponding visual inputs. To evaluate the degree of hallucination in LVLMs, previous works have proposed a series of benchmarks featuring different types of tasks and evaluation metrics. However, we find that the quality of the existing hallucination benchmark",
    "arxiv_url": "https://arxiv.org/abs/2406.17115v2",
    "pdf_url": "https://arxiv.org/pdf/2406.17115v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2406.17115",
    "arxiv_authors": [
      "Bei Yan",
      "Jie Zhang",
      "Zheng Yuan",
      "Shiguang Shan",
      "Xilin Chen"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Evaluating+the+Quality+of+Hallucination+Benchmarks+for+Large+Vision-Language+Models+Bei+Yan+Jie+Zhang+Zheng+Yuan+Shiguang+Shan+Xilin+Chen",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "B Yan",
        "id": "7Xcb8hoAAAAJ"
      },
      {
        "name": "J Zhang",
        "id": "hJAhF0sAAAAJ"
      },
      {
        "name": "Z Yuan",
        "id": "K1wH01cAAAAJ"
      },
      {
        "name": "S Shan",
        "id": "Vkzd7MIAAAAJ"
      },
      {
        "name": "X Chen -",
        "id": null
      }
    ],
    "citation_count": 19
  },
  {
    "arxiv_id": "2504.05444",
    "title": "Biomechanical Constraints Assimilation in Deep-Learning Image Registration: Application to sliding and locally rigid deformations",
    "year": 2025,
    "published": "2025-04-07T19:12:51Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Regularization strategies in medical image registration often take a one-size-fits-all approach by imposing uniform constraints across the entire image domain. Yet biological structures are anything but regular. Lacking structural awareness, these strategies may fail to consider a panoply of spatially inhomogeneous deformation properties, which would faithfully account for the biomechanics of soft and hard tissues, especially in poorly contrasted structures.   To bridge this gap, we propose a le",
    "arxiv_url": "https://arxiv.org/abs/2504.05444v1",
    "pdf_url": "https://arxiv.org/pdf/2504.05444v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2504.05444",
    "arxiv_authors": [
      "Ziad Kheil",
      "Soleakhena Ken",
      "Laurent Risser"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Biomechanical+Constraints+Assimilation+in+Deep-Learning+Image+Registration%3A+Application+to+sliding+and+locally+rigid+deformations+Ziad+Kheil+Soleakhena+Ken+Laurent+Risser",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "Z Kheil",
        "id": "RPX33-EAAAAJ"
      },
      {
        "name": "S Ken",
        "id": "FyQd_d0AAAAJ"
      },
      {
        "name": "L Risser -",
        "id": null
      }
    ],
    "citation_count": null
  },
  {
    "arxiv_id": "2403.17926",
    "title": "FastCAR: Fast Classification And Regression Multi-Task Learning via Task Consolidation for Modelling a Continuous Property Variable of Object Classes",
    "year": 2024,
    "published": "2024-03-26T17:57:20Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "FastCAR is a novel task consolidation approach in Multi-Task Learning (MTL) for a classification and a regression task, despite task heterogeneity with only subtle correlation. It addresses object classification and continuous property variable regression, a crucial use case in science and engineering. FastCAR involves a labeling transformation approach that can be used with a single-task regression network architecture. FastCAR outperforms traditional MTL model families, parametrized in the lan",
    "arxiv_url": "https://arxiv.org/abs/2403.17926v2",
    "pdf_url": "https://arxiv.org/pdf/2403.17926v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2403.17926",
    "arxiv_authors": [
      "Anoop Kini",
      "Andreas Jansche",
      "Timo Bernthaler",
      "Gerhard Schneider"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=FastCAR%3A+Fast+Classification+And+Regression+Multi-Task+Learning+via+Task+Consolidation+for+Modelling+a+Continuous+Property+Variable+of+Object+Classes+Anoop+Kini+Andreas+Jansche+Timo+Bernthaler+Gerhard+Schneider",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "A Kini",
        "id": "AGrgU5UAAAAJ"
      },
      {
        "name": "A Jansche",
        "id": null
      },
      {
        "name": "T Bernthaler",
        "id": null
      },
      {
        "name": "G Schneider -",
        "id": null
      }
    ],
    "citation_count": 1
  },
  {
    "arxiv_id": "2503.18783",
    "title": "Frequency Dynamic Convolution for Dense Image Prediction",
    "year": 2025,
    "published": "2025-03-24T15:32:06Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "While Dynamic Convolution (DY-Conv) has shown promising performance by enabling adaptive weight selection through multiple parallel weights combined with an attention mechanism, the frequency response of these weights tends to exhibit high similarity, resulting in high parameter costs but limited adaptability. In this work, we introduce Frequency Dynamic Convolution (FDConv), a novel approach that mitigates these limitations by learning a fixed parameter budget in the Fourier domain. FDConv divi",
    "arxiv_url": "https://arxiv.org/abs/2503.18783v2",
    "pdf_url": "https://arxiv.org/pdf/2503.18783v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2503.18783",
    "arxiv_authors": [
      "Linwei Chen",
      "Lin Gu",
      "Liang Li",
      "Chenggang Yan",
      "Ying Fu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Frequency+Dynamic+Convolution+for+Dense+Image+Prediction+Linwei+Chen+Lin+Gu+Liang+Li+Chenggang+Yan+Ying+Fu",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "L Chen",
        "id": "EGlOtL4AAAAJ"
      },
      {
        "name": "L Gu",
        "id": "gIEZe5IAAAAJ"
      },
      {
        "name": "L Li",
        "id": "Q-4mZnQAAAAJ"
      },
      {
        "name": "C Yan",
        "id": "acPALtAAAAAJ"
      },
      {
        "name": "Y Fu -",
        "id": null
      }
    ],
    "citation_count": 19
  },
  {
    "arxiv_id": "2503.14524",
    "title": "Salient Temporal Encoding for Dynamic Scene Graph Generation",
    "year": 2025,
    "published": "2025-03-15T08:01:36Z",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "Representing a dynamic scene using a structured spatial-temporal scene graph is a novel and particularly challenging task. To tackle this task, it is crucial to learn the temporal interactions between objects in addition to their spatial relations. Due to the lack of explicitly annotated temporal relations in current benchmark datasets, most of the existing spatial-temporal scene graph generation methods build dense and abstract temporal connections among all objects across frames. However, not ",
    "arxiv_url": "https://arxiv.org/abs/2503.14524v1",
    "pdf_url": "https://arxiv.org/pdf/2503.14524v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2503.14524",
    "arxiv_authors": [
      "Zhihao Zhu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Salient+Temporal+Encoding+for+Dynamic+Scene+Graph+Generation+Zhihao+Zhu",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "Z Zhu -",
        "id": null
      }
    ],
    "citation_count": null
  },
  {
    "arxiv_id": "2412.13708",
    "title": "JoVALE: Detecting Human Actions in Video Using Audiovisual and Language Contexts",
    "year": 2024,
    "published": "2024-12-18T10:51:31Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Video Action Detection (VAD) entails localizing and categorizing action instances within videos, which inherently consist of diverse information sources such as audio, visual cues, and surrounding scene contexts. Leveraging this multi-modal information effectively for VAD poses a significant challenge, as the model must identify action-relevant cues with precision. In this study, we introduce a novel multi-modal VAD architecture, referred to as the Joint Actor-centric Visual, Audio, Language Enc",
    "arxiv_url": "https://arxiv.org/abs/2412.13708v2",
    "pdf_url": "https://arxiv.org/pdf/2412.13708v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2412.13708",
    "arxiv_authors": [
      "Taein Son",
      "Soo Won Seo",
      "Jisong Kim",
      "Seok Hwan Lee",
      "Jun Won Choi"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=JoVALE%3A+Detecting+Human+Actions+in+Video+Using+Audiovisual+and+Language+Contexts+Taein+Son+Soo+Won+Seo+Jisong+Kim+Seok+Hwan+Lee+Jun+Won+Choi",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "T Son",
        "id": null
      },
      {
        "name": "SW Seo",
        "id": null
      },
      {
        "name": "J Kim",
        "id": "yD7Led0AAAAJ"
      },
      {
        "name": "SH Lee",
        "id": null
      },
      {
        "name": "JW Choi -",
        "id": null
      }
    ],
    "citation_count": 1
  },
  {
    "arxiv_id": "2409.08443",
    "title": "CF-PRNet: Coarse-to-Fine Prototype Refining Network for Point Cloud Completion and Reconstruction",
    "year": 2024,
    "published": "2024-09-13T00:20:10Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "In modern agriculture, precise monitoring of plants and fruits is crucial for tasks such as high-throughput phenotyping and automated harvesting. This paper addresses the challenge of reconstructing accurate 3D shapes of fruits from partial views, which is common in agricultural settings. We introduce CF-PRNet, a coarse-to-fine prototype refining network, leverages high-resolution 3D data during the training phase but requires only a single RGB-D image for real-time inference. Our approach begin",
    "arxiv_url": "https://arxiv.org/abs/2409.08443v1",
    "pdf_url": "https://arxiv.org/pdf/2409.08443v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2409.08443",
    "arxiv_authors": [
      "Zhi Chen",
      "Tianqi Wei",
      "Zecheng Zhao",
      "Jia Syuen Lim",
      "Yadan Luo",
      "Hu Zhang",
      "Xin Yu",
      "Scott Chapman",
      "Zi Huang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=CF-PRNet%3A+Coarse-to-Fine+Prototype+Refining+Network+for+Point+Cloud+Completion+and+Reconstruction+Zhi+Chen+Tianqi+Wei+Zecheng+Zhao+Jia+Syuen+Lim+Yadan+Luo",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "Z Chen",
        "id": "9ZypKEYAAAAJ"
      },
      {
        "name": "T Wei",
        "id": "Dc0WuhoAAAAJ"
      },
      {
        "name": "Z Zhao",
        "id": "pqZ5OlEAAAAJ"
      },
      {
        "name": "JS Lim",
        "id": "B9zn51sAAAAJ"
      },
      {
        "name": "Y Luo",
        "id": "3IfL11AAAAAJ"
      },
      {
        "name": "H Zhang",
        "id": "DkAZJX4AAAAJ"
      },
      {
        "name": "X Yu",
        "id": "oxdtuSEAAAAJ"
      },
      {
        "name": "S Chapman",
        "id": "v-giX5UAAAAJ"
      },
      {
        "name": "Z Huang",
        "id": "iAWMsgEAAAAJ"
      }
    ],
    "citation_count": 6
  },
  {
    "arxiv_id": "2310.00372",
    "title": "Deep Active Learning with Noisy Oracle in Object Detection",
    "year": 2023,
    "published": "2023-09-30T13:28:35Z",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "Obtaining annotations for complex computer vision tasks such as object detection is an expensive and time-intense endeavor involving a large number of human workers or expert opinions. Reducing the amount of annotations required while maintaining algorithm performance is, therefore, desirable for machine learning practitioners and has been successfully achieved by active learning algorithms. However, it is not merely the amount of annotations which influences model performance but also the annot",
    "arxiv_url": "https://arxiv.org/abs/2310.00372v1",
    "pdf_url": "https://arxiv.org/pdf/2310.00372v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2310.00372",
    "arxiv_authors": [
      "Marius Schubert",
      "Tobias Riedlinger",
      "Karsten Kahl",
      "Matthias Rottmann"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Deep+Active+Learning+with+Noisy+Oracle+in+Object+Detection+Marius+Schubert+Tobias+Riedlinger+Karsten+Kahl+Matthias+Rottmann",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "M Schubert",
        "id": null
      },
      {
        "name": "T Riedlinger",
        "id": "XggH5bwAAAAJ"
      },
      {
        "name": "K Kahl",
        "id": "rrt7FX0AAAAJ"
      },
      {
        "name": "M Rottmann",
        "id": "1Oofk3YAAAAJ"
      }
    ],
    "citation_count": 2
  },
  {
    "arxiv_id": "2505.01880",
    "title": "Weakly-supervised Audio Temporal Forgery Localization via Progressive Audio-language Co-learning Network",
    "year": 2025,
    "published": "2025-05-03T17:57:57Z",
    "categories": [
      "cs.SD",
      "cs.CV",
      "cs.MM",
      "eess.AS"
    ],
    "abstract": "Audio temporal forgery localization (ATFL) aims to find the precise forgery regions of the partial spoof audio that is purposefully modified. Existing ATFL methods rely on training efficient networks using fine-grained annotations, which are obtained costly and challenging in real-world scenarios. To meet this challenge, in this paper, we propose a progressive audio-language co-learning network (LOCO) that adopts co-learning and self-supervision manners to prompt localization performance under w",
    "arxiv_url": "https://arxiv.org/abs/2505.01880v2",
    "pdf_url": "https://arxiv.org/pdf/2505.01880v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2505.01880",
    "arxiv_authors": [
      "Junyan Wu",
      "Wenbo Xu",
      "Wei Lu",
      "Xiangyang Luo",
      "Rui Yang",
      "Shize Guo"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Weakly-supervised+Audio+Temporal+Forgery+Localization+via+Progressive+Audio-language+Co-learning+Network+Junyan+Wu+Wenbo+Xu+Wei+Lu+Xiangyang+Luo+Rui+Yang",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "J Wu",
        "id": "wLkiomYAAAAJ"
      },
      {
        "name": "W Xu",
        "id": "VkOfM84AAAAJ"
      },
      {
        "name": "W Lu",
        "id": "segsUCEAAAAJ"
      },
      {
        "name": "X Luo",
        "id": null
      },
      {
        "name": "R Yang",
        "id": null
      },
      {
        "name": "S Guo -",
        "id": null
      }
    ],
    "citation_count": 4
  },
  {
    "arxiv_id": "2304.02736",
    "title": "Image Stabilization for Hololens Camera in Remote Collaboration",
    "year": 2023,
    "published": "2023-04-05T20:35:49Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "With the advent of new technologies, Augmented Reality (AR) has become an effective tool in remote collaboration. Narrow field-of-view (FoV) and motion blur can offer an unpleasant experience with limited cognition for remote viewers of AR headsets. In this article, we propose a two-stage pipeline to tackle this issue and ensure a stable viewing experience with a larger FoV. The solution involves an offline 3D reconstruction of the indoor environment, followed by enhanced rendering using only th",
    "arxiv_url": "https://arxiv.org/abs/2304.02736v1",
    "pdf_url": "https://arxiv.org/pdf/2304.02736v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2304.02736",
    "arxiv_authors": [
      "Gowtham Senthil",
      "Siva Vignesh Krishnan",
      "Annamalai Lakshmanan",
      "Florence Kissling"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Image+Stabilization+for+Hololens+Camera+in+Remote+Collaboration+Gowtham+Senthil+Siva+Vignesh+Krishnan+Annamalai+Lakshmanan+Florence+Kissling",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "G Senthil",
        "id": null
      },
      {
        "name": "SV Krishnan",
        "id": "bi9TBpsAAAAJ"
      },
      {
        "name": "A Lakshmanan",
        "id": "cwl2lDIAAAAJ"
      },
      {
        "name": "F Kissling",
        "id": null
      }
    ],
    "citation_count": null
  },
  {
    "arxiv_id": "2304.03101",
    "title": "Multi-task learning for tissue segmentation and tumor detection in colorectal cancer histology slides",
    "year": 2023,
    "published": "2023-04-06T14:26:41Z",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "abstract": "Automating tissue segmentation and tumor detection in histopathology images of colorectal cancer (CRC) is an enabler for faster diagnostic pathology workflows. At the same time it is a challenging task due to low availability of public annotated datasets and high variability of image appearance. The semi-supervised learning for CRC detection (SemiCOL) challenge 2023 provides partially annotated data to encourage the development of automated solutions for tissue segmentation and tumor detection. ",
    "arxiv_url": "https://arxiv.org/abs/2304.03101v1",
    "pdf_url": "https://arxiv.org/pdf/2304.03101v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2304.03101",
    "arxiv_authors": [
      "Lydia A. Schoenpflug",
      "Maxime W. Lafarge",
      "Anja L. Frei",
      "Viktor H. Koelzer"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Multi-task+learning+for+tissue+segmentation+and+tumor+detection+in+colorectal+cancer+histology+slides+Lydia+A.+Schoenpflug+Maxime+W.+Lafarge+Anja+L.+Frei+Viktor+H.+Koelzer",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "LA Schoenpflug",
        "id": "3ymrjx8AAAAJ"
      },
      {
        "name": "MW Lafarge",
        "id": "XyOz2lwAAAAJ"
      },
      {
        "name": "AL Frei",
        "id": null
      },
      {
        "name": "VH Koelzer",
        "id": "wNsW4soAAAAJ"
      }
    ],
    "citation_count": 2
  },
  {
    "arxiv_id": "2312.02980",
    "title": "GPT4Point: A Unified Framework for Point-Language Understanding and Generation",
    "year": 2023,
    "published": "2023-12-05T18:59:55Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Multimodal Large Language Models (MLLMs) have excelled in 2D image-text comprehension and image generation, but their understanding of the 3D world is notably deficient, limiting progress in 3D language understanding and generation. To solve this problem, we introduce GPT4Point, an innovative groundbreaking point-language multimodal model designed specifically for unified 3D object understanding and generation within the MLLM framework. GPT4Point as a powerful 3D MLLM seamlessly can execute a va",
    "arxiv_url": "https://arxiv.org/abs/2312.02980v2",
    "pdf_url": "https://arxiv.org/pdf/2312.02980v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2312.02980",
    "arxiv_authors": [
      "Zhangyang Qi",
      "Ye Fang",
      "Zeyi Sun",
      "Xiaoyang Wu",
      "Tong Wu",
      "Jiaqi Wang",
      "Dahua Lin",
      "Hengshuang Zhao"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=GPT4Point%3A+A+Unified+Framework+for+Point-Language+Understanding+and+Generation+Zhangyang+Qi+Ye+Fang+Zeyi+Sun+Xiaoyang+Wu+Tong+Wu",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "Z Qi",
        "id": "kwVLpo8AAAAJ"
      },
      {
        "name": "Y Fang",
        "id": "ujHiil8AAAAJ"
      },
      {
        "name": "Z Sun",
        "id": "RvGxDLUAAAAJ"
      },
      {
        "name": "X Wu",
        "id": "Np1dTpQAAAAJ"
      },
      {
        "name": "T Wu",
        "id": "cLUgV4YAAAAJ"
      },
      {
        "name": "J Wang",
        "id": "GDvt570AAAAJ"
      },
      {
        "name": "D Lin",
        "id": "GMzzRRUAAAAJ"
      },
      {
        "name": "H Zhao",
        "id": "4uE10I0AAAAJ"
      }
    ],
    "citation_count": 78
  },
  {
    "arxiv_id": "2404.04619",
    "title": "Do We Really Need a Complex Agent System? Distill Embodied Agent into a Single Model",
    "year": 2024,
    "published": "2024-04-06T12:51:00Z",
    "categories": [
      "cs.AI",
      "cs.CV"
    ],
    "abstract": "With the power of large language models (LLMs), open-ended embodied agents can flexibly understand human instructions, generate interpretable guidance strategies, and output executable actions. Nowadays, Multi-modal Language Models~(MLMs) integrate multi-modal signals into LLMs, further bringing richer perception to entity agents and allowing embodied agents to perceive world-understanding tasks more delicately. However, existing works: 1) operate independently by agents, each containing multipl",
    "arxiv_url": "https://arxiv.org/abs/2404.04619v1",
    "pdf_url": "https://arxiv.org/pdf/2404.04619v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2404.04619",
    "arxiv_authors": [
      "Zhonghan Zhao",
      "Ke Ma",
      "Wenhao Chai",
      "Xuan Wang",
      "Kewei Chen",
      "Dongxu Guo",
      "Yanting Zhang",
      "Hongwei Wang",
      "Gaoang Wang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Do+We+Really+Need+a+Complex+Agent+System%3F+Distill+Embodied+Agent+into+a+Single+Model+Zhonghan+Zhao+Ke+Ma+Wenhao+Chai+Xuan+Wang+Kewei+Chen",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "Z Zhao",
        "id": "tGTa-EAAAAAJ"
      },
      {
        "name": "K Ma",
        "id": null
      },
      {
        "name": "W Chai",
        "id": "SL--7UMAAAAJ"
      },
      {
        "name": "X Wang",
        "id": null
      },
      {
        "name": "K Chen",
        "id": null
      },
      {
        "name": "D Guo",
        "id": "VRpWW8sAAAAJ"
      },
      {
        "name": "Y Zhang",
        "id": "g0tWOiIAAAAJ"
      },
      {
        "name": "H Wang",
        "id": "lFbTT5AAAAAJ"
      },
      {
        "name": "G Wang",
        "id": "GhsXNiwAAAAJ"
      }
    ],
    "citation_count": 21
  },
  {
    "arxiv_id": "2504.20405",
    "title": "SCOPE-MRI: Bankart Lesion Detection as a Case Study in Data Curation and Deep Learning for Challenging Diagnoses",
    "year": 2025,
    "published": "2025-04-29T04:02:44Z",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "While deep learning has shown strong performance in musculoskeletal imaging, existing work has largely focused on pathologies where diagnosis is not a clinical challenge, leaving more difficult problems underexplored, such as detecting Bankart lesions (anterior-inferior glenoid labral tears) on standard MRIs. Diagnosing these lesions is challenging due to their subtle imaging features, often leading to reliance on invasive MRI arthrograms (MRAs). This study introduces ScopeMRI, the first publicl",
    "arxiv_url": "https://arxiv.org/abs/2504.20405v1",
    "pdf_url": "https://arxiv.org/pdf/2504.20405v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2504.20405",
    "arxiv_authors": [
      "Sahil Sethi",
      "Sai Reddy",
      "Mansi Sakarvadia",
      "Jordan Serotte",
      "Darlington Nwaudo",
      "Nicholas Maassen",
      "Lewis Shi"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=SCOPE-MRI%3A+Bankart+Lesion+Detection+as+a+Case+Study+in+Data+Curation+and+Deep+Learning+for+Challenging+Diagnoses+Sahil+Sethi+Sai+Reddy+Mansi+Sakarvadia+Jordan+Serotte+Darlington+Nwaudo",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "S Sethi",
        "id": "-fIpotUAAAAJ"
      },
      {
        "name": "S Reddy",
        "id": null
      },
      {
        "name": "M Sakarvadia",
        "id": "MTZehsUAAAAJ"
      },
      {
        "name": "J Serotte",
        "id": null
      },
      {
        "name": "D Nwaudo",
        "id": null
      },
      {
        "name": "N Maassen",
        "id": "Crg9nTQAAAAJ"
      },
      {
        "name": "L Shi",
        "id": "q061KVoAAAAJ"
      }
    ],
    "citation_count": null
  },
  {
    "arxiv_id": "2312.13162",
    "title": "Brain-Inspired Visual Odometry: Balancing Speed and Interpretability through a System of Systems Approach",
    "year": 2023,
    "published": "2023-12-20T16:23:48Z",
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "abstract": "In this study, we address the critical challenge of balancing speed and accuracy while maintaining interpretablity in visual odometry (VO) systems, a pivotal aspect in the field of autonomous navigation and robotics. Traditional VO systems often face a trade-off between computational speed and the precision of pose estimation. To tackle this issue, we introduce an innovative system that synergistically combines traditional VO methods with a specifically tailored fully connected network (FCN). Ou",
    "arxiv_url": "https://arxiv.org/abs/2312.13162v1",
    "pdf_url": "https://arxiv.org/pdf/2312.13162v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2312.13162",
    "arxiv_authors": [
      "Habib Boloorchi Tabrizi",
      "Christopher Crick"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Brain-Inspired+Visual+Odometry%3A+Balancing+Speed+and+Interpretability+through+a+System+of+Systems+Approach+Habib+Boloorchi+Tabrizi+Christopher+Crick",
    "gs_search_success": false,
    "gs_authors": null,
    "citation_count": null,
    "error_type": "multiple_matches"
  },
  {
    "arxiv_id": "2304.12294",
    "title": "Explicit Correspondence Matching for Generalizable Neural Radiance Fields",
    "year": 2023,
    "published": "2023-04-24T17:46:01Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "We present a new generalizable NeRF method that is able to directly generalize to new unseen scenarios and perform novel view synthesis with as few as two source views. The key to our approach lies in the explicitly modeled correspondence matching information, so as to provide the geometry prior to the prediction of NeRF color and density for volume rendering. The explicit correspondence matching is quantified with the cosine similarity between image features sampled at the 2D projections of a 3",
    "arxiv_url": "https://arxiv.org/abs/2304.12294v2",
    "pdf_url": "https://arxiv.org/pdf/2304.12294v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2304.12294",
    "arxiv_authors": [
      "Yuedong Chen",
      "Haofei Xu",
      "Qianyi Wu",
      "Chuanxia Zheng",
      "Tat-Jen Cham",
      "Jianfei Cai"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Explicit+Correspondence+Matching+for+Generalizable+Neural+Radiance+Fields+Yuedong+Chen+Haofei+Xu+Qianyi+Wu+Chuanxia+Zheng+Tat-Jen+Cham",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "Y Chen",
        "id": "GqgGZlQAAAAJ"
      },
      {
        "name": "H Xu",
        "id": "NhUwq_8AAAAJ"
      },
      {
        "name": "Q Wu",
        "id": "XI0RtesAAAAJ"
      },
      {
        "name": "C Zheng",
        "id": "mvpE6bIAAAAJ"
      },
      {
        "name": "TJ Cham",
        "id": "Lx3X7W0AAAAJ"
      },
      {
        "name": "J CaiIEEE Transactions on Pattern Analysis and Machine Intelligence",
        "id": null
      }
    ],
    "citation_count": 65
  },
  {
    "arxiv_id": "2301.02307",
    "title": "What You Say Is What You Show: Visual Narration Detection in Instructional Videos",
    "year": 2023,
    "published": "2023-01-05T21:43:19Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Narrated ''how-to'' videos have emerged as a promising data source for a wide range of learning problems, from learning visual representations to training robot policies. However, this data is extremely noisy, as the narrations do not always describe the actions demonstrated in the video. To address this problem we introduce the novel task of visual narration detection, which entails determining whether a narration is visually depicted by the actions in the video. We propose What You Say is What",
    "arxiv_url": "https://arxiv.org/abs/2301.02307v2",
    "pdf_url": "https://arxiv.org/pdf/2301.02307v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2301.02307",
    "arxiv_authors": [
      "Kumar Ashutosh",
      "Rohit Girdhar",
      "Lorenzo Torresani",
      "Kristen Grauman"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=What+You+Say+Is+What+You+Show%3A+Visual+Narration+Detection+in+Instructional+Videos+Kumar+Ashutosh+Rohit+Girdhar+Lorenzo+Torresani+Kristen+Grauman",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "K Ashutosh",
        "id": "GDqE4f8AAAAJ"
      },
      {
        "name": "R Girdhar",
        "id": "7cuwdr8AAAAJ"
      },
      {
        "name": "L Torresani",
        "id": "ss8KR5gAAAAJ"
      },
      {
        "name": "K Grauman",
        "id": "Jp6Mz1sAAAAJ"
      }
    ],
    "citation_count": 5
  },
  {
    "arxiv_id": "2503.13947",
    "title": "Conformal Prediction and MLLM aided Uncertainty Quantification in Scene Graph Generation",
    "year": 2025,
    "published": "2025-03-18T06:27:57Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Scene Graph Generation (SGG) aims to represent visual scenes by identifying objects and their pairwise relationships, providing a structured understanding of image content. However, inherent challenges like long-tailed class distributions and prediction variability necessitate uncertainty quantification in SGG for its practical viability. In this paper, we introduce a novel Conformal Prediction (CP) based framework, adaptive to any existing SGG method, for quantifying their predictive uncertaint",
    "arxiv_url": "https://arxiv.org/abs/2503.13947v2",
    "pdf_url": "https://arxiv.org/pdf/2503.13947v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2503.13947",
    "arxiv_authors": [
      "Sayak Nag",
      "Udita Ghosh",
      "Calvin-Khang Ta",
      "Sarosij Bose",
      "Jiachen Li",
      "Amit K Roy Chowdhury"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Conformal+Prediction+and+MLLM+aided+Uncertainty+Quantification+in+Scene+Graph+Generation+Sayak+Nag+Udita+Ghosh+Calvin-Khang+Ta+Sarosij+Bose+Jiachen+Li",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "S Nag",
        "id": "cmK8g-oAAAAJ"
      },
      {
        "name": "U Ghosh",
        "id": "uzrOl-sAAAAJ"
      },
      {
        "name": "CK Ta",
        "id": "uuOml7gAAAAJ"
      },
      {
        "name": "S Bose",
        "id": "AxWd_loAAAAJ"
      },
      {
        "name": "J Li",
        "id": "1_f79vUAAAAJ"
      },
      {
        "name": "AK Roy-Chowdhury",
        "id": "hfgwx0oAAAAJ"
      }
    ],
    "citation_count": 2
  },
  {
    "arxiv_id": "2310.04837",
    "title": "Federated Self-Supervised Learning of Monocular Depth Estimators for Autonomous Vehicles",
    "year": 2023,
    "published": "2023-10-07T14:54:02Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.DC"
    ],
    "abstract": "Image-based depth estimation has gained significant attention in recent research on computer vision for autonomous vehicles in intelligent transportation systems. This focus stems from its cost-effectiveness and wide range of potential applications. Unlike binocular depth estimation methods that require two fixed cameras, monocular depth estimation methods only rely on a single camera, making them highly versatile. While state-of-the-art approaches for this task leverage self-supervised learning",
    "arxiv_url": "https://arxiv.org/abs/2310.04837v1",
    "pdf_url": "https://arxiv.org/pdf/2310.04837v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2310.04837",
    "arxiv_authors": [
      "Elton F. de S. Soares",
      "Carlos Alberto V. Campos"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Federated+Self-Supervised+Learning+of+Monocular+Depth+Estimators+for+Autonomous+Vehicles+Elton+F.+de+S.+Soares+Carlos+Alberto+V.+Campos",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "EFS Soares",
        "id": "BLpz8M4AAAAJ"
      },
      {
        "name": "CAV Campos -",
        "id": null
      }
    ],
    "citation_count": 4
  },
  {
    "arxiv_id": "2502.00156",
    "title": "ALBAR: Adversarial Learning approach to mitigate Biases in Action Recognition",
    "year": 2025,
    "published": "2025-01-31T20:47:06Z",
    "categories": [
      "cs.CV",
      "cs.CR"
    ],
    "abstract": "Bias in machine learning models can lead to unfair decision making, and while it has been well-studied in the image and text domains, it remains underexplored in action recognition. Action recognition models often suffer from background bias (i.e., inferring actions based on background cues) and foreground bias (i.e., relying on subject appearance), which can be detrimental to real-life applications such as autonomous vehicles or assisted living monitoring. While prior approaches have mainly foc",
    "arxiv_url": "https://arxiv.org/abs/2502.00156v2",
    "pdf_url": "https://arxiv.org/pdf/2502.00156v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2502.00156",
    "arxiv_authors": [
      "Joseph Fioresi",
      "Ishan Rajendrakumar Dave",
      "Mubarak Shah"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=ALBAR%3A+Adversarial+Learning+approach+to+mitigate+Biases+in+Action+Recognition+Joseph+Fioresi+Ishan+Rajendrakumar+Dave+Mubarak+Shah",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "J Fioresi",
        "id": "fSE6-0IAAAAJ"
      },
      {
        "name": "IR Dave",
        "id": "fWu6sFgAAAAJ"
      },
      {
        "name": "M Shah -",
        "id": null
      }
    ],
    "citation_count": 3
  },
  {
    "arxiv_id": "2312.08230",
    "title": "Partial Symmetry Detection for 3D Geometry using Contrastive Learning with Geodesic Point Cloud Patches",
    "year": 2023,
    "published": "2023-12-13T15:48:50Z",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "Symmetry detection, especially partial and extrinsic symmetry, is essential for various downstream tasks, like 3D geometry completion, segmentation, compression and structure-aware shape encoding or generation. In order to detect partial extrinsic symmetries, we propose to learn rotation, reflection, translation and scale invariant local shape features for geodesic point cloud patches via contrastive learning, which are robust across multiple classes and generalize over different datasets. We sh",
    "arxiv_url": "https://arxiv.org/abs/2312.08230v1",
    "pdf_url": "https://arxiv.org/pdf/2312.08230v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2312.08230",
    "arxiv_authors": [
      "Gregor Kobsik",
      "Isaak Lim",
      "Leif Kobbelt"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Partial+Symmetry+Detection+for+3D+Geometry+using+Contrastive+Learning+with+Geodesic+Point+Cloud+Patches+Gregor+Kobsik+Isaak+Lim+Leif+Kobbelt",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "G Kobsik",
        "id": "W2yNJ8cAAAAJ"
      },
      {
        "name": "I Lim",
        "id": "QonIdyUAAAAJ"
      },
      {
        "name": "L Kobbelt -",
        "id": null
      }
    ],
    "citation_count": 1
  },
  {
    "arxiv_id": "2505.06991",
    "title": "Technical Report for ICRA 2025 GOOSE 2D Semantic Segmentation Challenge: Leveraging Color Shift Correction, RoPE-Swin Backbone, and Quantile-based Label Denoising Strategy for Robust Outdoor Scene Understanding",
    "year": 2025,
    "published": "2025-05-11T14:35:06Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "This report presents our semantic segmentation framework developed by team ACVLAB for the ICRA 2025 GOOSE 2D Semantic Segmentation Challenge, which focuses on parsing outdoor scenes into nine semantic categories under real-world conditions. Our method integrates a Swin Transformer backbone enhanced with Rotary Position Embedding (RoPE) for improved spatial generalization, alongside a Color Shift Estimation-and-Correction module designed to compensate for illumination inconsistencies in natural e",
    "arxiv_url": "https://arxiv.org/abs/2505.06991v1",
    "pdf_url": "https://arxiv.org/pdf/2505.06991v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2505.06991",
    "arxiv_authors": [
      "Chih-Chung Hsu",
      "I-Hsuan Wu",
      "Wen-Hai Tseng",
      "Ching-Heng Cheng",
      "Ming-Hsuan Wu",
      "Jin-Hui Jiang",
      "Yu-Jou Hsiao"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Technical+Report+for+ICRA+2025+GOOSE+2D+Semantic+Segmentation+Challenge%3A+Leveraging+Color+Shift+Correction%2C+RoPE-Swin+Backbone%2C+and+Quantile-based+Label+Denoising+Strategy+for+Robust+Outdoor+Scene+Understanding+Chih-Chung+Hsu+I-Hsuan+Wu+Wen-Hai+Tseng+Ching-Heng+Cheng+Ming-Hsuan+Wu",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "CC Hsu",
        "id": "mIWRYc4AAAAJ"
      },
      {
        "name": "I Wu",
        "id": null
      },
      {
        "name": "WH Tseng",
        "id": null
      },
      {
        "name": "CH Cheng",
        "id": "2UmoEfcAAAAJ"
      },
      {
        "name": "MH Wu",
        "id": null
      }
    ],
    "citation_count": null
  },
  {
    "arxiv_id": "2501.05442",
    "title": "Progressive Growing of Video Tokenizers for Temporally Compact Latent Spaces",
    "year": 2025,
    "published": "2025-01-09T18:55:15Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "eess.IV"
    ],
    "abstract": "Video tokenizers are essential for latent video diffusion models, converting raw video data into spatiotemporally compressed latent spaces for efficient training. However, extending state-of-the-art video tokenizers to achieve a temporal compression ratio beyond 4x without increasing channel capacity poses significant challenges. In this work, we propose an alternative approach to enhance temporal compression. We find that the reconstruction quality of temporally subsampled videos from a low-com",
    "arxiv_url": "https://arxiv.org/abs/2501.05442v2",
    "pdf_url": "https://arxiv.org/pdf/2501.05442v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2501.05442",
    "arxiv_authors": [
      "Aniruddha Mahapatra",
      "Long Mai",
      "David Bourgin",
      "Yitian Zhang",
      "Feng Liu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Progressive+Growing+of+Video+Tokenizers+for+Temporally+Compact+Latent+Spaces+Aniruddha+Mahapatra+Long+Mai+David+Bourgin+Yitian+Zhang+Feng+Liu",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "A Mahapatra",
        "id": "p8Hdn7gAAAAJ"
      },
      {
        "name": "L Mai",
        "id": "QHCY1cgAAAAJ"
      },
      {
        "name": "D Bourgin",
        "id": "fwKsGokAAAAJ"
      },
      {
        "name": "Y Zhang",
        "id": null
      },
      {
        "name": "F Liu",
        "id": null
      }
    ],
    "citation_count": null
  },
  {
    "arxiv_id": "2407.04119",
    "title": "An Autoencoder Architecture for L-band Passive Microwave Retrieval of Landscape Freeze-Thaw Cycle",
    "year": 2024,
    "published": "2024-07-04T18:40:50Z",
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "abstract": "Estimating the landscape and soil freeze-thaw (FT) dynamics in the Northern Hemisphere is crucial for understanding permafrost response to global warming and changes in regional and global carbon budgets. A new framework is presented for surface FT-cycle retrievals using L-band microwave radiometry based on a deep convolutional autoencoder neural network. This framework defines the landscape FT-cycle retrieval as a time series anomaly detection problem considering the frozen states as normal and",
    "arxiv_url": "https://arxiv.org/abs/2407.04119v1",
    "pdf_url": "https://arxiv.org/pdf/2407.04119v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2407.04119",
    "arxiv_authors": [
      "Divya Kumawat",
      "Ardeshir Ebtehaj",
      "Xiaolan Xu",
      "Andreas Colliander",
      "Vipin Kumar"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=An+Autoencoder+Architecture+for+L-band+Passive+Microwave+Retrieval+of+Landscape+Freeze-Thaw+Cycle+Divya+Kumawat+Ardeshir+Ebtehaj+Xiaolan+Xu+Andreas+Colliander+Vipin+Kumar",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "D Kumawat",
        "id": "v--BLIAAAAAJ"
      },
      {
        "name": "A Ebtehaj",
        "id": "t6rmKg0AAAAJ"
      },
      {
        "name": "X Xu",
        "id": "TGNYXa8AAAAJ"
      },
      {
        "name": "A Colliander",
        "id": "Owwu1P4AAAAJ"
      },
      {
        "name": "V KumarIEEE Transactions on Geoscience and Remote Sensing",
        "id": null
      }
    ],
    "citation_count": 3
  },
  {
    "arxiv_id": "2309.01236",
    "title": "BodySLAM++: Fast and Tightly-Coupled Visual-Inertial Camera and Human Motion Tracking",
    "year": 2023,
    "published": "2023-09-03T18:09:37Z",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "abstract": "Robust, fast, and accurate human state - 6D pose and posture - estimation remains a challenging problem. For real-world applications, the ability to estimate the human state in real-time is highly desirable. In this paper, we present BodySLAM++, a fast, efficient, and accurate human and camera state estimation framework relying on visual-inertial data. BodySLAM++ extends an existing visual-inertial state estimation framework, OKVIS2, to solve the dual task of estimating camera and human states s",
    "arxiv_url": "https://arxiv.org/abs/2309.01236v1",
    "pdf_url": "https://arxiv.org/pdf/2309.01236v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2309.01236",
    "arxiv_authors": [
      "Dorian F. Henning",
      "Christopher Choi",
      "Simon Schaefer",
      "Stefan Leutenegger"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=BodySLAM%2B%2B%3A+Fast+and+Tightly-Coupled+Visual-Inertial+Camera+and+Human+Motion+Tracking+Dorian+F.+Henning+Christopher+Choi+Simon+Schaefer+Stefan+Leutenegger",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "DF Henning",
        "id": "Cx8QeakAAAAJ"
      },
      {
        "name": "C Choi",
        "id": "YEUDqnIAAAAJ"
      },
      {
        "name": "S Schaefer",
        "id": "arUOBcwAAAAJ"
      },
      {
        "name": "S Leutenegger2023 IEEE/RSJ International",
        "id": null
      }
    ],
    "citation_count": 17
  },
  {
    "arxiv_id": "2308.08154",
    "title": "Conditional Perceptual Quality Preserving Image Compression",
    "year": 2023,
    "published": "2023-08-16T05:57:09Z",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "abstract": "We propose conditional perceptual quality, an extension of the perceptual quality defined in \\citet{blau2018perception}, by conditioning it on user defined information. Specifically, we extend the original perceptual quality $d(p_{X},p_{\\hat{X}})$ to the conditional perceptual quality $d(p_{X|Y},p_{\\hat{X}|Y})$, where $X$ is the original image, $\\hat{X}$ is the reconstructed, $Y$ is side information defined by user and $d(.,.)$ is divergence. We show that conditional perceptual quality has simil",
    "arxiv_url": "https://arxiv.org/abs/2308.08154v1",
    "pdf_url": "https://arxiv.org/pdf/2308.08154v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2308.08154",
    "arxiv_authors": [
      "Tongda Xu",
      "Qian Zhang",
      "Yanghao Li",
      "Dailan He",
      "Zhe Wang",
      "Yuanyuan Wang",
      "Hongwei Qin",
      "Yan Wang",
      "Jingjing Liu",
      "Ya-Qin Zhang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Conditional+Perceptual+Quality+Preserving+Image+Compression+Tongda+Xu+Qian+Zhang+Yanghao+Li+Dailan+He+Zhe+Wang",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "T Xu",
        "id": "LO8GS7sAAAAJ"
      },
      {
        "name": "Q Zhang",
        "id": null
      },
      {
        "name": "Y Li",
        "id": null
      },
      {
        "name": "D He",
        "id": "f5MTTy4AAAAJ"
      },
      {
        "name": "Z Wang",
        "id": null
      },
      {
        "name": "Y Wang",
        "id": "QOZnsYYAAAAJ"
      },
      {
        "name": "H Qin",
        "id": "ZGM7HfgAAAAJ"
      },
      {
        "name": "Y Wang",
        "id": "QOZnsYYAAAAJ"
      },
      {
        "name": "J Liu",
        "id": null
      },
      {
        "name": "YQ Zhang",
        "id": "mDOMfxIAAAAJ"
      }
    ],
    "citation_count": 8
  },
  {
    "arxiv_id": "2301.04634",
    "title": "Street-View Image Generation from a Bird's-Eye View Layout",
    "year": 2023,
    "published": "2023-01-11T18:39:34Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Bird's-Eye View (BEV) Perception has received increasing attention in recent years as it provides a concise and unified spatial representation across views and benefits a diverse set of downstream driving applications. At the same time, data-driven simulation for autonomous driving has been a focal point of recent research but with few approaches that are both fully data-driven and controllable. Instead of using perception data from real-life scenarios, an ideal model for simulation would genera",
    "arxiv_url": "https://arxiv.org/abs/2301.04634v4",
    "pdf_url": "https://arxiv.org/pdf/2301.04634v4",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2301.04634",
    "arxiv_authors": [
      "Alexander Swerdlow",
      "Runsheng Xu",
      "Bolei Zhou"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Street-View+Image+Generation+from+a+Bird%27s-Eye+View+Layout+Alexander+Swerdlow+Runsheng+Xu+Bolei+Zhou",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "A Swerdlow",
        "id": "--NjJhoAAAAJ"
      },
      {
        "name": "R Xu",
        "id": "QW6Ro8IAAAAJ"
      },
      {
        "name": "B Zhou - IEEE Robotics and Automation",
        "id": null
      }
    ],
    "citation_count": 101
  },
  {
    "arxiv_id": "2412.16901",
    "title": "Learning to Generate Gradients for Test-Time Adaptation via Test-Time Training Layers",
    "year": 2024,
    "published": "2024-12-22T07:24:09Z",
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "abstract": "Test-time adaptation (TTA) aims to fine-tune a trained model online using unlabeled testing data to adapt to new environments or out-of-distribution data, demonstrating broad application potential in real-world scenarios. However, in this optimization process, unsupervised learning objectives like entropy minimization frequently encounter noisy learning signals. These signals produce unreliable gradients, which hinder the model ability to converge to an optimal solution quickly and introduce sig",
    "arxiv_url": "https://arxiv.org/abs/2412.16901v1",
    "pdf_url": "https://arxiv.org/pdf/2412.16901v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2412.16901",
    "arxiv_authors": [
      "Qi Deng",
      "Shuaicheng Niu",
      "Ronghao Zhang",
      "Yaofo Chen",
      "Runhao Zeng",
      "Jian Chen",
      "Xiping Hu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Learning+to+Generate+Gradients+for+Test-Time+Adaptation+via+Test-Time+Training+Layers+Qi+Deng+Shuaicheng+Niu+Ronghao+Zhang+Yaofo+Chen+Runhao+Zeng",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "Q Deng",
        "id": "s3X4YHwAAAAJ"
      },
      {
        "name": "S Niu",
        "id": "gi30SZ0AAAAJ"
      },
      {
        "name": "R Zhang",
        "id": null
      },
      {
        "name": "Y Chen",
        "id": "NHZCt2EAAAAJ"
      },
      {
        "name": "R Zeng",
        "id": null
      },
      {
        "name": "J Chen",
        "id": null
      },
      {
        "name": "X Hu",
        "id": "Mc93YSEAAAAJ"
      }
    ],
    "citation_count": 7
  },
  {
    "arxiv_id": "2403.15675",
    "title": "An active learning model to classify animal species in Hong Kong",
    "year": 2024,
    "published": "2024-03-23T01:42:19Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Camera traps are used by ecologists globally as an efficient and non-invasive method to monitor animals. While it is time-consuming to manually label the collected images, recent advances in deep learning and computer vision has made it possible to automating this process [1]. A major obstacle to this is the generalisability of these models when applying these images to independently collected data from other parts of the world [2]. Here, we use a deep active learning workflow [3], and train a m",
    "arxiv_url": "https://arxiv.org/abs/2403.15675v1",
    "pdf_url": "https://arxiv.org/pdf/2403.15675v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2403.15675",
    "arxiv_authors": [
      "Gareth Lamb",
      "Ching Hei Lo",
      "Jin Wu",
      "Calvin K. F. Lee"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=An+active+learning+model+to+classify+animal+species+in+Hong+Kong+Gareth+Lamb+Ching+Hei+Lo+Jin+Wu+Calvin+K.+F.+Lee",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "G Lamb",
        "id": null
      },
      {
        "name": "CH Lo",
        "id": null
      },
      {
        "name": "J Wu",
        "id": null
      },
      {
        "name": "CKF Lee -",
        "id": null
      }
    ],
    "citation_count": null
  },
  {
    "arxiv_id": "2311.17267",
    "title": "E-ViLM: Efficient Video-Language Model via Masked Video Modeling with Semantic Vector-Quantized Tokenizer",
    "year": 2023,
    "published": "2023-11-28T22:57:17Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "To build scalable models for challenging real-world tasks, it is important to learn from diverse, multi-modal data in various forms (e.g., videos, text, and images). Among the existing works, a plethora of them have focused on leveraging large but cumbersome cross-modal architectures. Regardless of their effectiveness, larger architectures unavoidably prevent the models from being extended to real-world applications, so building a lightweight VL architecture and an efficient learning schema is o",
    "arxiv_url": "https://arxiv.org/abs/2311.17267v1",
    "pdf_url": "https://arxiv.org/pdf/2311.17267v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2311.17267",
    "arxiv_authors": [
      "Jacob Zhiyuan Fang",
      "Skyler Zheng",
      "Vasu Sharma",
      "Robinson Piramuthu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=E-ViLM%3A+Efficient+Video-Language+Model+via+Masked+Video+Modeling+with+Semantic+Vector-Quantized+Tokenizer+Jacob+Zhiyuan+Fang+Skyler+Zheng+Vasu+Sharma+Robinson+Piramuthu",
    "gs_search_success": false,
    "gs_authors": null,
    "citation_count": null,
    "error_type": "multiple_matches"
  },
  {
    "arxiv_id": "2404.14657",
    "title": "Progressive Token Length Scaling in Transformer Encoders for Efficient Universal Segmentation",
    "year": 2024,
    "published": "2024-04-23T01:34:20Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "A powerful architecture for universal segmentation relies on transformers that encode multi-scale image features and decode object queries into mask predictions. With efficiency being a high priority for scaling such models, we observed that the state-of-the-art method Mask2Former uses 50% of its compute only on the transformer encoder. This is due to the retention of a full-length token-level representation of all backbone feature scales at each encoder layer. With this observation, we propose ",
    "arxiv_url": "https://arxiv.org/abs/2404.14657v3",
    "pdf_url": "https://arxiv.org/pdf/2404.14657v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2404.14657",
    "arxiv_authors": [
      "Abhishek Aich",
      "Yumin Suh",
      "Samuel Schulter",
      "Manmohan Chandraker"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Progressive+Token+Length+Scaling+in+Transformer+Encoders+for+Efficient+Universal+Segmentation+Abhishek+Aich+Yumin+Suh+Samuel+Schulter+Manmohan+Chandraker",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "A Aich",
        "id": "8GhG46cAAAAJ"
      },
      {
        "name": "Y Suh",
        "id": "a9k4nwQAAAAJ"
      },
      {
        "name": "S Schulter",
        "id": "VQ6dsFEAAAAJ"
      },
      {
        "name": "M Chandraker -",
        "id": null
      }
    ],
    "citation_count": 1
  },
  {
    "arxiv_id": "2401.01752",
    "title": "FullLoRA: Efficiently Boosting the Robustness of Pretrained Vision Transformers",
    "year": 2024,
    "published": "2024-01-03T14:08:39Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "In recent years, the Vision Transformer (ViT) model has gradually become mainstream in various computer vision tasks, and the robustness of the model has received increasing attention. However, existing large models tend to prioritize performance during training, potentially neglecting the robustness, which may lead to serious security concerns. In this paper, we establish a new challenge: exploring how to use a small number of additional parameters for adversarial finetuning to quickly and effe",
    "arxiv_url": "https://arxiv.org/abs/2401.01752v2",
    "pdf_url": "https://arxiv.org/pdf/2401.01752v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2401.01752",
    "arxiv_authors": [
      "Zheng Yuan",
      "Jie Zhang",
      "Shiguang Shan",
      "Xilin Chen"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=FullLoRA%3A+Efficiently+Boosting+the+Robustness+of+Pretrained+Vision+Transformers+Zheng+Yuan+Jie+Zhang+Shiguang+Shan+Xilin+Chen",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "Z Yuan",
        "id": "K1wH01cAAAAJ"
      },
      {
        "name": "J Zhang",
        "id": "hJAhF0sAAAAJ"
      },
      {
        "name": "S Shan",
        "id": "Vkzd7MIAAAAJ"
      },
      {
        "name": "X Chen - IEEE Transactions on",
        "id": null
      }
    ],
    "citation_count": 10
  },
  {
    "arxiv_id": "2403.11695",
    "title": "TrajectoryNAS: A Neural Architecture Search for Trajectory Prediction",
    "year": 2024,
    "published": "2024-03-18T11:48:41Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Autonomous driving systems are a rapidly evolving technology that enables driverless car production. Trajectory prediction is a critical component of autonomous driving systems, enabling cars to anticipate the movements of surrounding objects for safe navigation. Trajectory prediction using Lidar point-cloud data performs better than 2D images due to providing 3D information. However, processing point-cloud data is more complicated and time-consuming than 2D images. Hence, state-of-the-art 3D tr",
    "arxiv_url": "https://arxiv.org/abs/2403.11695v1",
    "pdf_url": "https://arxiv.org/pdf/2403.11695v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2403.11695",
    "arxiv_authors": [
      "Ali Asghar Sharifi",
      "Ali Zoljodi",
      "Masoud Daneshtalab"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=TrajectoryNAS%3A+A+Neural+Architecture+Search+for+Trajectory+Prediction+Ali+Asghar+Sharifi+Ali+Zoljodi+Masoud+Daneshtalab",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "AA Sharifi",
        "id": "a8JrKHoAAAAJ"
      },
      {
        "name": "A Zoljodi",
        "id": "YAyN2EsAAAAJ"
      },
      {
        "name": "M DaneshtalabSensors (Basel",
        "id": null
      },
      {
        "name": "Switzerland)",
        "id": null
      }
    ],
    "citation_count": 9
  },
  {
    "arxiv_id": "2406.15656",
    "title": "Self-Supervised Adversarial Diffusion Models for Fast MRI Reconstruction",
    "year": 2024,
    "published": "2024-06-21T21:22:17Z",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "abstract": "Purpose: To propose a self-supervised deep learning-based compressed sensing MRI (DL-based CS-MRI) method named \"Adaptive Self-Supervised Consistency Guided Diffusion Model (ASSCGD)\" to accelerate data acquisition without requiring fully sampled datasets. Materials and Methods: We used the fastMRI multi-coil brain axial T2-weighted (T2-w) dataset from 1,376 cases and single-coil brain quantitative magnetization prepared 2 rapid acquisition gradient echoes (MP2RAGE) T1 maps from 318 cases to trai",
    "arxiv_url": "https://arxiv.org/abs/2406.15656v2",
    "pdf_url": "https://arxiv.org/pdf/2406.15656v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2406.15656",
    "arxiv_authors": [
      "Mojtaba Safari",
      "Zach Eidex",
      "Shaoyan Pan",
      "Richard L. J. Qiu",
      "Xiaofeng Yang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Self-Supervised+Adversarial+Diffusion+Models+for+Fast+MRI+Reconstruction+Mojtaba+Safari+Zach+Eidex+Shaoyan+Pan+Richard+L.+J.+Qiu+Xiaofeng+Yang",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "M Safari",
        "id": "-G5yTDgAAAAJ"
      },
      {
        "name": "Z Eidex",
        "id": "VOiEZSEAAAAJ"
      },
      {
        "name": "S Pan",
        "id": "y9mhuJoAAAAJ"
      },
      {
        "name": "RLJ Qiu",
        "id": "iHWtuZcAAAAJ"
      },
      {
        "name": "X Yang - Medical Physics",
        "id": null
      }
    ],
    "citation_count": 7
  },
  {
    "arxiv_id": "2504.17364",
    "title": "I-INR: Iterative Implicit Neural Representations",
    "year": 2025,
    "published": "2025-04-24T08:27:22Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Implicit Neural Representations (INRs) have revolutionized signal processing and computer vision by modeling signals as continuous, differentiable functions parameterized by neural networks. However, their inherent formulation as a regression problem makes them prone to regression to the mean, limiting their ability to capture fine details, retain high-frequency information, and handle noise effectively. To address these challenges, we propose Iterative Implicit Neural Representations (I-INRs) a",
    "arxiv_url": "https://arxiv.org/abs/2504.17364v2",
    "pdf_url": "https://arxiv.org/pdf/2504.17364v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2504.17364",
    "arxiv_authors": [
      "Ali Haider",
      "Muhammad Salman Ali",
      "Maryam Qamar",
      "Tahir Khalil",
      "Soo Ye Kim",
      "Jihyong Oh",
      "Enzo Tartaglione",
      "Sung-Ho Bae"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=I-INR%3A+Iterative+Implicit+Neural+Representations+Ali+Haider+Muhammad+Salman+Ali+Maryam+Qamar+Tahir+Khalil+Soo+Ye+Kim",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "A Haider",
        "id": "-ADiGUAAAAAJ"
      },
      {
        "name": "MS Ali",
        "id": "qbreZUIAAAAJ"
      },
      {
        "name": "M Qamar",
        "id": "O5BOfCoAAAAJ"
      },
      {
        "name": "T Khalil",
        "id": null
      },
      {
        "name": "SY Kim",
        "id": "o7R_Jj8AAAAJ"
      },
      {
        "name": "J Oh",
        "id": "D5yPXMsAAAAJ"
      },
      {
        "name": "E Tartaglione",
        "id": "uKuvN64AAAAJ"
      },
      {
        "name": "SH Bae",
        "id": "EULut5oAAAAJ"
      }
    ],
    "citation_count": null
  },
  {
    "arxiv_id": "2505.23272",
    "title": "Are MLMs Trapped in the Visual Room?",
    "year": 2025,
    "published": "2025-05-29T09:20:12Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Can multi-modal large models (MLMs) that can ``see'' an image be said to ``understand'' it? Drawing inspiration from Searle's Chinese Room, we propose the \\textbf{Visual Room} argument: a system may process and describe every detail of visual inputs by following algorithmic rules, without genuinely comprehending the underlying intention. This dilemma challenges the prevailing assumption that perceptual mastery implies genuine understanding. In implementation, we introduce a two-tier evaluation f",
    "arxiv_url": "https://arxiv.org/abs/2505.23272v2",
    "pdf_url": "https://arxiv.org/pdf/2505.23272v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2505.23272",
    "arxiv_authors": [
      "Yazhou Zhang",
      "Chunwang Zou",
      "Qimeng Liu",
      "Lu Rong",
      "Ben Yao",
      "Zheng Lian",
      "Qiuchi Li",
      "Peng Zhang",
      "Jing Qin"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Are+MLMs+Trapped+in+the+Visual+Room%3F+Yazhou+Zhang+Chunwang+Zou+Qimeng+Liu+Lu+Rong+Ben+Yao",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "Y Zhang",
        "id": "OeINnp8AAAAJ"
      },
      {
        "name": "C Zou",
        "id": null
      },
      {
        "name": "Q Liu",
        "id": null
      },
      {
        "name": "L Rong",
        "id": null
      },
      {
        "name": "B Yao",
        "id": null
      },
      {
        "name": "Z Lian",
        "id": "S34nWz0AAAAJ"
      },
      {
        "name": "Q Li",
        "id": null
      },
      {
        "name": "P Zhang",
        "id": null
      },
      {
        "name": "J Qin",
        "id": null
      }
    ],
    "citation_count": 2
  },
  {
    "arxiv_id": "2411.06696",
    "title": "S√©paration en composantes structures, textures et bruit d'une image, apport de l'utilisation des contourlettes",
    "year": 2024,
    "published": "2024-11-11T03:35:38Z",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "abstract": "In this paper, we propose to improve image decomposition algorithms in the case of noisy images. In \\cite{gilles1,aujoluvw}, the authors propose to separate structures, textures and noise from an image. Unfortunately, the use of separable wavelets shows some artefacts. In this paper, we propose to replace the wavelet transform by the contourlet transform which better approximate geometry in images. For that, we define contourlet spaces and their associated norms. Then, we get an iterative algori",
    "arxiv_url": "https://arxiv.org/abs/2411.06696v1",
    "pdf_url": "https://arxiv.org/pdf/2411.06696v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2411.06696",
    "arxiv_authors": [
      "Jerome Gilles"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=S%C3%A9paration+en+composantes+structures%2C+textures+et+bruit+d%27une+image%2C+apport+de+l%27utilisation+des+contourlettes+Jerome+Gilles",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "J Gilles -",
        "id": null
      }
    ],
    "citation_count": null
  },
  {
    "arxiv_id": "2412.18235",
    "title": "Band Prompting Aided SAR and Multi-Spectral Data Fusion Framework for Local Climate Zone Classification",
    "year": 2024,
    "published": "2024-12-24T07:40:07Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Local climate zone (LCZ) classification is of great value for understanding the complex interactions between urban development and local climate. Recent studies have increasingly focused on the fusion of synthetic aperture radar (SAR) and multi-spectral data to improve LCZ classification performance. However, it remains challenging due to the distinct physical properties of these two types of data and the absence of effective fusion guidance. In this paper, a novel band prompting aided data fusi",
    "arxiv_url": "https://arxiv.org/abs/2412.18235v1",
    "pdf_url": "https://arxiv.org/pdf/2412.18235v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2412.18235",
    "arxiv_authors": [
      "Haiyan Lan",
      "Shujun Li",
      "Mingjie Xie",
      "Xuanjia Zhao",
      "Hongning Liu",
      "Pengming Feng",
      "Dongli Xu",
      "Guangjun He",
      "Jian Guan"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Band+Prompting+Aided+SAR+and+Multi-Spectral+Data+Fusion+Framework+for+Local+Climate+Zone+Classification+Haiyan+Lan+Shujun+Li+Mingjie+Xie+Xuanjia+Zhao+Hongning+Liu",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "H Lan",
        "id": null
      },
      {
        "name": "S Li",
        "id": null
      },
      {
        "name": "M Xie",
        "id": "v6FaTbwAAAAJ"
      },
      {
        "name": "X Zhao",
        "id": null
      },
      {
        "name": "H Liu",
        "id": "1GXPr6kAAAAJ"
      },
      {
        "name": "P Feng",
        "id": null
      },
      {
        "name": "D Xu",
        "id": "BfZECLsAAAAJ"
      },
      {
        "name": "G He",
        "id": null
      },
      {
        "name": "J GuanICASSP",
        "id": null
      }
    ],
    "citation_count": null
  },
  {
    "arxiv_id": "2411.17491",
    "title": "What's in the Image? A Deep-Dive into the Vision of Vision Language Models",
    "year": 2024,
    "published": "2024-11-26T14:59:06Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Vision-Language Models (VLMs) have recently demonstrated remarkable capabilities in comprehending complex visual content. However, the mechanisms underlying how VLMs process visual information remain largely unexplored. In this paper, we conduct a thorough empirical analysis, focusing on attention modules across layers. We reveal several key insights about how these models process visual data: (i) the internal representation of the query tokens (e.g., representations of \"describe the image\"), is",
    "arxiv_url": "https://arxiv.org/abs/2411.17491v1",
    "pdf_url": "https://arxiv.org/pdf/2411.17491v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2411.17491",
    "arxiv_authors": [
      "Omri Kaduri",
      "Shai Bagon",
      "Tali Dekel"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=What%27s+in+the+Image%3F+A+Deep-Dive+into+the+Vision+of+Vision+Language+Models+Omri+Kaduri+Shai+Bagon+Tali+Dekel",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "O Kaduri",
        "id": null
      },
      {
        "name": "S Bagon",
        "id": "N_maL_AAAAAJ"
      },
      {
        "name": "T Dekel -",
        "id": null
      }
    ],
    "citation_count": 18
  },
  {
    "arxiv_id": "2406.18430",
    "title": "Facial Image Feature Analysis and its Specialization for Fr√©chet Distance and Neighborhoods",
    "year": 2024,
    "published": "2024-06-26T15:27:26Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Assessing distances between images and image datasets is a fundamental task in vision-based research. It is a challenging open problem in the literature and despite the criticism it receives, the most ubiquitous method remains the Fr√©chet Inception Distance. The Inception network is trained on a specific labeled dataset, ImageNet, which has caused the core of its criticism in the most recent research. Improvements were shown by moving to self-supervision learning over ImageNet, leaving the train",
    "arxiv_url": "https://arxiv.org/abs/2406.18430v1",
    "pdf_url": "https://arxiv.org/pdf/2406.18430v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2406.18430",
    "arxiv_authors": [
      "Doruk Cetin",
      "Benedikt Schesch",
      "Petar Stamenkovic",
      "Niko Benjamin Huber",
      "Fabio Z√ºnd",
      "Majed El Helou"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Facial+Image+Feature+Analysis+and+its+Specialization+for+Fr%C3%A9chet+Distance+and+Neighborhoods+Doruk+Cetin+Benedikt+Schesch+Petar+Stamenkovic+Niko+Benjamin+Huber+Fabio+Z%C3%BCnd",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "D Cetin",
        "id": "xbYfOpAAAAAJ"
      },
      {
        "name": "B Schesch",
        "id": "CJCmPasAAAAJ"
      },
      {
        "name": "P Stamenkovic",
        "id": null
      },
      {
        "name": "NB Huber",
        "id": null
      },
      {
        "name": "F Z√ºnd",
        "id": "FVqI_6QAAAAJ"
      },
      {
        "name": "ME Helou",
        "id": "caOfhrkAAAAJ"
      }
    ],
    "citation_count": null
  },
  {
    "arxiv_id": "2311.17076",
    "title": "Compositional Chain-of-Thought Prompting for Large Multimodal Models",
    "year": 2023,
    "published": "2023-11-27T22:23:27Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "abstract": "The combination of strong visual backbones and Large Language Model (LLM) reasoning has led to Large Multimodal Models (LMMs) becoming the current standard for a wide range of vision and language (VL) tasks. However, recent research has shown that even the most advanced LMMs still struggle to capture aspects of compositional visual reasoning, such as attributes and relationships between objects. One solution is to utilize scene graphs (SGs)--a formalization of objects and their relations and att",
    "arxiv_url": "https://arxiv.org/abs/2311.17076v3",
    "pdf_url": "https://arxiv.org/pdf/2311.17076v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2311.17076",
    "arxiv_authors": [
      "Chancharik Mitra",
      "Brandon Huang",
      "Trevor Darrell",
      "Roei Herzig"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Compositional+Chain-of-Thought+Prompting+for+Large+Multimodal+Models+Chancharik+Mitra+Brandon+Huang+Trevor+Darrell+Roei+Herzig",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "C Mitra",
        "id": "k85XvocAAAAJ"
      },
      {
        "name": "B Huang",
        "id": "dyD6nsgAAAAJ"
      },
      {
        "name": "T Darrell",
        "id": "bh-uRFMAAAAJ"
      },
      {
        "name": "R Herzig",
        "id": "6Q-289IAAAAJ"
      }
    ],
    "citation_count": 223
  },
  {
    "arxiv_id": "2304.11670",
    "title": "Evading DeepFake Detectors via Adversarial Statistical Consistency",
    "year": 2023,
    "published": "2023-04-23T14:40:42Z",
    "categories": [
      "cs.CV",
      "eess.IV"
    ],
    "abstract": "In recent years, as various realistic face forgery techniques known as DeepFake improves by leaps and bounds,more and more DeepFake detection techniques have been proposed. These methods typically rely on detecting statistical differences between natural (i.e., real) and DeepFakegenerated images in both spatial and frequency domains. In this work, we propose to explicitly minimize the statistical differences to evade state-of-the-art DeepFake detectors. To this end, we propose a statistical cons",
    "arxiv_url": "https://arxiv.org/abs/2304.11670v1",
    "pdf_url": "https://arxiv.org/pdf/2304.11670v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2304.11670",
    "arxiv_authors": [
      "Yang Hou",
      "Qing Guo",
      "Yihao Huang",
      "Xiaofei Xie",
      "Lei Ma",
      "Jianjun Zhao"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Evading+DeepFake+Detectors+via+Adversarial+Statistical+Consistency+Yang+Hou+Qing+Guo+Yihao+Huang+Xiaofei+Xie+Lei+Ma",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "Y Hou",
        "id": null
      },
      {
        "name": "Q Guo",
        "id": "Rj2x4QUAAAAJ"
      },
      {
        "name": "Y Huang",
        "id": null
      },
      {
        "name": "X Xie",
        "id": "FfcZfJgAAAAJ"
      },
      {
        "name": "L Ma",
        "id": "xsfGc58AAAAJ"
      },
      {
        "name": "J Zhao",
        "id": "PZZ0iygAAAAJ"
      }
    ],
    "citation_count": 85
  },
  {
    "arxiv_id": "2309.06877",
    "title": "Video Infringement Detection via Feature Disentanglement and Mutual Information Maximization",
    "year": 2023,
    "published": "2023-09-13T10:53:12Z",
    "categories": [
      "cs.CV",
      "cs.MM"
    ],
    "abstract": "The self-media era provides us tremendous high quality videos. Unfortunately, frequent video copyright infringements are now seriously damaging the interests and enthusiasm of video creators. Identifying infringing videos is therefore a compelling task. Current state-of-the-art methods tend to simply feed high-dimensional mixed video features into deep neural networks and count on the networks to extract useful representations. Despite its simplicity, this paradigm heavily relies on the original",
    "arxiv_url": "https://arxiv.org/abs/2309.06877v1",
    "pdf_url": "https://arxiv.org/pdf/2309.06877v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2309.06877",
    "arxiv_authors": [
      "Zhenguang Liu",
      "Xinyang Yu",
      "Ruili Wang",
      "Shuai Ye",
      "Zhe Ma",
      "Jianfeng Dong",
      "Sifeng He",
      "Feng Qian",
      "Xiaobo Zhang",
      "Roger Zimmermann",
      "Lei Yang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Video+Infringement+Detection+via+Feature+Disentanglement+and+Mutual+Information+Maximization+Zhenguang+Liu+Xinyang+Yu+Ruili+Wang+Shuai+Ye+Zhe+Ma",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "Z Liu",
        "id": "OP2ySB8AAAAJ"
      },
      {
        "name": "X Yu",
        "id": null
      },
      {
        "name": "R Wang",
        "id": "KoQTBi4AAAAJ"
      },
      {
        "name": "S Ye",
        "id": null
      },
      {
        "name": "Z Ma",
        "id": null
      },
      {
        "name": "J Dong",
        "id": "8-zdk9wAAAAJ"
      },
      {
        "name": "S He",
        "id": "r3xYLDAAAAAJ"
      },
      {
        "name": "F Qian",
        "id": "5KurcogAAAAJ"
      },
      {
        "name": "X Zhang",
        "id": null
      },
      {
        "name": "R Zimmermann",
        "id": "IDREwXEAAAAJ"
      },
      {
        "name": "L Yang",
        "id": "Xrk1gdYAAAAJ"
      }
    ],
    "citation_count": 3
  },
  {
    "arxiv_id": "2501.15774",
    "title": "Efficient Attention-Sharing Information Distillation Transformer for Lightweight Single Image Super-Resolution",
    "year": 2025,
    "published": "2025-01-27T04:46:58Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Transformer-based Super-Resolution (SR) methods have demonstrated superior performance compared to convolutional neural network (CNN)-based SR approaches due to their capability to capture long-range dependencies. However, their high computational complexity necessitates the development of lightweight approaches for practical use. To address this challenge, we propose the Attention-Sharing Information Distillation (ASID) network, a lightweight SR network that integrates attention-sharing and an ",
    "arxiv_url": "https://arxiv.org/abs/2501.15774v2",
    "pdf_url": "https://arxiv.org/pdf/2501.15774v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2501.15774",
    "arxiv_authors": [
      "Karam Park",
      "Jae Woong Soh",
      "Nam Ik Cho"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Efficient+Attention-Sharing+Information+Distillation+Transformer+for+Lightweight+Single+Image+Super-Resolution+Karam+Park+Jae+Woong+Soh+Nam+Ik+Cho",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "K Park",
        "id": null
      },
      {
        "name": "JW Soh",
        "id": "Wmhi6jgAAAAJ"
      },
      {
        "name": "NI Cho -",
        "id": null
      }
    ],
    "citation_count": 9
  },
  {
    "arxiv_id": "2502.00379",
    "title": "Latent Action Learning Requires Supervision in the Presence of Distractors",
    "year": 2025,
    "published": "2025-02-01T09:35:51Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "abstract": "Recently, latent action learning, pioneered by Latent Action Policies (LAPO), have shown remarkable pre-training efficiency on observation-only data, offering potential for leveraging vast amounts of video available on the web for embodied AI. However, prior work has focused on distractor-free data, where changes between observations are primarily explained by ground-truth actions. Unfortunately, real-world videos contain action-correlated distractors that may hinder latent action learning. Usin",
    "arxiv_url": "https://arxiv.org/abs/2502.00379v5",
    "pdf_url": "https://arxiv.org/pdf/2502.00379v5",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2502.00379",
    "arxiv_authors": [
      "Alexander Nikulin",
      "Ilya Zisman",
      "Denis Tarasov",
      "Nikita Lyubaykin",
      "Andrei Polubarov",
      "Igor Kiselev",
      "Vladislav Kurenkov"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Latent+Action+Learning+Requires+Supervision+in+the+Presence+of+Distractors+Alexander+Nikulin+Ilya+Zisman+Denis+Tarasov+Nikita+Lyubaykin+Andrei+Polubarov",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "A Nikulin",
        "id": null
      },
      {
        "name": "I Zisman",
        "id": "tmh78sQAAAAJ"
      },
      {
        "name": "D Tarasov",
        "id": "LQcCkD8AAAAJ"
      },
      {
        "name": "N Lyubaykin",
        "id": null
      },
      {
        "name": "A Polubarov",
        "id": null
      },
      {
        "name": "I Kiselev",
        "id": "403wLvkAAAAJ"
      },
      {
        "name": "V Kurenkov",
        "id": "w09vtVsAAAAJ"
      }
    ],
    "citation_count": 6
  },
  {
    "arxiv_id": "2505.05573",
    "title": "Prompt to Polyp: Medical Text-Conditioned Image Synthesis with Diffusion Models",
    "year": 2025,
    "published": "2025-05-08T18:07:16Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "The generation of realistic medical images from text descriptions has significant potential to address data scarcity challenges in healthcare AI while preserving patient privacy. This paper presents a comprehensive study of text-to-image synthesis in the medical domain, comparing two distinct approaches: (1) fine-tuning large pre-trained latent diffusion models and (2) training small, domain-specific models. We introduce a novel model named MSDM, an optimized architecture based on Stable Diffusi",
    "arxiv_url": "https://arxiv.org/abs/2505.05573v2",
    "pdf_url": "https://arxiv.org/pdf/2505.05573v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2505.05573",
    "arxiv_authors": [
      "Mikhail Chaichuk",
      "Sushant Gautam",
      "Steven Hicks",
      "Elena Tutubalina"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Prompt+to+Polyp%3A+Medical+Text-Conditioned+Image+Synthesis+with+Diffusion+Models+Mikhail+Chaichuk+Sushant+Gautam+Steven+Hicks+Elena+Tutubalina",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "M Chaichuk",
        "id": "XuBuv-AAAAAJ"
      },
      {
        "name": "S Gautam",
        "id": "t3Iie8cAAAAJ"
      },
      {
        "name": "S Hicks",
        "id": "2fVVFSwAAAAJ"
      },
      {
        "name": "E Tutubalina",
        "id": "npM9yekAAAAJ"
      }
    ],
    "citation_count": 2
  },
  {
    "arxiv_id": "2403.14324",
    "title": "Neural Network-Based Processing and Reconstruction of Compromised Biophotonic Image Data",
    "year": 2024,
    "published": "2024-03-21T11:44:25Z",
    "categories": [
      "physics.optics",
      "cs.CV",
      "cs.LG",
      "physics.app-ph"
    ],
    "abstract": "The integration of deep learning techniques with biophotonic setups has opened new horizons in bioimaging. A compelling trend in this field involves deliberately compromising certain measurement metrics to engineer better bioimaging tools in terms of cost, speed, and form-factor, followed by compensating for the resulting defects through the utilization of deep learning models trained on a large amount of ideal, superior or alternative data. This strategic approach has found increasing popularit",
    "arxiv_url": "https://arxiv.org/abs/2403.14324v1",
    "pdf_url": "https://arxiv.org/pdf/2403.14324v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2403.14324",
    "arxiv_authors": [
      "Michael John Fanous",
      "Paloma Casteleiro Costa",
      "Cagatay Isil",
      "Luzhe Huang",
      "Aydogan Ozcan"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Neural+Network-Based+Processing+and+Reconstruction+of+Compromised+Biophotonic+Image+Data+Michael+John+Fanous+Paloma+Casteleiro+Costa+Cagatay+Isil+Luzhe+Huang+Aydogan+Ozcan",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "MJ Fanous",
        "id": "VdtFVhYAAAAJ"
      },
      {
        "name": "P Casteleiro Costa",
        "id": "YMY_cssAAAAJ"
      },
      {
        "name": "√á I≈üƒ±l",
        "id": "xpNu7vMAAAAJ"
      },
      {
        "name": "L Huang",
        "id": "qq9rtkkAAAAJ"
      },
      {
        "name": "A OzcanLight: Science & Applications",
        "id": null
      }
    ],
    "citation_count": 12
  },
  {
    "arxiv_id": "2401.00523",
    "title": "Compressing Deep Image Super-resolution Models",
    "year": 2023,
    "published": "2023-12-31T15:38:50Z",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "abstract": "Deep learning techniques have been applied in the context of image super-resolution (SR), achieving remarkable advances in terms of reconstruction performance. Existing techniques typically employ highly complex model structures which result in large model sizes and slow inference speeds. This often leads to high energy consumption and restricts their adoption for practical applications. To address this issue, this work employs a three-stage workflow for compressing deep SR models which signific",
    "arxiv_url": "https://arxiv.org/abs/2401.00523v2",
    "pdf_url": "https://arxiv.org/pdf/2401.00523v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2401.00523",
    "arxiv_authors": [
      "Yuxuan Jiang",
      "Jakub Nawala",
      "Fan Zhang",
      "David Bull"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Compressing+Deep+Image+Super-resolution+Models+Yuxuan+Jiang+Jakub+Nawala+Fan+Zhang+David+Bull",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "Y Jiang",
        "id": "5C_qRnYAAAAJ"
      },
      {
        "name": "J Nawala",
        "id": "vYbSxIgAAAAJ"
      },
      {
        "name": "F Zhang",
        "id": "BBujJNcAAAAJ"
      },
      {
        "name": "D Bull -",
        "id": null
      }
    ],
    "citation_count": 10
  },
  {
    "arxiv_id": "2404.14016",
    "title": "Ungeneralizable Examples",
    "year": 2024,
    "published": "2024-04-22T09:29:14Z",
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "abstract": "The training of contemporary deep learning models heavily relies on publicly available data, posing a risk of unauthorized access to online data and raising concerns about data privacy. Current approaches to creating unlearnable data involve incorporating small, specially designed noises, but these methods strictly limit data usability, overlooking its potential usage in authorized scenarios. In this paper, we extend the concept of unlearnable data to conditional data learnability and introduce ",
    "arxiv_url": "https://arxiv.org/abs/2404.14016v1",
    "pdf_url": "https://arxiv.org/pdf/2404.14016v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2404.14016",
    "arxiv_authors": [
      "Jingwen Ye",
      "Xinchao Wang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Ungeneralizable+Examples+Jingwen+Ye+Xinchao+Wang",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "J Ye",
        "id": "8GQnNP0AAAAJ"
      },
      {
        "name": "X Wang -",
        "id": null
      }
    ],
    "citation_count": 9
  },
  {
    "arxiv_id": "2503.09124",
    "title": "AdvAD: Exploring Non-Parametric Diffusion for Imperceptible Adversarial Attacks",
    "year": 2025,
    "published": "2025-03-12T07:22:39Z",
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "abstract": "Imperceptible adversarial attacks aim to fool DNNs by adding imperceptible perturbation to the input data. Previous methods typically improve the imperceptibility of attacks by integrating common attack paradigms with specifically designed perception-based losses or the capabilities of generative models. In this paper, we propose Adversarial Attacks in Diffusion (AdvAD), a novel modeling framework distinct from existing attack paradigms. AdvAD innovatively conceptualizes attacking as a non-param",
    "arxiv_url": "https://arxiv.org/abs/2503.09124v1",
    "pdf_url": "https://arxiv.org/pdf/2503.09124v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2503.09124",
    "arxiv_authors": [
      "Jin Li",
      "Ziqiang He",
      "Anwei Luo",
      "Jian-Fang Hu",
      "Z. Jane Wang",
      "Xiangui Kang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=AdvAD%3A+Exploring+Non-Parametric+Diffusion+for+Imperceptible+Adversarial+Attacks+Jin+Li+Ziqiang+He+Anwei+Luo+Jian-Fang+Hu+Z.+Jane+Wang",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "J Li",
        "id": "t4QF1YQAAAAJ"
      },
      {
        "name": "Z He",
        "id": null
      },
      {
        "name": "A Luo",
        "id": "yM46Z_YAAAAJ"
      },
      {
        "name": "JF Hu",
        "id": null
      },
      {
        "name": "ZJ Wang",
        "id": "W75uTm8AAAAJ"
      },
      {
        "name": "X KangAdvances in Neural Information Processing Systems",
        "id": null
      }
    ],
    "citation_count": 6
  },
  {
    "arxiv_id": "2303.10800",
    "title": "A Global Model Approach to Robust Few-Shot SAR Automatic Target Recognition",
    "year": 2023,
    "published": "2023-03-20T00:24:05Z",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "In real-world scenarios, it may not always be possible to collect hundreds of labeled samples per class for training deep learning-based SAR Automatic Target Recognition (ATR) models. This work specifically tackles the few-shot SAR ATR problem, where only a handful of labeled samples may be available to support the task of interest. Our approach is composed of two stages. In the first, a global representation model is trained via self-supervised learning on a large pool of diverse and unlabeled ",
    "arxiv_url": "https://arxiv.org/abs/2303.10800v1",
    "pdf_url": "https://arxiv.org/pdf/2303.10800v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2303.10800",
    "arxiv_authors": [
      "Nathan Inkawhich"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=A+Global+Model+Approach+to+Robust+Few-Shot+SAR+Automatic+Target+Recognition+Nathan+Inkawhich",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "N Inkawhich - IEEE Geoscience and Remote Sensing Letters",
        "id": null
      }
    ],
    "citation_count": 22
  },
  {
    "arxiv_id": "2403.16020",
    "title": "PaPr: Training-Free One-Step Patch Pruning with Lightweight ConvNets for Faster Inference",
    "year": 2024,
    "published": "2024-03-24T05:50:00Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "As deep neural networks evolve from convolutional neural networks (ConvNets) to advanced vision transformers (ViTs), there is an increased need to eliminate redundant data for faster processing without compromising accuracy. Previous methods are often architecture-specific or necessitate re-training, restricting their applicability with frequent model updates. To solve this, we first introduce a novel property of lightweight ConvNets: their ability to identify key discriminative patch regions in",
    "arxiv_url": "https://arxiv.org/abs/2403.16020v2",
    "pdf_url": "https://arxiv.org/pdf/2403.16020v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2403.16020",
    "arxiv_authors": [
      "Tanvir Mahmud",
      "Burhaneddin Yaman",
      "Chun-Hao Liu",
      "Diana Marculescu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=PaPr%3A+Training-Free+One-Step+Patch+Pruning+with+Lightweight+ConvNets+for+Faster+Inference+Tanvir+Mahmud+Burhaneddin+Yaman+Chun-Hao+Liu+Diana+Marculescu",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "T Mahmud",
        "id": "4aZPxRsAAAAJ"
      },
      {
        "name": "B Yaman",
        "id": "0JS9ozcAAAAJ"
      },
      {
        "name": "CH Liu",
        "id": "NsyhKxoAAAAJ"
      },
      {
        "name": "D Marculescu - European",
        "id": null
      }
    ],
    "citation_count": 5
  },
  {
    "arxiv_id": "2502.01081",
    "title": "The Jumping Reasoning Curve? Tracking the Evolution of Reasoning Performance in GPT-[n] and o-[n] Models on Multimodal Puzzles",
    "year": 2025,
    "published": "2025-02-03T05:47:04Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "abstract": "The releases of OpenAI's o-[n] series, such as o1, o3, and o4-mini, mark a significant paradigm shift in Large Language Models towards advanced reasoning capabilities. Notably, models like o3 have demonstrated strong performance on benchmarks like the Abstraction and Reasoning Corpus for Artificial General Intelligence (ARC-AGI). However, this benchmark is limited to symbolic patterns, whereas humans often perceive and reason about multimodal scenarios involving both vision and language data. Th",
    "arxiv_url": "https://arxiv.org/abs/2502.01081v2",
    "pdf_url": "https://arxiv.org/pdf/2502.01081v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2502.01081",
    "arxiv_authors": [
      "Vernon Y. H. Toh",
      "Yew Ken Chia",
      "Deepanway Ghosal",
      "Soujanya Poria"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=The+Jumping+Reasoning+Curve%3F+Tracking+the+Evolution+of+Reasoning+Performance+in+GPT-%5Bn%5D+and+o-%5Bn%5D+Models+on+Multimodal+Puzzles+Vernon+Y.+H.+Toh+Yew+Ken+Chia+Deepanway+Ghosal+Soujanya+Poria",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "VYH Toh",
        "id": "ZUPDabgAAAAJ"
      },
      {
        "name": "YK Chia",
        "id": null
      },
      {
        "name": "D Ghosal",
        "id": "95YiIWUAAAAJ"
      },
      {
        "name": "S Poria -",
        "id": null
      }
    ],
    "citation_count": 3
  },
  {
    "arxiv_id": "2503.00515",
    "title": "Class-Independent Increment: An Efficient Approach for Multi-label Class-Incremental Learning",
    "year": 2025,
    "published": "2025-03-01T14:40:52Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Current research on class-incremental learning primarily focuses on single-label classification tasks. However, real-world applications often involve multi-label scenarios, such as image retrieval and medical imaging. Therefore, this paper focuses on the challenging yet practical multi-label class-incremental learning (MLCIL) problem. In addition to the challenge of catastrophic forgetting, MLCIL encounters issues related to feature confusion, encompassing inter-session and intra-feature confusi",
    "arxiv_url": "https://arxiv.org/abs/2503.00515v2",
    "pdf_url": "https://arxiv.org/pdf/2503.00515v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2503.00515",
    "arxiv_authors": [
      "Chenhao Ding",
      "Songlin Dong",
      "Zhengdong Zhou",
      "Jizhou Han",
      "Qiang Wang",
      "Yuhang He",
      "Yihong Gong"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Class-Independent+Increment%3A+An+Efficient+Approach+for+Multi-label+Class-Incremental+Learning+Chenhao+Ding+Songlin+Dong+Zhengdong+Zhou+Jizhou+Han+Qiang+Wang",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "S Dong",
        "id": "nD-QpuYAAAAJ"
      },
      {
        "name": "Y He",
        "id": "9VCIiVcAAAAJ"
      },
      {
        "name": "Z Zhou",
        "id": null
      },
      {
        "name": "H Luo",
        "id": "uwMZHX8AAAAJ"
      },
      {
        "name": "X Wei",
        "id": "KNyC5EUAAAAJ"
      },
      {
        "name": "AC Kot",
        "id": "UGZXLxIAAAAJ"
      },
      {
        "name": "Y Gong",
        "id": "x2xdU7gAAAAJ"
      }
    ],
    "citation_count": 2
  },
  {
    "arxiv_id": "2406.09850",
    "title": "GradeADreamer: Enhanced Text-to-3D Generation Using Gaussian Splatting and Multi-View Diffusion",
    "year": 2024,
    "published": "2024-06-14T08:58:28Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Text-to-3D generation has shown promising results, yet common challenges such as the Multi-face Janus problem and extended generation time for high-quality assets. In this paper, we address these issues by introducing a novel three-stage training pipeline called GradeADreamer. This pipeline is capable of producing high-quality assets with a total generation time of under 30 minutes using only a single RTX 3090 GPU. Our proposed method employs a Multi-view Diffusion Model, MVDream, to generate Ga",
    "arxiv_url": "https://arxiv.org/abs/2406.09850v1",
    "pdf_url": "https://arxiv.org/pdf/2406.09850v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2406.09850",
    "arxiv_authors": [
      "Trapoom Ukarapol",
      "Kevin Pruvost"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=GradeADreamer%3A+Enhanced+Text-to-3D+Generation+Using+Gaussian+Splatting+and+Multi-View+Diffusion+Trapoom+Ukarapol+Kevin+Pruvost",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "T Ukarapol",
        "id": null
      },
      {
        "name": "K Pruvost -",
        "id": null
      }
    ],
    "citation_count": 4
  },
  {
    "arxiv_id": "2412.11596",
    "title": "MeshArt: Generating Articulated Meshes with Structure-Guided Transformers",
    "year": 2024,
    "published": "2024-12-16T09:35:08Z",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "abstract": "Articulated 3D object generation is fundamental for creating realistic, functional, and interactable virtual assets which are not simply static. We introduce MeshArt, a hierarchical transformer-based approach to generate articulated 3D meshes with clean, compact geometry, reminiscent of human-crafted 3D models. We approach articulated mesh generation in a part-by-part fashion across two stages. First, we generate a high-level articulation-aware object structure; then, based on this structural in",
    "arxiv_url": "https://arxiv.org/abs/2412.11596v2",
    "pdf_url": "https://arxiv.org/pdf/2412.11596v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2412.11596",
    "arxiv_authors": [
      "Daoyi Gao",
      "Yawar Siddiqui",
      "Lei Li",
      "Angela Dai"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=MeshArt%3A+Generating+Articulated+Meshes+with+Structure-Guided+Transformers+Daoyi+Gao+Yawar+Siddiqui+Lei+Li+Angela+Dai",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "D Gao",
        "id": "jbRrxncAAAAJ"
      },
      {
        "name": "Y Siddiqui",
        "id": "u26UK5QAAAAJ"
      },
      {
        "name": "L Li",
        "id": "uzh8LlIAAAAJ"
      },
      {
        "name": "A Dai -",
        "id": null
      }
    ],
    "citation_count": 12
  },
  {
    "arxiv_id": "2303.06550",
    "title": "Spatial Correspondence between Graph Neural Network-Segmented Images",
    "year": 2023,
    "published": "2023-03-12T03:25:01Z",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "abstract": "Graph neural networks (GNNs) have been proposed for medical image segmentation, by predicting anatomical structures represented by graphs of vertices and edges. One such type of graph is predefined with fixed size and connectivity to represent a reference of anatomical regions of interest, thus known as templates. This work explores the potentials in these GNNs with common topology for establishing spatial correspondence, implicitly maintained during segmenting two or more images. With an exampl",
    "arxiv_url": "https://arxiv.org/abs/2303.06550v2",
    "pdf_url": "https://arxiv.org/pdf/2303.06550v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2303.06550",
    "arxiv_authors": [
      "Qian Li",
      "Yunguan Fu",
      "Qianye Yang",
      "Zhijiang Du",
      "Hongjian Yu",
      "Yipeng Hu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Spatial+Correspondence+between+Graph+Neural+Network-Segmented+Images+Qian+Li+Yunguan+Fu+Qianye+Yang+Zhijiang+Du+Hongjian+Yu",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "Q Li",
        "id": "o0fDYzkAAAAJ"
      },
      {
        "name": "Y Fu",
        "id": "8X-IK9sAAAAJ"
      },
      {
        "name": "Q Yang",
        "id": "Se8eLJQAAAAJ"
      },
      {
        "name": "Z Du",
        "id": null
      },
      {
        "name": "H Yu",
        "id": null
      }
    ],
    "citation_count": 1
  },
  {
    "arxiv_id": "2305.03177",
    "title": "Deep Learning-Assisted Simultaneous Targets Sensing and Super-Resolution Imaging",
    "year": 2023,
    "published": "2023-05-02T04:27:30Z",
    "categories": [
      "eess.SP",
      "cs.CV",
      "cs.LG",
      "eess.IV",
      "physics.optics"
    ],
    "abstract": "Recently, metasurfaces have experienced revolutionary growth in the sensing and superresolution imaging field, due to their enabling of subwavelength manipulation of electromagnetic waves. However, the addition of metasurfaces multiplies the complexity of retrieving target information from the detected fields. Besides, although the deep learning method affords a compelling platform for a series of electromagnetic problems, many studies mainly concentrate on resolving one single function and limi",
    "arxiv_url": "https://arxiv.org/abs/2305.03177v1",
    "pdf_url": "https://arxiv.org/pdf/2305.03177v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2305.03177",
    "arxiv_authors": [
      "Jin Zhao",
      "Huang Zhao Zhang",
      "Ming-Zhe Chong",
      "Yue-Yi Zhang",
      "Zi-Wen Zhang",
      "Zong-Kun Zhang",
      "Chao-Hai Du",
      "Pu-Kun Liu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Deep+Learning-Assisted+Simultaneous+Targets+Sensing+and+Super-Resolution+Imaging+Jin+Zhao+Huang+Zhao+Zhang+Ming-Zhe+Chong+Yue-Yi+Zhang+Zi-Wen+Zhang",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "J Zhao",
        "id": null
      },
      {
        "name": "H Zhang",
        "id": "pAUQnh4AAAAJ"
      },
      {
        "name": "MZ Chong",
        "id": "x3EPXBgAAAAJ"
      },
      {
        "name": "YY Zhang",
        "id": "iz2CdEwAAAAJ"
      },
      {
        "name": "ZW Zhang",
        "id": "6xzYjZYAAAAJ"
      },
      {
        "name": "ZK Zhang",
        "id": null
      },
      {
        "name": "CH Du",
        "id": "e-oJ2AgAAAAJ"
      },
      {
        "name": "PK LiuACS Applied Materials & Interfaces",
        "id": null
      }
    ],
    "citation_count": 4
  },
  {
    "arxiv_id": "2305.10260",
    "title": "From Region to Patch: Attribute-Aware Foreground-Background Contrastive Learning for Fine-Grained Fashion Retrieval",
    "year": 2023,
    "published": "2023-05-17T14:49:20Z",
    "categories": [
      "cs.CV",
      "cs.MM"
    ],
    "abstract": "Attribute-specific fashion retrieval (ASFR) is a challenging information retrieval task, which has attracted increasing attention in recent years. Different from traditional fashion retrieval which mainly focuses on optimizing holistic similarity, the ASFR task concentrates on attribute-specific similarity, resulting in more fine-grained and interpretable retrieval results. As the attribute-specific similarity typically corresponds to the specific subtle regions of images, we propose a Region-to",
    "arxiv_url": "https://arxiv.org/abs/2305.10260v1",
    "pdf_url": "https://arxiv.org/pdf/2305.10260v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2305.10260",
    "arxiv_authors": [
      "Jianfeng Dong",
      "Xiaoman Peng",
      "Zhe Ma",
      "Daizong Liu",
      "Xiaoye Qu",
      "Xun Yang",
      "Jixiang Zhu",
      "Baolong Liu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=From+Region+to+Patch%3A+Attribute-Aware+Foreground-Background+Contrastive+Learning+for+Fine-Grained+Fashion+Retrieval+Jianfeng+Dong+Xiaoman+Peng+Zhe+Ma+Daizong+Liu+Xiaoye+Qu",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "J Dong",
        "id": "8-zdk9wAAAAJ"
      },
      {
        "name": "X Peng",
        "id": null
      },
      {
        "name": "Z Ma",
        "id": "ZGu2IgkAAAAJ"
      },
      {
        "name": "D Liu",
        "id": "lUw7tVIAAAAJ"
      },
      {
        "name": "X Qu",
        "id": "rT3hqdcAAAAJ"
      },
      {
        "name": "X Yang",
        "id": "ro8lzsUAAAAJ"
      },
      {
        "name": "J Zhu",
        "id": null
      },
      {
        "name": "B Liu",
        "id": null
      }
    ],
    "citation_count": 15
  },
  {
    "arxiv_id": "2411.00222",
    "title": "Protecting Feed-Forward Networks from Adversarial Attacks Using Predictive Coding",
    "year": 2024,
    "published": "2024-10-31T21:38:05Z",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CV",
      "cs.LG",
      "cs.NE"
    ],
    "abstract": "An adversarial example is a modified input image designed to cause a Machine Learning (ML) model to make a mistake; these perturbations are often invisible or subtle to human observers and highlight vulnerabilities in a model's ability to generalize from its training data. Several adversarial attacks can create such examples, each with a different perspective, effectiveness, and perceptibility of changes. Conversely, defending against such adversarial attacks improves the robustness of ML models",
    "arxiv_url": "https://arxiv.org/abs/2411.00222v1",
    "pdf_url": "https://arxiv.org/pdf/2411.00222v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2411.00222",
    "arxiv_authors": [
      "Ehsan Ganjidoost",
      "Jeff Orchard"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Protecting+Feed-Forward+Networks+from+Adversarial+Attacks+Using+Predictive+Coding+Ehsan+Ganjidoost+Jeff+Orchard",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "E Ganjidoost",
        "id": "ZcxpSlQAAAAJ"
      },
      {
        "name": "J Orchard -",
        "id": null
      }
    ],
    "citation_count": null
  },
  {
    "arxiv_id": "2503.11571",
    "title": "RASA: Replace Anyone, Say Anything -- A Training-Free Framework for Audio-Driven and Universal Portrait Video Editing",
    "year": 2025,
    "published": "2025-03-14T16:39:15Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Portrait video editing focuses on modifying specific attributes of portrait videos, guided by audio or video streams. Previous methods typically either concentrate on lip-region reenactment or require training specialized models to extract keypoints for motion transfer to a new identity. In this paper, we introduce a training-free universal portrait video editing framework that provides a versatile and adaptable editing strategy. This framework supports portrait appearance editing conditioned on",
    "arxiv_url": "https://arxiv.org/abs/2503.11571v1",
    "pdf_url": "https://arxiv.org/pdf/2503.11571v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2503.11571",
    "arxiv_authors": [
      "Tianrui Pan",
      "Lin Liu",
      "Jie Liu",
      "Xiaopeng Zhang",
      "Jie Tang",
      "Gangshan Wu",
      "Qi Tian"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=RASA%3A+Replace+Anyone%2C+Say+Anything+--+A+Training-Free+Framework+for+Audio-Driven+and+Universal+Portrait+Video+Editing+Tianrui+Pan+Lin+Liu+Jie+Liu+Xiaopeng+Zhang+Jie+Tang",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "T Pan",
        "id": null
      },
      {
        "name": "L Liu",
        "id": "aliP2WYAAAAJ"
      },
      {
        "name": "J Liu",
        "id": "oab9IRYAAAAJ"
      },
      {
        "name": "X Zhang",
        "id": null
      },
      {
        "name": "J Tang",
        "id": "ENJVDroAAAAJ"
      },
      {
        "name": "G Wu",
        "id": null
      },
      {
        "name": "Q Tian",
        "id": null
      }
    ],
    "citation_count": null
  },
  {
    "arxiv_id": "2503.14494",
    "title": "Deeply Supervised Flow-Based Generative Models",
    "year": 2025,
    "published": "2025-03-18T17:58:08Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Flow based generative models have charted an impressive path across multiple visual generation tasks by adhering to a simple principle: learning velocity representations of a linear interpolant. However, we observe that training velocity solely from the final layer output underutilizes the rich inter layer representations, potentially impeding model convergence. To address this limitation, we introduce DeepFlow, a novel framework that enhances velocity representation through inter layer communic",
    "arxiv_url": "https://arxiv.org/abs/2503.14494v2",
    "pdf_url": "https://arxiv.org/pdf/2503.14494v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2503.14494",
    "arxiv_authors": [
      "Inkyu Shin",
      "Chenglin Yang",
      "Liang-Chieh Chen"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Deeply+Supervised+Flow-Based+Generative+Models+Inkyu+Shin+Chenglin+Yang+Liang-Chieh+Chen",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "I Shin",
        "id": "XpHl_HEAAAAJ"
      },
      {
        "name": "C Yang",
        "id": "DsumNkgAAAAJ"
      },
      {
        "name": "LC Chen -",
        "id": null
      }
    ],
    "citation_count": 4
  },
  {
    "arxiv_id": "2403.07359",
    "title": "FSC: Few-point Shape Completion",
    "year": 2024,
    "published": "2024-03-12T06:45:34Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "While previous studies have demonstrated successful 3D object shape completion with a sufficient number of points, they often fail in scenarios when a few points, e.g. tens of points, are observed. Surprisingly, via entropy analysis, we find that even a few points, e.g. 64 points, could retain substantial information to help recover the 3D shape of the object. To address the challenge of shape completion with very sparse point clouds, we then propose Few-point Shape Completion (FSC) model, which",
    "arxiv_url": "https://arxiv.org/abs/2403.07359v5",
    "pdf_url": "https://arxiv.org/pdf/2403.07359v5",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2403.07359",
    "arxiv_authors": [
      "Xianzu Wu",
      "Xianfeng Wu",
      "Tianyu Luan",
      "Yajing Bai",
      "Zhongyuan Lai",
      "Junsong Yuan"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=FSC%3A+Few-point+Shape+Completion+Xianzu+Wu+Xianfeng+Wu+Tianyu+Luan+Yajing+Bai+Zhongyuan+Lai",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "X Wu",
        "id": "C9B5JKYAAAAJ"
      },
      {
        "name": "X Wu",
        "id": "C9B5JKYAAAAJ"
      },
      {
        "name": "T Luan",
        "id": "XNzPzTIAAAAJ"
      },
      {
        "name": "Y Bai",
        "id": "0bmTpcAAAAAJ"
      },
      {
        "name": "Z Lai",
        "id": "nEGuRZAAAAAJ"
      },
      {
        "name": "J Yuan",
        "id": "fJ7seq0AAAAJ"
      }
    ],
    "citation_count": 22
  },
  {
    "arxiv_id": "2307.11470",
    "title": "A Semi-supervised Physics-Aware Triple-Stream Underwater Image Enhancement Network",
    "year": 2023,
    "published": "2023-07-21T10:10:18Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Underwater images normally suffer from degradation due to the transmission medium of water bodies. Both traditional prior-based approaches and deep learning-based methods have been used to address this problem. However, the inflexible assumption of the former often impairs their effectiveness in handling diverse underwater scenes, while the generalization of the latter to unseen images is usually weakened by insufficient data. In this study, we leverage both the physics-based Image Formation Mod",
    "arxiv_url": "https://arxiv.org/abs/2307.11470v7",
    "pdf_url": "https://arxiv.org/pdf/2307.11470v7",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2307.11470",
    "arxiv_authors": [
      "Shixuan Xu",
      "Hao Qi",
      "Wei Wang",
      "Chao Huang",
      "Jie Wen",
      "Junyu Dong",
      "Xinghui Dong"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=A+Semi-supervised+Physics-Aware+Triple-Stream+Underwater+Image+Enhancement+Network+Shixuan+Xu+Hao+Qi+Wei+Wang+Chao+Huang+Jie+Wen",
    "gs_search_success": true,
    "gs_authors": [],
    "citation_count": null
  },
  {
    "arxiv_id": "2303.03023",
    "title": "Guiding Energy-based Models via Contrastive Latent Variables",
    "year": 2023,
    "published": "2023-03-06T10:50:25Z",
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "abstract": "An energy-based model (EBM) is a popular generative framework that offers both explicit density and architectural flexibility, but training them is difficult since it is often unstable and time-consuming. In recent years, various training techniques have been developed, e.g., better divergence measures or stabilization in MCMC sampling, but there often exists a large gap between EBMs and other generative frameworks like GANs in terms of generation quality. In this paper, we propose a novel and e",
    "arxiv_url": "https://arxiv.org/abs/2303.03023v1",
    "pdf_url": "https://arxiv.org/pdf/2303.03023v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2303.03023",
    "arxiv_authors": [
      "Hankook Lee",
      "Jongheon Jeong",
      "Sejun Park",
      "Jinwoo Shin"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Guiding+Energy-based+Models+via+Contrastive+Latent+Variables+Hankook+Lee+Jongheon+Jeong+Sejun+Park+Jinwoo+Shin",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "H Lee",
        "id": "CgqswXUAAAAJ"
      },
      {
        "name": "J Jeong",
        "id": "mZB2qfcAAAAJ"
      },
      {
        "name": "S Park",
        "id": "q2PMn8cAAAAJ"
      },
      {
        "name": "J Shin -",
        "id": null
      }
    ],
    "citation_count": 28
  },
  {
    "arxiv_id": "2409.14985",
    "title": "Sparse-to-Dense LiDAR Point Generation by LiDAR-Camera Fusion for 3D Object Detection",
    "year": 2024,
    "published": "2024-09-23T13:03:31Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Accurately detecting objects at long distances remains a critical challenge in 3D object detection when relying solely on LiDAR sensors due to the inherent limitations of data sparsity. To address this issue, we propose the LiDAR-Camera Augmentation Network (LCANet), a novel framework that reconstructs LiDAR point cloud data by fusing 2D image features, which contain rich semantic information, generating additional points to improve detection accuracy. LCANet fuses data from LiDAR sensors and ca",
    "arxiv_url": "https://arxiv.org/abs/2409.14985v2",
    "pdf_url": "https://arxiv.org/pdf/2409.14985v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2409.14985",
    "arxiv_authors": [
      "Minseung Lee",
      "Seokha Moon",
      "Seung Joon Lee",
      "Jinkyu Kim"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Sparse-to-Dense+LiDAR+Point+Generation+by+LiDAR-Camera+Fusion+for+3D+Object+Detection+Minseung+Lee+Seokha+Moon+Seung+Joon+Lee+Jinkyu+Kim",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "M Lee",
        "id": null
      },
      {
        "name": "S Moon",
        "id": "HhvS9d4AAAAJ"
      },
      {
        "name": "SJ Lee",
        "id": null
      },
      {
        "name": "J Kim -",
        "id": null
      }
    ],
    "citation_count": 2
  },
  {
    "arxiv_id": "2406.01605",
    "title": "An Enhanced Encoder-Decoder Network Architecture for Reducing Information Loss in Image Semantic Segmentation",
    "year": 2024,
    "published": "2024-05-26T05:15:53Z",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "abstract": "The traditional SegNet architecture commonly encounters significant information loss during the sampling process, which detrimentally affects its accuracy in image semantic segmentation tasks. To counter this challenge, we introduce an innovative encoder-decoder network structure enhanced with residual connections. Our approach employs a multi-residual connection strategy designed to preserve the intricate details across various image scales more effectively, thus minimizing the information loss",
    "arxiv_url": "https://arxiv.org/abs/2406.01605v1",
    "pdf_url": "https://arxiv.org/pdf/2406.01605v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2406.01605",
    "arxiv_authors": [
      "Zijun Gao",
      "Qi Wang",
      "Taiyuan Mei",
      "Xiaohan Cheng",
      "Yun Zi",
      "Haowei Yang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=An+Enhanced+Encoder-Decoder+Network+Architecture+for+Reducing+Information+Loss+in+Image+Semantic+Segmentation+Zijun+Gao+Qi+Wang+Taiyuan+Mei+Xiaohan+Cheng+Yun+Zi",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "Z Gao",
        "id": "6_B7SIUAAAAJ"
      },
      {
        "name": "Q Wang",
        "id": null
      },
      {
        "name": "T Mei",
        "id": "2ONe4yIAAAAJ"
      },
      {
        "name": "X Cheng",
        "id": "diE9zGwAAAAJ"
      },
      {
        "name": "Y Zi",
        "id": "_9njiHwAAAAJ"
      },
      {
        "name": "H Yang2024 5th International",
        "id": null
      }
    ],
    "citation_count": 20
  },
  {
    "arxiv_id": "2311.00603",
    "title": "Occluded Person Re-Identification with Deep Learning: A Survey and Perspectives",
    "year": 2023,
    "published": "2023-11-01T15:52:51Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Person re-identification (Re-ID) technology plays an increasingly crucial role in intelligent surveillance systems. Widespread occlusion significantly impacts the performance of person Re-ID. Occluded person Re-ID refers to a pedestrian matching method that deals with challenges such as pedestrian information loss, noise interference, and perspective misalignment. It has garnered extensive attention from researchers. Over the past few years, several occlusion-solving person Re-ID methods have be",
    "arxiv_url": "https://arxiv.org/abs/2311.00603v1",
    "pdf_url": "https://arxiv.org/pdf/2311.00603v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2311.00603",
    "arxiv_authors": [
      "Enhao Ning",
      "Changshuo Wang",
      "Huang Zhangc",
      "Xin Ning",
      "Prayag Tiwari"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Occluded+Person+Re-Identification+with+Deep+Learning%3A+A+Survey+and+Perspectives+Enhao+Ning+Changshuo+Wang+Huang+Zhangc+Xin+Ning+Prayag+Tiwari",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "E Ning",
        "id": null
      },
      {
        "name": "C Wang",
        "id": "dsV9sgwAAAAJ"
      },
      {
        "name": "H Zhang",
        "id": "xI6YYLEAAAAJ"
      },
      {
        "name": "X Ning",
        "id": "F3a4-tkAAAAJ"
      },
      {
        "name": "P Tiwari - Expert systems with",
        "id": null
      }
    ],
    "citation_count": 113
  },
  {
    "arxiv_id": "2412.15673",
    "title": "Learning Group Interactions and Semantic Intentions for Multi-Object Trajectory Prediction",
    "year": 2024,
    "published": "2024-12-20T08:38:26Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Effective modeling of group interactions and dynamic semantic intentions is crucial for forecasting behaviors like trajectories or movements. In complex scenarios like sports, agents' trajectories are influenced by group interactions and intentions, including team strategies and opponent actions. To this end, we propose a novel diffusion-based trajectory prediction framework that integrates group-level interactions into a conditional diffusion model, enabling the generation of diverse trajectori",
    "arxiv_url": "https://arxiv.org/abs/2412.15673v1",
    "pdf_url": "https://arxiv.org/pdf/2412.15673v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2412.15673",
    "arxiv_authors": [
      "Mengshi Qi",
      "Yuxin Yang",
      "Huadong Ma"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Learning+Group+Interactions+and+Semantic+Intentions+for+Multi-Object+Trajectory+Prediction+Mengshi+Qi+Yuxin+Yang+Huadong+Ma",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "M Qi",
        "id": "_gH7-4wAAAAJ"
      },
      {
        "name": "Y Yang",
        "id": null
      },
      {
        "name": "H Ma -",
        "id": null
      }
    ],
    "citation_count": 1
  },
  {
    "arxiv_id": "2305.10028",
    "title": "Pyramid Diffusion Models For Low-light Image Enhancement",
    "year": 2023,
    "published": "2023-05-17T08:15:45Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Recovering noise-covered details from low-light images is challenging, and the results given by previous methods leave room for improvement. Recent diffusion models show realistic and detailed image generation through a sequence of denoising refinements and motivate us to introduce them to low-light image enhancement for recovering realistic details. However, we found two problems when doing this, i.e., 1) diffusion models keep constant resolution in one reverse process, which limits the speed; ",
    "arxiv_url": "https://arxiv.org/abs/2305.10028v1",
    "pdf_url": "https://arxiv.org/pdf/2305.10028v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2305.10028",
    "arxiv_authors": [
      "Dewei Zhou",
      "Zongxin Yang",
      "Yi Yang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Pyramid+Diffusion+Models+For+Low-light+Image+Enhancement+Dewei+Zhou+Zongxin+Yang+Yi+Yang",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "D Zhou",
        "id": "4C_OwWMAAAAJ"
      },
      {
        "name": "Z Yang",
        "id": "8IE0CfwAAAAJ"
      },
      {
        "name": "Y Yang -",
        "id": null
      }
    ],
    "citation_count": 165
  },
  {
    "arxiv_id": "2305.00121",
    "title": "Learning Locally Editable Virtual Humans",
    "year": 2023,
    "published": "2023-04-28T23:06:17Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "In this paper, we propose a novel hybrid representation and end-to-end trainable network architecture to model fully editable and customizable neural avatars. At the core of our work lies a representation that combines the modeling power of neural fields with the ease of use and inherent 3D consistency of skinned meshes. To this end, we construct a trainable feature codebook to store local geometry and texture features on the vertices of a deformable body model, thus exploiting its consistent to",
    "arxiv_url": "https://arxiv.org/abs/2305.00121v1",
    "pdf_url": "https://arxiv.org/pdf/2305.00121v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2305.00121",
    "arxiv_authors": [
      "Hsuan-I Ho",
      "Lixin Xue",
      "Jie Song",
      "Otmar Hilliges"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Learning+Locally+Editable+Virtual+Humans+Hsuan-I+Ho+Lixin+Xue+Jie+Song+Otmar+Hilliges",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "HI Ho",
        "id": "xFRcsOcAAAAJ"
      },
      {
        "name": "L Xue",
        "id": "1T0TIsoAAAAJ"
      },
      {
        "name": "J Song",
        "id": "kBN1B6YAAAAJ"
      },
      {
        "name": "O Hilliges -",
        "id": null
      }
    ],
    "citation_count": 69
  },
  {
    "arxiv_id": "2401.04345",
    "title": "RomniStereo: Recurrent Omnidirectional Stereo Matching",
    "year": 2024,
    "published": "2024-01-09T04:06:01Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Omnidirectional stereo matching (OSM) is an essential and reliable means for $360^{\\circ}$ depth sensing. However, following earlier works on conventional stereo matching, prior state-of-the-art (SOTA) methods rely on a 3D encoder-decoder block to regularize the cost volume, causing the whole system complicated and sub-optimal results. Recently, the Recurrent All-pairs Field Transforms (RAFT) based approach employs the recurrent update in 2D and has efficiently improved image-matching tasks, ie,",
    "arxiv_url": "https://arxiv.org/abs/2401.04345v2",
    "pdf_url": "https://arxiv.org/pdf/2401.04345v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2401.04345",
    "arxiv_authors": [
      "Hualie Jiang",
      "Rui Xu",
      "Minglang Tan",
      "Wenjie Jiang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=RomniStereo%3A+Recurrent+Omnidirectional+Stereo+Matching+Hualie+Jiang+Rui+Xu+Minglang+Tan+Wenjie+Jiang",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "H Jiang",
        "id": "kKBsyGAAAAAJ"
      },
      {
        "name": "R Xu",
        "id": "71Pil2oAAAAJ"
      },
      {
        "name": "M Tan",
        "id": null
      },
      {
        "name": "W Jiang - IEEE Robotics and Automation",
        "id": null
      }
    ],
    "citation_count": 5
  },
  {
    "arxiv_id": "2403.16167",
    "title": "ESREAL: Exploiting Semantic Reconstruction to Mitigate Hallucinations in Vision-Language Models",
    "year": 2024,
    "published": "2024-03-24T14:21:06Z",
    "categories": [
      "cs.CV",
      "cs.CL"
    ],
    "abstract": "Hallucinations in vision-language models pose a significant challenge to their reliability, particularly in the generation of long captions. Current methods fall short of accurately identifying and mitigating these hallucinations. To address this issue, we introduce ESREAL, a novel unsupervised learning framework designed to suppress the generation of hallucinations through accurate localization and penalization of hallucinated tokens. Initially, ESREAL creates a reconstructed image based on the",
    "arxiv_url": "https://arxiv.org/abs/2403.16167v4",
    "pdf_url": "https://arxiv.org/pdf/2403.16167v4",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2403.16167",
    "arxiv_authors": [
      "Minchan Kim",
      "Minyeong Kim",
      "Junik Bae",
      "Suhwan Choi",
      "Sungkyung Kim",
      "Buru Chang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=ESREAL%3A+Exploiting+Semantic+Reconstruction+to+Mitigate+Hallucinations+in+Vision-Language+Models+Minchan+Kim+Minyeong+Kim+Junik+Bae+Suhwan+Choi+Sungkyung+Kim",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "M Kim",
        "id": "Jh3S9aAAAAAJ"
      },
      {
        "name": "M Kim",
        "id": "Jh3S9aAAAAAJ"
      },
      {
        "name": "J Bae",
        "id": null
      },
      {
        "name": "S Choi",
        "id": "MpNWR7MAAAAJ"
      },
      {
        "name": "S Kim",
        "id": "kOrK5b4AAAAJ"
      },
      {
        "name": "B ChangEuropean",
        "id": null
      }
    ],
    "citation_count": 13
  },
  {
    "arxiv_id": "2402.01422",
    "title": "EmoSpeaker: One-shot Fine-grained Emotion-Controlled Talking Face Generation",
    "year": 2024,
    "published": "2024-02-02T14:04:18Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Implementing fine-grained emotion control is crucial for emotion generation tasks because it enhances the expressive capability of the generative model, allowing it to accurately and comprehensively capture and express various nuanced emotional states, thereby improving the emotional quality and personalization of generated content. Generating fine-grained facial animations that accurately portray emotional expressions using only a portrait and an audio recording presents a challenge. In order t",
    "arxiv_url": "https://arxiv.org/abs/2402.01422v1",
    "pdf_url": "https://arxiv.org/pdf/2402.01422v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2402.01422",
    "arxiv_authors": [
      "Guanwen Feng",
      "Haoran Cheng",
      "Yunan Li",
      "Zhiyuan Ma",
      "Chaoneng Li",
      "Zhihao Qian",
      "Qiguang Miao",
      "Chi-Man Pun"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=EmoSpeaker%3A+One-shot+Fine-grained+Emotion-Controlled+Talking+Face+Generation+Guanwen+Feng+Haoran+Cheng+Yunan+Li+Zhiyuan+Ma+Chaoneng+Li",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "G Feng",
        "id": "YXpxilsAAAAJ"
      },
      {
        "name": "H Cheng",
        "id": null
      },
      {
        "name": "Y Li",
        "id": null
      },
      {
        "name": "Z Ma",
        "id": null
      },
      {
        "name": "C Li",
        "id": "AVyU6yUAAAAJ"
      },
      {
        "name": "Z Qian",
        "id": "o83AL3sAAAAJ"
      },
      {
        "name": "Q Miao",
        "id": "2TQfvt8AAAAJ"
      },
      {
        "name": "CM Pun",
        "id": "JTkP_EAAAAAJ"
      }
    ],
    "citation_count": 5
  },
  {
    "arxiv_id": "2303.14999",
    "title": "Transformer-based Multi-Instance Learning for Weakly Supervised Object Detection",
    "year": 2023,
    "published": "2023-03-27T08:42:45Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Weakly Supervised Object Detection (WSOD) enables the training of object detection models using only image-level annotations. State-of-the-art WSOD detectors commonly rely on multi-instance learning (MIL) as the backbone of their detectors and assume that the bounding box proposals of an image are independent of each other. However, since such approaches only utilize the highest score proposal and discard the potentially useful information from other proposals, their independent MIL backbone oft",
    "arxiv_url": "https://arxiv.org/abs/2303.14999v1",
    "pdf_url": "https://arxiv.org/pdf/2303.14999v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2303.14999",
    "arxiv_authors": [
      "Zhaofei Wang",
      "Weijia Zhang",
      "Min-Ling Zhang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Transformer-based+Multi-Instance+Learning+for+Weakly+Supervised+Object+Detection+Zhaofei+Wang+Weijia+Zhang+Min-Ling+Zhang",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "Z Wang",
        "id": null
      },
      {
        "name": "W Zhang",
        "id": "7jmAPvAAAAAJ"
      },
      {
        "name": "ML Zhang -",
        "id": null
      }
    ],
    "citation_count": 3
  },
  {
    "arxiv_id": "2501.01460",
    "title": "GDSR: Global-Detail Integration through Dual-Branch Network with Wavelet Losses for Remote Sensing Image Super-Resolution",
    "year": 2024,
    "published": "2024-12-31T10:43:19Z",
    "categories": [
      "eess.IV",
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "In recent years, deep neural networks, including Convolutional Neural Networks, Transformers, and State Space Models, have achieved significant progress in Remote Sensing Image (RSI) Super-Resolution (SR). However, existing SR methods typically overlook the complementary relationship between global and local dependencies. These methods either focus on capturing local information or prioritize global information, which results in models that are unable to effectively capture both global and local",
    "arxiv_url": "https://arxiv.org/abs/2501.01460v4",
    "pdf_url": "https://arxiv.org/pdf/2501.01460v4",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2501.01460",
    "arxiv_authors": [
      "Qiwei Zhu",
      "Kai Li",
      "Guojing Zhang",
      "Xiaoying Wang",
      "Jianqiang Huang",
      "Xilai Li"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=GDSR%3A+Global-Detail+Integration+through+Dual-Branch+Network+with+Wavelet+Losses+for+Remote+Sensing+Image+Super-Resolution+Qiwei+Zhu+Kai+Li+Guojing+Zhang+Xiaoying+Wang+Jianqiang+Huang",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "Q Zhu",
        "id": "02WejbYAAAAJ"
      },
      {
        "name": "K Li",
        "id": "fHkHcMsAAAAJ"
      },
      {
        "name": "G Zhang",
        "id": null
      },
      {
        "name": "X Wang",
        "id": null
      },
      {
        "name": "J Huang",
        "id": null
      },
      {
        "name": "X LiIEEE Transactions on Geoscience and Remote Sensing",
        "id": null
      }
    ],
    "citation_count": null
  },
  {
    "arxiv_id": "2306.01808",
    "title": "Morphology Edge Attention Network and Optimal Geometric Matching Connection model for vascular segmentation",
    "year": 2023,
    "published": "2023-06-02T01:52:35Z",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "abstract": "There are many unsolved problems in vascular image segmentation, including vascular structural connectivity, scarce branches and missing small vessels. Obtaining vessels that preserve their correct topological structures is currently a crucial research issue, as it provides an overall view of one vascular system. In order to preserve the topology and accuracy of vessel segmentation, we proposed a novel Morphology Edge Attention Network (MEA-Net) for the segmentation of vessel-like structures, an",
    "arxiv_url": "https://arxiv.org/abs/2306.01808v2",
    "pdf_url": "https://arxiv.org/pdf/2306.01808v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2306.01808",
    "arxiv_authors": [
      "Yuntao Zhu",
      "Yuxuan Qiao",
      "Xiaoping Yang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Morphology+Edge+Attention+Network+and+Optimal+Geometric+Matching+Connection+model+for+vascular+segmentation+Yuntao+Zhu+Yuxuan+Qiao+Xiaoping+Yang",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "Y Zhu",
        "id": null
      },
      {
        "name": "Y Qiao",
        "id": null
      },
      {
        "name": "X Yang -",
        "id": null
      }
    ],
    "citation_count": null
  },
  {
    "arxiv_id": "2303.17921",
    "title": "IC-FPS: Instance-Centroid Faster Point Sampling Module for 3D Point-base Object Detection",
    "year": 2023,
    "published": "2023-03-31T09:31:29Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "3D object detection is one of the most important tasks in autonomous driving and robotics. Our research focuses on tackling low efficiency issue of point-based methods on large-scale point clouds. Existing point-based methods adopt farthest point sampling (FPS) strategy for downsampling, which is computationally expensive in terms of inference time and memory consumption when the number of point cloud increases. In order to improve efficiency, we propose a novel Instance-Centroid Faster Point Sa",
    "arxiv_url": "https://arxiv.org/abs/2303.17921v1",
    "pdf_url": "https://arxiv.org/pdf/2303.17921v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2303.17921",
    "arxiv_authors": [
      "Hu Haotian",
      "Wang Fanyi",
      "Su Jingwen",
      "Gao Shiyu",
      "Zhang Zhiwang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=IC-FPS%3A+Instance-Centroid+Faster+Point+Sampling+Module+for+3D+Point-base+Object+Detection+Hu+Haotian+Wang+Fanyi+Su+Jingwen+Gao+Shiyu+Zhang+Zhiwang",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "H Haotian",
        "id": null
      },
      {
        "name": "W Fanyi",
        "id": null
      },
      {
        "name": "S Jingwen",
        "id": null
      },
      {
        "name": "G Shiyu",
        "id": "9AvAm70AAAAJ"
      },
      {
        "name": "Z Zhiwang",
        "id": "A99tzhgAAAAJ"
      }
    ],
    "citation_count": 3
  },
  {
    "arxiv_id": "2307.02836",
    "title": "Noise-to-Norm Reconstruction for Industrial Anomaly Detection and Localization",
    "year": 2023,
    "published": "2023-07-06T08:06:48Z",
    "categories": [
      "cs.CV",
      "eess.IV"
    ],
    "abstract": "Anomaly detection has a wide range of applications and is especially important in industrial quality inspection. Currently, many top-performing anomaly-detection models rely on feature-embedding methods. However, these methods do not perform well on datasets with large variations in object locations. Reconstruction-based methods use reconstruction errors to detect anomalies without considering positional differences between samples. In this study, a reconstruction-based method using the noise-to",
    "arxiv_url": "https://arxiv.org/abs/2307.02836v1",
    "pdf_url": "https://arxiv.org/pdf/2307.02836v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2307.02836",
    "arxiv_authors": [
      "Shiqi Deng",
      "Zhiyu Sun",
      "Ruiyan Zhuang",
      "Jun Gong"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Noise-to-Norm+Reconstruction+for+Industrial+Anomaly+Detection+and+Localization+Shiqi+Deng+Zhiyu+Sun+Ruiyan+Zhuang+Jun+Gong",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "S Deng",
        "id": null
      },
      {
        "name": "Z Sun",
        "id": "GVm0eTIAAAAJ"
      },
      {
        "name": "R Zhuang",
        "id": null
      },
      {
        "name": "J Gong - Applied Sciences",
        "id": null
      }
    ],
    "citation_count": 12
  },
  {
    "arxiv_id": "2505.08695",
    "title": "SPAST: Arbitrary Style Transfer with Style Priors via Pre-trained Large-scale Model",
    "year": 2025,
    "published": "2025-05-13T15:54:36Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Given an arbitrary content and style image, arbitrary style transfer aims to render a new stylized   image which preserves the content image's structure and possesses the style image's style. Existing   arbitrary style transfer methods are based on either small models or pre-trained large-scale models.   The small model-based methods fail to generate high-quality stylized images, bringing artifacts and   disharmonious patterns. The pre-trained large-scale model-based methods can generate high-qu",
    "arxiv_url": "https://arxiv.org/abs/2505.08695v1",
    "pdf_url": "https://arxiv.org/pdf/2505.08695v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2505.08695",
    "arxiv_authors": [
      "Zhanjie Zhang",
      "Quanwei Zhang",
      "Junsheng Luan",
      "Mengyuan Yang",
      "Yun Wang",
      "Lei Zhao"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=SPAST%3A+Arbitrary+Style+Transfer+with+Style+Priors+via+Pre-trained+Large-scale+Model+Zhanjie+Zhang+Quanwei+Zhang+Junsheng+Luan+Mengyuan+Yang+Yun+Wang",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "Z Zhang",
        "id": null
      },
      {
        "name": "Q Zhang",
        "id": "U91iO70AAAAJ"
      },
      {
        "name": "J Luan",
        "id": "ncfVsxAAAAAJ"
      },
      {
        "name": "M Yang",
        "id": "SpNEjnwAAAAJ"
      },
      {
        "name": "Y Wang",
        "id": "8fma3awAAAAJ"
      },
      {
        "name": "L Zhao - Neural Networks",
        "id": null
      }
    ],
    "citation_count": 6
  },
  {
    "arxiv_id": "2304.13455",
    "title": "From Chaos Comes Order: Ordering Event Representations for Object Recognition and Detection",
    "year": 2023,
    "published": "2023-04-26T11:27:34Z",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "Today, state-of-the-art deep neural networks that process events first convert them into dense, grid-like input representations before using an off-the-shelf network. However, selecting the appropriate representation for the task traditionally requires training a neural network for each representation and selecting the best one based on the validation score, which is very time-consuming. This work eliminates this bottleneck by selecting representations based on the Gromov-Wasserstein Discrepancy",
    "arxiv_url": "https://arxiv.org/abs/2304.13455v4",
    "pdf_url": "https://arxiv.org/pdf/2304.13455v4",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2304.13455",
    "arxiv_authors": [
      "Nikola Zubiƒá",
      "Daniel Gehrig",
      "Mathias Gehrig",
      "Davide Scaramuzza"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=From+Chaos+Comes+Order%3A+Ordering+Event+Representations+for+Object+Recognition+and+Detection+Nikola+Zubi%C4%87+Daniel+Gehrig+Mathias+Gehrig+Davide+Scaramuzza",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "N Zubiƒá",
        "id": "65OImV0AAAAJ"
      },
      {
        "name": "D Gehrig",
        "id": "FWpgbBsAAAAJ"
      },
      {
        "name": "M Gehrig",
        "id": "uTMjaVoAAAAJ"
      },
      {
        "name": "D Scaramuzza",
        "id": "SC9wV2kAAAAJ"
      }
    ],
    "citation_count": 73
  },
  {
    "arxiv_id": "2401.02424",
    "title": "Mapping of Land Use and Land Cover (LULC) using EuroSAT and Transfer Learning",
    "year": 2023,
    "published": "2023-11-06T18:10:25Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "abstract": "As the global population continues to expand, the demand for natural resources increases. Unfortunately, human activities account for 23% of greenhouse gas emissions. On a positive note, remote sensing technologies have emerged as a valuable tool in managing our environment. These technologies allow us to monitor land use, plan urban areas, and drive advancements in areas such as agriculture, climate change mitigation, disaster recovery, and environmental monitoring. Recent advances in AI, compu",
    "arxiv_url": "https://arxiv.org/abs/2401.02424v1",
    "pdf_url": "https://arxiv.org/pdf/2401.02424v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2401.02424",
    "arxiv_authors": [
      "Suman Kunwar",
      "Jannatul Ferdush"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Mapping+of+Land+Use+and+Land+Cover+%28LULC%29+using+EuroSAT+and+Transfer+Learning+Suman+Kunwar+Jannatul+Ferdush",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "S Kunwar",
        "id": "TDrOhrIAAAAJ"
      },
      {
        "name": "J Ferdush -",
        "id": null
      }
    ],
    "citation_count": 9
  },
  {
    "arxiv_id": "2403.12396",
    "title": "OV9D: Open-Vocabulary Category-Level 9D Object Pose and Size Estimation",
    "year": 2024,
    "published": "2024-03-19T03:09:24Z",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "abstract": "This paper studies a new open-set problem, the open-vocabulary category-level object pose and size estimation. Given human text descriptions of arbitrary novel object categories, the robot agent seeks to predict the position, orientation, and size of the target object in the observed scene image. To enable such generalizability, we first introduce OO3D-9D, a large-scale photorealistic dataset for this task. Derived from OmniObject3D, OO3D-9D is the largest and most diverse dataset in the field o",
    "arxiv_url": "https://arxiv.org/abs/2403.12396v1",
    "pdf_url": "https://arxiv.org/pdf/2403.12396v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2403.12396",
    "arxiv_authors": [
      "Junhao Cai",
      "Yisheng He",
      "Weihao Yuan",
      "Siyu Zhu",
      "Zilong Dong",
      "Liefeng Bo",
      "Qifeng Chen"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=OV9D%3A+Open-Vocabulary+Category-Level+9D+Object+Pose+and+Size+Estimation+Junhao+Cai+Yisheng+He+Weihao+Yuan+Siyu+Zhu+Zilong+Dong",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "J Cai",
        "id": "GeSCNR4AAAAJ"
      },
      {
        "name": "Y He",
        "id": "UM4qFCsAAAAJ"
      },
      {
        "name": "W Yuan",
        "id": "m3tqxRQAAAAJ"
      },
      {
        "name": "S Zhu",
        "id": "vNCnDiMAAAAJ"
      },
      {
        "name": "Z Dong",
        "id": "GHOQKCwAAAAJ"
      },
      {
        "name": "L Bo",
        "id": null
      },
      {
        "name": "Q Chen",
        "id": "lLMX9hcAAAAJ"
      }
    ],
    "citation_count": 13
  },
  {
    "arxiv_id": "2408.04261",
    "title": "Unveiling Hidden Visual Information: A Reconstruction Attack Against Adversarial Visual Information Hiding",
    "year": 2024,
    "published": "2024-08-08T06:58:48Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CR"
    ],
    "abstract": "This paper investigates the security vulnerabilities of adversarial-example-based image encryption by executing data reconstruction (DR) attacks on encrypted images. A representative image encryption method is the adversarial visual information hiding (AVIH), which uses type-I adversarial example training to protect gallery datasets used in image recognition tasks. In the AVIH method, the type-I adversarial example approach creates images that appear completely different but are still recognized",
    "arxiv_url": "https://arxiv.org/abs/2408.04261v1",
    "pdf_url": "https://arxiv.org/pdf/2408.04261v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2408.04261",
    "arxiv_authors": [
      "Jonggyu Jang",
      "Hyeonsu Lyu",
      "Seongjin Hwang",
      "Hyun Jong Yang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Unveiling+Hidden+Visual+Information%3A+A+Reconstruction+Attack+Against+Adversarial+Visual+Information+Hiding+Jonggyu+Jang+Hyeonsu+Lyu+Seongjin+Hwang+Hyun+Jong+Yang",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "J Jang",
        "id": null
      },
      {
        "name": "H Lyu",
        "id": "06N525AAAAAJ"
      },
      {
        "name": "S Hwang",
        "id": "7oUGSw8AAAAJ"
      },
      {
        "name": "HJ YangIEEE Transactions on Neural Networks and Learning Systems",
        "id": null
      }
    ],
    "citation_count": 3
  },
  {
    "arxiv_id": "2403.14279",
    "title": "Zero123-6D: Zero-shot Novel View Synthesis for RGB Category-level 6D Pose Estimation",
    "year": 2024,
    "published": "2024-03-21T10:38:18Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Estimating the pose of objects through vision is essential to make robotic platforms interact with the environment. Yet, it presents many challenges, often related to the lack of flexibility and generalizability of state-of-the-art solutions. Diffusion models are a cutting-edge neural architecture transforming 2D and 3D computer vision, outlining remarkable performances in zero-shot novel-view synthesis. Such a use case is particularly intriguing for reconstructing 3D objects. However, localizin",
    "arxiv_url": "https://arxiv.org/abs/2403.14279v2",
    "pdf_url": "https://arxiv.org/pdf/2403.14279v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2403.14279",
    "arxiv_authors": [
      "Francesco Di Felice",
      "Alberto Remus",
      "Stefano Gasperini",
      "Benjamin Busam",
      "Lionel Ott",
      "Federico Tombari",
      "Roland Siegwart",
      "Carlo Alberto Avizzano"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Zero123-6D%3A+Zero-shot+Novel+View+Synthesis+for+RGB+Category-level+6D+Pose+Estimation+Francesco+Di+Felice+Alberto+Remus+Stefano+Gasperini+Benjamin+Busam+Lionel+Ott",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "F Di Felice",
        "id": "kr0-5gYAAAAJ"
      },
      {
        "name": "A Remus",
        "id": "SyIEcpkAAAAJ"
      },
      {
        "name": "S Gasperini",
        "id": "YuWTPaIAAAAJ"
      },
      {
        "name": "B Busam",
        "id": "u4rJZwUAAAAJ"
      },
      {
        "name": "L Ott",
        "id": "BWpb8-kAAAAJ"
      },
      {
        "name": "F Tombari",
        "id": "TFsE4BIAAAAJ"
      },
      {
        "name": "R Siegwart",
        "id": "MDIyLnwAAAAJ"
      },
      {
        "name": "CA Avizzano2024 IEEE/RSJ International",
        "id": null
      }
    ],
    "citation_count": 13
  },
  {
    "arxiv_id": "2407.11859",
    "title": "Mitigating Background Shift in Class-Incremental Semantic Segmentation",
    "year": 2024,
    "published": "2024-07-16T15:44:37Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Class-Incremental Semantic Segmentation(CISS) aims to learn new classes without forgetting the old ones, using only the labels of the new classes. To achieve this, two popular strategies are employed: 1) pseudo-labeling and knowledge distillation to preserve prior knowledge; and 2) background weight transfer, which leverages the broad coverage of background in learning new classes by transferring background weight to the new class classifier. However, the first strategy heavily relies on the old",
    "arxiv_url": "https://arxiv.org/abs/2407.11859v1",
    "pdf_url": "https://arxiv.org/pdf/2407.11859v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2407.11859",
    "arxiv_authors": [
      "Gilhan Park",
      "WonJun Moon",
      "SuBeen Lee",
      "Tae-Young Kim",
      "Jae-Pil Heo"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Mitigating+Background+Shift+in+Class-Incremental+Semantic+Segmentation+Gilhan+Park+WonJun+Moon+SuBeen+Lee+Tae-Young+Kim+Jae-Pil+Heo",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "G Park",
        "id": "c6T-CjIAAAAJ"
      },
      {
        "name": "WJ Moon",
        "id": "_-FSWfoAAAAJ"
      },
      {
        "name": "SB Lee",
        "id": "ENwf2OkAAAAJ"
      },
      {
        "name": "TY Kim",
        "id": "8vCt8-QAAAAJ"
      },
      {
        "name": "JP Heo - European",
        "id": null
      }
    ],
    "citation_count": 15
  },
  {
    "arxiv_id": "2308.12558",
    "title": "Hyperbolic Audio-visual Zero-shot Learning",
    "year": 2023,
    "published": "2023-08-24T04:52:32Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Audio-visual zero-shot learning aims to classify samples consisting of a pair of corresponding audio and video sequences from classes that are not present during training. An analysis of the audio-visual data reveals a large degree of hyperbolicity, indicating the potential benefit of using a hyperbolic transformation to achieve curvature-aware geometric learning, with the aim of exploring more complex hierarchical data structures for this task. The proposed approach employs a novel loss functio",
    "arxiv_url": "https://arxiv.org/abs/2308.12558v2",
    "pdf_url": "https://arxiv.org/pdf/2308.12558v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2308.12558",
    "arxiv_authors": [
      "Jie Hong",
      "Zeeshan Hayder",
      "Junlin Han",
      "Pengfei Fang",
      "Mehrtash Harandi",
      "Lars Petersson"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Hyperbolic+Audio-visual+Zero-shot+Learning+Jie+Hong+Zeeshan+Hayder+Junlin+Han+Pengfei+Fang+Mehrtash+Harandi",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "J Hong",
        "id": "Uz7TC6IAAAAJ"
      },
      {
        "name": "Z Hayder",
        "id": "K2INPyYAAAAJ"
      },
      {
        "name": "J Han",
        "id": "5L0Uj_IAAAAJ"
      },
      {
        "name": "P Fang",
        "id": "Fk4A13IAAAAJ"
      },
      {
        "name": "M Harandi",
        "id": "Z9gvBegAAAAJ"
      },
      {
        "name": "L Petersson",
        "id": "32RHN4oAAAAJ"
      }
    ],
    "citation_count": 40
  },
  {
    "arxiv_id": "2503.08049",
    "title": "SphOR: A Representation Learning Perspective on Open-set Recognition for Identifying Unknown Classes in Deep Learning Models",
    "year": 2025,
    "published": "2025-03-11T05:06:11Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "The widespread use of deep learning classifiers necessitates Open-set recognition (OSR), which enables the identification of input data not only from classes known during training but also from unknown classes that might be present in test data. Many existing OSR methods are computationally expensive due to the reliance on complex generative models or suffer from high training costs. We investigate OSR from a representation-learning perspective, specifically through spherical embeddings. We intr",
    "arxiv_url": "https://arxiv.org/abs/2503.08049v2",
    "pdf_url": "https://arxiv.org/pdf/2503.08049v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2503.08049",
    "arxiv_authors": [
      "Nadarasar Bahavan",
      "Sachith Seneviratne",
      "Saman Halgamuge"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=SphOR%3A+A+Representation+Learning+Perspective+on+Open-set+Recognition+for+Identifying+Unknown+Classes+in+Deep+Learning+Models+Nadarasar+Bahavan+Sachith+Seneviratne+Saman+Halgamuge",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "N Bahavan",
        "id": "AW1LjIkAAAAJ"
      },
      {
        "name": "S Seneviratne",
        "id": "nvv8iZEAAAAJ"
      },
      {
        "name": "S Halgamuge -",
        "id": null
      }
    ],
    "citation_count": 3
  },
  {
    "arxiv_id": "2311.02922",
    "title": "Truly Scale-Equivariant Deep Nets with Fourier Layers",
    "year": 2023,
    "published": "2023-11-06T07:32:27Z",
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "abstract": "In computer vision, models must be able to adapt to changes in image resolution to effectively carry out tasks such as image segmentation; This is known as scale-equivariance. Recent works have made progress in developing scale-equivariant convolutional neural networks, e.g., through weight-sharing and kernel resizing. However, these networks are not truly scale-equivariant in practice. Specifically, they do not consider anti-aliasing as they formulate the down-scaling operation in the continuou",
    "arxiv_url": "https://arxiv.org/abs/2311.02922v1",
    "pdf_url": "https://arxiv.org/pdf/2311.02922v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2311.02922",
    "arxiv_authors": [
      "Md Ashiqur Rahman",
      "Raymond A. Yeh"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Truly+Scale-Equivariant+Deep+Nets+with+Fourier+Layers+Md+Ashiqur+Rahman+Raymond+A.+Yeh",
    "gs_search_success": false,
    "gs_authors": null,
    "citation_count": null,
    "error_type": "multiple_matches"
  },
  {
    "arxiv_id": "2312.12155",
    "title": "Towards Balanced Alignment: Modal-Enhanced Semantic Modeling for Video Moment Retrieval",
    "year": 2023,
    "published": "2023-12-19T13:38:48Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Video Moment Retrieval (VMR) aims to retrieve temporal segments in untrimmed videos corresponding to a given language query by constructing cross-modal alignment strategies. However, these existing strategies are often sub-optimal since they ignore the modality imbalance problem, \\textit{i.e.}, the semantic richness inherent in videos far exceeds that of a given limited-length sentence. Therefore, in pursuit of better alignment, a natural idea is enhancing the video modality to filter out query-",
    "arxiv_url": "https://arxiv.org/abs/2312.12155v1",
    "pdf_url": "https://arxiv.org/pdf/2312.12155v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2312.12155",
    "arxiv_authors": [
      "Zhihang Liu",
      "Jun Li",
      "Hongtao Xie",
      "Pandeng Li",
      "Jiannan Ge",
      "Sun-Ao Liu",
      "Guoqing Jin"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Towards+Balanced+Alignment%3A+Modal-Enhanced+Semantic+Modeling+for+Video+Moment+Retrieval+Zhihang+Liu+Jun+Li+Hongtao+Xie+Pandeng+Li+Jiannan+Ge",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "Z Liu",
        "id": null
      },
      {
        "name": "J Li",
        "id": null
      },
      {
        "name": "H Xie",
        "id": null
      },
      {
        "name": "P Li",
        "id": "Moy-4-0AAAAJ"
      },
      {
        "name": "J Ge",
        "id": "Y6xHVekAAAAJ"
      },
      {
        "name": "SA Liu",
        "id": "mU-igt0AAAAJ"
      },
      {
        "name": "G Jin -",
        "id": null
      }
    ],
    "citation_count": 43
  },
  {
    "arxiv_id": "2406.07707",
    "title": "A Deep Learning Approach to Detect Complete Safety Equipment For Construction Workers Based On YOLOv7",
    "year": 2024,
    "published": "2024-06-11T20:38:41Z",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "In the construction sector, ensuring worker safety is of the utmost significance. In this study, a deep learning-based technique is presented for identifying safety gear worn by construction workers, such as helmets, goggles, jackets, gloves, and footwears. The recommended approach uses the YOLO v7 (You Only Look Once) object detection algorithm to precisely locate these safety items. The dataset utilized in this work consists of labeled images split into training, testing and validation sets. E",
    "arxiv_url": "https://arxiv.org/abs/2406.07707v2",
    "pdf_url": "https://arxiv.org/pdf/2406.07707v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2406.07707",
    "arxiv_authors": [
      "Md. Shariful Islam",
      "SM Shaqib",
      "Shahriar Sultan Ramit",
      "Shahrun Akter Khushbu",
      "Abdus Sattar",
      "Sheak Rashed Haider Noori"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=A+Deep+Learning+Approach+to+Detect+Complete+Safety+Equipment+For+Construction+Workers+Based+On+YOLOv7+Md.+Shariful+Islam+SM+Shaqib+Shahriar+Sultan+Ramit+Shahrun+Akter+Khushbu+Abdus+Sattar",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "MS Islam",
        "id": null
      },
      {
        "name": "SM Shaqib",
        "id": "nNnRwrwAAAAJ"
      },
      {
        "name": "SS Ramit",
        "id": "beB-9aIAAAAJ"
      },
      {
        "name": "SA Khushbu",
        "id": null
      },
      {
        "name": "A Sattar",
        "id": null
      },
      {
        "name": "SRH Noori",
        "id": "rkAnD1wAAAAJ"
      }
    ],
    "citation_count": 11
  },
  {
    "arxiv_id": "2309.04022",
    "title": "Improving the Accuracy of Beauty Product Recommendations by Assessing Face Illumination Quality",
    "year": 2023,
    "published": "2023-09-07T21:29:21Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "We focus on addressing the challenges in responsible beauty product recommendation, particularly when it involves comparing the product's color with a person's skin tone, such as for foundation and concealer products. To make accurate recommendations, it is crucial to infer both the product attributes and the product specific facial features such as skin conditions or tone. However, while many product photos are taken under good light conditions, face photos are taken from a wide range of condit",
    "arxiv_url": "https://arxiv.org/abs/2309.04022v1",
    "pdf_url": "https://arxiv.org/pdf/2309.04022v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2309.04022",
    "arxiv_authors": [
      "Parnian Afshar",
      "Jenny Yeon",
      "Andriy Levitskyy",
      "Rahul Suresh",
      "Amin Banitalebi-Dehkordi"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Improving+the+Accuracy+of+Beauty+Product+Recommendations+by+Assessing+Face+Illumination+Quality+Parnian+Afshar+Jenny+Yeon+Andriy+Levitskyy+Rahul+Suresh+Amin+Banitalebi-Dehkordi",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "P Afshar",
        "id": "OGm8mW4AAAAJ"
      },
      {
        "name": "J Yeon",
        "id": null
      },
      {
        "name": "A Levitskyy",
        "id": null
      },
      {
        "name": "R Suresh",
        "id": "8UbsNJMAAAAJ"
      },
      {
        "name": "A Banitalebi-Dehkordi",
        "id": "fSz1PtYAAAAJ"
      }
    ],
    "citation_count": 1
  },
  {
    "arxiv_id": "2312.01850",
    "title": "Generalization by Adaptation: Diffusion-Based Domain Extension for Domain-Generalized Semantic Segmentation",
    "year": 2023,
    "published": "2023-12-04T12:31:45Z",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "When models, e.g., for semantic segmentation, are applied to images that are vastly different from training data, the performance will drop significantly. Domain adaptation methods try to overcome this issue, but need samples from the target domain. However, this might not always be feasible for various reasons and therefore domain generalization methods are useful as they do not require any target data. We present a new diffusion-based domain extension (DIDEX) method and employ a diffusion mode",
    "arxiv_url": "https://arxiv.org/abs/2312.01850v1",
    "pdf_url": "https://arxiv.org/pdf/2312.01850v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2312.01850",
    "arxiv_authors": [
      "Joshua Niemeijer",
      "Manuel Schwonberg",
      "Jan-Aike Term√∂hlen",
      "Nico M. Schmidt",
      "Tim Fingscheidt"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Generalization+by+Adaptation%3A+Diffusion-Based+Domain+Extension+for+Domain-Generalized+Semantic+Segmentation+Joshua+Niemeijer+Manuel+Schwonberg+Jan-Aike+Term%C3%B6hlen+Nico+M.+Schmidt+Tim+Fingscheidt",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "J Niemeijer",
        "id": "SK0mAJ0AAAAJ"
      },
      {
        "name": "M Schwonberg",
        "id": "eqsXwGIAAAAJ"
      },
      {
        "name": "JA Term√∂hlen",
        "id": "LkhzlxIAAAAJ"
      },
      {
        "name": "NM Schmidt",
        "id": "Kaei5zsAAAAJ"
      },
      {
        "name": "T Fingscheidt",
        "id": "KDgUWRMAAAAJ"
      }
    ],
    "citation_count": 57
  },
  {
    "arxiv_id": "2501.07451",
    "title": "A Survey on Dynamic Neural Networks: from Computer Vision to Multi-modal Sensor Fusion",
    "year": 2025,
    "published": "2025-01-13T16:24:49Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Model compression is essential in the deployment of large Computer Vision models on embedded devices. However, static optimization techniques (e.g. pruning, quantization, etc.) neglect the fact that different inputs have different complexities, thus requiring different amount of computations. Dynamic Neural Networks allow to condition the number of computations to the specific input. The current literature on the topic is very extensive and fragmented. We present a comprehensive survey that synt",
    "arxiv_url": "https://arxiv.org/abs/2501.07451v2",
    "pdf_url": "https://arxiv.org/pdf/2501.07451v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2501.07451",
    "arxiv_authors": [
      "Fabio Montello",
      "Ronja G√ºldenring",
      "Simone Scardapane",
      "Lazaros Nalpantidis"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=A+Survey+on+Dynamic+Neural+Networks%3A+from+Computer+Vision+to+Multi-modal+Sensor+Fusion+Fabio+Montello+Ronja+G%C3%BCldenring+Simone+Scardapane+Lazaros+Nalpantidis",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "F Montello",
        "id": "ZYf1TaEAAAAJ"
      },
      {
        "name": "R G√ºldenring",
        "id": "t2mUkG4AAAAJ"
      },
      {
        "name": "S Scardapane",
        "id": "aSuosYoAAAAJ"
      },
      {
        "name": "L Nalpantidis",
        "id": "YAx9230AAAAJ"
      }
    ],
    "citation_count": 3
  },
  {
    "arxiv_id": "2308.00307",
    "title": "Domain Adaptation based on Human Feedback for Enhancing Generative Model Denoising Abilities",
    "year": 2023,
    "published": "2023-08-01T05:59:02Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "How can we apply human feedback into generative model? As answer of this question, in this paper, we show the method applied on denoising problem and domain adaptation using human feedback. Deep generative models have demonstrated impressive results in image denoising. However, current image denoising models often produce inappropriate results when applied to domains different from the ones they were trained on. If there are `Good' and `Bad' result for unseen data, how to raise up quality of `Ba",
    "arxiv_url": "https://arxiv.org/abs/2308.00307v1",
    "pdf_url": "https://arxiv.org/pdf/2308.00307v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2308.00307",
    "arxiv_authors": [
      "Hyun-Cheol Park",
      "Sung Ho Kang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Domain+Adaptation+based+on+Human+Feedback+for+Enhancing+Generative+Model+Denoising+Abilities+Hyun-Cheol+Park+Sung+Ho+Kang",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "HC Park",
        "id": null
      },
      {
        "name": "SH Kang -",
        "id": null
      }
    ],
    "citation_count": 1
  },
  {
    "arxiv_id": "2307.02347",
    "title": "Detecting Images Generated by Deep Diffusion Models using their Local Intrinsic Dimensionality",
    "year": 2023,
    "published": "2023-07-05T15:03:10Z",
    "categories": [
      "cs.CV",
      "cs.CR"
    ],
    "abstract": "Diffusion models recently have been successfully applied for the visual synthesis of strikingly realistic appearing images. This raises strong concerns about their potential for malicious purposes. In this paper, we propose using the lightweight multi Local Intrinsic Dimensionality (multiLID), which has been originally developed in context of the detection of adversarial examples, for the automatic detection of synthetic images and the identification of the according generator networks. In contr",
    "arxiv_url": "https://arxiv.org/abs/2307.02347v7",
    "pdf_url": "https://arxiv.org/pdf/2307.02347v7",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2307.02347",
    "arxiv_authors": [
      "Peter Lorenz",
      "Ricard Durall",
      "Janis Keuper"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Detecting+Images+Generated+by+Deep+Diffusion+Models+using+their+Local+Intrinsic+Dimensionality+Peter+Lorenz+Ricard+Durall+Janis+Keuper",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "P Lorenz",
        "id": "sb4hPQMAAAAJ"
      },
      {
        "name": "RL Durall",
        "id": "bwxC07sAAAAJ"
      },
      {
        "name": "J Keuper -",
        "id": null
      }
    ],
    "citation_count": 60
  },
  {
    "arxiv_id": "2505.17544",
    "title": "FreqU-FNet: Frequency-Aware U-Net for Imbalanced Medical Image Segmentation",
    "year": 2025,
    "published": "2025-05-23T06:51:24Z",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "abstract": "Medical image segmentation faces persistent challenges due to severe class imbalance and the frequency-specific distribution of anatomical structures. Most conventional CNN-based methods operate in the spatial domain and struggle to capture minority class signals, often affected by frequency aliasing and limited spectral selectivity. Transformer-based models, while powerful in modeling global dependencies, tend to overlook critical local details necessary for fine-grained segmentation. To overco",
    "arxiv_url": "https://arxiv.org/abs/2505.17544v1",
    "pdf_url": "https://arxiv.org/pdf/2505.17544v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2505.17544",
    "arxiv_authors": [
      "Ruiqi Xing"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=FreqU-FNet%3A+Frequency-Aware+U-Net+for+Imbalanced+Medical+Image+Segmentation+Ruiqi+Xing",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "R Xing -",
        "id": null
      }
    ],
    "citation_count": 3
  },
  {
    "arxiv_id": "2310.08825",
    "title": "From CLIP to DINO: Visual Encoders Shout in Multi-modal Large Language Models",
    "year": 2023,
    "published": "2023-10-13T02:41:55Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Multi-modal Large Language Models (MLLMs) have made significant strides in expanding the capabilities of Large Language Models (LLMs) through the incorporation of visual perception interfaces. Despite the emergence of exciting applications and the availability of diverse instruction tuning data, existing approaches often rely on CLIP or its variants as the visual branch, and merely extract features from the deep layers. However, these methods lack a comprehensive analysis of the visual encoders ",
    "arxiv_url": "https://arxiv.org/abs/2310.08825v3",
    "pdf_url": "https://arxiv.org/pdf/2310.08825v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2310.08825",
    "arxiv_authors": [
      "Dongsheng Jiang",
      "Yuchen Liu",
      "Songlin Liu",
      "Jin'e Zhao",
      "Hao Zhang",
      "Zhen Gao",
      "Xiaopeng Zhang",
      "Jin Li",
      "Hongkai Xiong"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=From+CLIP+to+DINO%3A+Visual+Encoders+Shout+in+Multi-modal+Large+Language+Models+Dongsheng+Jiang+Yuchen+Liu+Songlin+Liu+Jin%27e+Zhao+Hao+Zhang",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "D Jiang",
        "id": "-eGIgsoAAAAJ"
      },
      {
        "name": "Y Liu",
        "id": "GRcH3nAAAAAJ"
      },
      {
        "name": "S Liu",
        "id": null
      },
      {
        "name": "J Zhao",
        "id": null
      },
      {
        "name": "H Zhang",
        "id": "Ud6aBAcAAAAJ"
      },
      {
        "name": "Z Gao",
        "id": null
      },
      {
        "name": "X Zhang",
        "id": null
      },
      {
        "name": "J Li",
        "id": "FJ89TCMAAAAJ"
      },
      {
        "name": "H Xiong",
        "id": "bB16iN4AAAAJ"
      }
    ],
    "citation_count": 3150
  },
  {
    "arxiv_id": "2408.11505",
    "title": "MSCPT: Few-shot Whole Slide Image Classification with Multi-scale and Context-focused Prompt Tuning",
    "year": 2024,
    "published": "2024-08-21T10:25:51Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Multiple instance learning (MIL) has become a standard paradigm for the weakly supervised classification of whole slide images (WSIs). However, this paradigm relies on using a large number of labeled WSIs for training. The lack of training data and the presence of rare diseases pose significant challenges for these methods. Prompt tuning combined with pre-trained Vision-Language models (VLMs) is an effective solution to the Few-shot Weakly Supervised WSI Classification (FSWC) task. Nevertheless,",
    "arxiv_url": "https://arxiv.org/abs/2408.11505v3",
    "pdf_url": "https://arxiv.org/pdf/2408.11505v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2408.11505",
    "arxiv_authors": [
      "Minghao Han",
      "Linhao Qu",
      "Dingkang Yang",
      "Xukun Zhang",
      "Xiaoying Wang",
      "Lihua Zhang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=MSCPT%3A+Few-shot+Whole+Slide+Image+Classification+with+Multi-scale+and+Context-focused+Prompt+Tuning+Minghao+Han+Linhao+Qu+Dingkang+Yang+Xukun+Zhang+Xiaoying+Wang",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "M Han",
        "id": "zE4vr08AAAAJ"
      },
      {
        "name": "L Qu",
        "id": "C8gTFhUAAAAJ"
      },
      {
        "name": "D Yang",
        "id": "jvlDhkcAAAAJ"
      },
      {
        "name": "X Zhang",
        "id": "34onOIoAAAAJ"
      },
      {
        "name": "X Wang",
        "id": null
      },
      {
        "name": "L ZhangIEEE Transactions on Medical Imaging",
        "id": null
      }
    ],
    "citation_count": 14
  },
  {
    "arxiv_id": "2402.01217",
    "title": "ID-NeRF: Indirect Diffusion-guided Neural Radiance Fields for Generalizable View Synthesis",
    "year": 2024,
    "published": "2024-02-02T08:39:51Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Implicit neural representations, represented by Neural Radiance Fields (NeRF), have dominated research in 3D computer vision by virtue of high-quality visual results and data-driven benefits. However, their realistic applications are hindered by the need for dense inputs and per-scene optimization. To solve this problem, previous methods implement generalizable NeRFs by extracting local features from sparse inputs as conditions for the NeRF decoder. However, although this way can allow feed-forw",
    "arxiv_url": "https://arxiv.org/abs/2402.01217v3",
    "pdf_url": "https://arxiv.org/pdf/2402.01217v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2402.01217",
    "arxiv_authors": [
      "Yaokun Li",
      "Chao Gou",
      "Guang Tan"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=ID-NeRF%3A+Indirect+Diffusion-guided+Neural+Radiance+Fields+for+Generalizable+View+Synthesis+Yaokun+Li+Chao+Gou+Guang+Tan",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "Y Li",
        "id": "plCD9wwAAAAJ"
      },
      {
        "name": "S Wang",
        "id": null
      },
      {
        "name": "G Tan - Expert Systems with Applications",
        "id": null
      }
    ],
    "citation_count": 7
  },
  {
    "arxiv_id": "2409.01128",
    "title": "Diffusion-Driven Data Replay: A Novel Approach to Combat Forgetting in Federated Class Continual Learning",
    "year": 2024,
    "published": "2024-09-02T10:07:24Z",
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "abstract": "Federated Class Continual Learning (FCCL) merges the challenges of distributed client learning with the need for seamless adaptation to new classes without forgetting old ones. The key challenge in FCCL is catastrophic forgetting, an issue that has been explored to some extent in Continual Learning (CL). However, due to privacy preservation requirements, some conventional methods, such as experience replay, are not directly applicable to FCCL. Existing FCCL methods mitigate forgetting by generat",
    "arxiv_url": "https://arxiv.org/abs/2409.01128v2",
    "pdf_url": "https://arxiv.org/pdf/2409.01128v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2409.01128",
    "arxiv_authors": [
      "Jinglin Liang",
      "Jin Zhong",
      "Hanlin Gu",
      "Zhongqi Lu",
      "Xingxing Tang",
      "Gang Dai",
      "Shuangping Huang",
      "Lixin Fan",
      "Qiang Yang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Diffusion-Driven+Data+Replay%3A+A+Novel+Approach+to+Combat+Forgetting+in+Federated+Class+Continual+Learning+Jinglin+Liang+Jin+Zhong+Hanlin+Gu+Zhongqi+Lu+Xingxing+Tang",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "J Liang",
        "id": "quFRR4QAAAAJ"
      },
      {
        "name": "J Zhong",
        "id": null
      },
      {
        "name": "H Gu",
        "id": "QDfUnf0AAAAJ"
      },
      {
        "name": "Z Lu",
        "id": null
      },
      {
        "name": "X Tang",
        "id": "a2SwkisAAAAJ"
      },
      {
        "name": "G Dai",
        "id": null
      },
      {
        "name": "S Huang",
        "id": "yEvxW0sAAAAJ"
      },
      {
        "name": "L Fan",
        "id": "fOsgdn0AAAAJ"
      },
      {
        "name": "Q YangEuropean",
        "id": null
      }
    ],
    "citation_count": 28
  },
  {
    "arxiv_id": "2310.11449",
    "title": "DELIFFAS: Deformable Light Fields for Fast Avatar Synthesis",
    "year": 2023,
    "published": "2023-10-17T17:58:00Z",
    "categories": [
      "cs.CV",
      "cs.GR",
      "cs.LG"
    ],
    "abstract": "Generating controllable and photorealistic digital human avatars is a long-standing and important problem in Vision and Graphics. Recent methods have shown great progress in terms of either photorealism or inference speed while the combination of the two desired properties still remains unsolved. To this end, we propose a novel method, called DELIFFAS, which parameterizes the appearance of the human as a surface light field that is attached to a controllable and deforming human mesh model. At th",
    "arxiv_url": "https://arxiv.org/abs/2310.11449v1",
    "pdf_url": "https://arxiv.org/pdf/2310.11449v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2310.11449",
    "arxiv_authors": [
      "Youngjoong Kwon",
      "Lingjie Liu",
      "Henry Fuchs",
      "Marc Habermann",
      "Christian Theobalt"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=DELIFFAS%3A+Deformable+Light+Fields+for+Fast+Avatar+Synthesis+Youngjoong+Kwon+Lingjie+Liu+Henry+Fuchs+Marc+Habermann+Christian+Theobalt",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "Y Kwon",
        "id": "j9RL6X0AAAAJ"
      },
      {
        "name": "L Liu",
        "id": "HZPnJ9gAAAAJ"
      },
      {
        "name": "H Fuchs",
        "id": "guhwcP8AAAAJ"
      },
      {
        "name": "M Habermann",
        "id": "oWstvNcAAAAJ"
      },
      {
        "name": "C TheobaltAdvances in neural information processing systems",
        "id": null
      }
    ],
    "citation_count": 44
  },
  {
    "arxiv_id": "2303.16609",
    "title": "Modified watershed approach for segmentation of complex optical coherence tomographic images",
    "year": 2023,
    "published": "2023-03-29T11:48:46Z",
    "categories": [
      "eess.IV",
      "cs.CV",
      "physics.med-ph"
    ],
    "abstract": "Watershed segmentation method has been used in various applications. But many a times, due to its over-segmentation attributes, it underperforms in several tasks where noise is a dominant source. In this study, Optical Coherence Tomography images have been acquired, and segmentation has been performed to analyse the different regions of fluid filled sacs in a lemon. A modified watershed algorithm has been proposed which gives promising results for segmentation of internal lemon structures.",
    "arxiv_url": "https://arxiv.org/abs/2303.16609v1",
    "pdf_url": "https://arxiv.org/pdf/2303.16609v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2303.16609",
    "arxiv_authors": [
      "Maryam Viqar",
      "Violeta Madjarova",
      "Elena Stoykova"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Modified+watershed+approach+for+segmentation+of+complex+optical+coherence+tomographic+images+Maryam+Viqar+Violeta+Madjarova+Elena+Stoykova",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "M Viqar",
        "id": null
      },
      {
        "name": "V Madjarova",
        "id": "vsRr5rsAAAAJ"
      },
      {
        "name": "E Stoykova -",
        "id": null
      }
    ],
    "citation_count": 2
  },
  {
    "arxiv_id": "2501.18313",
    "title": "Simulation of microstructures and machine learning",
    "year": 2025,
    "published": "2025-01-30T12:43:17Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Machine learning offers attractive solutions to challenging image processing tasks. Tedious development and parametrization of algorithmic solutions can be replaced by training a convolutional neural network or a random forest with a high potential to generalize. However, machine learning methods rely on huge amounts of representative image data along with a ground truth, usually obtained by manual annotation. Thus, limited availability of training data is a critical bottleneck. We discuss two u",
    "arxiv_url": "https://arxiv.org/abs/2501.18313v1",
    "pdf_url": "https://arxiv.org/pdf/2501.18313v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2501.18313",
    "arxiv_authors": [
      "Katja Schladitz",
      "Claudia Redenbach",
      "Tin Barisin",
      "Christian Jung",
      "Natascha Jeziorski",
      "Lovro Bosnar",
      "Juraj Fulir",
      "Petra Gospodnetiƒá"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Simulation+of+microstructures+and+machine+learning+Katja+Schladitz+Claudia+Redenbach+Tin+Barisin+Christian+Jung+Natascha+Jeziorski",
    "gs_search_success": false,
    "gs_authors": null,
    "citation_count": null,
    "error_type": "fetch_failed"
  },
  {
    "arxiv_id": "2306.16181",
    "title": "Learning to Pan-sharpening with Memories of Spatial Details",
    "year": 2023,
    "published": "2023-06-28T13:03:43Z",
    "categories": [
      "cs.CV",
      "cs.MM",
      "eess.IV"
    ],
    "abstract": "Pan-sharpening, as one of the most commonly used techniques in remote sensing systems, aims to inject spatial details from panchromatic images into multispectral images (MS) to obtain high-resolution multispectral images. Since deep learning has received widespread attention because of its powerful fitting ability and efficient feature extraction, a variety of pan-sharpening methods have been proposed to achieve remarkable performance. However, current pan-sharpening methods usually require the ",
    "arxiv_url": "https://arxiv.org/abs/2306.16181v3",
    "pdf_url": "https://arxiv.org/pdf/2306.16181v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2306.16181",
    "arxiv_authors": [
      "Maoxun Yuan",
      "Tianyi Zhao",
      "Bo Li",
      "Xingxing Wei"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Learning+to+Pan-sharpening+with+Memories+of+Spatial+Details+Maoxun+Yuan+Tianyi+Zhao+Bo+Li+Xingxing+Wei",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "M Yuan",
        "id": "I6aiGq8AAAAJ"
      },
      {
        "name": "T Zhao",
        "id": null
      },
      {
        "name": "B Li",
        "id": null
      },
      {
        "name": "X Wei -",
        "id": null
      }
    ],
    "citation_count": 2
  },
  {
    "arxiv_id": "2505.18881",
    "title": "SD-OVON: A Semantics-aware Dataset and Benchmark Generation Pipeline for Open-Vocabulary Object Navigation in Dynamic Scenes",
    "year": 2025,
    "published": "2025-05-24T21:37:06Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.RO"
    ],
    "abstract": "We present the Semantics-aware Dataset and Benchmark Generation Pipeline for Open-vocabulary Object Navigation in Dynamic Scenes (SD-OVON). It utilizes pretraining multimodal foundation models to generate infinite unique photo-realistic scene variants that adhere to real-world semantics and daily commonsense for the training and the evaluation of navigation agents, accompanied with a plugin for generating object navigation task episodes compatible to the Habitat simulator. In addition, we offer ",
    "arxiv_url": "https://arxiv.org/abs/2505.18881v1",
    "pdf_url": "https://arxiv.org/pdf/2505.18881v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2505.18881",
    "arxiv_authors": [
      "Dicong Qiu",
      "Jiadi You",
      "Zeying Gong",
      "Ronghe Qiu",
      "Hui Xiong",
      "Junwei Liang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=SD-OVON%3A+A+Semantics-aware+Dataset+and+Benchmark+Generation+Pipeline+for+Open-Vocabulary+Object+Navigation+in+Dynamic+Scenes+Dicong+Qiu+Jiadi+You+Zeying+Gong+Ronghe+Qiu+Hui+Xiong",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "D Qiu",
        "id": "ZFmSow8AAAAJ"
      },
      {
        "name": "J You",
        "id": null
      },
      {
        "name": "Z Gong",
        "id": "ze2Wh9EAAAAJ"
      },
      {
        "name": "R Qiu",
        "id": "QOwOHZMAAAAJ"
      },
      {
        "name": "H Xiong",
        "id": null
      },
      {
        "name": "J Liang",
        "id": "bMedjfUAAAAJ"
      }
    ],
    "citation_count": null
  },
  {
    "arxiv_id": "2306.10689",
    "title": "Realistic Restorer: artifact-free flow restorer(AF2R) for MRI motion artifact removal",
    "year": 2023,
    "published": "2023-06-19T04:02:01Z",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "abstract": "Motion artifact is a major challenge in magnetic resonance imaging (MRI) that severely degrades image quality, reduces examination efficiency, and makes accurate diagnosis difficult. However, previous methods often relied on implicit models for artifact correction, resulting in biases in modeling the artifact formation mechanism and characterizing the relationship between artifact information and anatomical details. These limitations have hindered the ability to obtain high-quality MR images. In",
    "arxiv_url": "https://arxiv.org/abs/2306.10689v1",
    "pdf_url": "https://arxiv.org/pdf/2306.10689v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2306.10689",
    "arxiv_authors": [
      "Jiandong Su",
      "Kun Shang",
      "Dong Liang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Realistic+Restorer%3A+artifact-free+flow+restorer%28AF2R%29+for+MRI+motion+artifact+removal+Jiandong+Su+Kun+Shang+Dong+Liang",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "J Su",
        "id": null
      },
      {
        "name": "K Shang",
        "id": "N3ZIPaoAAAAJ"
      },
      {
        "name": "D Liang -",
        "id": null
      }
    ],
    "citation_count": 1
  },
  {
    "arxiv_id": "2503.09403",
    "title": "Multi-Agent Image Restoration",
    "year": 2025,
    "published": "2025-03-12T13:53:57Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Image restoration (IR) is challenging due to the complexity of real-world degradations. While many specialized and all-in-one IR models have been developed, they fail to effectively handle complex, mixed degradations. Recent agentic methods RestoreAgent and AgenticIR leverage intelligent, autonomous workflows to alleviate this issue, yet they suffer from suboptimal results and inefficiency due to their resource-intensive finetunings, and ineffective searches and tool execution trials for satisfa",
    "arxiv_url": "https://arxiv.org/abs/2503.09403v2",
    "pdf_url": "https://arxiv.org/pdf/2503.09403v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2503.09403",
    "arxiv_authors": [
      "Xu Jiang",
      "Gehui Li",
      "Bin Chen",
      "Jian Zhang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Multi-Agent+Image+Restoration+Xu+Jiang+Gehui+Li+Bin+Chen+Jian+Zhang",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "X Jiang",
        "id": "tJaYoQgAAAAJ"
      },
      {
        "name": "G Li",
        "id": null
      },
      {
        "name": "B Chen",
        "id": "aZDNm98AAAAJ"
      },
      {
        "name": "J Zhang -",
        "id": null
      }
    ],
    "citation_count": 6
  },
  {
    "arxiv_id": "2303.07150",
    "title": "Multi PILOT: Learned Feasible Multiple Acquisition Trajectories for Dynamic MRI",
    "year": 2023,
    "published": "2023-03-13T14:23:39Z",
    "categories": [
      "eess.IV",
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "Dynamic Magnetic Resonance Imaging (MRI) is known to be a powerful and reliable technique for the dynamic imaging of internal organs and tissues, making it a leading diagnostic tool. A major difficulty in using MRI in this setting is the relatively long acquisition time (and, hence, increased cost) required for imaging in high spatio-temporal resolution, leading to the appearance of related motion artifacts and decrease in resolution. Compressed Sensing (CS) techniques have become a common tool ",
    "arxiv_url": "https://arxiv.org/abs/2303.07150v2",
    "pdf_url": "https://arxiv.org/pdf/2303.07150v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2303.07150",
    "arxiv_authors": [
      "Tamir Shor",
      "Tomer Weiss",
      "Dor Noti",
      "Alex Bronstein"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Multi+PILOT%3A+Learned+Feasible+Multiple+Acquisition+Trajectories+for+Dynamic+MRI+Tamir+Shor+Tomer+Weiss+Dor+Noti+Alex+Bronstein",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "T Shor",
        "id": "Tqad8mkAAAAJ"
      },
      {
        "name": "T Weiss",
        "id": "TRGkuYAAAAAJ"
      },
      {
        "name": "D Noti",
        "id": null
      },
      {
        "name": "A Bronstein -",
        "id": null
      }
    ],
    "citation_count": 1
  },
  {
    "arxiv_id": "2302.10412",
    "title": "Non-pooling Network for medical image segmentation",
    "year": 2023,
    "published": "2023-02-21T02:49:16Z",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "abstract": "Existing studies tend tofocus onmodel modifications and integration with higher accuracy, which improve performance but also carry huge computational costs, resulting in longer detection times. Inmedical imaging, the use of time is extremely sensitive. And at present most of the semantic segmentation models have encoder-decoder structure or double branch structure. Their several times of the pooling use with high-level semantic information extraction operation cause information loss although the",
    "arxiv_url": "https://arxiv.org/abs/2302.10412v1",
    "pdf_url": "https://arxiv.org/pdf/2302.10412v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2302.10412",
    "arxiv_authors": [
      "Weihu Song",
      "Heng Yu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Non-pooling+Network+for+medical+image+segmentation+Weihu+Song+Heng+Yu",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "W Song",
        "id": "ttZdtlYKymIC"
      },
      {
        "name": "H Yu",
        "id": "fOCAyuIAAAAJ"
      },
      {
        "name": "J Wu - International",
        "id": null
      }
    ],
    "citation_count": 2
  },
  {
    "arxiv_id": "2403.09400",
    "title": "ConDiSR: Contrastive Disentanglement and Style Regularization for Single Domain Generalization",
    "year": 2024,
    "published": "2024-03-14T13:50:44Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Medical data often exhibits distribution shifts, which cause test-time performance degradation for deep learning models trained using standard supervised learning pipelines. This challenge is addressed in the field of Domain Generalization (DG) with the sub-field of Single Domain Generalization (SDG) being specifically interesting due to the privacy- or logistics-related issues often associated with medical data. Existing disentanglement-based SDG methods heavily rely on structural information e",
    "arxiv_url": "https://arxiv.org/abs/2403.09400v3",
    "pdf_url": "https://arxiv.org/pdf/2403.09400v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2403.09400",
    "arxiv_authors": [
      "Aleksandr Matsun",
      "Numan Saeed",
      "Fadillah Adamsyah Maani",
      "Mohammad Yaqub"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=ConDiSR%3A+Contrastive+Disentanglement+and+Style+Regularization+for+Single+Domain+Generalization+Aleksandr+Matsun+Numan+Saeed+Fadillah+Adamsyah+Maani+Mohammad+Yaqub",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "A Matsun",
        "id": "w4n7nicAAAAJ"
      },
      {
        "name": "N Saeed",
        "id": "VMPEU20AAAAJ"
      },
      {
        "name": "FA Maani",
        "id": "W-4975wAAAAJ"
      },
      {
        "name": "M Yaqub2025 IEEE/CVF Winter",
        "id": null
      }
    ],
    "citation_count": 1
  },
  {
    "arxiv_id": "2308.09201",
    "title": "TinyProp -- Adaptive Sparse Backpropagation for Efficient TinyML On-device Learning",
    "year": 2023,
    "published": "2023-08-17T22:32:32Z",
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "abstract": "Training deep neural networks using backpropagation is very memory and computationally intensive. This makes it difficult to run on-device learning or fine-tune neural networks on tiny, embedded devices such as low-power micro-controller units (MCUs). Sparse backpropagation algorithms try to reduce the computational load of on-device learning by training only a subset of the weights and biases. Existing approaches use a static number of weights to train. A poor choice of this so-called backpropa",
    "arxiv_url": "https://arxiv.org/abs/2308.09201v1",
    "pdf_url": "https://arxiv.org/pdf/2308.09201v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2308.09201",
    "arxiv_authors": [
      "Marcus R√ºb",
      "Daniel Maier",
      "Daniel Mueller-Gritschneder",
      "Axel Sikora"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=TinyProp+--+Adaptive+Sparse+Backpropagation+for+Efficient+TinyML+On-device+Learning+Marcus+R%C3%BCb+Daniel+Maier+Daniel+Mueller-Gritschneder+Axel+Sikora",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "M R√ºb",
        "id": "NP0HQCEAAAAJ"
      },
      {
        "name": "D Maier",
        "id": null
      },
      {
        "name": "D Mueller-Gritschneder",
        "id": "uIy75_wAAAAJ"
      },
      {
        "name": "A Sikora",
        "id": "YLOrQroAAAAJ"
      }
    ],
    "citation_count": 4
  },
  {
    "arxiv_id": "2312.09922",
    "title": "A Unifying Tensor View for Lightweight CNNs",
    "year": 2023,
    "published": "2023-12-15T16:30:59Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Despite the decomposition of convolutional kernels for lightweight CNNs being well studied, existing works that rely on tensor network diagrams or hyperdimensional abstraction lack geometry intuition. This work devises a new perspective by linking a 3D-reshaped kernel tensor to its various slice-wise and rank-1 decompositions, permitting a straightforward connection between various tensor approximations and efficient CNN modules. Specifically, it is discovered that a pointwise-depthwise-pointwis",
    "arxiv_url": "https://arxiv.org/abs/2312.09922v1",
    "pdf_url": "https://arxiv.org/pdf/2312.09922v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2312.09922",
    "arxiv_authors": [
      "Jason Chun Lok Li",
      "Rui Lin",
      "Jiajun Zhou",
      "Edmund Yin Mun Lam",
      "Ngai Wong"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=A+Unifying+Tensor+View+for+Lightweight+CNNs+Jason+Chun+Lok+Li+Rui+Lin+Jiajun+Zhou+Edmund+Yin+Mun+Lam+Ngai+Wong",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "JCL Li",
        "id": "Tcpdsh0AAAAJ"
      },
      {
        "name": "R Lin",
        "id": "gx0RITkAAAAJ"
      },
      {
        "name": "J Zhou",
        "id": "4KQ6SKUAAAAJ"
      },
      {
        "name": "EYM Lam",
        "id": "MvOSTcUAAAAJ"
      },
      {
        "name": "N Wong2023 IEEE 15th International",
        "id": null
      }
    ],
    "citation_count": 1
  },
  {
    "arxiv_id": "2404.19615",
    "title": "SemiPL: A Semi-supervised Method for Event Sound Source Localization",
    "year": 2024,
    "published": "2024-04-30T15:13:57Z",
    "categories": [
      "cs.CV",
      "cs.MM",
      "cs.SD",
      "eess.AS"
    ],
    "abstract": "In recent years, Event Sound Source Localization has been widely applied in various fields. Recent works typically relying on the contrastive learning framework show impressive performance. However, all work is based on large relatively simple datasets. It's also crucial to understand and analyze human behaviors (actions and interactions of people), voices, and sounds in chaotic events in many applications, e.g., crowd management, and emergency response services. In this paper, we apply the exis",
    "arxiv_url": "https://arxiv.org/abs/2404.19615v1",
    "pdf_url": "https://arxiv.org/pdf/2404.19615v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2404.19615",
    "arxiv_authors": [
      "Yue Li",
      "Baiqiao Yin",
      "Jinfu Liu",
      "Jiajun Wen",
      "Jiaying Lin",
      "Mengyuan Liu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=SemiPL%3A+A+Semi-supervised+Method+for+Event+Sound+Source+Localization+Yue+Li+Baiqiao+Yin+Jinfu+Liu+Jiajun+Wen+Jiaying+Lin",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "Y Li",
        "id": "VzV1ps8AAAAJ"
      },
      {
        "name": "B Yin",
        "id": null
      },
      {
        "name": "J Liu",
        "id": "jdOJpl0AAAAJ"
      },
      {
        "name": "J Wen",
        "id": "goMNmRIAAAAJ"
      },
      {
        "name": "J Lin",
        "id": "kU4rtNQAAAAJ"
      },
      {
        "name": "M Liu2024 IEEE International",
        "id": null
      }
    ],
    "citation_count": 2
  },
  {
    "arxiv_id": "2411.14807",
    "title": "Harlequin: Color-driven Generation of Synthetic Data for Referring Expression Comprehension",
    "year": 2024,
    "published": "2024-11-22T09:08:36Z",
    "categories": [
      "cs.CV",
      "cs.CL",
      "cs.LG"
    ],
    "abstract": "Referring Expression Comprehension (REC) aims to identify a particular object in a scene by a natural language expression, and is an important topic in visual language understanding. State-of-the-art methods for this task are based on deep learning, which generally requires expensive and manually labeled annotations. Some works tackle the problem with limited-supervision learning or relying on Large Vision and Language Models. However, the development of techniques to synthesize labeled data is ",
    "arxiv_url": "https://arxiv.org/abs/2411.14807v1",
    "pdf_url": "https://arxiv.org/pdf/2411.14807v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2411.14807",
    "arxiv_authors": [
      "Luca Parolari",
      "Elena Izzo",
      "Lamberto Ballan"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Harlequin%3A+Color-driven+Generation+of+Synthetic+Data+for+Referring+Expression+Comprehension+Luca+Parolari+Elena+Izzo+Lamberto+Ballan",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "L Parolari",
        "id": "NYXQKKAAAAAJ"
      },
      {
        "name": "E Izzo",
        "id": "lHkGZWUAAAAJ"
      },
      {
        "name": "L Ballan - International",
        "id": null
      }
    ],
    "citation_count": 2
  },
  {
    "arxiv_id": "2310.06337",
    "title": "Local Style Awareness of Font Images",
    "year": 2023,
    "published": "2023-10-10T06:13:09Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "When we compare fonts, we often pay attention to styles of local parts, such as serifs and curvatures. This paper proposes an attention mechanism to find important local parts. The local parts with larger attention are then considered important. The proposed mechanism can be trained in a quasi-self-supervised manner that requires no manual annotation other than knowing that a set of character images is from the same font, such as Helvetica. After confirming that the trained attention mechanism c",
    "arxiv_url": "https://arxiv.org/abs/2310.06337v1",
    "pdf_url": "https://arxiv.org/pdf/2310.06337v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2310.06337",
    "arxiv_authors": [
      "Daichi Haraguchi",
      "Seiichi Uchida"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Local+Style+Awareness+of+Font+Images+Daichi+Haraguchi+Seiichi+Uchida",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "D Haraguchi",
        "id": "oEna_HAAAAAJ"
      },
      {
        "name": "S Uchida - International",
        "id": null
      }
    ],
    "citation_count": 1
  },
  {
    "arxiv_id": "2503.14111",
    "title": "Towards properties of adversarial image perturbations",
    "year": 2025,
    "published": "2025-03-18T10:28:14Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Using stochastic gradient approach we study the properties of adversarial perturbations resulting in noticeable growth of VMAF image quality metric. The structure of the perturbations is investigated depending on the acceptable PSNR values and based on the Fourier power spectrum computations for the perturbations. It is demonstrated that moderate variation of image brightness ($\\sim 10$ pixel units in a restricted region of an image can result in VMAF growth by $\\sim 60\\%$). Unlike some other me",
    "arxiv_url": "https://arxiv.org/abs/2503.14111v2",
    "pdf_url": "https://arxiv.org/pdf/2503.14111v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2503.14111",
    "arxiv_authors": [
      "Egor Kuznetsov",
      "Kirill Aistov",
      "Maxim Koroteev"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Towards+properties+of+adversarial+image+perturbations+Egor+Kuznetsov+Kirill+Aistov+Maxim+Koroteev",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "E Kuznetsov",
        "id": null
      },
      {
        "name": "K Aistov",
        "id": null
      },
      {
        "name": "M Koroteev -",
        "id": null
      }
    ],
    "citation_count": null
  },
  {
    "arxiv_id": "2502.19697",
    "title": "Prompt-driven Transferable Adversarial Attack on Person Re-Identification with Attribute-aware Textual Inversion",
    "year": 2025,
    "published": "2025-02-27T02:32:58Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Person re-identification (re-id) models are vital in security surveillance systems, requiring transferable adversarial attacks to explore the vulnerabilities of them. Recently, vision-language models (VLM) based attacks have shown superior transferability by attacking generalized image and textual features of VLM, but they lack comprehensive feature disruption due to the overemphasis on discriminative semantics in integral representation. In this paper, we introduce the Attribute-aware Prompt At",
    "arxiv_url": "https://arxiv.org/abs/2502.19697v3",
    "pdf_url": "https://arxiv.org/pdf/2502.19697v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2502.19697",
    "arxiv_authors": [
      "Yuan Bian",
      "Min Liu",
      "Yunqi Yi",
      "Xueping Wang",
      "Yaonan Wang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Prompt-driven+Transferable+Adversarial+Attack+on+Person+Re-Identification+with+Attribute-aware+Textual+Inversion+Yuan+Bian+Min+Liu+Yunqi+Yi+Xueping+Wang+Yaonan+Wang",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "Y Bian",
        "id": null
      },
      {
        "name": "M Liu",
        "id": null
      },
      {
        "name": "Y Yi",
        "id": null
      },
      {
        "name": "X Wang",
        "id": "GOx4B9MAAAAJ"
      },
      {
        "name": "S Jiang",
        "id": "9yWuLtsAAAAJ"
      },
      {
        "name": "Y Wang",
        "id": null
      }
    ],
    "citation_count": null
  },
  {
    "arxiv_id": "2308.14409",
    "title": "Steerable Conditional Diffusion for Out-of-Distribution Adaptation in Medical Image Reconstruction",
    "year": 2023,
    "published": "2023-08-28T08:47:06Z",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "Denoising diffusion models have emerged as the go-to generative framework for solving inverse problems in imaging. A critical concern regarding these models is their performance on out-of-distribution tasks, which remains an under-explored challenge. Using a diffusion model on an out-of-distribution dataset, realistic reconstructions can be generated, but with hallucinating image features that are uniquely present in the training dataset. To address this discrepancy during train-test time and im",
    "arxiv_url": "https://arxiv.org/abs/2308.14409v3",
    "pdf_url": "https://arxiv.org/pdf/2308.14409v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2308.14409",
    "arxiv_authors": [
      "Riccardo Barbano",
      "Alexander Denker",
      "Hyungjin Chung",
      "Tae Hoon Roh",
      "Simon Arridge",
      "Peter Maass",
      "Bangti Jin",
      "Jong Chul Ye"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Steerable+Conditional+Diffusion+for+Out-of-Distribution+Adaptation+in+Medical+Image+Reconstruction+Riccardo+Barbano+Alexander+Denker+Hyungjin+Chung+Tae+Hoon+Roh+Simon+Arridge",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "R Barbano",
        "id": "6jYGiC0AAAAJ"
      },
      {
        "name": "A Denker",
        "id": "j5NT9kAAAAAJ"
      },
      {
        "name": "H Chung",
        "id": "KdchEyoAAAAJ"
      },
      {
        "name": "TH Roh",
        "id": "AbhyArgAAAAJ"
      },
      {
        "name": "S Arridge",
        "id": "UKy9ejwAAAAJ"
      },
      {
        "name": "P Maass",
        "id": "NUjVCEwAAAAJ"
      },
      {
        "name": "B Jin",
        "id": "8Axgx3wAAAAJ"
      },
      {
        "name": "JC YeIEEE Transactions on Medical Imaging",
        "id": null
      }
    ],
    "citation_count": 14
  },
  {
    "arxiv_id": "2407.21317",
    "title": "Pathology Foundation Models",
    "year": 2024,
    "published": "2024-07-31T03:58:48Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Pathology has played a crucial role in the diagnosis and evaluation of patient tissue samples obtained from surgeries and biopsies for many years. The advent of Whole Slide Scanners and the development of deep learning technologies have significantly advanced the field, leading to extensive research and development in pathology AI (Artificial Intelligence). These advancements have contributed to reducing the workload of pathologists and supporting decision-making in treatment plans. Recently, la",
    "arxiv_url": "https://arxiv.org/abs/2407.21317v2",
    "pdf_url": "https://arxiv.org/pdf/2407.21317v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2407.21317",
    "arxiv_authors": [
      "Mieko Ochi",
      "Daisuke Komura",
      "Shumpei Ishikawa"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Pathology+Foundation+Models+Mieko+Ochi+Daisuke+Komura+Shumpei+Ishikawa",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "M Ochi",
        "id": null
      },
      {
        "name": "D Komura",
        "id": "k7DqR0EAAAAJ"
      },
      {
        "name": "S Ishikawa - JMA",
        "id": null
      }
    ],
    "citation_count": 24
  },
  {
    "arxiv_id": "2308.12435",
    "title": "Characterising representation dynamics in recurrent neural networks for object recognition",
    "year": 2023,
    "published": "2023-08-23T21:36:35Z",
    "categories": [
      "cs.CV",
      "q-bio.NC"
    ],
    "abstract": "Recurrent neural networks (RNNs) have yielded promising results for both recognizing objects in challenging conditions and modeling aspects of primate vision. However, the representational dynamics of recurrent computations remain poorly understood, especially in large-scale visual models. Here, we studied such dynamics in RNNs trained for object classification on MiniEcoset, a novel subset of ecoset. We report two main insights. First, upon inference, representations continued to evolve after c",
    "arxiv_url": "https://arxiv.org/abs/2308.12435v2",
    "pdf_url": "https://arxiv.org/pdf/2308.12435v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2308.12435",
    "arxiv_authors": [
      "Sushrut Thorat",
      "Adrien Doerig",
      "Tim C. Kietzmann"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Characterising+representation+dynamics+in+recurrent+neural+networks+for+object+recognition+Sushrut+Thorat+Adrien+Doerig+Tim+C.+Kietzmann",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "S Thorat",
        "id": "MPFzJQgAAAAJ"
      },
      {
        "name": "A Doerig",
        "id": "YA6DPIcAAAAJ"
      },
      {
        "name": "TC Kietzmann -",
        "id": null
      }
    ],
    "citation_count": 6
  },
  {
    "arxiv_id": "2305.00552",
    "title": "Deep Learning-based Spatio Temporal Facial Feature Visual Speech Recognition",
    "year": 2023,
    "published": "2023-04-30T18:52:29Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "In low-resource computing contexts, such as smartphones and other tiny devices, Both deep learning and machine learning are being used in a lot of identification systems. as authentication techniques. The transparent, contactless, and non-invasive nature of these face recognition technologies driven by AI has led to their meteoric rise in popularity in recent years. While they are mostly successful, there are still methods to get inside without permission by utilising things like pictures, masks",
    "arxiv_url": "https://arxiv.org/abs/2305.00552v1",
    "pdf_url": "https://arxiv.org/pdf/2305.00552v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2305.00552",
    "arxiv_authors": [
      "Pangoth Santhosh Kumar",
      "Garika Akshay"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Deep+Learning-based+Spatio+Temporal+Facial+Feature+Visual+Speech+Recognition+Pangoth+Santhosh+Kumar+Garika+Akshay",
    "gs_search_success": false,
    "gs_authors": null,
    "citation_count": null,
    "error_type": "fetch_failed"
  },
  {
    "arxiv_id": "2406.18151",
    "title": "SynRS3D: A Synthetic Dataset for Global 3D Semantic Understanding from Monocular Remote Sensing Imagery",
    "year": 2024,
    "published": "2024-06-26T08:04:42Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Global semantic 3D understanding from single-view high-resolution remote sensing (RS) imagery is crucial for Earth Observation (EO). However, this task faces significant challenges due to the high costs of annotations and data collection, as well as geographically restricted data availability. To address these challenges, synthetic data offer a promising solution by being easily accessible and thus enabling the provision of large and diverse datasets. We develop a specialized synthetic data gene",
    "arxiv_url": "https://arxiv.org/abs/2406.18151v2",
    "pdf_url": "https://arxiv.org/pdf/2406.18151v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2406.18151",
    "arxiv_authors": [
      "Jian Song",
      "Hongruixuan Chen",
      "Weihao Xuan",
      "Junshi Xia",
      "Naoto Yokoya"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=SynRS3D%3A+A+Synthetic+Dataset+for+Global+3D+Semantic+Understanding+from+Monocular+Remote+Sensing+Imagery+Jian+Song+Hongruixuan+Chen+Weihao+Xuan+Junshi+Xia+Naoto+Yokoya",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "J Song",
        "id": "I-Mr4x0AAAAJ"
      },
      {
        "name": "H Chen",
        "id": "XOk4Cf0AAAAJ"
      },
      {
        "name": "W Xuan",
        "id": "7e0W-2AAAAAJ"
      },
      {
        "name": "J Xia",
        "id": "n1aKdTkAAAAJ"
      },
      {
        "name": "N YokoyaAdvances in Neural Information Processing Systems",
        "id": null
      }
    ],
    "citation_count": 13
  },
  {
    "arxiv_id": "2504.17819",
    "title": "A Deep Bayesian Convolutional Spiking Neural Network-based CAD system with Uncertainty Quantification for Medical Images Classification",
    "year": 2025,
    "published": "2025-04-23T07:42:05Z",
    "categories": [
      "eess.IV",
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "The Computer_Aided Diagnosis (CAD) systems facilitate accurate diagnosis of diseases. The development of CADs by leveraging third generation neural network, namely, Spiking Neural Network (SNN), is essential to utilize of the benefits of SNNs, such as their event_driven processing, parallelism, low power consumption, and the ability to process sparse temporal_spatial information. However, Deep SNN as a deep learning model faces challenges with unreliability. To deal with unreliability challenges",
    "arxiv_url": "https://arxiv.org/abs/2504.17819v1",
    "pdf_url": "https://arxiv.org/pdf/2504.17819v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2504.17819",
    "arxiv_authors": [
      "Mohaddeseh Chegini",
      "Ali Mahloojifar"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=A+Deep+Bayesian+Convolutional+Spiking+Neural+Network-based+CAD+system+with+Uncertainty+Quantification+for+Medical+Images+Classification+Mohaddeseh+Chegini+Ali+Mahloojifar",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "M Chegini",
        "id": null
      },
      {
        "name": "A Mahloojifar -",
        "id": null
      }
    ],
    "citation_count": null
  },
  {
    "arxiv_id": "2303.12513",
    "title": "Is BERT Blind? Exploring the Effect of Vision-and-Language Pretraining on Visual Language Understanding",
    "year": 2023,
    "published": "2023-03-21T17:30:40Z",
    "categories": [
      "cs.CV",
      "cs.CL",
      "cs.LG"
    ],
    "abstract": "Most humans use visual imagination to understand and reason about language, but models such as BERT reason about language using knowledge acquired during text-only pretraining. In this work, we investigate whether vision-and-language pretraining can improve performance on text-only tasks that involve implicit visual reasoning, focusing primarily on zero-shot probing methods. We propose a suite of visual language understanding (VLU) tasks for probing the visual reasoning abilities of text encoder",
    "arxiv_url": "https://arxiv.org/abs/2303.12513v2",
    "pdf_url": "https://arxiv.org/pdf/2303.12513v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2303.12513",
    "arxiv_authors": [
      "Morris Alper",
      "Michael Fiman",
      "Hadar Averbuch-Elor"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Is+BERT+Blind%3F+Exploring+the+Effect+of+Vision-and-Language+Pretraining+on+Visual+Language+Understanding+Morris+Alper+Michael+Fiman+Hadar+Averbuch-Elor",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "M Alper",
        "id": "M2RsdCUAAAAJ"
      },
      {
        "name": "M Fiman",
        "id": null
      },
      {
        "name": "H Averbuch-Elor",
        "id": "oBldxf0AAAAJ"
      }
    ],
    "citation_count": 18
  },
  {
    "arxiv_id": "2307.16418",
    "title": "DRAW: Defending Camera-shooted RAW against Image Manipulation",
    "year": 2023,
    "published": "2023-07-31T05:57:41Z",
    "categories": [
      "cs.CV",
      "cs.MM",
      "eess.IV"
    ],
    "abstract": "RAW files are the initial measurement of scene radiance widely used in most cameras, and the ubiquitously-used RGB images are converted from RAW data through Image Signal Processing (ISP) pipelines. Nowadays, digital images are risky of being nefariously manipulated. Inspired by the fact that innate immunity is the first line of body defense, we propose DRAW, a novel scheme of defending images against manipulation by protecting their sources, i.e., camera-shooted RAWs. Specifically, we design a ",
    "arxiv_url": "https://arxiv.org/abs/2307.16418v1",
    "pdf_url": "https://arxiv.org/pdf/2307.16418v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2307.16418",
    "arxiv_authors": [
      "Xiaoxiao Hu",
      "Qichao Ying",
      "Zhenxing Qian",
      "Sheng Li",
      "Xinpeng Zhang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=DRAW%3A+Defending+Camera-shooted+RAW+against+Image+Manipulation+Xiaoxiao+Hu+Qichao+Ying+Zhenxing+Qian+Sheng+Li+Xinpeng+Zhang",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "X Hu",
        "id": "9C7Fj2kAAAAJ"
      },
      {
        "name": "Q Ying",
        "id": "3sletjoAAAAJ"
      },
      {
        "name": "Z Qian",
        "id": "90AsMtQAAAAJ"
      },
      {
        "name": "S Li",
        "id": "akpK6tgAAAAJ"
      },
      {
        "name": "X Zhang",
        "id": "P76GtHwAAAAJ"
      }
    ],
    "citation_count": 12
  },
  {
    "arxiv_id": "2502.09620",
    "title": "Exploring the Potential of Encoder-free Architectures in 3D LMMs",
    "year": 2025,
    "published": "2025-02-13T18:59:45Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "abstract": "Encoder-free architectures have been preliminarily explored in the 2D visual domain, yet it remains an open question whether they can be effectively applied to 3D understanding scenarios. In this paper, we present the first comprehensive investigation into the potential of encoder-free architectures to alleviate the challenges of encoder-based 3D Large Multimodal Models (LMMs). These challenges include the failure to adapt to varying point cloud resolutions and the point features from the encode",
    "arxiv_url": "https://arxiv.org/abs/2502.09620v3",
    "pdf_url": "https://arxiv.org/pdf/2502.09620v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2502.09620",
    "arxiv_authors": [
      "Yiwen Tang",
      "Zoey Guo",
      "Zhuhao Wang",
      "Ray Zhang",
      "Qizhi Chen",
      "Junli Liu",
      "Delin Qu",
      "Zhigang Wang",
      "Dong Wang",
      "Xuelong Li",
      "Bin Zhao"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Exploring+the+Potential+of+Encoder-free+Architectures+in+3D+LMMs+Yiwen+Tang+Zoey+Guo+Zhuhao+Wang+Ray+Zhang+Qizhi+Chen",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "Y Tang",
        "id": null
      },
      {
        "name": "Z Guo",
        "id": "S9GLetwAAAAJ"
      },
      {
        "name": "Z Wang",
        "id": "cw3EaAYAAAAJ"
      },
      {
        "name": "R Zhang",
        "id": "YlL3xN4AAAAJ"
      },
      {
        "name": "Q Chen",
        "id": "8IFwRIQAAAAJ"
      },
      {
        "name": "J Liu",
        "id": "-4gYhMYAAAAJ"
      },
      {
        "name": "D Qu",
        "id": "zgiFoOwAAAAJ"
      },
      {
        "name": "Z Wang",
        "id": "cw3EaAYAAAAJ"
      },
      {
        "name": "D Wang",
        "id": null
      },
      {
        "name": "X Li",
        "id": null
      },
      {
        "name": "B Zhao",
        "id": "DQB0hqwAAAAJ"
      }
    ],
    "citation_count": 7
  },
  {
    "arxiv_id": "2312.03700",
    "title": "OneLLM: One Framework to Align All Modalities with Language",
    "year": 2023,
    "published": "2023-12-06T18:59:19Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "cs.MM"
    ],
    "abstract": "Multimodal large language models (MLLMs) have gained significant attention due to their strong multimodal understanding capability. However, existing works rely heavily on modality-specific encoders, which usually differ in architecture and are limited to common modalities. In this paper, we present OneLLM, an MLLM that aligns eight modalities to language using a unified framework. We achieve this through a unified multimodal encoder and a progressive multimodal alignment pipeline. In detail, we",
    "arxiv_url": "https://arxiv.org/abs/2312.03700v2",
    "pdf_url": "https://arxiv.org/pdf/2312.03700v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2312.03700",
    "arxiv_authors": [
      "Jiaming Han",
      "Kaixiong Gong",
      "Yiyuan Zhang",
      "Jiaqi Wang",
      "Kaipeng Zhang",
      "Dahua Lin",
      "Yu Qiao",
      "Peng Gao",
      "Xiangyu Yue"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=OneLLM%3A+One+Framework+to+Align+All+Modalities+with+Language+Jiaming+Han+Kaixiong+Gong+Yiyuan+Zhang+Jiaqi+Wang+Kaipeng+Zhang",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "J Han",
        "id": "vgcxKEcAAAAJ"
      },
      {
        "name": "K Gong",
        "id": "kBVshUUAAAAJ"
      },
      {
        "name": "Y Zhang",
        "id": "KuYlJCIAAAAJ"
      },
      {
        "name": "J Wang",
        "id": "GDvt570AAAAJ"
      },
      {
        "name": "K Zhang",
        "id": "4OqZBmYAAAAJ"
      },
      {
        "name": "D Lin",
        "id": "GMzzRRUAAAAJ"
      },
      {
        "name": "Y Qiao",
        "id": "gFtI-8QAAAAJ"
      },
      {
        "name": "P Gao",
        "id": null
      },
      {
        "name": "X Yue",
        "id": "-xQ-C1sAAAAJ"
      }
    ],
    "citation_count": 208
  },
  {
    "arxiv_id": "2308.13093",
    "title": "EgoBlur: Responsible Innovation in Aria",
    "year": 2023,
    "published": "2023-08-24T21:36:11Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Project Aria pushes the frontiers of Egocentric AI with large-scale real-world data collection using purposely designed glasses with privacy first approach. To protect the privacy of bystanders being recorded by the glasses, our research protocols are designed to ensure recorded video is processed by an AI anonymization model that removes bystander faces and vehicle license plates. Detected face and license plate regions are processed with a Gaussian blur such that these personal identification ",
    "arxiv_url": "https://arxiv.org/abs/2308.13093v2",
    "pdf_url": "https://arxiv.org/pdf/2308.13093v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2308.13093",
    "arxiv_authors": [
      "Nikhil Raina",
      "Guruprasad Somasundaram",
      "Kang Zheng",
      "Sagar Miglani",
      "Steve Saarinen",
      "Jeff Meissner",
      "Mark Schwesinger",
      "Luis Pesqueira",
      "Ishita Prasad",
      "Edward Miller",
      "Prince Gupta",
      "Mingfei Yan",
      "Richard Newcombe",
      "Carl Ren",
      "Omkar M Parkhi"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=EgoBlur%3A+Responsible+Innovation+in+Aria+Nikhil+Raina+Guruprasad+Somasundaram+Kang+Zheng+Sagar+Miglani+Steve+Saarinen",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "N Raina",
        "id": "LkAAukkAAAAJ"
      },
      {
        "name": "G Somasundaram",
        "id": null
      },
      {
        "name": "K Zheng",
        "id": "bMDB5WEAAAAJ"
      },
      {
        "name": "S Miglani",
        "id": "MyAOpVsAAAAJ"
      },
      {
        "name": "S Saarinen",
        "id": null
      },
      {
        "name": "J Meissner",
        "id": null
      },
      {
        "name": "M Schwesinger",
        "id": null
      }
    ],
    "citation_count": 16
  },
  {
    "arxiv_id": "2306.11287",
    "title": "Spatiotemporal Pyramidal CNN with Depth-Wise Separable Convolution for Eye Blinking Detection in the Wild",
    "year": 2023,
    "published": "2023-06-20T04:59:09Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Eye blinking detection in the wild plays an essential role in deception detection, driving fatigue detection, etc. Despite the fact that numerous attempts have already been made, the majority of them have encountered difficulties, such as the derived eye images having different resolutions as the distance between the face and the camera changes; or the requirement of a lightweight detection model to obtain a short inference time in order to perform in real-time. In this research, two problems ar",
    "arxiv_url": "https://arxiv.org/abs/2306.11287v1",
    "pdf_url": "https://arxiv.org/pdf/2306.11287v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2306.11287",
    "arxiv_authors": [
      "Lan Anh Thi Nguy",
      "Bach Nguyen Gia",
      "Thanh Tu Thi Nguyen",
      "Kamioka Eiji",
      "Tan Xuan Phan"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Spatiotemporal+Pyramidal+CNN+with+Depth-Wise+Separable+Convolution+for+Eye+Blinking+Detection+in+the+Wild+Lan+Anh+Thi+Nguy+Bach+Nguyen+Gia+Thanh+Tu+Thi+Nguyen+Kamioka+Eiji+Tan+Xuan+Phan",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "LAT Nguy",
        "id": null
      },
      {
        "name": "BN Gia",
        "id": null
      },
      {
        "name": "TTT Nguyen",
        "id": null
      },
      {
        "name": "K Eiji",
        "id": "U_ZkZ8wAAAAJ"
      },
      {
        "name": "TX Phan",
        "id": "p0maui4AAAAJ"
      }
    ],
    "citation_count": null
  },
  {
    "arxiv_id": "2403.06322",
    "title": "Leveraging Computer Vision in the Intensive Care Unit (ICU) for Examining Visitation and Mobility",
    "year": 2024,
    "published": "2024-03-10T21:43:47Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Despite the importance of closely monitoring patients in the Intensive Care Unit (ICU), many aspects are still assessed in a limited manner due to the time constraints imposed on healthcare providers. For example, although excessive visitations during rest hours can potentially exacerbate the risk of circadian rhythm disruption and delirium, it is not captured in the ICU. Likewise, while mobility can be an important indicator of recovery or deterioration in ICU patients, it is only captured spor",
    "arxiv_url": "https://arxiv.org/abs/2403.06322v2",
    "pdf_url": "https://arxiv.org/pdf/2403.06322v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2403.06322",
    "arxiv_authors": [
      "Scott Siegel",
      "Jiaqing Zhang",
      "Sabyasachi Bandyopadhyay",
      "Subhash Nerella",
      "Brandon Silva",
      "Tezcan Baslanti",
      "Azra Bihorac",
      "Parisa Rashidi"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Leveraging+Computer+Vision+in+the+Intensive+Care+Unit+%28ICU%29+for+Examining+Visitation+and+Mobility+Scott+Siegel+Jiaqing+Zhang+Sabyasachi+Bandyopadhyay+Subhash+Nerella+Brandon+Silva",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "S Siegel",
        "id": "KOkTGkoAAAAJ"
      },
      {
        "name": "J Zhang",
        "id": "Tbiwu-4AAAAJ"
      },
      {
        "name": "S Bandyopadhyay",
        "id": "6b9AuSIAAAAJ"
      },
      {
        "name": "S Nerella",
        "id": "SH7bWKAAAAAJ"
      },
      {
        "name": "B Silva",
        "id": null
      },
      {
        "name": "T Baslanti",
        "id": "Nr0_ydcAAAAJ"
      },
      {
        "name": "A Bihorac",
        "id": "65qIDuYAAAAJ"
      },
      {
        "name": "P Rashidi",
        "id": "Rtej0FIAAAAJ"
      }
    ],
    "citation_count": 4
  },
  {
    "arxiv_id": "2503.17497",
    "title": "You Only Look Once at Anytime (AnytimeYOLO): Analysis and Optimization of Early-Exits for Object-Detection",
    "year": 2025,
    "published": "2025-03-21T19:16:38Z",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "We introduce AnytimeYOLO, a family of variants of the YOLO architecture that enables anytime object detection. Our AnytimeYOLO networks allow for interruptible inference, i.e., they provide a prediction at any point in time, a property desirable for safety-critical real-time applications.   We present structured explorations to modify the YOLO architecture, enabling early termination to obtain intermediate results. We focus on providing fine-grained control through high granularity of available ",
    "arxiv_url": "https://arxiv.org/abs/2503.17497v1",
    "pdf_url": "https://arxiv.org/pdf/2503.17497v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2503.17497",
    "arxiv_authors": [
      "Daniel Kuhse",
      "Harun Teper",
      "Sebastian Buschj√§ger",
      "Chien-Yao Wang",
      "Jian-Jia Chen"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=You+Only+Look+Once+at+Anytime+%28AnytimeYOLO%29%3A+Analysis+and+Optimization+of+Early-Exits+for+Object-Detection+Daniel+Kuhse+Harun+Teper+Sebastian+Buschj%C3%A4ger+Chien-Yao+Wang+Jian-Jia+Chen",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "D Kuhse",
        "id": null
      },
      {
        "name": "H Teper",
        "id": "rgz7nzoAAAAJ"
      },
      {
        "name": "S Buschj√§ger",
        "id": "EsNs6kIAAAAJ"
      },
      {
        "name": "CY Wang",
        "id": "DkQh4M4AAAAJ"
      },
      {
        "name": "JJ Chen",
        "id": null
      }
    ],
    "citation_count": 3
  },
  {
    "arxiv_id": "2312.17243",
    "title": "Unsupervised Universal Image Segmentation",
    "year": 2023,
    "published": "2023-12-28T18:59:04Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Several unsupervised image segmentation approaches have been proposed which eliminate the need for dense manually-annotated segmentation masks; current models separately handle either semantic segmentation (e.g., STEGO) or class-agnostic instance segmentation (e.g., CutLER), but not both (i.e., panoptic segmentation). We propose an Unsupervised Universal Segmentation model (U2Seg) adept at performing various image segmentation tasks -- instance, semantic and panoptic -- using a novel unified fra",
    "arxiv_url": "https://arxiv.org/abs/2312.17243v1",
    "pdf_url": "https://arxiv.org/pdf/2312.17243v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2312.17243",
    "arxiv_authors": [
      "Dantong Niu",
      "Xudong Wang",
      "Xinyang Han",
      "Long Lian",
      "Roei Herzig",
      "Trevor Darrell"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Unsupervised+Universal+Image+Segmentation+Dantong+Niu+Xudong+Wang+Xinyang+Han+Long+Lian+Roei+Herzig",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "D Niu",
        "id": "AzlUrvUAAAAJ"
      },
      {
        "name": "X Wang",
        "id": "Azf07WcAAAAJ"
      },
      {
        "name": "X Han",
        "id": "HjmxmJoAAAAJ"
      },
      {
        "name": "L Lian",
        "id": "eOLxyqUAAAAJ"
      },
      {
        "name": "R Herzig",
        "id": "6Q-289IAAAAJ"
      },
      {
        "name": "T Darrell",
        "id": "bh-uRFMAAAAJ"
      }
    ],
    "citation_count": 56
  },
  {
    "arxiv_id": "2409.08494",
    "title": "WheelPoser: Sparse-IMU Based Body Pose Estimation for Wheelchair Users",
    "year": 2024,
    "published": "2024-09-13T02:41:49Z",
    "categories": [
      "cs.GR",
      "cs.CV",
      "cs.HC"
    ],
    "abstract": "Despite researchers having extensively studied various ways to track body pose on-the-go, most prior work does not take into account wheelchair users, leading to poor tracking performance. Wheelchair users could greatly benefit from this pose information to prevent injuries, monitor their health, identify environmental accessibility barriers, and interact with gaming and VR experiences. In this work, we present WheelPoser, a real-time pose estimation system specifically designed for wheelchair u",
    "arxiv_url": "https://arxiv.org/abs/2409.08494v1",
    "pdf_url": "https://arxiv.org/pdf/2409.08494v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2409.08494",
    "arxiv_authors": [
      "Yunzhi Li",
      "Vimal Mollyn",
      "Kuang Yuan",
      "Patrick Carrington"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=WheelPoser%3A+Sparse-IMU+Based+Body+Pose+Estimation+for+Wheelchair+Users+Yunzhi+Li+Vimal+Mollyn+Kuang+Yuan+Patrick+Carrington",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "Y Li",
        "id": "9cpdXDsAAAAJ"
      },
      {
        "name": "V Mollyn",
        "id": "dZnHVv4AAAAJ"
      },
      {
        "name": "K Yuan",
        "id": "5e_9JaUAAAAJ"
      },
      {
        "name": "P Carrington -",
        "id": null
      }
    ],
    "citation_count": 13
  },
  {
    "arxiv_id": "2307.08693",
    "title": "SEMI-DiffusionInst: A Diffusion Model Based Approach for Semiconductor Defect Classification and Segmentation",
    "year": 2023,
    "published": "2023-07-17T17:53:36Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "With continuous progression of Moore's Law, integrated circuit (IC) device complexity is also increasing. Scanning Electron Microscope (SEM) image based extensive defect inspection and accurate metrology extraction are two main challenges in advanced node (2 nm and beyond) technology. Deep learning (DL) algorithm based computer vision approaches gained popularity in semiconductor defect inspection over last few years. In this research work, a new semiconductor defect inspection framework \"SEMI-D",
    "arxiv_url": "https://arxiv.org/abs/2307.08693v2",
    "pdf_url": "https://arxiv.org/pdf/2307.08693v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2307.08693",
    "arxiv_authors": [
      "Vic De Ridder",
      "Bappaditya Dey",
      "Sandip Halder",
      "Bartel Van Waeyenberge"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=SEMI-DiffusionInst%3A+A+Diffusion+Model+Based+Approach+for+Semiconductor+Defect+Classification+and+Segmentation+Vic+De+Ridder+Bappaditya+Dey+Sandip+Halder+Bartel+Van+Waeyenberge",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "V De Ridder",
        "id": "vbsPJJ8AAAAJ"
      },
      {
        "name": "B Dey",
        "id": "GEZtZOMAAAAJ"
      },
      {
        "name": "S Halder",
        "id": null
      },
      {
        "name": "B Van Waeyenberge2023 International",
        "id": null
      }
    ],
    "citation_count": 15
  },
  {
    "arxiv_id": "2407.03817",
    "title": "Markerless Multi-view 3D Human Pose Estimation: a survey",
    "year": 2024,
    "published": "2024-07-04T10:44:35Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "3D human pose estimation involves reconstructing the human skeleton by detecting the body joints. Accurate and efficient solutions are required for several real-world applications including animation, human-robot interaction, surveillance, and sports. However, challenges such as occlusions, 2D pose mismatches, random camera perspectives, and limited 3D labelled data have been hampering the models' performance and limiting their deployment in real-world scenarios. The higher availability of camer",
    "arxiv_url": "https://arxiv.org/abs/2407.03817v2",
    "pdf_url": "https://arxiv.org/pdf/2407.03817v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2407.03817",
    "arxiv_authors": [
      "Ana Filipa Rodrigues Nogueira",
      "H√©lder P. Oliveira",
      "Lu√≠s F. Teixeira"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Markerless+Multi-view+3D+Human+Pose+Estimation%3A+a+survey+Ana+Filipa+Rodrigues+Nogueira+H%C3%A9lder+P.+Oliveira+Lu%C3%ADs+F.+Teixeira",
    "gs_search_success": false,
    "gs_authors": null,
    "citation_count": null,
    "error_type": "multiple_matches"
  },
  {
    "arxiv_id": "2502.09619",
    "title": "Can this Model Also Recognize Dogs? Zero-Shot Model Search from Weights",
    "year": 2025,
    "published": "2025-02-13T18:59:44Z",
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "abstract": "With the increasing numbers of publicly available models, there are probably pretrained, online models for most tasks users require. However, current model search methods are rudimentary, essentially a text-based search in the documentation, thus users cannot find the relevant models. This paper presents ProbeLog, a method for retrieving classification models that can recognize a target concept, such as \"Dog\", without access to model metadata or training data. Differently from previous probing m",
    "arxiv_url": "https://arxiv.org/abs/2502.09619v1",
    "pdf_url": "https://arxiv.org/pdf/2502.09619v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2502.09619",
    "arxiv_authors": [
      "Jonathan Kahana",
      "Or Nathan",
      "Eliahu Horwitz",
      "Yedid Hoshen"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Can+this+Model+Also+Recognize+Dogs%3F+Zero-Shot+Model+Search+from+Weights+Jonathan+Kahana+Or+Nathan+Eliahu+Horwitz+Yedid+Hoshen",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "J Kahana",
        "id": "xQlB6jsAAAAJ"
      },
      {
        "name": "O Nathan",
        "id": null
      },
      {
        "name": "E Horwitz",
        "id": "NyLx5nIAAAAJ"
      },
      {
        "name": "Y Hoshen -",
        "id": null
      }
    ],
    "citation_count": 3
  },
  {
    "arxiv_id": "2302.05283",
    "title": "Deep Learning from Parametrically Generated Virtual Buildings for Real-World Object Recognition",
    "year": 2023,
    "published": "2023-01-03T09:52:13Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "abstract": "We study the use of parametric building information modeling (BIM) to automatically generate training data for artificial neural networks (ANNs) to recognize building objects in photos. Teaching artificial intelligence (AI) machines to detect building objects in images is the foundation toward AI-assisted semantic 3D reconstruction of existing buildings. However, there exists the challenge of acquiring training data which is typically human-annotated, that is, unless a computer machine can gener",
    "arxiv_url": "https://arxiv.org/abs/2302.05283v1",
    "pdf_url": "https://arxiv.org/pdf/2302.05283v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2302.05283",
    "arxiv_authors": [
      "Mohammad Alawadhi",
      "Wei Yan"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Deep+Learning+from+Parametrically+Generated+Virtual+Buildings+for+Real-World+Object+Recognition+Mohammad+Alawadhi+Wei+Yan",
    "gs_search_success": false,
    "gs_authors": null,
    "citation_count": null,
    "error_type": "fetch_failed"
  },
  {
    "arxiv_id": "2310.19415",
    "title": "Text-to-3D with Classifier Score Distillation",
    "year": 2023,
    "published": "2023-10-30T10:25:40Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR"
    ],
    "abstract": "Text-to-3D generation has made remarkable progress recently, particularly with methods based on Score Distillation Sampling (SDS) that leverages pre-trained 2D diffusion models. While the usage of classifier-free guidance is well acknowledged to be crucial for successful optimization, it is considered an auxiliary trick rather than the most essential component. In this paper, we re-evaluate the role of classifier-free guidance in score distillation and discover a surprising finding: the guidance",
    "arxiv_url": "https://arxiv.org/abs/2310.19415v2",
    "pdf_url": "https://arxiv.org/pdf/2310.19415v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2310.19415",
    "arxiv_authors": [
      "Xin Yu",
      "Yuan-Chen Guo",
      "Yangguang Li",
      "Ding Liang",
      "Song-Hai Zhang",
      "Xiaojuan Qi"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Text-to-3D+with+Classifier+Score+Distillation+Xin+Yu+Yuan-Chen+Guo+Yangguang+Li+Ding+Liang+Song-Hai+Zhang",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "X Yu",
        "id": "JX8kSoEAAAAJ"
      },
      {
        "name": "YC Guo",
        "id": "b7ZJV9oAAAAJ"
      },
      {
        "name": "Y Li",
        "id": "a7AMvgkAAAAJ"
      },
      {
        "name": "D Liang",
        "id": "Dqjnn0gAAAAJ"
      },
      {
        "name": "SH Zhang",
        "id": "AWtV-EQAAAAJ"
      },
      {
        "name": "X Qi -",
        "id": null
      }
    ],
    "citation_count": 120
  },
  {
    "arxiv_id": "2311.15965",
    "title": "FALCON: Fairness Learning via Contrastive Attention Approach to Continual Semantic Scene Understanding",
    "year": 2023,
    "published": "2023-11-27T16:07:39Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Continual Learning in semantic scene segmentation aims to continually learn new unseen classes in dynamic environments while maintaining previously learned knowledge. Prior studies focused on modeling the catastrophic forgetting and background shift challenges in continual learning. However, fairness, another major challenge that causes unfair predictions leading to low performance among major and minor classes, still needs to be well addressed. In addition, prior methods have yet to model the u",
    "arxiv_url": "https://arxiv.org/abs/2311.15965v3",
    "pdf_url": "https://arxiv.org/pdf/2311.15965v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2311.15965",
    "arxiv_authors": [
      "Thanh-Dat Truong",
      "Utsav Prabhu",
      "Bhiksha Raj",
      "Jackson Cothren",
      "Khoa Luu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=FALCON%3A+Fairness+Learning+via+Contrastive+Attention+Approach+to+Continual+Semantic+Scene+Understanding+Thanh-Dat+Truong+Utsav+Prabhu+Bhiksha+Raj+Jackson+Cothren+Khoa+Luu",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "TD Truong",
        "id": "qrmxykkAAAAJ"
      },
      {
        "name": "U Prabhu",
        "id": "o9GWp-YAAAAJ"
      },
      {
        "name": "B Raj",
        "id": "IWcGY98AAAAJ"
      },
      {
        "name": "J Cothren",
        "id": "_WB9fo4AAAAJ"
      },
      {
        "name": "K Luu",
        "id": "JPAl8-gAAAAJ"
      }
    ],
    "citation_count": 5
  },
  {
    "arxiv_id": "2303.08289",
    "title": "Improving Adversarial Robustness with Hypersphere Embedding and Angular-based Regularizations",
    "year": 2023,
    "published": "2023-03-15T00:35:03Z",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "abstract": "Adversarial training (AT) methods have been found to be effective against adversarial attacks on deep neural networks. Many variants of AT have been proposed to improve its performance. Pang et al. [1] have recently shown that incorporating hypersphere embedding (HE) into the existing AT procedures enhances robustness. We observe that the existing AT procedures are not designed for the HE framework, and thus fail to adequately learn the angular discriminative information available in the HE fram",
    "arxiv_url": "https://arxiv.org/abs/2303.08289v1",
    "pdf_url": "https://arxiv.org/pdf/2303.08289v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2303.08289",
    "arxiv_authors": [
      "Olukorede Fakorede",
      "Ashutosh Nirala",
      "Modeste Atsague",
      "Jin Tian"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Improving+Adversarial+Robustness+with+Hypersphere+Embedding+and+Angular-based+Regularizations+Olukorede+Fakorede+Ashutosh+Nirala+Modeste+Atsague+Jin+Tian",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "O Fakorede",
        "id": "czM9zQUAAAAJ"
      },
      {
        "name": "A Nirala",
        "id": "4MJQOyYAAAAJ"
      },
      {
        "name": "M Atsague",
        "id": "Ut3nFL4AAAAJ"
      },
      {
        "name": "J TianICASSP",
        "id": null
      }
    ],
    "citation_count": 8
  },
  {
    "arxiv_id": "2310.09291",
    "title": "Vision-by-Language for Training-Free Compositional Image Retrieval",
    "year": 2023,
    "published": "2023-10-13T17:59:38Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Given an image and a target modification (e.g an image of the Eiffel tower and the text \"without people and at night-time\"), Compositional Image Retrieval (CIR) aims to retrieve the relevant target image in a database. While supervised approaches rely on annotating triplets that is costly (i.e. query image, textual modification, and target image), recent research sidesteps this need by using large-scale vision-language models (VLMs), performing Zero-Shot CIR (ZS-CIR). However, state-of-the-art a",
    "arxiv_url": "https://arxiv.org/abs/2310.09291v2",
    "pdf_url": "https://arxiv.org/pdf/2310.09291v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2310.09291",
    "arxiv_authors": [
      "Shyamgopal Karthik",
      "Karsten Roth",
      "Massimiliano Mancini",
      "Zeynep Akata"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Vision-by-Language+for+Training-Free+Compositional+Image+Retrieval+Shyamgopal+Karthik+Karsten+Roth+Massimiliano+Mancini+Zeynep+Akata",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "S Karthik",
        "id": "OiVCfscAAAAJ"
      },
      {
        "name": "K Roth",
        "id": "93ZjIs0AAAAJ"
      },
      {
        "name": "M Mancini",
        "id": "bqTPA8kAAAAJ"
      },
      {
        "name": "Z Akata -",
        "id": null
      }
    ],
    "citation_count": 106
  },
  {
    "arxiv_id": "2409.17095",
    "title": "General Detection-based Text Line Recognition",
    "year": 2024,
    "published": "2024-09-25T17:05:55Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "We introduce a general detection-based approach to text line recognition, be it printed (OCR) or handwritten (HTR), with Latin, Chinese, or ciphered characters. Detection-based approaches have until now been largely discarded for HTR because reading characters separately is often challenging, and character-level annotation is difficult and expensive. We overcome these challenges thanks to three main insights: (i) synthetic pre-training with sufficiently diverse data enables learning reasonable c",
    "arxiv_url": "https://arxiv.org/abs/2409.17095v2",
    "pdf_url": "https://arxiv.org/pdf/2409.17095v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2409.17095",
    "arxiv_authors": [
      "Raphael Baena",
      "Syrine Kalleli",
      "Mathieu Aubry"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=General+Detection-based+Text+Line+Recognition+Raphael+Baena+Syrine+Kalleli+Mathieu+Aubry",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "R Baena",
        "id": "8MU98WQAAAAJ"
      },
      {
        "name": "S Kalleli",
        "id": "V307cnsAAAAJ"
      },
      {
        "name": "M Aubry - Advances in Neural",
        "id": null
      }
    ],
    "citation_count": 4
  },
  {
    "arxiv_id": "2308.07537",
    "title": "AttMOT: Improving Multiple-Object Tracking by Introducing Auxiliary Pedestrian Attributes",
    "year": 2023,
    "published": "2023-08-15T02:39:39Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Multi-object tracking (MOT) is a fundamental problem in computer vision with numerous applications, such as intelligent surveillance and automated driving. Despite the significant progress made in MOT, pedestrian attributes, such as gender, hairstyle, body shape, and clothing features, which contain rich and high-level information, have been less explored. To address this gap, we propose a simple, effective, and generic method to predict pedestrian attributes to support general Re-ID embedding. ",
    "arxiv_url": "https://arxiv.org/abs/2308.07537v1",
    "pdf_url": "https://arxiv.org/pdf/2308.07537v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2308.07537",
    "arxiv_authors": [
      "Yunhao Li",
      "Zhen Xiao",
      "Lin Yang",
      "Dan Meng",
      "Xin Zhou",
      "Heng Fan",
      "Libo Zhang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=AttMOT%3A+Improving+Multiple-Object+Tracking+by+Introducing+Auxiliary+Pedestrian+Attributes+Yunhao+Li+Zhen+Xiao+Lin+Yang+Dan+Meng+Xin+Zhou",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "Y Li",
        "id": "Jlcf6nEAAAAJ"
      },
      {
        "name": "Z Xiao",
        "id": "wpzqmsYAAAAJ"
      },
      {
        "name": "L Yang",
        "id": null
      },
      {
        "name": "D Meng",
        "id": "Mtd7u-QAAAAJ"
      },
      {
        "name": "X Zhou",
        "id": null
      },
      {
        "name": "H Fan",
        "id": "MVQYJiMAAAAJ"
      },
      {
        "name": "L ZhangIEEE transactions on neural networks and learning systems",
        "id": null
      }
    ],
    "citation_count": 9
  },
  {
    "arxiv_id": "2308.02194",
    "title": "Paired Competing Neurons Improving STDP Supervised Local Learning In Spiking Neural Networks",
    "year": 2023,
    "published": "2023-08-04T08:20:54Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Direct training of Spiking Neural Networks (SNNs) on neuromorphic hardware has the potential to significantly reduce the energy consumption of artificial neural network training. SNNs trained with Spike Timing-Dependent Plasticity (STDP) benefit from gradient-free and unsupervised local learning, which can be easily implemented on ultra-low-power neuromorphic hardware. However, classification tasks cannot be performed solely with unsupervised STDP. In this paper, we propose Stabilized Supervised",
    "arxiv_url": "https://arxiv.org/abs/2308.02194v2",
    "pdf_url": "https://arxiv.org/pdf/2308.02194v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2308.02194",
    "arxiv_authors": [
      "Gaspard Goupy",
      "Pierre Tirilly",
      "Ioan Marius Bilasco"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Paired+Competing+Neurons+Improving+STDP+Supervised+Local+Learning+In+Spiking+Neural+Networks+Gaspard+Goupy+Pierre+Tirilly+Ioan+Marius+Bilasco",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "G Goupy",
        "id": "bDWZsrAAAAAJ"
      },
      {
        "name": "P Tirilly",
        "id": "CKGKuOAAAAAJ"
      },
      {
        "name": "IM Bilasco - Frontiers in Neuroscience",
        "id": null
      }
    ],
    "citation_count": 11
  },
  {
    "arxiv_id": "2410.02705",
    "title": "ControlAR: Controllable Image Generation with Autoregressive Models",
    "year": 2024,
    "published": "2024-10-03T17:28:07Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Autoregressive (AR) models have reformulated image generation as next-token prediction, demonstrating remarkable potential and emerging as strong competitors to diffusion models. However, control-to-image generation, akin to ControlNet, remains largely unexplored within AR models. Although a natural approach, inspired by advancements in Large Language Models, is to tokenize control images into tokens and prefill them into the autoregressive model before decoding image tokens, it still falls shor",
    "arxiv_url": "https://arxiv.org/abs/2410.02705v3",
    "pdf_url": "https://arxiv.org/pdf/2410.02705v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2410.02705",
    "arxiv_authors": [
      "Zongming Li",
      "Tianheng Cheng",
      "Shoufa Chen",
      "Peize Sun",
      "Haocheng Shen",
      "Longjin Ran",
      "Xiaoxin Chen",
      "Wenyu Liu",
      "Xinggang Wang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=ControlAR%3A+Controllable+Image+Generation+with+Autoregressive+Models+Zongming+Li+Tianheng+Cheng+Shoufa+Chen+Peize+Sun+Haocheng+Shen",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "Z Li",
        "id": "s697AYIAAAAJ"
      },
      {
        "name": "T Cheng",
        "id": "PH8rJHYAAAAJ"
      },
      {
        "name": "S Chen",
        "id": "ogoCvHEAAAAJ"
      },
      {
        "name": "P Sun",
        "id": "Grkp5AQAAAAJ"
      },
      {
        "name": "H Shen",
        "id": "AfC_R58AAAAJ"
      },
      {
        "name": "L Ran",
        "id": null
      },
      {
        "name": "X Chen",
        "id": null
      },
      {
        "name": "W Liu",
        "id": "SmHr4W4AAAAJ"
      },
      {
        "name": "X Wang",
        "id": "qNCTLV0AAAAJ"
      }
    ],
    "citation_count": 41
  },
  {
    "arxiv_id": "2312.12102",
    "title": "I-CEE: Tailoring Explanations of Image Classification Models to User Expertise",
    "year": 2023,
    "published": "2023-12-19T12:26:57Z",
    "categories": [
      "cs.AI",
      "cs.CV",
      "cs.HC",
      "cs.LG"
    ],
    "abstract": "Effectively explaining decisions of black-box machine learning models is critical to responsible deployment of AI systems that rely on them. Recognizing their importance, the field of explainable AI (XAI) provides several techniques to generate these explanations. Yet, there is relatively little emphasis on the user (the explainee) in this growing body of work and most XAI techniques generate \"one-size-fits-all\" explanations. To bridge this gap and achieve a step closer towards human-centered XA",
    "arxiv_url": "https://arxiv.org/abs/2312.12102v3",
    "pdf_url": "https://arxiv.org/pdf/2312.12102v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2312.12102",
    "arxiv_authors": [
      "Yao Rong",
      "Peizhu Qian",
      "Vaibhav Unhelkar",
      "Enkelejda Kasneci"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=I-CEE%3A+Tailoring+Explanations+of+Image+Classification+Models+to+User+Expertise+Yao+Rong+Peizhu+Qian+Vaibhav+Unhelkar+Enkelejda+Kasneci",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "Y Rong",
        "id": "2CGNfAEAAAAJ"
      },
      {
        "name": "P Qian",
        "id": "wUi-SVsAAAAJ"
      },
      {
        "name": "V Unhelkar",
        "id": "A6j9ikkAAAAJ"
      },
      {
        "name": "E Kasneci -",
        "id": null
      }
    ],
    "citation_count": 4
  },
  {
    "arxiv_id": "2503.17731",
    "title": "Co-op: Correspondence-based Novel Object Pose Estimation",
    "year": 2025,
    "published": "2025-03-22T11:24:19Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "We propose Co-op, a novel method for accurately and robustly estimating the 6DoF pose of objects unseen during training from a single RGB image. Our method requires only the CAD model of the target object and can precisely estimate its pose without any additional fine-tuning. While existing model-based methods suffer from inefficiency due to using a large number of templates, our method enables fast and accurate estimation with a small number of templates. This improvement is achieved by finding",
    "arxiv_url": "https://arxiv.org/abs/2503.17731v1",
    "pdf_url": "https://arxiv.org/pdf/2503.17731v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2503.17731",
    "arxiv_authors": [
      "Sungphill Moon",
      "Hyeontae Son",
      "Dongcheol Hur",
      "Sangwook Kim"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Co-op%3A+Correspondence-based+Novel+Object+Pose+Estimation+Sungphill+Moon+Hyeontae+Son+Dongcheol+Hur+Sangwook+Kim",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "S Moon",
        "id": "ktPHs1oAAAAJ"
      },
      {
        "name": "H Son",
        "id": "krYfI4AAAAAJ"
      },
      {
        "name": "D Hur",
        "id": null
      },
      {
        "name": "S Kim -",
        "id": null
      }
    ],
    "citation_count": 7
  },
  {
    "arxiv_id": "2309.00494",
    "title": "Multi-stage Deep Learning Artifact Reduction for Pallel-beam Computed Tomography",
    "year": 2023,
    "published": "2023-09-01T14:40:25Z",
    "categories": [
      "eess.IV",
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "Computed Tomography (CT) using synchrotron radiation is a powerful technique that, compared to lab-CT techniques, boosts high spatial and temporal resolution while also providing access to a range of contrast-formation mechanisms. The acquired projection data is typically processed by a computational pipeline composed of multiple stages. Artifacts introduced during data acquisition can propagate through the pipeline, and degrade image quality in the reconstructed images. Recently, deep learning ",
    "arxiv_url": "https://arxiv.org/abs/2309.00494v2",
    "pdf_url": "https://arxiv.org/pdf/2309.00494v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2309.00494",
    "arxiv_authors": [
      "Jiayang Shi",
      "Daniel M. Pelt",
      "K. Joost Batenburg"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Multi-stage+Deep+Learning+Artifact+Reduction+for+Pallel-beam+Computed+Tomography+Jiayang+Shi+Daniel+M.+Pelt+K.+Joost+Batenburg",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "J Shi",
        "id": "9GJ3jyMAAAAJ"
      },
      {
        "name": "DM Pelt",
        "id": "54V8aTMAAAAJ"
      },
      {
        "name": "KJ Batenburg -",
        "id": null
      }
    ],
    "citation_count": 3
  },
  {
    "arxiv_id": "2403.03535",
    "title": "Task Attribute Distance for Few-Shot Learning: Theoretical Analysis and Applications",
    "year": 2024,
    "published": "2024-03-06T08:29:45Z",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "Few-shot learning (FSL) aims to learn novel tasks with very few labeled samples by leveraging experience from \\emph{related} training tasks. In this paper, we try to understand FSL by delving into two key questions: (1) How to quantify the relationship between \\emph{training} and \\emph{novel} tasks? (2) How does the relationship affect the \\emph{adaptation difficulty} on novel tasks for different models? To answer the two questions, we introduce Task Attribute Distance (TAD) built upon attribute",
    "arxiv_url": "https://arxiv.org/abs/2403.03535v1",
    "pdf_url": "https://arxiv.org/pdf/2403.03535v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2403.03535",
    "arxiv_authors": [
      "Minyang Hu",
      "Hong Chang",
      "Zong Guo",
      "Bingpeng Ma",
      "Shiguan Shan",
      "Xilin Chen"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Task+Attribute+Distance+for+Few-Shot+Learning%3A+Theoretical+Analysis+and+Applications+Minyang+Hu+Hong+Chang+Zong+Guo+Bingpeng+Ma+Shiguan+Shan",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "M Hu",
        "id": "6Saa1ugAAAAJ"
      },
      {
        "name": "H Chang",
        "id": null
      },
      {
        "name": "Z Guo",
        "id": null
      },
      {
        "name": "B Ma",
        "id": null
      },
      {
        "name": "S Shan",
        "id": null
      },
      {
        "name": "X Chen",
        "id": null
      }
    ],
    "citation_count": 1
  },
  {
    "arxiv_id": "2310.18815",
    "title": "Rethinking Semi-Supervised Federated Learning: How to co-train fully-labeled and fully-unlabeled client imaging data",
    "year": 2023,
    "published": "2023-10-28T20:41:41Z",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "abstract": "The most challenging, yet practical, setting of semi-supervised federated learning (SSFL) is where a few clients have fully labeled data whereas the other clients have fully unlabeled data. This is particularly common in healthcare settings where collaborating partners (typically hospitals) may have images but not annotations. The bottleneck in this setting is the joint training of labeled and unlabeled clients as the objective function for each client varies based on the availability of labels.",
    "arxiv_url": "https://arxiv.org/abs/2310.18815v1",
    "pdf_url": "https://arxiv.org/pdf/2310.18815v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2310.18815",
    "arxiv_authors": [
      "Pramit Saha",
      "Divyanshu Mishra",
      "J. Alison Noble"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Rethinking+Semi-Supervised+Federated+Learning%3A+How+to+co-train+fully-labeled+and+fully-unlabeled+client+imaging+data+Pramit+Saha+Divyanshu+Mishra+J.+Alison+Noble",
    "gs_search_success": false,
    "gs_authors": null,
    "citation_count": null,
    "error_type": "fetch_failed"
  },
  {
    "arxiv_id": "2505.22971",
    "title": "iHDR: Iterative HDR Imaging with Arbitrary Number of Exposures",
    "year": 2025,
    "published": "2025-05-29T01:20:31Z",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "abstract": "High dynamic range (HDR) imaging aims to obtain a high-quality HDR image by fusing information from multiple low dynamic range (LDR) images. Numerous learning-based HDR imaging methods have been proposed to achieve this for static and dynamic scenes. However, their architectures are mostly tailored for a fixed number (e.g., three) of inputs and, therefore, cannot apply directly to situations beyond the pre-defined limited scope. To address this issue, we propose a novel framework, iHDR, for iter",
    "arxiv_url": "https://arxiv.org/abs/2505.22971v1",
    "pdf_url": "https://arxiv.org/pdf/2505.22971v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2505.22971",
    "arxiv_authors": [
      "Yu Yuan",
      "Yiheng Chi",
      "Xingguang Zhang",
      "Stanley Chan"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=iHDR%3A+Iterative+HDR+Imaging+with+Arbitrary+Number+of+Exposures+Yu+Yuan+Yiheng+Chi+Xingguang+Zhang+Stanley+Chan",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "Y Yuan",
        "id": "qjxBqDkAAAAJ"
      },
      {
        "name": "Y Chi",
        "id": "WZYUtwQAAAAJ"
      },
      {
        "name": "X Zhang",
        "id": "hWeQHPYAAAAJ"
      },
      {
        "name": "S Chan -",
        "id": null
      }
    ],
    "citation_count": null
  },
  {
    "arxiv_id": "2411.19652",
    "title": "Uniform Attention Maps: Boosting Image Fidelity in Reconstruction and Editing",
    "year": 2024,
    "published": "2024-11-29T12:11:28Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "abstract": "Text-guided image generation and editing using diffusion models have achieved remarkable advancements. Among these, tuning-free methods have gained attention for their ability to perform edits without extensive model adjustments, offering simplicity and efficiency. However, existing tuning-free approaches often struggle with balancing fidelity and editing precision. Reconstruction errors in DDIM Inversion are partly attributed to the cross-attention mechanism in U-Net, which introduces misalignm",
    "arxiv_url": "https://arxiv.org/abs/2411.19652v1",
    "pdf_url": "https://arxiv.org/pdf/2411.19652v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2411.19652",
    "arxiv_authors": [
      "Wenyi Mo",
      "Tianyu Zhang",
      "Yalong Bai",
      "Bing Su",
      "Ji-Rong Wen"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Uniform+Attention+Maps%3A+Boosting+Image+Fidelity+in+Reconstruction+and+Editing+Wenyi+Mo+Tianyu+Zhang+Yalong+Bai+Bing+Su+Ji-Rong+Wen",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "W Mo",
        "id": "e3s9h8MAAAAJ"
      },
      {
        "name": "T Zhang",
        "id": "yuqNKw4AAAAJ"
      },
      {
        "name": "Y Bai",
        "id": "iYMBoHwAAAAJ"
      },
      {
        "name": "B Su",
        "id": "d3g2VJQAAAAJ"
      },
      {
        "name": "JR Wen2025 IEEE/CVF Winter",
        "id": null
      }
    ],
    "citation_count": 1
  },
  {
    "arxiv_id": "2302.04264",
    "title": "Nerfstudio: A Modular Framework for Neural Radiance Field Development",
    "year": 2023,
    "published": "2023-02-08T18:58:00Z",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "abstract": "Neural Radiance Fields (NeRF) are a rapidly growing area of research with wide-ranging applications in computer vision, graphics, robotics, and more. In order to streamline the development and deployment of NeRF research, we propose a modular PyTorch framework, Nerfstudio. Our framework includes plug-and-play components for implementing NeRF-based methods, which make it easy for researchers and practitioners to incorporate NeRF into their projects. Additionally, the modular design enables suppor",
    "arxiv_url": "https://arxiv.org/abs/2302.04264v4",
    "pdf_url": "https://arxiv.org/pdf/2302.04264v4",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2302.04264",
    "arxiv_authors": [
      "Matthew Tancik",
      "Ethan Weber",
      "Evonne Ng",
      "Ruilong Li",
      "Brent Yi",
      "Justin Kerr",
      "Terrance Wang",
      "Alexander Kristoffersen",
      "Jake Austin",
      "Kamyar Salahi",
      "Abhik Ahuja",
      "David McAllister",
      "Angjoo Kanazawa"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Nerfstudio%3A+A+Modular+Framework+for+Neural+Radiance+Field+Development+Matthew+Tancik+Ethan+Weber+Evonne+Ng+Ruilong+Li+Brent+Yi",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "M Tancik",
        "id": "l0Bj7U8AAAAJ"
      },
      {
        "name": "E Weber",
        "id": "zw1TzeEAAAAJ"
      },
      {
        "name": "E Ng",
        "id": "wp1MXzsAAAAJ"
      },
      {
        "name": "R Li",
        "id": "mhI3bxsAAAAJ"
      },
      {
        "name": "B Yi",
        "id": "Ecy6lXwAAAAJ"
      },
      {
        "name": "T Wang",
        "id": null
      },
      {
        "name": "A Kristoffersen",
        "id": "u8p5Y1EAAAAJ"
      },
      {
        "name": "J Austin",
        "id": "siW2DBoAAAAJ"
      },
      {
        "name": "K Salahi",
        "id": "UV0IzT4AAAAJ"
      },
      {
        "name": "A Ahuja",
        "id": "yh7H1fIAAAAJ"
      }
    ],
    "citation_count": 914
  },
  {
    "arxiv_id": "2406.18050",
    "title": "A Multi-Stage Goal-Driven Network for Pedestrian Trajectory Prediction",
    "year": 2024,
    "published": "2024-06-26T03:59:21Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Pedestrian trajectory prediction plays a pivotal role in ensuring the safety and efficiency of various applications, including autonomous vehicles and traffic management systems. This paper proposes a novel method for pedestrian trajectory prediction, called multi-stage goal-driven network (MGNet). Diverging from prior approaches relying on stepwise recursive prediction and the singular forecasting of a long-term goal, MGNet directs trajectory generation by forecasting intermediate stage goals, ",
    "arxiv_url": "https://arxiv.org/abs/2406.18050v1",
    "pdf_url": "https://arxiv.org/pdf/2406.18050v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2406.18050",
    "arxiv_authors": [
      "Xiuen Wu",
      "Tao Wang",
      "Yuanzheng Cai",
      "Lingyu Liang",
      "George Papageorgiou"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=A+Multi-Stage+Goal-Driven+Network+for+Pedestrian+Trajectory+Prediction+Xiuen+Wu+Tao+Wang+Yuanzheng+Cai+Lingyu+Liang+George+Papageorgiou",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "X Wu",
        "id": null
      },
      {
        "name": "T Wang",
        "id": "ZQolALsAAAAJ"
      },
      {
        "name": "Y Cai",
        "id": "vg6oOEEAAAAJ"
      },
      {
        "name": "L Liang",
        "id": null
      },
      {
        "name": "G Papageorgiou2024 5th International",
        "id": null
      }
    ],
    "citation_count": 3
  },
  {
    "arxiv_id": "2301.10593",
    "title": "Faster DAN: Multi-target Queries with Document Positional Encoding for End-to-end Handwritten Document Recognition",
    "year": 2023,
    "published": "2023-01-25T13:55:14Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Recent advances in handwritten text recognition enabled to recognize whole documents in an end-to-end way: the Document Attention Network (DAN) recognizes the characters one after the other through an attention-based prediction process until reaching the end of the document. However, this autoregressive process leads to inference that cannot benefit from any parallelization optimization. In this paper, we propose Faster DAN, a two-step strategy to speed up the recognition process at prediction t",
    "arxiv_url": "https://arxiv.org/abs/2301.10593v1",
    "pdf_url": "https://arxiv.org/pdf/2301.10593v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2301.10593",
    "arxiv_authors": [
      "Denis Coquenet",
      "Cl√©ment Chatelain",
      "Thierry Paquet"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Faster+DAN%3A+Multi-target+Queries+with+Document+Positional+Encoding+for+End-to-end+Handwritten+Document+Recognition+Denis+Coquenet+Cl%C3%A9ment+Chatelain+Thierry+Paquet",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "D Coquenet",
        "id": "4FPHJ5cAAAAJ"
      },
      {
        "name": "C Chatelain",
        "id": "yv5aIM4AAAAJ"
      },
      {
        "name": "T Paquet - International",
        "id": null
      }
    ],
    "citation_count": 21
  },
  {
    "arxiv_id": "2310.05321",
    "title": "Edge Computing-Enabled Road Condition Monitoring: System Development and Evaluation",
    "year": 2023,
    "published": "2023-10-09T00:55:41Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Real-time pavement condition monitoring provides highway agencies with timely and accurate information that could form the basis of pavement maintenance and rehabilitation policies. Existing technologies rely heavily on manual data processing, are expensive and therefore, difficult to scale for frequent, networklevel pavement condition monitoring. Additionally, these systems require sending large packets of data to the cloud which requires large storage space, are computationally expensive to pr",
    "arxiv_url": "https://arxiv.org/abs/2310.05321v1",
    "pdf_url": "https://arxiv.org/pdf/2310.05321v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2310.05321",
    "arxiv_authors": [
      "Abdulateef Daud",
      "Mark Amo-Boateng",
      "Neema Jakisa Owor",
      "Armstrong Aboah",
      "Yaw Adu-Gyamfi"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Edge+Computing-Enabled+Road+Condition+Monitoring%3A+System+Development+and+Evaluation+Abdulateef+Daud+Mark+Amo-Boateng+Neema+Jakisa+Owor+Armstrong+Aboah+Yaw+Adu-Gyamfi",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "A Daud -",
        "id": null
      }
    ],
    "citation_count": 7
  },
  {
    "arxiv_id": "2405.13388",
    "title": "Unsupervised Pre-training with Language-Vision Prompts for Low-Data Instance Segmentation",
    "year": 2024,
    "published": "2024-05-22T06:48:43Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "In recent times, following the paradigm of DETR (DEtection TRansformer), query-based end-to-end instance segmentation (QEIS) methods have exhibited superior performance compared to CNN-based models, particularly when trained on large-scale datasets. Nevertheless, the effectiveness of these QEIS methods diminishes significantly when confronted with limited training data. This limitation arises from their reliance on substantial data volumes to effectively train the pivotal queries/kernels that ar",
    "arxiv_url": "https://arxiv.org/abs/2405.13388v1",
    "pdf_url": "https://arxiv.org/pdf/2405.13388v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2405.13388",
    "arxiv_authors": [
      "Dingwen Zhang",
      "Hao Li",
      "Diqi He",
      "Nian Liu",
      "Lechao Cheng",
      "Jingdong Wang",
      "Junwei Han"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Unsupervised+Pre-training+with+Language-Vision+Prompts+for+Low-Data+Instance+Segmentation+Dingwen+Zhang+Hao+Li+Diqi+He+Nian+Liu+Lechao+Cheng",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "D Zhang",
        "id": null
      },
      {
        "name": "H Li",
        "id": "4dokjDoAAAAJ"
      },
      {
        "name": "D He",
        "id": "wAGuU_wAAAAJ"
      },
      {
        "name": "N Liu",
        "id": "ZSilWs4AAAAJ"
      },
      {
        "name": "L Cheng",
        "id": "PKFAv-cAAAAJ"
      },
      {
        "name": "J Wang",
        "id": "z5SPCmgAAAAJ"
      },
      {
        "name": "J HanIEEE Transactions on Pattern Analysis and Machine Intelligence",
        "id": null
      }
    ],
    "citation_count": 13
  },
  {
    "arxiv_id": "2407.13748",
    "title": "General Geometry-aware Weakly Supervised 3D Object Detection",
    "year": 2024,
    "published": "2024-07-18T17:52:08Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "3D object detection is an indispensable component for scene understanding. However, the annotation of large-scale 3D datasets requires significant human effort. To tackle this problem, many methods adopt weakly supervised 3D object detection that estimates 3D boxes by leveraging 2D boxes and scene/class-specific priors. However, these approaches generally depend on sophisticated manual priors, which is hard to generalize to novel categories and scenes. In this paper, we are motivated to propose ",
    "arxiv_url": "https://arxiv.org/abs/2407.13748v1",
    "pdf_url": "https://arxiv.org/pdf/2407.13748v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2407.13748",
    "arxiv_authors": [
      "Guowen Zhang",
      "Junsong Fan",
      "Liyi Chen",
      "Zhaoxiang Zhang",
      "Zhen Lei",
      "Lei Zhang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=General+Geometry-aware+Weakly+Supervised+3D+Object+Detection+Guowen+Zhang+Junsong+Fan+Liyi+Chen+Zhaoxiang+Zhang+Zhen+Lei",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "G Zhang",
        "id": "DxcLKZIAAAAJ"
      },
      {
        "name": "J Fan",
        "id": "AfK4UcUAAAAJ"
      },
      {
        "name": "L Chen",
        "id": "nMev-10AAAAJ"
      },
      {
        "name": "Z Zhang",
        "id": "qxWfV6cAAAAJ"
      },
      {
        "name": "Z Lei",
        "id": "cuJ3QG8AAAAJ"
      },
      {
        "name": "L ZhangEuropean",
        "id": null
      }
    ],
    "citation_count": 3
  },
  {
    "arxiv_id": "2303.16938",
    "title": "Are Neural Architecture Search Benchmarks Well Designed? A Deeper Look Into Operation Importance",
    "year": 2023,
    "published": "2023-03-29T18:03:28Z",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "stat.ML"
    ],
    "abstract": "Neural Architecture Search (NAS) benchmarks significantly improved the capability of developing and comparing NAS methods while at the same time drastically reduced the computational overhead by providing meta-information about thousands of trained neural networks. However, tabular benchmarks have several drawbacks that can hinder fair comparisons and provide unreliable results. These usually focus on providing a small pool of operations in heavily constrained search spaces -- usually cell-based",
    "arxiv_url": "https://arxiv.org/abs/2303.16938v1",
    "pdf_url": "https://arxiv.org/pdf/2303.16938v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2303.16938",
    "arxiv_authors": [
      "Vasco Lopes",
      "Bruno Degardin",
      "Lu√≠s A. Alexandre"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Are+Neural+Architecture+Search+Benchmarks+Well+Designed%3F+A+Deeper+Look+Into+Operation+Importance+Vasco+Lopes+Bruno+Degardin+Lu%C3%ADs+A.+Alexandre",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "V Lopes",
        "id": "bvYBcRkAAAAJ"
      },
      {
        "name": "B Degardin",
        "id": "0G3dinIAAAAJ"
      },
      {
        "name": "LA Alexandre - Information Sciences",
        "id": null
      }
    ],
    "citation_count": 11
  },
  {
    "arxiv_id": "2503.09069",
    "title": "Theoretical Guarantees for High Order Trajectory Refinement in Generative Flows",
    "year": 2025,
    "published": "2025-03-12T05:07:07Z",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "stat.ML"
    ],
    "abstract": "Flow matching has emerged as a powerful framework for generative modeling, offering computational advantages over diffusion models by leveraging deterministic Ordinary Differential Equations (ODEs) instead of stochastic dynamics. While prior work established the worst case optimality of standard flow matching under Wasserstein distances, the theoretical guarantees for higher-order flow matching - which incorporates acceleration terms to refine sample trajectories - remain unexplored. In this pap",
    "arxiv_url": "https://arxiv.org/abs/2503.09069v1",
    "pdf_url": "https://arxiv.org/pdf/2503.09069v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2503.09069",
    "arxiv_authors": [
      "Chengyue Gong",
      "Xiaoyu Li",
      "Yingyu Liang",
      "Jiangxuan Long",
      "Zhenmei Shi",
      "Zhao Song",
      "Yu Tian"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Theoretical+Guarantees+for+High+Order+Trajectory+Refinement+in+Generative+Flows+Chengyue+Gong+Xiaoyu+Li+Yingyu+Liang+Jiangxuan+Long+Zhenmei+Shi",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "C Gong",
        "id": "AscakBgAAAAJ"
      },
      {
        "name": "X Li",
        "id": null
      },
      {
        "name": "Y Liang",
        "id": "_RVvnS4AAAAJ"
      },
      {
        "name": "J Long",
        "id": null
      },
      {
        "name": "Z Shi",
        "id": null
      },
      {
        "name": "Z Song",
        "id": "yDZct7UAAAAJ"
      },
      {
        "name": "Y Tian",
        "id": null
      }
    ],
    "citation_count": 1
  },
  {
    "arxiv_id": "2407.19468",
    "title": "MVPbev: Multi-view Perspective Image Generation from BEV with Test-time Controllability and Generalizability",
    "year": 2024,
    "published": "2024-07-28T11:39:40Z",
    "categories": [
      "cs.CV",
      "cs.MM"
    ],
    "abstract": "This work aims to address the multi-view perspective RGB generation from text prompts given Bird-Eye-View(BEV) semantics. Unlike prior methods that neglect layout consistency, lack the ability to handle detailed text prompts, or are incapable of generalizing to unseen view points, MVPbev simultaneously generates cross-view consistent images of different perspective views with a two-stage design, allowing object-level control and novel view generation at test-time. Specifically, MVPbev firstly pr",
    "arxiv_url": "https://arxiv.org/abs/2407.19468v1",
    "pdf_url": "https://arxiv.org/pdf/2407.19468v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2407.19468",
    "arxiv_authors": [
      "Buyu Liu",
      "Kai Wang",
      "Yansong Liu",
      "Jun Bao",
      "Tingting Han",
      "Jun Yu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=MVPbev%3A+Multi-view+Perspective+Image+Generation+from+BEV+with+Test-time+Controllability+and+Generalizability+Buyu+Liu+Kai+Wang+Yansong+Liu+Jun+Bao+Tingting+Han",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "B Liu",
        "id": "67HpPiEAAAAJ"
      },
      {
        "name": "K Wang",
        "id": "GBnksEMAAAAJ"
      },
      {
        "name": "Y Liu",
        "id": null
      },
      {
        "name": "J Bao",
        "id": "MtrhMEUAAAAJ"
      },
      {
        "name": "T Han",
        "id": "wSAyFmsAAAAJ"
      },
      {
        "name": "J Yu -",
        "id": null
      }
    ],
    "citation_count": 4
  },
  {
    "arxiv_id": "2409.02598",
    "title": "SurgTrack: CAD-Free 3D Tracking of Real-world Surgical Instruments",
    "year": 2024,
    "published": "2024-09-04T10:29:59Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.RO"
    ],
    "abstract": "Vision-based surgical navigation has received increasing attention due to its non-invasive, cost-effective, and flexible advantages. In particular, a critical element of the vision-based navigation system is tracking surgical instruments. Compared with 2D instrument tracking methods, 3D instrument tracking has broader value in clinical practice, but is also more challenging due to weak texture, occlusion, and lack of Computer-Aided Design (CAD) models for 3D registration. To solve these challeng",
    "arxiv_url": "https://arxiv.org/abs/2409.02598v1",
    "pdf_url": "https://arxiv.org/pdf/2409.02598v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2409.02598",
    "arxiv_authors": [
      "Wenwu Guo",
      "Jinlin Wu",
      "Zhen Chen",
      "Qingxiang Zhao",
      "Miao Xu",
      "Zhen Lei",
      "Hongbin Liu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=SurgTrack%3A+CAD-Free+3D+Tracking+of+Real-world+Surgical+Instruments+Wenwu+Guo+Jinlin+Wu+Zhen+Chen+Qingxiang+Zhao+Miao+Xu",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "W Guo",
        "id": null
      },
      {
        "name": "J Wu",
        "id": "XujjZmUAAAAJ"
      },
      {
        "name": "Z Chen",
        "id": "oVG2zEkAAAAJ"
      },
      {
        "name": "Q Zhao",
        "id": null
      },
      {
        "name": "M Xu",
        "id": "eHbkeRsAAAAJ"
      },
      {
        "name": "Z Lei",
        "id": "cuJ3QG8AAAAJ"
      },
      {
        "name": "H LiuInternational",
        "id": null
      }
    ],
    "citation_count": null
  },
  {
    "arxiv_id": "2305.11718",
    "title": "Towards Accurate Image Coding: Improved Autoregressive Image Generation with Dynamic Vector Quantization",
    "year": 2023,
    "published": "2023-05-19T14:56:05Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Existing vector quantization (VQ) based autoregressive models follow a two-stage generation paradigm that first learns a codebook to encode images as discrete codes, and then completes generation based on the learned codebook. However, they encode fixed-size image regions into fixed-length codes and ignore their naturally different information densities, which results in insufficiency in important regions and redundancy in unimportant ones, and finally degrades the generation quality and speed. ",
    "arxiv_url": "https://arxiv.org/abs/2305.11718v1",
    "pdf_url": "https://arxiv.org/pdf/2305.11718v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2305.11718",
    "arxiv_authors": [
      "Mengqi Huang",
      "Zhendong Mao",
      "Zhuowei Chen",
      "Yongdong Zhang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Towards+Accurate+Image+Coding%3A+Improved+Autoregressive+Image+Generation+with+Dynamic+Vector+Quantization+Mengqi+Huang+Zhendong+Mao+Zhuowei+Chen+Yongdong+Zhang",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "M Huang",
        "id": "BJvM6gsAAAAJ"
      },
      {
        "name": "Z Mao",
        "id": "m-0P8sgAAAAJ"
      },
      {
        "name": "Z Chen",
        "id": "ow1jGJkAAAAJ"
      },
      {
        "name": "Y Zhang",
        "id": null
      }
    ],
    "citation_count": 59
  },
  {
    "arxiv_id": "2409.09560",
    "title": "Evaluating authenticity and quality of image captions via sentiment and semantic analyses",
    "year": 2024,
    "published": "2024-09-14T23:50:23Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "abstract": "The growth of deep learning (DL) relies heavily on huge amounts of labelled data for tasks such as natural language processing and computer vision. Specifically, in image-to-text or image-to-image pipelines, opinion (sentiment) may be inadvertently learned by a model from human-generated image captions. Additionally, learning may be affected by the variety and diversity of the provided captions. While labelling large datasets has largely relied on crowd-sourcing or data-worker pools, evaluating ",
    "arxiv_url": "https://arxiv.org/abs/2409.09560v1",
    "pdf_url": "https://arxiv.org/pdf/2409.09560v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2409.09560",
    "arxiv_authors": [
      "Aleksei Krotov",
      "Alison Tebo",
      "Dylan K. Picart",
      "Aaron Dean Algave"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Evaluating+authenticity+and+quality+of+image+captions+via+sentiment+and+semantic+analyses+Aleksei+Krotov+Alison+Tebo+Dylan+K.+Picart+Aaron+Dean+Algave",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "A Krotov",
        "id": "Iv2yzR0AAAAJ"
      },
      {
        "name": "A Tebo",
        "id": "eBNpt5QAAAAJ"
      },
      {
        "name": "DK Picart",
        "id": null
      },
      {
        "name": "AD Algave -",
        "id": null
      }
    ],
    "citation_count": 1
  },
  {
    "arxiv_id": "2403.11193",
    "title": "Neural Markov Random Field for Stereo Matching",
    "year": 2024,
    "published": "2024-03-17T12:40:46Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Stereo matching is a core task for many computer vision and robotics applications. Despite their dominance in traditional stereo methods, the hand-crafted Markov Random Field (MRF) models lack sufficient modeling accuracy compared to end-to-end deep models. While deep learning representations have greatly improved the unary terms of the MRF models, the overall accuracy is still severely limited by the hand-crafted pairwise terms and message passing. To address these issues, we propose a neural M",
    "arxiv_url": "https://arxiv.org/abs/2403.11193v2",
    "pdf_url": "https://arxiv.org/pdf/2403.11193v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2403.11193",
    "arxiv_authors": [
      "Tongfan Guan",
      "Chen Wang",
      "Yun-Hui Liu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Neural+Markov+Random+Field+for+Stereo+Matching+Tongfan+Guan+Chen+Wang+Yun-Hui+Liu",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "T Guan",
        "id": null
      },
      {
        "name": "C Wang",
        "id": "vZfmKl4AAAAJ"
      },
      {
        "name": "YH Liu -",
        "id": null
      }
    ],
    "citation_count": 54
  },
  {
    "arxiv_id": "2505.11018",
    "title": "Dual Teacher-Student Learning for Semi-supervised Medical Image Segmentation",
    "year": 2025,
    "published": "2025-05-16T09:14:06Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Semi-supervised learning reduces the costly manual annotation burden in medical image segmentation. A popular approach is the mean teacher (MT) strategy, which applies consistency regularization using a temporally averaged teacher model. In this work, the MT strategy is reinterpreted as a form of self-paced learning in the context of supervised learning, where agreement between the teacher's predictions and the ground truth implicitly guides the model from easy to hard. Extending this insight to",
    "arxiv_url": "https://arxiv.org/abs/2505.11018v2",
    "pdf_url": "https://arxiv.org/pdf/2505.11018v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2505.11018",
    "arxiv_authors": [
      "Pengchen Zhang",
      "Alan J. X. Guo",
      "Sipin Luo",
      "Zhe Han",
      "Lin Guo"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Dual+Teacher-Student+Learning+for+Semi-supervised+Medical+Image+Segmentation+Pengchen+Zhang+Alan+J.+X.+Guo+Sipin+Luo+Zhe+Han+Lin+Guo",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "P Zhang",
        "id": null
      },
      {
        "name": "AJX Guo",
        "id": "euGk5toAAAAJ"
      },
      {
        "name": "S Luo",
        "id": null
      },
      {
        "name": "Z Han",
        "id": null
      },
      {
        "name": "L Guo -",
        "id": null
      }
    ],
    "citation_count": 1
  },
  {
    "arxiv_id": "2404.07711",
    "title": "OpenTrench3D: A Photogrammetric 3D Point Cloud Dataset for Semantic Segmentation of Underground Utilities",
    "year": 2024,
    "published": "2024-04-11T12:58:12Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Identifying and classifying underground utilities is an important task for efficient and effective urban planning and infrastructure maintenance. We present OpenTrench3D, a novel and comprehensive 3D Semantic Segmentation point cloud dataset, designed to advance research and development in underground utility surveying and mapping. OpenTrench3D covers a completely novel domain for public 3D point cloud datasets and is unique in its focus, scope, and cost-effective capturing method. The dataset c",
    "arxiv_url": "https://arxiv.org/abs/2404.07711v1",
    "pdf_url": "https://arxiv.org/pdf/2404.07711v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2404.07711",
    "arxiv_authors": [
      "Lasse H. Hansen",
      "Simon B. Jensen",
      "Mark P. Philipsen",
      "Andreas M√∏gelmose",
      "Lars Bodum",
      "Thomas B. Moeslund"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=OpenTrench3D%3A+A+Photogrammetric+3D+Point+Cloud+Dataset+for+Semantic+Segmentation+of+Underground+Utilities+Lasse+H.+Hansen+Simon+B.+Jensen+Mark+P.+Philipsen+Andreas+M%C3%B8gelmose+Lars+Bodum",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "LH Hansen",
        "id": "X7XPWKEAAAAJ"
      },
      {
        "name": "SB Jensen",
        "id": "vJMsABkAAAAJ"
      },
      {
        "name": "MP Philipsen",
        "id": "p2f_-EcAAAAJ"
      },
      {
        "name": "A M√∏gelmose",
        "id": "OuMmK7kAAAAJ"
      },
      {
        "name": "L Bodum",
        "id": "-w_2_UEAAAAJ"
      },
      {
        "name": "TB Moeslund",
        "id": "XmkDts4AAAAJ"
      }
    ],
    "citation_count": 5
  },
  {
    "arxiv_id": "2503.09187",
    "title": "Polygonizing Roof Segments from High-Resolution Aerial Images Using Yolov8-Based Edge Detection",
    "year": 2025,
    "published": "2025-03-12T09:29:10Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "This study presents a novel approach for roof detail extraction and vectorization using remote sensing images. Unlike previous geometric-primitive-based methods that rely on the detection of corners, our method focuses on edge detection as the primary mechanism for roof reconstruction, while utilizing geometric relationships to define corners and faces. We adapt the YOLOv8 OBB model, originally designed for rotated object detection, to extract roof edges effectively. Our method demonstrates robu",
    "arxiv_url": "https://arxiv.org/abs/2503.09187v1",
    "pdf_url": "https://arxiv.org/pdf/2503.09187v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2503.09187",
    "arxiv_authors": [
      "Qipeng Mei",
      "Dimitri Bulatov",
      "Dorota Iwaszczuk"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Polygonizing+Roof+Segments+from+High-Resolution+Aerial+Images+Using+Yolov8-Based+Edge+Detection+Qipeng+Mei+Dimitri+Bulatov+Dorota+Iwaszczuk",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "Q Mei",
        "id": "CEwI7OgAAAAJ"
      },
      {
        "name": "D Bulatov",
        "id": "n2SRH9oAAAAJ"
      },
      {
        "name": "D Iwaszczuk -",
        "id": null
      }
    ],
    "citation_count": 1
  },
  {
    "arxiv_id": "2311.12033",
    "title": "An improved two-threshold quantum segmentation algorithm for NEQR image",
    "year": 2023,
    "published": "2023-10-02T17:04:36Z",
    "categories": [
      "quant-ph",
      "cs.CV"
    ],
    "abstract": "The quantum image segmentation algorithm is to divide a quantum image into several parts, but most of the existing algorithms use more quantum resource(qubit) or cannot process the complex image. In this paper, an improved two-threshold quantum segmentation algorithm for NEQR image is proposed, which can segment the complex gray-scale image into a clear ternary image by using fewer qubits and can be scaled to use n thresholds for n + 1 segmentations. In addition, a feasible quantum comparator is",
    "arxiv_url": "https://arxiv.org/abs/2311.12033v1",
    "pdf_url": "https://arxiv.org/pdf/2311.12033v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2311.12033",
    "arxiv_authors": [
      "Lu Wang",
      "Zhiliang Deng",
      "Wenjie Liu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=An+improved+two-threshold+quantum+segmentation+algorithm+for+NEQR+image+Lu+Wang+Zhiliang+Deng+Wenjie+Liu",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "L Wang",
        "id": "54YkpVIAAAAJ"
      },
      {
        "name": "Z Deng",
        "id": null
      },
      {
        "name": "W Liu -",
        "id": null
      }
    ],
    "citation_count": 9
  },
  {
    "arxiv_id": "2503.07523",
    "title": "VisRL: Intention-Driven Visual Perception via Reinforced Reasoning",
    "year": 2025,
    "published": "2025-03-10T16:49:35Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Visual understanding is inherently intention-driven - humans selectively focus on different regions of a scene based on their goals. Recent advances in large multimodal models (LMMs) enable flexible expression of such intentions through natural language, allowing queries to guide visual reasoning processes. Frameworks like Visual Chain-of-Thought have demonstrated the benefit of incorporating explicit reasoning steps, where the model predicts a focus region before answering a query. However, exi",
    "arxiv_url": "https://arxiv.org/abs/2503.07523v2",
    "pdf_url": "https://arxiv.org/pdf/2503.07523v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2503.07523",
    "arxiv_authors": [
      "Zhangquan Chen",
      "Xufang Luo",
      "Dongsheng Li"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=VisRL%3A+Intention-Driven+Visual+Perception+via+Reinforced+Reasoning+Zhangquan+Chen+Xufang+Luo+Dongsheng+Li",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "Z Chen",
        "id": null
      },
      {
        "name": "X Luo",
        "id": null
      },
      {
        "name": "D Li -",
        "id": null
      }
    ],
    "citation_count": 19
  },
  {
    "arxiv_id": "2405.18897",
    "title": "MLAE: Masked LoRA Experts for Visual Parameter-Efficient Fine-Tuning",
    "year": 2024,
    "published": "2024-05-29T08:57:23Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "In response to the challenges posed by the extensive parameter updates required for full fine-tuning of large-scale pre-trained models, parameter-efficient fine-tuning (PEFT) methods, exemplified by Low-Rank Adaptation (LoRA), have emerged. LoRA simplifies the fine-tuning process but may still struggle with a certain level of redundancy in low-rank matrices and limited effectiveness from merely increasing their rank. To address these issues, a natural idea is to enhance the independence and dive",
    "arxiv_url": "https://arxiv.org/abs/2405.18897v2",
    "pdf_url": "https://arxiv.org/pdf/2405.18897v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2405.18897",
    "arxiv_authors": [
      "Junjie Wang",
      "Guangjing Yang",
      "Wentao Chen",
      "Huahui Yi",
      "Xiaohu Wu",
      "Zhouchen Lin",
      "Qicheng Lao"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=MLAE%3A+Masked+LoRA+Experts+for+Visual+Parameter-Efficient+Fine-Tuning+Junjie+Wang+Guangjing+Yang+Wentao+Chen+Huahui+Yi+Xiaohu+Wu",
    "gs_search_success": false,
    "gs_authors": null,
    "citation_count": null,
    "error_type": "fetch_failed"
  },
  {
    "arxiv_id": "2405.06828",
    "title": "G-FARS: Gradient-Field-based Auto-Regressive Sampling for 3D Part Grouping",
    "year": 2024,
    "published": "2024-05-10T21:58:40Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "This paper proposes a novel task named \"3D part grouping\". Suppose there is a mixed set containing scattered parts from various shapes. This task requires algorithms to find out every possible combination among all the parts. To address this challenge, we propose the so called Gradient Field-based Auto-Regressive Sampling framework (G-FARS) tailored specifically for the 3D part grouping task. In our framework, we design a gradient-field-based selection graph neural network (GNN) to learn the gra",
    "arxiv_url": "https://arxiv.org/abs/2405.06828v1",
    "pdf_url": "https://arxiv.org/pdf/2405.06828v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2405.06828",
    "arxiv_authors": [
      "Junfeng Cheng",
      "Tania Stathaki"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=G-FARS%3A+Gradient-Field-based+Auto-Regressive+Sampling+for+3D+Part+Grouping+Junfeng+Cheng+Tania+Stathaki",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "J Cheng",
        "id": "hPEIniwAAAAJ"
      },
      {
        "name": "T Stathaki -",
        "id": null
      }
    ],
    "citation_count": 2
  },
  {
    "arxiv_id": "2403.11162",
    "title": "CGI-DM: Digital Copyright Authentication for Diffusion Models via Contrasting Gradient Inversion",
    "year": 2024,
    "published": "2024-03-17T10:06:38Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CR",
      "cs.CY",
      "cs.LG"
    ],
    "abstract": "Diffusion Models (DMs) have evolved into advanced image generation tools, especially for few-shot generation where a pretrained model is fine-tuned on a small set of images to capture a specific style or object. Despite their success, concerns exist about potential copyright violations stemming from the use of unauthorized data in this process. In response, we present Contrasting Gradient Inversion for Diffusion Models (CGI-DM), a novel method featuring vivid visual representations for digital c",
    "arxiv_url": "https://arxiv.org/abs/2403.11162v1",
    "pdf_url": "https://arxiv.org/pdf/2403.11162v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2403.11162",
    "arxiv_authors": [
      "Xiaoyu Wu",
      "Yang Hua",
      "Chumeng Liang",
      "Jiaru Zhang",
      "Hao Wang",
      "Tao Song",
      "Haibing Guan"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=CGI-DM%3A+Digital+Copyright+Authentication+for+Diffusion+Models+via+Contrasting+Gradient+Inversion+Xiaoyu+Wu+Yang+Hua+Chumeng+Liang+Jiaru+Zhang+Hao+Wang",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "X Wu",
        "id": "VzQkSAkAAAAJ"
      },
      {
        "name": "Y Hua",
        "id": "N0tFi8MAAAAJ"
      },
      {
        "name": "C Liang",
        "id": "4S0PYJYAAAAJ"
      },
      {
        "name": "J Zhang",
        "id": "d6q4zkMAAAAJ"
      },
      {
        "name": "H Wang",
        "id": "r-Ik__gAAAAJ"
      },
      {
        "name": "T Song",
        "id": "tIjK-3QAAAAJ"
      },
      {
        "name": "H Guan2024 IEEE/CVF",
        "id": null
      }
    ],
    "citation_count": 8
  },
  {
    "arxiv_id": "2307.09810",
    "title": "GenKL: An Iterative Framework for Resolving Label Ambiguity and Label Non-conformity in Web Images Via a New Generalized KL Divergence",
    "year": 2023,
    "published": "2023-07-19T07:58:21Z",
    "categories": [
      "cs.CV",
      "cs.IT",
      "cs.LG"
    ],
    "abstract": "Web image datasets curated online inherently contain ambiguous in-distribution (ID) instances and out-of-distribution (OOD) instances, which we collectively call non-conforming (NC) instances. In many recent approaches for mitigating the negative effects of NC instances, the core implicit assumption is that the NC instances can be found via entropy maximization. For \"entropy\" to be well-defined, we are interpreting the output prediction vector of an instance as the parameter vector of a multinom",
    "arxiv_url": "https://arxiv.org/abs/2307.09810v1",
    "pdf_url": "https://arxiv.org/pdf/2307.09810v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2307.09810",
    "arxiv_authors": [
      "Xia Huang",
      "Kai Fong Ernest Chong"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=GenKL%3A+An+Iterative+Framework+for+Resolving+Label+Ambiguity+and+Label+Non-conformity+in+Web+Images+Via+a+New+Generalized+KL+Divergence+Xia+Huang+Kai+Fong+Ernest+Chong",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "X Huang",
        "id": null
      },
      {
        "name": "KFE Chong - International",
        "id": null
      }
    ],
    "citation_count": 7
  },
  {
    "arxiv_id": "2305.15420",
    "title": "A Hybrid Semantic-Geometric Approach for Clutter-Resistant Floorplan Generation from Building Point Clouds",
    "year": 2023,
    "published": "2023-05-15T20:08:43Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Building Information Modeling (BIM) technology is a key component of modern construction engineering and project management workflows. As-is BIM models that represent the spatial reality of a project site can offer crucial information to stakeholders for construction progress monitoring, error checking, and building maintenance purposes. Geometric methods for automatically converting raw scan data into BIM models (Scan-to-BIM) often fail to make use of higher-level semantic information in the da",
    "arxiv_url": "https://arxiv.org/abs/2305.15420v1",
    "pdf_url": "https://arxiv.org/pdf/2305.15420v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2305.15420",
    "arxiv_authors": [
      "Seongyong Kim",
      "Yosuke Yajima",
      "Jisoo Park",
      "Jingdao Chen",
      "Yong K. Cho"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=A+Hybrid+Semantic-Geometric+Approach+for+Clutter-Resistant+Floorplan+Generation+from+Building+Point+Clouds+Seongyong+Kim+Yosuke+Yajima+Jisoo+Park+Jingdao+Chen+Yong+K.+Cho",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "S Kim",
        "id": "64HeatcAAAAJ"
      },
      {
        "name": "Y Yajima",
        "id": null
      },
      {
        "name": "J Park",
        "id": "FMbWIvkAAAAJ"
      },
      {
        "name": "J Chen",
        "id": "ndx5aOcAAAAJ"
      },
      {
        "name": "YK Cho -",
        "id": null
      }
    ],
    "citation_count": 1
  },
  {
    "arxiv_id": "2307.08339",
    "title": "Multi-Task Cross-Modality Attention-Fusion for 2D Object Detection",
    "year": 2023,
    "published": "2023-07-17T09:26:13Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.MM"
    ],
    "abstract": "Accurate and robust object detection is critical for autonomous driving. Image-based detectors face difficulties caused by low visibility in adverse weather conditions. Thus, radar-camera fusion is of particular interest but presents challenges in optimally fusing heterogeneous data sources. To approach this issue, we propose two new radar preprocessing techniques to better align radar and camera data. In addition, we introduce a Multi-Task Cross-Modality Attention-Fusion Network (MCAF-Net) for ",
    "arxiv_url": "https://arxiv.org/abs/2307.08339v1",
    "pdf_url": "https://arxiv.org/pdf/2307.08339v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2307.08339",
    "arxiv_authors": [
      "Huawei Sun",
      "Hao Feng",
      "Georg Stettinger",
      "Lorenzo Servadei",
      "Robert Wille"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Multi-Task+Cross-Modality+Attention-Fusion+for+2D+Object+Detection+Huawei+Sun+Hao+Feng+Georg+Stettinger+Lorenzo+Servadei+Robert+Wille",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "H Sun",
        "id": "9BzUW-IAAAAJ"
      },
      {
        "name": "H Feng",
        "id": "3vRljycAAAAJ"
      },
      {
        "name": "G Stettinger",
        "id": null
      },
      {
        "name": "L Servadei",
        "id": "aUrYY2EAAAAJ"
      },
      {
        "name": "R Wille2023 IEEE 26th International",
        "id": null
      }
    ],
    "citation_count": 10
  },
  {
    "arxiv_id": "2505.16314",
    "title": "NTIRE 2025 challenge on Text to Image Generation Model Quality Assessment",
    "year": 2025,
    "published": "2025-05-22T07:12:36Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "This paper reports on the NTIRE 2025 challenge on Text to Image (T2I) generation model quality assessment, which will be held in conjunction with the New Trends in Image Restoration and Enhancement Workshop (NTIRE) at CVPR 2025. The aim of this challenge is to address the fine-grained quality assessment of text-to-image generation models. This challenge evaluates text-to-image models from two aspects: image-text alignment and image structural distortion detection, and is divided into the alignme",
    "arxiv_url": "https://arxiv.org/abs/2505.16314v1",
    "pdf_url": "https://arxiv.org/pdf/2505.16314v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2505.16314",
    "arxiv_authors": [
      "Shuhao Han",
      "Haotian Fan",
      "Fangyuan Kong",
      "Wenjie Liao",
      "Chunle Guo",
      "Chongyi Li",
      "Radu Timofte",
      "Liang Li",
      "Tao Li",
      "Junhui Cui",
      "Yunqiu Wang",
      "Yang Tai",
      "Jingwei Sun",
      "Jianhui Sun",
      "Xinli Yue",
      "Tianyi Wang",
      "Huan Hou",
      "Junda Lu",
      "Xinyang Huang",
      "Zitang Zhou",
      "Zijian Zhang",
      "Xuhui Zheng",
      "Xuecheng Wu",
      "Chong Peng",
      "Xuezhi Cao",
      "Trong-Hieu Nguyen-Mau",
      "Minh-Hoang Le",
      "Minh-Khoa Le-Phan",
      "Duy-Nam Ly",
      "Hai-Dang Nguyen",
      "Minh-Triet Tran",
      "Yukang Lin",
      "Yan Hong",
      "Chuanbiao Song",
      "Siyuan Li",
      "Jun Lan",
      "Zhichao Zhang",
      "Xinyue Li",
      "Wei Sun",
      "Zicheng Zhang",
      "Yunhao Li",
      "Xiaohong Liu",
      "Guangtao Zhai",
      "Zitong Xu",
      "Huiyu Duan",
      "Jiarui Wang",
      "Guangji Ma",
      "Liu Yang",
      "Lu Liu",
      "Qiang Hu",
      "Xiongkuo Min",
      "Zichuan Wang",
      "Zhenchen Tang",
      "Bo Peng",
      "Jing Dong",
      "Fengbin Guan",
      "Zihao Yu",
      "Yiting Lu",
      "Wei Luo",
      "Xin Li",
      "Minhao Lin",
      "Haofeng Chen",
      "Xuanxuan He",
      "Kele Xu",
      "Qisheng Xu",
      "Zijian Gao",
      "Tianjiao Wan",
      "Bo-Cheng Qiu",
      "Chih-Chung Hsu",
      "Chia-ming Lee",
      "Yu-Fan Lin",
      "Bo Yu",
      "Zehao Wang",
      "Da Mu",
      "Mingxiu Chen",
      "Junkang Fang",
      "Huamei Sun",
      "Wending Zhao",
      "Zhiyu Wang",
      "Wang Liu",
      "Weikang Yu",
      "Puhong Duan",
      "Bin Sun",
      "Xudong Kang",
      "Shutao Li",
      "Shuai He",
      "Lingzhi Fu",
      "Heng Cong",
      "Rongyu Zhang",
      "Jiarong He",
      "Zhishan Qiao",
      "Yongqing Huang",
      "Zewen Chen",
      "Zhe Pang",
      "Juan Wang",
      "Jian Guo",
      "Zhizhuo Shao",
      "Ziyu Feng",
      "Bing Li",
      "Weiming Hu",
      "Hesong Li",
      "Dehua Liu",
      "Zeming Liu",
      "Qingsong Xie",
      "Ruichen Wang",
      "Zhihao Li",
      "Yuqi Liang",
      "Jianqi Bi",
      "Jun Luo",
      "Junfeng Yang",
      "Can Li",
      "Jing Fu",
      "Hongwei Xu",
      "Mingrui Long",
      "Lulin Tang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=NTIRE+2025+challenge+on+Text+to+Image+Generation+Model+Quality+Assessment+Shuhao+Han+Haotian+Fan+Fangyuan+Kong+Wenjie+Liao+Chunle+Guo",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "S Han",
        "id": null
      },
      {
        "name": "H Fan",
        "id": "DwNatkMAAAAJ"
      },
      {
        "name": "F Kong",
        "id": null
      },
      {
        "name": "W Liao",
        "id": null
      },
      {
        "name": "C Guo",
        "id": "RZLYwR0AAAAJ"
      },
      {
        "name": "C Li",
        "id": "1_I0P-AAAAAJ"
      },
      {
        "name": "R Timofte",
        "id": "u3MwH5kAAAAJ"
      },
      {
        "name": "L Li",
        "id": "z_fYeJoAAAAJ"
      },
      {
        "name": "T Li",
        "id": null
      },
      {
        "name": "J Cui",
        "id": null
      },
      {
        "name": "Y Wang",
        "id": "EZAZy7AAAAAJ"
      },
      {
        "name": "Y Tai",
        "id": null
      },
      {
        "name": "J Sun",
        "id": null
      }
    ],
    "citation_count": null
  },
  {
    "arxiv_id": "2404.00679",
    "title": "Weak-to-Strong 3D Object Detection with X-Ray Distillation",
    "year": 2024,
    "published": "2024-03-31T13:09:06Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "This paper addresses the critical challenges of sparsity and occlusion in LiDAR-based 3D object detection. Current methods often rely on supplementary modules or specific architectural designs, potentially limiting their applicability to new and evolving architectures. To our knowledge, we are the first to propose a versatile technique that seamlessly integrates into any existing framework for 3D Object Detection, marking the first instance of Weak-to-Strong generalization in 3D computer vision.",
    "arxiv_url": "https://arxiv.org/abs/2404.00679v1",
    "pdf_url": "https://arxiv.org/pdf/2404.00679v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2404.00679",
    "arxiv_authors": [
      "Alexander Gambashidze",
      "Aleksandr Dadukin",
      "Maksim Golyadkin",
      "Maria Razzhivina",
      "Ilya Makarov"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Weak-to-Strong+3D+Object+Detection+with+X-Ray+Distillation+Alexander+Gambashidze+Aleksandr+Dadukin+Maksim+Golyadkin+Maria+Razzhivina+Ilya+Makarov",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "A Gambashidze",
        "id": null
      },
      {
        "name": "A Dadukin",
        "id": "Y_kv_6sAAAAJ"
      },
      {
        "name": "M Golyadkin",
        "id": "U7gopOoAAAAJ"
      },
      {
        "name": "M Razzhivina",
        "id": "DhNH46gAAAAJ"
      },
      {
        "name": "I Makarov",
        "id": "cFpDMzIAAAAJ"
      }
    ],
    "citation_count": 8
  },
  {
    "arxiv_id": "2502.06020",
    "title": "Temporal Working Memory: Query-Guided Segment Refinement for Enhanced Multimodal Understanding",
    "year": 2025,
    "published": "2025-02-09T20:26:30Z",
    "categories": [
      "cs.CV",
      "cs.MM",
      "cs.SD",
      "eess.AS"
    ],
    "abstract": "Multimodal foundation models (MFMs) have demonstrated significant success in tasks such as visual captioning, question answering, and image-text retrieval. However, these models face inherent limitations due to their finite internal capacity, which restricts their ability to process extended temporal sequences, a crucial requirement for comprehensive video and audio analysis. To overcome these challenges, we introduce a specialized cognitive module, temporal working memory (TWM), which aims to e",
    "arxiv_url": "https://arxiv.org/abs/2502.06020v1",
    "pdf_url": "https://arxiv.org/pdf/2502.06020v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2502.06020",
    "arxiv_authors": [
      "Xingjian Diao",
      "Chunhui Zhang",
      "Weiyi Wu",
      "Zhongyu Ouyang",
      "Peijun Qing",
      "Ming Cheng",
      "Soroush Vosoughi",
      "Jiang Gui"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Temporal+Working+Memory%3A+Query-Guided+Segment+Refinement+for+Enhanced+Multimodal+Understanding+Xingjian+Diao+Chunhui+Zhang+Weiyi+Wu+Zhongyu+Ouyang+Peijun+Qing",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "X Diao",
        "id": "-U2KzpYAAAAJ"
      },
      {
        "name": "C Zhang",
        "id": "jlqnbkAAAAAJ"
      },
      {
        "name": "W Wu",
        "id": "H6ypK0sAAAAJ"
      },
      {
        "name": "Z Ouyang",
        "id": "ySWheKcAAAAJ"
      },
      {
        "name": "P Qing",
        "id": "8iYd-1UAAAAJ"
      },
      {
        "name": "M Cheng",
        "id": "MPyUxv4AAAAJ"
      },
      {
        "name": "S Vosoughi",
        "id": "45DAXkwAAAAJ"
      },
      {
        "name": "J Gui",
        "id": "Yd2HEqsAAAAJ"
      }
    ],
    "citation_count": 19
  },
  {
    "arxiv_id": "2411.10649",
    "title": "Deep Loss Convexification for Learning Iterative Models",
    "year": 2024,
    "published": "2024-11-16T01:13:04Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Iterative methods such as iterative closest point (ICP) for point cloud registration often suffer from bad local optimality (e.g. saddle points), due to the nature of nonconvex optimization. To address this fundamental challenge, in this paper we propose learning to form the loss landscape of a deep iterative method w.r.t. predictions at test time into a convex-like shape locally around each ground truth given data, namely Deep Loss Convexification (DLC), thanks to the overparametrization in neu",
    "arxiv_url": "https://arxiv.org/abs/2411.10649v1",
    "pdf_url": "https://arxiv.org/pdf/2411.10649v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2411.10649",
    "arxiv_authors": [
      "Ziming Zhang",
      "Yuping Shao",
      "Yiqing Zhang",
      "Fangzhou Lin",
      "Haichong Zhang",
      "Elke Rundensteiner"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Deep+Loss+Convexification+for+Learning+Iterative+Models+Ziming+Zhang+Yuping+Shao+Yiqing+Zhang+Fangzhou+Lin+Haichong+Zhang",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "Z Zhang",
        "id": "2yqx3oIAAAAJ"
      },
      {
        "name": "Y Shao",
        "id": null
      },
      {
        "name": "Y Zhang",
        "id": "NFQx3HUAAAAJ"
      },
      {
        "name": "F Lin",
        "id": "ninTViIAAAAJ"
      },
      {
        "name": "H Zhang",
        "id": "8HTrAP4AAAAJ"
      },
      {
        "name": "E RundensteinerIEEE Transactions on Pattern Analysis and Machine Intelligence",
        "id": null
      }
    ],
    "citation_count": 4
  },
  {
    "arxiv_id": "2306.05720",
    "title": "Beyond Surface Statistics: Scene Representations in a Latent Diffusion Model",
    "year": 2023,
    "published": "2023-06-09T07:34:34Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "abstract": "Latent diffusion models (LDMs) exhibit an impressive ability to produce realistic images, yet the inner workings of these models remain mysterious. Even when trained purely on images without explicit depth information, they typically output coherent pictures of 3D scenes. In this work, we investigate a basic interpretability question: does an LDM create and use an internal representation of simple scene geometry? Using linear probes, we find evidence that the internal activations of the LDM enco",
    "arxiv_url": "https://arxiv.org/abs/2306.05720v2",
    "pdf_url": "https://arxiv.org/pdf/2306.05720v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2306.05720",
    "arxiv_authors": [
      "Yida Chen",
      "Fernanda Vi√©gas",
      "Martin Wattenberg"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Beyond+Surface+Statistics%3A+Scene+Representations+in+a+Latent+Diffusion+Model+Yida+Chen+Fernanda+Vi%C3%A9gas+Martin+Wattenberg",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "Y Chen",
        "id": "PptmLrYAAAAJ"
      },
      {
        "name": "F Vi√©gas",
        "id": "GvXDNsYAAAAJ"
      },
      {
        "name": "M Wattenberg -",
        "id": null
      }
    ],
    "citation_count": 27
  },
  {
    "arxiv_id": "2404.14322",
    "title": "A Novel Approach to Chest X-ray Lung Segmentation Using U-net and Modified Convolutional Block Attention Module",
    "year": 2024,
    "published": "2024-04-22T16:33:06Z",
    "categories": [
      "eess.IV",
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "Lung segmentation in chest X-ray images is of paramount importance as it plays a crucial role in the diagnosis and treatment of various lung diseases. This paper presents a novel approach for lung segmentation in chest X-ray images by integrating U-net with attention mechanisms. The proposed method enhances the U-net architecture by incorporating a Convolutional Block Attention Module (CBAM), which unifies three distinct attention mechanisms: channel attention, spatial attention, and pixel atten",
    "arxiv_url": "https://arxiv.org/abs/2404.14322v2",
    "pdf_url": "https://arxiv.org/pdf/2404.14322v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2404.14322",
    "arxiv_authors": [
      "Mohammad Ali Labbaf Khaniki",
      "Mohammad Manthouri"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=A+Novel+Approach+to+Chest+X-ray+Lung+Segmentation+Using+U-net+and+Modified+Convolutional+Block+Attention+Module+Mohammad+Ali+Labbaf+Khaniki+Mohammad+Manthouri",
    "gs_search_success": false,
    "gs_authors": null,
    "citation_count": null,
    "error_type": "multiple_matches"
  },
  {
    "arxiv_id": "2312.08558",
    "title": "Leveraging Driver Field-of-View for Multimodal Ego-Trajectory Prediction",
    "year": 2023,
    "published": "2023-12-13T23:06:30Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Understanding drivers' decision-making is crucial for road safety. Although predicting the ego-vehicle's path is valuable for driver-assistance systems, existing methods mainly focus on external factors like other vehicles' motions, often neglecting the driver's attention and intent. To address this gap, we infer the ego-trajectory by integrating the driver's gaze and the surrounding scene. We introduce RouteFormer, a novel multimodal ego-trajectory prediction network combining GPS data, environ",
    "arxiv_url": "https://arxiv.org/abs/2312.08558v2",
    "pdf_url": "https://arxiv.org/pdf/2312.08558v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2312.08558",
    "arxiv_authors": [
      "M. Eren Akbiyik",
      "Nedko Savov",
      "Danda Pani Paudel",
      "Nikola Popovic",
      "Christian Vater",
      "Otmar Hilliges",
      "Luc Van Gool",
      "Xi Wang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Leveraging+Driver+Field-of-View+for+Multimodal+Ego-Trajectory+Prediction+M.+Eren+Akbiyik+Nedko+Savov+Danda+Pani+Paudel+Nikola+Popovic+Christian+Vater",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "ME Akbiyik",
        "id": "pZ7AXAcAAAAJ"
      },
      {
        "name": "N Savov",
        "id": "6zIcUgkAAAAJ"
      },
      {
        "name": "DP Paudel",
        "id": "W43pvPkAAAAJ"
      },
      {
        "name": "N Popovic",
        "id": "aY2lypgAAAAJ"
      }
    ],
    "citation_count": null
  },
  {
    "arxiv_id": "2412.14272",
    "title": "Split Learning in Computer Vision for Semantic Segmentation Delay Minimization",
    "year": 2024,
    "published": "2024-12-18T19:07:25Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.DC",
      "cs.IT",
      "cs.LG"
    ],
    "abstract": "In this paper, we propose a novel approach to minimize the inference delay in semantic segmentation using split learning (SL), tailored to the needs of real-time computer vision (CV) applications for resource-constrained devices. Semantic segmentation is essential for applications such as autonomous vehicles and smart city infrastructure, but faces significant latency challenges due to high computational and communication loads. Traditional centralized processing methods are inefficient for such",
    "arxiv_url": "https://arxiv.org/abs/2412.14272v1",
    "pdf_url": "https://arxiv.org/pdf/2412.14272v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2412.14272",
    "arxiv_authors": [
      "Nikos G. Evgenidis",
      "Nikos A. Mitsiou",
      "Sotiris A. Tegos",
      "Panagiotis D. Diamantoulakis",
      "George K. Karagiannidis"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Split+Learning+in+Computer+Vision+for+Semantic+Segmentation+Delay+Minimization+Nikos+G.+Evgenidis+Nikos+A.+Mitsiou+Sotiris+A.+Tegos+Panagiotis+D.+Diamantoulakis+George+K.+Karagiannidis",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "NG Evgenidis",
        "id": "WYLfFosAAAAJ"
      },
      {
        "name": "NA Mitsiou",
        "id": "q2q8dGMAAAAJ"
      },
      {
        "name": "SA Tegos",
        "id": "KOUdxz0AAAAJ"
      },
      {
        "name": "PD Diamantoulakis",
        "id": "LiKpLi8AAAAJ"
      },
      {
        "name": "GK KaragiannidisIEEE",
        "id": null
      }
    ],
    "citation_count": null
  },
  {
    "arxiv_id": "2411.10504",
    "title": "USP-Gaussian: Unifying Spike-based Image Reconstruction, Pose Correction and Gaussian Splatting",
    "year": 2024,
    "published": "2024-11-15T14:15:16Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Spike cameras, as an innovative neuromorphic camera that captures scenes with the 0-1 bit stream at 40 kHz, are increasingly employed for the 3D reconstruction task via Neural Radiance Fields (NeRF) or 3D Gaussian Splatting (3DGS). Previous spike-based 3D reconstruction approaches often employ a casecased pipeline: starting with high-quality image reconstruction from spike streams based on established spike-to-image reconstruction algorithms, then progressing to camera pose estimation and 3D rec",
    "arxiv_url": "https://arxiv.org/abs/2411.10504v2",
    "pdf_url": "https://arxiv.org/pdf/2411.10504v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2411.10504",
    "arxiv_authors": [
      "Kang Chen",
      "Jiyuan Zhang",
      "Zecheng Hao",
      "Yajing Zheng",
      "Tiejun Huang",
      "Zhaofei Yu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=USP-Gaussian%3A+Unifying+Spike-based+Image+Reconstruction%2C+Pose+Correction+and+Gaussian+Splatting+Kang+Chen+Jiyuan+Zhang+Zecheng+Hao+Yajing+Zheng+Tiejun+Huang",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "K Chen",
        "id": "F5feBP4AAAAJ"
      },
      {
        "name": "J Zhang",
        "id": "ukHrw0IAAAAJ"
      },
      {
        "name": "Z Hao",
        "id": "txTkX7YAAAAJ"
      },
      {
        "name": "Y Zheng",
        "id": "_bUM0NcAAAAJ"
      },
      {
        "name": "T Huang",
        "id": "knvEK4AAAAAJ"
      },
      {
        "name": "Z Yu",
        "id": "qaUgD50AAAAJ"
      }
    ],
    "citation_count": 4
  },
  {
    "arxiv_id": "2502.02118",
    "title": "BRIDLE: Generalized Self-supervised Learning with Quantization",
    "year": 2025,
    "published": "2025-02-04T08:54:06Z",
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "abstract": "Self-supervised learning has been a powerful approach for learning meaningful representations from unlabeled data across various domains, reducing the reliance on large labeled datasets. Inspired by BERT's success in capturing deep bidirectional contexts in natural language processing, similar frameworks have been adapted to other modalities such as audio, with models like BEATs extending the bidirectional training paradigm to audio signals using vector quantization (VQ). However, these framewor",
    "arxiv_url": "https://arxiv.org/abs/2502.02118v1",
    "pdf_url": "https://arxiv.org/pdf/2502.02118v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2502.02118",
    "arxiv_authors": [
      "Hoang M. Nguyen",
      "Satya N. Shukla",
      "Qiang Zhang",
      "Hanchao Yu",
      "Sreya D. Roy",
      "Taipeng Tian",
      "Lingjiong Zhu",
      "Yuchen Liu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=BRIDLE%3A+Generalized+Self-supervised+Learning+with+Quantization+Hoang+M.+Nguyen+Satya+N.+Shukla+Qiang+Zhang+Hanchao+Yu+Sreya+D.+Roy",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "HM Nguyen",
        "id": "rdA18PMAAAAJ"
      },
      {
        "name": "SN Shukla",
        "id": "l1tsmesAAAAJ"
      },
      {
        "name": "Q Zhang",
        "id": null
      },
      {
        "name": "H Yu",
        "id": "vBkncqgAAAAJ"
      },
      {
        "name": "SD Roy",
        "id": "ahFtDysAAAAJ"
      },
      {
        "name": "T Tian",
        "id": "1MceCxsAAAAJ"
      },
      {
        "name": "L Zhu",
        "id": "Z9JkFaoAAAAJ"
      },
      {
        "name": "Y Liu",
        "id": "7RJtIWYAAAAJ"
      }
    ],
    "citation_count": null
  },
  {
    "arxiv_id": "2301.13838",
    "title": "Image Shortcut Squeezing: Countering Perturbative Availability Poisons with Compression",
    "year": 2023,
    "published": "2023-01-31T18:31:20Z",
    "categories": [
      "cs.CR",
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "Perturbative availability poisons (PAPs) add small changes to images to prevent their use for model training. Current research adopts the belief that practical and effective approaches to countering PAPs do not exist. In this paper, we argue that it is time to abandon this belief. We present extensive experiments showing that 12 state-of-the-art PAP methods are vulnerable to Image Shortcut Squeezing (ISS), which is based on simple compression. For example, on average, ISS restores the CIFAR-10 m",
    "arxiv_url": "https://arxiv.org/abs/2301.13838v2",
    "pdf_url": "https://arxiv.org/pdf/2301.13838v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2301.13838",
    "arxiv_authors": [
      "Zhuoran Liu",
      "Zhengyu Zhao",
      "Martha Larson"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Image+Shortcut+Squeezing%3A+Countering+Perturbative+Availability+Poisons+with+Compression+Zhuoran+Liu+Zhengyu+Zhao+Martha+Larson",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "Z Liu",
        "id": "9U3aIWsAAAAJ"
      },
      {
        "name": "Z Zhao",
        "id": "pC8KpPMAAAAJ"
      },
      {
        "name": "M Larson - International",
        "id": null
      }
    ],
    "citation_count": 48
  },
  {
    "arxiv_id": "2408.05713",
    "title": "SSL: A Self-similarity Loss for Improving Generative Image Super-resolution",
    "year": 2024,
    "published": "2024-08-11T07:46:06Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Generative adversarial networks (GAN) and generative diffusion models (DM) have been widely used in real-world image super-resolution (Real-ISR) to enhance the image perceptual quality. However, these generative models are prone to generating visual artifacts and false image structures, resulting in unnatural Real-ISR results. Based on the fact that natural images exhibit high self-similarities, i.e., a local patch can have many similar patches to it in the whole image, in this work we propose a",
    "arxiv_url": "https://arxiv.org/abs/2408.05713v2",
    "pdf_url": "https://arxiv.org/pdf/2408.05713v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2408.05713",
    "arxiv_authors": [
      "Du Chen",
      "Zhengqiang Zhang",
      "Jie Liang",
      "Lei Zhang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=SSL%3A+A+Self-similarity+Loss+for+Improving+Generative+Image+Super-resolution+Du+Chen+Zhengqiang+Zhang+Jie+Liang+Lei+Zhang",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "D Chen",
        "id": "mmtNSisAAAAJ"
      },
      {
        "name": "Z Zhang",
        "id": "UX26wSMAAAAJ"
      },
      {
        "name": "J Liang",
        "id": "REWxLZsAAAAJ"
      },
      {
        "name": "L Zhang -",
        "id": null
      }
    ],
    "citation_count": 4
  },
  {
    "arxiv_id": "2406.08960",
    "title": "AirPlanes: Accurate Plane Estimation via 3D-Consistent Embeddings",
    "year": 2024,
    "published": "2024-06-13T09:49:31Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Extracting planes from a 3D scene is useful for downstream tasks in robotics and augmented reality. In this paper we tackle the problem of estimating the planar surfaces in a scene from posed images. Our first finding is that a surprisingly competitive baseline results from combining popular clustering algorithms with recent improvements in 3D geometry estimation. However, such purely geometric methods are understandably oblivious to plane semantics, which are crucial to discerning distinct plan",
    "arxiv_url": "https://arxiv.org/abs/2406.08960v1",
    "pdf_url": "https://arxiv.org/pdf/2406.08960v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2406.08960",
    "arxiv_authors": [
      "Jamie Watson",
      "Filippo Aleotti",
      "Mohamed Sayed",
      "Zawar Qureshi",
      "Oisin Mac Aodha",
      "Gabriel Brostow",
      "Michael Firman",
      "Sara Vicente"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=AirPlanes%3A+Accurate+Plane+Estimation+via+3D-Consistent+Embeddings+Jamie+Watson+Filippo+Aleotti+Mohamed+Sayed+Zawar+Qureshi+Oisin+Mac+Aodha",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "J Watson",
        "id": null
      },
      {
        "name": "F Aleotti",
        "id": "XN0qbX8AAAAJ"
      },
      {
        "name": "MAA Sayed",
        "id": "XkQp64MAAAAJ"
      },
      {
        "name": "ZI Qureshi",
        "id": "P2jZC8YAAAAJ"
      }
    ],
    "citation_count": 2
  },
  {
    "arxiv_id": "2502.15907",
    "title": "Graph Attention Convolutional U-NET: A Semantic Segmentation Model for Identifying Flooded Areas",
    "year": 2025,
    "published": "2025-02-21T19:50:13Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "The increasing impact of human-induced climate change and unplanned urban constructions has increased flooding incidents in recent years. Accurate identification of flooded areas is crucial for effective disaster management and urban planning. While few works have utilized convolutional neural networks and transformer-based semantic segmentation techniques for identifying flooded areas from aerial footage, recent developments in graph neural networks have created improvement opportunities. This ",
    "arxiv_url": "https://arxiv.org/abs/2502.15907v1",
    "pdf_url": "https://arxiv.org/pdf/2502.15907v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2502.15907",
    "arxiv_authors": [
      "Muhammad Umair Danish",
      "Madhushan Buwaneswaran",
      "Tehara Fonseka",
      "Katarina Grolinger"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Graph+Attention+Convolutional+U-NET%3A+A+Semantic+Segmentation+Model+for+Identifying+Flooded+Areas+Muhammad+Umair+Danish+Madhushan+Buwaneswaran+Tehara+Fonseka+Katarina+Grolinger",
    "gs_search_success": false,
    "gs_authors": null,
    "citation_count": null,
    "error_type": "multiple_matches"
  },
  {
    "arxiv_id": "2305.18367",
    "title": "Using VGG16 Algorithms for classification of lung cancer in CT scans Image",
    "year": 2023,
    "published": "2023-05-27T18:50:12Z",
    "categories": [
      "eess.IV",
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "Lung cancer is the leading reason behind cancer-related deaths within the world. Early detection of lung nodules is vital for increasing the survival rate of cancer patients. Traditionally, physicians should manually identify the world suspected of getting carcinoma. When developing these detection systems, the arbitrariness of lung nodules' shape, size, and texture could be a challenge. Many studies showed the applied of computer vision algorithms to accurate diagnosis and classification of lun",
    "arxiv_url": "https://arxiv.org/abs/2305.18367v1",
    "pdf_url": "https://arxiv.org/pdf/2305.18367v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2305.18367",
    "arxiv_authors": [
      "Hasan Hejbari Zargar",
      "Saha Hejbari Zargar",
      "Raziye Mehri",
      "Farzane Tajidini"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Using+VGG16+Algorithms+for+classification+of+lung+cancer+in+CT+scans+Image+Hasan+Hejbari+Zargar+Saha+Hejbari+Zargar+Raziye+Mehri+Farzane+Tajidini",
    "gs_search_success": false,
    "gs_authors": null,
    "citation_count": null,
    "error_type": "multiple_matches"
  },
  {
    "arxiv_id": "2502.01157",
    "title": "Radiant Foam: Real-Time Differentiable Ray Tracing",
    "year": 2025,
    "published": "2025-02-03T08:49:57Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Research on differentiable scene representations is consistently moving towards more efficient, real-time models. Recently, this has led to the popularization of splatting methods, which eschew the traditional ray-based rendering of radiance fields in favor of rasterization. This has yielded a significant improvement in rendering speeds due to the efficiency of rasterization algorithms and hardware, but has come at a cost: the approximations that make rasterization efficient also make implementa",
    "arxiv_url": "https://arxiv.org/abs/2502.01157v1",
    "pdf_url": "https://arxiv.org/pdf/2502.01157v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2502.01157",
    "arxiv_authors": [
      "Shrisudhan Govindarajan",
      "Daniel Rebain",
      "Kwang Moo Yi",
      "Andrea Tagliasacchi"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Radiant+Foam%3A+Real-Time+Differentiable+Ray+Tracing+Shrisudhan+Govindarajan+Daniel+Rebain+Kwang+Moo+Yi+Andrea+Tagliasacchi",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "S Govindarajan",
        "id": "6JS1yc0AAAAJ"
      },
      {
        "name": "D Rebain",
        "id": "h-qFKrQAAAAJ"
      },
      {
        "name": "KM Yi",
        "id": "pr6rIJEAAAAJ"
      },
      {
        "name": "A Tagliasacchi",
        "id": "1RmD-YsAAAAJ"
      }
    ],
    "citation_count": 13
  },
  {
    "arxiv_id": "2304.04429",
    "title": "BerDiff: Conditional Bernoulli Diffusion Model for Medical Image Segmentation",
    "year": 2023,
    "published": "2023-04-10T07:21:38Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Medical image segmentation is a challenging task with inherent ambiguity and high uncertainty, attributed to factors such as unclear tumor boundaries and multiple plausible annotations. The accuracy and diversity of segmentation masks are both crucial for providing valuable references to radiologists in clinical practice. While existing diffusion models have shown strong capacities in various visual generation tasks, it is still challenging to deal with discrete masks in segmentation. To achieve",
    "arxiv_url": "https://arxiv.org/abs/2304.04429v1",
    "pdf_url": "https://arxiv.org/pdf/2304.04429v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2304.04429",
    "arxiv_authors": [
      "Tao Chen",
      "Chenhui Wang",
      "Hongming Shan"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=BerDiff%3A+Conditional+Bernoulli+Diffusion+Model+for+Medical+Image+Segmentation+Tao+Chen+Chenhui+Wang+Hongming+Shan",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "T Chen",
        "id": "Rq-YLV4AAAAJ"
      },
      {
        "name": "C Wang",
        "id": "GlGrGBAAAAAJ"
      },
      {
        "name": "H Shan -",
        "id": null
      }
    ],
    "citation_count": 78
  },
  {
    "arxiv_id": "2501.06027",
    "title": "Geometric-Based Nail Segmentation for Clinical Measurements",
    "year": 2025,
    "published": "2025-01-10T15:04:23Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "A robust segmentation method that can be used to perform measurements on toenails is presented. The proposed method is used as the first step in a clinical trial to objectively quantify the incidence of a particular pathology. For such an assessment, it is necessary to distinguish a nail, which locally appears to be similar to the skin. Many algorithms have been used, each of which leverages different aspects of toenail appearance. We used the Hough transform to locate the tip of the toe and est",
    "arxiv_url": "https://arxiv.org/abs/2501.06027v1",
    "pdf_url": "https://arxiv.org/pdf/2501.06027v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2501.06027",
    "arxiv_authors": [
      "Bernat Galm√©s",
      "Gabriel Moy√†-Alcover",
      "Pedro Bibiloni",
      "Javier Varona",
      "Antoni Jaume-i-Cap√≥"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Geometric-Based+Nail+Segmentation+for+Clinical+Measurements+Bernat+Galm%C3%A9s+Gabriel+Moy%C3%A0-Alcover+Pedro+Bibiloni+Javier+Varona+Antoni+Jaume-i-Cap%C3%B3",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "B Galmes",
        "id": null
      },
      {
        "name": "G Moya-Alcover",
        "id": "aZa2ejUAAAAJ"
      },
      {
        "name": "P Bibiloni",
        "id": "St8ZpRgAAAAJ"
      },
      {
        "name": "J Varona",
        "id": "JVgrWbkAAAAJ"
      },
      {
        "name": "A Jaume-i-CapoMultimedia Tools and Applications",
        "id": null
      }
    ],
    "citation_count": 3
  },
  {
    "arxiv_id": "2312.08644",
    "title": "Generative Model-based Feature Knowledge Distillation for Action Recognition",
    "year": 2023,
    "published": "2023-12-14T03:55:29Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Knowledge distillation (KD), a technique widely employed in computer vision, has emerged as a de facto standard for improving the performance of small neural networks. However, prevailing KD-based approaches in video tasks primarily focus on designing loss functions and fusing cross-modal information. This overlooks the spatial-temporal feature semantics, resulting in limited advancements in model compression. Addressing this gap, our paper introduces an innovative knowledge distillation framewo",
    "arxiv_url": "https://arxiv.org/abs/2312.08644v1",
    "pdf_url": "https://arxiv.org/pdf/2312.08644v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2312.08644",
    "arxiv_authors": [
      "Guiqin Wang",
      "Peng Zhao",
      "Yanjiang Shi",
      "Cong Zhao",
      "Shusen Yang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Generative+Model-based+Feature+Knowledge+Distillation+for+Action+Recognition+Guiqin+Wang+Peng+Zhao+Yanjiang+Shi+Cong+Zhao+Shusen+Yang",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "G Wang",
        "id": "JEnbksYAAAAJ"
      },
      {
        "name": "P Zhao",
        "id": null
      },
      {
        "name": "Y Shi",
        "id": null
      },
      {
        "name": "C Zhao",
        "id": "KkmFWroAAAAJ"
      },
      {
        "name": "S Yang -",
        "id": null
      }
    ],
    "citation_count": 11
  },
  {
    "arxiv_id": "2505.16384",
    "title": "MAGE: A Multi-task Architecture for Gaze Estimation with an Efficient Calibration Module",
    "year": 2025,
    "published": "2025-05-22T08:36:58Z",
    "categories": [
      "cs.CV",
      "cs.HC"
    ],
    "abstract": "Eye gaze can provide rich information on human psychological activities, and has garnered significant attention in the field of Human-Robot Interaction (HRI). However, existing gaze estimation methods merely predict either the gaze direction or the Point-of-Gaze (PoG) on the screen, failing to provide sufficient information for a comprehensive six Degree-of-Freedom (DoF) gaze analysis in 3D space. Moreover, the variations of eye shape and structure among individuals also impede the generalizatio",
    "arxiv_url": "https://arxiv.org/abs/2505.16384v1",
    "pdf_url": "https://arxiv.org/pdf/2505.16384v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2505.16384",
    "arxiv_authors": [
      "Haoming Huang",
      "Musen Zhang",
      "Jianxin Yang",
      "Zhen Li",
      "Jinkai Li",
      "Yao Guo"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=MAGE%3A+A+Multi-task+Architecture+for+Gaze+Estimation+with+an+Efficient+Calibration+Module+Haoming+Huang+Musen+Zhang+Jianxin+Yang+Zhen+Li+Jinkai+Li",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "H Huang",
        "id": null
      },
      {
        "name": "M Zhang",
        "id": null
      },
      {
        "name": "J Yang",
        "id": null
      },
      {
        "name": "Z Li",
        "id": null
      },
      {
        "name": "J Li",
        "id": "3rayCDAAAAAJ"
      },
      {
        "name": "Y Guo -",
        "id": null
      }
    ],
    "citation_count": null
  },
  {
    "arxiv_id": "2404.12385",
    "title": "MeshLRM: Large Reconstruction Model for High-Quality Meshes",
    "year": 2024,
    "published": "2024-04-18T17:59:41Z",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "abstract": "We propose MeshLRM, a novel LRM-based approach that can reconstruct a high-quality mesh from merely four input images in less than one second. Different from previous large reconstruction models (LRMs) that focus on NeRF-based reconstruction, MeshLRM incorporates differentiable mesh extraction and rendering within the LRM framework. This allows for end-to-end mesh reconstruction by fine-tuning a pre-trained NeRF LRM with mesh rendering. Moreover, we improve the LRM architecture by simplifying se",
    "arxiv_url": "https://arxiv.org/abs/2404.12385v2",
    "pdf_url": "https://arxiv.org/pdf/2404.12385v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2404.12385",
    "arxiv_authors": [
      "Xinyue Wei",
      "Kai Zhang",
      "Sai Bi",
      "Hao Tan",
      "Fujun Luan",
      "Valentin Deschaintre",
      "Kalyan Sunkavalli",
      "Hao Su",
      "Zexiang Xu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=MeshLRM%3A+Large+Reconstruction+Model+for+High-Quality+Meshes+Xinyue+Wei+Kai+Zhang+Sai+Bi+Hao+Tan+Fujun+Luan",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "X Wei",
        "id": "UipTKqoAAAAJ"
      },
      {
        "name": "K Zhang",
        "id": "eVv0MrsAAAAJ"
      },
      {
        "name": "S Bi",
        "id": "-q4nE1kAAAAJ"
      },
      {
        "name": "H Tan",
        "id": "OV1Y3FUAAAAJ"
      },
      {
        "name": "F Luan",
        "id": "NLxrmYQAAAAJ"
      },
      {
        "name": "V Deschaintre",
        "id": "UnO0Ap8AAAAJ"
      },
      {
        "name": "K Sunkavalli",
        "id": "j7uL6VEAAAAJ"
      },
      {
        "name": "H Su",
        "id": "1P8Zu04AAAAJ"
      },
      {
        "name": "Z Xu",
        "id": "_RRIYvEAAAAJ"
      }
    ],
    "citation_count": 112
  },
  {
    "arxiv_id": "2501.07163",
    "title": "Adaptive Noise-Tolerant Network for Image Segmentation",
    "year": 2025,
    "published": "2025-01-13T09:49:34Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Unlike image classification and annotation, for which deep network models have achieved dominating superior performances compared to traditional computer vision algorithms, deep learning for automatic image segmentation still faces critical challenges. One of such hurdles is to obtain ground-truth segmentations as the training labels for deep network training. Especially when we study biomedical images, such as histopathological images (histo-images), it is unrealistic to ask for manual segmenta",
    "arxiv_url": "https://arxiv.org/abs/2501.07163v2",
    "pdf_url": "https://arxiv.org/pdf/2501.07163v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2501.07163",
    "arxiv_authors": [
      "Weizhi Li"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Adaptive+Noise-Tolerant+Network+for+Image+Segmentation+Weizhi+Li",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "W Li -",
        "id": null
      }
    ],
    "citation_count": null
  },
  {
    "arxiv_id": "2410.02800",
    "title": "Estimating Body Volume and Height Using 3D Data",
    "year": 2024,
    "published": "2024-09-18T16:20:46Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Accurate body weight estimation is critical in emergency medicine for proper dosing of weight-based medications, yet direct measurement is often impractical in urgent situations. This paper presents a non-invasive method for estimating body weight by calculating total body volume and height using 3D imaging technology. A RealSense D415 camera is employed to capture high-resolution depth maps of the patient, from which 3D models are generated. The Convex Hull Algorithm is then applied to calculat",
    "arxiv_url": "https://arxiv.org/abs/2410.02800v1",
    "pdf_url": "https://arxiv.org/pdf/2410.02800v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2410.02800",
    "arxiv_authors": [
      "Vivek Ganesh Sonar",
      "Muhammad Tanveer Jan",
      "Mike Wells",
      "Abhijit Pandya",
      "Gabriela Engstrom",
      "Richard Shih",
      "Borko Furht"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Estimating+Body+Volume+and+Height+Using+3D+Data+Vivek+Ganesh+Sonar+Muhammad+Tanveer+Jan+Mike+Wells+Abhijit+Pandya+Gabriela+Engstrom",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "VG Sonar",
        "id": null
      },
      {
        "name": "MT Jan",
        "id": "OYpxY2MAAAAJ"
      },
      {
        "name": "M Wells",
        "id": "RilE8XAAAAAJ"
      },
      {
        "name": "A Pandya",
        "id": "MNcOWcsAAAAJ"
      },
      {
        "name": "G Engstrom",
        "id": null
      },
      {
        "name": "R Shih",
        "id": "wv6hb0wAAAAJ"
      },
      {
        "name": "B Furht",
        "id": "z1BEGM8AAAAJ"
      }
    ],
    "citation_count": 3
  },
  {
    "arxiv_id": "2408.15276",
    "title": "A Survey of Deep Learning for Group-level Emotion Recognition",
    "year": 2024,
    "published": "2024-08-13T11:54:09Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "abstract": "With the advancement of artificial intelligence (AI) technology, group-level emotion recognition (GER) has emerged as an important area in analyzing human behavior. Early GER methods are primarily relied on handcrafted features. However, with the proliferation of Deep Learning (DL) techniques and their remarkable success in diverse tasks, neural networks have garnered increasing interest in GER. Unlike individual's emotion, group emotions exhibit diversity and dynamics. Presently, several DL app",
    "arxiv_url": "https://arxiv.org/abs/2408.15276v1",
    "pdf_url": "https://arxiv.org/pdf/2408.15276v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2408.15276",
    "arxiv_authors": [
      "Xiaohua Huang",
      "Jinke Xu",
      "Wenming Zheng",
      "Qirong Mao",
      "Abhinav Dhall"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=A+Survey+of+Deep+Learning+for+Group-level+Emotion+Recognition+Xiaohua+Huang+Jinke+Xu+Wenming+Zheng+Qirong+Mao+Abhinav+Dhall",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "X Huang",
        "id": "k2xp2doAAAAJ"
      },
      {
        "name": "J Xu",
        "id": null
      },
      {
        "name": "W Zheng",
        "id": "k5fqIogAAAAJ"
      },
      {
        "name": "Q Mao",
        "id": "ArChR1AAAAAJ"
      },
      {
        "name": "A Dhall -",
        "id": null
      }
    ],
    "citation_count": 7
  },
  {
    "arxiv_id": "2303.12649",
    "title": "MI-SegNet: Mutual Information-Based US Segmentation for Unseen Domain Generalization",
    "year": 2023,
    "published": "2023-03-22T15:30:44Z",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "abstract": "Generalization capabilities of learning-based medical image segmentation across domains are currently limited by the performance degradation caused by the domain shift, particularly for ultrasound (US) imaging. The quality of US images heavily relies on carefully tuned acoustic parameters, which vary across sonographers, machines, and settings. To improve the generalizability on US images across domains, we propose MI-SegNet, a novel mutual information (MI) based framework to explicitly disentan",
    "arxiv_url": "https://arxiv.org/abs/2303.12649v3",
    "pdf_url": "https://arxiv.org/pdf/2303.12649v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2303.12649",
    "arxiv_authors": [
      "Yuan Bi",
      "Zhongliang Jiang",
      "Ricarda Clarenbach",
      "Reza Ghotbi",
      "Angelos Karlas",
      "Nassir Navab"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=MI-SegNet%3A+Mutual+Information-Based+US+Segmentation+for+Unseen+Domain+Generalization+Yuan+Bi+Zhongliang+Jiang+Ricarda+Clarenbach+Reza+Ghotbi+Angelos+Karlas",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "Y Bi",
        "id": "pbiNrskAAAAJ"
      },
      {
        "name": "Z Jiang",
        "id": "X41OzcYAAAAJ"
      },
      {
        "name": "R Clarenbach",
        "id": null
      },
      {
        "name": "R Ghotbi",
        "id": null
      },
      {
        "name": "A Karlas",
        "id": "_LVqCgQAAAAJ"
      },
      {
        "name": "N NavabInternational",
        "id": null
      }
    ],
    "citation_count": 31
  },
  {
    "arxiv_id": "2303.00971",
    "title": "Disentangling Orthogonal Planes for Indoor Panoramic Room Layout Estimation with Cross-Scale Distortion Awareness",
    "year": 2023,
    "published": "2023-03-02T05:10:23Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Based on the Manhattan World assumption, most existing indoor layout estimation schemes focus on recovering layouts from vertically compressed 1D sequences. However, the compression procedure confuses the semantics of different planes, yielding inferior performance with ambiguous interpretability.   To address this issue, we propose to disentangle this 1D representation by pre-segmenting orthogonal (vertical and horizontal) planes from a complex scene, explicitly capturing the geometric cues for",
    "arxiv_url": "https://arxiv.org/abs/2303.00971v2",
    "pdf_url": "https://arxiv.org/pdf/2303.00971v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2303.00971",
    "arxiv_authors": [
      "Zhijie Shen",
      "Zishuo Zheng",
      "Chunyu Lin",
      "Lang Nie",
      "Kang Liao",
      "Shuai Zheng",
      "Yao Zhao"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Disentangling+Orthogonal+Planes+for+Indoor+Panoramic+Room+Layout+Estimation+with+Cross-Scale+Distortion+Awareness+Zhijie+Shen+Zishuo+Zheng+Chunyu+Lin+Lang+Nie+Kang+Liao",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "Z Shen",
        "id": "FEgH1IEAAAAJ"
      },
      {
        "name": "Z Zheng",
        "id": null
      },
      {
        "name": "C Lin",
        "id": "t8xkhscAAAAJ"
      },
      {
        "name": "L Nie",
        "id": "vo__egkAAAAJ"
      },
      {
        "name": "K Liao",
        "id": "YmmfEBwAAAAJ"
      },
      {
        "name": "S Zheng",
        "id": "8UFwA_0AAAAJ"
      },
      {
        "name": "Y Zhao",
        "id": "eXXePDsAAAAJ"
      }
    ],
    "citation_count": 26
  },
  {
    "arxiv_id": "2503.04121",
    "title": "Simple Self Organizing Map with Visual Transformer",
    "year": 2025,
    "published": "2025-03-06T05:58:41Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "abstract": "Vision Transformers (ViTs) have demonstrated exceptional performance in various vision tasks. However, they tend to underperform on smaller datasets due to their inherent lack of inductive biases. Current approaches address this limitation implicitly-often by pairing ViTs with pretext tasks or by distilling knowledge from convolutional neural networks (CNNs) to strengthen the prior. In contrast, Self-Organizing Maps (SOMs), a widely adopted self-supervised framework, are inherently structured to",
    "arxiv_url": "https://arxiv.org/abs/2503.04121v1",
    "pdf_url": "https://arxiv.org/pdf/2503.04121v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2503.04121",
    "arxiv_authors": [
      "Alan Luo",
      "Kaiwen Yuan"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Simple+Self+Organizing+Map+with+Visual+Transformer+Alan+Luo+Kaiwen+Yuan",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "A Luo",
        "id": "k5ZQlRUAAAAJ"
      },
      {
        "name": "K Yuan -",
        "id": null
      }
    ],
    "citation_count": null
  },
  {
    "arxiv_id": "2505.15185",
    "title": "MonoSplat: Generalizable 3D Gaussian Splatting from Monocular Depth Foundation Models",
    "year": 2025,
    "published": "2025-05-21T07:03:16Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Recent advances in generalizable 3D Gaussian Splatting have demonstrated promising results in real-time high-fidelity rendering without per-scene optimization, yet existing approaches still struggle to handle unfamiliar visual content during inference on novel scenes due to limited generalizability. To address this challenge, we introduce MonoSplat, a novel framework that leverages rich visual priors from pre-trained monocular depth foundation models for robust Gaussian reconstruction. Our appro",
    "arxiv_url": "https://arxiv.org/abs/2505.15185v1",
    "pdf_url": "https://arxiv.org/pdf/2505.15185v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2505.15185",
    "arxiv_authors": [
      "Yifan Liu",
      "Keyu Fan",
      "Weihao Yu",
      "Chenxin Li",
      "Hao Lu",
      "Yixuan Yuan"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=MonoSplat%3A+Generalizable+3D+Gaussian+Splatting+from+Monocular+Depth+Foundation+Models+Yifan+Liu+Keyu+Fan+Weihao+Yu+Chenxin+Li+Hao+Lu",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "Y Liu",
        "id": "Uv7f-HMAAAAJ"
      },
      {
        "name": "K Fan",
        "id": "3xKPyXAAAAAJ"
      },
      {
        "name": "W Yu",
        "id": "fCzlLE4AAAAJ"
      },
      {
        "name": "C Li",
        "id": "yfptgYMAAAAJ"
      },
      {
        "name": "H Lu",
        "id": "OrbGCGkAAAAJ"
      },
      {
        "name": "Y Yuan",
        "id": "Aho5Jv8AAAAJ"
      }
    ],
    "citation_count": 3
  },
  {
    "arxiv_id": "2407.09537",
    "title": "ViPro: Enabling and Controlling Video Prediction for Complex Dynamical Scenarios using Procedural Knowledge",
    "year": 2024,
    "published": "2024-06-26T09:01:35Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "abstract": "We propose a novel architecture design for video prediction in order to utilize procedural domain knowledge directly as part of the computational graph of data-driven models. On the basis of new challenging scenarios we show that state-of-the-art video predictors struggle in complex dynamical settings, and highlight that the introduction of prior process knowledge makes their learning problem feasible. Our approach results in the learning of a symbolically addressable interface between data-driv",
    "arxiv_url": "https://arxiv.org/abs/2407.09537v1",
    "pdf_url": "https://arxiv.org/pdf/2407.09537v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2407.09537",
    "arxiv_authors": [
      "Patrick Takenaka",
      "Johannes Maucher",
      "Marco F. Huber"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=ViPro%3A+Enabling+and+Controlling+Video+Prediction+for+Complex+Dynamical+Scenarios+using+Procedural+Knowledge+Patrick+Takenaka+Johannes+Maucher+Marco+F.+Huber",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "P Takenaka",
        "id": "27Vt8asAAAAJ"
      },
      {
        "name": "J Maucher",
        "id": "gbvBHtgAAAAJ"
      },
      {
        "name": "MF Huber - International",
        "id": null
      }
    ],
    "citation_count": 1
  },
  {
    "arxiv_id": "2503.08683",
    "title": "CoLMDriver: LLM-based Negotiation Benefits Cooperative Autonomous Driving",
    "year": 2025,
    "published": "2025-03-11T17:58:42Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.MA"
    ],
    "abstract": "Vehicle-to-vehicle (V2V) cooperative autonomous driving holds great promise for improving safety by addressing the perception and prediction uncertainties inherent in single-agent systems. However, traditional cooperative methods are constrained by rigid collaboration protocols and limited generalization to unseen interactive scenarios. While LLM-based approaches offer generalized reasoning capabilities, their challenges in spatial planning and unstable inference latency hinder their direct appl",
    "arxiv_url": "https://arxiv.org/abs/2503.08683v1",
    "pdf_url": "https://arxiv.org/pdf/2503.08683v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2503.08683",
    "arxiv_authors": [
      "Changxing Liu",
      "Genjia Liu",
      "Zijun Wang",
      "Jinchang Yang",
      "Siheng Chen"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=CoLMDriver%3A+LLM-based+Negotiation+Benefits+Cooperative+Autonomous+Driving+Changxing+Liu+Genjia+Liu+Zijun+Wang+Jinchang+Yang+Siheng+Chen",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "C Liu",
        "id": "MpUEflgAAAAJ"
      },
      {
        "name": "G Liu",
        "id": "U20-KgoAAAAJ"
      },
      {
        "name": "Z Wang",
        "id": null
      },
      {
        "name": "J Yang",
        "id": "Gc8veGYAAAAJ"
      },
      {
        "name": "S Chen -",
        "id": null
      }
    ],
    "citation_count": 5
  },
  {
    "arxiv_id": "2308.14324",
    "title": "CPFES: Physical Fitness Evaluation Based on Canadian Agility and Movement Skill Assessment",
    "year": 2023,
    "published": "2023-08-28T06:09:25Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "In recent years, the assessment of fundamental movement skills integrated with physical education has focused on both teaching practice and the feasibility of assessment. The object of assessment has shifted from multiple ages to subdivided ages, while the content of assessment has changed from complex and time-consuming to concise and efficient. Therefore, we apply deep learning to physical fitness evaluation, we propose a system based on the Canadian Agility and Movement Skill Assessment (CAMS",
    "arxiv_url": "https://arxiv.org/abs/2308.14324v1",
    "pdf_url": "https://arxiv.org/pdf/2308.14324v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2308.14324",
    "arxiv_authors": [
      "Pengcheng Dong",
      "Xiaojin Mao",
      "Lixia Fan",
      "Wenbo Wan",
      "Jiande Sun"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=CPFES%3A+Physical+Fitness+Evaluation+Based+on+Canadian+Agility+and+Movement+Skill+Assessment+Pengcheng+Dong+Xiaojin+Mao+Lixia+Fan+Wenbo+Wan+Jiande+Sun",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "P Dong",
        "id": "dG6JiRgAAAAJ"
      },
      {
        "name": "X Mao",
        "id": null
      },
      {
        "name": "L Fan",
        "id": null
      },
      {
        "name": "W Wan",
        "id": "RbmDFdEAAAAJ"
      },
      {
        "name": "J Sun -",
        "id": null
      }
    ],
    "citation_count": 1
  },
  {
    "arxiv_id": "2307.09861",
    "title": "BSDM: Background Suppression Diffusion Model for Hyperspectral Anomaly Detection",
    "year": 2023,
    "published": "2023-07-19T09:45:06Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Hyperspectral anomaly detection (HAD) is widely used in Earth observation and deep space exploration. A major challenge for HAD is the complex background of the input hyperspectral images (HSIs), resulting in anomalies confused in the background. On the other hand, the lack of labeled samples for HSIs leads to poor generalization of existing HAD methods. This paper starts the first attempt to study a new and generalizable background learning problem without labeled samples. We present a novel so",
    "arxiv_url": "https://arxiv.org/abs/2307.09861v1",
    "pdf_url": "https://arxiv.org/pdf/2307.09861v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2307.09861",
    "arxiv_authors": [
      "Jitao Ma",
      "Weiying Xie",
      "Yunsong Li",
      "Leyuan Fang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=BSDM%3A+Background+Suppression+Diffusion+Model+for+Hyperspectral+Anomaly+Detection+Jitao+Ma+Weiying+Xie+Yunsong+Li+Leyuan+Fang",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "J Ma",
        "id": "p1fW6GcAAAAJ"
      },
      {
        "name": "W Xie",
        "id": "y0ha5lMAAAAJ"
      },
      {
        "name": "Y Shi",
        "id": null
      },
      {
        "name": "X Xiang",
        "id": null
      },
      {
        "name": "Y Li",
        "id": "aY_2RzkAAAAJ"
      },
      {
        "name": "L FangIEEE Transactions on Circuits and Systems for Video Technology",
        "id": null
      }
    ],
    "citation_count": 21
  },
  {
    "arxiv_id": "2505.10841",
    "title": "RefPose: Leveraging Reference Geometric Correspondences for Accurate 6D Pose Estimation of Unseen Objects",
    "year": 2025,
    "published": "2025-05-16T04:17:58Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Estimating the 6D pose of unseen objects from monocular RGB images remains a challenging problem, especially due to the lack of prior object-specific knowledge. To tackle this issue, we propose RefPose, an innovative approach to object pose estimation that leverages a reference image and geometric correspondence as guidance. RefPose first predicts an initial pose by using object templates to render the reference image and establish the geometric correspondence needed for the refinement stage. Du",
    "arxiv_url": "https://arxiv.org/abs/2505.10841v1",
    "pdf_url": "https://arxiv.org/pdf/2505.10841v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2505.10841",
    "arxiv_authors": [
      "Jaeguk Kim",
      "Jaewoo Park",
      "Keuntek Lee",
      "Nam Ik Cho"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=RefPose%3A+Leveraging+Reference+Geometric+Correspondences+for+Accurate+6D+Pose+Estimation+of+Unseen+Objects+Jaeguk+Kim+Jaewoo+Park+Keuntek+Lee+Nam+Ik+Cho",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "J Kim",
        "id": "MUPBqK4AAAAJ"
      },
      {
        "name": "J Park",
        "id": "9sUcU40AAAAJ"
      },
      {
        "name": "K Lee",
        "id": "s4Ct2eoAAAAJ"
      },
      {
        "name": "NI Cho -",
        "id": null
      }
    ],
    "citation_count": null
  },
  {
    "arxiv_id": "2306.13990",
    "title": "Cross-Validation Is All You Need: A Statistical Approach To Label Noise Estimation",
    "year": 2023,
    "published": "2023-06-24T14:50:20Z",
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "abstract": "Machine learning models experience deteriorated performance when trained in the presence of noisy labels. This is particularly problematic for medical tasks, such as survival prediction, which typically face high label noise complexity with few clear-cut solutions. Inspired by the large fluctuations across folds in the cross-validation performance of survival analyses, we design Monte-Carlo experiments to show that such fluctuation could be caused by label noise. We propose two novel and straigh",
    "arxiv_url": "https://arxiv.org/abs/2306.13990v2",
    "pdf_url": "https://arxiv.org/pdf/2306.13990v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2306.13990",
    "arxiv_authors": [
      "Jianan Chen",
      "Vishwesh Ramanathan",
      "Tony Xu",
      "Anne L. Martel"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Cross-Validation+Is+All+You+Need%3A+A+Statistical+Approach+To+Label+Noise+Estimation+Jianan+Chen+Vishwesh+Ramanathan+Tony+Xu+Anne+L.+Martel",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "J Chen",
        "id": "B9tp488AAAAJ"
      },
      {
        "name": "V Ramanathan",
        "id": "dSkS3EEAAAAJ"
      },
      {
        "name": "T Xu",
        "id": "3vwB4oEAAAAJ"
      },
      {
        "name": "AL Martel -",
        "id": null
      }
    ],
    "citation_count": 4
  },
  {
    "arxiv_id": "2312.00386",
    "title": "Local monotone operator learning using non-monotone operators: MnM-MOL",
    "year": 2023,
    "published": "2023-12-01T07:15:51Z",
    "categories": [
      "eess.IV",
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "The recovery of magnetic resonance (MR) images from undersampled measurements is a key problem that has seen extensive research in recent years. Unrolled approaches, which rely on end-to-end training of convolutional neural network (CNN) blocks within iterative reconstruction algorithms, offer state-of-the-art performance. These algorithms require a large amount of memory during training, making them difficult to employ in high-dimensional applications. Deep equilibrium (DEQ) models and the rece",
    "arxiv_url": "https://arxiv.org/abs/2312.00386v1",
    "pdf_url": "https://arxiv.org/pdf/2312.00386v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2312.00386",
    "arxiv_authors": [
      "Maneesh John",
      "Jyothi Rikhab Chand",
      "Mathews Jacob"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Local+monotone+operator+learning+using+non-monotone+operators%3A+MnM-MOL+Maneesh+John+Jyothi+Rikhab+Chand+Mathews+Jacob",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "M John",
        "id": "31QLHO0AAAAJ"
      },
      {
        "name": "JR Chand",
        "id": "1MrnwgwAAAAJ"
      },
      {
        "name": "M Jacob - IEEE transactions on",
        "id": null
      }
    ],
    "citation_count": 3
  },
  {
    "arxiv_id": "2305.11733",
    "title": "Long-tailed Visual Recognition via Gaussian Clouded Logit Adjustment",
    "year": 2023,
    "published": "2023-05-19T15:11:06Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Long-tailed data is still a big challenge for deep neural networks, even though they have achieved great success on balanced data. We observe that vanilla training on long-tailed data with cross-entropy loss makes the instance-rich head classes severely squeeze the spatial distribution of the tail classes, which leads to difficulty in classifying tail class samples. Furthermore, the original cross-entropy loss can only propagate gradient short-lively because the gradient in softmax form rapidly ",
    "arxiv_url": "https://arxiv.org/abs/2305.11733v1",
    "pdf_url": "https://arxiv.org/pdf/2305.11733v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2305.11733",
    "arxiv_authors": [
      "Mengke Li",
      "Yiu-ming Cheung",
      "Yang Lu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Long-tailed+Visual+Recognition+via+Gaussian+Clouded+Logit+Adjustment+Mengke+Li+Yiu-ming+Cheung+Yang+Lu",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "M Li",
        "id": "0N26QgMAAAAJ"
      },
      {
        "name": "Y Cheung",
        "id": "VM7FCvUAAAAJ"
      },
      {
        "name": "Y Lu -",
        "id": null
      }
    ],
    "citation_count": 183
  },
  {
    "arxiv_id": "2405.13202",
    "title": "Empowering Urban Traffic Management: Elevated 3D LiDAR for Data Collection and Advanced Object Detection Analysis",
    "year": 2024,
    "published": "2024-05-21T21:12:09Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "abstract": "The 3D object detection capabilities in urban environments have been enormously improved by recent developments in Light Detection and Range (LiDAR) technology. This paper presents a novel framework that transforms the detection and analysis of 3D objects in traffic scenarios by utilizing the power of elevated LiDAR sensors. We are presenting our methodology's remarkable capacity to collect complex 3D point cloud data, which allows us to accurately and in detail capture the dynamics of urban tra",
    "arxiv_url": "https://arxiv.org/abs/2405.13202v1",
    "pdf_url": "https://arxiv.org/pdf/2405.13202v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2405.13202",
    "arxiv_authors": [
      "Nawfal Guefrachi",
      "Hakim Ghazzai",
      "Ahmad Alsharoa"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Empowering+Urban+Traffic+Management%3A+Elevated+3D+LiDAR+for+Data+Collection+and+Advanced+Object+Detection+Analysis+Nawfal+Guefrachi+Hakim+Ghazzai+Ahmad+Alsharoa",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "N Guefrachi",
        "id": "qNGBEkMAAAAJ"
      },
      {
        "name": "H Ghazzai",
        "id": "UdzAFTIAAAAJ"
      },
      {
        "name": "A Alsharoa2024 IEEE 30th International",
        "id": null
      }
    ],
    "citation_count": 1
  },
  {
    "arxiv_id": "2402.05712",
    "title": "DiffSpeaker: Speech-Driven 3D Facial Animation with Diffusion Transformer",
    "year": 2024,
    "published": "2024-02-08T14:39:16Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Speech-driven 3D facial animation is important for many multimedia applications. Recent work has shown promise in using either Diffusion models or Transformer architectures for this task. However, their mere aggregation does not lead to improved performance. We suspect this is due to a shortage of paired audio-4D data, which is crucial for the Transformer to effectively perform as a denoiser within the Diffusion framework. To tackle this issue, we present DiffSpeaker, a Transformer-based network",
    "arxiv_url": "https://arxiv.org/abs/2402.05712v1",
    "pdf_url": "https://arxiv.org/pdf/2402.05712v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2402.05712",
    "arxiv_authors": [
      "Zhiyuan Ma",
      "Xiangyu Zhu",
      "Guojun Qi",
      "Chen Qian",
      "Zhaoxiang Zhang",
      "Zhen Lei"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=DiffSpeaker%3A+Speech-Driven+3D+Facial+Animation+with+Diffusion+Transformer+Zhiyuan+Ma+Xiangyu+Zhu+Guojun+Qi+Chen+Qian+Zhaoxiang+Zhang",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "Z Ma",
        "id": "F15mLDYAAAAJ"
      },
      {
        "name": "X Zhu",
        "id": "1rbNk5oAAAAJ"
      },
      {
        "name": "G Qi",
        "id": "Nut-uvoAAAAJ"
      },
      {
        "name": "C Qian",
        "id": "AerkT0YAAAAJ"
      },
      {
        "name": "Z Zhang",
        "id": "qxWfV6cAAAAJ"
      },
      {
        "name": "Z Lei -",
        "id": null
      }
    ],
    "citation_count": 30
  },
  {
    "arxiv_id": "2409.01761",
    "title": "PRoGS: Progressive Rendering of Gaussian Splats",
    "year": 2024,
    "published": "2024-09-03T10:15:30Z",
    "categories": [
      "cs.CV",
      "cs.MM"
    ],
    "abstract": "Over the past year, 3D Gaussian Splatting (3DGS) has received significant attention for its ability to represent 3D scenes in a perceptually accurate manner. However, it can require a substantial amount of storage since each splat's individual data must be stored. While compression techniques offer a potential solution by reducing the memory footprint, they still necessitate retrieving the entire scene before any part of it can be rendered. In this work, we introduce a novel approach for progres",
    "arxiv_url": "https://arxiv.org/abs/2409.01761v1",
    "pdf_url": "https://arxiv.org/pdf/2409.01761v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2409.01761",
    "arxiv_authors": [
      "Brent Zoomers",
      "Maarten Wijnants",
      "Ivan Molenaers",
      "Joni Vanherck",
      "Jeroen Put",
      "Lode Jorissen",
      "Nick Michiels"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=PRoGS%3A+Progressive+Rendering+of+Gaussian+Splats+Brent+Zoomers+Maarten+Wijnants+Ivan+Molenaers+Joni+Vanherck+Jeroen+Put",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "B Zoomers",
        "id": "OPatO_AAAAAJ"
      },
      {
        "name": "M Wijnants",
        "id": "2TP3YDkAAAAJ"
      },
      {
        "name": "I Molenaers",
        "id": null
      },
      {
        "name": "J Vanherck",
        "id": "z5LtNhIAAAAJ"
      },
      {
        "name": "J Put",
        "id": null
      },
      {
        "name": "N Michiels2025 IEEE/CVF Winter",
        "id": null
      }
    ],
    "citation_count": 4
  },
  {
    "arxiv_id": "2305.00221",
    "title": "Sensor Equivariance by LiDAR Projection Images",
    "year": 2023,
    "published": "2023-04-29T10:16:02Z",
    "categories": [
      "cs.CV",
      "eess.IV"
    ],
    "abstract": "In this work, we propose an extension of conventional image data by an additional channel in which the associated projection properties are encoded. This addresses the issue of sensor-dependent object representation in projection-based sensors, such as LiDAR, which can lead to distorted physical and geometric properties due to variations in sensor resolution and field of view. To that end, we propose an architecture for processing this data in an instance segmentation framework. We focus specifi",
    "arxiv_url": "https://arxiv.org/abs/2305.00221v1",
    "pdf_url": "https://arxiv.org/pdf/2305.00221v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2305.00221",
    "arxiv_authors": [
      "Hannes Reichert",
      "Manuel Hetzel",
      "Steven Schreck",
      "Konrad Doll",
      "Bernhard Sick"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Sensor+Equivariance+by+LiDAR+Projection+Images+Hannes+Reichert+Manuel+Hetzel+Steven+Schreck+Konrad+Doll+Bernhard+Sick",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "H Reichert",
        "id": "VCGlB18AAAAJ"
      },
      {
        "name": "M Hetzel",
        "id": null
      },
      {
        "name": "S Schreck",
        "id": "NVCseEEAAAAJ"
      },
      {
        "name": "K Doll",
        "id": null
      },
      {
        "name": "B Sick2023 IEEE Intelligent Vehicles",
        "id": null
      }
    ],
    "citation_count": 2
  },
  {
    "arxiv_id": "2412.07155",
    "title": "Annotation Techniques for Judo Combat Phase Classification from Tournament Footage",
    "year": 2024,
    "published": "2024-12-10T03:24:14Z",
    "categories": [
      "cs.CV",
      "cs.MM"
    ],
    "abstract": "This paper presents a semi-supervised approach to extracting and analyzing combat phases in judo tournaments using live-streamed footage. The objective is to automate the annotation and summarization of live streamed judo matches. We train models that extract relevant entities and classify combat phases from fixed-perspective judo recordings. We employ semi-supervised methods to address limited labeled data in the domain. We build a model of combat phases via transfer learning from a fine-tuned ",
    "arxiv_url": "https://arxiv.org/abs/2412.07155v1",
    "pdf_url": "https://arxiv.org/pdf/2412.07155v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2412.07155",
    "arxiv_authors": [
      "Anthony Miyaguchi",
      "Jed Moutahir",
      "Tanmay Sutar"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Annotation+Techniques+for+Judo+Combat+Phase+Classification+from+Tournament+Footage+Anthony+Miyaguchi+Jed+Moutahir+Tanmay+Sutar",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "A Miyaguchi",
        "id": "RcEnqoQAAAAJ"
      },
      {
        "name": "J Moutahir",
        "id": "7yhefQ4AAAAJ"
      },
      {
        "name": "T Sutar -",
        "id": null
      }
    ],
    "citation_count": null
  },
  {
    "arxiv_id": "2505.05501",
    "title": "Preliminary Explorations with GPT-4o(mni) Native Image Generation",
    "year": 2025,
    "published": "2025-05-06T19:35:29Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "eess.IV"
    ],
    "abstract": "Recently, the visual generation ability by GPT-4o(mni) has been unlocked by OpenAI. It demonstrates a very remarkable generation capability with excellent multimodal condition understanding and varied task instructions. In this paper, we aim to explore the capabilities of GPT-4o across various tasks. Inspired by previous study, we constructed a task taxonomy along with a carefully curated set of test samples to conduct a comprehensive qualitative test. Benefiting from GPT-4o's powerful multimoda",
    "arxiv_url": "https://arxiv.org/abs/2505.05501v1",
    "pdf_url": "https://arxiv.org/pdf/2505.05501v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2505.05501",
    "arxiv_authors": [
      "Pu Cao",
      "Feng Zhou",
      "Junyi Ji",
      "Qingye Kong",
      "Zhixiang Lv",
      "Mingjian Zhang",
      "Xuekun Zhao",
      "Siqi Wu",
      "Yinghui Lin",
      "Qing Song",
      "Lu Yang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Preliminary+Explorations+with+GPT-4o%28mni%29+Native+Image+Generation+Pu+Cao+Feng+Zhou+Junyi+Ji+Qingye+Kong+Zhixiang+Lv",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "P Cao",
        "id": "i_R1l9UAAAAJ"
      },
      {
        "name": "F Zhou",
        "id": "1XPQWKIAAAAJ"
      },
      {
        "name": "J Ji",
        "id": null
      },
      {
        "name": "Q Kong",
        "id": null
      },
      {
        "name": "Z Lv",
        "id": null
      },
      {
        "name": "M Zhang",
        "id": null
      },
      {
        "name": "X Zhao",
        "id": null
      },
      {
        "name": "S Wu",
        "id": null
      },
      {
        "name": "Y Lin",
        "id": null
      },
      {
        "name": "Q Song",
        "id": null
      },
      {
        "name": "L Yang",
        "id": "V-6H56AAAAAJ"
      }
    ],
    "citation_count": null
  },
  {
    "arxiv_id": "2406.17100",
    "title": "FaceScore: Benchmarking and Enhancing Face Quality in Human Generation",
    "year": 2024,
    "published": "2024-06-24T19:39:59Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Diffusion models (DMs) have achieved significant success in generating imaginative images given textual descriptions. However, they are likely to fall short when it comes to real-life scenarios with intricate details. The low-quality, unrealistic human faces in text-to-image generation are one of the most prominent issues, hindering the wide application of DMs in practice. Targeting addressing such an issue, we first assess the face quality of generations from popular pre-trained DMs with the ai",
    "arxiv_url": "https://arxiv.org/abs/2406.17100v2",
    "pdf_url": "https://arxiv.org/pdf/2406.17100v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2406.17100",
    "arxiv_authors": [
      "Zhenyi Liao",
      "Qingsong Xie",
      "Chen Chen",
      "Hannan Lu",
      "Zhijie Deng"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=FaceScore%3A+Benchmarking+and+Enhancing+Face+Quality+in+Human+Generation+Zhenyi+Liao+Qingsong+Xie+Chen+Chen+Hannan+Lu+Zhijie+Deng",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "Z Liao",
        "id": null
      },
      {
        "name": "Q Xie",
        "id": "v12EcuAAAAAJ"
      },
      {
        "name": "C Chen",
        "id": "CANDhfAAAAAJ"
      },
      {
        "name": "H Lu",
        "id": null
      },
      {
        "name": "Z Deng -",
        "id": null
      }
    ],
    "citation_count": 8
  },
  {
    "arxiv_id": "2304.10764",
    "title": "Hyperbolic Geometry in Computer Vision: A Survey",
    "year": 2023,
    "published": "2023-04-21T06:22:16Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Hyperbolic geometry, a Riemannian manifold endowed with constant sectional negative curvature, has been considered an alternative embedding space in many learning scenarios, \\eg, natural language processing, graph learning, \\etc, as a result of its intriguing property of encoding the data's hierarchical structure (like irregular graph or tree-likeness data). Recent studies prove that such data hierarchy also exists in the visual dataset, and investigate the successful practice of hyperbolic geom",
    "arxiv_url": "https://arxiv.org/abs/2304.10764v1",
    "pdf_url": "https://arxiv.org/pdf/2304.10764v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2304.10764",
    "arxiv_authors": [
      "Pengfei Fang",
      "Mehrtash Harandi",
      "Trung Le",
      "Dinh Phung"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Hyperbolic+Geometry+in+Computer+Vision%3A+A+Survey+Pengfei+Fang+Mehrtash+Harandi+Trung+Le+Dinh+Phung",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "P Fang",
        "id": "Fk4A13IAAAAJ"
      },
      {
        "name": "M Harandi",
        "id": "Z9gvBegAAAAJ"
      },
      {
        "name": "T Le",
        "id": "gysdMxwAAAAJ"
      },
      {
        "name": "D Phung -",
        "id": null
      }
    ],
    "citation_count": 12
  },
  {
    "arxiv_id": "2304.10616",
    "title": "Multi-domain learning CNN model for microscopy image classification",
    "year": 2023,
    "published": "2023-04-20T19:32:23Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "For any type of microscopy image, getting a deep learning model to work well requires considerable effort to select a suitable architecture and time to train it. As there is a wide range of microscopes and experimental setups, designing a single model that can apply to multiple imaging domains, instead of having multiple per-domain models, becomes more essential. This task is challenging and somehow overlooked in the literature. In this paper, we present a multi-domain learning architecture for ",
    "arxiv_url": "https://arxiv.org/abs/2304.10616v1",
    "pdf_url": "https://arxiv.org/pdf/2304.10616v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2304.10616",
    "arxiv_authors": [
      "Duc Hoa Tran",
      "Michel Meunier",
      "Farida Cheriet"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Multi-domain+learning+CNN+model+for+microscopy+image+classification+Duc+Hoa+Tran+Michel+Meunier+Farida+Cheriet",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "DH Tran",
        "id": "Tu94Z7cAAAAJ"
      },
      {
        "name": "M Meunier",
        "id": "wyM4dDMAAAAJ"
      },
      {
        "name": "F Cheriet -",
        "id": null
      }
    ],
    "citation_count": 2
  },
  {
    "arxiv_id": "2304.01227",
    "title": "Resolution-Invariant Image Classification based on Fourier Neural Operators",
    "year": 2023,
    "published": "2023-04-02T10:23:36Z",
    "categories": [
      "cs.CV",
      "cs.LG",
      "math.NA"
    ],
    "abstract": "In this paper we investigate the use of Fourier Neural Operators (FNOs) for image classification in comparison to standard Convolutional Neural Networks (CNNs). Neural operators are a discretization-invariant generalization of neural networks to approximate operators between infinite dimensional function spaces. FNOs - which are neural operators with a specific parametrization - have been applied successfully in the context of parametric PDEs. We derive the FNO architecture as an example for con",
    "arxiv_url": "https://arxiv.org/abs/2304.01227v1",
    "pdf_url": "https://arxiv.org/pdf/2304.01227v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2304.01227",
    "arxiv_authors": [
      "Samira Kabri",
      "Tim Roith",
      "Daniel Tenbrinck",
      "Martin Burger"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Resolution-Invariant+Image+Classification+based+on+Fourier+Neural+Operators+Samira+Kabri+Tim+Roith+Daniel+Tenbrinck+Martin+Burger",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "S Kabri",
        "id": "kVi18K0AAAAJ"
      },
      {
        "name": "T Roith",
        "id": "BKlbQTAAAAAJ"
      },
      {
        "name": "D Tenbrinck",
        "id": "cDDMZv4AAAAJ"
      },
      {
        "name": "M Burger - International",
        "id": null
      }
    ],
    "citation_count": 16
  },
  {
    "arxiv_id": "2301.09522",
    "title": "Optimising Event-Driven Spiking Neural Network with Regularisation and Cutoff",
    "year": 2023,
    "published": "2023-01-23T16:14:09Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Spiking neural network (SNN), as the next generation of artificial neural network (ANN), offer a closer mimicry of natural neural networks and hold promise for significant improvements in computational efficiency. However, the current SNN is trained to infer over a fixed duration, overlooking the potential of dynamic inference in SNN. In this paper, we strengthen the marriage between SNN and event-driven processing with a proposal to consider a cutoff in SNN, which can terminate SNN anytime duri",
    "arxiv_url": "https://arxiv.org/abs/2301.09522v4",
    "pdf_url": "https://arxiv.org/pdf/2301.09522v4",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2301.09522",
    "arxiv_authors": [
      "Dengyu Wu",
      "Gaojie Jin",
      "Han Yu",
      "Xinping Yi",
      "Xiaowei Huang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Optimising+Event-Driven+Spiking+Neural+Network+with+Regularisation+and+Cutoff+Dengyu+Wu+Gaojie+Jin+Han+Yu+Xinping+Yi+Xiaowei+Huang",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "D Wu",
        "id": "-Ji_VmkAAAAJ"
      },
      {
        "name": "G Jin",
        "id": "n_cu7jwAAAAJ"
      },
      {
        "name": "H Yu",
        "id": "6yeFqWwAAAAJ"
      },
      {
        "name": "X Yi",
        "id": "wAcbI5kAAAAJ"
      },
      {
        "name": "X Huang - Frontiers in neuroscience",
        "id": null
      }
    ],
    "citation_count": 6
  },
  {
    "arxiv_id": "2501.11734",
    "title": "MedicoSAM: Towards foundation models for medical image segmentation",
    "year": 2025,
    "published": "2025-01-20T20:40:28Z",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "abstract": "Medical image segmentation is an important analysis task in clinical practice and research. Deep learning has massively advanced the field, but current approaches are mostly based on models trained for a specific task. Training such models or adapting them to a new condition is costly due to the need for (manually) labeled data. The emergence of vision foundation models, especially Segment Anything, offers a path to universal segmentation for medical images, overcoming these issues. Here, we stu",
    "arxiv_url": "https://arxiv.org/abs/2501.11734v1",
    "pdf_url": "https://arxiv.org/pdf/2501.11734v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2501.11734",
    "arxiv_authors": [
      "Anwai Archit",
      "Luca Freckmann",
      "Constantin Pape"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=MedicoSAM%3A+Towards+foundation+models+for+medical+image+segmentation+Anwai+Archit+Luca+Freckmann+Constantin+Pape",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "A Archit",
        "id": "0I8KO8UAAAAJ"
      },
      {
        "name": "L Freckmann",
        "id": null
      },
      {
        "name": "C Pape -",
        "id": null
      }
    ],
    "citation_count": 3
  },
  {
    "arxiv_id": "2303.15266",
    "title": "Multi-Granularity Archaeological Dating of Chinese Bronze Dings Based on a Knowledge-Guided Relation Graph",
    "year": 2023,
    "published": "2023-03-27T14:54:50Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "The archaeological dating of bronze dings has played a critical role in the study of ancient Chinese history. Current archaeology depends on trained experts to carry out bronze dating, which is time-consuming and labor-intensive. For such dating, in this study, we propose a learning-based approach to integrate advanced deep learning techniques and archaeological knowledge. To achieve this, we first collect a large-scale image dataset of bronze dings, which contains richer attribute information t",
    "arxiv_url": "https://arxiv.org/abs/2303.15266v3",
    "pdf_url": "https://arxiv.org/pdf/2303.15266v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2303.15266",
    "arxiv_authors": [
      "Rixin Zhou",
      "Jiafu Wei",
      "Qian Zhang",
      "Ruihua Qi",
      "Xi Yang",
      "Chuntao Li"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Multi-Granularity+Archaeological+Dating+of+Chinese+Bronze+Dings+Based+on+a+Knowledge-Guided+Relation+Graph+Rixin+Zhou+Jiafu+Wei+Qian+Zhang+Ruihua+Qi+Xi+Yang",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "R Zhou",
        "id": "tzI-OTQAAAAJ"
      },
      {
        "name": "J Wei",
        "id": null
      },
      {
        "name": "Q Zhang",
        "id": null
      },
      {
        "name": "R Qi",
        "id": null
      },
      {
        "name": "X Yang",
        "id": "EefsD-EAAAAJ"
      },
      {
        "name": "C Li",
        "id": null
      }
    ],
    "citation_count": 10
  },
  {
    "arxiv_id": "2310.16148",
    "title": "Yin Yang Convolutional Nets: Image Manifold Extraction by the Analysis of Opposites",
    "year": 2023,
    "published": "2023-10-24T19:48:07Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Computer vision in general presented several advances such as training optimizations, new architectures (pure attention, efficient block, vision language models, generative models, among others). This have improved performance in several tasks such as classification, and others. However, the majority of these models focus on modifications that are taking distance from realistic neuroscientific approaches related to the brain. In this work, we adopt a more bio-inspired approach and present the Yi",
    "arxiv_url": "https://arxiv.org/abs/2310.16148v1",
    "pdf_url": "https://arxiv.org/pdf/2310.16148v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2310.16148",
    "arxiv_authors": [
      "Augusto Seben da Rosa",
      "Frederico Santos de Oliveira",
      "Anderson da Silva Soares",
      "Arnaldo Candido Junior"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Yin+Yang+Convolutional+Nets%3A+Image+Manifold+Extraction+by+the+Analysis+of+Opposites+Augusto+Seben+da+Rosa+Frederico+Santos+de+Oliveira+Anderson+da+Silva+Soares+Arnaldo+Candido+Junior",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "A Seben da Rosa",
        "id": null
      },
      {
        "name": "F Santos de Oliveira",
        "id": null
      }
    ],
    "citation_count": null
  },
  {
    "arxiv_id": "2502.12582",
    "title": "Adaptive Prototype Model for Attribute-based Multi-label Few-shot Action Recognition",
    "year": 2025,
    "published": "2025-02-18T06:39:28Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "In real-world action recognition systems, incorporating more attributes helps achieve a more comprehensive understanding of human behavior. However, using a single model to simultaneously recognize multiple attributes can lead to a decrease in accuracy. In this work, we propose a novel method i.e. Adaptive Attribute Prototype Model (AAPM) for human action recognition, which captures rich action-relevant attribute information and strikes a balance between accuracy and robustness. Firstly, we intr",
    "arxiv_url": "https://arxiv.org/abs/2502.12582v1",
    "pdf_url": "https://arxiv.org/pdf/2502.12582v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2502.12582",
    "arxiv_authors": [
      "Juefeng Xiao",
      "Tianqi Xiang",
      "Zhigang Tu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Adaptive+Prototype+Model+for+Attribute-based+Multi-label+Few-shot+Action+Recognition+Juefeng+Xiao+Tianqi+Xiang+Zhigang+Tu",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "J Xiao",
        "id": null
      },
      {
        "name": "T Xiang",
        "id": null
      },
      {
        "name": "Z Tu -",
        "id": null
      }
    ],
    "citation_count": 3
  },
  {
    "arxiv_id": "2304.13390",
    "title": "Group Equivariant BEV for 3D Object Detection",
    "year": 2023,
    "published": "2023-04-26T09:00:31Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Recently, 3D object detection has attracted significant attention and achieved continuous improvement in real road scenarios. The environmental information is collected from a single sensor or multi-sensor fusion to detect interested objects. However, most of the current 3D object detection approaches focus on developing advanced network architectures to improve the detection precision of the object rather than considering the dynamic driving scenes, where data collected from sensors equipped in",
    "arxiv_url": "https://arxiv.org/abs/2304.13390v2",
    "pdf_url": "https://arxiv.org/pdf/2304.13390v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2304.13390",
    "arxiv_authors": [
      "Hongwei Liu",
      "Jian Yang",
      "Jianfeng Zhang",
      "Dongheng Shao",
      "Jielong Guo",
      "Shaobo Li",
      "Xuan Tang",
      "Xian Wei"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Group+Equivariant+BEV+for+3D+Object+Detection+Hongwei+Liu+Jian+Yang+Jianfeng+Zhang+Dongheng+Shao+Jielong+Guo",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "H Liu",
        "id": null
      },
      {
        "name": "J Yang",
        "id": "xArwbX8AAAAJ"
      },
      {
        "name": "J Zhang",
        "id": null
      },
      {
        "name": "D Shao",
        "id": null
      },
      {
        "name": "J Guo",
        "id": "NjXMlKsAAAAJ"
      },
      {
        "name": "S Li",
        "id": null
      },
      {
        "name": "X Tang",
        "id": "mFj-I10AAAAJ"
      },
      {
        "name": "X Wei",
        "id": "o3m_s5YAAAAJ"
      }
    ],
    "citation_count": 1
  },
  {
    "arxiv_id": "2404.00252",
    "title": "Learned Scanpaths Aid Blind Panoramic Video Quality Assessment",
    "year": 2024,
    "published": "2024-03-30T05:42:17Z",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "abstract": "Panoramic videos have the advantage of providing an immersive and interactive viewing experience. Nevertheless, their spherical nature gives rise to various and uncertain user viewing behaviors, which poses significant challenges for panoramic video quality assessment (PVQA). In this work, we propose an end-to-end optimized, blind PVQA method with explicit modeling of user viewing patterns through visual scanpaths. Our method consists of two modules: a scanpath generator and a quality assessor. ",
    "arxiv_url": "https://arxiv.org/abs/2404.00252v2",
    "pdf_url": "https://arxiv.org/pdf/2404.00252v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2404.00252",
    "arxiv_authors": [
      "Kanglong Fan",
      "Wen Wen",
      "Mu Li",
      "Yifan Peng",
      "Kede Ma"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Learned+Scanpaths+Aid+Blind+Panoramic+Video+Quality+Assessment+Kanglong+Fan+Wen+Wen+Mu+Li+Yifan+Peng+Kede+Ma",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "K Fan",
        "id": "H-62ab0AAAAJ"
      },
      {
        "name": "W Wen",
        "id": "wYqdizQAAAAJ"
      },
      {
        "name": "M Li",
        "id": "qKfskogAAAAJ"
      },
      {
        "name": "Y Peng",
        "id": "UMveGGwAAAAJ"
      },
      {
        "name": "K Ma",
        "id": "sfzOyFoAAAAJ"
      }
    ],
    "citation_count": 5
  },
  {
    "arxiv_id": "2403.17935",
    "title": "OmniVid: A Generative Framework for Universal Video Understanding",
    "year": 2024,
    "published": "2024-03-26T17:59:24Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "The core of video understanding tasks, such as recognition, captioning, and tracking, is to automatically detect objects or actions in a video and analyze their temporal evolution. Despite sharing a common goal, different tasks often rely on distinct model architectures and annotation formats. In contrast, natural language processing benefits from a unified output space, i.e., text sequences, which simplifies the training of powerful foundational language models, such as GPT-3, with extensive tr",
    "arxiv_url": "https://arxiv.org/abs/2403.17935v1",
    "pdf_url": "https://arxiv.org/pdf/2403.17935v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2403.17935",
    "arxiv_authors": [
      "Junke Wang",
      "Dongdong Chen",
      "Chong Luo",
      "Bo He",
      "Lu Yuan",
      "Zuxuan Wu",
      "Yu-Gang Jiang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=OmniVid%3A+A+Generative+Framework+for+Universal+Video+Understanding+Junke+Wang+Dongdong+Chen+Chong+Luo+Bo+He+Lu+Yuan",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "J Wang",
        "id": "qQuxuo0AAAAJ"
      },
      {
        "name": "D Chen",
        "id": "sYKpKqEAAAAJ"
      },
      {
        "name": "C Luo",
        "id": "01iBf38AAAAJ"
      },
      {
        "name": "B He",
        "id": "G2RXa6EAAAAJ"
      },
      {
        "name": "L Yuan",
        "id": null
      },
      {
        "name": "Z Wu",
        "id": "7t12hVkAAAAJ"
      },
      {
        "name": "YG Jiang",
        "id": "f3_FP8AAAAAJ"
      }
    ],
    "citation_count": 35
  },
  {
    "arxiv_id": "2305.19671",
    "title": "Signal Is Harder To Learn Than Bias: Debiasing with Focal Loss",
    "year": 2023,
    "published": "2023-05-31T09:09:59Z",
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "abstract": "Spurious correlations are everywhere. While humans often do not perceive them, neural networks are notorious for learning unwanted associations, also known as biases, instead of the underlying decision rule. As a result, practitioners are often unaware of the biased decision-making of their classifiers. Such a biased model based on spurious correlations might not generalize to unobserved data, leading to unintended, adverse consequences. We propose Signal is Harder (SiH), a variational-autoencod",
    "arxiv_url": "https://arxiv.org/abs/2305.19671v1",
    "pdf_url": "https://arxiv.org/pdf/2305.19671v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2305.19671",
    "arxiv_authors": [
      "Moritz Vandenhirtz",
      "Laura Manduchi",
      "Riƒçards Marcinkeviƒçs",
      "Julia E. Vogt"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Signal+Is+Harder+To+Learn+Than+Bias%3A+Debiasing+with+Focal+Loss+Moritz+Vandenhirtz+Laura+Manduchi+Ri%C4%8Dards+Marcinkevi%C4%8Ds+Julia+E.+Vogt",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "M Vandenhirtz",
        "id": "H2cG0BwAAAAJ"
      },
      {
        "name": "L Manduchi",
        "id": "Yn4tGJgAAAAJ"
      },
      {
        "name": "R Marcinkeviƒçs",
        "id": "XcxXOJsAAAAJ"
      },
      {
        "name": "JE Vogt",
        "id": "UoeV-8kAAAAJ"
      }
    ],
    "citation_count": 4
  },
  {
    "arxiv_id": "2410.07157",
    "title": "InstructG2I: Synthesizing Images from Multimodal Attributed Graphs",
    "year": 2024,
    "published": "2024-10-09T17:56:15Z",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.CV",
      "cs.LG",
      "cs.SI"
    ],
    "abstract": "In this paper, we approach an overlooked yet critical task Graph2Image: generating images from multimodal attributed graphs (MMAGs). This task poses significant challenges due to the explosion in graph size, dependencies among graph entities, and the need for controllability in graph conditions. To address these challenges, we propose a graph context-conditioned diffusion model called InstructG2I. InstructG2I first exploits the graph structure and multimodal information to conduct informative ne",
    "arxiv_url": "https://arxiv.org/abs/2410.07157v1",
    "pdf_url": "https://arxiv.org/pdf/2410.07157v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2410.07157",
    "arxiv_authors": [
      "Bowen Jin",
      "Ziqi Pang",
      "Bingjun Guo",
      "Yu-Xiong Wang",
      "Jiaxuan You",
      "Jiawei Han"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=InstructG2I%3A+Synthesizing+Images+from+Multimodal+Attributed+Graphs+Bowen+Jin+Ziqi+Pang+Bingjun+Guo+Yu-Xiong+Wang+Jiaxuan+You",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "B Jin",
        "id": "dMwdOPkAAAAJ"
      },
      {
        "name": "Z Pang",
        "id": "imNMDhoAAAAJ"
      },
      {
        "name": "B Guo",
        "id": null
      },
      {
        "name": "YX Wang",
        "id": "T_Q-xDkAAAAJ"
      },
      {
        "name": "J You",
        "id": "NDbMl7oAAAAJ"
      },
      {
        "name": "J HanAdvances in Neural Information Processing Systems",
        "id": null
      }
    ],
    "citation_count": 2
  },
  {
    "arxiv_id": "2409.09293",
    "title": "Associate Everything Detected: Facilitating Tracking-by-Detection to the Unknown",
    "year": 2024,
    "published": "2024-09-14T03:52:49Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Multi-object tracking (MOT) emerges as a pivotal and highly promising branch in the field of computer vision. Classical closed-vocabulary MOT (CV-MOT) methods aim to track objects of predefined categories. Recently, some open-vocabulary MOT (OV-MOT) methods have successfully addressed the problem of tracking unknown categories. However, we found that the CV-MOT and OV-MOT methods each struggle to excel in the tasks of the other. In this paper, we present a unified framework, Associate Everything",
    "arxiv_url": "https://arxiv.org/abs/2409.09293v2",
    "pdf_url": "https://arxiv.org/pdf/2409.09293v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2409.09293",
    "arxiv_authors": [
      "Zimeng Fang",
      "Chao Liang",
      "Xue Zhou",
      "Shuyuan Zhu",
      "Xi Li"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Associate+Everything+Detected%3A+Facilitating+Tracking-by-Detection+to+the+Unknown+Zimeng+Fang+Chao+Liang+Xue+Zhou+Shuyuan+Zhu+Xi+Li",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "Z Fang",
        "id": "JEi2ODAAAAAJ"
      },
      {
        "name": "C Liang",
        "id": "RbOk44kAAAAJ"
      },
      {
        "name": "X Zhou",
        "id": null
      },
      {
        "name": "S Zhu",
        "id": null
      },
      {
        "name": "X LiIEEE Transactions on Image Processing",
        "id": null
      }
    ],
    "citation_count": 5
  },
  {
    "arxiv_id": "2405.18267",
    "title": "CT-based brain ventricle segmentation via diffusion Schr√∂dinger Bridge without target domain ground truths",
    "year": 2024,
    "published": "2024-05-28T15:17:58Z",
    "categories": [
      "eess.IV",
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "Efficient and accurate brain ventricle segmentation from clinical CT scans is critical for emergency surgeries like ventriculostomy. With the challenges in poor soft tissue contrast and a scarcity of well-annotated databases for clinical brain CTs, we introduce a novel uncertainty-aware ventricle segmentation technique without the need of CT segmentation ground truths by leveraging diffusion-model-based domain adaptation. Specifically, our method employs the diffusion Schr√∂dinger Bridge and an a",
    "arxiv_url": "https://arxiv.org/abs/2405.18267v2",
    "pdf_url": "https://arxiv.org/pdf/2405.18267v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2405.18267",
    "arxiv_authors": [
      "Reihaneh Teimouri",
      "Marta Kersten-Oertel",
      "Yiming Xiao"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=CT-based+brain+ventricle+segmentation+via+diffusion+Schr%C3%B6dinger+Bridge+without+target+domain+ground+truths+Reihaneh+Teimouri+Marta+Kersten-Oertel+Yiming+Xiao",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "R Teimouri",
        "id": "cCR67VsAAAAJ"
      },
      {
        "name": "M Kersten-Oertel",
        "id": "fy4CeBoAAAAJ"
      },
      {
        "name": "Y Xiao - International",
        "id": null
      }
    ],
    "citation_count": 3
  },
  {
    "arxiv_id": "2411.06530",
    "title": "Image Segmentation from Shadow-Hints using Minimum Spanning Trees",
    "year": 2024,
    "published": "2024-11-10T17:13:01Z",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "abstract": "Image segmentation in RGB space is a notoriously difficult task where state-of-the-art methods are trained on thousands or even millions of annotated images. While the performance is impressive, it is still not perfect. We propose a novel image segmentation method, achieving similar segmentation quality but without training. Instead, we require an image sequence with a static camera and a single light source at varying positions, as used in for photometric stereo, for example.",
    "arxiv_url": "https://arxiv.org/abs/2411.06530v1",
    "pdf_url": "https://arxiv.org/pdf/2411.06530v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2411.06530",
    "arxiv_authors": [
      "Moritz Heep",
      "Eduard Zell"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Image+Segmentation+from+Shadow-Hints+using+Minimum+Spanning+Trees+Moritz+Heep+Eduard+Zell",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "M Heep",
        "id": "yCHKHtwAAAAJ"
      },
      {
        "name": "E Zell - ACM SIGGRAPH",
        "id": null
      }
    ],
    "citation_count": null
  },
  {
    "arxiv_id": "2311.11099",
    "title": "Introducing NCL-SM: A Fully Annotated Dataset of Images from Human Skeletal Muscle Biopsies",
    "year": 2023,
    "published": "2023-11-18T15:18:38Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "q-bio.TO"
    ],
    "abstract": "Single cell analysis of skeletal muscle (SM) tissue is a fundamental tool for understanding many neuromuscular disorders. For this analysis to be reliable and reproducible, identification of individual fibres within microscopy images (segmentation) of SM tissue should be precise. There is currently no tool or pipeline that makes automatic and precise segmentation and curation of images of SM tissue cross-sections possible. Biomedical scientists in this field rely on custom tools and general mach",
    "arxiv_url": "https://arxiv.org/abs/2311.11099v1",
    "pdf_url": "https://arxiv.org/pdf/2311.11099v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2311.11099",
    "arxiv_authors": [
      "Atif Khan",
      "Conor Lawless",
      "Amy Vincent",
      "Charlotte Warren",
      "Valeria Di Leo",
      "Tiago Gomes",
      "A. Stephen McGough"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Introducing+NCL-SM%3A+A+Fully+Annotated+Dataset+of+Images+from+Human+Skeletal+Muscle+Biopsies+Atif+Khan+Conor+Lawless+Amy+Vincent+Charlotte+Warren+Valeria+Di+Leo",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "A Khan",
        "id": "4vn38zYAAAAJ"
      },
      {
        "name": "C Lawless",
        "id": "qAxPi50AAAAJ"
      },
      {
        "name": "A Vincent",
        "id": "_Ma0wHoAAAAJ"
      },
      {
        "name": "C Warren",
        "id": null
      }
    ],
    "citation_count": null
  },
  {
    "arxiv_id": "2402.05728",
    "title": "CTGAN: Semantic-guided Conditional Texture Generator for 3D Shapes",
    "year": 2024,
    "published": "2024-02-08T15:00:15Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "The entertainment industry relies on 3D visual content to create immersive experiences, but traditional methods for creating textured 3D models can be time-consuming and subjective. Generative networks such as StyleGAN have advanced image synthesis, but generating 3D objects with high-fidelity textures is still not well explored, and existing methods have limitations. We propose the Semantic-guided Conditional Texture Generator (CTGAN), producing high-quality textures for 3D shapes that are cons",
    "arxiv_url": "https://arxiv.org/abs/2402.05728v1",
    "pdf_url": "https://arxiv.org/pdf/2402.05728v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2402.05728",
    "arxiv_authors": [
      "Yi-Ting Pan",
      "Chai-Rong Lee",
      "Shu-Ho Fan",
      "Jheng-Wei Su",
      "Jia-Bin Huang",
      "Yung-Yu Chuang",
      "Hung-Kuo Chu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=CTGAN%3A+Semantic-guided+Conditional+Texture+Generator+for+3D+Shapes+Yi-Ting+Pan+Chai-Rong+Lee+Shu-Ho+Fan+Jheng-Wei+Su+Jia-Bin+Huang",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "YT Pan",
        "id": null
      },
      {
        "name": "CR Lee",
        "id": null
      },
      {
        "name": "SH Fan",
        "id": null
      },
      {
        "name": "JW Su",
        "id": "UJgDyCUAAAAJ"
      },
      {
        "name": "JB Huang",
        "id": "pp848fYAAAAJ"
      },
      {
        "name": "YY Chuang",
        "id": null
      },
      {
        "name": "HK Chu",
        "id": "XM4QrScAAAAJ"
      }
    ],
    "citation_count": 2
  },
  {
    "arxiv_id": "2411.05714",
    "title": "STARS: Sensor-agnostic Transformer Architecture for Remote Sensing",
    "year": 2024,
    "published": "2024-11-08T17:16:02Z",
    "categories": [
      "cs.CV",
      "cs.LG",
      "eess.IV"
    ],
    "abstract": "We present a sensor-agnostic spectral transformer as the basis for spectral foundation models. To that end, we introduce a Universal Spectral Representation (USR) that leverages sensor meta-data, such as sensing kernel specifications and sensing wavelengths, to encode spectra obtained from any spectral instrument into a common representation, such that a single model can ingest data from any sensor. Furthermore, we develop a methodology for pre-training such models in a self-supervised manner us",
    "arxiv_url": "https://arxiv.org/abs/2411.05714v1",
    "pdf_url": "https://arxiv.org/pdf/2411.05714v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2411.05714",
    "arxiv_authors": [
      "Ethan King",
      "Jaime Rodriguez",
      "Diego Llanes",
      "Timothy Doster",
      "Tegan Emerson",
      "James Koch"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=STARS%3A+Sensor-agnostic+Transformer+Architecture+for+Remote+Sensing+Ethan+King+Jaime+Rodriguez+Diego+Llanes+Timothy+Doster+Tegan+Emerson",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "E King",
        "id": null
      },
      {
        "name": "J Rodriguez",
        "id": null
      },
      {
        "name": "D Llanes",
        "id": "8MSKnjwAAAAJ"
      },
      {
        "name": "T Doster",
        "id": "5kJfT1gAAAAJ"
      },
      {
        "name": "T Emerson",
        "id": "sxRHY7MAAAAJ"
      },
      {
        "name": "J Koch2024 14th",
        "id": null
      }
    ],
    "citation_count": 2
  },
  {
    "arxiv_id": "2311.16664",
    "title": "DGNR: Density-Guided Neural Point Rendering of Large Driving Scenes",
    "year": 2023,
    "published": "2023-11-28T10:25:55Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Despite the recent success of Neural Radiance Field (NeRF), it is still challenging to render large-scale driving scenes with long trajectories, particularly when the rendering quality and efficiency are in high demand. Existing methods for such scenes usually involve with spatial warping, geometric supervision from zero-shot normal or depth estimation, or scene division strategies, where the synthesized views are often blurry or fail to meet the requirement of efficient rendering. To address th",
    "arxiv_url": "https://arxiv.org/abs/2311.16664v1",
    "pdf_url": "https://arxiv.org/pdf/2311.16664v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2311.16664",
    "arxiv_authors": [
      "Zhuopeng Li",
      "Chenming Wu",
      "Liangjun Zhang",
      "Jianke Zhu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=DGNR%3A+Density-Guided+Neural+Point+Rendering+of+Large+Driving+Scenes+Zhuopeng+Li+Chenming+Wu+Liangjun+Zhang+Jianke+Zhu",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "Z Li",
        "id": "Qrf9gZsAAAAJ"
      },
      {
        "name": "C Wu",
        "id": "eOkkQWUAAAAJ"
      },
      {
        "name": "L Zhang",
        "id": "Byzk604AAAAJ"
      },
      {
        "name": "J Zhu - IEEE Transactions on Automation",
        "id": null
      }
    ],
    "citation_count": 11
  },
  {
    "arxiv_id": "2406.11641",
    "title": "YOLO-FEDER FusionNet: A Novel Deep Learning Architecture for Drone Detection",
    "year": 2024,
    "published": "2024-06-17T15:25:31Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Predominant methods for image-based drone detection frequently rely on employing generic object detection algorithms like YOLOv5. While proficient in identifying drones against homogeneous backgrounds, these algorithms often struggle in complex, highly textured environments. In such scenarios, drones seamlessly integrate into the background, creating camouflage effects that adversely affect the detection quality. To address this issue, we introduce a novel deep learning architecture called YOLO-",
    "arxiv_url": "https://arxiv.org/abs/2406.11641v1",
    "pdf_url": "https://arxiv.org/pdf/2406.11641v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2406.11641",
    "arxiv_authors": [
      "Tamara R. Lenhard",
      "Andreas Weinmann",
      "Stefan J√§ger",
      "Tobias Koch"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=YOLO-FEDER+FusionNet%3A+A+Novel+Deep+Learning+Architecture+for+Drone+Detection+Tamara+R.+Lenhard+Andreas+Weinmann+Stefan+J%C3%A4ger+Tobias+Koch",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "TR Lenhard",
        "id": "xByiKDYAAAAJ"
      },
      {
        "name": "A Weinmann",
        "id": "UmCfkcQAAAAJ"
      },
      {
        "name": "S J√§ger",
        "id": null
      },
      {
        "name": "T Koch2024 IEEE International",
        "id": null
      }
    ],
    "citation_count": 8
  },
  {
    "arxiv_id": "2404.10600",
    "title": "Intra-operative tumour margin evaluation in breast-conserving surgery with deep learning",
    "year": 2024,
    "published": "2024-04-16T14:26:55Z",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "A positive margin may result in an increased risk of local recurrences after breast retention surgery for any malignant tumour. In order to reduce the number of positive margins would offer surgeon real-time intra-operative information on the presence of positive resection margins. This study aims to design an intra-operative tumour margin evaluation scheme by using specimen mammography in breast-conserving surgery. Total of 30 cases were evaluated and compared with the manually determined conto",
    "arxiv_url": "https://arxiv.org/abs/2404.10600v1",
    "pdf_url": "https://arxiv.org/pdf/2404.10600v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2404.10600",
    "arxiv_authors": [
      "Wei-Chung Shia",
      "Yu-Len Huang",
      "Yi-Chun Chen",
      "Hwa-Koon Wu",
      "Dar-Ren Chen"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Intra-operative+tumour+margin+evaluation+in+breast-conserving+surgery+with+deep+learning+Wei-Chung+Shia+Yu-Len+Huang+Yi-Chun+Chen+Hwa-Koon+Wu+Dar-Ren+Chen",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "WC Shia",
        "id": "FWPSTsUAAAAJ"
      },
      {
        "name": "YL Huang",
        "id": "AYkg-zwAAAAJ"
      },
      {
        "name": "YC Chen",
        "id": null
      },
      {
        "name": "HK Wu",
        "id": null
      },
      {
        "name": "DR Chen",
        "id": null
      }
    ],
    "citation_count": null
  },
  {
    "arxiv_id": "2311.17425",
    "title": "SpeechAct: Towards Generating Whole-body Motion from Speech",
    "year": 2023,
    "published": "2023-11-29T07:57:30Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "This paper addresses the problem of generating whole-body motion from speech. Despite great successes, prior methods still struggle to produce reasonable and diverse whole-body motions from speech. This is due to their reliance on suboptimal representations and a lack of strategies for generating diverse results. To address these challenges, we present a novel hybrid point representation to achieve accurate and continuous motion generation, e.g., avoiding foot skating, and this representation ca",
    "arxiv_url": "https://arxiv.org/abs/2311.17425v5",
    "pdf_url": "https://arxiv.org/pdf/2311.17425v5",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2311.17425",
    "arxiv_authors": [
      "Jinsong Zhang",
      "Minjie Zhu",
      "Yuxiang Zhang",
      "Yebin Liu",
      "Kun Li"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=SpeechAct%3A+Towards+Generating+Whole-body+Motion+from+Speech+Jinsong+Zhang+Minjie+Zhu+Yuxiang+Zhang+Yebin+Liu+Kun+Li",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "J Zhang",
        "id": "UcGHO1QAAAAJ"
      },
      {
        "name": "M Zhu",
        "id": null
      },
      {
        "name": "Y Zhang",
        "id": null
      },
      {
        "name": "Z Zheng",
        "id": "12qaaesAAAAJ"
      },
      {
        "name": "Y Liu",
        "id": "ogXIdlYAAAAJ"
      }
    ],
    "citation_count": 4
  },
  {
    "arxiv_id": "2311.17389",
    "title": "360Loc: A Dataset and Benchmark for Omnidirectional Visual Localization with Cross-device Queries",
    "year": 2023,
    "published": "2023-11-29T06:42:12Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Portable 360$^\\circ$ cameras are becoming a cheap and efficient tool to establish large visual databases. By capturing omnidirectional views of a scene, these cameras could expedite building environment models that are essential for visual localization. However, such an advantage is often overlooked due to the lack of valuable datasets. This paper introduces a new benchmark dataset, 360Loc, composed of 360$^\\circ$ images with ground truth poses for visual localization. We present a practical imp",
    "arxiv_url": "https://arxiv.org/abs/2311.17389v3",
    "pdf_url": "https://arxiv.org/pdf/2311.17389v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2311.17389",
    "arxiv_authors": [
      "Huajian Huang",
      "Changkun Liu",
      "Yipeng Zhu",
      "Hui Cheng",
      "Tristan Braud",
      "Sai-Kit Yeung"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=360Loc%3A+A+Dataset+and+Benchmark+for+Omnidirectional+Visual+Localization+with+Cross-device+Queries+Huajian+Huang+Changkun+Liu+Yipeng+Zhu+Hui+Cheng+Tristan+Braud",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "H Huang",
        "id": "rOhG9NoAAAAJ"
      },
      {
        "name": "C Liu",
        "id": "25npC_YAAAAJ"
      },
      {
        "name": "Y Zhu",
        "id": "pb3sxigAAAAJ"
      },
      {
        "name": "H Cheng",
        "id": "c8KbVtcAAAAJ"
      },
      {
        "name": "T Braud",
        "id": "ZOZtoQUAAAAJ"
      },
      {
        "name": "SK Yeung",
        "id": "16iMMwwAAAAJ"
      }
    ],
    "citation_count": 23
  },
  {
    "arxiv_id": "2504.09608",
    "title": "ERL-MPP: Evolutionary Reinforcement Learning with Multi-head Puzzle Perception for Solving Large-scale Jigsaw Puzzles of Eroded Gaps",
    "year": 2025,
    "published": "2025-04-13T14:56:41Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Solving jigsaw puzzles has been extensively studied. While most existing models focus on solving either small-scale puzzles or puzzles with no gap between fragments, solving large-scale puzzles with gaps presents distinctive challenges in both image understanding and combinatorial optimization. To tackle these challenges, we propose a framework of Evolutionary Reinforcement Learning with Multi-head Puzzle Perception (ERL-MPP) to derive a better set of swapping actions for solving the puzzles. Sp",
    "arxiv_url": "https://arxiv.org/abs/2504.09608v1",
    "pdf_url": "https://arxiv.org/pdf/2504.09608v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2504.09608",
    "arxiv_authors": [
      "Xingke Song",
      "Xiaoying Yang",
      "Chenglin Yao",
      "Jianfeng Ren",
      "Ruibin Bai",
      "Xin Chen",
      "Xudong Jiang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=ERL-MPP%3A+Evolutionary+Reinforcement+Learning+with+Multi-head+Puzzle+Perception+for+Solving+Large-scale+Jigsaw+Puzzles+of+Eroded+Gaps+Xingke+Song+Xiaoying+Yang+Chenglin+Yao+Jianfeng+Ren+Ruibin+Bai",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "X Song",
        "id": "FeyWRzwAAAAJ"
      },
      {
        "name": "X Yang",
        "id": "aysEyGsAAAAJ"
      },
      {
        "name": "C Yao",
        "id": "m3zqydsAAAAJ"
      },
      {
        "name": "J Ren",
        "id": "ZZ928OgAAAAJ"
      },
      {
        "name": "R Bai",
        "id": "oP6AThIAAAAJ"
      },
      {
        "name": "X Chen",
        "id": "yqpHF90AAAAJ"
      },
      {
        "name": "X Jiang",
        "id": "IL3mSioAAAAJ"
      }
    ],
    "citation_count": 5
  },
  {
    "arxiv_id": "2312.04780",
    "title": "Fine-Tuning InstructPix2Pix for Advanced Image Colorization",
    "year": 2023,
    "published": "2023-12-08T01:36:49Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "This paper presents a novel approach to human image colorization by fine-tuning the InstructPix2Pix model, which integrates a language model (GPT-3) with a text-to-image model (Stable Diffusion). Despite the original InstructPix2Pix model's proficiency in editing images based on textual instructions, it exhibits limitations in the focused domain of colorization. To address this, we fine-tuned the model using the IMDB-WIKI dataset, pairing black-and-white images with a diverse set of colorization",
    "arxiv_url": "https://arxiv.org/abs/2312.04780v1",
    "pdf_url": "https://arxiv.org/pdf/2312.04780v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2312.04780",
    "arxiv_authors": [
      "Zifeng An",
      "Zijing Xu",
      "Eric Fan",
      "Qi Cao"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Fine-Tuning+InstructPix2Pix+for+Advanced+Image+Colorization+Zifeng+An+Zijing+Xu+Eric+Fan+Qi+Cao",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "Z An",
        "id": "0fsaJRUAAAAJ"
      },
      {
        "name": "Z Xu",
        "id": "riIAJbQAAAAJ"
      },
      {
        "name": "E Fan",
        "id": null
      },
      {
        "name": "Q Cao -",
        "id": null
      }
    ],
    "citation_count": 1
  },
  {
    "arxiv_id": "2301.02211",
    "title": "Teaching Computer Vision for Ecology",
    "year": 2023,
    "published": "2023-01-05T18:30:17Z",
    "categories": [
      "cs.CY",
      "cs.CV"
    ],
    "abstract": "Computer vision can accelerate ecology research by automating the analysis of raw imagery from sensors like camera traps, drones, and satellites. However, computer vision is an emerging discipline that is rarely taught to ecologists. This work discusses our experience teaching a diverse group of ecologists to prototype and evaluate computer vision systems in the context of an intensive hands-on summer workshop. We explain the workshop structure, discuss common challenges, and propose best practi",
    "arxiv_url": "https://arxiv.org/abs/2301.02211v1",
    "pdf_url": "https://arxiv.org/pdf/2301.02211v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2301.02211",
    "arxiv_authors": [
      "Elijah Cole",
      "Suzanne Stathatos",
      "Bj√∂rn L√ºtjens",
      "Tarun Sharma",
      "Justin Kay",
      "Jason Parham",
      "Benjamin Kellenberger",
      "Sara Beery"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Teaching+Computer+Vision+for+Ecology+Elijah+Cole+Suzanne+Stathatos+Bj%C3%B6rn+L%C3%BCtjens+Tarun+Sharma+Justin+Kay",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "E Cole",
        "id": "-atuVWQAAAAJ"
      },
      {
        "name": "S Stathatos",
        "id": "JAAaAIcAAAAJ"
      },
      {
        "name": "B L√ºtjens",
        "id": "AayqHVcAAAAJ"
      },
      {
        "name": "T Sharma",
        "id": "CHfQlvwAAAAJ"
      },
      {
        "name": "J Kay",
        "id": "Qurq7foAAAAJ"
      },
      {
        "name": "J Parham",
        "id": "jwz3HbwAAAAJ"
      },
      {
        "name": "B Kellenberger",
        "id": "Ol2bwNAAAAAJ"
      },
      {
        "name": "S Beery",
        "id": "Hbr4c10AAAAJ"
      }
    ],
    "citation_count": 8
  },
  {
    "arxiv_id": "2302.02907",
    "title": "GAT: Guided Adversarial Training with Pareto-optimal Auxiliary Tasks",
    "year": 2023,
    "published": "2023-02-06T16:23:24Z",
    "categories": [
      "cs.CV",
      "cs.CR",
      "cs.LG"
    ],
    "abstract": "While leveraging additional training data is well established to improve adversarial robustness, it incurs the unavoidable cost of data collection and the heavy computation to train models. To mitigate the costs, we propose Guided Adversarial Training (GAT), a novel adversarial training technique that exploits auxiliary tasks under a limited set of training data. Our approach extends single-task models into multi-task models during the min-max optimization of adversarial training, and drives the",
    "arxiv_url": "https://arxiv.org/abs/2302.02907v2",
    "pdf_url": "https://arxiv.org/pdf/2302.02907v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2302.02907",
    "arxiv_authors": [
      "Salah Ghamizi",
      "Jingfeng Zhang",
      "Maxime Cordy",
      "Mike Papadakis",
      "Masashi Sugiyama",
      "Yves Le Traon"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=GAT%3A+Guided+Adversarial+Training+with+Pareto-optimal+Auxiliary+Tasks+Salah+Ghamizi+Jingfeng+Zhang+Maxime+Cordy+Mike+Papadakis+Masashi+Sugiyama",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "S Ghamizi",
        "id": "UcvKgR0AAAAJ"
      },
      {
        "name": "J Zhang",
        "id": "NS0P1FkAAAAJ"
      },
      {
        "name": "M Cordy",
        "id": "sRXHjkIAAAAJ"
      },
      {
        "name": "M Papadakis",
        "id": "4O3EolUAAAAJ"
      },
      {
        "name": "M Sugiyama",
        "id": "GkYIrlIAAAAJ"
      },
      {
        "name": "Y Le TraonInternational",
        "id": null
      }
    ],
    "citation_count": 7
  },
  {
    "arxiv_id": "2505.21099",
    "title": "Instance Data Condensation for Image Super-Resolution",
    "year": 2025,
    "published": "2025-05-27T12:25:09Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Deep learning based image Super-Resolution (ISR) relies on large training datasets to optimize model generalization; this requires substantial computational and storage resources during training. While dataset condensation has shown potential in improving data efficiency and privacy for high-level computer vision tasks, it has not yet been fully exploited for ISR. In this paper, we propose a novel Instance Data Condensation (IDC) framework specifically for ISR, which achieves instance-level data",
    "arxiv_url": "https://arxiv.org/abs/2505.21099v1",
    "pdf_url": "https://arxiv.org/pdf/2505.21099v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2505.21099",
    "arxiv_authors": [
      "Tianhao Peng",
      "Ho Man Kwan",
      "Yuxuan Jiang",
      "Ge Gao",
      "Fan Zhang",
      "Xiaozhong Xu",
      "Shan Liu",
      "David Bull"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Instance+Data+Condensation+for+Image+Super-Resolution+Tianhao+Peng+Ho+Man+Kwan+Yuxuan+Jiang+Ge+Gao+Fan+Zhang",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "T Peng",
        "id": "bkXX0hUAAAAJ"
      },
      {
        "name": "HM Kwan",
        "id": "xNwKDN8AAAAJ"
      },
      {
        "name": "Y Jiang",
        "id": "5C_qRnYAAAAJ"
      },
      {
        "name": "G Gao",
        "id": "j2_80ewAAAAJ"
      },
      {
        "name": "F Zhang",
        "id": "BBujJNcAAAAJ"
      },
      {
        "name": "X Xu",
        "id": "Xs4QqgcAAAAJ"
      },
      {
        "name": "S Liu",
        "id": "bdBZ43wAAAAJ"
      },
      {
        "name": "D Bull",
        "id": "WraDXlkAAAAJ"
      }
    ],
    "citation_count": 1
  },
  {
    "arxiv_id": "2302.01058",
    "title": "IKOL: Inverse kinematics optimization layer for 3D human pose and shape estimation via Gauss-Newton differentiation",
    "year": 2023,
    "published": "2023-02-02T12:43:29Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "This paper presents an inverse kinematic optimization layer (IKOL) for 3D human pose and shape estimation that leverages the strength of both optimization- and regression-based methods within an end-to-end framework. IKOL involves a nonconvex optimization that establishes an implicit mapping from an image's 3D keypoints and body shapes to the relative body-part rotations. The 3D keypoints and the body shapes are the inputs and the relative body-part rotations are the solutions. However, this pro",
    "arxiv_url": "https://arxiv.org/abs/2302.01058v2",
    "pdf_url": "https://arxiv.org/pdf/2302.01058v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2302.01058",
    "arxiv_authors": [
      "Juze Zhang",
      "Ye Shi",
      "Yuexin Ma",
      "Lan Xu",
      "Jingyi Yu",
      "Jingya Wang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=IKOL%3A+Inverse+kinematics+optimization+layer+for+3D+human+pose+and+shape+estimation+via+Gauss-Newton+differentiation+Juze+Zhang+Ye+Shi+Yuexin+Ma+Lan+Xu+Jingyi+Yu",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "J Zhang",
        "id": "qbovRUIAAAAJ"
      },
      {
        "name": "Y Shi",
        "id": "gMqbZPUAAAAJ"
      },
      {
        "name": "Y Ma",
        "id": "DIsP7rUAAAAJ"
      },
      {
        "name": "L Xu",
        "id": "aPS5pJkAAAAJ"
      },
      {
        "name": "J Yu",
        "id": null
      },
      {
        "name": "J Wang -",
        "id": null
      }
    ],
    "citation_count": 20
  },
  {
    "arxiv_id": "2503.07371",
    "title": "HGO-YOLO: Advancing Anomaly Behavior Detection with Hierarchical Features and Lightweight Optimized Detection",
    "year": 2025,
    "published": "2025-03-10T14:29:12Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Accurate, real-time object detection on resource-constrained hardware is critical for anomaly-behavior monitoring. We introduce HGO-YOLO, a lightweight detector that combines GhostHGNetv2 with an optimized parameter-sharing head (OptiConvDetect) to deliver an outstanding accuracy-efficiency trade-off. By embedding GhostConv into the HGNetv2 backbone with multi-scale residual fusion, the receptive field is enlarged while redundant computation is reduced by 50%. OptiConvDetect shares a partial-con",
    "arxiv_url": "https://arxiv.org/abs/2503.07371v2",
    "pdf_url": "https://arxiv.org/pdf/2503.07371v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2503.07371",
    "arxiv_authors": [
      "Qizhi Zheng",
      "Zhongze Luo",
      "Meiyan Guo",
      "Xinzhu Wang",
      "Renqimuge Wu",
      "Qiu Meng",
      "Guanghui Dong"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=HGO-YOLO%3A+Advancing+Anomaly+Behavior+Detection+with+Hierarchical+Features+and+Lightweight+Optimized+Detection+Qizhi+Zheng+Zhongze+Luo+Meiyan+Guo+Xinzhu+Wang+Renqimuge+Wu",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "Q Zheng",
        "id": null
      },
      {
        "name": "Z Luo",
        "id": "3kBbqG4AAAAJ"
      },
      {
        "name": "M Guo",
        "id": null
      },
      {
        "name": "X Wang",
        "id": null
      },
      {
        "name": "R Wu",
        "id": null
      },
      {
        "name": "Q Meng",
        "id": null
      },
      {
        "name": "G Dong",
        "id": null
      }
    ],
    "citation_count": 4
  },
  {
    "arxiv_id": "2502.13974",
    "title": "Segmentation-free integration of nuclei morphology and spatial transcriptomics for retinal images",
    "year": 2025,
    "published": "2025-02-08T14:03:02Z",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "abstract": "This study introduces SEFI (SEgmentation-Free Integration), a novel method for integrating morphological features of cell nuclei with spatial transcriptomics data. Cell segmentation poses a significant challenge in the analysis of spatial transcriptomics data, as tissue-specific structural complexities and densely packed cells in certain regions make it difficult to develop a universal approach. SEFI addresses this by utilizing self-supervised learning to extract morphological features from fluo",
    "arxiv_url": "https://arxiv.org/abs/2502.13974v1",
    "pdf_url": "https://arxiv.org/pdf/2502.13974v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2502.13974",
    "arxiv_authors": [
      "Eduard Chelebian",
      "Pratiti Dasgupta",
      "Zainalabedin Samadi",
      "Carolina W√§hlby",
      "Amjad Askary"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Segmentation-free+integration+of+nuclei+morphology+and+spatial+transcriptomics+for+retinal+images+Eduard+Chelebian+Pratiti+Dasgupta+Zainalabedin+Samadi+Carolina+W%C3%A4hlby+Amjad+Askary",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "E Chelebian",
        "id": "nGKLYz4AAAAJ"
      },
      {
        "name": "P Dasgupta",
        "id": null
      },
      {
        "name": "Z Samadi",
        "id": "zdV_CB4AAAAJ"
      },
      {
        "name": "C W√§hlby",
        "id": "17soDRoAAAAJ"
      },
      {
        "name": "A Askary",
        "id": "Gl9BIaIAAAAJ"
      }
    ],
    "citation_count": null
  },
  {
    "arxiv_id": "2505.11165",
    "title": "Maximizing Asynchronicity in Event-based Neural Networks",
    "year": 2025,
    "published": "2025-05-16T12:07:50Z",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.CV"
    ],
    "abstract": "Event cameras deliver visual data with high temporal resolution, low latency, and minimal redundancy, yet their asynchronous, sparse sequential nature challenges standard tensor-based machine learning (ML). While the recent asynchronous-to-synchronous (A2S) paradigm aims to bridge this gap by asynchronously encoding events into learned representations for ML pipelines, existing A2S approaches often sacrifice representation expressivity and generalizability compared to dense, synchronous methods.",
    "arxiv_url": "https://arxiv.org/abs/2505.11165v1",
    "pdf_url": "https://arxiv.org/pdf/2505.11165v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2505.11165",
    "arxiv_authors": [
      "Haiqing Hao",
      "Nikola Zubiƒá",
      "Weihua He",
      "Zhipeng Sui",
      "Davide Scaramuzza",
      "Wenhui Wang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Maximizing+Asynchronicity+in+Event-based+Neural+Networks+Haiqing+Hao+Nikola+Zubi%C4%87+Weihua+He+Zhipeng+Sui+Davide+Scaramuzza",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "H Hao",
        "id": "dnRoIJIAAAAJ"
      },
      {
        "name": "N Zubiƒá",
        "id": "65OImV0AAAAJ"
      },
      {
        "name": "W He",
        "id": "PRX4APYAAAAJ"
      },
      {
        "name": "Z Sui",
        "id": null
      },
      {
        "name": "D Scaramuzza",
        "id": "SC9wV2kAAAAJ"
      },
      {
        "name": "W Wang",
        "id": null
      }
    ],
    "citation_count": null
  },
  {
    "arxiv_id": "2410.15036",
    "title": "EViT-Unet: U-Net Like Efficient Vision Transformer for Medical Image Segmentation on Mobile and Edge Devices",
    "year": 2024,
    "published": "2024-10-19T08:42:53Z",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "abstract": "With the rapid development of deep learning, CNN-based U-shaped networks have succeeded in medical image segmentation and are widely applied for various tasks. However, their limitations in capturing global features hinder their performance in complex segmentation tasks. The rise of Vision Transformer (ViT) has effectively compensated for this deficiency of CNNs and promoted the application of ViT-based U-networks in medical image segmentation. However, the high computational demands of ViT make",
    "arxiv_url": "https://arxiv.org/abs/2410.15036v1",
    "pdf_url": "https://arxiv.org/pdf/2410.15036v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2410.15036",
    "arxiv_authors": [
      "Xin Li",
      "Wenhui Zhu",
      "Xuanzhao Dong",
      "Oana M. Dumitrascu",
      "Yalin Wang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=EViT-Unet%3A+U-Net+Like+Efficient+Vision+Transformer+for+Medical+Image+Segmentation+on+Mobile+and+Edge+Devices+Xin+Li+Wenhui+Zhu+Xuanzhao+Dong+Oana+M.+Dumitrascu+Yalin+Wang",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "X Li",
        "id": "e8FLeP8AAAAJ"
      },
      {
        "name": "W Zhu",
        "id": "Se8aIO4YIp8C"
      },
      {
        "name": "X Dong",
        "id": "Oxc6RG8AAAAJ"
      },
      {
        "name": "OM Dumitrascu",
        "id": "4evGH5wAAAAJ"
      },
      {
        "name": "Y Wang2025 IEEE 22nd International",
        "id": null
      }
    ],
    "citation_count": 7
  },
  {
    "arxiv_id": "2411.04501",
    "title": "Pose2Trajectory: Using Transformers on Body Pose to Predict Tennis Player's Trajectory",
    "year": 2024,
    "published": "2024-11-07T07:50:58Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Tracking the trajectory of tennis players can help camera operators in production. Predicting future movement enables cameras to automatically track and predict a player's future trajectory without human intervention. Predicting future human movement in the context of complex physical tasks is also intellectually satisfying. Swift advancements in sports analytics and the wide availability of videos for tennis have inspired us to propose a novel method called Pose2Trajectory, which predicts a ten",
    "arxiv_url": "https://arxiv.org/abs/2411.04501v1",
    "pdf_url": "https://arxiv.org/pdf/2411.04501v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2411.04501",
    "arxiv_authors": [
      "Ali K. AlShami",
      "Terrance Boult",
      "Jugal Kalita"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Pose2Trajectory%3A+Using+Transformers+on+Body+Pose+to+Predict+Tennis+Player%27s+Trajectory+Ali+K.+AlShami+Terrance+Boult+Jugal+Kalita",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "A AlShami",
        "id": "Vv1FePkAAAAJ"
      },
      {
        "name": "T Boult",
        "id": "fF1B7mAAAAAJ"
      },
      {
        "name": "J Kalita -",
        "id": null
      }
    ],
    "citation_count": 22
  },
  {
    "arxiv_id": "2311.00734",
    "title": "On Manipulating Scene Text in the Wild with Diffusion Models",
    "year": 2023,
    "published": "2023-11-01T11:31:50Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Diffusion models have gained attention for image editing yielding impressive results in text-to-image tasks. On the downside, one might notice that generated images of stable diffusion models suffer from deteriorated details. This pitfall impacts image editing tasks that require information preservation e.g., scene text editing. As a desired result, the model must show the capability to replace the text on the source image to the target text while preserving the details e.g., color, font size, a",
    "arxiv_url": "https://arxiv.org/abs/2311.00734v2",
    "pdf_url": "https://arxiv.org/pdf/2311.00734v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2311.00734",
    "arxiv_authors": [
      "Joshua Santoso",
      "Christian Simon",
      "Williem"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=On+Manipulating+Scene+Text+in+the+Wild+with+Diffusion+Models+Joshua+Santoso+Christian+Simon+Williem",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "J Santoso",
        "id": "GP2VJk4AAAAJ"
      },
      {
        "name": "C Simon -",
        "id": null
      }
    ],
    "citation_count": 15
  },
  {
    "arxiv_id": "2403.03273",
    "title": "DINOv2 based Self Supervised Learning For Few Shot Medical Image Segmentation",
    "year": 2024,
    "published": "2024-03-05T19:13:45Z",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "Deep learning models have emerged as the cornerstone of medical image segmentation, but their efficacy hinges on the availability of extensive manually labeled datasets and their adaptability to unforeseen categories remains a challenge. Few-shot segmentation (FSS) offers a promising solution by endowing models with the capacity to learn novel classes from limited labeled examples. A leading method for FSS is ALPNet, which compares features between the query image and the few available support s",
    "arxiv_url": "https://arxiv.org/abs/2403.03273v1",
    "pdf_url": "https://arxiv.org/pdf/2403.03273v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2403.03273",
    "arxiv_authors": [
      "Lev Ayzenberg",
      "Raja Giryes",
      "Hayit Greenspan"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=DINOv2+based+Self+Supervised+Learning+For+Few+Shot+Medical+Image+Segmentation+Lev+Ayzenberg+Raja+Giryes+Hayit+Greenspan",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "L Ayzenberg",
        "id": "xk5BTdUAAAAJ"
      },
      {
        "name": "R Giryes",
        "id": "9aQUYVQAAAAJ"
      },
      {
        "name": "H Greenspan2024 IEEE International",
        "id": null
      }
    ],
    "citation_count": 18
  },
  {
    "arxiv_id": "2404.19311",
    "title": "A Light-weight Transformer-based Self-supervised Matching Network for Heterogeneous Images",
    "year": 2024,
    "published": "2024-04-30T07:30:33Z",
    "categories": [
      "cs.CV",
      "cs.MM"
    ],
    "abstract": "Matching visible and near-infrared (NIR) images remains a significant challenge in remote sensing image fusion. The nonlinear radiometric differences between heterogeneous remote sensing images make the image matching task even more difficult. Deep learning has gained substantial attention in computer vision tasks in recent years. However, many methods rely on supervised learning and necessitate large amounts of annotated data. Nevertheless, annotated data is frequently limited in the field of r",
    "arxiv_url": "https://arxiv.org/abs/2404.19311v1",
    "pdf_url": "https://arxiv.org/pdf/2404.19311v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2404.19311",
    "arxiv_authors": [
      "Wang Zhang",
      "Tingting Li",
      "Yuntian Zhang",
      "Gensheng Pei",
      "Xiruo Jiang",
      "Yazhou Yao"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=A+Light-weight+Transformer-based+Self-supervised+Matching+Network+for+Heterogeneous+Images+Wang+Zhang+Tingting+Li+Yuntian+Zhang+Gensheng+Pei+Xiruo+Jiang",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "W Zhang",
        "id": "otmSeowAAAAJ"
      },
      {
        "name": "T Li",
        "id": null
      },
      {
        "name": "Y Zhang",
        "id": null
      },
      {
        "name": "G Pei",
        "id": null
      },
      {
        "name": "X Jiang",
        "id": null
      },
      {
        "name": "Y Yao",
        "id": "_3Ucwv4AAAAJ"
      }
    ],
    "citation_count": null
  },
  {
    "arxiv_id": "2410.09380",
    "title": "Prompting Video-Language Foundation Models with Domain-specific Fine-grained Heuristics for Video Question Answering",
    "year": 2024,
    "published": "2024-10-12T06:22:23Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Video Question Answering (VideoQA) represents a crucial intersection between video understanding and language processing, requiring both discriminative unimodal comprehension and sophisticated cross-modal interaction for accurate inference. Despite advancements in multi-modal pre-trained models and video-language foundation models, these systems often struggle with domain-specific VideoQA due to their generalized pre-training objectives. Addressing this gap necessitates bridging the divide betwe",
    "arxiv_url": "https://arxiv.org/abs/2410.09380v1",
    "pdf_url": "https://arxiv.org/pdf/2410.09380v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2410.09380",
    "arxiv_authors": [
      "Ting Yu",
      "Kunhao Fu",
      "Shuhui Wang",
      "Qingming Huang",
      "Jun Yu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Prompting+Video-Language+Foundation+Models+with+Domain-specific+Fine-grained+Heuristics+for+Video+Question+Answering+Ting+Yu+Kunhao+Fu+Shuhui+Wang+Qingming+Huang+Jun+Yu",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "T Yu",
        "id": "01_-U40AAAAJ"
      },
      {
        "name": "K Fu",
        "id": "457To0EAAAAJ"
      },
      {
        "name": "S Wang",
        "id": "h-JxBSYAAAAJ"
      },
      {
        "name": "Q Huang",
        "id": "J1vMnRgAAAAJ"
      },
      {
        "name": "J YuIEEE Transactions on Circuits and Systems for Video Technology",
        "id": null
      }
    ],
    "citation_count": 9
  },
  {
    "arxiv_id": "2405.10175",
    "title": "Filling Missing Values Matters for Range Image-Based Point Cloud Segmentation",
    "year": 2024,
    "published": "2024-05-16T15:13:42Z",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "abstract": "Point cloud segmentation (PCS) plays an essential role in robot perception and navigation tasks. To efficiently understand large-scale outdoor point clouds, their range image representation is commonly adopted. This image-like representation is compact and structured, making range image-based PCS models practical. However, undesirable missing values in the range images damage the shapes and patterns of objects. This problem creates difficulty for the models in learning coherent and complete geom",
    "arxiv_url": "https://arxiv.org/abs/2405.10175v2",
    "pdf_url": "https://arxiv.org/pdf/2405.10175v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2405.10175",
    "arxiv_authors": [
      "Bike Chen",
      "Chen Gong",
      "Juha R√∂ning"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Filling+Missing+Values+Matters+for+Range+Image-Based+Point+Cloud+Segmentation+Bike+Chen+Chen+Gong+Juha+R%C3%B6ning",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "B Chen",
        "id": null
      },
      {
        "name": "C Gong",
        "id": "guttoBwAAAAJ"
      },
      {
        "name": "J R√∂ning - IEEE Transactions on Intelligent",
        "id": null
      }
    ],
    "citation_count": 4
  },
  {
    "arxiv_id": "2310.18285",
    "title": "Unlocking the Potential of Prompt-Tuning in Bridging Generalized and Personalized Federated Learning",
    "year": 2023,
    "published": "2023-10-27T17:22:09Z",
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "abstract": "Vision Transformers (ViT) and Visual Prompt Tuning (VPT) achieve state-of-the-art performance with improved efficiency in various computer vision tasks. This suggests a promising paradigm shift of adapting pre-trained ViT models to Federated Learning (FL) settings. However, the challenge of data heterogeneity among FL clients presents a significant hurdle in effectively deploying ViT models. Existing Generalized FL (GFL) and Personalized FL (PFL) methods have limitations in balancing performance",
    "arxiv_url": "https://arxiv.org/abs/2310.18285v4",
    "pdf_url": "https://arxiv.org/pdf/2310.18285v4",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2310.18285",
    "arxiv_authors": [
      "Wenlong Deng",
      "Christos Thrampoulidis",
      "Xiaoxiao Li"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Unlocking+the+Potential+of+Prompt-Tuning+in+Bridging+Generalized+and+Personalized+Federated+Learning+Wenlong+Deng+Christos+Thrampoulidis+Xiaoxiao+Li",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "W Deng",
        "id": "qckUjrMAAAAJ"
      },
      {
        "name": "C Thrampoulidis",
        "id": "X54nJqcAAAAJ"
      },
      {
        "name": "X Li -",
        "id": null
      }
    ],
    "citation_count": 28
  },
  {
    "arxiv_id": "2409.16292",
    "title": "Explaining Human Comparisons using Alignment-Importance Heatmaps",
    "year": 2024,
    "published": "2024-09-08T08:28:09Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "We present a computational explainability approach for human comparison tasks, using Alignment Importance Score (AIS) heatmaps derived from deep-vision models. The AIS reflects a feature-map's unique contribution to the alignment between Deep Neural Network's (DNN) representational geometry and that of humans. We first validate the AIS by showing that prediction of out-of-sample human similarity judgments is improved when constructing representations using only higher-scoring AIS feature maps id",
    "arxiv_url": "https://arxiv.org/abs/2409.16292v1",
    "pdf_url": "https://arxiv.org/pdf/2409.16292v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2409.16292",
    "arxiv_authors": [
      "Nhut Truong",
      "Dario Pesenti",
      "Uri Hasson"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Explaining+Human+Comparisons+using+Alignment-Importance+Heatmaps+Nhut+Truong+Dario+Pesenti+Uri+Hasson",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "N Truong",
        "id": "vs1cgLcAAAAJ"
      },
      {
        "name": "D Pesenti",
        "id": "8RKaiA4AAAAJ"
      }
    ],
    "citation_count": 3
  },
  {
    "arxiv_id": "2501.12319",
    "title": "Metric for Evaluating Performance of Reference-Free Demorphing Methods",
    "year": 2025,
    "published": "2025-01-21T17:38:55Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "A facial morph is an image created by combining two (or more) face images pertaining to two (or more) distinct identities. Reference-free face demorphing inverts the process and tries to recover the face images constituting a facial morph without using any other information. However, there is no consensus on the evaluation metrics to be used to evaluate and compare such demorphing techniques. In this paper, we first analyze the shortcomings of the demorphing metrics currently used in the literat",
    "arxiv_url": "https://arxiv.org/abs/2501.12319v1",
    "pdf_url": "https://arxiv.org/pdf/2501.12319v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2501.12319",
    "arxiv_authors": [
      "Nitish Shukla",
      "Arun Ross"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Metric+for+Evaluating+Performance+of+Reference-Free+Demorphing+Methods+Nitish+Shukla+Arun+Ross",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "N Shukla",
        "id": "pAedm2oAAAAJ"
      },
      {
        "name": "A Ross -",
        "id": null
      }
    ],
    "citation_count": 3
  },
  {
    "arxiv_id": "2407.00921",
    "title": "PointViG: A Lightweight GNN-based Model for Efficient Point Cloud Analysis",
    "year": 2024,
    "published": "2024-07-01T02:55:45Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "In the domain of point cloud analysis, despite the significant capabilities of Graph Neural Networks (GNNs) in managing complex 3D datasets, existing approaches encounter challenges like high computational costs and scalability issues with extensive scenarios. These limitations restrict the practical deployment of GNNs, notably in resource-constrained environments. To address these issues, this study introduce <b>Point<\\b> <b>Vi<\\b>sion <b>G<\\b>NN (PointViG), an efficient framework for point clo",
    "arxiv_url": "https://arxiv.org/abs/2407.00921v2",
    "pdf_url": "https://arxiv.org/pdf/2407.00921v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2407.00921",
    "arxiv_authors": [
      "Qiang Zheng",
      "Yafei Qi",
      "Chen Wang",
      "Chao Zhang",
      "Jian Sun"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=PointViG%3A+A+Lightweight+GNN-based+Model+for+Efficient+Point+Cloud+Analysis+Qiang+Zheng+Yafei+Qi+Chen+Wang+Chao+Zhang+Jian+Sun",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "Q Zheng",
        "id": null
      },
      {
        "name": "Y Qi",
        "id": null
      },
      {
        "name": "C Wang",
        "id": "CHdZV6sAAAAJ"
      },
      {
        "name": "C Zhang",
        "id": null
      },
      {
        "name": "J Sun -",
        "id": null
      }
    ],
    "citation_count": 4
  },
  {
    "arxiv_id": "2311.16476",
    "title": "LANS: A Layout-Aware Neural Solver for Plane Geometry Problem",
    "year": 2023,
    "published": "2023-11-25T04:11:19Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Geometry problem solving (GPS) is a challenging mathematical reasoning task requiring multi-modal understanding, fusion, and reasoning. Existing neural solvers take GPS as a vision-language task but are short in the representation of geometry diagrams that carry rich and complex layout information. In this paper, we propose a layout-aware neural solver named LANS, integrated with two new modules: multimodal layout-aware pre-trained language module (MLA-PLM) and layout-aware fusion attention (LA-",
    "arxiv_url": "https://arxiv.org/abs/2311.16476v2",
    "pdf_url": "https://arxiv.org/pdf/2311.16476v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2311.16476",
    "arxiv_authors": [
      "Zhong-Zhi Li",
      "Ming-Liang Zhang",
      "Fei Yin",
      "Cheng-Lin Liu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=LANS%3A+A+Layout-Aware+Neural+Solver+for+Plane+Geometry+Problem+Zhong-Zhi+Li+Ming-Liang+Zhang+Fei+Yin+Cheng-Lin+Liu",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "ZZ Li",
        "id": "wmaFBB8AAAAJ"
      },
      {
        "name": "ML Zhang",
        "id": "Laj7AiIAAAAJ"
      },
      {
        "name": "F Yin",
        "id": "CeWCYJYAAAAJ"
      },
      {
        "name": "CL Liu - Findings of the Association for",
        "id": null
      }
    ],
    "citation_count": 24
  },
  {
    "arxiv_id": "2505.06166",
    "title": "DiffLocks: Generating 3D Hair from a Single Image using Diffusion Models",
    "year": 2025,
    "published": "2025-05-09T16:16:42Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "We address the task of generating 3D hair geometry from a single image, which is challenging due to the diversity of hairstyles and the lack of paired image-to-3D hair data. Previous methods are primarily trained on synthetic data and cope with the limited amount of such data by using low-dimensional intermediate representations, such as guide strands and scalp-level embeddings, that require post-processing to decode, upsample, and add realism. These approaches fail to reconstruct detailed hair,",
    "arxiv_url": "https://arxiv.org/abs/2505.06166v1",
    "pdf_url": "https://arxiv.org/pdf/2505.06166v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2505.06166",
    "arxiv_authors": [
      "Radu Alexandru Rosu",
      "Keyu Wu",
      "Yao Feng",
      "Youyi Zheng",
      "Michael J. Black"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=DiffLocks%3A+Generating+3D+Hair+from+a+Single+Image+using+Diffusion+Models+Radu+Alexandru+Rosu+Keyu+Wu+Yao+Feng+Youyi+Zheng+Michael+J.+Black",
    "gs_search_success": false,
    "gs_authors": null,
    "citation_count": null,
    "error_type": "multiple_matches"
  },
  {
    "arxiv_id": "2402.01169",
    "title": "Faster Inference of Integer SWIN Transformer by Removing the GELU Activation",
    "year": 2024,
    "published": "2024-02-02T06:23:00Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "SWIN transformer is a prominent vision transformer model that has state-of-the-art accuracy in image classification tasks. Despite this success, its unique architecture causes slower inference compared with similar deep neural networks. Integer quantization of the model is one of the methods used to improve its inference latency. However, state-of-the-art has not been able to fully quantize the model. In this work, we improve upon the inference latency of the state-of-the-art methods by removing",
    "arxiv_url": "https://arxiv.org/abs/2402.01169v1",
    "pdf_url": "https://arxiv.org/pdf/2402.01169v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2402.01169",
    "arxiv_authors": [
      "Mohammadreza Tayaranian",
      "Seyyed Hasan Mozafari",
      "James J. Clark",
      "Brett Meyer",
      "Warren Gross"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Faster+Inference+of+Integer+SWIN+Transformer+by+Removing+the+GELU+Activation+Mohammadreza+Tayaranian+Seyyed+Hasan+Mozafari+James+J.+Clark+Brett+Meyer+Warren+Gross",
    "gs_search_success": false,
    "gs_authors": null,
    "citation_count": null,
    "error_type": "multiple_matches"
  },
  {
    "arxiv_id": "2306.16060",
    "title": "Dynamic Path-Controllable Deep Unfolding Network for Compressive Sensing",
    "year": 2023,
    "published": "2023-06-28T09:49:15Z",
    "categories": [
      "cs.CV",
      "eess.IV"
    ],
    "abstract": "Deep unfolding network (DUN) that unfolds the optimization algorithm into a deep neural network has achieved great success in compressive sensing (CS) due to its good interpretability and high performance. Each stage in DUN corresponds to one iteration in optimization. At the test time, all the sampling images generally need to be processed by all stages, which comes at a price of computation burden and is also unnecessary for the images whose contents are easier to restore. In this paper, we fo",
    "arxiv_url": "https://arxiv.org/abs/2306.16060v2",
    "pdf_url": "https://arxiv.org/pdf/2306.16060v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2306.16060",
    "arxiv_authors": [
      "Jiechong Song",
      "Bin Chen",
      "Jian Zhang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Dynamic+Path-Controllable+Deep+Unfolding+Network+for+Compressive+Sensing+Jiechong+Song+Bin+Chen+Jian+Zhang",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "J Song",
        "id": "EBOtupAAAAAJ"
      },
      {
        "name": "B Chen",
        "id": "aZDNm98AAAAJ"
      },
      {
        "name": "J Zhang - IEEE Transactions on Image",
        "id": null
      }
    ],
    "citation_count": 92
  },
  {
    "arxiv_id": "2407.05712",
    "title": "MobilePortrait: Real-Time One-Shot Neural Head Avatars on Mobile Devices",
    "year": 2024,
    "published": "2024-07-08T08:12:57Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Existing neural head avatars methods have achieved significant progress in the image quality and motion range of portrait animation. However, these methods neglect the computational overhead, and to the best of our knowledge, none is designed to run on mobile devices. This paper presents MobilePortrait, a lightweight one-shot neural head avatars method that reduces learning complexity by integrating external knowledge into both the motion modeling and image synthesis, enabling real-time inferenc",
    "arxiv_url": "https://arxiv.org/abs/2407.05712v3",
    "pdf_url": "https://arxiv.org/pdf/2407.05712v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2407.05712",
    "arxiv_authors": [
      "Jianwen Jiang",
      "Gaojie Lin",
      "Zhengkun Rong",
      "Chao Liang",
      "Yongming Zhu",
      "Jiaqi Yang",
      "Tianyun Zhong"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=MobilePortrait%3A+Real-Time+One-Shot+Neural+Head+Avatars+on+Mobile+Devices+Jianwen+Jiang+Gaojie+Lin+Zhengkun+Rong+Chao+Liang+Yongming+Zhu",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "J Jiang",
        "id": null
      },
      {
        "name": "G Lin",
        "id": "ADLvtYMAAAAJ"
      },
      {
        "name": "Z Rong",
        "id": "t_zugcoAAAAJ"
      },
      {
        "name": "C Liang",
        "id": null
      },
      {
        "name": "Y Zhu",
        "id": null
      },
      {
        "name": "J Yang",
        "id": "nsi2dIUAAAAJ"
      },
      {
        "name": "T Zhong",
        "id": "SGCGnmsAAAAJ"
      }
    ],
    "citation_count": 13
  },
  {
    "arxiv_id": "2505.21635",
    "title": "Object Concepts Emerge from Motion",
    "year": 2025,
    "published": "2025-05-27T18:09:02Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Object concepts play a foundational role in human visual cognition, enabling perception, memory, and interaction in the physical world. Inspired by findings in developmental neuroscience - where infants are shown to acquire object understanding through observation of motion - we propose a biologically inspired framework for learning object-centric visual representations in an unsupervised manner. Our key insight is that motion boundary serves as a strong signal for object-level grouping, which c",
    "arxiv_url": "https://arxiv.org/abs/2505.21635v1",
    "pdf_url": "https://arxiv.org/pdf/2505.21635v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2505.21635",
    "arxiv_authors": [
      "Haoqian Liang",
      "Xiaohui Wang",
      "Zhichao Li",
      "Ya Yang",
      "Naiyan Wang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Object+Concepts+Emerge+from+Motion+Haoqian+Liang+Xiaohui+Wang+Zhichao+Li+Ya+Yang+Naiyan+Wang",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "H Liang",
        "id": null
      },
      {
        "name": "X Wang",
        "id": null
      },
      {
        "name": "Z Li",
        "id": null
      },
      {
        "name": "Y Yang",
        "id": null
      },
      {
        "name": "N Wang -",
        "id": null
      }
    ],
    "citation_count": null
  },
  {
    "arxiv_id": "2309.10511",
    "title": "Self2Seg: Single-Image Self-Supervised Joint Segmentation and Denoising",
    "year": 2023,
    "published": "2023-09-19T10:47:32Z",
    "categories": [
      "cs.CV",
      "cs.LG",
      "math.OC"
    ],
    "abstract": "We develop Self2Seg, a self-supervised method for the joint segmentation and denoising of a single image. To this end, we combine the advantages of variational segmentation with self-supervised deep learning. One major benefit of our method lies in the fact, that in contrast to data-driven methods, where huge amounts of labeled samples are necessary, Self2Seg segments an image into meaningful regions without any training database. Moreover, we demonstrate that self-supervised denoising itself is",
    "arxiv_url": "https://arxiv.org/abs/2309.10511v2",
    "pdf_url": "https://arxiv.org/pdf/2309.10511v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2309.10511",
    "arxiv_authors": [
      "Nadja Gruber",
      "Johannes Schwab",
      "No√©mie Debroux",
      "Nicolas Papadakis",
      "Markus Haltmeier"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Self2Seg%3A+Single-Image+Self-Supervised+Joint+Segmentation+and+Denoising+Nadja+Gruber+Johannes+Schwab+No%C3%A9mie+Debroux+Nicolas+Papadakis+Markus+Haltmeier",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "N Gruber",
        "id": "klIjVWwAAAAJ"
      },
      {
        "name": "J Schwab",
        "id": "qhevh4EAAAAJ"
      },
      {
        "name": "N Debroux",
        "id": null
      },
      {
        "name": "N Papadakis",
        "id": "hfyLiLYAAAAJ"
      },
      {
        "name": "M Haltmeier",
        "id": "K-Apf0sAAAAJ"
      }
    ],
    "citation_count": 1
  },
  {
    "arxiv_id": "2403.17006",
    "title": "Invertible Diffusion Models for Compressed Sensing",
    "year": 2024,
    "published": "2024-03-25T17:59:41Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "While deep neural networks (NN) significantly advance image compressed sensing (CS) by improving reconstruction quality, the necessity of training current CS NNs from scratch constrains their effectiveness and hampers rapid deployment. Although recent methods utilize pre-trained diffusion models for image reconstruction, they struggle with slow inference and restricted adaptability to CS. To tackle these challenges, this paper proposes Invertible Diffusion Models (IDM), a novel efficient, end-to",
    "arxiv_url": "https://arxiv.org/abs/2403.17006v2",
    "pdf_url": "https://arxiv.org/pdf/2403.17006v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2403.17006",
    "arxiv_authors": [
      "Bin Chen",
      "Zhenyu Zhang",
      "Weiqi Li",
      "Chen Zhao",
      "Jiwen Yu",
      "Shijie Zhao",
      "Jie Chen",
      "Jian Zhang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Invertible+Diffusion+Models+for+Compressed+Sensing+Bin+Chen+Zhenyu+Zhang+Weiqi+Li+Chen+Zhao+Jiwen+Yu",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "B Chen",
        "id": "aZDNm98AAAAJ"
      },
      {
        "name": "Z Zhang",
        "id": "4TbicrcAAAAJ"
      },
      {
        "name": "W Li",
        "id": "SIkQdEsAAAAJ"
      },
      {
        "name": "C Zhao",
        "id": "dUWdX5EAAAAJ"
      },
      {
        "name": "J Yu",
        "id": "uoRPLHIAAAAJ"
      },
      {
        "name": "S Zhao",
        "id": null
      },
      {
        "name": "J Chen",
        "id": null
      },
      {
        "name": "J ZhangIEEE Transactions on Pattern Analysis and Machine Intelligence",
        "id": null
      }
    ],
    "citation_count": 23
  },
  {
    "arxiv_id": "2309.04650",
    "title": "Exploring Robust Features for Improving Adversarial Robustness",
    "year": 2023,
    "published": "2023-09-09T00:30:04Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "While deep neural networks (DNNs) have revolutionized many fields, their fragility to carefully designed adversarial attacks impedes the usage of DNNs in safety-critical applications. In this paper, we strive to explore the robust features which are not affected by the adversarial perturbations, i.e., invariant to the clean image and its adversarial examples, to improve the model's adversarial robustness. Specifically, we propose a feature disentanglement model to segregate the robust features f",
    "arxiv_url": "https://arxiv.org/abs/2309.04650v1",
    "pdf_url": "https://arxiv.org/pdf/2309.04650v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2309.04650",
    "arxiv_authors": [
      "Hong Wang",
      "Yuefan Deng",
      "Shinjae Yoo",
      "Yuewei Lin"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Exploring+Robust+Features+for+Improving+Adversarial+Robustness+Hong+Wang+Yuefan+Deng+Shinjae+Yoo+Yuewei+Lin",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "H Wang",
        "id": "g1VQVeEAAAAJ"
      },
      {
        "name": "Y Deng",
        "id": "9k2KwC4AAAAJ"
      },
      {
        "name": "S Yoo",
        "id": "KFYsgnIAAAAJ"
      },
      {
        "name": "Y Lin - IEEE Transactions on",
        "id": null
      }
    ],
    "citation_count": 9
  },
  {
    "arxiv_id": "2401.11541",
    "title": "Multi-View Neural 3D Reconstruction of Micro-/Nanostructures with Atomic Force Microscopy",
    "year": 2024,
    "published": "2024-01-21T16:46:04Z",
    "categories": [
      "cs.CV",
      "cond-mat.mtrl-sci"
    ],
    "abstract": "Atomic Force Microscopy (AFM) is a widely employed tool for micro-/nanoscale topographic imaging. However, conventional AFM scanning struggles to reconstruct complex 3D micro-/nanostructures precisely due to limitations such as incomplete sample topography capturing and tip-sample convolution artifacts. Here, we propose a multi-view neural-network-based framework with AFM (MVN-AFM), which accurately reconstructs surface models of intricate micro-/nanostructures. Unlike previous works, MVN-AFM do",
    "arxiv_url": "https://arxiv.org/abs/2401.11541v1",
    "pdf_url": "https://arxiv.org/pdf/2401.11541v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2401.11541",
    "arxiv_authors": [
      "Shuo Chen",
      "Mao Peng",
      "Yijin Li",
      "Bing-Feng Ju",
      "Hujun Bao",
      "Yuan-Liu Chen",
      "Guofeng Zhang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Multi-View+Neural+3D+Reconstruction+of+Micro-%2FNanostructures+with+Atomic+Force+Microscopy+Shuo+Chen+Mao+Peng+Yijin+Li+Bing-Feng+Ju+Hujun+Bao",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "S Chen",
        "id": null
      },
      {
        "name": "M Peng",
        "id": null
      },
      {
        "name": "Y Li",
        "id": "KCooJasAAAAJ"
      },
      {
        "name": "BF Ju",
        "id": null
      },
      {
        "name": "H Bao",
        "id": null
      },
      {
        "name": "YL Chen",
        "id": null
      }
    ],
    "citation_count": 10
  },
  {
    "arxiv_id": "2503.14530",
    "title": "SAUCE: Selective Concept Unlearning in Vision-Language Models with Sparse Autoencoders",
    "year": 2025,
    "published": "2025-03-16T17:32:23Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Unlearning methods for vision-language models (VLMs) have primarily adapted techniques from large language models (LLMs), relying on weight updates that demand extensive annotated forget sets. Moreover, these methods perform unlearning at a coarse granularity, often leading to excessive forgetting and reduced model utility. To address this issue, we introduce SAUCE, a novel method that leverages sparse autoencoders (SAEs) for fine-grained and selective concept unlearning in VLMs. Briefly, SAUCE ",
    "arxiv_url": "https://arxiv.org/abs/2503.14530v2",
    "pdf_url": "https://arxiv.org/pdf/2503.14530v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2503.14530",
    "arxiv_authors": [
      "Qing Li",
      "Jiahui Geng",
      "Derui Zhu",
      "Fengyu Cai",
      "Chenyang Lyu",
      "Fakhri Karray"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=SAUCE%3A+Selective+Concept+Unlearning+in+Vision-Language+Models+with+Sparse+Autoencoders+Qing+Li+Jiahui+Geng+Derui+Zhu+Fengyu+Cai+Chenyang+Lyu",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "Q Li",
        "id": "CFzKpEUAAAAJ"
      },
      {
        "name": "J Geng",
        "id": "eMC-gQUAAAAJ"
      },
      {
        "name": "D Zhu",
        "id": "iELr97kAAAAJ"
      },
      {
        "name": "F Cai",
        "id": "DMycNA4AAAAJ"
      },
      {
        "name": "C Lyu",
        "id": "0n7cAw0AAAAJ"
      },
      {
        "name": "F Karray -",
        "id": null
      }
    ],
    "citation_count": 3
  },
  {
    "arxiv_id": "2502.21054",
    "title": "HoloMine: A Synthetic Dataset for Buried Landmines Recognition using Microwave Holographic Imaging",
    "year": 2025,
    "published": "2025-02-28T13:53:35Z",
    "categories": [
      "cs.CV",
      "eess.IV",
      "eess.SP"
    ],
    "abstract": "The detection and removal of landmines is a complex and risky task that requires advanced remote sensing techniques to reduce the risk for the professionals involved in this task. In this paper, we propose a novel synthetic dataset for buried landmine detection to provide researchers with a valuable resource to observe, measure, locate, and address issues in landmine detection. The dataset consists of 41,800 microwave holographic images (2D) and their holographic inverted scans (3D) of different",
    "arxiv_url": "https://arxiv.org/abs/2502.21054v1",
    "pdf_url": "https://arxiv.org/pdf/2502.21054v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2502.21054",
    "arxiv_authors": [
      "Emanuele Vivoli",
      "Lorenzo Capineri",
      "Marco Bertini"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=HoloMine%3A+A+Synthetic+Dataset+for+Buried+Landmines+Recognition+using+Microwave+Holographic+Imaging+Emanuele+Vivoli+Lorenzo+Capineri+Marco+Bertini",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "E Vivoli",
        "id": "BCzPjawAAAAJ"
      },
      {
        "name": "L Capineri",
        "id": "oLL3PrwAAAAJ"
      },
      {
        "name": "M Bertini - IEEE",
        "id": null
      }
    ],
    "citation_count": 3
  },
  {
    "arxiv_id": "2505.18010",
    "title": "Clinical Validation of Deep Learning for Real-Time Tissue Oxygenation Estimation Using Spectral Imaging",
    "year": 2025,
    "published": "2025-05-23T15:14:27Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Accurate, real-time monitoring of tissue ischemia is crucial to understand tissue health and guide surgery. Spectral imaging shows great potential for contactless and intraoperative monitoring of tissue oxygenation. Due to the difficulty of obtaining direct reference oxygenation values, conventional methods are based on linear unmixing techniques. These are prone to assumptions and these linear relations may not always hold in practice. In this work, we present deep learning approaches for real-",
    "arxiv_url": "https://arxiv.org/abs/2505.18010v1",
    "pdf_url": "https://arxiv.org/pdf/2505.18010v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2505.18010",
    "arxiv_authors": [
      "Jens De Winne",
      "Siri Willems",
      "Siri Luthman",
      "Danilo Babin",
      "Hiep Luong",
      "Wim Ceelen"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Clinical+Validation+of+Deep+Learning+for+Real-Time+Tissue+Oxygenation+Estimation+Using+Spectral+Imaging+Jens+De+Winne+Siri+Willems+Siri+Luthman+Danilo+Babin+Hiep+Luong",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "J De Winne",
        "id": "s5EflNwAAAAJ"
      },
      {
        "name": "S Willems",
        "id": "45k9vvQAAAAJ"
      },
      {
        "name": "S Luthman",
        "id": null
      },
      {
        "name": "D Babin",
        "id": "TDFyCXsAAAAJ"
      },
      {
        "name": "H Luong",
        "id": "Who7FY8AAAAJ"
      },
      {
        "name": "W CeelenInternational",
        "id": null
      }
    ],
    "citation_count": null
  },
  {
    "arxiv_id": "2312.09754",
    "title": "PPFM: Image denoising in photon-counting CT using single-step posterior sampling Poisson flow generative models",
    "year": 2023,
    "published": "2023-12-15T12:49:08Z",
    "categories": [
      "eess.IV",
      "cs.CV",
      "physics.med-ph"
    ],
    "abstract": "Diffusion and Poisson flow models have shown impressive performance in a wide range of generative tasks, including low-dose CT image denoising. However, one limitation in general, and for clinical applications in particular, is slow sampling. Due to their iterative nature, the number of function evaluations (NFE) required is usually on the order of $10-10^3$, both for conditional and unconditional generation. In this paper, we present posterior sampling Poisson flow generative models (PPFM), a n",
    "arxiv_url": "https://arxiv.org/abs/2312.09754v2",
    "pdf_url": "https://arxiv.org/pdf/2312.09754v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2312.09754",
    "arxiv_authors": [
      "Dennis Hein",
      "Staffan Holmin",
      "Timothy Szczykutowicz",
      "Jonathan S Maltz",
      "Mats Danielsson",
      "Ge Wang",
      "Mats Persson"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=PPFM%3A+Image+denoising+in+photon-counting+CT+using+single-step+posterior+sampling+Poisson+flow+generative+models+Dennis+Hein+Staffan+Holmin+Timothy+Szczykutowicz+Jonathan+S+Maltz+Mats+Danielsson",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "D Hein",
        "id": null
      },
      {
        "name": "S Holmin",
        "id": null
      },
      {
        "name": "T Szczykutowicz",
        "id": "ovZGXjMAAAAJ"
      },
      {
        "name": "JS Maltz",
        "id": "nLDzUGUAAAAJ"
      },
      {
        "name": "M Danielsson",
        "id": "62JsXMIAAAAJ"
      },
      {
        "name": "G Wang",
        "id": "pjK2mQwAAAAJ"
      },
      {
        "name": "M PerssonIEEE Transactions on Radiation and Plasma Medical Sciences",
        "id": null
      }
    ],
    "citation_count": 10
  },
  {
    "arxiv_id": "2304.14403",
    "title": "Make It So: Steering StyleGAN for Any Image Inversion and Editing",
    "year": 2023,
    "published": "2023-04-27T17:59:24Z",
    "categories": [
      "cs.CV",
      "cs.GR",
      "cs.LG"
    ],
    "abstract": "StyleGAN's disentangled style representation enables powerful image editing by manipulating the latent variables, but accurately mapping real-world images to their latent variables (GAN inversion) remains a challenge. Existing GAN inversion methods struggle to maintain editing directions and produce realistic results.   To address these limitations, we propose Make It So, a novel GAN inversion method that operates in the $\\mathcal{Z}$ (noise) space rather than the typical $\\mathcal{W}$ (latent s",
    "arxiv_url": "https://arxiv.org/abs/2304.14403v1",
    "pdf_url": "https://arxiv.org/pdf/2304.14403v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2304.14403",
    "arxiv_authors": [
      "Anand Bhattad",
      "Viraj Shah",
      "Derek Hoiem",
      "D. A. Forsyth"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Make+It+So%3A+Steering+StyleGAN+for+Any+Image+Inversion+and+Editing+Anand+Bhattad+Viraj+Shah+Derek+Hoiem+D.+A.+Forsyth",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "A Bhattad",
        "id": "XUsauXIAAAAJ"
      },
      {
        "name": "V Shah",
        "id": "MFRFeKMAAAAJ"
      },
      {
        "name": "D Hoiem",
        "id": "8Sfj7q8AAAAJ"
      },
      {
        "name": "DA Forsyth -",
        "id": null
      }
    ],
    "citation_count": 14
  },
  {
    "arxiv_id": "2408.08149",
    "title": "Unsupervised Variational Translator for Bridging Image Restoration and High-Level Vision Tasks",
    "year": 2024,
    "published": "2024-08-15T13:35:59Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Recent research tries to extend image restoration capabilities from human perception to machine perception, thereby enhancing the performance of high-level vision tasks in degraded environments. These methods, primarily based on supervised learning, typically involve the retraining of restoration networks or high-level vision networks. However, collecting paired data in real-world scenarios and retraining large-scale models are challenge. To this end, we propose an unsupervised learning method c",
    "arxiv_url": "https://arxiv.org/abs/2408.08149v3",
    "pdf_url": "https://arxiv.org/pdf/2408.08149v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2408.08149",
    "arxiv_authors": [
      "Jiawei Wu",
      "Zhi Jin"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Unsupervised+Variational+Translator+for+Bridging+Image+Restoration+and+High-Level+Vision+Tasks+Jiawei+Wu+Zhi+Jin",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "J Wu",
        "id": "wwHeXF0AAAAJ"
      },
      {
        "name": "Z Jin - European",
        "id": null
      }
    ],
    "citation_count": 3
  },
  {
    "arxiv_id": "2307.15569",
    "title": "Point Clouds Are Specialized Images: A Knowledge Transfer Approach for 3D Understanding",
    "year": 2023,
    "published": "2023-07-28T14:04:54Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Self-supervised representation learning (SSRL) has gained increasing attention in point cloud understanding, in addressing the challenges posed by 3D data scarcity and high annotation costs. This paper presents PCExpert, a novel SSRL approach that reinterprets point clouds as \"specialized images\". This conceptual shift allows PCExpert to leverage knowledge derived from large-scale image modality in a more direct and deeper manner, via extensively sharing the parameters with a pre-trained image e",
    "arxiv_url": "https://arxiv.org/abs/2307.15569v2",
    "pdf_url": "https://arxiv.org/pdf/2307.15569v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2307.15569",
    "arxiv_authors": [
      "Jiachen Kang",
      "Wenjing Jia",
      "Xiangjian He",
      "Kin Man Lam"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Point+Clouds+Are+Specialized+Images%3A+A+Knowledge+Transfer+Approach+for+3D+Understanding+Jiachen+Kang+Wenjing+Jia+Xiangjian+He+Kin+Man+Lam",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "J Kang",
        "id": null
      },
      {
        "name": "W Jia",
        "id": "BzXhftkAAAAJ"
      },
      {
        "name": "X He",
        "id": "BiBXGfIAAAAJ"
      },
      {
        "name": "KM Lam - IEEE Transactions on",
        "id": null
      }
    ],
    "citation_count": 7
  },
  {
    "arxiv_id": "2503.11008",
    "title": "Comparative Analysis of Advanced AI-based Object Detection Models for Pavement Marking Quality Assessment during Daytime",
    "year": 2025,
    "published": "2025-03-14T02:06:46Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Visual object detection utilizing deep learning plays a vital role in computer vision and has extensive applications in transportation engineering. This paper focuses on detecting pavement marking quality during daytime using the You Only Look Once (YOLO) model, leveraging its advanced architectural features to enhance road safety through precise and real-time assessments. Utilizing image data from New Jersey, this study employed three YOLOv8 variants: YOLOv8m, YOLOv8n, and YOLOv8x. The models w",
    "arxiv_url": "https://arxiv.org/abs/2503.11008v2",
    "pdf_url": "https://arxiv.org/pdf/2503.11008v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2503.11008",
    "arxiv_authors": [
      "Gian Antariksa",
      "Rohit Chakraborty",
      "Shriyank Somvanshi",
      "Subasish Das",
      "Mohammad Jalayer",
      "Deep Rameshkumar Patel",
      "David Mills"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Comparative+Analysis+of+Advanced+AI-based+Object+Detection+Models+for+Pavement+Marking+Quality+Assessment+during+Daytime+Gian+Antariksa+Rohit+Chakraborty+Shriyank+Somvanshi+Subasish+Das+Mohammad+Jalayer",
    "gs_search_success": false,
    "gs_authors": null,
    "citation_count": null,
    "error_type": "fetch_failed"
  },
  {
    "arxiv_id": "2305.19939",
    "title": "Image Registration of In Vivo Micro-Ultrasound and Ex Vivo Pseudo-Whole Mount Histopathology Images of the Prostate: A Proof-of-Concept Study",
    "year": 2023,
    "published": "2023-05-31T15:22:58Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "eess.IV"
    ],
    "abstract": "Early diagnosis of prostate cancer significantly improves a patient's 5-year survival rate. Biopsy of small prostate cancers is improved with image-guided biopsy. MRI-ultrasound fusion-guided biopsy is sensitive to smaller tumors but is underutilized due to the high cost of MRI and fusion equipment. Micro-ultrasound (micro-US), a novel high-resolution ultrasound technology, provides a cost-effective alternative to MRI while delivering comparable diagnostic accuracy. However, the interpretation o",
    "arxiv_url": "https://arxiv.org/abs/2305.19939v2",
    "pdf_url": "https://arxiv.org/pdf/2305.19939v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2305.19939",
    "arxiv_authors": [
      "Muhammad Imran",
      "Brianna Nguyen",
      "Jake Pensa",
      "Sara M. Falzarano",
      "Anthony E. Sisk",
      "Muxuan Liang",
      "John Michael DiBianco",
      "Li-Ming Su",
      "Yuyin Zhou",
      "Wayne G. Brisbane",
      "Wei Shao"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Image+Registration+of+In+Vivo+Micro-Ultrasound+and+Ex+Vivo+Pseudo-Whole+Mount+Histopathology+Images+of+the+Prostate%3A+A+Proof-of-Concept+Study+Muhammad+Imran+Brianna+Nguyen+Jake+Pensa+Sara+M.+Falzarano+Anthony+E.+Sisk",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "M Imran",
        "id": "m0vCHn4AAAAJ"
      },
      {
        "name": "B Nguyen",
        "id": null
      },
      {
        "name": "J Pensa",
        "id": null
      },
      {
        "name": "SM Falzarano",
        "id": null
      },
      {
        "name": "AE Sisk",
        "id": null
      },
      {
        "name": "M Liang",
        "id": "7MuuifUAAAAJ"
      },
      {
        "name": "JM DiBianco",
        "id": null
      },
      {
        "name": "LM Su",
        "id": "syFSfQkAAAAJ"
      },
      {
        "name": "Y Zhou",
        "id": "eiqVLC0AAAAJ"
      }
    ],
    "citation_count": 4
  },
  {
    "arxiv_id": "2301.06133",
    "title": "Improving Reliability of Fine-tuning with Block-wise Optimisation",
    "year": 2023,
    "published": "2023-01-15T16:20:18Z",
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "abstract": "Finetuning can be used to tackle domain-specific tasks by transferring knowledge. Previous studies on finetuning focused on adapting only the weights of a task-specific classifier or re-optimizing all layers of the pre-trained model using the new task data. The first type of methods cannot mitigate the mismatch between a pre-trained model and the new task data, and the second type of methods easily cause over-fitting when processing tasks with limited data. To explore the effectiveness of fine-t",
    "arxiv_url": "https://arxiv.org/abs/2301.06133v1",
    "pdf_url": "https://arxiv.org/pdf/2301.06133v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2301.06133",
    "arxiv_authors": [
      "Basel Barakat",
      "Qiang Huang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Improving+Reliability+of+Fine-tuning+with+Block-wise+Optimisation+Basel+Barakat+Qiang+Huang",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "B Barakat",
        "id": "Nxk_QjgAAAAJ"
      },
      {
        "name": "Q Huang -",
        "id": null
      }
    ],
    "citation_count": 1
  },
  {
    "arxiv_id": "2301.10687",
    "title": "Self-Supervised Curricular Deep Learning for Chest X-Ray Image Classification",
    "year": 2023,
    "published": "2023-01-25T16:45:13Z",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "abstract": "Deep learning technologies have already demonstrated a high potential to build diagnosis support systems from medical imaging data, such as Chest X-Ray images. However, the shortage of labeled data in the medical field represents one key obstacle to narrow down the performance gap with respect to applications in other image domains. In this work, we investigate the benefits of a curricular Self-Supervised Learning (SSL) pretraining scheme with respect to fully-supervised training regimes for pne",
    "arxiv_url": "https://arxiv.org/abs/2301.10687v1",
    "pdf_url": "https://arxiv.org/pdf/2301.10687v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2301.10687",
    "arxiv_authors": [
      "Iv√°n de Andr√©s Tam√©",
      "Kirill Sirotkin",
      "Pablo Carballeira",
      "Marcos Escudero-Vi√±olo"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Self-Supervised+Curricular+Deep+Learning+for+Chest+X-Ray+Image+Classification+Iv%C3%A1n+de+Andr%C3%A9s+Tam%C3%A9+Kirill+Sirotkin+Pablo+Carballeira+Marcos+Escudero-Vi%C3%B1olo",
    "gs_search_success": false,
    "gs_authors": null,
    "citation_count": null,
    "error_type": "multiple_matches"
  },
  {
    "arxiv_id": "2407.00186",
    "title": "DCSM 2.0: Deep Conditional Shape Models for Data Efficient Segmentation",
    "year": 2024,
    "published": "2024-06-28T18:52:11Z",
    "categories": [
      "eess.IV",
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "Segmentation is often the first step in many medical image analyses workflows. Deep learning approaches, while giving state-of-the-art accuracies, are data intensive and do not scale well to low data regimes. We introduce Deep Conditional Shape Models 2.0, which uses an edge detector, along with an implicit shape function conditioned on edge maps, to leverage cross-modality shape information. The shape function is trained exclusively on a source domain (contrasted CT) and applied to the target d",
    "arxiv_url": "https://arxiv.org/abs/2407.00186v1",
    "pdf_url": "https://arxiv.org/pdf/2407.00186v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2407.00186",
    "arxiv_authors": [
      "Athira J Jacob",
      "Puneet Sharma",
      "Daniel Rueckert"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=DCSM+2.0%3A+Deep+Conditional+Shape+Models+for+Data+Efficient+Segmentation+Athira+J+Jacob+Puneet+Sharma+Daniel+Rueckert",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "AJ Jacob",
        "id": "75mVXkIAAAAJ"
      },
      {
        "name": "P Sharma",
        "id": "SFFdA1MAAAAJ"
      },
      {
        "name": "D Rueckert -",
        "id": null
      }
    ],
    "citation_count": 2
  },
  {
    "arxiv_id": "2311.11919",
    "title": "An Image is Worth Multiple Words: Multi-attribute Inversion for Constrained Text-to-Image Synthesis",
    "year": 2023,
    "published": "2023-11-20T16:54:07Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "We consider the problem of constraining diffusion model outputs with a user-supplied reference image. Our key objective is to extract multiple attributes (e.g., color, object, layout, style) from this single reference image, and then generate new samples with them. One line of existing work proposes to invert the reference images into a single textual conditioning vector, enabling generation of new samples with this learned token. These methods, however, do not learn multiple tokens that are nec",
    "arxiv_url": "https://arxiv.org/abs/2311.11919v1",
    "pdf_url": "https://arxiv.org/pdf/2311.11919v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2311.11919",
    "arxiv_authors": [
      "Aishwarya Agarwal",
      "Srikrishna Karanam",
      "Tripti Shukla",
      "Balaji Vasan Srinivasan"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=An+Image+is+Worth+Multiple+Words%3A+Multi-attribute+Inversion+for+Constrained+Text-to-Image+Synthesis+Aishwarya+Agarwal+Srikrishna+Karanam+Tripti+Shukla+Balaji+Vasan+Srinivasan",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "A Agarwal",
        "id": "PnIR8V4AAAAJ"
      },
      {
        "name": "S Karanam",
        "id": "G2-skyUAAAAJ"
      },
      {
        "name": "T Shukla",
        "id": "PLtMXBMAAAAJ"
      },
      {
        "name": "BV Srinivasan2025 IEEE/CVF Winter",
        "id": null
      }
    ],
    "citation_count": 31
  },
  {
    "arxiv_id": "2304.03994",
    "title": "RIDCP: Revitalizing Real Image Dehazing via High-Quality Codebook Priors",
    "year": 2023,
    "published": "2023-04-08T12:12:24Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Existing dehazing approaches struggle to process real-world hazy images owing to the lack of paired real data and robust priors. In this work, we present a new paradigm for real image dehazing from the perspectives of synthesizing more realistic hazy data and introducing more robust priors into the network. Specifically, (1) instead of adopting the de facto physical scattering model, we rethink the degradation of real hazy images and propose a phenomenological pipeline considering diverse degrad",
    "arxiv_url": "https://arxiv.org/abs/2304.03994v1",
    "pdf_url": "https://arxiv.org/pdf/2304.03994v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2304.03994",
    "arxiv_authors": [
      "Rui-Qi Wu",
      "Zheng-Peng Duan",
      "Chun-Le Guo",
      "Zhi Chai",
      "Chong-Yi Li"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=RIDCP%3A+Revitalizing+Real+Image+Dehazing+via+High-Quality+Codebook+Priors+Rui-Qi+Wu+Zheng-Peng+Duan+Chun-Le+Guo+Zhi+Chai+Chong-Yi+Li",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "RQ Wu",
        "id": "orim0kUAAAAJ"
      },
      {
        "name": "ZP Duan",
        "id": "S_v3euQAAAAJ"
      },
      {
        "name": "CL Guo",
        "id": "RZLYwR0AAAAJ"
      },
      {
        "name": "Z Chai",
        "id": null
      },
      {
        "name": "C Li",
        "id": "1_I0P-AAAAAJ"
      }
    ],
    "citation_count": 223
  },
  {
    "arxiv_id": "2403.08498",
    "title": "Gaussian Splatting in Style",
    "year": 2024,
    "published": "2024-03-13T13:06:31Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "3D scene stylization extends the work of neural style transfer to 3D. A vital challenge in this problem is to maintain the uniformity of the stylized appearance across multiple views. A vast majority of the previous works achieve this by training a 3D model for every stylized image and a set of multi-view images. In contrast, we propose a novel architecture trained on a collection of style images that, at test time, produces real time high-quality stylized novel views. We choose the underlying 3",
    "arxiv_url": "https://arxiv.org/abs/2403.08498v2",
    "pdf_url": "https://arxiv.org/pdf/2403.08498v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2403.08498",
    "arxiv_authors": [
      "Abhishek Saroha",
      "Mariia Gladkova",
      "Cecilia Curreli",
      "Dominik Muhle",
      "Tarun Yenamandra",
      "Daniel Cremers"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Gaussian+Splatting+in+Style+Abhishek+Saroha+Mariia+Gladkova+Cecilia+Curreli+Dominik+Muhle+Tarun+Yenamandra",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "A Saroha",
        "id": "nSuHckYAAAAJ"
      },
      {
        "name": "M Gladkova",
        "id": "bj6Wf14AAAAJ"
      },
      {
        "name": "C Curreli",
        "id": "Y6wCaD4AAAAJ"
      },
      {
        "name": "D Muhle",
        "id": "IyFJMlQAAAAJ"
      },
      {
        "name": "T Yenamandra",
        "id": "0sX2Hd8AAAAJ"
      },
      {
        "name": "D CremersDAGM German",
        "id": null
      }
    ],
    "citation_count": 21
  },
  {
    "arxiv_id": "2306.09330",
    "title": "ArtFusion: Controllable Arbitrary Style Transfer using Dual Conditional Latent Diffusion Models",
    "year": 2023,
    "published": "2023-06-15T17:58:36Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Arbitrary Style Transfer (AST) aims to transform images by adopting the style from any selected artwork. Nonetheless, the need to accommodate diverse and subjective user preferences poses a significant challenge. While some users wish to preserve distinct content structures, others might favor a more pronounced stylization. Despite advances in feed-forward AST methods, their limited customizability hinders their practical application. We propose a new approach, ArtFusion, which provides a flexib",
    "arxiv_url": "https://arxiv.org/abs/2306.09330v2",
    "pdf_url": "https://arxiv.org/pdf/2306.09330v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2306.09330",
    "arxiv_authors": [
      "Dar-Yen Chen"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=ArtFusion%3A+Controllable+Arbitrary+Style+Transfer+using+Dual+Conditional+Latent+Diffusion+Models+Dar-Yen+Chen",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "DY Chen -",
        "id": null
      }
    ],
    "citation_count": 14
  },
  {
    "arxiv_id": "2410.15374",
    "title": "Explainability of Point Cloud Neural Networks Using SMILE: Statistical Model-Agnostic Interpretability with Local Explanations",
    "year": 2024,
    "published": "2024-10-20T12:13:59Z",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "abstract": "In today's world, the significance of explainable AI (XAI) is growing in robotics and point cloud applications, as the lack of transparency in decision-making can pose considerable safety risks, particularly in autonomous systems. As these technologies are integrated into real-world environments, ensuring that model decisions are interpretable and trustworthy is vital for operational reliability and safety assurance. This study explores the implementation of SMILE, a novel explainability method ",
    "arxiv_url": "https://arxiv.org/abs/2410.15374v1",
    "pdf_url": "https://arxiv.org/pdf/2410.15374v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2410.15374",
    "arxiv_authors": [
      "Seyed Mohammad Ahmadi",
      "Koorosh Aslansefat",
      "Ruben Valcarce-Dineiro",
      "Joshua Barnfather"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Explainability+of+Point+Cloud+Neural+Networks+Using+SMILE%3A+Statistical+Model-Agnostic+Interpretability+with+Local+Explanations+Seyed+Mohammad+Ahmadi+Koorosh+Aslansefat+Ruben+Valcarce-Dineiro+Joshua+Barnfather",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "SM Ahmadi",
        "id": null
      },
      {
        "name": "K Aslansefat",
        "id": "YBa4Tl8AAAAJ"
      },
      {
        "name": "R Valcarce-Dineiro",
        "id": "bf8msksAAAAJ"
      },
      {
        "name": "J Barnfather",
        "id": null
      }
    ],
    "citation_count": 3
  },
  {
    "arxiv_id": "2405.17267",
    "title": "FedHPL: Efficient Heterogeneous Federated Learning with Prompt Tuning and Logit Distillation",
    "year": 2024,
    "published": "2024-05-27T15:25:32Z",
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "abstract": "Federated learning (FL) is a popular privacy-preserving paradigm that enables distributed clients to collaboratively train models with a central server while keeping raw data locally. In practice, distinct model architectures, varying data distributions, and limited resources across local clients inevitably cause model performance degradation and a slowdown in convergence speed. However, existing FL methods can only solve some of the above heterogeneous challenges and have obvious performance li",
    "arxiv_url": "https://arxiv.org/abs/2405.17267v1",
    "pdf_url": "https://arxiv.org/pdf/2405.17267v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2405.17267",
    "arxiv_authors": [
      "Yuting Ma",
      "Lechao Cheng",
      "Yaxiong Wang",
      "Zhun Zhong",
      "Xiaohua Xu",
      "Meng Wang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=FedHPL%3A+Efficient+Heterogeneous+Federated+Learning+with+Prompt+Tuning+and+Logit+Distillation+Yuting+Ma+Lechao+Cheng+Yaxiong+Wang+Zhun+Zhong+Xiaohua+Xu",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "Y Ma",
        "id": null
      },
      {
        "name": "L Cheng",
        "id": "PKFAv-cAAAAJ"
      },
      {
        "name": "Y Wang",
        "id": "lDChiR4AAAAJ"
      },
      {
        "name": "Z Zhong",
        "id": "nZizkQ0AAAAJ"
      },
      {
        "name": "X Xu",
        "id": null
      },
      {
        "name": "M Wang",
        "id": null
      }
    ],
    "citation_count": 5
  },
  {
    "arxiv_id": "2407.16413",
    "title": "Low Complexity Regularized Phase Retrieval",
    "year": 2024,
    "published": "2024-07-23T11:58:08Z",
    "categories": [
      "math.OC",
      "cs.CV",
      "cs.IT"
    ],
    "abstract": "In this paper, we study the phase retrieval problem in the situation where the vector to be recovered has an a priori structure that can encoded into a regularization term. This regularizer is intended to promote solutions conforming to some notion of simplicity or low complexity. We investigate both noiseless recovery and stability to noise and provide a very general and unified analysis framework that goes far beyond the sparse phase retrieval mostly considered in the literature. In the noisel",
    "arxiv_url": "https://arxiv.org/abs/2407.16413v1",
    "pdf_url": "https://arxiv.org/pdf/2407.16413v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2407.16413",
    "arxiv_authors": [
      "Jean-Jacques Godeme",
      "Jalal Fadili"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Low+Complexity+Regularized+Phase+Retrieval+Jean-Jacques+Godeme+Jalal+Fadili",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "JJ Godeme",
        "id": "2drBRNUAAAAJ"
      },
      {
        "name": "J Fadili -",
        "id": null
      }
    ],
    "citation_count": 1
  },
  {
    "arxiv_id": "2306.04236",
    "title": "Flare7K++: Mixing Synthetic and Real Datasets for Nighttime Flare Removal and Beyond",
    "year": 2023,
    "published": "2023-06-07T08:27:44Z",
    "categories": [
      "cs.CV",
      "eess.IV"
    ],
    "abstract": "Artificial lights commonly leave strong lens flare artifacts on the images captured at night, degrading both the visual quality and performance of vision algorithms. Existing flare removal approaches mainly focus on removing daytime flares and fail in nighttime cases. Nighttime flare removal is challenging due to the unique luminance and spectrum of artificial lights, as well as the diverse patterns and image degradation of the flares. The scarcity of the nighttime flare removal dataset constrai",
    "arxiv_url": "https://arxiv.org/abs/2306.04236v2",
    "pdf_url": "https://arxiv.org/pdf/2306.04236v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2306.04236",
    "arxiv_authors": [
      "Yuekun Dai",
      "Chongyi Li",
      "Shangchen Zhou",
      "Ruicheng Feng",
      "Yihang Luo",
      "Chen Change Loy"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Flare7K%2B%2B%3A+Mixing+Synthetic+and+Real+Datasets+for+Nighttime+Flare+Removal+and+Beyond+Yuekun+Dai+Chongyi+Li+Shangchen+Zhou+Ruicheng+Feng+Yihang+Luo",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "Y Dai",
        "id": "UyKX7ZsAAAAJ"
      },
      {
        "name": "C Li",
        "id": "1_I0P-AAAAAJ"
      },
      {
        "name": "S Zhou",
        "id": "suaDwBQAAAAJ"
      },
      {
        "name": "R Feng",
        "id": "nDrw-wwAAAAJ"
      },
      {
        "name": "Y Luo",
        "id": "fZxK2B0AAAAJ"
      },
      {
        "name": "CC LoyIEEE Transactions on Pattern Analysis and Machine Intelligence",
        "id": null
      }
    ],
    "citation_count": 58
  },
  {
    "arxiv_id": "2307.00122",
    "title": "An End-to-End Review of Gaze Estimation and its Interactive Applications on Handheld Mobile Devices",
    "year": 2023,
    "published": "2023-06-30T20:26:49Z",
    "categories": [
      "cs.HC",
      "cs.CV"
    ],
    "abstract": "In recent years we have witnessed an increasing number of interactive systems on handheld mobile devices which utilise gaze as a single or complementary interaction modality. This trend is driven by the enhanced computational power of these devices, higher resolution and capacity of their cameras, and improved gaze estimation accuracy obtained from advanced machine learning techniques, especially in deep learning. As the literature is fast progressing, there is a pressing need to review the stat",
    "arxiv_url": "https://arxiv.org/abs/2307.00122v1",
    "pdf_url": "https://arxiv.org/pdf/2307.00122v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2307.00122",
    "arxiv_authors": [
      "Yaxiong Lei",
      "Shijing He",
      "Mohamed Khamis",
      "Juan Ye"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=An+End-to-End+Review+of+Gaze+Estimation+and+its+Interactive+Applications+on+Handheld+Mobile+Devices+Yaxiong+Lei+Shijing+He+Mohamed+Khamis+Juan+Ye",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "Y Lei",
        "id": "AXeMHXkAAAAJ"
      },
      {
        "name": "S He",
        "id": "hQAjXS4AAAAJ"
      },
      {
        "name": "M Khamis",
        "id": "0OupgU0AAAAJ"
      }
    ],
    "citation_count": 30
  },
  {
    "arxiv_id": "2503.19034",
    "title": "Color Conditional Generation with Sliced Wasserstein Guidance",
    "year": 2025,
    "published": "2025-03-24T18:06:03Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "We propose SW-Guidance, a training-free approach for image generation conditioned on the color distribution of a reference image. While it is possible to generate an image with fixed colors by first creating an image from a text prompt and then applying a color style transfer method, this approach often results in semantically meaningless colors in the generated image. Our method solves this problem by modifying the sampling process of a diffusion model to incorporate the differentiable Sliced 1",
    "arxiv_url": "https://arxiv.org/abs/2503.19034v1",
    "pdf_url": "https://arxiv.org/pdf/2503.19034v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2503.19034",
    "arxiv_authors": [
      "Alexander Lobashev",
      "Maria Larchenko",
      "Dmitry Guskov"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Color+Conditional+Generation+with+Sliced+Wasserstein+Guidance+Alexander+Lobashev+Maria+Larchenko+Dmitry+Guskov",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "A Lobashev",
        "id": "mYRTkdkAAAAJ"
      },
      {
        "name": "M Larchenko",
        "id": "9sjCJ1IAAAAJ"
      },
      {
        "name": "D Guskov -",
        "id": null
      }
    ],
    "citation_count": 3
  },
  {
    "arxiv_id": "2407.09539",
    "title": "Classification of Inkjet Printers based on Droplet Statistics",
    "year": 2024,
    "published": "2024-06-26T10:20:01Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "eess.IV"
    ],
    "abstract": "Knowing the printer model used to print a given document may provide a crucial lead towards identifying counterfeits or conversely verifying the validity of a real document. Inkjet printers produce probabilistic droplet patterns that appear to be distinct for each printer model and as such we investigate the utilization of droplet characteristics including frequency domain features extracted from printed document scans for the classification of the underlying printer model. We collect and publis",
    "arxiv_url": "https://arxiv.org/abs/2407.09539v1",
    "pdf_url": "https://arxiv.org/pdf/2407.09539v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2407.09539",
    "arxiv_authors": [
      "Patrick Takenaka",
      "Manuel Eberhardinger",
      "Daniel Grie√ühaber",
      "Johannes Maucher"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Classification+of+Inkjet+Printers+based+on+Droplet+Statistics+Patrick+Takenaka+Manuel+Eberhardinger+Daniel+Grie%C3%9Fhaber+Johannes+Maucher",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "P Takenaka",
        "id": "27Vt8asAAAAJ"
      },
      {
        "name": "M Eberhardinger",
        "id": "t0L7sukAAAAJ"
      },
      {
        "name": "D Grie√ühaber",
        "id": "DFRKCqgAAAAJ"
      },
      {
        "name": "J Maucher2024 International Joint",
        "id": null
      }
    ],
    "citation_count": 2
  },
  {
    "arxiv_id": "2405.20380",
    "title": "Gradient Inversion of Federated Diffusion Models",
    "year": 2024,
    "published": "2024-05-30T18:00:03Z",
    "categories": [
      "cs.AI",
      "cs.CR",
      "cs.CV"
    ],
    "abstract": "Diffusion models are becoming defector generative models, which generate exceptionally high-resolution image data. Training effective diffusion models require massive real data, which is privately owned by distributed parties. Each data party can collaboratively train diffusion models in a federated learning manner by sharing gradients instead of the raw data. In this paper, we study the privacy leakage risk of gradient inversion attacks. First, we design a two-phase fusion optimization, GIDM, t",
    "arxiv_url": "https://arxiv.org/abs/2405.20380v1",
    "pdf_url": "https://arxiv.org/pdf/2405.20380v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2405.20380",
    "arxiv_authors": [
      "Jiyue Huang",
      "Chi Hong",
      "Lydia Y. Chen",
      "Stefanie Roos"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Gradient+Inversion+of+Federated+Diffusion+Models+Jiyue+Huang+Chi+Hong+Lydia+Y.+Chen+Stefanie+Roos",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "J Huang",
        "id": "ICKOpU4AAAAJ"
      },
      {
        "name": "C Hong",
        "id": "zppla80AAAAJ"
      },
      {
        "name": "LY Chen",
        "id": "xXu_fMcAAAAJ"
      },
      {
        "name": "S Roos -",
        "id": null
      }
    ],
    "citation_count": 2
  },
  {
    "arxiv_id": "2501.02509",
    "title": "Facial Attractiveness Prediction in Live Streaming: A New Benchmark and Multi-modal Method",
    "year": 2025,
    "published": "2025-01-05T11:43:35Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Facial attractiveness prediction (FAP) has long been an important computer vision task, which could be widely applied in live streaming for facial retouching, content recommendation, etc. However, previous FAP datasets are either small, closed-source, or lack diversity. Moreover, the corresponding FAP models exhibit limited generalization and adaptation ability. To overcome these limitations, in this paper we present LiveBeauty, the first large-scale live-specific FAP dataset, in a more challeng",
    "arxiv_url": "https://arxiv.org/abs/2501.02509v2",
    "pdf_url": "https://arxiv.org/pdf/2501.02509v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2501.02509",
    "arxiv_authors": [
      "Hui Li",
      "Xiaoyu Ren",
      "Hongjiu Yu",
      "Huiyu Duan",
      "Kai Li",
      "Ying Chen",
      "Libo Wang",
      "Xiongkuo Min",
      "Guangtao Zhai",
      "Xu Liu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Facial+Attractiveness+Prediction+in+Live+Streaming%3A+A+New+Benchmark+and+Multi-modal+Method+Hui+Li+Xiaoyu+Ren+Hongjiu+Yu+Huiyu+Duan+Kai+Li",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "H Li",
        "id": null
      },
      {
        "name": "X Ren",
        "id": null
      },
      {
        "name": "H Yu",
        "id": null
      },
      {
        "name": "H Duan",
        "id": "r0bRaCMAAAAJ"
      },
      {
        "name": "K Li",
        "id": null
      },
      {
        "name": "Y Chen",
        "id": null
      },
      {
        "name": "L Wang",
        "id": null
      },
      {
        "name": "X Min",
        "id": "91sjuWIAAAAJ"
      },
      {
        "name": "G Zhai",
        "id": "E6zbSYgAAAAJ"
      },
      {
        "name": "X Liu",
        "id": null
      }
    ],
    "citation_count": 1
  },
  {
    "arxiv_id": "2312.16693",
    "title": "I2V-Adapter: A General Image-to-Video Adapter for Diffusion Models",
    "year": 2023,
    "published": "2023-12-27T19:11:50Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Text-guided image-to-video (I2V) generation aims to generate a coherent video that preserves the identity of the input image and semantically aligns with the input prompt. Existing methods typically augment pretrained text-to-video (T2V) models by either concatenating the image with noised video frames channel-wise before being fed into the model or injecting the image embedding produced by pretrained image encoders in cross-attention modules. However, the former approach often necessitates alte",
    "arxiv_url": "https://arxiv.org/abs/2312.16693v4",
    "pdf_url": "https://arxiv.org/pdf/2312.16693v4",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2312.16693",
    "arxiv_authors": [
      "Xun Guo",
      "Mingwu Zheng",
      "Liang Hou",
      "Yuan Gao",
      "Yufan Deng",
      "Pengfei Wan",
      "Di Zhang",
      "Yufan Liu",
      "Weiming Hu",
      "Zhengjun Zha",
      "Haibin Huang",
      "Chongyang Ma"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=I2V-Adapter%3A+A+General+Image-to-Video+Adapter+for+Diffusion+Models+Xun+Guo+Mingwu+Zheng+Liang+Hou+Yuan+Gao+Yufan+Deng",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "X Guo",
        "id": "XtHtIDcAAAAJ"
      },
      {
        "name": "M Zheng",
        "id": "MdizB60AAAAJ"
      },
      {
        "name": "L Hou",
        "id": "X48pntMAAAAJ"
      },
      {
        "name": "Y Gao",
        "id": "CJWJDvoAAAAJ"
      },
      {
        "name": "Y Deng",
        "id": null
      },
      {
        "name": "P Wan",
        "id": "P6MraaYAAAAJ"
      },
      {
        "name": "D Zhang",
        "id": "gDnBC1gAAAAJ"
      },
      {
        "name": "Y Liu",
        "id": "CHymRYQAAAAJ"
      },
      {
        "name": "W Hu",
        "id": "Wl4tl4QAAAAJ"
      },
      {
        "name": "Z Zha",
        "id": null
      },
      {
        "name": "H Huang",
        "id": "YDl1M80AAAAJ"
      }
    ],
    "citation_count": 68
  },
  {
    "arxiv_id": "2505.24222",
    "title": "Unleashing High-Quality Image Generation in Diffusion Sampling Using Second-Order Levenberg-Marquardt-Langevin",
    "year": 2025,
    "published": "2025-05-30T05:21:44Z",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "The diffusion models (DMs) have demonstrated the remarkable capability of generating images via learning the noised score function of data distribution. Current DM sampling techniques typically rely on first-order Langevin dynamics at each noise level, with efforts concentrated on refining inter-level denoising strategies. While leveraging additional second-order Hessian geometry to enhance the sampling quality of Langevin is a common practice in Markov chain Monte Carlo (MCMC), the naive attemp",
    "arxiv_url": "https://arxiv.org/abs/2505.24222v1",
    "pdf_url": "https://arxiv.org/pdf/2505.24222v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2505.24222",
    "arxiv_authors": [
      "Fangyikang Wang",
      "Hubery Yin",
      "Lei Qian",
      "Yinan Li",
      "Shaobin Zhuang",
      "Huminhao Zhu",
      "Yilin Zhang",
      "Yanlong Tang",
      "Chao Zhang",
      "Hanbin Zhao",
      "Hui Qian",
      "Chen Li"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Unleashing+High-Quality+Image+Generation+in+Diffusion+Sampling+Using+Second-Order+Levenberg-Marquardt-Langevin+Fangyikang+Wang+Hubery+Yin+Lei+Qian+Yinan+Li+Shaobin+Zhuang",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "F Wang",
        "id": "j80akcEAAAAJ"
      },
      {
        "name": "H Yin",
        "id": null
      },
      {
        "name": "L Qian",
        "id": null
      },
      {
        "name": "Y Li",
        "id": null
      },
      {
        "name": "S Zhuang",
        "id": "PGaDirMAAAAJ"
      },
      {
        "name": "H Zhu",
        "id": "_A95xeEAAAAJ"
      },
      {
        "name": "Y Zhang",
        "id": null
      },
      {
        "name": "Y Tang",
        "id": null
      },
      {
        "name": "C Zhang",
        "id": "Clhvcw8AAAAJ"
      },
      {
        "name": "H Zhao",
        "id": "F2kiw10AAAAJ"
      },
      {
        "name": "H Qian",
        "id": "n4csXw0AAAAJ"
      }
    ],
    "citation_count": 1
  },
  {
    "arxiv_id": "2404.00185",
    "title": "On Inherent Adversarial Robustness of Active Vision Systems",
    "year": 2024,
    "published": "2024-03-29T22:51:45Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Current Deep Neural Networks are vulnerable to adversarial examples, which alter their predictions by adding carefully crafted noise. Since human eyes are robust to such inputs, it is possible that the vulnerability stems from the standard way of processing inputs in one shot by processing every pixel with the same importance. In contrast, neuroscience suggests that the human vision system can differentiate salient features by (1) switching between multiple fixation points (saccades) and (2) pro",
    "arxiv_url": "https://arxiv.org/abs/2404.00185v2",
    "pdf_url": "https://arxiv.org/pdf/2404.00185v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2404.00185",
    "arxiv_authors": [
      "Amitangshu Mukherjee",
      "Timur Ibrayev",
      "Kaushik Roy"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=On+Inherent+Adversarial+Robustness+of+Active+Vision+Systems+Amitangshu+Mukherjee+Timur+Ibrayev+Kaushik+Roy",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "A Mukherjee",
        "id": "aK1eTNkAAAAJ"
      },
      {
        "name": "T Ibrayev",
        "id": "CtP3iccAAAAJ"
      },
      {
        "name": "K Roy - Transactions on Machine Learning",
        "id": null
      }
    ],
    "citation_count": 1
  },
  {
    "arxiv_id": "2307.05256",
    "title": "Towards exploring adversarial learning for anomaly detection in complex driving scenes",
    "year": 2023,
    "published": "2023-06-17T15:32:16Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "One of the many Autonomous Systems (ASs), such as autonomous driving cars, performs various safety-critical functions. Many of these autonomous systems take advantage of Artificial Intelligence (AI) techniques to perceive their environment. But these perceiving components could not be formally verified, since, the accuracy of such AI-based components has a high dependency on the quality of training data. So Machine learning (ML) based anomaly detection, a technique to identify data that does not",
    "arxiv_url": "https://arxiv.org/abs/2307.05256v1",
    "pdf_url": "https://arxiv.org/pdf/2307.05256v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2307.05256",
    "arxiv_authors": [
      "Nour Habib",
      "Yunsu Cho",
      "Abhishek Buragohain",
      "Andreas Rausch"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Towards+exploring+adversarial+learning+for+anomaly+detection+in+complex+driving+scenes+Nour+Habib+Yunsu+Cho+Abhishek+Buragohain+Andreas+Rausch",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "N Habib",
        "id": null
      },
      {
        "name": "Y Cho",
        "id": null
      },
      {
        "name": "A Buragohain",
        "id": "CulLwYsAAAAJ"
      },
      {
        "name": "A Rausch - International",
        "id": null
      }
    ],
    "citation_count": 2
  },
  {
    "arxiv_id": "2406.11737",
    "title": "InterNeRF: Scaling Radiance Fields via Parameter Interpolation",
    "year": 2024,
    "published": "2024-06-17T16:55:22Z",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "abstract": "Neural Radiance Fields (NeRFs) have unmatched fidelity on large, real-world scenes. A common approach for scaling NeRFs is to partition the scene into regions, each of which is assigned its own parameters. When implemented naively, such an approach is limited by poor test-time scaling and inconsistent appearance and geometry. We instead propose InterNeRF, a novel architecture for rendering a target view using a subset of the model's parameters. Our approach enables out-of-core training and rende",
    "arxiv_url": "https://arxiv.org/abs/2406.11737v1",
    "pdf_url": "https://arxiv.org/pdf/2406.11737v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2406.11737",
    "arxiv_authors": [
      "Clinton Wang",
      "Peter Hedman",
      "Polina Golland",
      "Jonathan T. Barron",
      "Daniel Duckworth"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=InterNeRF%3A+Scaling+Radiance+Fields+via+Parameter+Interpolation+Clinton+Wang+Peter+Hedman+Polina+Golland+Jonathan+T.+Barron+Daniel+Duckworth",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "C Wang",
        "id": "7ICTJmoAAAAJ"
      },
      {
        "name": "P Hedman",
        "id": "dNCrPskAAAAJ"
      },
      {
        "name": "P Golland",
        "id": "4GpKQUIAAAAJ"
      },
      {
        "name": "JT Barron",
        "id": "jktWnL8AAAAJ"
      },
      {
        "name": "D Duckworth",
        "id": "2fWmq-4AAAAJ"
      }
    ],
    "citation_count": null
  },
  {
    "arxiv_id": "2312.09955",
    "title": "DHFormer: A Vision Transformer-Based Attention Module for Image Dehazing",
    "year": 2023,
    "published": "2023-12-15T17:05:32Z",
    "categories": [
      "cs.CV",
      "eess.IV"
    ],
    "abstract": "Images acquired in hazy conditions have degradations induced in them. Dehazing such images is a vexed and ill-posed problem. Scores of prior-based and learning-based approaches have been proposed to mitigate the effect of haze and generate haze-free images. Many conventional methods are constrained by their lack of awareness regarding scene depth and their incapacity to capture long-range dependencies. In this paper, a method that uses residual learning and vision transformers in an attention mo",
    "arxiv_url": "https://arxiv.org/abs/2312.09955v1",
    "pdf_url": "https://arxiv.org/pdf/2312.09955v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2312.09955",
    "arxiv_authors": [
      "Abdul Wasi",
      "O. Jeba Shiney"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=DHFormer%3A+A+Vision+Transformer-Based+Attention+Module+for+Image+Dehazing+Abdul+Wasi+O.+Jeba+Shiney",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "A Wasi",
        "id": "_2friTYAAAAJ"
      },
      {
        "name": "OJ Shiney - International",
        "id": null
      }
    ],
    "citation_count": 4
  },
  {
    "arxiv_id": "2410.16857",
    "title": "Nash Meets Wertheimer: Using Good Continuation in Jigsaw Puzzles",
    "year": 2024,
    "published": "2024-10-22T09:46:09Z",
    "categories": [
      "cs.GT",
      "cs.CV"
    ],
    "abstract": "Jigsaw puzzle solving is a challenging task for computer vision since it requires high-level spatial and semantic reasoning. To solve the problem, existing approaches invariably use color and/or shape information but in many real-world scenarios, such as in archaeological fresco reconstruction, this kind of clues is often unreliable due to severe physical and pictorial deterioration of the individual fragments. This makes state-of-the-art approaches entirely unusable in practice. On the other ha",
    "arxiv_url": "https://arxiv.org/abs/2410.16857v1",
    "pdf_url": "https://arxiv.org/pdf/2410.16857v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2410.16857",
    "arxiv_authors": [
      "Marina Khoroshiltseva",
      "Luca Palmieri",
      "Sinem Aslan",
      "Sebastiano Vascon",
      "Marcello Pelillo"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Nash+Meets+Wertheimer%3A+Using+Good+Continuation+in+Jigsaw+Puzzles+Marina+Khoroshiltseva+Luca+Palmieri+Sinem+Aslan+Sebastiano+Vascon+Marcello+Pelillo",
    "gs_search_success": false,
    "gs_authors": null,
    "citation_count": null,
    "error_type": "fetch_failed"
  },
  {
    "arxiv_id": "2312.07823",
    "title": "Semantic Lens: Instance-Centric Semantic Alignment for Video Super-Resolution",
    "year": 2023,
    "published": "2023-12-13T01:16:50Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "As a critical clue of video super-resolution (VSR), inter-frame alignment significantly impacts overall performance. However, accurate pixel-level alignment is a challenging task due to the intricate motion interweaving in the video. In response to this issue, we introduce a novel paradigm for VSR named Semantic Lens, predicated on semantic priors drawn from degraded videos. Specifically, video is modeled as instances, events, and scenes via a Semantic Extractor. Those semantics assist the Pixel",
    "arxiv_url": "https://arxiv.org/abs/2312.07823v4",
    "pdf_url": "https://arxiv.org/pdf/2312.07823v4",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2312.07823",
    "arxiv_authors": [
      "Qi Tang",
      "Yao Zhao",
      "Meiqin Liu",
      "Jian Jin",
      "Chao Yao"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Semantic+Lens%3A+Instance-Centric+Semantic+Alignment+for+Video+Super-Resolution+Qi+Tang+Yao+Zhao+Meiqin+Liu+Jian+Jin+Chao+Yao",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "Q Tang",
        "id": "TZQUv1MAAAAJ"
      },
      {
        "name": "Y Zhao",
        "id": "eXXePDsAAAAJ"
      },
      {
        "name": "M Liu",
        "id": null
      },
      {
        "name": "J Jin",
        "id": "EFR2iigAAAAJ"
      },
      {
        "name": "C Yao -",
        "id": null
      }
    ],
    "citation_count": 11
  },
  {
    "arxiv_id": "2402.13122",
    "title": "Cross-Domain Transfer Learning with CoRTe: Consistent and Reliable Transfer from Black-Box to Lightweight Segmentation Model",
    "year": 2024,
    "published": "2024-02-20T16:35:14Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Many practical applications require training of semantic segmentation models on unlabelled datasets and their execution on low-resource hardware. Distillation from a trained source model may represent a solution for the first but does not account for the different distribution of the training data. Unsupervised domain adaptation (UDA) techniques claim to solve the domain shift, but in most cases assume the availability of the source data or an accessible white-box source model, which in practica",
    "arxiv_url": "https://arxiv.org/abs/2402.13122v1",
    "pdf_url": "https://arxiv.org/pdf/2402.13122v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2402.13122",
    "arxiv_authors": [
      "Claudia Cuttano",
      "Antonio Tavera",
      "Fabio Cermelli",
      "Giuseppe Averta",
      "Barbara Caputo"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Cross-Domain+Transfer+Learning+with+CoRTe%3A+Consistent+and+Reliable+Transfer+from+Black-Box+to+Lightweight+Segmentation+Model+Claudia+Cuttano+Antonio+Tavera+Fabio+Cermelli+Giuseppe+Averta+Barbara+Caputo",
    "gs_search_success": false,
    "gs_authors": null,
    "citation_count": null,
    "error_type": "multiple_matches"
  },
  {
    "arxiv_id": "2411.13024",
    "title": "Prior-based Objective Inference Mining Potential Uncertainty for Facial Expression Recognition",
    "year": 2024,
    "published": "2024-11-20T04:13:05Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Annotation ambiguity caused by the inherent subjectivity of visual judgment has always been a major challenge for Facial Expression Recognition (FER) tasks, particularly for largescale datasets from in-the-wild scenarios. A potential solution is the evaluation of relatively objective emotional distributions to help mitigate the ambiguity of subjective annotations. To this end, this paper proposes a novel Prior-based Objective Inference (POI) network. This network employs prior knowledge to deriv",
    "arxiv_url": "https://arxiv.org/abs/2411.13024v1",
    "pdf_url": "https://arxiv.org/pdf/2411.13024v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2411.13024",
    "arxiv_authors": [
      "Hanwei Liu",
      "Huiling Cai",
      "Qingcheng Lin",
      "Xuefeng Li",
      "Hui Xiao"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Prior-based+Objective+Inference+Mining+Potential+Uncertainty+for+Facial+Expression+Recognition+Hanwei+Liu+Huiling+Cai+Qingcheng+Lin+Xuefeng+Li+Hui+Xiao",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "H Liu",
        "id": "0yCTQFEAAAAJ"
      },
      {
        "name": "H Cai",
        "id": "M14Lqz0AAAAJ"
      },
      {
        "name": "Q Lin",
        "id": null
      },
      {
        "name": "X Li",
        "id": null
      },
      {
        "name": "H Xiao -",
        "id": null
      }
    ],
    "citation_count": null
  },
  {
    "arxiv_id": "2505.15058",
    "title": "AsynFusion: Towards Asynchronous Latent Consistency Models for Decoupled Whole-Body Audio-Driven Avatars",
    "year": 2025,
    "published": "2025-05-21T03:28:53Z",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.CV",
      "cs.GR",
      "eess.AS"
    ],
    "abstract": "Whole-body audio-driven avatar pose and expression generation is a critical task for creating lifelike digital humans and enhancing the capabilities of interactive virtual agents, with wide-ranging applications in virtual reality, digital entertainment, and remote communication. Existing approaches often generate audio-driven facial expressions and gestures independently, which introduces a significant limitation: the lack of seamless coordination between facial and gestural elements, resulting ",
    "arxiv_url": "https://arxiv.org/abs/2505.15058v2",
    "pdf_url": "https://arxiv.org/pdf/2505.15058v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2505.15058",
    "arxiv_authors": [
      "Tianbao Zhang",
      "Jian Zhao",
      "Yuer Li",
      "Zheng Zhu",
      "Ping Hu",
      "Zhaoxin Fan",
      "Wenjun Wu",
      "Xuelong Li"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=AsynFusion%3A+Towards+Asynchronous+Latent+Consistency+Models+for+Decoupled+Whole-Body+Audio-Driven+Avatars+Tianbao+Zhang+Jian+Zhao+Yuer+Li+Zheng+Zhu+Ping+Hu",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "T Zhang",
        "id": "H6C5pUEAAAAJ"
      },
      {
        "name": "J Zhao",
        "id": "zdhRJCkAAAAJ"
      },
      {
        "name": "Y Li",
        "id": null
      },
      {
        "name": "Z Zhu",
        "id": "NmwjI0AAAAAJ"
      },
      {
        "name": "P Hu",
        "id": null
      },
      {
        "name": "Z Fan",
        "id": "JHvyYDQAAAAJ"
      },
      {
        "name": "W Wu",
        "id": null
      },
      {
        "name": "X Li",
        "id": null
      }
    ],
    "citation_count": null
  },
  {
    "arxiv_id": "2407.14086",
    "title": "Temporal Correlation Meets Embedding: Towards a 2nd Generation of JDE-based Real-Time Multi-Object Tracking",
    "year": 2024,
    "published": "2024-07-19T07:48:45Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Joint Detection and Embedding (JDE) trackers have demonstrated excellent performance in Multi-Object Tracking (MOT) tasks by incorporating the extraction of appearance features as auxiliary tasks through embedding Re-Identification task (ReID) into the detector, achieving a balance between inference speed and tracking performance. However, solving the competition between the detector and the feature extractor has always been a challenge. Meanwhile, the issue of directly embedding the ReID task i",
    "arxiv_url": "https://arxiv.org/abs/2407.14086v2",
    "pdf_url": "https://arxiv.org/pdf/2407.14086v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2407.14086",
    "arxiv_authors": [
      "Yunfei Zhang",
      "Chao Liang",
      "Jin Gao",
      "Zhipeng Zhang",
      "Weiming Hu",
      "Stephen Maybank",
      "Xue Zhou",
      "Liang Li"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Temporal+Correlation+Meets+Embedding%3A+Towards+a+2nd+Generation+of+JDE-based+Real-Time+Multi-Object+Tracking+Yunfei+Zhang+Chao+Liang+Jin+Gao+Zhipeng+Zhang+Weiming+Hu",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "Y Zhang",
        "id": null
      },
      {
        "name": "C Liang",
        "id": "JQpmKD0AAAAJ"
      },
      {
        "name": "J Gao",
        "id": "z3Z1ZsUAAAAJ"
      },
      {
        "name": "Z Zhang",
        "id": null
      },
      {
        "name": "W Hu",
        "id": "Wl4tl4QAAAAJ"
      },
      {
        "name": "S Maybank",
        "id": "gpyHJmcAAAAJ"
      },
      {
        "name": "X Zhou",
        "id": null
      },
      {
        "name": "L Li",
        "id": null
      }
    ],
    "citation_count": 2
  },
  {
    "arxiv_id": "2306.14262",
    "title": "A Spectral Perspective towards Understanding and Improving Adversarial Robustness",
    "year": 2023,
    "published": "2023-06-25T14:47:03Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Deep neural networks (DNNs) are incredibly vulnerable to crafted, imperceptible adversarial perturbations. While adversarial training (AT) has proven to be an effective defense approach, the AT mechanism for robustness improvement is not fully understood. This work investigates AT from a spectral perspective, adding new insights to the design of effective defenses. In particular, we show that AT induces the deep model to focus more on the low-frequency region, which retains the shape-biased repr",
    "arxiv_url": "https://arxiv.org/abs/2306.14262v1",
    "pdf_url": "https://arxiv.org/pdf/2306.14262v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2306.14262",
    "arxiv_authors": [
      "Binxiao Huang",
      "Rui Lin",
      "Chaofan Tao",
      "Ngai Wong"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=A+Spectral+Perspective+towards+Understanding+and+Improving+Adversarial+Robustness+Binxiao+Huang+Rui+Lin+Chaofan+Tao+Ngai+Wong",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "B Huang",
        "id": "kJ_qMjoAAAAJ"
      },
      {
        "name": "R Lin",
        "id": "gx0RITkAAAAJ"
      },
      {
        "name": "C Tao",
        "id": "gjmfLroAAAAJ"
      },
      {
        "name": "N Wong -",
        "id": null
      }
    ],
    "citation_count": 1
  },
  {
    "arxiv_id": "2502.11196",
    "title": "How Do LLMs Acquire New Knowledge? A Knowledge Circuits Perspective on Continual Pre-Training",
    "year": 2025,
    "published": "2025-02-16T16:55:43Z",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.CV",
      "cs.HC"
    ],
    "abstract": "Despite exceptional capabilities in knowledge-intensive tasks, Large Language Models (LLMs) face a critical gap in understanding how they internalize new knowledge, particularly how to structurally embed acquired knowledge in their neural computations. We address this issue through the lens of knowledge circuit evolution, identifying computational subgraphs that facilitate knowledge storage and processing. Our systematic analysis of circuit evolution throughout continual pre-training reveals sev",
    "arxiv_url": "https://arxiv.org/abs/2502.11196v2",
    "pdf_url": "https://arxiv.org/pdf/2502.11196v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2502.11196",
    "arxiv_authors": [
      "Yixin Ou",
      "Yunzhi Yao",
      "Ningyu Zhang",
      "Hui Jin",
      "Jiacheng Sun",
      "Shumin Deng",
      "Zhenguo Li",
      "Huajun Chen"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=How+Do+LLMs+Acquire+New+Knowledge%3F+A+Knowledge+Circuits+Perspective+on+Continual+Pre-Training+Yixin+Ou+Yunzhi+Yao+Ningyu+Zhang+Hui+Jin+Jiacheng+Sun",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "Y Ou",
        "id": "QVTr5dQAAAAJ"
      },
      {
        "name": "Y Yao",
        "id": "nAagIwEAAAAJ"
      },
      {
        "name": "N Zhang",
        "id": "xQDOPvsAAAAJ"
      },
      {
        "name": "H Jin",
        "id": "eicCkOkAAAAJ"
      },
      {
        "name": "J Sun",
        "id": "hVnC-jIAAAAJ"
      },
      {
        "name": "S Deng",
        "id": "3am3hL4AAAAJ"
      },
      {
        "name": "Z Li",
        "id": null
      },
      {
        "name": "H Chen",
        "id": "T6om-m4AAAAJ"
      }
    ],
    "citation_count": 9
  },
  {
    "arxiv_id": "2403.04172",
    "title": "SDPL: Shifting-Dense Partition Learning for UAV-View Geo-Localization",
    "year": 2024,
    "published": "2024-03-07T03:07:54Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Cross-view geo-localization aims to match images of the same target from different platforms, e.g., drone and satellite. It is a challenging task due to the changing appearance of targets and environmental content from different views. Most methods focus on obtaining more comprehensive information through feature map segmentation, while inevitably destroying the image structure, and are sensitive to the shifting and scale of the target in the query. To address the above issues, we introduce simp",
    "arxiv_url": "https://arxiv.org/abs/2403.04172v2",
    "pdf_url": "https://arxiv.org/pdf/2403.04172v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2403.04172",
    "arxiv_authors": [
      "Quan Chen",
      "Tingyu Wang",
      "Zihao Yang",
      "Haoran Li",
      "Rongfeng Lu",
      "Yaoqi Sun",
      "Bolun Zheng",
      "Chenggang Yan"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=SDPL%3A+Shifting-Dense+Partition+Learning+for+UAV-View+Geo-Localization+Quan+Chen+Tingyu+Wang+Zihao+Yang+Haoran+Li+Rongfeng+Lu",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "Q Chen",
        "id": null
      },
      {
        "name": "T Wang",
        "id": "wv3H-F4AAAAJ"
      },
      {
        "name": "Z Yang",
        "id": null
      },
      {
        "name": "H Li",
        "id": null
      },
      {
        "name": "R Lu",
        "id": "zcgOu7gAAAAJ"
      },
      {
        "name": "Y Sun",
        "id": null
      },
      {
        "name": "B Zheng",
        "id": "cZRVzVYAAAAJ"
      },
      {
        "name": "C YanIEEE Transactions on Circuits and Systems for Video Technology",
        "id": null
      }
    ],
    "citation_count": 43
  },
  {
    "arxiv_id": "2310.13574",
    "title": "Progressive Dual Priori Network for Generalized Breast Tumor Segmentation",
    "year": 2023,
    "published": "2023-10-20T15:12:06Z",
    "categories": [
      "eess.IV",
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "To promote the generalization ability of breast tumor segmentation models, as well as to improve the segmentation performance for breast tumors with smaller size, low-contrast and irregular shape, we propose a progressive dual priori network (PDPNet) to segment breast tumors from dynamic enhanced magnetic resonance images (DCE-MRI) acquired at different centers. The PDPNet first cropped tumor regions with a coarse-segmentation based localization module, then the breast tumor mask was progressive",
    "arxiv_url": "https://arxiv.org/abs/2310.13574v2",
    "pdf_url": "https://arxiv.org/pdf/2310.13574v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2310.13574",
    "arxiv_authors": [
      "Li Wang",
      "Lihui Wang",
      "Zixiang Kuai",
      "Lei Tang",
      "Yingfeng Ou",
      "Chen Ye",
      "Yuemin Zhu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Progressive+Dual+Priori+Network+for+Generalized+Breast+Tumor+Segmentation+Li+Wang+Lihui+Wang+Zixiang+Kuai+Lei+Tang+Yingfeng+Ou",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "L Wang",
        "id": null
      },
      {
        "name": "L Wang",
        "id": null
      },
      {
        "name": "Z Kuai",
        "id": null
      },
      {
        "name": "L Tang",
        "id": null
      },
      {
        "name": "Y Ou",
        "id": null
      },
      {
        "name": "M Wu",
        "id": null
      },
      {
        "name": "T Shi",
        "id": null
      },
      {
        "name": "C Ye",
        "id": null
      },
      {
        "name": "Y ZhuIEEE",
        "id": null
      }
    ],
    "citation_count": 8
  },
  {
    "arxiv_id": "2503.23775",
    "title": "Evaluation of (Un-)Supervised Machine Learning Methods for GNSS Interference Classification with Real-World Data Discrepancies",
    "year": 2025,
    "published": "2025-03-31T06:51:52Z",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "The accuracy and reliability of vehicle localization on roads are crucial for applications such as self-driving cars, toll systems, and digital tachographs. To achieve accurate positioning, vehicles typically use global navigation satellite system (GNSS) receivers to validate their absolute positions. However, GNSS-based positioning can be compromised by interference signals, necessitating the identification, classification, determination of purpose, and localization of such interference to miti",
    "arxiv_url": "https://arxiv.org/abs/2503.23775v1",
    "pdf_url": "https://arxiv.org/pdf/2503.23775v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2503.23775",
    "arxiv_authors": [
      "Lucas Heublein",
      "Nisha L. Raichur",
      "Tobias Feigl",
      "Tobias Brieger",
      "Fin Heuer",
      "Lennart Asbach",
      "Alexander R√ºgamer",
      "Felix Ott"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Evaluation+of+%28Un-%29Supervised+Machine+Learning+Methods+for+GNSS+Interference+Classification+with+Real-World+Data+Discrepancies+Lucas+Heublein+Nisha+L.+Raichur+Tobias+Feigl+Tobias+Brieger+Fin+Heuer",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "L Heublein",
        "id": null
      },
      {
        "name": "NL Raichur",
        "id": null
      },
      {
        "name": "T Feigl",
        "id": "SDHdsbwAAAAJ"
      },
      {
        "name": "T Brieger",
        "id": null
      },
      {
        "name": "F Heuer",
        "id": null
      },
      {
        "name": "L Asbach",
        "id": null
      },
      {
        "name": "A R√ºgamer",
        "id": null
      },
      {
        "name": "F Ott",
        "id": "8OU4_x8AAAAJ"
      }
    ],
    "citation_count": 10
  },
  {
    "arxiv_id": "2402.16086",
    "title": "Deep Homography Estimation for Visual Place Recognition",
    "year": 2024,
    "published": "2024-02-25T13:22:17Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Visual place recognition (VPR) is a fundamental task for many applications such as robot localization and augmented reality. Recently, the hierarchical VPR methods have received considerable attention due to the trade-off between accuracy and efficiency. They usually first use global features to retrieve the candidate images, then verify the spatial consistency of matched local features for re-ranking. However, the latter typically relies on the RANSAC algorithm for fitting homography, which is ",
    "arxiv_url": "https://arxiv.org/abs/2402.16086v2",
    "pdf_url": "https://arxiv.org/pdf/2402.16086v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2402.16086",
    "arxiv_authors": [
      "Feng Lu",
      "Shuting Dong",
      "Lijun Zhang",
      "Bingxi Liu",
      "Xiangyuan Lan",
      "Dongmei Jiang",
      "Chun Yuan"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Deep+Homography+Estimation+for+Visual+Place+Recognition+Feng+Lu+Shuting+Dong+Lijun+Zhang+Bingxi+Liu+Xiangyuan+Lan",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "F Lu",
        "id": "Gws7FKMAAAAJ"
      },
      {
        "name": "S Dong",
        "id": "pbNolugAAAAJ"
      },
      {
        "name": "L Zhang",
        "id": null
      },
      {
        "name": "B Liu",
        "id": "T2_7muEAAAAJ"
      },
      {
        "name": "X Lan",
        "id": "c3iwWRcAAAAJ"
      },
      {
        "name": "D Jiang",
        "id": null
      },
      {
        "name": "C Yuan",
        "id": "fYdxi2sAAAAJ"
      }
    ],
    "citation_count": 18
  },
  {
    "arxiv_id": "2312.04008",
    "title": "Natural-language-driven Simulation Benchmark and Copilot for Efficient Production of Object Interactions in Virtual Road Scenes",
    "year": 2023,
    "published": "2023-12-07T02:55:46Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "We advocate the idea of the natural-language-driven(NLD) simulation to efficiently produce the object interactions between multiple objects in the virtual road scenes, for teaching and testing the autonomous driving systems that should take quick action to avoid collision with obstacles with unpredictable motions. The NLD simulation allows the brief natural-language description to control the object interactions, significantly reducing the human efforts for creating a large amount of interaction",
    "arxiv_url": "https://arxiv.org/abs/2312.04008v4",
    "pdf_url": "https://arxiv.org/pdf/2312.04008v4",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2312.04008",
    "arxiv_authors": [
      "Kairui Yang",
      "Zihao Guo",
      "Gengjie Lin",
      "Haotian Dong",
      "Die Zuo",
      "Jibin Peng",
      "Zhao Huang",
      "Zhecheng Xu",
      "Fupeng Li",
      "Ziyun Bai",
      "Di Lin"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Natural-language-driven+Simulation+Benchmark+and+Copilot+for+Efficient+Production+of+Object+Interactions+in+Virtual+Road+Scenes+Kairui+Yang+Zihao+Guo+Gengjie+Lin+Haotian+Dong+Die+Zuo",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "K Yang",
        "id": null
      },
      {
        "name": "Z Guo",
        "id": "z60frzcAAAAJ"
      },
      {
        "name": "G Lin",
        "id": null
      },
      {
        "name": "H Dong",
        "id": "tFiO2ggAAAAJ"
      },
      {
        "name": "D Zuo",
        "id": "wIW1-ToAAAAJ"
      },
      {
        "name": "J Peng",
        "id": "mNlNom4AAAAJ"
      },
      {
        "name": "Z Huang",
        "id": null
      },
      {
        "name": "Z Xu",
        "id": "VrcyIMoAAAAJ"
      },
      {
        "name": "F Li",
        "id": null
      },
      {
        "name": "Z Bai",
        "id": null
      },
      {
        "name": "D Lin",
        "id": "rW0r-hMAAAAJ"
      }
    ],
    "citation_count": 2
  },
  {
    "arxiv_id": "2405.05363",
    "title": "LOC-ZSON: Language-driven Object-Centric Zero-Shot Object Retrieval and Navigation",
    "year": 2024,
    "published": "2024-05-08T18:45:37Z",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "abstract": "In this paper, we present LOC-ZSON, a novel Language-driven Object-Centric image representation for object navigation task within complex scenes. We propose an object-centric image representation and corresponding losses for visual-language model (VLM) fine-tuning, which can handle complex object-level queries. In addition, we design a novel LLM-based augmentation and prompt templates for stability during training and zero-shot inference. We implement our method on Astro robot and deploy it in b",
    "arxiv_url": "https://arxiv.org/abs/2405.05363v1",
    "pdf_url": "https://arxiv.org/pdf/2405.05363v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2405.05363",
    "arxiv_authors": [
      "Tianrui Guan",
      "Yurou Yang",
      "Harry Cheng",
      "Muyuan Lin",
      "Richard Kim",
      "Rajasimman Madhivanan",
      "Arnie Sen",
      "Dinesh Manocha"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=LOC-ZSON%3A+Language-driven+Object-Centric+Zero-Shot+Object+Retrieval+and+Navigation+Tianrui+Guan+Yurou+Yang+Harry+Cheng+Muyuan+Lin+Richard+Kim",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "T Guan",
        "id": "_7mX21UAAAAJ"
      },
      {
        "name": "Y Yang",
        "id": null
      },
      {
        "name": "H Cheng",
        "id": null
      },
      {
        "name": "M Lin",
        "id": "hJ20hdcAAAAJ"
      },
      {
        "name": "R Kim",
        "id": "EuuYoFsAAAAJ"
      },
      {
        "name": "R Madhivanan",
        "id": null
      },
      {
        "name": "A Sen",
        "id": null
      },
      {
        "name": "D Manocha",
        "id": "X08l_4IAAAAJ"
      }
    ],
    "citation_count": 20
  },
  {
    "arxiv_id": "2305.18547",
    "title": "Learning from Multi-Perception Features for Real-Word Image Super-resolution",
    "year": 2023,
    "published": "2023-05-26T07:35:49Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Currently, there are two popular approaches for addressing real-world image super-resolution problems: degradation-estimation-based and blind-based methods. However, degradation-estimation-based methods may be inaccurate in estimating the degradation, making them less applicable to real-world LR images. On the other hand, blind-based methods are often limited by their fixed single perception information, which hinders their ability to handle diverse perceptual characteristics. To overcome this l",
    "arxiv_url": "https://arxiv.org/abs/2305.18547v1",
    "pdf_url": "https://arxiv.org/pdf/2305.18547v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2305.18547",
    "arxiv_authors": [
      "Axi Niu",
      "Kang Zhang",
      "Trung X. Pham",
      "Pei Wang",
      "Jinqiu Sun",
      "In So Kweon",
      "Yanning Zhang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Learning+from+Multi-Perception+Features+for+Real-Word+Image+Super-resolution+Axi+Niu+Kang+Zhang+Trung+X.+Pham+Pei+Wang+Jinqiu+Sun",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "A Niu",
        "id": "5apnc_UAAAAJ"
      },
      {
        "name": "K Zhang",
        "id": "nj19btQAAAAJ"
      },
      {
        "name": "TX Pham",
        "id": "4DkPIIAAAAAJ"
      },
      {
        "name": "P Wang",
        "id": "ZD1-waIAAAAJ"
      },
      {
        "name": "J Sun",
        "id": null
      },
      {
        "name": "IS Kweon",
        "id": "XA8EOlEAAAAJ"
      },
      {
        "name": "Y ZhangIEEE Transactions on Circuits and Systems for Video Technology",
        "id": null
      }
    ],
    "citation_count": 10
  },
  {
    "arxiv_id": "2303.02715",
    "title": "Deep Learning in the Field of Biometric Template Protection: An Overview",
    "year": 2023,
    "published": "2023-03-05T17:06:40Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Today, deep learning represents the most popular and successful form of machine learning. Deep learning has revolutionised the field of pattern recognition, including biometric recognition. Biometric systems utilising deep learning have been shown to achieve auspicious recognition accuracy, surpassing human performance. Apart from said breakthrough advances in terms of biometric performance, the use of deep learning was reported to impact different covariates of biometrics such as algorithmic fa",
    "arxiv_url": "https://arxiv.org/abs/2303.02715v1",
    "pdf_url": "https://arxiv.org/pdf/2303.02715v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2303.02715",
    "arxiv_authors": [
      "Christian Rathgeb",
      "Jascha Kolberg",
      "Andreas Uhl",
      "Christoph Busch"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Deep+Learning+in+the+Field+of+Biometric+Template+Protection%3A+An+Overview+Christian+Rathgeb+Jascha+Kolberg+Andreas+Uhl+Christoph+Busch",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "C Rathgeb",
        "id": "_itMaUcAAAAJ"
      },
      {
        "name": "J Kolberg",
        "id": "uGjQmuIAAAAJ"
      },
      {
        "name": "A Uhl",
        "id": null
      },
      {
        "name": "C Busch -",
        "id": null
      }
    ],
    "citation_count": 19
  },
  {
    "arxiv_id": "2411.15759",
    "title": "Advanced Learning-Based Inter Prediction for Future Video Coding",
    "year": 2024,
    "published": "2024-11-24T08:47:00Z",
    "categories": [
      "cs.MM",
      "cs.CV"
    ],
    "abstract": "In the fourth generation Audio Video coding Standard (AVS4), the Inter Prediction Filter (INTERPF) reduces discontinuities between prediction and adjacent reconstructed pixels in inter prediction. The paper proposes a low complexity learning-based inter prediction (LLIP) method to replace the traditional INTERPF. LLIP enhances the filtering process by leveraging a lightweight neural network model, where parameters can be exported for efficient inference. Specifically, we extract pixels and coord",
    "arxiv_url": "https://arxiv.org/abs/2411.15759v1",
    "pdf_url": "https://arxiv.org/pdf/2411.15759v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2411.15759",
    "arxiv_authors": [
      "Yanchen Zhao",
      "Wenhong Duan",
      "Chuanmin Jia",
      "Shanshe Wang",
      "Siwei Ma"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Advanced+Learning-Based+Inter+Prediction+for+Future+Video+Coding+Yanchen+Zhao+Wenhong+Duan+Chuanmin+Jia+Shanshe+Wang+Siwei+Ma",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "Y Zhao",
        "id": "RAwsZ7gAAAAJ"
      },
      {
        "name": "W Duan",
        "id": "EFg19aMAAAAJ"
      },
      {
        "name": "C Jia",
        "id": "x5Na9n0AAAAJ"
      },
      {
        "name": "S Wang",
        "id": "-Ouox68AAAAJ"
      },
      {
        "name": "S Ma2024 IEEE International",
        "id": null
      }
    ],
    "citation_count": 2
  },
  {
    "arxiv_id": "2309.05914",
    "title": "Medical Image Segmentation with Belief Function Theory and Deep Learning",
    "year": 2023,
    "published": "2023-09-12T02:04:36Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Deep learning has shown promising contributions in medical image segmentation with powerful learning and feature representation abilities. However, it has limitations for reasoning with and combining imperfect (imprecise, uncertain, and partial) information. In this thesis, we study medical image segmentation approaches with belief function theory and deep learning, specifically focusing on information modeling and fusion based on uncertain evidence.   First, we review existing belief function t",
    "arxiv_url": "https://arxiv.org/abs/2309.05914v1",
    "pdf_url": "https://arxiv.org/pdf/2309.05914v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2309.05914",
    "arxiv_authors": [
      "Ling Huang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Medical+Image+Segmentation+with+Belief+Function+Theory+and+Deep+Learning+Ling+Huang",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "L Huang -",
        "id": null
      }
    ],
    "citation_count": null
  },
  {
    "arxiv_id": "2405.13771",
    "title": "Multi-Dataset Multi-Task Learning for COVID-19 Prognosis",
    "year": 2024,
    "published": "2024-05-22T15:57:44Z",
    "categories": [
      "eess.IV",
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "In the fight against the COVID-19 pandemic, leveraging artificial intelligence to predict disease outcomes from chest radiographic images represents a significant scientific aim. The challenge, however, lies in the scarcity of large, labeled datasets with compatible tasks for training deep learning models without leading to overfitting. Addressing this issue, we introduce a novel multi-dataset multi-task training framework that predicts COVID-19 prognostic outcomes from chest X-rays (CXR) by int",
    "arxiv_url": "https://arxiv.org/abs/2405.13771v1",
    "pdf_url": "https://arxiv.org/pdf/2405.13771v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2405.13771",
    "arxiv_authors": [
      "Filippo Ruffini",
      "Lorenzo Tronchin",
      "Zhuoru Wu",
      "Wenting Chen",
      "Paolo Soda",
      "Linlin Shen",
      "Valerio Guarrasi"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Multi-Dataset+Multi-Task+Learning+for+COVID-19+Prognosis+Filippo+Ruffini+Lorenzo+Tronchin+Zhuoru+Wu+Wenting+Chen+Paolo+Soda",
    "gs_search_success": false,
    "gs_authors": null,
    "citation_count": null,
    "error_type": "fetch_failed"
  },
  {
    "arxiv_id": "2502.14129",
    "title": "GlossGau: Efficient Inverse Rendering for Glossy Surface with Anisotropic Spherical Gaussian",
    "year": 2025,
    "published": "2025-02-19T22:20:57Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "The reconstruction of 3D objects from calibrated photographs represents a fundamental yet intricate challenge in the domains of computer graphics and vision. Although neural reconstruction approaches based on Neural Radiance Fields (NeRF) have shown remarkable capabilities, their processing costs remain substantial. Recently, the advent of 3D Gaussian Splatting (3D-GS) largely improves the training efficiency and facilitates to generate realistic rendering in real-time. However, due to the limit",
    "arxiv_url": "https://arxiv.org/abs/2502.14129v1",
    "pdf_url": "https://arxiv.org/pdf/2502.14129v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2502.14129",
    "arxiv_authors": [
      "Bang Du",
      "Runfa Blark Li",
      "Chen Du",
      "Truong Nguyen"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=GlossGau%3A+Efficient+Inverse+Rendering+for+Glossy+Surface+with+Anisotropic+Spherical+Gaussian+Bang+Du+Runfa+Blark+Li+Chen+Du+Truong+Nguyen",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "B Du",
        "id": null
      },
      {
        "name": "RB Li",
        "id": "vntCBHQAAAAJ"
      },
      {
        "name": "C Du",
        "id": null
      },
      {
        "name": "T Nguyen -",
        "id": null
      }
    ],
    "citation_count": 1
  },
  {
    "arxiv_id": "2309.14908",
    "title": "Face Cartoonisation For Various Poses Using StyleGAN",
    "year": 2023,
    "published": "2023-09-26T13:10:25Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "This paper presents an innovative approach to achieve face cartoonisation while preserving the original identity and accommodating various poses. Unlike previous methods in this field that relied on conditional-GANs, which posed challenges related to dataset requirements and pose training, our approach leverages the expressive latent space of StyleGAN. We achieve this by introducing an encoder that captures both pose and identity information from images and generates a corresponding embedding wi",
    "arxiv_url": "https://arxiv.org/abs/2309.14908v1",
    "pdf_url": "https://arxiv.org/pdf/2309.14908v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2309.14908",
    "arxiv_authors": [
      "Kushal Jain",
      "Ankith Varun J",
      "Anoop Namboodiri"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Face+Cartoonisation+For+Various+Poses+Using+StyleGAN+Kushal+Jain+Ankith+Varun+J+Anoop+Namboodiri",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "K Jain",
        "id": "YdvloxsAAAAJ"
      },
      {
        "name": "A Namboodiri -",
        "id": null
      }
    ],
    "citation_count": null
  },
  {
    "arxiv_id": "2407.14128",
    "title": "OCTolyzer: Fully automatic toolkit for segmentation and feature extracting in optical coherence tomography and scanning laser ophthalmoscopy data",
    "year": 2024,
    "published": "2024-07-19T08:56:12Z",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "abstract": "Optical coherence tomography (OCT) and scanning laser ophthalmoscopy (SLO) of the eye has become essential to ophthalmology and the emerging field of oculomics, thus requiring a need for transparent, reproducible, and rapid analysis of this data for clinical research and the wider research community. Here, we introduce OCTolyzer, the first open-source toolkit for retinochoroidal analysis in OCT/SLO data. It features two analysis suites for OCT and SLO data, facilitating deep learning-based anato",
    "arxiv_url": "https://arxiv.org/abs/2407.14128v2",
    "pdf_url": "https://arxiv.org/pdf/2407.14128v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2407.14128",
    "arxiv_authors": [
      "Jamie Burke",
      "Justin Engelmann",
      "Samuel Gibbon",
      "Charlene Hamid",
      "Diana Moukaddem",
      "Dan Pugh",
      "Tariq Farrah",
      "Niall Strang",
      "Neeraj Dhaun",
      "Tom MacGillivray",
      "Stuart King",
      "Ian J. C. MacCormick"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=OCTolyzer%3A+Fully+automatic+toolkit+for+segmentation+and+feature+extracting+in+optical+coherence+tomography+and+scanning+laser+ophthalmoscopy+data+Jamie+Burke+Justin+Engelmann+Samuel+Gibbon+Charlene+Hamid+Diana+Moukaddem",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "J Burke",
        "id": "bxadaNQAAAAJ"
      },
      {
        "name": "J Engelmann",
        "id": "ckuoXnYAAAAJ"
      },
      {
        "name": "S Gibbon",
        "id": "0u0XGjwAAAAJ"
      },
      {
        "name": "C Hamid",
        "id": null
      },
      {
        "name": "D Moukaddem",
        "id": null
      },
      {
        "name": "D Pugh",
        "id": null
      },
      {
        "name": "T Farrah",
        "id": null
      },
      {
        "name": "N Strang",
        "id": "zj4MdaR5mEwC"
      }
    ],
    "citation_count": 2
  },
  {
    "arxiv_id": "2310.08143",
    "title": "A Deep Learning Framework for Spatiotemporal Ultrasound Localization Microscopy",
    "year": 2023,
    "published": "2023-10-12T08:58:01Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Ultrasound Localization Microscopy can resolve the microvascular bed down to a few micrometers. To achieve such performance microbubble contrast agents must perfuse the entire microvascular network. Microbubbles are then located individually and tracked over time to sample individual vessels, typically over hundreds of thousands of images. To overcome the fundamental limit of diffraction and achieve a dense reconstruction of the network, low microbubble concentrations must be used, which lead to",
    "arxiv_url": "https://arxiv.org/abs/2310.08143v1",
    "pdf_url": "https://arxiv.org/pdf/2310.08143v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2310.08143",
    "arxiv_authors": [
      "L√©o Milecki",
      "Jonathan Por√©e",
      "Hatim Belgharbi",
      "Chlo√© Bourquin",
      "Rafat Damseh",
      "Patrick Delafontaine-Martel",
      "Fr√©d√©ric Lesage",
      "Maxime Gasse",
      "Jean Provost"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=A+Deep+Learning+Framework+for+Spatiotemporal+Ultrasound+Localization+Microscopy+L%C3%A9o+Milecki+Jonathan+Por%C3%A9e+Hatim+Belgharbi+Chlo%C3%A9+Bourquin+Rafat+Damseh",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "L Milecki",
        "id": "R8ko8mYAAAAJ"
      },
      {
        "name": "J Por√©e",
        "id": "BJLUR4MAAAAJ"
      },
      {
        "name": "H Belgharbi",
        "id": "JKtfc9MAAAAJ"
      },
      {
        "name": "C Bourquin",
        "id": "fRXWK7cAAAAJ"
      },
      {
        "name": "R Damseh",
        "id": "vmQUi8AAAAAJ"
      },
      {
        "name": "P Delafontaine-Martel",
        "id": "knSqboUAAAAJ"
      },
      {
        "name": "F Lesage",
        "id": "HHvOcygAAAAJ"
      }
    ],
    "citation_count": 94
  },
  {
    "arxiv_id": "2408.16907",
    "title": "Ig3D: Integrating 3D Face Representations in Facial Expression Inference",
    "year": 2024,
    "published": "2024-08-29T21:08:07Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Reconstructing 3D faces with facial geometry from single images has allowed for major advances in animation, generative models, and virtual reality. However, this ability to represent faces with their 3D features is not as fully explored by the facial expression inference (FEI) community. This study therefore aims to investigate the impacts of integrating such 3D representations into the FEI task, specifically for facial expression classification and face-based valence-arousal (VA) estimation. T",
    "arxiv_url": "https://arxiv.org/abs/2408.16907v1",
    "pdf_url": "https://arxiv.org/pdf/2408.16907v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2408.16907",
    "arxiv_authors": [
      "Lu Dong",
      "Xiao Wang",
      "Srirangaraj Setlur",
      "Venu Govindaraju",
      "Ifeoma Nwogu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Ig3D%3A+Integrating+3D+Face+Representations+in+Facial+Expression+Inference+Lu+Dong+Xiao+Wang+Srirangaraj+Setlur+Venu+Govindaraju+Ifeoma+Nwogu",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "L Dong",
        "id": "48ReRMkAAAAJ"
      },
      {
        "name": "X Wang",
        "id": "d-TZH9gAAAAJ"
      },
      {
        "name": "S Setlur",
        "id": "BPEF3ZwAAAAJ"
      },
      {
        "name": "V Govindaraju",
        "id": "ruIgbscAAAAJ"
      },
      {
        "name": "I NwoguEuropean",
        "id": null
      }
    ],
    "citation_count": 3
  },
  {
    "arxiv_id": "2306.17431",
    "title": "Defense against Adversarial Cloud Attack on Remote Sensing Salient Object Detection",
    "year": 2023,
    "published": "2023-06-30T07:06:13Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Detecting the salient objects in a remote sensing image has wide applications for the interdisciplinary research. Many existing deep learning methods have been proposed for Salient Object Detection (SOD) in remote sensing images and get remarkable results. However, the recent adversarial attack examples, generated by changing a few pixel values on the original remote sensing image, could result in a collapse for the well-trained deep learning based SOD model. Different with existing methods addi",
    "arxiv_url": "https://arxiv.org/abs/2306.17431v2",
    "pdf_url": "https://arxiv.org/pdf/2306.17431v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2306.17431",
    "arxiv_authors": [
      "Huiming Sun",
      "Lan Fu",
      "Jinlong Li",
      "Qing Guo",
      "Zibo Meng",
      "Tianyun Zhang",
      "Yuewei Lin",
      "Hongkai Yu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Defense+against+Adversarial+Cloud+Attack+on+Remote+Sensing+Salient+Object+Detection+Huiming+Sun+Lan+Fu+Jinlong+Li+Qing+Guo+Zibo+Meng",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "H Sun",
        "id": "gN2GpPwAAAAJ"
      },
      {
        "name": "L Fu",
        "id": "RntkSRIAAAAJ"
      },
      {
        "name": "J Li",
        "id": "tYIQiHgAAAAJ"
      },
      {
        "name": "Q Guo",
        "id": null
      },
      {
        "name": "Z Meng",
        "id": "u0_ZWHcAAAAJ"
      },
      {
        "name": "T Zhang",
        "id": "DpeIOjEAAAAJ"
      },
      {
        "name": "Y Lin",
        "id": "wOFhljYAAAAJ"
      },
      {
        "name": "H Yu",
        "id": "JnQts0kAAAAJ"
      }
    ],
    "citation_count": 25
  },
  {
    "arxiv_id": "2305.00604",
    "title": "ISAAC Newton: Input-based Approximate Curvature for Newton's Method",
    "year": 2023,
    "published": "2023-05-01T00:00:04Z",
    "categories": [
      "cs.LG",
      "cs.CV",
      "math.OC",
      "stat.ML"
    ],
    "abstract": "We present ISAAC (Input-baSed ApproximAte Curvature), a novel method that conditions the gradient using selected second-order information and has an asymptotically vanishing computational overhead, assuming a batch size smaller than the number of neurons. We show that it is possible to compute a good conditioner based on only the input to a respective layer without a substantial computational overhead. The proposed method allows effective training even in small-batch stochastic regimes, which ma",
    "arxiv_url": "https://arxiv.org/abs/2305.00604v1",
    "pdf_url": "https://arxiv.org/pdf/2305.00604v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2305.00604",
    "arxiv_authors": [
      "Felix Petersen",
      "Tobias Sutter",
      "Christian Borgelt",
      "Dongsung Huh",
      "Hilde Kuehne",
      "Yuekai Sun",
      "Oliver Deussen"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=ISAAC+Newton%3A+Input-based+Approximate+Curvature+for+Newton%27s+Method+Felix+Petersen+Tobias+Sutter+Christian+Borgelt+Dongsung+Huh+Hilde+Kuehne",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "F Petersen",
        "id": "v8Kat6YAAAAJ"
      },
      {
        "name": "T Sutter",
        "id": "11gxHJIAAAAJ"
      },
      {
        "name": "C Borgelt",
        "id": "T50Bxb8AAAAJ"
      },
      {
        "name": "D Huh",
        "id": null
      },
      {
        "name": "H Kuehne",
        "id": "pxhCcH0AAAAJ"
      },
      {
        "name": "Y Sun",
        "id": "6T1XtW8AAAAJ"
      },
      {
        "name": "O Deussen",
        "id": "y3j0c80AAAAJ"
      }
    ],
    "citation_count": 10
  },
  {
    "arxiv_id": "2505.20810",
    "title": "The Role of AI in Early Detection of Life-Threatening Diseases: A Retinal Imaging Perspective",
    "year": 2025,
    "published": "2025-05-27T07:19:37Z",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "abstract": "Retinal imaging has emerged as a powerful, non-invasive modality for detecting and quantifying biomarkers of systemic diseases-ranging from diabetes and hypertension to Alzheimer's disease and cardiovascular disorders but current insights remain dispersed across platforms and specialties. Recent technological advances in optical coherence tomography (OCT/OCTA) and adaptive optics (AO) now deliver ultra-high-resolution scans (down to 5 Œºm ) with superior contrast and spatial integration, allowing",
    "arxiv_url": "https://arxiv.org/abs/2505.20810v1",
    "pdf_url": "https://arxiv.org/pdf/2505.20810v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2505.20810",
    "arxiv_authors": [
      "Tariq M Khan",
      "Toufique Ahmed Soomro",
      "Imran Razzak"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=The+Role+of+AI+in+Early+Detection+of+Life-Threatening+Diseases%3A+A+Retinal+Imaging+Perspective+Tariq+M+Khan+Toufique+Ahmed+Soomro+Imran+Razzak",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "TM Khan",
        "id": "YDx41x4AAAAJ"
      },
      {
        "name": "TA Soomro",
        "id": "cd3eBIcAAAAJ"
      },
      {
        "name": "I Razzak -",
        "id": null
      }
    ],
    "citation_count": 3
  },
  {
    "arxiv_id": "2412.08603",
    "title": "Design2GarmentCode: Turning Design Concepts to Tangible Garments Through Program Synthesis",
    "year": 2024,
    "published": "2024-12-11T18:26:45Z",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "abstract": "Sewing patterns, the essential blueprints for fabric cutting and tailoring, act as a crucial bridge between design concepts and producible garments. However, existing uni-modal sewing pattern generation models struggle to effectively encode complex design concepts with a multi-modal nature and correlate them with vectorized sewing patterns that possess precise geometric structures and intricate sewing relations. In this work, we propose a novel sewing pattern generation approach \\textbf{Design2G",
    "arxiv_url": "https://arxiv.org/abs/2412.08603v3",
    "pdf_url": "https://arxiv.org/pdf/2412.08603v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2412.08603",
    "arxiv_authors": [
      "Feng Zhou",
      "Ruiyang Liu",
      "Chen Liu",
      "Gaofeng He",
      "Yong-Lu Li",
      "Xiaogang Jin",
      "Huamin Wang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Design2GarmentCode%3A+Turning+Design+Concepts+to+Tangible+Garments+Through+Program+Synthesis+Feng+Zhou+Ruiyang+Liu+Chen+Liu+Gaofeng+He+Yong-Lu+Li",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "F Zhou",
        "id": null
      },
      {
        "name": "R Liu",
        "id": "YFsP900AAAAJ"
      },
      {
        "name": "C Liu",
        "id": null
      },
      {
        "name": "G He",
        "id": null
      },
      {
        "name": "YL Li",
        "id": "UExAaVgAAAAJ"
      },
      {
        "name": "X Jin",
        "id": "yryOvLwAAAAJ"
      },
      {
        "name": "H Wang",
        "id": null
      }
    ],
    "citation_count": 10
  },
  {
    "arxiv_id": "2505.22978",
    "title": "Pose-free 3D Gaussian splatting via shape-ray estimation",
    "year": 2025,
    "published": "2025-05-29T01:34:40Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "While generalizable 3D Gaussian splatting enables efficient, high-quality rendering of unseen scenes, it heavily depends on precise camera poses for accurate geometry. In real-world scenarios, obtaining accurate poses is challenging, leading to noisy pose estimates and geometric misalignments. To address this, we introduce SHARE, a pose-free, feed-forward Gaussian splatting framework that overcomes these ambiguities by joint shape and camera rays estimation. Instead of relying on explicit 3D tra",
    "arxiv_url": "https://arxiv.org/abs/2505.22978v3",
    "pdf_url": "https://arxiv.org/pdf/2505.22978v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2505.22978",
    "arxiv_authors": [
      "Youngju Na",
      "Taeyeon Kim",
      "Jumin Lee",
      "Kyu Beom Han",
      "Woo Jae Kim",
      "Sung-eui Yoon"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Pose-free+3D+Gaussian+splatting+via+shape-ray+estimation+Youngju+Na+Taeyeon+Kim+Jumin+Lee+Kyu+Beom+Han+Woo+Jae+Kim",
    "gs_search_success": false,
    "gs_authors": null,
    "citation_count": null,
    "error_type": "fetch_failed"
  },
  {
    "arxiv_id": "2502.06593",
    "title": "SAGI: Semantically Aligned and Uncertainty Guided AI Image Inpainting",
    "year": 2025,
    "published": "2025-02-10T15:56:28Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Recent advancements in generative AI have made text-guided image inpainting - adding, removing, or altering image regions using textual prompts - widely accessible. However, generating semantically correct photorealistic imagery, typically requires carefully-crafted prompts and iterative refinement by evaluating the realism of the generated content - tasks commonly performed by humans. To automate the generative process, we propose Semantically Aligned and Uncertainty Guided AI Image Inpainting ",
    "arxiv_url": "https://arxiv.org/abs/2502.06593v3",
    "pdf_url": "https://arxiv.org/pdf/2502.06593v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2502.06593",
    "arxiv_authors": [
      "Paschalis Giakoumoglou",
      "Dimitrios Karageorgiou",
      "Symeon Papadopoulos",
      "Panagiotis C. Petrantonakis"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=SAGI%3A+Semantically+Aligned+and+Uncertainty+Guided+AI+Image+Inpainting+Paschalis+Giakoumoglou+Dimitrios+Karageorgiou+Symeon+Papadopoulos+Panagiotis+C.+Petrantonakis",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "P Giakoumoglou",
        "id": "0g0Ot_8AAAAJ"
      },
      {
        "name": "D Karageorgiou",
        "id": "kFxdDCkAAAAJ"
      },
      {
        "name": "S Papadopoulos",
        "id": "GuhyORoAAAAJ"
      },
      {
        "name": "PC Petrantonakis",
        "id": "A-PkIjQAAAAJ"
      }
    ],
    "citation_count": 1
  },
  {
    "arxiv_id": "2307.13345",
    "title": "Do humans and Convolutional Neural Networks attend to similar areas during scene classification: Effects of task and image type",
    "year": 2023,
    "published": "2023-07-25T09:02:29Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.HC"
    ],
    "abstract": "Deep Learning models like Convolutional Neural Networks (CNN) are powerful image classifiers, but what factors determine whether they attend to similar image areas as humans do? While previous studies have focused on technological factors, little is known about the role of factors that affect human attention. In the present study, we investigated how the tasks used to elicit human attention maps interact with image characteristics in modulating the similarity between humans and CNN. We varied th",
    "arxiv_url": "https://arxiv.org/abs/2307.13345v2",
    "pdf_url": "https://arxiv.org/pdf/2307.13345v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2307.13345",
    "arxiv_authors": [
      "Romy M√ºller",
      "Marcel D√ºrschmidt",
      "Julian Ullrich",
      "Carsten Knoll",
      "Sascha Weber",
      "Steffen Seitz"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Do+humans+and+Convolutional+Neural+Networks+attend+to+similar+areas+during+scene+classification%3A+Effects+of+task+and+image+type+Romy+M%C3%BCller+Marcel+D%C3%BCrschmidt+Julian+Ullrich+Carsten+Knoll+Sascha+Weber",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "R M√ºller",
        "id": "kHq75jIAAAAJ"
      },
      {
        "name": "M D√ºrschmidt",
        "id": null
      },
      {
        "name": "J Ullrich",
        "id": null
      },
      {
        "name": "C Knoll",
        "id": "1tbndxgAAAAJ"
      },
      {
        "name": "S Weber",
        "id": null
      },
      {
        "name": "S SeitzApplied Sciences",
        "id": null
      }
    ],
    "citation_count": 9
  },
  {
    "arxiv_id": "2312.16516",
    "title": "ConstScene: Dataset and Model for Advancing Robust Semantic Segmentation in Construction Environments",
    "year": 2023,
    "published": "2023-12-27T10:49:19Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "The increasing demand for autonomous machines in construction environments necessitates the development of robust object detection algorithms that can perform effectively across various weather and environmental conditions. This paper introduces a new semantic segmentation dataset specifically tailored for construction sites, taking into account the diverse challenges posed by adverse weather and environmental conditions. The dataset is designed to enhance the training and evaluation of object d",
    "arxiv_url": "https://arxiv.org/abs/2312.16516v2",
    "pdf_url": "https://arxiv.org/pdf/2312.16516v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2312.16516",
    "arxiv_authors": [
      "Maghsood Salimi",
      "Mohammad Loni",
      "Sara Afshar",
      "Antonio Cicchetti",
      "Marjan Sirjani"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=ConstScene%3A+Dataset+and+Model+for+Advancing+Robust+Semantic+Segmentation+in+Construction+Environments+Maghsood+Salimi+Mohammad+Loni+Sara+Afshar+Antonio+Cicchetti+Marjan+Sirjani",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "M Salimi",
        "id": "x7kIHbwAAAAJ"
      },
      {
        "name": "M Loni",
        "id": "J2HhPSQAAAAJ"
      },
      {
        "name": "S Afshar",
        "id": null
      },
      {
        "name": "A Cicchetti",
        "id": "I-9eMoMAAAAJ"
      },
      {
        "name": "M SirjaniInternational",
        "id": null
      }
    ],
    "citation_count": 4
  },
  {
    "arxiv_id": "2411.15923",
    "title": "Deep Learning for automated multi-scale functional field boundaries extraction using multi-date Sentinel-2 and PlanetScope imagery: Case Study of Netherlands and Pakistan",
    "year": 2024,
    "published": "2024-11-24T17:10:36Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "abstract": "This study explores the effectiveness of multi-temporal satellite imagery for better functional field boundary delineation using deep learning semantic segmentation architecture on two distinct geographical and multi-scale farming systems of Netherlands and Pakistan. Multidate images of April, August and October 2022 were acquired for PlanetScope and Sentinel-2 in sub regions of Netherlands and November 2022, February and March 2023 for selected area of Dunyapur in Pakistan. For Netherlands, Bas",
    "arxiv_url": "https://arxiv.org/abs/2411.15923v1",
    "pdf_url": "https://arxiv.org/pdf/2411.15923v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2411.15923",
    "arxiv_authors": [
      "Saba Zahid",
      "Sajid Ghuffar",
      "Obaid-ur-Rehman",
      "Syed Roshaan Ali Shah"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Deep+Learning+for+automated+multi-scale+functional+field+boundaries+extraction+using+multi-date+Sentinel-2+and+PlanetScope+imagery%3A+Case+Study+of+Netherlands+and+Pakistan+Saba+Zahid+Sajid+Ghuffar+Obaid-ur-Rehman+Syed+Roshaan+Ali+Shah",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "S Zahid",
        "id": null
      },
      {
        "name": "S Ghuffar",
        "id": "IRbTXOMAAAAJ"
      },
      {
        "name": "SRA Shah -",
        "id": null
      }
    ],
    "citation_count": null
  },
  {
    "arxiv_id": "2401.12480",
    "title": "IDPro: Flexible Interactive Video Object Segmentation by ID-queried Concurrent Propagation",
    "year": 2024,
    "published": "2024-01-23T04:19:15Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Interactive Video Object Segmentation (iVOS) is a challenging task that requires real-time human-computer interaction. To improve the user experience, it is important to consider the user's input habits, segmentation quality, running time and memory consumption.However, existing methods compromise user experience with single input mode and slow running speed. Specifically, these methods only allow the user to interact with one single frame, which limits the expression of the user's intent.To ove",
    "arxiv_url": "https://arxiv.org/abs/2401.12480v3",
    "pdf_url": "https://arxiv.org/pdf/2401.12480v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2401.12480",
    "arxiv_authors": [
      "Kexin Li",
      "Tao Jiang",
      "Zongxin Yang",
      "Yi Yang",
      "Yueting Zhuang",
      "Jun Xiao"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=IDPro%3A+Flexible+Interactive+Video+Object+Segmentation+by+ID-queried+Concurrent+Propagation+Kexin+Li+Tao+Jiang+Zongxin+Yang+Yi+Yang+Yueting+Zhuang",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "K Li",
        "id": null
      },
      {
        "name": "T Jiang",
        "id": null
      },
      {
        "name": "Z Yang",
        "id": "8IE0CfwAAAAJ"
      },
      {
        "name": "Y Yang",
        "id": "RMSuNFwAAAAJ"
      },
      {
        "name": "Y Zhuang",
        "id": null
      },
      {
        "name": "J XiaoIEEE Transactions on Circuits and Systems for Video Technology",
        "id": null
      }
    ],
    "citation_count": 3
  },
  {
    "arxiv_id": "2411.07025",
    "title": "Scaling Mesh Generation via Compressive Tokenization",
    "year": 2024,
    "published": "2024-11-11T14:30:35Z",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "abstract": "We propose a compressive yet effective mesh representation, Blocked and Patchified Tokenization (BPT), facilitating the generation of meshes exceeding 8k faces. BPT compresses mesh sequences by employing block-wise indexing and patch aggregation, reducing their length by approximately 75\\% compared to the original sequences. This compression milestone unlocks the potential to utilize mesh data with significantly more faces, thereby enhancing detail richness and improving generation robustness. E",
    "arxiv_url": "https://arxiv.org/abs/2411.07025v1",
    "pdf_url": "https://arxiv.org/pdf/2411.07025v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2411.07025",
    "arxiv_authors": [
      "Haohan Weng",
      "Zibo Zhao",
      "Biwen Lei",
      "Xianghui Yang",
      "Jian Liu",
      "Zeqiang Lai",
      "Zhuo Chen",
      "Yuhong Liu",
      "Jie Jiang",
      "Chunchao Guo",
      "Tong Zhang",
      "Shenghua Gao",
      "C. L. Philip Chen"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Scaling+Mesh+Generation+via+Compressive+Tokenization+Haohan+Weng+Zibo+Zhao+Biwen+Lei+Xianghui+Yang+Jian+Liu",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "H Weng",
        "id": "V08HvHUAAAAJ"
      },
      {
        "name": "Z Zhao",
        "id": null
      },
      {
        "name": "B Lei",
        "id": "5d5Y9BoAAAAJ"
      },
      {
        "name": "X Yang",
        "id": "e9EzzWAAAAAJ"
      },
      {
        "name": "J Liu",
        "id": "lgtXgTUAAAAJ"
      },
      {
        "name": "Z Lai",
        "id": "WUMu1KkAAAAJ"
      },
      {
        "name": "Z Chen",
        "id": "qTIpn-8AAAAJ"
      },
      {
        "name": "Y Liu",
        "id": null
      },
      {
        "name": "J Jiang",
        "id": null
      },
      {
        "name": "C Guo",
        "id": null
      },
      {
        "name": "T Zhang",
        "id": "-neWbpUAAAAJ"
      },
      {
        "name": "S Gao",
        "id": "fe-1v0MAAAAJ"
      }
    ],
    "citation_count": 32
  },
  {
    "arxiv_id": "2412.00051",
    "title": "TransFair: Transferring Fairness from Ocular Disease Classification to Progression Prediction",
    "year": 2024,
    "published": "2024-11-24T06:39:06Z",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "cs.CY"
    ],
    "abstract": "The use of artificial intelligence (AI) in automated disease classification significantly reduces healthcare costs and improves the accessibility of services. However, this transformation has given rise to concerns about the fairness of AI, which disproportionately affects certain groups, particularly patients from underprivileged populations. Recently, a number of methods and large-scale datasets have been proposed to address group performance disparities. Although these methods have shown effe",
    "arxiv_url": "https://arxiv.org/abs/2412.00051v2",
    "pdf_url": "https://arxiv.org/pdf/2412.00051v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2412.00051",
    "arxiv_authors": [
      "Leila Gheisi",
      "Henry Chu",
      "Raju Gottumukkala",
      "Yan Luo",
      "Xingquan Zhu",
      "Mengyu Wang",
      "Min Shi"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=TransFair%3A+Transferring+Fairness+from+Ocular+Disease+Classification+to+Progression+Prediction+Leila+Gheisi+Henry+Chu+Raju+Gottumukkala+Yan+Luo+Xingquan+Zhu",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "L Gheisi",
        "id": "XOX8a7wAAAAJ"
      },
      {
        "name": "H Chu",
        "id": "B5PmuzgAAAAJ"
      },
      {
        "name": "R Gottumukkala",
        "id": null
      },
      {
        "name": "Y Luo",
        "id": null
      },
      {
        "name": "X Zhu",
        "id": "YhKZXtcAAAAJ"
      },
      {
        "name": "M Wang",
        "id": "i9B02k4AAAAJ"
      },
      {
        "name": "M Shi",
        "id": "NoA9KFIAAAAJ"
      }
    ],
    "citation_count": 1
  },
  {
    "arxiv_id": "2505.12978",
    "title": "Enhancing Diffusion-Weighted Images (DWI) for Diffusion MRI: Is it Enough without Non-Diffusion-Weighted B=0 Reference?",
    "year": 2025,
    "published": "2025-05-19T11:16:43Z",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "abstract": "Diffusion MRI (dMRI) is essential for studying brain microstructure, but high-resolution imaging remains challenging due to the inherent trade-offs between acquisition time and signal-to-noise ratio (SNR). Conventional methods often optimize only the diffusion-weighted images (DWIs) without considering their relationship with the non-diffusion-weighted (b=0) reference images. However, calculating diffusion metrics, such as the apparent diffusion coefficient (ADC) and diffusion tensor with its de",
    "arxiv_url": "https://arxiv.org/abs/2505.12978v1",
    "pdf_url": "https://arxiv.org/pdf/2505.12978v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2505.12978",
    "arxiv_authors": [
      "Yinzhe Wu",
      "Jiahao Huang",
      "Fanwen Wang",
      "Mengze Gao",
      "Congyu Liao",
      "Guang Yang",
      "Kawin Setsompop"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Enhancing+Diffusion-Weighted+Images+%28DWI%29+for+Diffusion+MRI%3A+Is+it+Enough+without+Non-Diffusion-Weighted+B%3D0+Reference%3F+Yinzhe+Wu+Jiahao+Huang+Fanwen+Wang+Mengze+Gao+Congyu+Liao",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "Y Wu",
        "id": "1EsA7vcAAAAJ"
      },
      {
        "name": "J Huang",
        "id": "ap-tq8cAAAAJ"
      },
      {
        "name": "F Wang",
        "id": "-lT1KRoAAAAJ"
      },
      {
        "name": "M Gao",
        "id": "A2v2qs0AAAAJ"
      },
      {
        "name": "C Liao",
        "id": "MY87HSAAAAAJ"
      },
      {
        "name": "G Yang",
        "id": "ZfzEFpsAAAAJ"
      },
      {
        "name": "K Setsompop2025 IEEE 22nd International",
        "id": null
      }
    ],
    "citation_count": 1
  },
  {
    "arxiv_id": "2407.15337",
    "title": "ThermalNeRF: Thermal Radiance Fields",
    "year": 2024,
    "published": "2024-07-22T02:51:29Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Thermal imaging has a variety of applications, from agricultural monitoring to building inspection to imaging under poor visibility, such as in low light, fog, and rain. However, reconstructing thermal scenes in 3D presents several challenges due to the comparatively lower resolution and limited features present in long-wave infrared (LWIR) images. To overcome these challenges, we propose a unified framework for scene reconstruction from a set of LWIR and RGB images, using a multispectral radian",
    "arxiv_url": "https://arxiv.org/abs/2407.15337v1",
    "pdf_url": "https://arxiv.org/pdf/2407.15337v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2407.15337",
    "arxiv_authors": [
      "Yvette Y. Lin",
      "Xin-Yi Pan",
      "Sara Fridovich-Keil",
      "Gordon Wetzstein"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=ThermalNeRF%3A+Thermal+Radiance+Fields+Yvette+Y.+Lin+Xin-Yi+Pan+Sara+Fridovich-Keil+Gordon+Wetzstein",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "YY Lin",
        "id": "-ZPBTaQAAAAJ"
      },
      {
        "name": "XY Pan",
        "id": null
      },
      {
        "name": "S Fridovich-Keil",
        "id": "9xF7M6wAAAAJ"
      },
      {
        "name": "G Wetzstein2024 IEEE International",
        "id": null
      }
    ],
    "citation_count": 19
  },
  {
    "arxiv_id": "2303.09234",
    "title": "NAISR: A 3D Neural Additive Model for Interpretable Shape Representation",
    "year": 2023,
    "published": "2023-03-16T11:18:04Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Deep implicit functions (DIFs) have emerged as a powerful paradigm for many computer vision tasks such as 3D shape reconstruction, generation, registration, completion, editing, and understanding. However, given a set of 3D shapes with associated covariates there is at present no shape representation method which allows to precisely represent the shapes while capturing the individual dependencies on each covariate. Such a method would be of high utility to researchers to discover knowledge hidde",
    "arxiv_url": "https://arxiv.org/abs/2303.09234v5",
    "pdf_url": "https://arxiv.org/pdf/2303.09234v5",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2303.09234",
    "arxiv_authors": [
      "Yining Jiao",
      "Carlton Zdanski",
      "Julia Kimbell",
      "Andrew Prince",
      "Cameron Worden",
      "Samuel Kirse",
      "Christopher Rutter",
      "Benjamin Shields",
      "William Dunn",
      "Jisan Mahmud",
      "Marc Niethammer"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=NAISR%3A+A+3D+Neural+Additive+Model+for+Interpretable+Shape+Representation+Yining+Jiao+Carlton+Zdanski+Julia+Kimbell+Andrew+Prince+Cameron+Worden",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "Y Jiao",
        "id": "9FVJ-CkAAAAJ"
      },
      {
        "name": "C Zdanski",
        "id": null
      },
      {
        "name": "J Kimbell",
        "id": "gJviokgAAAAJ"
      },
      {
        "name": "A Prince",
        "id": null
      },
      {
        "name": "C Worden",
        "id": null
      },
      {
        "name": "S Kirse",
        "id": null
      },
      {
        "name": "C Rutter",
        "id": null
      },
      {
        "name": "B Shields",
        "id": "GifAaXcAAAAJ"
      },
      {
        "name": "W Dunn",
        "id": null
      }
    ],
    "citation_count": 6
  },
  {
    "arxiv_id": "2407.07726",
    "title": "PaliGemma: A versatile 3B VLM for transfer",
    "year": 2024,
    "published": "2024-07-10T14:57:46Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "abstract": "PaliGemma is an open Vision-Language Model (VLM) that is based on the SigLIP-So400m vision encoder and the Gemma-2B language model. It is trained to be a versatile and broadly knowledgeable base model that is effective to transfer. It achieves strong performance on a wide variety of open-world tasks. We evaluate PaliGemma on almost 40 diverse tasks including standard VLM benchmarks, but also more specialized tasks such as remote-sensing and segmentation.",
    "arxiv_url": "https://arxiv.org/abs/2407.07726v2",
    "pdf_url": "https://arxiv.org/pdf/2407.07726v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2407.07726",
    "arxiv_authors": [
      "Lucas Beyer",
      "Andreas Steiner",
      "Andr√© Susano Pinto",
      "Alexander Kolesnikov",
      "Xiao Wang",
      "Daniel Salz",
      "Maxim Neumann",
      "Ibrahim Alabdulmohsin",
      "Michael Tschannen",
      "Emanuele Bugliarello",
      "Thomas Unterthiner",
      "Daniel Keysers",
      "Skanda Koppula",
      "Fangyu Liu",
      "Adam Grycner",
      "Alexey Gritsenko",
      "Neil Houlsby",
      "Manoj Kumar",
      "Keran Rong",
      "Julian Eisenschlos",
      "Rishabh Kabra",
      "Matthias Bauer",
      "Matko Bo≈°njak",
      "Xi Chen",
      "Matthias Minderer",
      "Paul Voigtlaender",
      "Ioana Bica",
      "Ivana Balazevic",
      "Joan Puigcerver",
      "Pinelopi Papalampidi",
      "Olivier Henaff",
      "Xi Xiong",
      "Radu Soricut",
      "Jeremiah Harmsen",
      "Xiaohua Zhai"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=PaliGemma%3A+A+versatile+3B+VLM+for+transfer+Lucas+Beyer+Andreas+Steiner+Andr%C3%A9+Susano+Pinto+Alexander+Kolesnikov+Xiao+Wang",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "L Beyer",
        "id": "p2gwhK4AAAAJ"
      },
      {
        "name": "A Steiner",
        "id": "vIZeAu4AAAAJ"
      },
      {
        "name": "AS Pinto",
        "id": "pTYo1vYAAAAJ"
      },
      {
        "name": "A Kolesnikov",
        "id": "H9I0CVwAAAAJ"
      },
      {
        "name": "X Wang",
        "id": "ukyXqzMAAAAJ"
      },
      {
        "name": "D Salz",
        "id": "7g-qqJIAAAAJ"
      },
      {
        "name": "M Neumann",
        "id": "XaX1OGIAAAAJ"
      },
      {
        "name": "I Alabdulmohsin",
        "id": "8WNMsPYAAAAJ"
      }
    ],
    "citation_count": 445
  },
  {
    "arxiv_id": "2412.08580",
    "title": "LAION-SG: An Enhanced Large-Scale Dataset for Training Complex Image-Text Models with Structural Annotations",
    "year": 2024,
    "published": "2024-12-11T17:57:10Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Recent advances in text-to-image (T2I) generation have shown remarkable success in producing high-quality images from text. However, existing T2I models show decayed performance in compositional image generation involving multiple objects and intricate relationships. We attribute this problem to limitations in existing datasets of image-text pairs, which lack precise inter-object relationship annotations with prompts only. To address this problem, we construct LAION-SG, a large-scale dataset wit",
    "arxiv_url": "https://arxiv.org/abs/2412.08580v2",
    "pdf_url": "https://arxiv.org/pdf/2412.08580v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2412.08580",
    "arxiv_authors": [
      "Zejian Li",
      "Chenye Meng",
      "Yize Li",
      "Ling Yang",
      "Shengyuan Zhang",
      "Jiarui Ma",
      "Jiayi Li",
      "Guang Yang",
      "Changyuan Yang",
      "Zhiyuan Yang",
      "Jinxiong Chang",
      "Lingyun Sun"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=LAION-SG%3A+An+Enhanced+Large-Scale+Dataset+for+Training+Complex+Image-Text+Models+with+Structural+Annotations+Zejian+Li+Chenye+Meng+Yize+Li+Ling+Yang+Shengyuan+Zhang",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "Z Li",
        "id": "VhdTIdMAAAAJ"
      },
      {
        "name": "C Meng",
        "id": null
      },
      {
        "name": "Y Li",
        "id": null
      },
      {
        "name": "L Yang",
        "id": "sIKujqAAAAAJ"
      },
      {
        "name": "S Zhang",
        "id": "mIChtY4AAAAJ"
      },
      {
        "name": "J Ma",
        "id": null
      },
      {
        "name": "J Li",
        "id": null
      },
      {
        "name": "G Yang",
        "id": null
      },
      {
        "name": "C Yang",
        "id": null
      },
      {
        "name": "Z Yang",
        "id": null
      },
      {
        "name": "J Chang",
        "id": null
      },
      {
        "name": "L Sun",
        "id": "zzW8d-wAAAAJ"
      }
    ],
    "citation_count": 9
  },
  {
    "arxiv_id": "2310.02129",
    "title": "Unveiling the Pitfalls of Knowledge Editing for Large Language Models",
    "year": 2023,
    "published": "2023-10-03T15:10:46Z",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV",
      "cs.DB",
      "cs.LG"
    ],
    "abstract": "As the cost associated with fine-tuning Large Language Models (LLMs) continues to rise, recent research efforts have pivoted towards developing methodologies to edit implicit knowledge embedded within LLMs. Yet, there's still a dark cloud lingering overhead -- will knowledge editing trigger butterfly effect? since it is still unclear whether knowledge editing might introduce side effects that pose potential risks or not. This paper pioneers the investigation into the potential pitfalls associate",
    "arxiv_url": "https://arxiv.org/abs/2310.02129v5",
    "pdf_url": "https://arxiv.org/pdf/2310.02129v5",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2310.02129",
    "arxiv_authors": [
      "Zhoubo Li",
      "Ningyu Zhang",
      "Yunzhi Yao",
      "Mengru Wang",
      "Xi Chen",
      "Huajun Chen"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Unveiling+the+Pitfalls+of+Knowledge+Editing+for+Large+Language+Models+Zhoubo+Li+Ningyu+Zhang+Yunzhi+Yao+Mengru+Wang+Xi+Chen",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "Z Li",
        "id": "KUydfvUAAAAJ"
      },
      {
        "name": "N Zhang",
        "id": "xQDOPvsAAAAJ"
      },
      {
        "name": "Y Yao",
        "id": "nAagIwEAAAAJ"
      },
      {
        "name": "M Wang",
        "id": "P3bp0egAAAAJ"
      },
      {
        "name": "X Chen",
        "id": "qy0QX0MAAAAJ"
      },
      {
        "name": "H Chen",
        "id": "T6om-m4AAAAJ"
      }
    ],
    "citation_count": 80
  },
  {
    "arxiv_id": "2406.18864",
    "title": "Learning Modality Knowledge Alignment for Cross-Modality Transfer",
    "year": 2024,
    "published": "2024-06-27T03:23:47Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Cross-modality transfer aims to leverage large pretrained models to complete tasks that may not belong to the modality of pretraining data. Existing works achieve certain success in extending classical finetuning to cross-modal scenarios, yet we still lack understanding about the influence of modality gap on the transfer. In this work, a series of experiments focusing on the source representation quality during transfer are conducted, revealing the connection between larger modality gap and less",
    "arxiv_url": "https://arxiv.org/abs/2406.18864v1",
    "pdf_url": "https://arxiv.org/pdf/2406.18864v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2406.18864",
    "arxiv_authors": [
      "Wenxuan Ma",
      "Shuang Li",
      "Lincan Cai",
      "Jingxuan Kang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Learning+Modality+Knowledge+Alignment+for+Cross-Modality+Transfer+Wenxuan+Ma+Shuang+Li+Lincan+Cai+Jingxuan+Kang",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "W Ma",
        "id": null
      },
      {
        "name": "S Li",
        "id": "VXCiAc4AAAAJ"
      },
      {
        "name": "L Cai",
        "id": "wH-dNbAAAAAJ"
      },
      {
        "name": "J Kang -",
        "id": null
      }
    ],
    "citation_count": 11
  },
  {
    "arxiv_id": "2410.18079",
    "title": "FreeVS: Generative View Synthesis on Free Driving Trajectory",
    "year": 2024,
    "published": "2024-10-23T17:59:11Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Existing reconstruction-based novel view synthesis methods for driving scenes focus on synthesizing camera views along the recorded trajectory of the ego vehicle. Their image rendering performance will severely degrade on viewpoints falling out of the recorded trajectory, where camera rays are untrained. We propose FreeVS, a novel fully generative approach that can synthesize camera views on free new trajectories in real driving scenes. To control the generation results to be 3D consistent with ",
    "arxiv_url": "https://arxiv.org/abs/2410.18079v1",
    "pdf_url": "https://arxiv.org/pdf/2410.18079v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2410.18079",
    "arxiv_authors": [
      "Qitai Wang",
      "Lue Fan",
      "Yuqi Wang",
      "Yuntao Chen",
      "Zhaoxiang Zhang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=FreeVS%3A+Generative+View+Synthesis+on+Free+Driving+Trajectory+Qitai+Wang+Lue+Fan+Yuqi+Wang+Yuntao+Chen+Zhaoxiang+Zhang",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "Q Wang",
        "id": "RgM_VVIAAAAJ"
      },
      {
        "name": "L Fan",
        "id": "6ZzmkHEAAAAJ"
      },
      {
        "name": "Y Wang",
        "id": "35UcX9sAAAAJ"
      },
      {
        "name": "Y Chen",
        "id": "iLOoUqIAAAAJ"
      },
      {
        "name": "Z Zhang -",
        "id": null
      }
    ],
    "citation_count": 28
  },
  {
    "arxiv_id": "2402.05773",
    "title": "UAV-Rain1k: A Benchmark for Raindrop Removal from UAV Aerial Imagery",
    "year": 2024,
    "published": "2024-02-08T16:00:25Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Raindrops adhering to the lens of UAVs can obstruct visibility of the background scene and degrade image quality. Despite recent progress in image deraining methods and datasets, there is a lack of focus on raindrop removal from UAV aerial imagery due to the unique challenges posed by varying angles and rapid movement during drone flight. To fill the gap in this research, we first construct a new benchmark dataset for removing raindrops from UAV images, called UAV-Rain1k. In this letter, we prov",
    "arxiv_url": "https://arxiv.org/abs/2402.05773v3",
    "pdf_url": "https://arxiv.org/pdf/2402.05773v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2402.05773",
    "arxiv_authors": [
      "Wenhui Chang",
      "Hongming Chen",
      "Xin He",
      "Xiang Chen",
      "Liangduo Shen"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=UAV-Rain1k%3A+A+Benchmark+for+Raindrop+Removal+from+UAV+Aerial+Imagery+Wenhui+Chang+Hongming+Chen+Xin+He+Xiang+Chen+Liangduo+Shen",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "W Chang",
        "id": "L7K-ywIAAAAJ"
      },
      {
        "name": "H Chen",
        "id": "2DBuqfkAAAAJ"
      },
      {
        "name": "X He",
        "id": null
      },
      {
        "name": "X Chen",
        "id": "C9LyxWEAAAAJ"
      },
      {
        "name": "L Shen",
        "id": "vjqtXegAAAAJ"
      }
    ],
    "citation_count": 20
  },
  {
    "arxiv_id": "2504.20438",
    "title": "PixelHacker: Image Inpainting with Structural and Semantic Consistency",
    "year": 2025,
    "published": "2025-04-29T05:28:36Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Image inpainting is a fundamental research area between image editing and image generation. Recent state-of-the-art (SOTA) methods have explored novel attention mechanisms, lightweight architectures, and context-aware modeling, demonstrating impressive performance. However, they often struggle with complex structure (e.g., texture, shape, spatial relations) and semantics (e.g., color consistency, object restoration, and logical correctness), leading to artifacts and inappropriate generation. To ",
    "arxiv_url": "https://arxiv.org/abs/2504.20438v2",
    "pdf_url": "https://arxiv.org/pdf/2504.20438v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2504.20438",
    "arxiv_authors": [
      "Ziyang Xu",
      "Kangsheng Duan",
      "Xiaolei Shen",
      "Zhifeng Ding",
      "Wenyu Liu",
      "Xiaohu Ruan",
      "Xiaoxin Chen",
      "Xinggang Wang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=PixelHacker%3A+Image+Inpainting+with+Structural+and+Semantic+Consistency+Ziyang+Xu+Kangsheng+Duan+Xiaolei+Shen+Zhifeng+Ding+Wenyu+Liu",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "Z Xu",
        "id": "ZYwJrRMAAAAJ"
      },
      {
        "name": "K Duan",
        "id": null
      },
      {
        "name": "X Shen",
        "id": null
      },
      {
        "name": "Z Ding",
        "id": null
      },
      {
        "name": "W Liu",
        "id": null
      },
      {
        "name": "X Ruan",
        "id": null
      },
      {
        "name": "X Chen",
        "id": null
      },
      {
        "name": "X Wang",
        "id": "qNCTLV0AAAAJ"
      }
    ],
    "citation_count": 2
  },
  {
    "arxiv_id": "2504.09149",
    "title": "MASH: Masked Anchored SpHerical Distances for 3D Shape Representation and Generation",
    "year": 2025,
    "published": "2025-04-12T09:28:12Z",
    "categories": [
      "cs.CV",
      "cs.CG"
    ],
    "abstract": "We introduce Masked Anchored SpHerical Distances (MASH), a novel multi-view and parametrized representation of 3D shapes. Inspired by multi-view geometry and motivated by the importance of perceptual shape understanding for learning 3D shapes, MASH represents a 3D shape as a collection of observable local surface patches, each defined by a spherical distance function emanating from an anchor point. We further leverage the compactness of spherical harmonics to encode the MASH functions, combined ",
    "arxiv_url": "https://arxiv.org/abs/2504.09149v3",
    "pdf_url": "https://arxiv.org/pdf/2504.09149v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2504.09149",
    "arxiv_authors": [
      "Changhao Li",
      "Yu Xin",
      "Xiaowei Zhou",
      "Ariel Shamir",
      "Hao Zhang",
      "Ligang Liu",
      "Ruizhen Hu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=MASH%3A+Masked+Anchored+SpHerical+Distances+for+3D+Shape+Representation+and+Generation+Changhao+Li+Yu+Xin+Xiaowei+Zhou+Ariel+Shamir+Hao+Zhang",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "C Li",
        "id": null
      },
      {
        "name": "Y Xin",
        "id": null
      },
      {
        "name": "X Zhou",
        "id": "E1vVpg4AAAAJ"
      },
      {
        "name": "A Shamir",
        "id": "-q90a0EAAAAJ"
      },
      {
        "name": "H Zhang",
        "id": null
      },
      {
        "name": "L Liu",
        "id": null
      },
      {
        "name": "R Hu",
        "id": "MloRITsAAAAJ"
      }
    ],
    "citation_count": 1
  },
  {
    "arxiv_id": "2304.11422",
    "title": "STNet: Spatial and Temporal feature fusion network for change detection in remote sensing images",
    "year": 2023,
    "published": "2023-04-22T14:40:41Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "As an important task in remote sensing image analysis, remote sensing change detection (RSCD) aims to identify changes of interest in a region from spatially co-registered multi-temporal remote sensing images, so as to monitor the local development. Existing RSCD methods usually formulate RSCD as a binary classification task, representing changes of interest by merely feature concatenation or feature subtraction and recovering the spatial details via densely connected change representations, who",
    "arxiv_url": "https://arxiv.org/abs/2304.11422v1",
    "pdf_url": "https://arxiv.org/pdf/2304.11422v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2304.11422",
    "arxiv_authors": [
      "Xiaowen Ma",
      "Jiawei Yang",
      "Tingfeng Hong",
      "Mengting Ma",
      "Ziyan Zhao",
      "Tian Feng",
      "Wei Zhang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=STNet%3A+Spatial+and+Temporal+feature+fusion+network+for+change+detection+in+remote+sensing+images+Xiaowen+Ma+Jiawei+Yang+Tingfeng+Hong+Mengting+Ma+Ziyan+Zhao",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "X Ma",
        "id": "UXj8Q6kAAAAJ"
      },
      {
        "name": "J Yang",
        "id": null
      },
      {
        "name": "T Hong",
        "id": null
      },
      {
        "name": "M Ma",
        "id": null
      },
      {
        "name": "Z Zhao",
        "id": null
      },
      {
        "name": "T Feng",
        "id": "fhZyIeEAAAAJ"
      },
      {
        "name": "W Zhang2023 IEEE International",
        "id": null
      }
    ],
    "citation_count": 36
  },
  {
    "arxiv_id": "2402.17207",
    "title": "Deployment Prior Injection for Run-time Calibratable Object Detection",
    "year": 2024,
    "published": "2024-02-27T04:56:04Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "With a strong alignment between the training and test distributions, object relation as a context prior facilitates object detection. Yet, it turns into a harmful but inevitable training set bias upon test distributions that shift differently across space and time. Nevertheless, the existing detectors cannot incorporate deployment context prior during the test phase without parameter update. Such kind of capability requires the model to explicitly learn disentangled representations with respect ",
    "arxiv_url": "https://arxiv.org/abs/2402.17207v1",
    "pdf_url": "https://arxiv.org/pdf/2402.17207v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2402.17207",
    "arxiv_authors": [
      "Mo Zhou",
      "Yiding Yang",
      "Haoxiang Li",
      "Vishal M. Patel",
      "Gang Hua"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Deployment+Prior+Injection+for+Run-time+Calibratable+Object+Detection+Mo+Zhou+Yiding+Yang+Haoxiang+Li+Vishal+M.+Patel+Gang+Hua",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "M Zhou",
        "id": "BVIO95UAAAAJ"
      },
      {
        "name": "Y Yang",
        "id": "8wqco1kAAAAJ"
      },
      {
        "name": "H Li",
        "id": null
      },
      {
        "name": "VM Patel",
        "id": "AkEXTbIAAAAJ"
      },
      {
        "name": "G Hua -",
        "id": null
      }
    ],
    "citation_count": null
  },
  {
    "arxiv_id": "2303.17354",
    "title": "ISSTAD: Incremental Self-Supervised Learning Based on Transformer for Anomaly Detection and Localization",
    "year": 2023,
    "published": "2023-03-30T13:11:26Z",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "In the realm of machine learning, the study of anomaly detection and localization within image data has gained substantial traction, particularly for practical applications such as industrial defect detection. While the majority of existing methods predominantly use Convolutional Neural Networks (CNN) as their primary network architecture, we introduce a novel approach based on the Transformer backbone network. Our method employs a two-stage incremental learning strategy. During the first stage,",
    "arxiv_url": "https://arxiv.org/abs/2303.17354v4",
    "pdf_url": "https://arxiv.org/pdf/2303.17354v4",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2303.17354",
    "arxiv_authors": [
      "Wenping Jin",
      "Fei Guo",
      "Li Zhu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=ISSTAD%3A+Incremental+Self-Supervised+Learning+Based+on+Transformer+for+Anomaly+Detection+and+Localization+Wenping+Jin+Fei+Guo+Li+Zhu",
    "gs_search_success": false,
    "gs_authors": null,
    "citation_count": null,
    "error_type": "fetch_failed"
  },
  {
    "arxiv_id": "2411.05747",
    "title": "WavShadow: Wavelet Based Shadow Segmentation and Removal",
    "year": 2024,
    "published": "2024-11-08T18:08:33Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Shadow removal and segmentation remain challenging tasks in computer vision, particularly in complex real world scenarios. This study presents a novel approach that enhances the ShadowFormer model by incorporating Masked Autoencoder (MAE) priors and Fast Fourier Convolution (FFC) blocks, leading to significantly faster convergence and improved performance. We introduce key innovations: (1) integration of MAE priors trained on Places2 dataset for better context understanding, (2) adoption of Haar",
    "arxiv_url": "https://arxiv.org/abs/2411.05747v3",
    "pdf_url": "https://arxiv.org/pdf/2411.05747v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2411.05747",
    "arxiv_authors": [
      "Shreyans Jain",
      "Viraj Vekaria",
      "Karan Gandhi",
      "Aadya Arora"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=WavShadow%3A+Wavelet+Based+Shadow+Segmentation+and+Removal+Shreyans+Jain+Viraj+Vekaria+Karan+Gandhi+Aadya+Arora",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "S Jain",
        "id": "ZxlWev4AAAAJ"
      },
      {
        "name": "V Vekaria",
        "id": "3rA4X5cAAAAJ"
      },
      {
        "name": "K Gandhi",
        "id": "aWA7Y5UAAAAJ"
      },
      {
        "name": "A Arora -",
        "id": null
      }
    ],
    "citation_count": null
  },
  {
    "arxiv_id": "2503.22180",
    "title": "Knowledge Rectification for Camouflaged Object Detection: Unlocking Insights from Low-Quality Data",
    "year": 2025,
    "published": "2025-03-28T06:53:21Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Low-quality data often suffer from insufficient image details, introducing an extra implicit aspect of camouflage that complicates camouflaged object detection (COD). Existing COD methods focus primarily on high-quality data, overlooking the challenges posed by low-quality data, which leads to significant performance degradation. Therefore, we propose KRNet, the first framework explicitly designed for COD on low-quality data. KRNet presents a Leader-Follower framework where the Leader extracts d",
    "arxiv_url": "https://arxiv.org/abs/2503.22180v1",
    "pdf_url": "https://arxiv.org/pdf/2503.22180v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2503.22180",
    "arxiv_authors": [
      "Juwei Guan",
      "Xiaolin Fang",
      "Donghyun Kim",
      "Haotian Gong",
      "Tongxin Zhu",
      "Zhen Ling",
      "Ming Yang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Knowledge+Rectification+for+Camouflaged+Object+Detection%3A+Unlocking+Insights+from+Low-Quality+Data+Juwei+Guan+Xiaolin+Fang+Donghyun+Kim+Haotian+Gong+Tongxin+Zhu",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "J Guan",
        "id": null
      },
      {
        "name": "X Fang",
        "id": "XjljaMsAAAAJ"
      },
      {
        "name": "D Kim",
        "id": null
      },
      {
        "name": "H Gong",
        "id": null
      },
      {
        "name": "T Zhu",
        "id": "j06hbyUAAAAJ"
      },
      {
        "name": "Z Ling",
        "id": "di6y1p8AAAAJ"
      },
      {
        "name": "M Yang",
        "id": null
      }
    ],
    "citation_count": null
  },
  {
    "arxiv_id": "2310.06275",
    "title": "High-Fidelity 3D Head Avatars Reconstruction through Spatially-Varying Expression Conditioned Neural Radiance Field",
    "year": 2023,
    "published": "2023-10-10T03:13:33Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "One crucial aspect of 3D head avatar reconstruction lies in the details of facial expressions. Although recent NeRF-based photo-realistic 3D head avatar methods achieve high-quality avatar rendering, they still encounter challenges retaining intricate facial expression details because they overlook the potential of specific expression variations at different spatial positions when conditioning the radiance field. Motivated by this observation, we introduce a novel Spatially-Varying Expression (S",
    "arxiv_url": "https://arxiv.org/abs/2310.06275v1",
    "pdf_url": "https://arxiv.org/pdf/2310.06275v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2310.06275",
    "arxiv_authors": [
      "Minghan Qin",
      "Yifan Liu",
      "Yuelang Xu",
      "Xiaochen Zhao",
      "Yebin Liu",
      "Haoqian Wang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=High-Fidelity+3D+Head+Avatars+Reconstruction+through+Spatially-Varying+Expression+Conditioned+Neural+Radiance+Field+Minghan+Qin+Yifan+Liu+Yuelang+Xu+Xiaochen+Zhao+Yebin+Liu",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "M Qin",
        "id": "ngEXyLkAAAAJ"
      },
      {
        "name": "Y Liu",
        "id": "ogXIdlYAAAAJ"
      },
      {
        "name": "Y Xu",
        "id": "6lJRXkAAAAAJ"
      },
      {
        "name": "X Zhao",
        "id": "IfV9sO8AAAAJ"
      },
      {
        "name": "Y Liu",
        "id": "ogXIdlYAAAAJ"
      },
      {
        "name": "H Wang -",
        "id": null
      }
    ],
    "citation_count": 9
  },
  {
    "arxiv_id": "2407.07315",
    "title": "CosmoCLIP: Generalizing Large Vision-Language Models for Astronomical Imaging",
    "year": 2024,
    "published": "2024-07-10T02:24:43Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Existing vision-text contrastive learning models enhance representation transferability and support zero-shot prediction by matching paired image and caption embeddings while pushing unrelated pairs apart. However, astronomical image-label datasets are significantly smaller compared to general image and label datasets available from the internet. We introduce CosmoCLIP, an astronomical image-text contrastive learning framework precisely fine-tuned on the pre-trained CLIP model using SpaceNet and",
    "arxiv_url": "https://arxiv.org/abs/2407.07315v2",
    "pdf_url": "https://arxiv.org/pdf/2407.07315v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2407.07315",
    "arxiv_authors": [
      "Raza Imam",
      "Mohammed Talha Alam",
      "Umaima Rahman",
      "Mohsen Guizani",
      "Fakhri Karray"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=CosmoCLIP%3A+Generalizing+Large+Vision-Language+Models+for+Astronomical+Imaging+Raza+Imam+Mohammed+Talha+Alam+Umaima+Rahman+Mohsen+Guizani+Fakhri+Karray",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "R Imam",
        "id": "iipzwX0AAAAJ"
      },
      {
        "name": "MT Alam",
        "id": "CfJ2E0EAAAAJ"
      },
      {
        "name": "U Rahman",
        "id": "nTZQZ0EAAAAJ"
      },
      {
        "name": "M Guizani",
        "id": "RigrYkcAAAAJ"
      },
      {
        "name": "F Karray",
        "id": "9_Hpd5kAAAAJ"
      }
    ],
    "citation_count": 5
  },
  {
    "arxiv_id": "2503.10324",
    "title": "IDEA: Inverted Text with Cooperative Deformable Aggregation for Multi-modal Object Re-Identification",
    "year": 2025,
    "published": "2025-03-13T13:00:31Z",
    "categories": [
      "cs.CV",
      "cs.MM"
    ],
    "abstract": "Multi-modal object Re-IDentification (ReID) aims to retrieve specific objects by utilizing complementary information from various modalities. However, existing methods focus on fusing heterogeneous visual features, neglecting the potential benefits of text-based semantic information. To address this issue, we first construct three text-enhanced multi-modal object ReID benchmarks. To be specific, we propose a standardized multi-modal caption generation pipeline for structured and concise text ann",
    "arxiv_url": "https://arxiv.org/abs/2503.10324v1",
    "pdf_url": "https://arxiv.org/pdf/2503.10324v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2503.10324",
    "arxiv_authors": [
      "Yuhao Wang",
      "Yongfeng Lv",
      "Pingping Zhang",
      "Huchuan Lu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=IDEA%3A+Inverted+Text+with+Cooperative+Deformable+Aggregation+for+Multi-modal+Object+Re-Identification+Yuhao+Wang+Yongfeng+Lv+Pingping+Zhang+Huchuan+Lu",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "Y Wang",
        "id": "WZvjVLkAAAAJ"
      },
      {
        "name": "Y Lv",
        "id": null
      },
      {
        "name": "P Zhang",
        "id": "MfbIbuEAAAAJ"
      },
      {
        "name": "H Lu -",
        "id": null
      }
    ],
    "citation_count": 15
  },
  {
    "arxiv_id": "2405.04496",
    "title": "Edit-Your-Motion: Space-Time Diffusion Decoupling Learning for Video Motion Editing",
    "year": 2024,
    "published": "2024-05-07T17:06:59Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Existing diffusion-based methods have achieved impressive results in human motion editing. However, these methods often exhibit significant ghosting and body distortion in unseen in-the-wild cases. In this paper, we introduce Edit-Your-Motion, a video motion editing method that tackles these challenges through one-shot fine-tuning on unseen cases. Specifically, firstly, we utilized DDIM inversion to initialize the noise, preserving the appearance of the source video and designed a lightweight mo",
    "arxiv_url": "https://arxiv.org/abs/2405.04496v3",
    "pdf_url": "https://arxiv.org/pdf/2405.04496v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2405.04496",
    "arxiv_authors": [
      "Yi Zuo",
      "Lingling Li",
      "Licheng Jiao",
      "Fang Liu",
      "Xu Liu",
      "Wenping Ma",
      "Shuyuan Yang",
      "Yuwei Guo"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Edit-Your-Motion%3A+Space-Time+Diffusion+Decoupling+Learning+for+Video+Motion+Editing+Yi+Zuo+Lingling+Li+Licheng+Jiao+Fang+Liu+Xu+Liu",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "Y Zuo",
        "id": "VGRST9cAAAAJ"
      },
      {
        "name": "L Li",
        "id": "W_Vi_hIAAAAJ"
      },
      {
        "name": "L Jiao",
        "id": null
      },
      {
        "name": "F Liu",
        "id": "PXo2aVUAAAAJ"
      },
      {
        "name": "X Liu",
        "id": "_09bkMgAAAAJ"
      },
      {
        "name": "W Ma",
        "id": "I1pPv1QAAAAJ"
      },
      {
        "name": "S Yang",
        "id": "TimAUN0AAAAJ"
      },
      {
        "name": "Y Guo",
        "id": null
      }
    ],
    "citation_count": 3
  },
  {
    "arxiv_id": "2308.04733",
    "title": "TextPainter: Multimodal Text Image Generation with Visual-harmony and Text-comprehension for Poster Design",
    "year": 2023,
    "published": "2023-08-09T06:59:29Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Text design is one of the most critical procedures in poster design, as it relies heavily on the creativity and expertise of humans to design text images considering the visual harmony and text-semantic. This study introduces TextPainter, a novel multimodal approach that leverages contextual visual information and corresponding text semantics to generate text images. Specifically, TextPainter takes the global-local background image as a hint of style and guides the text image generation with vis",
    "arxiv_url": "https://arxiv.org/abs/2308.04733v3",
    "pdf_url": "https://arxiv.org/pdf/2308.04733v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2308.04733",
    "arxiv_authors": [
      "Yifan Gao",
      "Jinpeng Lin",
      "Min Zhou",
      "Chuanbin Liu",
      "Hongtao Xie",
      "Tiezheng Ge",
      "Yuning Jiang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=TextPainter%3A+Multimodal+Text+Image+Generation+with+Visual-harmony+and+Text-comprehension+for+Poster+Design+Yifan+Gao+Jinpeng+Lin+Min+Zhou+Chuanbin+Liu+Hongtao+Xie",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "Y Gao",
        "id": "7JHExkIAAAAJ"
      },
      {
        "name": "J Lin",
        "id": null
      },
      {
        "name": "M Zhou",
        "id": "A7xfbIYAAAAJ"
      },
      {
        "name": "C Liu",
        "id": "TvAygXEAAAAJ"
      },
      {
        "name": "H Xie",
        "id": null
      },
      {
        "name": "T Ge",
        "id": "db5ZTlMAAAAJ"
      },
      {
        "name": "Y Jiang",
        "id": "Z7v-NA4AAAAJ"
      }
    ],
    "citation_count": 17
  },
  {
    "arxiv_id": "2301.12334",
    "title": "Don't Play Favorites: Minority Guidance for Diffusion Models",
    "year": 2023,
    "published": "2023-01-29T03:08:47Z",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "stat.ML"
    ],
    "abstract": "We explore the problem of generating minority samples using diffusion models. The minority samples are instances that lie on low-density regions of a data manifold. Generating a sufficient number of such minority instances is important, since they often contain some unique attributes of the data. However, the conventional generation process of the diffusion models mostly yields majority samples (that lie on high-density regions of the manifold) due to their high likelihoods, making themselves in",
    "arxiv_url": "https://arxiv.org/abs/2301.12334v2",
    "pdf_url": "https://arxiv.org/pdf/2301.12334v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2301.12334",
    "arxiv_authors": [
      "Soobin Um",
      "Suhyeon Lee",
      "Jong Chul Ye"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Don%27t+Play+Favorites%3A+Minority+Guidance+for+Diffusion+Models+Soobin+Um+Suhyeon+Lee+Jong+Chul+Ye",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "S Um",
        "id": "JBGNjA0AAAAJ"
      },
      {
        "name": "S Lee",
        "id": "V9rMrFQAAAAJ"
      },
      {
        "name": "JC Ye -",
        "id": null
      }
    ],
    "citation_count": 30
  },
  {
    "arxiv_id": "2308.00622",
    "title": "NeRT: Implicit Neural Representations for General Unsupervised Turbulence Mitigation",
    "year": 2023,
    "published": "2023-08-01T15:49:40Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "The atmospheric and water turbulence mitigation problems have emerged as challenging inverse problems in computer vision and optics communities over the years. However, current methods either rely heavily on the quality of the training dataset or fail to generalize over various scenarios, such as static scenes, dynamic scenes, and text reconstructions. We propose a general implicit neural representation for unsupervised atmospheric and water turbulence mitigation (NeRT). NeRT leverages the impli",
    "arxiv_url": "https://arxiv.org/abs/2308.00622v2",
    "pdf_url": "https://arxiv.org/pdf/2308.00622v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2308.00622",
    "arxiv_authors": [
      "Weiyun Jiang",
      "Yuhao Liu",
      "Vivek Boominathan",
      "Ashok Veeraraghavan"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=NeRT%3A+Implicit+Neural+Representations+for+General+Unsupervised+Turbulence+Mitigation+Weiyun+Jiang+Yuhao+Liu+Vivek+Boominathan+Ashok+Veeraraghavan",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "W Jiang",
        "id": "fsOwLQgAAAAJ"
      },
      {
        "name": "Y Liu",
        "id": "2eI0k5gAAAAJ"
      },
      {
        "name": "V Boominathan",
        "id": "wwf-WgUAAAAJ"
      },
      {
        "name": "A Veeraraghavan",
        "id": "tI-oUmsAAAAJ"
      }
    ],
    "citation_count": 3
  },
  {
    "arxiv_id": "2309.17133",
    "title": "Fine-grained Late-interaction Multi-modal Retrieval for Retrieval Augmented Visual Question Answering",
    "year": 2023,
    "published": "2023-09-29T10:54:10Z",
    "categories": [
      "cs.CL",
      "cs.CV"
    ],
    "abstract": "Knowledge-based Visual Question Answering (KB-VQA) requires VQA systems to utilize knowledge from external knowledge bases to answer visually-grounded questions. Retrieval-Augmented Visual Question Answering (RA-VQA), a strong framework to tackle KB-VQA, first retrieves related documents with Dense Passage Retrieval (DPR) and then uses them to answer questions. This paper proposes Fine-grained Late-interaction Multi-modal Retrieval (FLMR) which significantly improves knowledge retrieval in RA-VQ",
    "arxiv_url": "https://arxiv.org/abs/2309.17133v2",
    "pdf_url": "https://arxiv.org/pdf/2309.17133v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2309.17133",
    "arxiv_authors": [
      "Weizhe Lin",
      "Jinghong Chen",
      "Jingbiao Mei",
      "Alexandru Coca",
      "Bill Byrne"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Fine-grained+Late-interaction+Multi-modal+Retrieval+for+Retrieval+Augmented+Visual+Question+Answering+Weizhe+Lin+Jinghong+Chen+Jingbiao+Mei+Alexandru+Coca+Bill+Byrne",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "W Lin",
        "id": "4hMhIecAAAAJ"
      },
      {
        "name": "J Chen",
        "id": "pYOXaKEAAAAJ"
      },
      {
        "name": "J Mei",
        "id": "31bT80wAAAAJ"
      },
      {
        "name": "A Coca",
        "id": "WqcTDlkAAAAJ"
      },
      {
        "name": "B ByrneAdvances in Neural Information Processing Systems",
        "id": null
      }
    ],
    "citation_count": 110
  },
  {
    "arxiv_id": "2502.17425",
    "title": "Introducing Visual Perception Token into Multimodal Large Language Model",
    "year": 2025,
    "published": "2025-02-24T18:56:12Z",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "To utilize visual information, Multimodal Large Language Model (MLLM) relies on the perception process of its vision encoder. The completeness and accuracy of visual perception significantly influence the precision of spatial reasoning, fine-grained understanding, and other tasks. However, MLLM still lacks the autonomous capability to control its own visual perception processes, for example, selectively reviewing specific regions of an image or focusing on information related to specific object ",
    "arxiv_url": "https://arxiv.org/abs/2502.17425v1",
    "pdf_url": "https://arxiv.org/pdf/2502.17425v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2502.17425",
    "arxiv_authors": [
      "Runpeng Yu",
      "Xinyin Ma",
      "Xinchao Wang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Introducing+Visual+Perception+Token+into+Multimodal+Large+Language+Model+Runpeng+Yu+Xinyin+Ma+Xinchao+Wang",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "R Yu",
        "id": "sMgjdygAAAAJ"
      },
      {
        "name": "X Ma",
        "id": "jFUKS0oAAAAJ"
      },
      {
        "name": "X Wang -",
        "id": null
      }
    ],
    "citation_count": 10
  },
  {
    "arxiv_id": "2402.09611",
    "title": "Towards Privacy-Aware Sign Language Translation at Scale",
    "year": 2024,
    "published": "2024-02-14T22:57:03Z",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "A major impediment to the advancement of sign language translation (SLT) is data scarcity. Much of the sign language data currently available on the web cannot be used for training supervised models due to the lack of aligned captions. Furthermore, scaling SLT using large-scale web-scraped datasets bears privacy risks due to the presence of biometric information, which the responsible development of SLT technologies should account for. In this work, we propose a two-stage framework for privacy-a",
    "arxiv_url": "https://arxiv.org/abs/2402.09611v2",
    "pdf_url": "https://arxiv.org/pdf/2402.09611v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2402.09611",
    "arxiv_authors": [
      "Phillip Rust",
      "Bowen Shi",
      "Skyler Wang",
      "Necati Cihan Camg√∂z",
      "Jean Maillard"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Towards+Privacy-Aware+Sign+Language+Translation+at+Scale+Phillip+Rust+Bowen+Shi+Skyler+Wang+Necati+Cihan+Camg%C3%B6z+Jean+Maillard",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "P Rust",
        "id": "6MxyDqcAAAAJ"
      },
      {
        "name": "B Shi",
        "id": "xqyoorYAAAAJ"
      },
      {
        "name": "S Wang",
        "id": "4g0yVg4AAAAJ"
      },
      {
        "name": "NC Camg√∂z",
        "id": "Tk5Egv8AAAAJ"
      },
      {
        "name": "J Maillard",
        "id": "_ewOoK0AAAAJ"
      }
    ],
    "citation_count": 37
  },
  {
    "arxiv_id": "2304.11968",
    "title": "Track Anything: Segment Anything Meets Videos",
    "year": 2023,
    "published": "2023-04-24T10:04:06Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Recently, the Segment Anything Model (SAM) gains lots of attention rapidly due to its impressive segmentation performance on images. Regarding its strong ability on image segmentation and high interactivity with different prompts, we found that it performs poorly on consistent segmentation in videos. Therefore, in this report, we propose Track Anything Model (TAM), which achieves high-performance interactive tracking and segmentation in videos. To be detailed, given a video sequence, only with v",
    "arxiv_url": "https://arxiv.org/abs/2304.11968v2",
    "pdf_url": "https://arxiv.org/pdf/2304.11968v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2304.11968",
    "arxiv_authors": [
      "Jinyu Yang",
      "Mingqi Gao",
      "Zhe Li",
      "Shang Gao",
      "Fangjing Wang",
      "Feng Zheng"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Track+Anything%3A+Segment+Anything+Meets+Videos+Jinyu+Yang+Mingqi+Gao+Zhe+Li+Shang+Gao+Fangjing+Wang",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "J Yang",
        "id": null
      },
      {
        "name": "M Gao",
        "id": "ECCd0hwAAAAJ"
      },
      {
        "name": "Z Li",
        "id": "XZqhEdgAAAAJ"
      },
      {
        "name": "S Gao",
        "id": "of0KrQQAAAAJ"
      },
      {
        "name": "F Wang",
        "id": null
      },
      {
        "name": "F Zheng",
        "id": "PcmyXHMAAAAJ"
      }
    ],
    "citation_count": 362
  },
  {
    "arxiv_id": "2404.10438",
    "title": "The Unreasonable Effectiveness of Pre-Trained Features for Camera Pose Refinement",
    "year": 2024,
    "published": "2024-04-16T10:04:38Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Pose refinement is an interesting and practically relevant research direction. Pose refinement can be used to (1) obtain a more accurate pose estimate from an initial prior (e.g., from retrieval), (2) as pre-processing, i.e., to provide a better starting point to a more expensive pose estimator, (3) as post-processing of a more accurate localizer. Existing approaches focus on learning features / scene representations for the pose refinement task. This involves training an implicit scene represen",
    "arxiv_url": "https://arxiv.org/abs/2404.10438v1",
    "pdf_url": "https://arxiv.org/pdf/2404.10438v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2404.10438",
    "arxiv_authors": [
      "Gabriele Trivigno",
      "Carlo Masone",
      "Barbara Caputo",
      "Torsten Sattler"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=The+Unreasonable+Effectiveness+of+Pre-Trained+Features+for+Camera+Pose+Refinement+Gabriele+Trivigno+Carlo+Masone+Barbara+Caputo+Torsten+Sattler",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "G Trivigno",
        "id": "JXf_iToAAAAJ"
      },
      {
        "name": "C Masone",
        "id": "cM3Iz_4AAAAJ"
      },
      {
        "name": "B Caputo",
        "id": "mHbdIAwAAAAJ"
      },
      {
        "name": "T Sattler",
        "id": "jzx6_ZIAAAAJ"
      }
    ],
    "citation_count": 22
  },
  {
    "arxiv_id": "2505.01109",
    "title": "Self-Supervision Enhances Instance-based Multiple Instance Learning Methods in Digital Pathology: A Benchmark Study",
    "year": 2025,
    "published": "2025-05-02T08:43:50Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Multiple Instance Learning (MIL) has emerged as the best solution for Whole Slide Image (WSI) classification. It consists of dividing each slide into patches, which are treated as a bag of instances labeled with a global label. MIL includes two main approaches: instance-based and embedding-based. In the former, each patch is classified independently, and then the patch scores are aggregated to predict the bag label. In the latter, bag classification is performed after aggregating patch embedding",
    "arxiv_url": "https://arxiv.org/abs/2505.01109v1",
    "pdf_url": "https://arxiv.org/pdf/2505.01109v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2505.01109",
    "arxiv_authors": [
      "Ali Mammadov",
      "Loic Le Folgoc",
      "Julien Adam",
      "Anne Buronfosse",
      "Gilles Hayem",
      "Guillaume Hocquet",
      "Pietro Gori"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Self-Supervision+Enhances+Instance-based+Multiple+Instance+Learning+Methods+in+Digital+Pathology%3A+A+Benchmark+Study+Ali+Mammadov+Loic+Le+Folgoc+Julien+Adam+Anne+Buronfosse+Gilles+Hayem",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "A Mammadov",
        "id": "Z_5LkGAAAAAJ"
      },
      {
        "name": "L Le Folgoc",
        "id": "_7MXR2MAAAAJ"
      },
      {
        "name": "J Adam",
        "id": null
      },
      {
        "name": "A Buronfosse",
        "id": null
      },
      {
        "name": "G Hayem",
        "id": null
      },
      {
        "name": "G Hocquet",
        "id": null
      },
      {
        "name": "P Gori",
        "id": "id9wCjsAAAAJ"
      }
    ],
    "citation_count": 3
  },
  {
    "arxiv_id": "2502.10982",
    "title": "TEASER: Token Enhanced Spatial Modeling for Expressions Reconstruction",
    "year": 2025,
    "published": "2025-02-16T04:00:06Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "3D facial reconstruction from a single in-the-wild image is a crucial task in human-centered computer vision tasks. While existing methods can recover accurate facial shapes, there remains significant space for improvement in fine-grained expression capture. Current approaches struggle with irregular mouth shapes, exaggerated expressions, and asymmetrical facial movements. We present TEASER (Token EnhAnced Spatial modeling for Expressions Reconstruction), which addresses these challenges and enh",
    "arxiv_url": "https://arxiv.org/abs/2502.10982v3",
    "pdf_url": "https://arxiv.org/pdf/2502.10982v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2502.10982",
    "arxiv_authors": [
      "Yunfei Liu",
      "Lei Zhu",
      "Lijian Lin",
      "Ye Zhu",
      "Ailing Zhang",
      "Yu Li"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=TEASER%3A+Token+Enhanced+Spatial+Modeling+for+Expressions+Reconstruction+Yunfei+Liu+Lei+Zhu+Lijian+Lin+Ye+Zhu+Ailing+Zhang",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "Y Liu",
        "id": "B1Z1vTMAAAAJ"
      },
      {
        "name": "L Zhu",
        "id": null
      },
      {
        "name": "L Lin",
        "id": "Xf5_TfcAAAAJ"
      },
      {
        "name": "Y Zhu",
        "id": "qhp9rIMAAAAJ"
      },
      {
        "name": "A Zhang",
        "id": "ZJGYSzYAAAAJ"
      },
      {
        "name": "Y Li -",
        "id": null
      }
    ],
    "citation_count": 8
  },
  {
    "arxiv_id": "2409.14289",
    "title": "Deep Learning Technology for Face Forgery Detection: A Survey",
    "year": 2024,
    "published": "2024-09-22T01:42:01Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Currently, the rapid development of computer vision and deep learning has enabled the creation or manipulation of high-fidelity facial images and videos via deep generative approaches. This technology, also known as deepfake, has achieved dramatic progress and become increasingly popular in social media. However, the technology can generate threats to personal privacy and national security by spreading misinformation. To diminish the risks of deepfake, it is desirable to develop powerful forgery",
    "arxiv_url": "https://arxiv.org/abs/2409.14289v3",
    "pdf_url": "https://arxiv.org/pdf/2409.14289v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2409.14289",
    "arxiv_authors": [
      "Lixia Ma",
      "Puning Yang",
      "Yuting Xu",
      "Ziming Yang",
      "Peipei Li",
      "Huaibo Huang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Deep+Learning+Technology+for+Face+Forgery+Detection%3A+A+Survey+Lixia+Ma+Puning+Yang+Yuting+Xu+Ziming+Yang+Peipei+Li",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "L Ma",
        "id": null
      },
      {
        "name": "P Yang",
        "id": "_QGfhW8AAAAJ"
      },
      {
        "name": "Y Xu",
        "id": "_lMCBnoAAAAJ"
      },
      {
        "name": "Z Yang",
        "id": null
      },
      {
        "name": "P Li",
        "id": "A0khpKYAAAAJ"
      }
    ],
    "citation_count": 9
  },
  {
    "arxiv_id": "2307.00421",
    "title": "Brightness-Restricted Adversarial Attack Patch",
    "year": 2023,
    "published": "2023-07-01T20:08:55Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Adversarial attack patches have gained increasing attention due to their practical applicability in physical-world scenarios. However, the bright colors used in attack patches represent a significant drawback, as they can be easily identified by human observers. Moreover, even though these attacks have been highly successful in deceiving target networks, which specific features of the attack patch contribute to its success are still unknown. Our paper introduces a brightness-restricted patch (Br",
    "arxiv_url": "https://arxiv.org/abs/2307.00421v1",
    "pdf_url": "https://arxiv.org/pdf/2307.00421v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2307.00421",
    "arxiv_authors": [
      "Mingzhen Shao"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Brightness-Restricted+Adversarial+Attack+Patch+Mingzhen+Shao",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "M Shao -",
        "id": null
      }
    ],
    "citation_count": 1
  },
  {
    "arxiv_id": "2411.05705",
    "title": "Image inpainting enhancement by replacing the original mask with a self-attended region from the input image",
    "year": 2024,
    "published": "2024-11-08T17:04:05Z",
    "categories": [
      "cs.CV",
      "eess.IV"
    ],
    "abstract": "Image inpainting, the process of restoring missing or corrupted regions of an image by reconstructing pixel information, has recently seen considerable advancements through deep learning-based approaches. In this paper, we introduce a novel deep learning-based pre-processing methodology for image inpainting utilizing the Vision Transformer (ViT). Our approach involves replacing masked pixel values with those generated by the ViT, leveraging diverse visual patches within the attention matrix to c",
    "arxiv_url": "https://arxiv.org/abs/2411.05705v1",
    "pdf_url": "https://arxiv.org/pdf/2411.05705v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2411.05705",
    "arxiv_authors": [
      "Kourosh Kiani",
      "Razieh Rastgoo",
      "Alireza Chaji",
      "Sergio Escalera"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Image+inpainting+enhancement+by+replacing+the+original+mask+with+a+self-attended+region+from+the+input+image+Kourosh+Kiani+Razieh+Rastgoo+Alireza+Chaji+Sergio+Escalera",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "K Kiani",
        "id": "mJJLDcYAAAAJ"
      },
      {
        "name": "R Rastgoo",
        "id": "zfvwqc0AAAAJ"
      },
      {
        "name": "A Chaji",
        "id": null
      },
      {
        "name": "S Escalera -",
        "id": null
      }
    ],
    "citation_count": 1
  },
  {
    "arxiv_id": "2305.18362",
    "title": "Statistically Significant Concept-based Explanation of Image Classifiers via Model Knockoffs",
    "year": 2023,
    "published": "2023-05-27T05:40:05Z",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "abstract": "A concept-based classifier can explain the decision process of a deep learning model by human-understandable concepts in image classification problems. However, sometimes concept-based explanations may cause false positives, which misregards unrelated concepts as important for the prediction task. Our goal is to find the statistically significant concept for classification to prevent misinterpretation. In this study, we propose a method using a deep learning model to learn the image concept and ",
    "arxiv_url": "https://arxiv.org/abs/2305.18362v2",
    "pdf_url": "https://arxiv.org/pdf/2305.18362v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2305.18362",
    "arxiv_authors": [
      "Kaiwen Xu",
      "Kazuto Fukuchi",
      "Youhei Akimoto",
      "Jun Sakuma"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Statistically+Significant+Concept-based+Explanation+of+Image+Classifiers+via+Model+Knockoffs+Kaiwen+Xu+Kazuto+Fukuchi+Youhei+Akimoto+Jun+Sakuma",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "K Xu",
        "id": "I3GBi8EAAAAJ"
      },
      {
        "name": "K Fukuchi",
        "id": "496_ICsAAAAJ"
      },
      {
        "name": "Y Akimoto",
        "id": "m7OXdsUAAAAJ"
      },
      {
        "name": "J Sakuma -",
        "id": null
      }
    ],
    "citation_count": 3
  },
  {
    "arxiv_id": "2303.02978",
    "title": "System for 3D Acquisition and 3D Reconstruction using Structured Light for Sewer Line Inspection",
    "year": 2023,
    "published": "2023-03-06T09:10:55Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "The assessment of sewer pipe systems is a highly important, but at the same time cumbersome and error-prone task. We introduce an innovative system based on single-shot structured light modules that facilitates the detection and classification of spatial defects like jutting intrusions, spallings, or misaligned joints. This system creates highly accurate 3D measurements with sub-millimeter resolution of pipe surfaces and fuses them into a holistic 3D model. The benefit of such a holistic 3D mode",
    "arxiv_url": "https://arxiv.org/abs/2303.02978v1",
    "pdf_url": "https://arxiv.org/pdf/2303.02978v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2303.02978",
    "arxiv_authors": [
      "Johannes K√ºnzel",
      "Darko Vehar",
      "Rico Nestler",
      "Karl-Heinz Franke",
      "Anna Hilsmann",
      "Peter Eisert"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=System+for+3D+Acquisition+and+3D+Reconstruction+using+Structured+Light+for+Sewer+Line+Inspection+Johannes+K%C3%BCnzel+Darko+Vehar+Rico+Nestler+Karl-Heinz+Franke+Anna+Hilsmann",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "J K√ºnzel",
        "id": "ybMR38kAAAAJ"
      },
      {
        "name": "D Vehar",
        "id": null
      },
      {
        "name": "R Nestler",
        "id": null
      },
      {
        "name": "KH Franke",
        "id": null
      },
      {
        "name": "A Hilsmann",
        "id": "5yTuyGIAAAAJ"
      },
      {
        "name": "P Eisert",
        "id": "BCElyCkAAAAJ"
      }
    ],
    "citation_count": null
  },
  {
    "arxiv_id": "2302.04304",
    "title": "Q-Diffusion: Quantizing Diffusion Models",
    "year": 2023,
    "published": "2023-02-08T19:38:59Z",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "Diffusion models have achieved great success in image synthesis through iterative noise estimation using deep neural networks. However, the slow inference, high memory consumption, and computation intensity of the noise estimation model hinder the efficient adoption of diffusion models. Although post-training quantization (PTQ) is considered a go-to compression method for other tasks, it does not work out-of-the-box on diffusion models. We propose a novel PTQ method specifically tailored towards",
    "arxiv_url": "https://arxiv.org/abs/2302.04304v3",
    "pdf_url": "https://arxiv.org/pdf/2302.04304v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2302.04304",
    "arxiv_authors": [
      "Xiuyu Li",
      "Yijiang Liu",
      "Long Lian",
      "Huanrui Yang",
      "Zhen Dong",
      "Daniel Kang",
      "Shanghang Zhang",
      "Kurt Keutzer"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Q-Diffusion%3A+Quantizing+Diffusion+Models+Xiuyu+Li+Yijiang+Liu+Long+Lian+Huanrui+Yang+Zhen+Dong",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "X Li",
        "id": "Kq0feJIAAAAJ"
      },
      {
        "name": "Y Liu",
        "id": "uOyz518AAAAJ"
      },
      {
        "name": "L Lian",
        "id": "eOLxyqUAAAAJ"
      },
      {
        "name": "H Yang",
        "id": "bjNCUt8AAAAJ"
      },
      {
        "name": "Z Dong",
        "id": "czxMUzcAAAAJ"
      },
      {
        "name": "D Kang",
        "id": "CpMjT0YAAAAJ"
      },
      {
        "name": "S Zhang",
        "id": "voqw10cAAAAJ"
      },
      {
        "name": "K Keutzer",
        "id": "ID9QePIAAAAJ"
      }
    ],
    "citation_count": 310
  },
  {
    "arxiv_id": "2406.14643",
    "title": "Holistic Evaluation for Interleaved Text-and-Image Generation",
    "year": 2024,
    "published": "2024-06-20T18:07:19Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "abstract": "Interleaved text-and-image generation has been an intriguing research direction, where the models are required to generate both images and text pieces in an arbitrary order. Despite the emerging advancements in interleaved generation, the progress in its evaluation still significantly lags behind. Existing evaluation benchmarks do not support arbitrarily interleaved images and text for both inputs and outputs, and they only cover a limited number of domains and use cases. Also, current works pre",
    "arxiv_url": "https://arxiv.org/abs/2406.14643v3",
    "pdf_url": "https://arxiv.org/pdf/2406.14643v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2406.14643",
    "arxiv_authors": [
      "Minqian Liu",
      "Zhiyang Xu",
      "Zihao Lin",
      "Trevor Ashby",
      "Joy Rimchala",
      "Jiaxin Zhang",
      "Lifu Huang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Holistic+Evaluation+for+Interleaved+Text-and-Image+Generation+Minqian+Liu+Zhiyang+Xu+Zihao+Lin+Trevor+Ashby+Joy+Rimchala",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "M Liu",
        "id": "xCR8nrwAAAAJ"
      },
      {
        "name": "Z Xu",
        "id": "Qcshi8UAAAAJ"
      },
      {
        "name": "Z Lin",
        "id": "4h_A4n4AAAAJ"
      },
      {
        "name": "T Ashby",
        "id": "jeSp_1wAAAAJ"
      },
      {
        "name": "J Rimchala",
        "id": null
      },
      {
        "name": "J Zhang",
        "id": "LiDm8jEAAAAJ"
      },
      {
        "name": "L Huang",
        "id": "76IEGtYAAAAJ"
      }
    ],
    "citation_count": 22
  },
  {
    "arxiv_id": "2504.04115",
    "title": "Overcoming the Identity Mapping Problem in Self-Supervised Hyperspectral Anomaly Detection",
    "year": 2025,
    "published": "2025-04-05T09:12:25Z",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "abstract": "The surge of deep learning has catalyzed considerable progress in self-supervised Hyperspectral Anomaly Detection (HAD). The core premise for self-supervised HAD is that anomalous pixels are inherently more challenging to reconstruct, resulting in larger errors compared to the background. However, owing to the powerful nonlinear fitting capabilities of neural networks, self-supervised models often suffer from the Identity Mapping Problem (IMP). The IMP manifests as a tendency for the model to ov",
    "arxiv_url": "https://arxiv.org/abs/2504.04115v1",
    "pdf_url": "https://arxiv.org/pdf/2504.04115v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2504.04115",
    "arxiv_authors": [
      "Yongchuan Cui",
      "Jinhe Zhang",
      "Peng Liu",
      "Weijing Song",
      "Yi Zeng"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Overcoming+the+Identity+Mapping+Problem+in+Self-Supervised+Hyperspectral+Anomaly+Detection+Yongchuan+Cui+Jinhe+Zhang+Peng+Liu+Weijing+Song+Yi+Zeng",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "Y Cui",
        "id": "dRK8mV0AAAAJ"
      },
      {
        "name": "J Zhang",
        "id": null
      },
      {
        "name": "P Liu",
        "id": "5wOjyo4AAAAJ"
      },
      {
        "name": "W Song",
        "id": null
      },
      {
        "name": "Y Zeng -",
        "id": null
      }
    ],
    "citation_count": 1
  },
  {
    "arxiv_id": "2309.07970",
    "title": "Language Embedded Radiance Fields for Zero-Shot Task-Oriented Grasping",
    "year": 2023,
    "published": "2023-09-14T18:10:29Z",
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "abstract": "Grasping objects by a specific part is often crucial for safety and for executing downstream tasks. Yet, learning-based grasp planners lack this behavior unless they are trained on specific object part data, making it a significant challenge to scale object diversity. Instead, we propose LERF-TOGO, Language Embedded Radiance Fields for Task-Oriented Grasping of Objects, which uses vision-language models zero-shot to output a grasp distribution over an object given a natural language query. To ac",
    "arxiv_url": "https://arxiv.org/abs/2309.07970v2",
    "pdf_url": "https://arxiv.org/pdf/2309.07970v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2309.07970",
    "arxiv_authors": [
      "Adam Rashid",
      "Satvik Sharma",
      "Chung Min Kim",
      "Justin Kerr",
      "Lawrence Chen",
      "Angjoo Kanazawa",
      "Ken Goldberg"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Language+Embedded+Radiance+Fields+for+Zero-Shot+Task-Oriented+Grasping+Adam+Rashid+Satvik+Sharma+Chung+Min+Kim+Justin+Kerr+Lawrence+Chen",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "A Rashid",
        "id": "M8yGg9EAAAAJ"
      },
      {
        "name": "S Sharma",
        "id": "0wZN6hEAAAAJ"
      },
      {
        "name": "CM Kim",
        "id": "ODr5lMgAAAAJ"
      },
      {
        "name": "J Kerr",
        "id": "k6fOmQgAAAAJ"
      },
      {
        "name": "LY Chen",
        "id": "xYP4uuIAAAAJ"
      },
      {
        "name": "A Kanazawa",
        "id": "Ci-_QYIAAAAJ"
      },
      {
        "name": "K Goldberg7th Annual",
        "id": null
      }
    ],
    "citation_count": 126
  },
  {
    "arxiv_id": "2501.12295",
    "title": "Towards Accurate Unified Anomaly Segmentation",
    "year": 2025,
    "published": "2025-01-21T17:02:51Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Unsupervised anomaly detection (UAD) from images strives to model normal data distributions, creating discriminative representations to distinguish and precisely localize anomalies. Despite recent advancements in the efficient and unified one-for-all scheme, challenges persist in accurately segmenting anomalies for further monitoring. Moreover, this problem is obscured by the widely-used AUROC metric under imbalanced UAD settings. This motivates us to emphasize the significance of precise segmen",
    "arxiv_url": "https://arxiv.org/abs/2501.12295v1",
    "pdf_url": "https://arxiv.org/pdf/2501.12295v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2501.12295",
    "arxiv_authors": [
      "Wenxin Ma",
      "Qingsong Yao",
      "Xiang Zhang",
      "Zhelong Huang",
      "Zihang Jiang",
      "S. Kevin Zhou"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Towards+Accurate+Unified+Anomaly+Segmentation+Wenxin+Ma+Qingsong+Yao+Xiang+Zhang+Zhelong+Huang+Zihang+Jiang",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "W Ma",
        "id": "r0-tZ8cAAAAJ"
      },
      {
        "name": "Q Yao",
        "id": "CMiRzlAAAAAJ"
      },
      {
        "name": "X Zhang",
        "id": null
      },
      {
        "name": "Z Huang",
        "id": "vBKRGOcAAAAJ"
      },
      {
        "name": "Z Jiang",
        "id": "Wo8tMSMAAAAJ"
      },
      {
        "name": "SK Zhou2025 IEEE/CVF Winter",
        "id": null
      }
    ],
    "citation_count": 9
  },
  {
    "arxiv_id": "2312.03626",
    "title": "TokenCompose: Text-to-Image Diffusion with Token-level Supervision",
    "year": 2023,
    "published": "2023-12-06T17:13:15Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "We present TokenCompose, a Latent Diffusion Model for text-to-image generation that achieves enhanced consistency between user-specified text prompts and model-generated images. Despite its tremendous success, the standard denoising process in the Latent Diffusion Model takes text prompts as conditions only, absent explicit constraint for the consistency between the text prompts and the image contents, leading to unsatisfactory results for composing multiple object categories. TokenCompose aims ",
    "arxiv_url": "https://arxiv.org/abs/2312.03626v2",
    "pdf_url": "https://arxiv.org/pdf/2312.03626v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2312.03626",
    "arxiv_authors": [
      "Zirui Wang",
      "Zhizhou Sha",
      "Zheng Ding",
      "Yilin Wang",
      "Zhuowen Tu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=TokenCompose%3A+Text-to-Image+Diffusion+with+Token-level+Supervision+Zirui+Wang+Zhizhou+Sha+Zheng+Ding+Yilin+Wang+Zhuowen+Tu",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "Z Wang",
        "id": "xfnk-58AAAAJ"
      },
      {
        "name": "Z Sha",
        "id": "eAObdYgAAAAJ"
      },
      {
        "name": "Z Ding",
        "id": "TOYBXFQAAAAJ"
      },
      {
        "name": "Y Wang",
        "id": "w-XdUN8AAAAJ"
      },
      {
        "name": "Z Tu",
        "id": "9oz-dvgAAAAJ"
      }
    ],
    "citation_count": 52
  },
  {
    "arxiv_id": "2408.01986",
    "title": "DeMansia: Mamba Never Forgets Any Tokens",
    "year": 2024,
    "published": "2024-08-04T10:54:36Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "This paper examines the mathematical foundations of transformer architectures, highlighting their limitations particularly in handling long sequences. We explore prerequisite models such as Mamba, Vision Mamba (ViM), and LV-ViT that pave the way for our proposed architecture, DeMansia. DeMansia integrates state space models with token labeling techniques to enhance performance in image classification tasks, efficiently addressing the computational challenges posed by traditional transformers. Th",
    "arxiv_url": "https://arxiv.org/abs/2408.01986v1",
    "pdf_url": "https://arxiv.org/pdf/2408.01986v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2408.01986",
    "arxiv_authors": [
      "Ricky Fang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=DeMansia%3A+Mamba+Never+Forgets+Any+Tokens+Ricky+Fang",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "R Fang -",
        "id": null
      }
    ],
    "citation_count": null
  },
  {
    "arxiv_id": "2406.18011",
    "title": "Expressive Keypoints for Skeleton-based Action Recognition via Skeleton Transformation",
    "year": 2024,
    "published": "2024-06-26T01:48:56Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "In the realm of skeleton-based action recognition, the traditional methods which rely on coarse body keypoints fall short of capturing subtle human actions. In this work, we propose Expressive Keypoints that incorporates hand and foot details to form a fine-grained skeletal representation, improving the discriminative ability for existing models in discerning intricate actions. To efficiently model Expressive Keypoints, the Skeleton Transformation strategy is presented to gradually downsample th",
    "arxiv_url": "https://arxiv.org/abs/2406.18011v1",
    "pdf_url": "https://arxiv.org/pdf/2406.18011v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2406.18011",
    "arxiv_authors": [
      "Yijie Yang",
      "Jinlu Zhang",
      "Jiaxu Zhang",
      "Zhigang Tu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Expressive+Keypoints+for+Skeleton-based+Action+Recognition+via+Skeleton+Transformation+Yijie+Yang+Jinlu+Zhang+Jiaxu+Zhang+Zhigang+Tu",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "Y Yang",
        "id": null
      },
      {
        "name": "J Zhang",
        "id": "jUAyNjEAAAAJ"
      },
      {
        "name": "J Zhang",
        "id": "jUAyNjEAAAAJ"
      },
      {
        "name": "Z Tu -",
        "id": null
      }
    ],
    "citation_count": 10
  },
  {
    "arxiv_id": "2302.12764",
    "title": "Modulating Pretrained Diffusion Models for Multimodal Image Synthesis",
    "year": 2023,
    "published": "2023-02-24T17:28:08Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "We present multimodal conditioning modules (MCM) for enabling conditional image synthesis using pretrained diffusion models. Previous multimodal synthesis works rely on training networks from scratch or fine-tuning pretrained networks, both of which are computationally expensive for large, state-of-the-art diffusion models. Our method uses pretrained networks but \\textit{does not require any updates to the diffusion network's parameters}. MCM is a small module trained to modulate the diffusion n",
    "arxiv_url": "https://arxiv.org/abs/2302.12764v2",
    "pdf_url": "https://arxiv.org/pdf/2302.12764v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2302.12764",
    "arxiv_authors": [
      "Cusuh Ham",
      "James Hays",
      "Jingwan Lu",
      "Krishna Kumar Singh",
      "Zhifei Zhang",
      "Tobias Hinz"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Modulating+Pretrained+Diffusion+Models+for+Multimodal+Image+Synthesis+Cusuh+Ham+James+Hays+Jingwan+Lu+Krishna+Kumar+Singh+Zhifei+Zhang",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "C Ham",
        "id": "-2uX93cAAAAJ"
      },
      {
        "name": "J Hays",
        "id": "vjZrDKQAAAAJ"
      },
      {
        "name": "J Lu",
        "id": null
      },
      {
        "name": "KK Singh",
        "id": "3TMipekAAAAJ"
      },
      {
        "name": "Z Zhang",
        "id": "HuerflQAAAAJ"
      },
      {
        "name": "T HinzACM SIGGRAPH",
        "id": null
      }
    ],
    "citation_count": 23
  },
  {
    "arxiv_id": "2404.02117",
    "title": "Pre-trained Vision and Language Transformers Are Few-Shot Incremental Learners",
    "year": 2024,
    "published": "2024-04-02T17:23:22Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Few-Shot Class Incremental Learning (FSCIL) is a task that requires a model to learn new classes incrementally without forgetting when only a few samples for each class are given. FSCIL encounters two significant challenges: catastrophic forgetting and overfitting, and these challenges have driven prior studies to primarily rely on shallow models, such as ResNet-18. Even though their limited capacity can mitigate both forgetting and overfitting issues, it leads to inadequate knowledge transfer d",
    "arxiv_url": "https://arxiv.org/abs/2404.02117v1",
    "pdf_url": "https://arxiv.org/pdf/2404.02117v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2404.02117",
    "arxiv_authors": [
      "Keon-Hee Park",
      "Kyungwoo Song",
      "Gyeong-Moon Park"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Pre-trained+Vision+and+Language+Transformers+Are+Few-Shot+Incremental+Learners+Keon-Hee+Park+Kyungwoo+Song+Gyeong-Moon+Park",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "KH Park",
        "id": "tdoOYbwAAAAJ"
      },
      {
        "name": "K Song",
        "id": "HWxRii4AAAAJ"
      },
      {
        "name": "GM Park -",
        "id": null
      }
    ],
    "citation_count": 56
  },
  {
    "arxiv_id": "2305.12943",
    "title": "Album Storytelling with Iterative Story-aware Captioning and Large Language Models",
    "year": 2023,
    "published": "2023-05-22T11:45:10Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "This work studies how to transform an album to vivid and coherent stories, a task we refer to as \"album storytelling\". While this task can help preserve memories and facilitate experience sharing, it remains an underexplored area in current literature. With recent advances in Large Language Models (LLMs), it is now possible to generate lengthy, coherent text, opening up the opportunity to develop an AI assistant for album storytelling. One natural approach is to use caption models to describe ea",
    "arxiv_url": "https://arxiv.org/abs/2305.12943v2",
    "pdf_url": "https://arxiv.org/pdf/2305.12943v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2305.12943",
    "arxiv_authors": [
      "Munan Ning",
      "Yujia Xie",
      "Dongdong Chen",
      "Zeyin Song",
      "Lu Yuan",
      "Yonghong Tian",
      "Qixiang Ye",
      "Li Yuan"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Album+Storytelling+with+Iterative+Story-aware+Captioning+and+Large+Language+Models+Munan+Ning+Yujia+Xie+Dongdong+Chen+Zeyin+Song+Lu+Yuan",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "M Ning",
        "id": "zdBKgeUAAAAJ"
      },
      {
        "name": "Y Xie",
        "id": "r2FiAE4AAAAJ"
      },
      {
        "name": "D Chen",
        "id": "sYKpKqEAAAAJ"
      },
      {
        "name": "Z Song",
        "id": null
      },
      {
        "name": "L Yuan",
        "id": "-5juAR0AAAAJ"
      },
      {
        "name": "Y Tian",
        "id": "fn6hJx0AAAAJ"
      },
      {
        "name": "Q Ye",
        "id": "tjEfgsEAAAAJ"
      },
      {
        "name": "L Yuan",
        "id": "-5juAR0AAAAJ"
      }
    ],
    "citation_count": 7
  },
  {
    "arxiv_id": "2301.11274",
    "title": "Self-Supervised RGB-T Tracking with Cross-Input Consistency",
    "year": 2023,
    "published": "2023-01-26T18:11:16Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.MM"
    ],
    "abstract": "In this paper, we propose a self-supervised RGB-T tracking method. Different from existing deep RGB-T trackers that use a large number of annotated RGB-T image pairs for training, our RGB-T tracker is trained using unlabeled RGB-T video pairs in a self-supervised manner. We propose a novel cross-input consistency-based self-supervised training strategy based on the idea that tracking can be performed using different inputs. Specifically, we construct two distinct inputs using unlabeled RGB-T vid",
    "arxiv_url": "https://arxiv.org/abs/2301.11274v1",
    "pdf_url": "https://arxiv.org/pdf/2301.11274v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2301.11274",
    "arxiv_authors": [
      "Xingchen Zhang",
      "Yiannis Demiris"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Self-Supervised+RGB-T+Tracking+with+Cross-Input+Consistency+Xingchen+Zhang+Yiannis+Demiris",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "X Zhang",
        "id": "w-7b36cAAAAJ"
      },
      {
        "name": "Y Demiris -",
        "id": null
      }
    ],
    "citation_count": 3
  },
  {
    "arxiv_id": "2505.17783",
    "title": "Generative Data Augmentation for Object Point Cloud Segmentation",
    "year": 2025,
    "published": "2025-05-23T11:56:06Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Data augmentation is widely used to train deep learning models to address data scarcity. However, traditional data augmentation (TDA) typically relies on simple geometric transformation, such as random rotation and rescaling, resulting in minimal data diversity enrichment and limited model performance improvement. State-of-the-art generative models for 3D shape generation rely on the denoising diffusion probabilistic models and manage to generate realistic novel point clouds for 3D content creat",
    "arxiv_url": "https://arxiv.org/abs/2505.17783v2",
    "pdf_url": "https://arxiv.org/pdf/2505.17783v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2505.17783",
    "arxiv_authors": [
      "Dekai Zhu",
      "Stefan Gavranovic",
      "Flavien Boussuge",
      "Benjamin Busam",
      "Slobodan Ilic"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Generative+Data+Augmentation+for+Object+Point+Cloud+Segmentation+Dekai+Zhu+Stefan+Gavranovic+Flavien+Boussuge+Benjamin+Busam+Slobodan+Ilic",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "D Zhu",
        "id": "rehMAJUAAAAJ"
      },
      {
        "name": "S Gavranovic",
        "id": null
      },
      {
        "name": "F Boussuge",
        "id": "r-CI-pQAAAAJ"
      },
      {
        "name": "B Busam",
        "id": "u4rJZwUAAAAJ"
      },
      {
        "name": "S Ilic",
        "id": "ELOVd8sAAAAJ"
      }
    ],
    "citation_count": null
  },
  {
    "arxiv_id": "2309.06202",
    "title": "Fast Sparse PCA via Positive Semidefinite Projection for Unsupervised Feature Selection",
    "year": 2023,
    "published": "2023-09-12T13:10:06Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "In the field of unsupervised feature selection, sparse principal component analysis (SPCA) methods have attracted more and more attention recently. Compared to spectral-based methods, SPCA methods don't rely on the construction of a similarity matrix and show better feature selection ability on real-world data. The original SPCA formulates a nonconvex optimization problem. Existing convex SPCA methods reformulate SPCA as a convex model by regarding the reconstruction matrix as an optimization va",
    "arxiv_url": "https://arxiv.org/abs/2309.06202v1",
    "pdf_url": "https://arxiv.org/pdf/2309.06202v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2309.06202",
    "arxiv_authors": [
      "Junjing Zheng",
      "Xinyu Zhang",
      "Yongxiang Liu",
      "Weidong Jiang",
      "Kai Huo",
      "Li Liu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Fast+Sparse+PCA+via+Positive+Semidefinite+Projection+for+Unsupervised+Feature+Selection+Junjing+Zheng+Xinyu+Zhang+Yongxiang+Liu+Weidong+Jiang+Kai+Huo",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "J Zheng",
        "id": "kpeUhMcAAAAJ"
      },
      {
        "name": "X Zhang",
        "id": null
      },
      {
        "name": "Y Liu",
        "id": "a9tTHSEAAAAJ"
      },
      {
        "name": "W Jiang",
        "id": null
      },
      {
        "name": "K Huo",
        "id": null
      },
      {
        "name": "L Liu",
        "id": "9cMQrVsAAAAJ"
      }
    ],
    "citation_count": 6
  },
  {
    "arxiv_id": "2310.03015",
    "title": "Efficient-3DiM: Learning a Generalizable Single-image Novel-view Synthesizer in One Day",
    "year": 2023,
    "published": "2023-10-04T17:57:07Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "The task of novel view synthesis aims to generate unseen perspectives of an object or scene from a limited set of input images. Nevertheless, synthesizing novel views from a single image still remains a significant challenge in the realm of computer vision. Previous approaches tackle this problem by adopting mesh prediction, multi-plain image construction, or more advanced techniques such as neural radiance fields. Recently, a pre-trained diffusion model that is specifically designed for 2D imag",
    "arxiv_url": "https://arxiv.org/abs/2310.03015v1",
    "pdf_url": "https://arxiv.org/pdf/2310.03015v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2310.03015",
    "arxiv_authors": [
      "Yifan Jiang",
      "Hao Tang",
      "Jen-Hao Rick Chang",
      "Liangchen Song",
      "Zhangyang Wang",
      "Liangliang Cao"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Efficient-3DiM%3A+Learning+a+Generalizable+Single-image+Novel-view+Synthesizer+in+One+Day+Yifan+Jiang+Hao+Tang+Jen-Hao+Rick+Chang+Liangchen+Song+Zhangyang+Wang",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "Y Jiang",
        "id": "PMeFEOIAAAAJ"
      },
      {
        "name": "H Tang",
        "id": null
      },
      {
        "name": "JHR Chang",
        "id": "F5Z9kN4AAAAJ"
      },
      {
        "name": "L Song",
        "id": "Kl4T9FYAAAAJ"
      },
      {
        "name": "Z Wang",
        "id": "pxFyKAIAAAAJ"
      },
      {
        "name": "L Cao",
        "id": "S-hBSfIAAAAJ"
      }
    ],
    "citation_count": 13
  },
  {
    "arxiv_id": "2311.15331",
    "title": "How much data do I need? A case study on medical data",
    "year": 2023,
    "published": "2023-11-26T15:31:51Z",
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "abstract": "The collection of data to train a Deep Learning network is costly in terms of effort and resources. In many cases, especially in a medical context, it may have detrimental impacts. Such as requiring invasive medical procedures or processes which could in themselves cause medical harm. However, Deep Learning is seen as a data hungry method. Here, we look at two commonly held adages i) more data gives better results and ii) transfer learning will aid you when you don't have enough data. These are ",
    "arxiv_url": "https://arxiv.org/abs/2311.15331v1",
    "pdf_url": "https://arxiv.org/pdf/2311.15331v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2311.15331",
    "arxiv_authors": [
      "Ayse Betul Cengiz",
      "A. Stephen McGough"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=How+much+data+do+I+need%3F+A+case+study+on+medical+data+Ayse+Betul+Cengiz+A.+Stephen+McGough",
    "gs_search_success": false,
    "gs_authors": null,
    "citation_count": null,
    "error_type": "multiple_matches"
  },
  {
    "arxiv_id": "2505.23637",
    "title": "Comparing the Effects of Persistence Barcodes Aggregation and Feature Concatenation on Medical Imaging",
    "year": 2025,
    "published": "2025-05-29T16:45:33Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "In medical image analysis, feature engineering plays an important role in the design and performance of machine learning models. Persistent homology (PH), from the field of topological data analysis (TDA), demonstrates robustness and stability to data perturbations and addresses the limitation from traditional feature extraction approaches where a small change in input results in a large change in feature representation. Using PH, we store persistent topological and geometrical features in the f",
    "arxiv_url": "https://arxiv.org/abs/2505.23637v2",
    "pdf_url": "https://arxiv.org/pdf/2505.23637v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2505.23637",
    "arxiv_authors": [
      "Dashti A. Ali",
      "Richard K. G. Do",
      "William R. Jarnagin",
      "Aras T. Asaad",
      "Amber L. Simpson"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Comparing+the+Effects+of+Persistence+Barcodes+Aggregation+and+Feature+Concatenation+on+Medical+Imaging+Dashti+A.+Ali+Richard+K.+G.+Do+William+R.+Jarnagin+Aras+T.+Asaad+Amber+L.+Simpson",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "DA Ali",
        "id": "80rf1SUAAAAJ"
      },
      {
        "name": "RKG Do",
        "id": null
      },
      {
        "name": "WR Jarnagin",
        "id": null
      },
      {
        "name": "AT Asaad",
        "id": "HYPTxFkAAAAJ"
      },
      {
        "name": "AL Simpson",
        "id": "MZc-H1EAAAAJ"
      }
    ],
    "citation_count": null
  },
  {
    "arxiv_id": "2309.02450",
    "title": "Self-Supervised Video Transformers for Isolated Sign Language Recognition",
    "year": 2023,
    "published": "2023-09-02T03:00:03Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "This paper presents an in-depth analysis of various self-supervision methods for isolated sign language recognition (ISLR). We consider four recently introduced transformer-based approaches to self-supervised learning from videos, and four pre-training data regimes, and study all the combinations on the WLASL2000 dataset. Our findings reveal that MaskFeat achieves performance superior to pose-based and supervised video models, with a top-1 accuracy of 79.02% on gloss-based WLASL2000. Furthermore",
    "arxiv_url": "https://arxiv.org/abs/2309.02450v1",
    "pdf_url": "https://arxiv.org/pdf/2309.02450v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2309.02450",
    "arxiv_authors": [
      "Marcelo Sandoval-Castaneda",
      "Yanhong Li",
      "Diane Brentari",
      "Karen Livescu",
      "Gregory Shakhnarovich"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Self-Supervised+Video+Transformers+for+Isolated+Sign+Language+Recognition+Marcelo+Sandoval-Castaneda+Yanhong+Li+Diane+Brentari+Karen+Livescu+Gregory+Shakhnarovich",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "M Sandoval-Castaneda",
        "id": "vJkEuTEAAAAJ"
      },
      {
        "name": "Y Li",
        "id": "N5QAWfMAAAAJ"
      },
      {
        "name": "D Brentari",
        "id": "AVT2-U4AAAAJ"
      },
      {
        "name": "K Livescu",
        "id": "kCYbVq0AAAAJ"
      },
      {
        "name": "G Shakhnarovich",
        "id": "YLOz1kgAAAAJ"
      }
    ],
    "citation_count": 13
  },
  {
    "arxiv_id": "2312.15707",
    "title": "High-Fidelity Diffusion-based Image Editing",
    "year": 2023,
    "published": "2023-12-25T12:12:36Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Diffusion models have attained remarkable success in the domains of image generation and editing. It is widely recognized that employing larger inversion and denoising steps in diffusion model leads to improved image reconstruction quality. However, the editing performance of diffusion models tends to be no more satisfactory even with increasing denoising steps. The deficiency in editing could be attributed to the conditional Markovian property of the editing process, where errors accumulate thr",
    "arxiv_url": "https://arxiv.org/abs/2312.15707v3",
    "pdf_url": "https://arxiv.org/pdf/2312.15707v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2312.15707",
    "arxiv_authors": [
      "Chen Hou",
      "Guoqiang Wei",
      "Zhibo Chen"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=High-Fidelity+Diffusion-based+Image+Editing+Chen+Hou+Guoqiang+Wei+Zhibo+Chen",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "C Hou",
        "id": null
      },
      {
        "name": "G Wei",
        "id": "TxeZUTgAAAAJ"
      },
      {
        "name": "Z Chen -",
        "id": null
      }
    ],
    "citation_count": 8
  },
  {
    "arxiv_id": "2306.02850",
    "title": "TRACE: 5D Temporal Regression of Avatars with Dynamic Cameras in 3D Environments",
    "year": 2023,
    "published": "2023-06-05T13:00:44Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Although the estimation of 3D human pose and shape (HPS) is rapidly progressing, current methods still cannot reliably estimate moving humans in global coordinates, which is critical for many applications. This is particularly challenging when the camera is also moving, entangling human and camera motion. To address these issues, we adopt a novel 5D representation (space, time, and identity) that enables end-to-end reasoning about people in scenes. Our method, called TRACE, introduces several no",
    "arxiv_url": "https://arxiv.org/abs/2306.02850v2",
    "pdf_url": "https://arxiv.org/pdf/2306.02850v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2306.02850",
    "arxiv_authors": [
      "Yu Sun",
      "Qian Bao",
      "Wu Liu",
      "Tao Mei",
      "Michael J. Black"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=TRACE%3A+5D+Temporal+Regression+of+Avatars+with+Dynamic+Cameras+in+3D+Environments+Yu+Sun+Qian+Bao+Wu+Liu+Tao+Mei+Michael+J.+Black",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "Y Sun",
        "id": null
      },
      {
        "name": "Q Bao",
        "id": "vhM_c14AAAAJ"
      },
      {
        "name": "W Liu",
        "id": "rQpizr0AAAAJ"
      },
      {
        "name": "T Mei",
        "id": "7Yq4wf4AAAAJ"
      },
      {
        "name": "MJ Black",
        "id": "6NjbexEAAAAJ"
      }
    ],
    "citation_count": 98
  },
  {
    "arxiv_id": "2504.08175",
    "title": "Multi-person Physics-based Pose Estimation for Combat Sports",
    "year": 2025,
    "published": "2025-04-11T00:08:14Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "We propose a novel framework for accurate 3D human pose estimation in combat sports using sparse multi-camera setups. Our method integrates robust multi-view 2D pose tracking via a transformer-based top-down approach, employing epipolar geometry constraints and long-term video object segmentation for consistent identity tracking across views. Initial 3D poses are obtained through weighted triangulation and spline smoothing, followed by kinematic optimization to refine pose accuracy. We further e",
    "arxiv_url": "https://arxiv.org/abs/2504.08175v3",
    "pdf_url": "https://arxiv.org/pdf/2504.08175v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2504.08175",
    "arxiv_authors": [
      "Hossein Feiz",
      "David Labb√©",
      "Thomas Romeas",
      "Jocelyn Faubert",
      "Sheldon Andrews"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Multi-person+Physics-based+Pose+Estimation+for+Combat+Sports+Hossein+Feiz+David+Labb%C3%A9+Thomas+Romeas+Jocelyn+Faubert+Sheldon+Andrews",
    "gs_search_success": false,
    "gs_authors": null,
    "citation_count": null,
    "error_type": "fetch_failed"
  },
  {
    "arxiv_id": "2403.18996",
    "title": "Envisioning MedCLIP: A Deep Dive into Explainability for Medical Vision-Language Models",
    "year": 2024,
    "published": "2024-03-27T20:30:01Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Explaining Deep Learning models is becoming increasingly important in the face of daily emerging multimodal models, particularly in safety-critical domains like medical imaging. However, the lack of detailed investigations into the performance of explainability methods on these models is widening the gap between their development and safe deployment. In this work, we analyze the performance of various explainable AI methods on a vision-language model, MedCLIP, to demystify its inner workings. We",
    "arxiv_url": "https://arxiv.org/abs/2403.18996v1",
    "pdf_url": "https://arxiv.org/pdf/2403.18996v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2403.18996",
    "arxiv_authors": [
      "Anees Ur Rehman Hashmi",
      "Dwarikanath Mahapatra",
      "Mohammad Yaqub"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Envisioning+MedCLIP%3A+A+Deep+Dive+into+Explainability+for+Medical+Vision-Language+Models+Anees+Ur+Rehman+Hashmi+Dwarikanath+Mahapatra+Mohammad+Yaqub",
    "gs_search_success": false,
    "gs_authors": null,
    "citation_count": null,
    "error_type": "multiple_matches"
  },
  {
    "arxiv_id": "2308.08321",
    "title": "Stable and Causal Inference for Discriminative Self-supervised Deep Visual Representations",
    "year": 2023,
    "published": "2023-08-16T12:30:17Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "In recent years, discriminative self-supervised methods have made significant strides in advancing various visual tasks. The central idea of learning a data encoder that is robust to data distortions/augmentations is straightforward yet highly effective. Although many studies have demonstrated the empirical success of various learning methods, the resulting learned representations can exhibit instability and hinder downstream performance. In this study, we analyze discriminative self-supervised ",
    "arxiv_url": "https://arxiv.org/abs/2308.08321v1",
    "pdf_url": "https://arxiv.org/pdf/2308.08321v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2308.08321",
    "arxiv_authors": [
      "Yuewei Yang",
      "Hai Li",
      "Yiran Chen"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Stable+and+Causal+Inference+for+Discriminative+Self-supervised+Deep+Visual+Representations+Yuewei+Yang+Hai+Li+Yiran+Chen",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "Y Yang",
        "id": "dTO35VYAAAAJ"
      },
      {
        "name": "H Li",
        "id": "E6Tpfq8AAAAJ"
      },
      {
        "name": "Y Chen -",
        "id": null
      }
    ],
    "citation_count": 1
  },
  {
    "arxiv_id": "2303.03817",
    "title": "Region and Spatial Aware Anomaly Detection for Fundus Images",
    "year": 2023,
    "published": "2023-03-07T11:33:22Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Recently anomaly detection has drawn much attention in diagnosing ocular diseases. Most existing anomaly detection research in fundus images has relatively large anomaly scores in the salient retinal structures, such as blood vessels, optical cups and discs. In this paper, we propose a Region and Spatial Aware Anomaly Detection (ReSAD) method for fundus images, which obtains local region and long-range spatial information to reduce the false positives in the normal structure. ReSAD transfers a p",
    "arxiv_url": "https://arxiv.org/abs/2303.03817v1",
    "pdf_url": "https://arxiv.org/pdf/2303.03817v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2303.03817",
    "arxiv_authors": [
      "Jingqi Niu",
      "Shiwen Dong",
      "Qinji Yu",
      "Kang Dang",
      "Xiaowei Ding"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Region+and+Spatial+Aware+Anomaly+Detection+for+Fundus+Images+Jingqi+Niu+Shiwen+Dong+Qinji+Yu+Kang+Dang+Xiaowei+Ding",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "J Niu",
        "id": null
      },
      {
        "name": "S Dong",
        "id": null
      },
      {
        "name": "Q Yu",
        "id": "M80j6WIAAAAJ"
      },
      {
        "name": "K Dang",
        "id": "LzZJR4kAAAAJ"
      },
      {
        "name": "X Ding2023 IEEE 20th International",
        "id": null
      }
    ],
    "citation_count": 2
  },
  {
    "arxiv_id": "2407.04712",
    "title": "Sensing technologies and machine learning methods for emotion recognition in autism: Systematic review",
    "year": 2024,
    "published": "2024-05-15T19:48:04Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "abstract": "Background: Human Emotion Recognition (HER) has been a popular field of study in the past years. Despite the great progresses made so far, relatively little attention has been paid to the use of HER in autism. People with autism are known to face problems with daily social communication and the prototypical interpretation of emotional responses, which are most frequently exerted via facial expressions. This poses significant practical challenges to the application of regular HER systems, which a",
    "arxiv_url": "https://arxiv.org/abs/2407.04712v1",
    "pdf_url": "https://arxiv.org/pdf/2407.04712v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2407.04712",
    "arxiv_authors": [
      "Oresti Banos",
      "Zhoe Comas-Gonz√°lez",
      "Javier Medina",
      "Aurora Polo-Rodr√≠guez",
      "David Gil",
      "Jes√∫s Peral",
      "Sandra Amador",
      "Claudia Villalonga"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Sensing+technologies+and+machine+learning+methods+for+emotion+recognition+in+autism%3A+Systematic+review+Oresti+Banos+Zhoe+Comas-Gonz%C3%A1lez+Javier+Medina+Aurora+Polo-Rodr%C3%ADguez+David+Gil",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "O Banos",
        "id": "iwtfxPwAAAAJ"
      },
      {
        "name": "J Medina",
        "id": "Zuj3OmIAAAAJ"
      },
      {
        "name": "A Polo-Rodr√≠guez",
        "id": "8v9QnncAAAAJ"
      },
      {
        "name": "D Gil",
        "id": "fMoDEMoAAAAJ"
      },
      {
        "name": "J Peral",
        "id": "fD5SBhcAAAAJ"
      },
      {
        "name": "S Amador",
        "id": "P7jFA1QAAAAJ"
      }
    ],
    "citation_count": 13
  },
  {
    "arxiv_id": "2505.22980",
    "title": "MOVi: Training-free Text-conditioned Multi-Object Video Generation",
    "year": 2025,
    "published": "2025-05-29T01:41:10Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Recent advances in diffusion-based text-to-video (T2V) models have demonstrated remarkable progress, but these models still face challenges in generating videos with multiple objects. Most models struggle with accurately capturing complex object interactions, often treating some objects as static background elements and limiting their movement. In addition, they often fail to generate multiple distinct objects as specified in the prompt, resulting in incorrect generations or mixed features acros",
    "arxiv_url": "https://arxiv.org/abs/2505.22980v1",
    "pdf_url": "https://arxiv.org/pdf/2505.22980v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2505.22980",
    "arxiv_authors": [
      "Aimon Rahman",
      "Jiang Liu",
      "Ze Wang",
      "Ximeng Sun",
      "Jialian Wu",
      "Xiaodong Yu",
      "Yusheng Su",
      "Vishal M. Patel",
      "Zicheng Liu",
      "Emad Barsoum"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=MOVi%3A+Training-free+Text-conditioned+Multi-Object+Video+Generation+Aimon+Rahman+Jiang+Liu+Ze+Wang+Ximeng+Sun+Jialian+Wu",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "A Rahman",
        "id": "h52EUGcAAAAJ"
      },
      {
        "name": "J Liu",
        "id": "IbeXR9cAAAAJ"
      },
      {
        "name": "Z Wang",
        "id": "80Jw_w8AAAAJ"
      },
      {
        "name": "X Sun",
        "id": null
      },
      {
        "name": "J Wu",
        "id": "6Abc8OgAAAAJ"
      },
      {
        "name": "X Yu",
        "id": "nmyIoRMAAAAJ"
      },
      {
        "name": "Y Su",
        "id": null
      },
      {
        "name": "VM Patel",
        "id": "AkEXTbIAAAAJ"
      },
      {
        "name": "Z Liu",
        "id": "bkALdvsAAAAJ"
      },
      {
        "name": "E Barsoum",
        "id": "bX1YILcAAAAJ"
      }
    ],
    "citation_count": 1
  },
  {
    "arxiv_id": "2310.02753",
    "title": "MUNCH: Modelling Unique 'N Controllable Heads",
    "year": 2023,
    "published": "2023-10-04T11:44:20Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR",
      "cs.LG"
    ],
    "abstract": "The automated generation of 3D human heads has been an intriguing and challenging task for computer vision researchers. Prevailing methods synthesize realistic avatars but with limited control over the diversity and quality of rendered outputs and suffer from limited correlation between shape and texture of the character. We propose a method that offers quality, diversity, control, and realism along with explainable network design, all desirable features to game-design artists in the domain. Fir",
    "arxiv_url": "https://arxiv.org/abs/2310.02753v1",
    "pdf_url": "https://arxiv.org/pdf/2310.02753v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2310.02753",
    "arxiv_authors": [
      "Debayan Deb",
      "Suvidha Tripathi",
      "Pranit Puri"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=MUNCH%3A+Modelling+Unique+%27N+Controllable+Heads+Debayan+Deb+Suvidha+Tripathi+Pranit+Puri",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "D Deb",
        "id": "tLQHjrAAAAAJ"
      },
      {
        "name": "S Tripathi",
        "id": "CURvyPcAAAAJ"
      },
      {
        "name": "P Puri -",
        "id": null
      }
    ],
    "citation_count": 1
  },
  {
    "arxiv_id": "2408.08258",
    "title": "Snuffy: Efficient Whole Slide Image Classifier",
    "year": 2024,
    "published": "2024-08-15T16:59:15Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "cs.NE",
      "eess.IV"
    ],
    "abstract": "Whole Slide Image (WSI) classification with multiple instance learning (MIL) in digital pathology faces significant computational challenges. Current methods mostly rely on extensive self-supervised learning (SSL) for satisfactory performance, requiring long training periods and considerable computational resources. At the same time, no pre-training affects performance due to domain shifts from natural images to WSIs. We introduce Snuffy architecture, a novel MIL-pooling method based on sparse t",
    "arxiv_url": "https://arxiv.org/abs/2408.08258v3",
    "pdf_url": "https://arxiv.org/pdf/2408.08258v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2408.08258",
    "arxiv_authors": [
      "Hossein Jafarinia",
      "Alireza Alipanah",
      "Danial Hamdi",
      "Saeed Razavi",
      "Nahal Mirzaie",
      "Mohammad Hossein Rohban"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Snuffy%3A+Efficient+Whole+Slide+Image+Classifier+Hossein+Jafarinia+Alireza+Alipanah+Danial+Hamdi+Saeed+Razavi+Nahal+Mirzaie",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "H Jafarinia",
        "id": "TkxK_OgAAAAJ"
      },
      {
        "name": "A Alipanah",
        "id": "HholaK4AAAAJ"
      },
      {
        "name": "S Razavi",
        "id": "5I-A3XsAAAAJ"
      },
      {
        "name": "N Mirzaie",
        "id": "7IaTpQQAAAAJ"
      },
      {
        "name": "MH RohbanEuropean",
        "id": null
      }
    ],
    "citation_count": 7
  },
  {
    "arxiv_id": "2403.02329",
    "title": "COMMIT: Certifying Robustness of Multi-Sensor Fusion Systems against Semantic Attacks",
    "year": 2024,
    "published": "2024-03-04T18:57:11Z",
    "categories": [
      "cs.LG",
      "cs.CR",
      "cs.CV"
    ],
    "abstract": "Multi-sensor fusion systems (MSFs) play a vital role as the perception module in modern autonomous vehicles (AVs). Therefore, ensuring their robustness against common and realistic adversarial semantic transformations, such as rotation and shifting in the physical world, is crucial for the safety of AVs. While empirical evidence suggests that MSFs exhibit improved robustness compared to single-modal models, they are still vulnerable to adversarial semantic transformations. Despite the proposal o",
    "arxiv_url": "https://arxiv.org/abs/2403.02329v1",
    "pdf_url": "https://arxiv.org/pdf/2403.02329v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2403.02329",
    "arxiv_authors": [
      "Zijian Huang",
      "Wenda Chu",
      "Linyi Li",
      "Chejian Xu",
      "Bo Li"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=COMMIT%3A+Certifying+Robustness+of+Multi-Sensor+Fusion+Systems+against+Semantic+Attacks+Zijian+Huang+Wenda+Chu+Linyi+Li+Chejian+Xu+Bo+Li",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "Z Huang",
        "id": "9dlrr8MAAAAJ"
      },
      {
        "name": "W Chu",
        "id": "2QE98SEAAAAJ"
      },
      {
        "name": "L Li",
        "id": "-b0sk-YAAAAJ"
      },
      {
        "name": "C Xu",
        "id": "YbDy6k0AAAAJ"
      },
      {
        "name": "B Li -",
        "id": null
      }
    ],
    "citation_count": null
  },
  {
    "arxiv_id": "2301.09914",
    "title": "Multimodal Interactive Lung Lesion Segmentation: A Framework for Annotating PET/CT Images based on Physiological and Anatomical Cues",
    "year": 2023,
    "published": "2023-01-24T10:50:45Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Recently, deep learning enabled the accurate segmentation of various diseases in medical imaging. These performances, however, typically demand large amounts of manual voxel annotations. This tedious process for volumetric data becomes more complex when not all required information is available in a single imaging domain as is the case for PET/CT data. We propose a multimodal interactive segmentation framework that mitigates these issues by combining anatomical and physiological cues from PET/CT",
    "arxiv_url": "https://arxiv.org/abs/2301.09914v1",
    "pdf_url": "https://arxiv.org/pdf/2301.09914v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2301.09914",
    "arxiv_authors": [
      "Verena Jasmin Hallitschke",
      "Tobias Schlumberger",
      "Philipp Kataliakos",
      "Zdravko Marinov",
      "Moon Kim",
      "Lars Heiliger",
      "Constantin Seibold",
      "Jens Kleesiek",
      "Rainer Stiefelhagen"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Multimodal+Interactive+Lung+Lesion+Segmentation%3A+A+Framework+for+Annotating+PET%2FCT+Images+based+on+Physiological+and+Anatomical+Cues+Verena+Jasmin+Hallitschke+Tobias+Schlumberger+Philipp+Kataliakos+Zdravko+Marinov+Moon+Kim",
    "gs_search_success": false,
    "gs_authors": null,
    "citation_count": null,
    "error_type": "multiple_matches"
  },
  {
    "arxiv_id": "2407.15339",
    "title": "Deep Learning for Economists",
    "year": 2024,
    "published": "2024-07-22T02:53:18Z",
    "categories": [
      "econ.GN",
      "cs.CL",
      "cs.CV"
    ],
    "abstract": "Deep learning provides powerful methods to impute structured information from large-scale, unstructured text and image datasets. For example, economists might wish to detect the presence of economic activity in satellite images, or to measure the topics or entities mentioned in social media, the congressional record, or firm filings. This review introduces deep neural networks, covering methods such as classifiers, regression models, generative AI, and embedding models. Applications include clas",
    "arxiv_url": "https://arxiv.org/abs/2407.15339v3",
    "pdf_url": "https://arxiv.org/pdf/2407.15339v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2407.15339",
    "arxiv_authors": [
      "Melissa Dell"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Deep+Learning+for+Economists+Melissa+Dell",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "M Dell -",
        "id": null
      }
    ],
    "citation_count": 86
  },
  {
    "arxiv_id": "2408.03388",
    "title": "A Non-negative VAE:the Generalized Gamma Belief Network",
    "year": 2024,
    "published": "2024-08-06T18:18:37Z",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "abstract": "The gamma belief network (GBN), often regarded as a deep topic model, has demonstrated its potential for uncovering multi-layer interpretable latent representations in text data. Its notable capability to acquire interpretable latent factors is partially attributed to sparse and non-negative gamma-distributed latent variables. However, the existing GBN and its variations are constrained by the linear generative model, thereby limiting their expressiveness and applicability. To address this limit",
    "arxiv_url": "https://arxiv.org/abs/2408.03388v2",
    "pdf_url": "https://arxiv.org/pdf/2408.03388v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2408.03388",
    "arxiv_authors": [
      "Zhibin Duan",
      "Tiansheng Wen",
      "Muyao Wang",
      "Bo Chen",
      "Mingyuan Zhou"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=A+Non-negative+VAE%3Athe+Generalized+Gamma+Belief+Network+Zhibin+Duan+Tiansheng+Wen+Muyao+Wang+Bo+Chen+Mingyuan+Zhou",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "Z Duan",
        "id": "bITyHaEAAAAJ"
      },
      {
        "name": "T Wen",
        "id": "mrdyOyQAAAAJ"
      },
      {
        "name": "M Wang",
        "id": null
      },
      {
        "name": "B Chen",
        "id": "uv16_-UAAAAJ"
      },
      {
        "name": "M Zhou -",
        "id": null
      }
    ],
    "citation_count": 2
  },
  {
    "arxiv_id": "2412.16576",
    "title": "Open-Vocabulary Mobile Manipulation Based on Double Relaxed Contrastive Learning with Dense Labeling",
    "year": 2024,
    "published": "2024-12-21T10:40:56Z",
    "categories": [
      "cs.RO",
      "cs.CL",
      "cs.CV"
    ],
    "abstract": "Growing labor shortages are increasing the demand for domestic service robots (DSRs) to assist in various settings. In this study, we develop a DSR that transports everyday objects to specified pieces of furniture based on open-vocabulary instructions. Our approach focuses on retrieving images of target objects and receptacles from pre-collected images of indoor environments. For example, given an instruction \"Please get the right red towel hanging on the metal towel rack and put it in the white",
    "arxiv_url": "https://arxiv.org/abs/2412.16576v2",
    "pdf_url": "https://arxiv.org/pdf/2412.16576v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2412.16576",
    "arxiv_authors": [
      "Daichi Yashima",
      "Ryosuke Korekata",
      "Komei Sugiura"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Open-Vocabulary+Mobile+Manipulation+Based+on+Double+Relaxed+Contrastive+Learning+with+Dense+Labeling+Daichi+Yashima+Ryosuke+Korekata+Komei+Sugiura",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "D Yashima",
        "id": "dB2alekAAAAJ"
      },
      {
        "name": "R Korekata",
        "id": "zcbtV6QAAAAJ"
      },
      {
        "name": "K SugiuraIEEE Robotics and Automation Letters",
        "id": null
      }
    ],
    "citation_count": 6
  },
  {
    "arxiv_id": "2408.16195",
    "title": "DLM-VMTL:A Double Layer Mapper for heterogeneous data video Multi-task prompt learning",
    "year": 2024,
    "published": "2024-08-29T01:25:36Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "In recent years, the parameters of backbones of Video Understanding tasks continue to increase and even reach billion-level. Whether fine-tuning a specific task on the Video Foundation Model or pre-training the model designed for the specific task, incurs a lot of overhead. How to make these models play other values than their own tasks becomes a worthy question. Multi-Task Learning(MTL) makes the visual task acquire the rich shareable knowledge from other tasks while joint training. It is fully",
    "arxiv_url": "https://arxiv.org/abs/2408.16195v1",
    "pdf_url": "https://arxiv.org/pdf/2408.16195v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2408.16195",
    "arxiv_authors": [
      "Zeyi Bo",
      "Wuxi Sun",
      "Ye Jin"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=DLM-VMTL%3AA+Double+Layer+Mapper+for+heterogeneous+data+video+Multi-task+prompt+learning+Zeyi+Bo+Wuxi+Sun+Ye+Jin",
    "gs_search_success": false,
    "gs_authors": null,
    "citation_count": null,
    "error_type": "fetch_failed"
  },
  {
    "arxiv_id": "2410.13807",
    "title": "Improving Consistency in Diffusion Models for Image Super-Resolution",
    "year": 2024,
    "published": "2024-10-17T17:41:52Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Recent methods exploit the powerful text-to-image (T2I) diffusion models for real-world image super-resolution (Real-ISR) and achieve impressive results compared to previous models. However, we observe two kinds of inconsistencies in diffusion-based methods which hinder existing models from fully exploiting diffusion priors. The first is the semantic inconsistency arising from diffusion guidance. T2I generation focuses on semantic-level consistency with text prompts, while Real-ISR emphasizes pi",
    "arxiv_url": "https://arxiv.org/abs/2410.13807v2",
    "pdf_url": "https://arxiv.org/pdf/2410.13807v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2410.13807",
    "arxiv_authors": [
      "Junhao Gu",
      "Peng-Tao Jiang",
      "Hao Zhang",
      "Mi Zhou",
      "Jinwei Chen",
      "Wenming Yang",
      "Bo Li"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Improving+Consistency+in+Diffusion+Models+for+Image+Super-Resolution+Junhao+Gu+Peng-Tao+Jiang+Hao+Zhang+Mi+Zhou+Jinwei+Chen",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "J Gu",
        "id": null
      },
      {
        "name": "PT Jiang",
        "id": "85QJ_i4AAAAJ"
      },
      {
        "name": "H Zhang",
        "id": null
      },
      {
        "name": "M Zhou",
        "id": "QQO_lvkAAAAJ"
      },
      {
        "name": "J Chen",
        "id": null
      },
      {
        "name": "W Yang",
        "id": null
      },
      {
        "name": "B Li",
        "id": "NVzQ87sAAAAJ"
      }
    ],
    "citation_count": null
  },
  {
    "arxiv_id": "2502.01312",
    "title": "CleanPose: Category-Level Object Pose Estimation via Causal Learning and Knowledge Distillation",
    "year": 2025,
    "published": "2025-02-03T12:41:36Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Category-level object pose estimation aims to recover the rotation, translation and size of unseen instances within predefined categories. In this task, deep neural network-based methods have demonstrated remarkable performance. However, previous studies show they suffer from spurious correlations raised by \"unclean\" confounders in models, hindering their performance on novel instances with significant variations. To address this issue, we propose CleanPose, a novel approach integrating causal l",
    "arxiv_url": "https://arxiv.org/abs/2502.01312v2",
    "pdf_url": "https://arxiv.org/pdf/2502.01312v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2502.01312",
    "arxiv_authors": [
      "Xiao Lin",
      "Yun Peng",
      "Liuyi Wang",
      "Xianyou Zhong",
      "Minghao Zhu",
      "Jingwei Yang",
      "Yi Feng",
      "Chengju Liu",
      "Qijun Chen"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=CleanPose%3A+Category-Level+Object+Pose+Estimation+via+Causal+Learning+and+Knowledge+Distillation+Xiao+Lin+Yun+Peng+Liuyi+Wang+Xianyou+Zhong+Minghao+Zhu",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "X Lin",
        "id": "KGDYSHQAAAAJ"
      },
      {
        "name": "Y Peng",
        "id": "wKkiLa0AAAAJ"
      },
      {
        "name": "L Wang",
        "id": null
      },
      {
        "name": "X Zhong",
        "id": "Rv0e_ukAAAAJ"
      },
      {
        "name": "M Zhu",
        "id": "8RzvsyEAAAAJ"
      },
      {
        "name": "Y Feng",
        "id": null
      },
      {
        "name": "J Yang",
        "id": null
      },
      {
        "name": "C Liu",
        "id": "O55rDMQAAAAJ"
      },
      {
        "name": "Q Chen",
        "id": null
      }
    ],
    "citation_count": 3
  },
  {
    "arxiv_id": "2405.14584",
    "title": "SE3D: A Framework For Saliency Method Evaluation In 3D Imaging",
    "year": 2024,
    "published": "2024-05-23T13:55:11Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "For more than a decade, deep learning models have been dominating in various 2D imaging tasks. Their application is now extending to 3D imaging, with 3D Convolutional Neural Networks (3D CNNs) being able to process LIDAR, MRI, and CT scans, with significant implications for fields such as autonomous driving and medical imaging. In these critical settings, explaining the model's decisions is fundamental. Despite recent advances in Explainable Artificial Intelligence, however, little effort has be",
    "arxiv_url": "https://arxiv.org/abs/2405.14584v2",
    "pdf_url": "https://arxiv.org/pdf/2405.14584v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2405.14584",
    "arxiv_authors": [
      "Mariusz Wi≈õniewski",
      "Loris Giulivi",
      "Giacomo Boracchi"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=SE3D%3A+A+Framework+For+Saliency+Method+Evaluation+In+3D+Imaging+Mariusz+Wi%C5%9Bniewski+Loris+Giulivi+Giacomo+Boracchi",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "M Wi≈õniewski",
        "id": "Sb8OUW8AAAAJ"
      },
      {
        "name": "L Giulivi",
        "id": "RYkpBV0AAAAJ"
      },
      {
        "name": "G Boracchi2024 IEEE International",
        "id": null
      }
    ],
    "citation_count": 1
  },
  {
    "arxiv_id": "2306.09347",
    "title": "Segment Any Point Cloud Sequences by Distilling Vision Foundation Models",
    "year": 2023,
    "published": "2023-06-15T17:59:54Z",
    "categories": [
      "cs.CV",
      "cs.LG",
      "cs.RO"
    ],
    "abstract": "Recent advancements in vision foundation models (VFMs) have opened up new possibilities for versatile and efficient visual perception. In this work, we introduce Seal, a novel framework that harnesses VFMs for segmenting diverse automotive point cloud sequences. Seal exhibits three appealing properties: i) Scalability: VFMs are directly distilled into point clouds, obviating the need for annotations in either 2D or 3D during pretraining. ii) Consistency: Spatial and temporal relationships are en",
    "arxiv_url": "https://arxiv.org/abs/2306.09347v2",
    "pdf_url": "https://arxiv.org/pdf/2306.09347v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2306.09347",
    "arxiv_authors": [
      "Youquan Liu",
      "Lingdong Kong",
      "Jun Cen",
      "Runnan Chen",
      "Wenwei Zhang",
      "Liang Pan",
      "Kai Chen",
      "Ziwei Liu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Segment+Any+Point+Cloud+Sequences+by+Distilling+Vision+Foundation+Models+Youquan+Liu+Lingdong+Kong+Jun+Cen+Runnan+Chen+Wenwei+Zhang",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "Y Liu",
        "id": "J9a48hMAAAAJ"
      },
      {
        "name": "L Kong",
        "id": "-j1j7TkAAAAJ"
      },
      {
        "name": "J Cen",
        "id": "7SKAhBwAAAAJ"
      },
      {
        "name": "R Chen",
        "id": "Uq2DuzkAAAAJ"
      },
      {
        "name": "W Zhang",
        "id": "QDXADSEAAAAJ"
      },
      {
        "name": "L Pan",
        "id": "lSDISOcAAAAJ"
      },
      {
        "name": "K Chen",
        "id": "eGD0b7IAAAAJ"
      },
      {
        "name": "Z LiuAdvances in Neural Information Processing Systems",
        "id": null
      }
    ],
    "citation_count": 99
  },
  {
    "arxiv_id": "2302.14452",
    "title": "An Effective Crop-Paste Pipeline for Few-shot Object Detection",
    "year": 2023,
    "published": "2023-02-28T09:56:45Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Few-shot object detection (FSOD) aims to expand an object detector for novel categories given only a few instances for training. However, detecting novel categories with only a few samples usually leads to the problem of misclassification. In FSOD, we notice the false positive (FP) of novel categories is prominent, in which the base categories are often recognized as novel ones. To address this issue, a novel data augmentation pipeline that Crops the Novel instances and Pastes them on the select",
    "arxiv_url": "https://arxiv.org/abs/2302.14452v2",
    "pdf_url": "https://arxiv.org/pdf/2302.14452v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2302.14452",
    "arxiv_authors": [
      "Shaobo Lin",
      "Kun Wang",
      "Xingyu Zeng",
      "Rui Zhao"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=An+Effective+Crop-Paste+Pipeline+for+Few-shot+Object+Detection+Shaobo+Lin+Kun+Wang+Xingyu+Zeng+Rui+Zhao",
    "gs_search_success": false,
    "gs_authors": null,
    "citation_count": null,
    "error_type": "fetch_failed"
  },
  {
    "arxiv_id": "2309.10533",
    "title": "Decoupling the Curve Modeling and Pavement Regression for Lane Detection",
    "year": 2023,
    "published": "2023-09-19T11:24:14Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "The curve-based lane representation is a popular approach in many lane detection methods, as it allows for the representation of lanes as a whole object and maximizes the use of holistic information about the lanes. However, the curves produced by these methods may not fit well with irregular lines, which can lead to gaps in performance compared to indirect representations such as segmentation-based or point-based methods. We have observed that these lanes are not intended to be irregular, but t",
    "arxiv_url": "https://arxiv.org/abs/2309.10533v1",
    "pdf_url": "https://arxiv.org/pdf/2309.10533v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2309.10533",
    "arxiv_authors": [
      "Wencheng Han",
      "Jianbing Shen"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Decoupling+the+Curve+Modeling+and+Pavement+Regression+for+Lane+Detection+Wencheng+Han+Jianbing+Shen",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "W Han",
        "id": "hGZueIUAAAAJ"
      },
      {
        "name": "J Shen -",
        "id": null
      }
    ],
    "citation_count": 16
  },
  {
    "arxiv_id": "2411.07335",
    "title": "Balancing Multimodal Training Through Game-Theoretic Regularization",
    "year": 2024,
    "published": "2024-11-11T19:53:05Z",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "cs.GT",
      "cs.MM"
    ],
    "abstract": "Multimodal learning holds promise for richer information extraction by capturing dependencies across data sources. Yet, current training methods often underperform due to modality competition, a phenomenon where modalities contend for training resources leaving some underoptimized. This raises a pivotal question: how can we address training imbalances, ensure adequate optimization across all modalities, and achieve consistent performance improvements as we transition from unimodal to multimodal ",
    "arxiv_url": "https://arxiv.org/abs/2411.07335v3",
    "pdf_url": "https://arxiv.org/pdf/2411.07335v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2411.07335",
    "arxiv_authors": [
      "Konstantinos Kontras",
      "Thomas Strypsteen",
      "Christos Chatzichristos",
      "Paul Pu Liang",
      "Matthew Blaschko",
      "Maarten De Vos"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Balancing+Multimodal+Training+Through+Game-Theoretic+Regularization+Konstantinos+Kontras+Thomas+Strypsteen+Christos+Chatzichristos+Paul+Pu+Liang+Matthew+Blaschko",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "K Kontras",
        "id": "5BuwpwIAAAAJ"
      },
      {
        "name": "T Strypsteen",
        "id": null
      },
      {
        "name": "C Chatzichristos",
        "id": "eZ8HDxcAAAAJ"
      },
      {
        "name": "PP Liang",
        "id": "pKf5LtQAAAAJ"
      },
      {
        "name": "MB Blaschko",
        "id": "EmmO7LcAAAAJ"
      },
      {
        "name": "M De VosThe Thirty-ninth Annual",
        "id": null
      }
    ],
    "citation_count": 1
  },
  {
    "arxiv_id": "2304.07918",
    "title": "Likelihood-Based Generative Radiance Field with Latent Space Energy-Based Model for 3D-Aware Disentangled Image Representation",
    "year": 2023,
    "published": "2023-04-16T23:44:41Z",
    "categories": [
      "cs.CV",
      "stat.ML"
    ],
    "abstract": "We propose the NeRF-LEBM, a likelihood-based top-down 3D-aware 2D image generative model that incorporates 3D representation via Neural Radiance Fields (NeRF) and 2D imaging process via differentiable volume rendering. The model represents an image as a rendering process from 3D object to 2D image and is conditioned on some latent variables that account for object characteristics and are assumed to follow informative trainable energy-based prior models. We propose two likelihood-based learning f",
    "arxiv_url": "https://arxiv.org/abs/2304.07918v1",
    "pdf_url": "https://arxiv.org/pdf/2304.07918v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2304.07918",
    "arxiv_authors": [
      "Yaxuan Zhu",
      "Jianwen Xie",
      "Ping Li"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Likelihood-Based+Generative+Radiance+Field+with+Latent+Space+Energy-Based+Model+for+3D-Aware+Disentangled+Image+Representation+Yaxuan+Zhu+Jianwen+Xie+Ping+Li",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "Y Zhu",
        "id": "EptgCGsAAAAJ"
      },
      {
        "name": "J Xie",
        "id": "O3p4CIQAAAAJ"
      },
      {
        "name": "P Li -",
        "id": null
      }
    ],
    "citation_count": 6
  },
  {
    "arxiv_id": "2407.10414",
    "title": "Teaching CORnet Human fMRI Representations for Enhanced Model-Brain Alignment",
    "year": 2024,
    "published": "2024-07-15T03:31:42Z",
    "categories": [
      "eess.IV",
      "cs.CV",
      "cs.LG",
      "q-bio.NC"
    ],
    "abstract": "Deep convolutional neural networks (DCNNs) have demonstrated excellent performance in object recognition and have been found to share some similarities with brain visual processing. However, the substantial gap between DCNNs and human visual perception still exists. Functional magnetic resonance imaging (fMRI) as a widely used technique in cognitive neuroscience can record neural activation in the human visual cortex during the process of visual perception. Can we teach DCNNs human fMRI signals ",
    "arxiv_url": "https://arxiv.org/abs/2407.10414v1",
    "pdf_url": "https://arxiv.org/pdf/2407.10414v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2407.10414",
    "arxiv_authors": [
      "Zitong Lu",
      "Yile Wang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Teaching+CORnet+Human+fMRI+Representations+for+Enhanced+Model-Brain+Alignment+Zitong+Lu+Yile+Wang",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "Z Lu",
        "id": "bE5VCKsAAAAJ"
      },
      {
        "name": "Y Wang - Cognitive Neurodynamics",
        "id": null
      }
    ],
    "citation_count": 3
  },
  {
    "arxiv_id": "2304.10415",
    "title": "NTIRE 2023 Challenge on Light Field Image Super-Resolution: Dataset, Methods and Results",
    "year": 2023,
    "published": "2023-04-20T15:59:31Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "In this report, we summarize the first NTIRE challenge on light field (LF) image super-resolution (SR), which aims at super-resolving LF images under the standard bicubic degradation with a magnification factor of 4. This challenge develops a new LF dataset called NTIRE-2023 for validation and test, and provides a toolbox called BasicLFSR to facilitate model development. Compared with single image SR, the major challenge of LF image SR lies in how to exploit complementary angular information fro",
    "arxiv_url": "https://arxiv.org/abs/2304.10415v1",
    "pdf_url": "https://arxiv.org/pdf/2304.10415v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2304.10415",
    "arxiv_authors": [
      "Yingqian Wang",
      "Longguang Wang",
      "Zhengyu Liang",
      "Jungang Yang",
      "Radu Timofte",
      "Yulan Guo"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=NTIRE+2023+Challenge+on+Light+Field+Image+Super-Resolution%3A+Dataset%2C+Methods+and+Results+Yingqian+Wang+Longguang+Wang+Zhengyu+Liang+Jungang+Yang+Radu+Timofte",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "Y Wang",
        "id": "tBA4alMAAAAJ"
      },
      {
        "name": "L Wang",
        "id": "gbBAujsAAAAJ"
      },
      {
        "name": "Z Liang",
        "id": "-Kvo988AAAAJ"
      },
      {
        "name": "J Yang",
        "id": "x0jlbE4AAAAJ"
      },
      {
        "name": "R Timofte",
        "id": "u3MwH5kAAAAJ"
      },
      {
        "name": "Y Guo",
        "id": "WQRNvdsAAAAJ"
      },
      {
        "name": "K Jin",
        "id": "M77BCKEAAAAJ"
      },
      {
        "name": "Z Wei",
        "id": "NVxaEUsvZeoC"
      },
      {
        "name": "A Yang",
        "id": null
      },
      {
        "name": "S Guo",
        "id": "GQLrk8MAAAAJ"
      },
      {
        "name": "M Gao",
        "id": null
      }
    ],
    "citation_count": 57
  },
  {
    "arxiv_id": "2306.07809",
    "title": "Low-Resource White-Box Semantic Segmentation of Supporting Towers on 3D Point Clouds via Signature Shape Identification",
    "year": 2023,
    "published": "2023-06-13T14:36:06Z",
    "categories": [
      "cs.CV",
      "cs.LG",
      "math.GT"
    ],
    "abstract": "Research in 3D semantic segmentation has been increasing performance metrics, like the IoU, by scaling model complexity and computational resources, leaving behind researchers and practitioners that (1) cannot access the necessary resources and (2) do need transparency on the model decision mechanisms. In this paper, we propose SCENE-Net, a low-resource white-box model for 3D point cloud semantic segmentation. SCENE-Net identifies signature shapes on the point cloud via group equivariant non-exp",
    "arxiv_url": "https://arxiv.org/abs/2306.07809v1",
    "pdf_url": "https://arxiv.org/pdf/2306.07809v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2306.07809",
    "arxiv_authors": [
      "Diogo Lavado",
      "Cl√°udia Soares",
      "Alessandra Micheletti",
      "Giovanni Bocchi",
      "Alex Coronati",
      "Manuel Silva",
      "Patrizio Frosini"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Low-Resource+White-Box+Semantic+Segmentation+of+Supporting+Towers+on+3D+Point+Clouds+via+Signature+Shape+Identification+Diogo+Lavado+Cl%C3%A1udia+Soares+Alessandra+Micheletti+Giovanni+Bocchi+Alex+Coronati",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "D Lavado",
        "id": null
      },
      {
        "name": "C Soares",
        "id": "Y9CArh4AAAAJ"
      },
      {
        "name": "A Micheletti",
        "id": "VyI_M_UAAAAJ"
      },
      {
        "name": "G Bocchi",
        "id": "Q0fZPM4AAAAJ"
      },
      {
        "name": "A Coronati",
        "id": null
      },
      {
        "name": "M Silva",
        "id": null
      },
      {
        "name": "P Frosini",
        "id": "xEayxzcAAAAJ"
      }
    ],
    "citation_count": 4
  },
  {
    "arxiv_id": "2403.03472",
    "title": "Boosting Meta-Training with Base Class Information for Few-Shot Learning",
    "year": 2024,
    "published": "2024-03-06T05:13:23Z",
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "abstract": "Few-shot learning, a challenging task in machine learning, aims to learn a classifier adaptable to recognize new, unseen classes with limited labeled examples. Meta-learning has emerged as a prominent framework for few-shot learning. Its training framework is originally a task-level learning method, such as Model-Agnostic Meta-Learning (MAML) and Prototypical Networks. And a recently proposed training paradigm called Meta-Baseline, which consists of sequential pre-training and meta-training stag",
    "arxiv_url": "https://arxiv.org/abs/2403.03472v1",
    "pdf_url": "https://arxiv.org/pdf/2403.03472v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2403.03472",
    "arxiv_authors": [
      "Weihao Jiang",
      "Guodong Liu",
      "Di He",
      "Kun He"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Boosting+Meta-Training+with+Base+Class+Information+for+Few-Shot+Learning+Weihao+Jiang+Guodong+Liu+Di+He+Kun+He",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "W Jiang",
        "id": null
      },
      {
        "name": "G Liu",
        "id": null
      },
      {
        "name": "D He",
        "id": null
      },
      {
        "name": "K He -",
        "id": null
      }
    ],
    "citation_count": 2
  },
  {
    "arxiv_id": "2409.00690",
    "title": "Decoupled and Interactive Regression Modeling for High-performance One-stage 3D Object Detection",
    "year": 2024,
    "published": "2024-09-01T10:47:22Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Inadequate bounding box modeling in regression tasks constrains the performance of one-stage 3D object detection. Our study reveals that the primary reason lies in two aspects: (1) The limited center-offset prediction seriously impairs the bounding box localization since many highest response positions significantly deviate from object centers. (2) The low-quality sample ignored in regression tasks significantly impacts the bounding box prediction since it produces unreliable quality (IoU) recti",
    "arxiv_url": "https://arxiv.org/abs/2409.00690v1",
    "pdf_url": "https://arxiv.org/pdf/2409.00690v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2409.00690",
    "arxiv_authors": [
      "Weiping Xiao",
      "Yiqiang Wu",
      "Chang Liu",
      "Yu Qin",
      "Xiaomao Li",
      "Liming Xin"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Decoupled+and+Interactive+Regression+Modeling+for+High-performance+One-stage+3D+Object+Detection+Weiping+Xiao+Yiqiang+Wu+Chang+Liu+Yu+Qin+Xiaomao+Li",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "W Xiao",
        "id": null
      },
      {
        "name": "Y Wu",
        "id": null
      },
      {
        "name": "C Liu",
        "id": null
      },
      {
        "name": "Y Qin",
        "id": "dAAzqHAAAAAJ"
      },
      {
        "name": "X Li",
        "id": "-_jUXnMAAAAJ"
      },
      {
        "name": "L Xin -",
        "id": null
      }
    ],
    "citation_count": 1
  },
  {
    "arxiv_id": "2308.00210",
    "title": "Scene Separation & Data Selection: Temporal Segmentation Algorithm for Real-Time Video Stream Analysis",
    "year": 2023,
    "published": "2023-08-01T00:53:05Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "We present 2SDS (Scene Separation and Data Selection algorithm), a temporal segmentation algorithm used in real-time video stream interpretation. It complements CNN-based models to make use of temporal information in videos. 2SDS can detect the change between scenes in a video stream by com-paring the image difference between two frames. It separates a video into segments (scenes), and by combining itself with a CNN model, 2SDS can select the optimal result for each scene. In this paper, we will",
    "arxiv_url": "https://arxiv.org/abs/2308.00210v1",
    "pdf_url": "https://arxiv.org/pdf/2308.00210v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2308.00210",
    "arxiv_authors": [
      "Yuelin Xin",
      "Zihan Zhou",
      "Yuxuan Xia"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Scene+Separation+%26+Data+Selection%3A+Temporal+Segmentation+Algorithm+for+Real-Time+Video+Stream+Analysis+Yuelin+Xin+Zihan+Zhou+Yuxuan+Xia",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "Y Xin",
        "id": "JTa0ZXYAAAAJ"
      },
      {
        "name": "Z Zhou",
        "id": "Xjd9fRcAAAAJ"
      },
      {
        "name": "Y Xia -",
        "id": null
      }
    ],
    "citation_count": 1
  },
  {
    "arxiv_id": "2304.03435",
    "title": "Towards Unified Scene Text Spotting based on Sequence Generation",
    "year": 2023,
    "published": "2023-04-07T01:28:08Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Sequence generation models have recently made significant progress in unifying various vision tasks. Although some auto-regressive models have demonstrated promising results in end-to-end text spotting, they use specific detection formats while ignoring various text shapes and are limited in the maximum number of text instances that can be detected. To overcome these limitations, we propose a UNIfied scene Text Spotter, called UNITS. Our model unifies various detection formats, including quadril",
    "arxiv_url": "https://arxiv.org/abs/2304.03435v1",
    "pdf_url": "https://arxiv.org/pdf/2304.03435v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2304.03435",
    "arxiv_authors": [
      "Taeho Kil",
      "Seonghyeon Kim",
      "Sukmin Seo",
      "Yoonsik Kim",
      "Daehee Kim"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Towards+Unified+Scene+Text+Spotting+based+on+Sequence+Generation+Taeho+Kil+Seonghyeon+Kim+Sukmin+Seo+Yoonsik+Kim+Daehee+Kim",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "T Kil",
        "id": "cV4h5MsAAAAJ"
      },
      {
        "name": "S Kim",
        "id": "tBaFIHYAAAAJ"
      },
      {
        "name": "S Seo",
        "id": null
      },
      {
        "name": "Y Kim",
        "id": "nuxd_BsAAAAJ"
      },
      {
        "name": "D Kim",
        "id": "CqVZ_jkAAAAJ"
      }
    ],
    "citation_count": 39
  },
  {
    "arxiv_id": "2404.10625",
    "title": "Gaussian Splatting Decoder for 3D-aware Generative Adversarial Networks",
    "year": 2024,
    "published": "2024-04-16T14:48:40Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "NeRF-based 3D-aware Generative Adversarial Networks (GANs) like EG3D or GIRAFFE have shown very high rendering quality under large representational variety. However, rendering with Neural Radiance Fields poses challenges for 3D applications: First, the significant computational demands of NeRF rendering preclude its use on low-power devices, such as mobiles and VR/AR headsets. Second, implicit representations based on neural networks are difficult to incorporate into explicit 3D scenes, such as ",
    "arxiv_url": "https://arxiv.org/abs/2404.10625v2",
    "pdf_url": "https://arxiv.org/pdf/2404.10625v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2404.10625",
    "arxiv_authors": [
      "Florian Barthel",
      "Arian Beckmann",
      "Wieland Morgenstern",
      "Anna Hilsmann",
      "Peter Eisert"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Gaussian+Splatting+Decoder+for+3D-aware+Generative+Adversarial+Networks+Florian+Barthel+Arian+Beckmann+Wieland+Morgenstern+Anna+Hilsmann+Peter+Eisert",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "F Barthel",
        "id": "KBxqk0MAAAAJ"
      },
      {
        "name": "A Beckmann",
        "id": "yDGgG2EAAAAJ"
      },
      {
        "name": "W Morgenstern",
        "id": "C5FY_wwAAAAJ"
      },
      {
        "name": "A Hilsmann",
        "id": "5yTuyGIAAAAJ"
      },
      {
        "name": "P Eisert",
        "id": "BCElyCkAAAAJ"
      }
    ],
    "citation_count": 14
  },
  {
    "arxiv_id": "2409.05558",
    "title": "Seeing Through the Mask: Rethinking Adversarial Examples for CAPTCHAs",
    "year": 2024,
    "published": "2024-09-09T12:29:53Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Modern CAPTCHAs rely heavily on vision tasks that are supposedly hard for computers but easy for humans. However, advances in image recognition models pose a significant threat to such CAPTCHAs. These models can easily be fooled by generating some well-hidden \"random\" noise and adding it to the image, or hiding objects in the image. However, these methods are model-specific and thus can not aid CAPTCHAs in fooling all models. We show in this work that by allowing for more significant changes to ",
    "arxiv_url": "https://arxiv.org/abs/2409.05558v1",
    "pdf_url": "https://arxiv.org/pdf/2409.05558v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2409.05558",
    "arxiv_authors": [
      "Yahya Jabary",
      "Andreas Plesner",
      "Turlan Kuzhagaliyev",
      "Roger Wattenhofer"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Seeing+Through+the+Mask%3A+Rethinking+Adversarial+Examples+for+CAPTCHAs+Yahya+Jabary+Andreas+Plesner+Turlan+Kuzhagaliyev+Roger+Wattenhofer",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "A Plesner",
        "id": "VyBLS4kAAAAJ"
      },
      {
        "name": "T Kuzhagaliyev",
        "id": null
      },
      {
        "name": "R Wattenhofer -",
        "id": null
      }
    ],
    "citation_count": 2
  },
  {
    "arxiv_id": "2305.05784",
    "title": "Comprehensive Dataset of Synthetic and Manipulated Overhead Imagery for Development and Evaluation of Forensic Tools",
    "year": 2023,
    "published": "2023-05-09T22:09:35Z",
    "categories": [
      "cs.CV",
      "cs.CR"
    ],
    "abstract": "We present a first of its kind dataset of overhead imagery for development and evaluation of forensic tools. Our dataset consists of real, fully synthetic and partially manipulated overhead imagery generated from a custom diffusion model trained on two sets of different zoom levels and on two sources of pristine data. We developed our model to support controllable generation of multiple manipulation categories including fully synthetic imagery conditioned on real and generated base maps, and loc",
    "arxiv_url": "https://arxiv.org/abs/2305.05784v1",
    "pdf_url": "https://arxiv.org/pdf/2305.05784v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2305.05784",
    "arxiv_authors": [
      "Brandon B. May",
      "Kirill Trapeznikov",
      "Shengbang Fang",
      "Matthew C. Stamm"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Comprehensive+Dataset+of+Synthetic+and+Manipulated+Overhead+Imagery+for+Development+and+Evaluation+of+Forensic+Tools+Brandon+B.+May+Kirill+Trapeznikov+Shengbang+Fang+Matthew+C.+Stamm",
    "gs_search_success": false,
    "gs_authors": null,
    "citation_count": null,
    "error_type": "fetch_failed"
  },
  {
    "arxiv_id": "2307.05899",
    "title": "DiffuseGAE: Controllable and High-fidelity Image Manipulation from Disentangled Representation",
    "year": 2023,
    "published": "2023-07-12T04:11:08Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Diffusion probabilistic models (DPMs) have shown remarkable results on various image synthesis tasks such as text-to-image generation and image inpainting. However, compared to other generative methods like VAEs and GANs, DPMs lack a low-dimensional, interpretable, and well-decoupled latent code. Recently, diffusion autoencoders (Diff-AE) were proposed to explore the potential of DPMs for representation learning via autoencoding. Diff-AE provides an accessible latent space that exhibits remarkab",
    "arxiv_url": "https://arxiv.org/abs/2307.05899v1",
    "pdf_url": "https://arxiv.org/pdf/2307.05899v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2307.05899",
    "arxiv_authors": [
      "Yipeng Leng",
      "Qiangjuan Huang",
      "Zhiyuan Wang",
      "Yangyang Liu",
      "Haoyu Zhang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=DiffuseGAE%3A+Controllable+and+High-fidelity+Image+Manipulation+from+Disentangled+Representation+Yipeng+Leng+Qiangjuan+Huang+Zhiyuan+Wang+Yangyang+Liu+Haoyu+Zhang",
    "gs_search_success": false,
    "gs_authors": null,
    "citation_count": null,
    "error_type": "fetch_failed"
  },
  {
    "arxiv_id": "2311.13793",
    "title": "Evidential Active Recognition: Intelligent and Prudent Open-World Embodied Perception",
    "year": 2023,
    "published": "2023-11-23T03:51:46Z",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "abstract": "Active recognition enables robots to intelligently explore novel observations, thereby acquiring more information while circumventing undesired viewing conditions. Recent approaches favor learning policies from simulated or collected data, wherein appropriate actions are more frequently selected when the recognition is accurate. However, most recognition modules are developed under the closed-world assumption, which makes them ill-equipped to handle unexpected inputs, such as the absence of the ",
    "arxiv_url": "https://arxiv.org/abs/2311.13793v1",
    "pdf_url": "https://arxiv.org/pdf/2311.13793v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2311.13793",
    "arxiv_authors": [
      "Lei Fan",
      "Mingfu Liang",
      "Yunxuan Li",
      "Gang Hua",
      "Ying Wu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Evidential+Active+Recognition%3A+Intelligent+and+Prudent+Open-World+Embodied+Perception+Lei+Fan+Mingfu+Liang+Yunxuan+Li+Gang+Hua+Ying+Wu",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "L Fan",
        "id": "OWMmwZQAAAAJ"
      },
      {
        "name": "M Liang",
        "id": "_uUUvt4AAAAJ"
      },
      {
        "name": "Y Li",
        "id": null
      },
      {
        "name": "G Hua",
        "id": "7SgUlggAAAAJ"
      },
      {
        "name": "Y Wu",
        "id": "zAlz89wAAAAJ"
      }
    ],
    "citation_count": 17
  },
  {
    "arxiv_id": "2305.14167",
    "title": "DetGPT: Detect What You Need via Reasoning",
    "year": 2023,
    "published": "2023-05-23T15:37:28Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "In recent years, the field of computer vision has seen significant advancements thanks to the development of large language models (LLMs). These models have enabled more effective and sophisticated interactions between humans and machines, paving the way for novel techniques that blur the lines between human and machine intelligence. In this paper, we introduce a new paradigm for object detection that we call reasoning-based object detection. Unlike conventional object detection methods that rel",
    "arxiv_url": "https://arxiv.org/abs/2305.14167v2",
    "pdf_url": "https://arxiv.org/pdf/2305.14167v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2305.14167",
    "arxiv_authors": [
      "Renjie Pi",
      "Jiahui Gao",
      "Shizhe Diao",
      "Rui Pan",
      "Hanze Dong",
      "Jipeng Zhang",
      "Lewei Yao",
      "Jianhua Han",
      "Hang Xu",
      "Lingpeng Kong",
      "Tong Zhang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=DetGPT%3A+Detect+What+You+Need+via+Reasoning+Renjie+Pi+Jiahui+Gao+Shizhe+Diao+Rui+Pan+Hanze+Dong",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "R Pi",
        "id": "XUq0HwcAAAAJ"
      },
      {
        "name": "J Gao",
        "id": "0LzbaZcAAAAJ"
      },
      {
        "name": "S Diao",
        "id": "NDFQrLQAAAAJ"
      },
      {
        "name": "R Pan",
        "id": "SBPkYz8AAAAJ"
      },
      {
        "name": "H Dong",
        "id": "g9WLzWoAAAAJ"
      },
      {
        "name": "J Zhang",
        "id": "q0De288AAAAJ"
      },
      {
        "name": "L Yao",
        "id": "hqDyTg8AAAAJ"
      },
      {
        "name": "J Han",
        "id": "OEPMQEMAAAAJ"
      },
      {
        "name": "H Xu",
        "id": "J_8TX6sAAAAJ"
      },
      {
        "name": "L Kong",
        "id": "f1hBi5wAAAAJ"
      },
      {
        "name": "T Zhang",
        "id": "LurWtuYAAAAJ"
      }
    ],
    "citation_count": 98
  },
  {
    "arxiv_id": "2503.07399",
    "title": "Exploring Representation Invariance in Finetuning",
    "year": 2025,
    "published": "2025-03-10T14:44:37Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Foundation models pretrained on large-scale natural images are widely adapted to various cross-domain low-resource downstream tasks, benefiting from generalizable and transferable patterns captured by their representations. However, these representations are later found to gradually vanish during finetuning, accompanied by a degradation of model's original generalizability. In this paper, we argue that such tasks can be effectively adapted without sacrificing the benefits of pretrained represent",
    "arxiv_url": "https://arxiv.org/abs/2503.07399v3",
    "pdf_url": "https://arxiv.org/pdf/2503.07399v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2503.07399",
    "arxiv_authors": [
      "Wenqiang Zu",
      "Shenghao Xie",
      "Hao Chen",
      "Zhiqiang Chen",
      "Liwen Hu",
      "Yuanhao Xi",
      "Yiming Liang",
      "Junliang Ye",
      "Bo Lei",
      "Tiejun Huang",
      "Guoqi Li",
      "Lei Ma"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Exploring+Representation+Invariance+in+Finetuning+Wenqiang+Zu+Shenghao+Xie+Hao+Chen+Zhiqiang+Chen+Liwen+Hu",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "X Liu",
        "id": null
      },
      {
        "name": "Y Li",
        "id": null
      },
      {
        "name": "Y Teng",
        "id": null
      },
      {
        "name": "H Bao",
        "id": null
      },
      {
        "name": "G Zhang",
        "id": null
      },
      {
        "name": "Y Zhang",
        "id": null
      },
      {
        "name": "Z Cui",
        "id": "vwIRwDUAAAAJ"
      }
    ],
    "citation_count": null
  },
  {
    "arxiv_id": "2401.10090",
    "title": "Cross-Modality Perturbation Synergy Attack for Person Re-identification",
    "year": 2024,
    "published": "2024-01-18T15:56:23Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "In recent years, there has been significant research focusing on addressing security concerns in single-modal person re-identification (ReID) systems that are based on RGB images. However, the safety of cross-modality scenarios, which are more commonly encountered in practical applications involving images captured by infrared cameras, has not received adequate attention. The main challenge in cross-modality ReID lies in effectively dealing with visual differences between different modalities. F",
    "arxiv_url": "https://arxiv.org/abs/2401.10090v6",
    "pdf_url": "https://arxiv.org/pdf/2401.10090v6",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2401.10090",
    "arxiv_authors": [
      "Yunpeng Gong",
      "Zhun Zhong",
      "Yansong Qu",
      "Zhiming Luo",
      "Rongrong Ji",
      "Min Jiang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Cross-Modality+Perturbation+Synergy+Attack+for+Person+Re-identification+Yunpeng+Gong+Zhun+Zhong+Yansong+Qu+Zhiming+Luo+Rongrong+Ji",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "Y Gong",
        "id": "raM-0cUAAAAJ"
      },
      {
        "name": "Z Zhong",
        "id": "nZizkQ0AAAAJ"
      },
      {
        "name": "Y Qu",
        "id": "zBLDzs4AAAAJ"
      },
      {
        "name": "Z Luo",
        "id": "RdRCIIYAAAAJ"
      },
      {
        "name": "R Ji",
        "id": "lRSD7PQAAAAJ"
      },
      {
        "name": "M JiangAdvances in Neural Information Processing Systems",
        "id": null
      }
    ],
    "citation_count": 51
  },
  {
    "arxiv_id": "2309.04109",
    "title": "From Text to Mask: Localizing Entities Using the Attention of Text-to-Image Diffusion Models",
    "year": 2023,
    "published": "2023-09-08T04:10:01Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Diffusion models have revolted the field of text-to-image generation recently. The unique way of fusing text and image information contributes to their remarkable capability of generating highly text-related images. From another perspective, these generative models imply clues about the precise correlation between words and pixels. In this work, a simple but effective method is proposed to utilize the attention mechanism in the denoising network of text-to-image diffusion models. Without re-trai",
    "arxiv_url": "https://arxiv.org/abs/2309.04109v2",
    "pdf_url": "https://arxiv.org/pdf/2309.04109v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2309.04109",
    "arxiv_authors": [
      "Changming Xiao",
      "Qi Yang",
      "Feng Zhou",
      "Changshui Zhang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=From+Text+to+Mask%3A+Localizing+Entities+Using+the+Attention+of+Text-to-Image+Diffusion+Models+Changming+Xiao+Qi+Yang+Feng+Zhou+Changshui+Zhang",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "C Xiao",
        "id": null
      },
      {
        "name": "Q Yang",
        "id": null
      },
      {
        "name": "F Zhou",
        "id": null
      }
    ],
    "citation_count": 26
  },
  {
    "arxiv_id": "2409.03228",
    "title": "Labeled-to-Unlabeled Distribution Alignment for Partially-Supervised Multi-Organ Medical Image Segmentation",
    "year": 2024,
    "published": "2024-09-05T03:55:37Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Partially-supervised multi-organ medical image segmentation aims to develop a unified semantic segmentation model by utilizing multiple partially-labeled datasets, with each dataset providing labels for a single class of organs. However, the limited availability of labeled foreground organs and the absence of supervision to distinguish unlabeled foreground organs from the background pose a significant challenge, which leads to a distribution mismatch between labeled and unlabeled pixels. Althoug",
    "arxiv_url": "https://arxiv.org/abs/2409.03228v1",
    "pdf_url": "https://arxiv.org/pdf/2409.03228v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2409.03228",
    "arxiv_authors": [
      "Xixi Jiang",
      "Dong Zhang",
      "Xiang Li",
      "Kangyi Liu",
      "Kwang-Ting Cheng",
      "Xin Yang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Labeled-to-Unlabeled+Distribution+Alignment+for+Partially-Supervised+Multi-Organ+Medical+Image+Segmentation+Xixi+Jiang+Dong+Zhang+Xiang+Li+Kangyi+Liu+Kwang-Ting+Cheng",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "X Jiang",
        "id": "x_-CP2cAAAAJ"
      },
      {
        "name": "D Zhang",
        "id": "zxVy7sIAAAAJ"
      },
      {
        "name": "X Li",
        "id": null
      },
      {
        "name": "K Liu",
        "id": null
      },
      {
        "name": "KT Cheng",
        "id": "-SgpaF8AAAAJ"
      },
      {
        "name": "X Yang - Medical Image Analysis",
        "id": null
      }
    ],
    "citation_count": 6
  },
  {
    "arxiv_id": "2401.06946",
    "title": "3D Object Detection and High-Resolution Traffic Parameters Extraction Using Low-Resolution LiDAR Data",
    "year": 2024,
    "published": "2024-01-13T01:22:20Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "abstract": "Traffic volume data collection is a crucial aspect of transportation engineering and urban planning, as it provides vital insights into traffic patterns, congestion, and infrastructure efficiency. Traditional manual methods of traffic data collection are both time-consuming and costly. However, the emergence of modern technologies, particularly Light Detection and Ranging (LiDAR), has revolutionized the process by enabling efficient and accurate data collection. Despite the benefits of using LiD",
    "arxiv_url": "https://arxiv.org/abs/2401.06946v1",
    "pdf_url": "https://arxiv.org/pdf/2401.06946v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2401.06946",
    "arxiv_authors": [
      "Linlin Zhang",
      "Xiang Yu",
      "Armstrong Aboah",
      "Yaw Adu-Gyamfi"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=3D+Object+Detection+and+High-Resolution+Traffic+Parameters+Extraction+Using+Low-Resolution+LiDAR+Data+Linlin+Zhang+Xiang+Yu+Armstrong+Aboah+Yaw+Adu-Gyamfi",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "L Zhang",
        "id": "FdYBGkkAAAAJ"
      },
      {
        "name": "X Yu",
        "id": "xK84FR4AAAAJ"
      },
      {
        "name": "A Aboah",
        "id": "Ev1PAAwAAAAJ"
      },
      {
        "name": "Y Adu-Gyamfi -",
        "id": null
      }
    ],
    "citation_count": 6
  },
  {
    "arxiv_id": "2303.09158",
    "title": "Facial Affect Recognition based on Transformer Encoder and Audiovisual Fusion for the ABAW5 Challenge",
    "year": 2023,
    "published": "2023-03-16T08:47:36Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "In this paper, we present our solutions for the 5th Workshop and Competition on Affective Behavior Analysis in-the-wild (ABAW), which includes four sub-challenges of Valence-Arousal (VA) Estimation, Expression (Expr) Classification, Action Unit (AU) Detection and Emotional Reaction Intensity (ERI) Estimation. The 5th ABAW competition focuses on facial affect recognition utilizing different modalities and datasets. In our work, we extract powerful audio and visual features using a large number of",
    "arxiv_url": "https://arxiv.org/abs/2303.09158v2",
    "pdf_url": "https://arxiv.org/pdf/2303.09158v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2303.09158",
    "arxiv_authors": [
      "Ziyang Zhang",
      "Liuwei An",
      "Zishun Cui",
      "Ao xu",
      "Tengteng Dong",
      "Yueqi Jiang",
      "Jingyi Shi",
      "Xin Liu",
      "Xiao Sun",
      "Meng Wang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Facial+Affect+Recognition+based+on+Transformer+Encoder+and+Audiovisual+Fusion+for+the+ABAW5+Challenge+Ziyang+Zhang+Liuwei+An+Zishun+Cui+Ao+xu+Tengteng+Dong",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "Z Zhang",
        "id": "E3S1nSsAAAAJ"
      },
      {
        "name": "L An",
        "id": null
      },
      {
        "name": "Z Cui",
        "id": null
      },
      {
        "name": "T Dong",
        "id": null
      },
      {
        "name": "Y Jiang",
        "id": null
      },
      {
        "name": "J Shi",
        "id": null
      },
      {
        "name": "X Liu",
        "id": null
      },
      {
        "name": "X Sun",
        "id": null
      },
      {
        "name": "M Wang",
        "id": "rHagaaIAAAAJ"
      }
    ],
    "citation_count": 25
  },
  {
    "arxiv_id": "2312.10611",
    "title": "Bi-directional Adapter for Multi-modal Tracking",
    "year": 2023,
    "published": "2023-12-17T05:27:31Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Due to the rapid development of computer vision, single-modal (RGB) object tracking has made significant progress in recent years. Considering the limitation of single imaging sensor, multi-modal images (RGB, Infrared, etc.) are introduced to compensate for this deficiency for all-weather object tracking in complex environments. However, as acquiring sufficient multi-modal tracking data is hard while the dominant modality changes with the open environment, most existing techniques fail to extrac",
    "arxiv_url": "https://arxiv.org/abs/2312.10611v1",
    "pdf_url": "https://arxiv.org/pdf/2312.10611v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2312.10611",
    "arxiv_authors": [
      "Bing Cao",
      "Junliang Guo",
      "Pengfei Zhu",
      "Qinghua Hu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Bi-directional+Adapter+for+Multi-modal+Tracking+Bing+Cao+Junliang+Guo+Pengfei+Zhu+Qinghua+Hu",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "B Cao",
        "id": "6KeTXm4AAAAJ"
      },
      {
        "name": "J Guo",
        "id": null
      },
      {
        "name": "P Zhu",
        "id": "iS27HZ8AAAAJ"
      },
      {
        "name": "Q Hu -",
        "id": null
      }
    ],
    "citation_count": 135
  },
  {
    "arxiv_id": "2406.11105",
    "title": "Exploiting Diffusion Prior for Out-of-Distribution Detection",
    "year": 2024,
    "published": "2024-06-16T23:55:25Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Out-of-distribution (OOD) detection is crucial for deploying robust machine learning models, especially in areas where security is critical. However, traditional OOD detection methods often fail to capture complex data distributions from large scale date. In this paper, we present a novel approach for OOD detection that leverages the generative ability of diffusion models and the powerful feature extraction capabilities of CLIP. By using these features as conditional inputs to a diffusion model,",
    "arxiv_url": "https://arxiv.org/abs/2406.11105v2",
    "pdf_url": "https://arxiv.org/pdf/2406.11105v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2406.11105",
    "arxiv_authors": [
      "Armando Zhu",
      "Jiabei Liu",
      "Keqin Li",
      "Shuying Dai",
      "Bo Hong",
      "Peng Zhao",
      "Changsong Wei"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Exploiting+Diffusion+Prior+for+Out-of-Distribution+Detection+Armando+Zhu+Jiabei+Liu+Keqin+Li+Shuying+Dai+Bo+Hong",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "A Zhu",
        "id": "EvQIiAwAAAAJ"
      },
      {
        "name": "J Liu",
        "id": null
      },
      {
        "name": "K Li",
        "id": "YhixGW0AAAAJ"
      },
      {
        "name": "S Dai",
        "id": null
      },
      {
        "name": "B Hong",
        "id": null
      },
      {
        "name": "P Zhao",
        "id": null
      },
      {
        "name": "C Wei",
        "id": null
      }
    ],
    "citation_count": 28
  },
  {
    "arxiv_id": "2501.19061",
    "title": "EgoMe: A New Dataset and Challenge for Following Me via Egocentric View in Real World",
    "year": 2025,
    "published": "2025-01-31T11:48:22Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "In human imitation learning, the imitator typically take the egocentric view as a benchmark, naturally transferring behaviors observed from an exocentric view to their owns, which provides inspiration for researching how robots can more effectively imitate human behavior. However, current research primarily focuses on the basic alignment issues of ego-exo data from different cameras, rather than collecting data from the imitator's perspective, which is inconsistent with the high-level cognitive ",
    "arxiv_url": "https://arxiv.org/abs/2501.19061v2",
    "pdf_url": "https://arxiv.org/pdf/2501.19061v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2501.19061",
    "arxiv_authors": [
      "Heqian Qiu",
      "Zhaofeng Shi",
      "Lanxiao Wang",
      "Huiyu Xiong",
      "Xiang Li",
      "Hongliang Li"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=EgoMe%3A+A+New+Dataset+and+Challenge+for+Following+Me+via+Egocentric+View+in+Real+World+Heqian+Qiu+Zhaofeng+Shi+Lanxiao+Wang+Huiyu+Xiong+Xiang+Li",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "H Qiu",
        "id": "dJx4SDEAAAAJ"
      },
      {
        "name": "Z Shi",
        "id": "JGj7iaYAAAAJ"
      },
      {
        "name": "L Wang",
        "id": "YKSiBwIAAAAJ"
      },
      {
        "name": "H Xiong",
        "id": null
      },
      {
        "name": "X Li",
        "id": "ZjFKo_wAAAAJ"
      },
      {
        "name": "H Li -",
        "id": null
      }
    ],
    "citation_count": null
  },
  {
    "arxiv_id": "2502.01690",
    "title": "HuViDPO:Enhancing Video Generation through Direct Preference Optimization for Human-Centric Alignment",
    "year": 2025,
    "published": "2025-02-02T16:55:42Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "With the rapid development of AIGC technology, significant progress has been made in diffusion model-based technologies for text-to-image (T2I) and text-to-video (T2V). In recent years, a few studies have introduced the strategy of Direct Preference Optimization (DPO) into T2I tasks, significantly enhancing human preferences in generated images. However, existing T2V generation methods lack a well-formed pipeline with exact loss function to guide the alignment of generated videos with human pref",
    "arxiv_url": "https://arxiv.org/abs/2502.01690v1",
    "pdf_url": "https://arxiv.org/pdf/2502.01690v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2502.01690",
    "arxiv_authors": [
      "Lifan Jiang",
      "Boxi Wu",
      "Jiahui Zhang",
      "Xiaotong Guan",
      "Shuang Chen"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=HuViDPO%3AEnhancing+Video+Generation+through+Direct+Preference+Optimization+for+Human-Centric+Alignment+Lifan+Jiang+Boxi+Wu+Jiahui+Zhang+Xiaotong+Guan+Shuang+Chen",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "L Jiang",
        "id": "CHuMUyEAAAAJ"
      },
      {
        "name": "B Wu",
        "id": "AqDe35sAAAAJ"
      },
      {
        "name": "J Zhang",
        "id": null
      },
      {
        "name": "X Guan",
        "id": null
      },
      {
        "name": "S Chen -",
        "id": null
      }
    ],
    "citation_count": 4
  },
  {
    "arxiv_id": "2503.16251",
    "title": "RESFL: An Uncertainty-Aware Framework for Responsible Federated Learning by Balancing Privacy, Fairness and Utility in Autonomous Vehicles",
    "year": 2025,
    "published": "2025-03-20T15:46:03Z",
    "categories": [
      "cs.LG",
      "cs.CV",
      "cs.DC",
      "cs.ET"
    ],
    "abstract": "Autonomous vehicles (AVs) increasingly rely on Federated Learning (FL) to enhance perception models while preserving privacy. However, existing FL frameworks struggle to balance privacy, fairness, and robustness, leading to performance disparities across demographic groups. Privacy-preserving techniques like differential privacy mitigate data leakage risks but worsen fairness by restricting access to sensitive attributes needed for bias correction. This work explores the trade-off between privac",
    "arxiv_url": "https://arxiv.org/abs/2503.16251v1",
    "pdf_url": "https://arxiv.org/pdf/2503.16251v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2503.16251",
    "arxiv_authors": [
      "Dawood Wasif",
      "Terrence J. Moore",
      "Jin-Hee Cho"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=RESFL%3A+An+Uncertainty-Aware+Framework+for+Responsible+Federated+Learning+by+Balancing+Privacy%2C+Fairness+and+Utility+in+Autonomous+Vehicles+Dawood+Wasif+Terrence+J.+Moore+Jin-Hee+Cho",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "D Wasif",
        "id": "IRzw11MAAAAJ"
      },
      {
        "name": "TJ Moore",
        "id": "mTH2O1UAAAAJ"
      },
      {
        "name": "JH Cho -",
        "id": null
      }
    ],
    "citation_count": 1
  },
  {
    "arxiv_id": "2408.10581",
    "title": "Multi-view Hand Reconstruction with a Point-Embedded Transformer",
    "year": 2024,
    "published": "2024-08-20T06:42:17Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "This work introduces a novel and generalizable multi-view Hand Mesh Reconstruction (HMR) model, named POEM, designed for practical use in real-world hand motion capture scenarios. The advances of the POEM model consist of two main aspects. First, concerning the modeling of the problem, we propose embedding a static basis point within the multi-view stereo space. A point represents a natural form of 3D information and serves as an ideal medium for fusing features across different views, given its",
    "arxiv_url": "https://arxiv.org/abs/2408.10581v2",
    "pdf_url": "https://arxiv.org/pdf/2408.10581v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2408.10581",
    "arxiv_authors": [
      "Lixin Yang",
      "Licheng Zhong",
      "Pengxiang Zhu",
      "Xinyu Zhan",
      "Junxiao Kong",
      "Jian Xu",
      "Cewu Lu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Multi-view+Hand+Reconstruction+with+a+Point-Embedded+Transformer+Lixin+Yang+Licheng+Zhong+Pengxiang+Zhu+Xinyu+Zhan+Junxiao+Kong",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "L Yang",
        "id": "Bm8p4JsAAAAJ"
      },
      {
        "name": "L Zhong",
        "id": "ixobBrEAAAAJ"
      },
      {
        "name": "P Zhu",
        "id": "4IQDolIAAAAJ"
      },
      {
        "name": "X Zhan",
        "id": "WurpqEMAAAAJ"
      },
      {
        "name": "J Kong",
        "id": "9FuZrPYAAAAJ"
      },
      {
        "name": "J Xu",
        "id": "hqyYoO0AAAAJ"
      },
      {
        "name": "C LuIEEE Transactions on Pattern Analysis and Machine Intelligence",
        "id": null
      }
    ],
    "citation_count": 2
  },
  {
    "arxiv_id": "2309.00817",
    "title": "Soil Image Segmentation Based on Mask R-CNN",
    "year": 2023,
    "published": "2023-09-02T04:08:06Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "The complex background in the soil image collected in the field natural environment will affect the subsequent soil image recognition based on machine vision. Segmenting the soil center area from the soil image can eliminate the influence of the complex background, which is an important preprocessing work for subsequent soil image recognition. For the first time, the deep learning method was applied to soil image segmentation, and the Mask R-CNN model was selected to complete the positioning and",
    "arxiv_url": "https://arxiv.org/abs/2309.00817v1",
    "pdf_url": "https://arxiv.org/pdf/2309.00817v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2309.00817",
    "arxiv_authors": [
      "Yida Chen",
      "Kang Liu",
      "Yi Xin",
      "Xinru Zhao"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Soil+Image+Segmentation+Based+on+Mask+R-CNN+Yida+Chen+Kang+Liu+Yi+Xin+Xinru+Zhao",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "Y Chen",
        "id": "ggBlW70AAAAJ"
      },
      {
        "name": "K Liu",
        "id": null
      },
      {
        "name": "Y Xin",
        "id": null
      },
      {
        "name": "X Zhao -",
        "id": null
      }
    ],
    "citation_count": 7
  },
  {
    "arxiv_id": "2306.11290",
    "title": "Habitat Synthetic Scenes Dataset (HSSD-200): An Analysis of 3D Scene Scale and Realism Tradeoffs for ObjectGoal Navigation",
    "year": 2023,
    "published": "2023-06-20T05:07:23Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "We contribute the Habitat Synthetic Scene Dataset, a dataset of 211 high-quality 3D scenes, and use it to test navigation agent generalization to realistic 3D environments. Our dataset represents real interiors and contains a diverse set of 18,656 models of real-world objects. We investigate the impact of synthetic 3D scene dataset scale and realism on the task of training embodied agents to find and navigate to objects (ObjectGoal navigation). By comparing to synthetic 3D scene datasets from pr",
    "arxiv_url": "https://arxiv.org/abs/2306.11290v3",
    "pdf_url": "https://arxiv.org/pdf/2306.11290v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2306.11290",
    "arxiv_authors": [
      "Mukul Khanna",
      "Yongsen Mao",
      "Hanxiao Jiang",
      "Sanjay Haresh",
      "Brennan Shacklett",
      "Dhruv Batra",
      "Alexander Clegg",
      "Eric Undersander",
      "Angel X. Chang",
      "Manolis Savva"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Habitat+Synthetic+Scenes+Dataset+%28HSSD-200%29%3A+An+Analysis+of+3D+Scene+Scale+and+Realism+Tradeoffs+for+ObjectGoal+Navigation+Mukul+Khanna+Yongsen+Mao+Hanxiao+Jiang+Sanjay+Haresh+Brennan+Shacklett",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "M Khanna",
        "id": "kWAlOAkAAAAJ"
      },
      {
        "name": "Y Mao",
        "id": "bm9JqwMAAAAJ"
      },
      {
        "name": "H Jiang",
        "id": "-XWZKZAAAAAJ"
      },
      {
        "name": "S Haresh",
        "id": "boFO-7gAAAAJ"
      },
      {
        "name": "B Shacklett",
        "id": null
      },
      {
        "name": "D Batra",
        "id": "_bs7PqgAAAAJ"
      },
      {
        "name": "A Clegg",
        "id": "p463opcAAAAJ"
      },
      {
        "name": "E Undersander",
        "id": null
      }
    ],
    "citation_count": 99
  },
  {
    "arxiv_id": "2402.10021",
    "title": "SAWEC: Sensing-Assisted Wireless Edge Computing",
    "year": 2024,
    "published": "2024-02-15T15:39:46Z",
    "categories": [
      "cs.CV",
      "cs.NI"
    ],
    "abstract": "Emerging mobile virtual reality (VR) systems will require to continuously perform complex computer vision tasks on ultra-high-resolution video frames through the execution of deep neural networks (DNNs)-based algorithms. Since state-of-the-art DNNs require computational power that is excessive for mobile devices, techniques based on wireless edge computing (WEC) have been recently proposed. However, existing WEC methods require the transmission and processing of a high amount of video data which",
    "arxiv_url": "https://arxiv.org/abs/2402.10021v2",
    "pdf_url": "https://arxiv.org/pdf/2402.10021v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2402.10021",
    "arxiv_authors": [
      "Khandaker Foysal Haque",
      "Francesca Meneghello",
      "Md. Ebtidaul Karim",
      "Francesco Restuccia"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=SAWEC%3A+Sensing-Assisted+Wireless+Edge+Computing+Khandaker+Foysal+Haque+Francesca+Meneghello+Md.+Ebtidaul+Karim+Francesco+Restuccia",
    "gs_search_success": false,
    "gs_authors": null,
    "citation_count": null,
    "error_type": "multiple_matches"
  },
  {
    "arxiv_id": "2412.19439",
    "title": "Paleoinspired Vision: From Exploring Colour Vision Evolution to Inspiring Camera Design",
    "year": 2024,
    "published": "2024-12-27T04:07:52Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "The evolution of colour vision is captivating, as it reveals the adaptive strategies of extinct species while simultaneously inspiring innovations in modern imaging technology. In this study, we present a simplified model of visual transduction in the retina, introducing a novel opsin layer. We quantify evolutionary pressures by measuring machine vision recognition accuracy on colour images shaped by specific opsins. Building on this, we develop an evolutionary conservation optimisation algorith",
    "arxiv_url": "https://arxiv.org/abs/2412.19439v1",
    "pdf_url": "https://arxiv.org/pdf/2412.19439v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2412.19439",
    "arxiv_authors": [
      "Junjie Zhang",
      "Zhimin Zong",
      "Lin Gu",
      "Shenghan Su",
      "Ziteng Cui",
      "Yan Pu",
      "Zirui Chen",
      "Jing Lu",
      "Daisuke Kojima",
      "Tatsuya Harada",
      "Ruogu Fang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Paleoinspired+Vision%3A+From+Exploring+Colour+Vision+Evolution+to+Inspiring+Camera+Design+Junjie+Zhang+Zhimin+Zong+Lin+Gu+Shenghan+Su+Ziteng+Cui",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "J Zhang",
        "id": "ZL6AVTQAAAAJ"
      },
      {
        "name": "Z Zong",
        "id": null
      },
      {
        "name": "L Gu",
        "id": "gIEZe5IAAAAJ"
      },
      {
        "name": "S Su",
        "id": "i5WfBowAAAAJ"
      },
      {
        "name": "Z Cui",
        "id": "niXIRXgAAAAJ"
      },
      {
        "name": "Y Pu",
        "id": null
      },
      {
        "name": "Z Chen",
        "id": null
      },
      {
        "name": "J Lu",
        "id": null
      },
      {
        "name": "D Kojima",
        "id": "AIrGqG0AAAAJ"
      },
      {
        "name": "T Harada",
        "id": "k8rlJ8AAAAAJ"
      },
      {
        "name": "R Fang",
        "id": "LVb46zEAAAAJ"
      }
    ],
    "citation_count": null
  },
  {
    "arxiv_id": "2312.12222",
    "title": "EarthVQA: Towards Queryable Earth via Relational Reasoning-Based Remote Sensing Visual Question Answering",
    "year": 2023,
    "published": "2023-12-19T15:11:32Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Earth vision research typically focuses on extracting geospatial object locations and categories but neglects the exploration of relations between objects and comprehensive reasoning. Based on city planning needs, we develop a multi-modal multi-task VQA dataset (EarthVQA) to advance relational reasoning-based judging, counting, and comprehensive analysis. The EarthVQA dataset contains 6000 images, corresponding semantic masks, and 208,593 QA pairs with urban and rural governance requirements emb",
    "arxiv_url": "https://arxiv.org/abs/2312.12222v1",
    "pdf_url": "https://arxiv.org/pdf/2312.12222v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2312.12222",
    "arxiv_authors": [
      "Junjue Wang",
      "Zhuo Zheng",
      "Zihang Chen",
      "Ailong Ma",
      "Yanfei Zhong"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=EarthVQA%3A+Towards+Queryable+Earth+via+Relational+Reasoning-Based+Remote+Sensing+Visual+Question+Answering+Junjue+Wang+Zhuo+Zheng+Zihang+Chen+Ailong+Ma+Yanfei+Zhong",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "J Wang",
        "id": "H58gKSAAAAAJ"
      },
      {
        "name": "Z Zheng",
        "id": "CREpn_AAAAAJ"
      },
      {
        "name": "Z Chen",
        "id": "OiDZbZAAAAAJ"
      },
      {
        "name": "A Ma",
        "id": "GPjZ_2gAAAAJ"
      },
      {
        "name": "Y Zhong -",
        "id": null
      }
    ],
    "citation_count": 50
  },
  {
    "arxiv_id": "2409.01216",
    "title": "ESP-PCT: Enhanced VR Semantic Performance through Efficient Compression of Temporal and Spatial Redundancies in Point Cloud Transformers",
    "year": 2024,
    "published": "2024-09-02T12:48:40Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Semantic recognition is pivotal in virtual reality (VR) applications, enabling immersive and interactive experiences. A promising approach is utilizing millimeter-wave (mmWave) signals to generate point clouds. However, the high computational and memory demands of current mmWave point cloud models hinder their efficiency and reliability. To address this limitation, our paper introduces ESP-PCT, a novel Enhanced Semantic Performance Point Cloud Transformer with a two-stage semantic recognition fr",
    "arxiv_url": "https://arxiv.org/abs/2409.01216v1",
    "pdf_url": "https://arxiv.org/pdf/2409.01216v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2409.01216",
    "arxiv_authors": [
      "Luoyu Mei",
      "Shuai Wang",
      "Yun Cheng",
      "Ruofeng Liu",
      "Zhimeng Yin",
      "Wenchao Jiang",
      "Shuai Wang",
      "Wei Gong"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=ESP-PCT%3A+Enhanced+VR+Semantic+Performance+through+Efficient+Compression+of+Temporal+and+Spatial+Redundancies+in+Point+Cloud+Transformers+Luoyu+Mei+Shuai+Wang+Yun+Cheng+Ruofeng+Liu+Zhimeng+Yin",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "L Mei",
        "id": "nOMD2YIAAAAJ"
      },
      {
        "name": "S Wang",
        "id": "gfDfZqAAAAAJ"
      },
      {
        "name": "Y Cheng",
        "id": null
      },
      {
        "name": "R Liu",
        "id": "adggGSQAAAAJ"
      },
      {
        "name": "Z Yin",
        "id": "T89V0RAAAAAJ"
      },
      {
        "name": "W Jiang",
        "id": "bve5zI0AAAAJ"
      },
      {
        "name": "W Gong",
        "id": "CtbzNl8AAAAJ"
      }
    ],
    "citation_count": 9
  },
  {
    "arxiv_id": "2302.12495",
    "title": "Data fusion of satellite imagery for generation of daily cloud free images at high resolution level",
    "year": 2023,
    "published": "2023-02-24T07:29:13Z",
    "categories": [
      "math.OC",
      "cs.CV"
    ],
    "abstract": "In this paper we discuss a new variational approach to the Date Fusion problem of multi-spectral satellite images from Sentinel-2 and MODIS that have been captured at different resolution level and, arguably, on different days. The crucial point of our approach that the MODIS image is cloud-free whereas the images from Sentinel-2 can be corrupted by clouds or noise.",
    "arxiv_url": "https://arxiv.org/abs/2302.12495v1",
    "pdf_url": "https://arxiv.org/pdf/2302.12495v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2302.12495",
    "arxiv_authors": [
      "Natalya Ivanchuk",
      "Peter Kogut",
      "Petro Martyniuk"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Data+fusion+of+satellite+imagery+for+generation+of+daily+cloud+free+images+at+high+resolution+level+Natalya+Ivanchuk+Peter+Kogut+Petro+Martyniuk",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "N Ivanchuk",
        "id": null
      },
      {
        "name": "P Kogut",
        "id": "3xe-tKYAAAAJ"
      },
      {
        "name": "P Martyniuk -",
        "id": null
      }
    ],
    "citation_count": 4
  },
  {
    "arxiv_id": "2407.19108",
    "title": "ObjectCarver: Semi-automatic segmentation, reconstruction and separation of 3D objects",
    "year": 2024,
    "published": "2024-07-26T22:13:20Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Implicit neural fields have made remarkable progress in reconstructing 3D surfaces from multiple images; however, they encounter challenges when it comes to separating individual objects within a scene. Previous work has attempted to tackle this problem by introducing a framework to train separate signed distance fields (SDFs) simultaneously for each of N objects and using a regularization term to prevent objects from overlapping. However, all of these methods require segmentation masks to be pr",
    "arxiv_url": "https://arxiv.org/abs/2407.19108v1",
    "pdf_url": "https://arxiv.org/pdf/2407.19108v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2407.19108",
    "arxiv_authors": [
      "Gemmechu Hassena",
      "Jonathan Moon",
      "Ryan Fujii",
      "Andrew Yuen",
      "Noah Snavely",
      "Steve Marschner",
      "Bharath Hariharan"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=ObjectCarver%3A+Semi-automatic+segmentation%2C+reconstruction+and+separation+of+3D+objects+Gemmechu+Hassena+Jonathan+Moon+Ryan+Fujii+Andrew+Yuen+Noah+Snavely",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "G Hassena",
        "id": "n_RQk6EAAAAJ"
      },
      {
        "name": "J Moon",
        "id": null
      },
      {
        "name": "R Fujii",
        "id": null
      },
      {
        "name": "A Yuen",
        "id": null
      },
      {
        "name": "N Snavely",
        "id": "Db4BCX8AAAAJ"
      },
      {
        "name": "S Marschner",
        "id": "llo3F3QAAAAJ"
      },
      {
        "name": "B Hariharan2025 International",
        "id": null
      }
    ],
    "citation_count": 1
  },
  {
    "arxiv_id": "2411.13620",
    "title": "Robust SG-NeRF: Robust Scene Graph Aided Neural Surface Reconstruction",
    "year": 2024,
    "published": "2024-11-20T09:48:06Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Neural surface reconstruction relies heavily on accurate camera poses as input. Despite utilizing advanced pose estimators like COLMAP or ARKit, camera poses can still be noisy. Existing pose-NeRF joint optimization methods handle poses with small noise (inliers) effectively but struggle with large noise (outliers), such as mirrored poses. In this work, we focus on mitigating the impact of outlier poses. Our method integrates an inlier-outlier confidence estimation scheme, leveraging scene graph",
    "arxiv_url": "https://arxiv.org/abs/2411.13620v1",
    "pdf_url": "https://arxiv.org/pdf/2411.13620v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2411.13620",
    "arxiv_authors": [
      "Yi Gu",
      "Dongjun Ye",
      "Zhaorui Wang",
      "Jiaxu Wang",
      "Jiahang Cao",
      "Renjing Xu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Robust+SG-NeRF%3A+Robust+Scene+Graph+Aided+Neural+Surface+Reconstruction+Yi+Gu+Dongjun+Ye+Zhaorui+Wang+Jiaxu+Wang+Jiahang+Cao",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "Y Gu",
        "id": "CPakAgoAAAAJ"
      },
      {
        "name": "D Ye",
        "id": null
      },
      {
        "name": "Z Wang",
        "id": "eY4-UVsAAAAJ"
      },
      {
        "name": "J Wang",
        "id": "DjQT_G0AAAAJ"
      },
      {
        "name": "J Cao",
        "id": "RTy-jyAAAAAJ"
      },
      {
        "name": "R Xu -",
        "id": null
      }
    ],
    "citation_count": null
  },
  {
    "arxiv_id": "2306.00427",
    "title": "Out-of-distribution forgetting: vulnerability of continual learning to intra-class distribution shift",
    "year": 2023,
    "published": "2023-06-01T08:07:58Z",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "abstract": "Continual learning (CL) is an important technique to allow artificial neural networks to work in open environments. CL enables a system to learn new tasks without severe interference to its performance on old tasks, i.e., overcome the problems of catastrophic forgetting. In joint learning, it is well known that the out-of-distribution (OOD) problem caused by intentional attacks or environmental perturbations will severely impair the ability of networks to generalize. In this work, we reported a ",
    "arxiv_url": "https://arxiv.org/abs/2306.00427v2",
    "pdf_url": "https://arxiv.org/pdf/2306.00427v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2306.00427",
    "arxiv_authors": [
      "Liangxuan Guo",
      "Yang Chen",
      "Shan Yu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Out-of-distribution+forgetting%3A+vulnerability+of+continual+learning+to+intra-class+distribution+shift+Liangxuan+Guo+Yang+Chen+Shan+Yu",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "L Guo",
        "id": "mgUhgvAAAAAJ"
      },
      {
        "name": "Y Chen",
        "id": null
      },
      {
        "name": "S Yu - International",
        "id": null
      }
    ],
    "citation_count": 1
  },
  {
    "arxiv_id": "2506.00467",
    "title": "SST: Self-training with Self-adaptive Thresholding for Semi-supervised Learning",
    "year": 2025,
    "published": "2025-05-31T08:34:04Z",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "abstract": "Neural networks have demonstrated exceptional performance in supervised learning, benefiting from abundant high-quality annotated data. However, obtaining such data in real-world scenarios is costly and labor-intensive. Semi-supervised learning (SSL) offers a solution to this problem. Recent studies, such as Semi-ViT and Noisy Student, which employ consistency regularization or pseudo-labeling, have demonstrated significant achievements. However, they still face challenges, particularly in accur",
    "arxiv_url": "https://arxiv.org/abs/2506.00467v1",
    "pdf_url": "https://arxiv.org/pdf/2506.00467v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2506.00467",
    "arxiv_authors": [
      "Shuai Zhao",
      "Heyan Huang",
      "Xinge Li",
      "Xiaokang Chen",
      "Rui Wang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=SST%3A+Self-training+with+Self-adaptive+Thresholding+for+Semi-supervised+Learning+Shuai+Zhao+Heyan+Huang+Xinge+Li+Xiaokang+Chen+Rui+Wang",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "S Zhao",
        "id": "1uEaBV4AAAAJ"
      },
      {
        "name": "H Huang",
        "id": null
      },
      {
        "name": "X Li",
        "id": null
      },
      {
        "name": "X Chen",
        "id": null
      },
      {
        "name": "R Wang - Information Processing &",
        "id": null
      }
    ],
    "citation_count": 3
  },
  {
    "arxiv_id": "2301.09637",
    "title": "InfiniCity: Infinite-Scale City Synthesis",
    "year": 2023,
    "published": "2023-01-23T18:59:59Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR",
      "cs.LG"
    ],
    "abstract": "Toward infinite-scale 3D city synthesis, we propose a novel framework, InfiniCity, which constructs and renders an unconstrainedly large and 3D-grounded environment from random noises. InfiniCity decomposes the seemingly impractical task into three feasible modules, taking advantage of both 2D and 3D data. First, an infinite-pixel image synthesis module generates arbitrary-scale 2D maps from the bird's-eye view. Next, an octree-based voxel completion module lifts the generated 2D map to 3D octre",
    "arxiv_url": "https://arxiv.org/abs/2301.09637v2",
    "pdf_url": "https://arxiv.org/pdf/2301.09637v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2301.09637",
    "arxiv_authors": [
      "Chieh Hubert Lin",
      "Hsin-Ying Lee",
      "Willi Menapace",
      "Menglei Chai",
      "Aliaksandr Siarohin",
      "Ming-Hsuan Yang",
      "Sergey Tulyakov"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=InfiniCity%3A+Infinite-Scale+City+Synthesis+Chieh+Hubert+Lin+Hsin-Ying+Lee+Willi+Menapace+Menglei+Chai+Aliaksandr+Siarohin",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "CH Lin",
        "id": null
      },
      {
        "name": "HY Lee",
        "id": "SeozinYAAAAJ"
      },
      {
        "name": "W Menapace",
        "id": "31ha1LgAAAAJ"
      },
      {
        "name": "M Chai",
        "id": "6Lnb1Z4AAAAJ"
      },
      {
        "name": "A Siarohin",
        "id": "uMl5-k4AAAAJ"
      },
      {
        "name": "MH Yang",
        "id": "p9-ohHsAAAAJ"
      },
      {
        "name": "S Tulyakov",
        "id": "mgzXR0sAAAAJ"
      }
    ],
    "citation_count": 91
  },
  {
    "arxiv_id": "2301.03829",
    "title": "From Plate to Prevention: A Dietary Nutrient-aided Platform for Health Promotion in Singapore",
    "year": 2023,
    "published": "2023-01-10T07:51:36Z",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "cs.DB",
      "cs.MM"
    ],
    "abstract": "Singapore has been striving to improve the provision of healthcare services to her people. In this course, the government has taken note of the deficiency in regulating and supervising people's nutrient intake, which is identified as a contributing factor to the development of chronic diseases. Consequently, this issue has garnered significant attention. In this paper, we share our experience in addressing this issue and attaining medical-grade nutrient intake information to benefit Singaporeans",
    "arxiv_url": "https://arxiv.org/abs/2301.03829v2",
    "pdf_url": "https://arxiv.org/pdf/2301.03829v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2301.03829",
    "arxiv_authors": [
      "Kaiping Zheng",
      "Thao Nguyen",
      "Jesslyn Hwei Sing Chong",
      "Charlene Enhui Goh",
      "Melanie Herschel",
      "Hee Hoon Lee",
      "Changshuo Liu",
      "Beng Chin Ooi",
      "Wei Wang",
      "James Yip"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=From+Plate+to+Prevention%3A+A+Dietary+Nutrient-aided+Platform+for+Health+Promotion+in+Singapore+Kaiping+Zheng+Thao+Nguyen+Jesslyn+Hwei+Sing+Chong+Charlene+Enhui+Goh+Melanie+Herschel",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "K Zheng",
        "id": "V1PdtzQAAAAJ"
      },
      {
        "name": "T Nguyen",
        "id": "7gL3EmMAAAAJ"
      },
      {
        "name": "JHS Chong",
        "id": null
      },
      {
        "name": "CE Goh",
        "id": "yT4o_TcAAAAJ"
      },
      {
        "name": "M Herschel",
        "id": "K5VPw-IAAAAJ"
      },
      {
        "name": "HH Lee",
        "id": null
      },
      {
        "name": "C Liu",
        "id": null
      },
      {
        "name": "BC Ooi",
        "id": "9560QjYAAAAJ"
      },
      {
        "name": "W Wang",
        "id": "46Dd4v4AAAAJ"
      }
    ],
    "citation_count": 3
  },
  {
    "arxiv_id": "2501.03674",
    "title": "Action Quality Assessment via Hierarchical Pose-guided Multi-stage Contrastive Regression",
    "year": 2025,
    "published": "2025-01-07T10:20:16Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Action Quality Assessment (AQA), which aims at automatic and fair evaluation of athletic performance, has gained increasing attention in recent years. However, athletes are often in rapid movement and the corresponding visual appearance variances are subtle, making it challenging to capture fine-grained pose differences and leading to poor estimation performance. Furthermore, most common AQA tasks, such as diving in sports, are usually divided into multiple sub-actions, each of which contains di",
    "arxiv_url": "https://arxiv.org/abs/2501.03674v2",
    "pdf_url": "https://arxiv.org/pdf/2501.03674v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2501.03674",
    "arxiv_authors": [
      "Mengshi Qi",
      "Hao Ye",
      "Jiaxuan Peng",
      "Huadong Ma"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Action+Quality+Assessment+via+Hierarchical+Pose-guided+Multi-stage+Contrastive+Regression+Mengshi+Qi+Hao+Ye+Jiaxuan+Peng+Huadong+Ma",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "M Qi",
        "id": "_gH7-4wAAAAJ"
      },
      {
        "name": "H Ye",
        "id": null
      },
      {
        "name": "J Peng",
        "id": null
      },
      {
        "name": "H Ma -",
        "id": null
      }
    ],
    "citation_count": 2
  },
  {
    "arxiv_id": "2301.07301",
    "title": "PTA-Det: Point Transformer Associating Point cloud and Image for 3D Object Detection",
    "year": 2023,
    "published": "2023-01-18T04:35:49Z",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "In autonomous driving, 3D object detection based on multi-modal data has become an indispensable approach when facing complex environments around the vehicle. During multi-modal detection, LiDAR and camera are simultaneously applied for capturing and modeling. However, due to the intrinsic discrepancies between the LiDAR point and camera image, the fusion of the data for object detection encounters a series of problems. Most multi-modal detection methods perform even worse than LiDAR-only method",
    "arxiv_url": "https://arxiv.org/abs/2301.07301v1",
    "pdf_url": "https://arxiv.org/pdf/2301.07301v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2301.07301",
    "arxiv_authors": [
      "Rui Wan",
      "Tianyun Zhao",
      "Wei Zhao"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=PTA-Det%3A+Point+Transformer+Associating+Point+cloud+and+Image+for+3D+Object+Detection+Rui+Wan+Tianyun+Zhao+Wei+Zhao",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "R Wan",
        "id": null
      },
      {
        "name": "T Zhao",
        "id": "63OStaEAAAAJ"
      },
      {
        "name": "W Zhao - Sensors",
        "id": null
      }
    ],
    "citation_count": 13
  },
  {
    "arxiv_id": "2311.05565",
    "title": "High-Performance Transformers for Table Structure Recognition Need Early Convolutions",
    "year": 2023,
    "published": "2023-11-09T18:20:52Z",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "Table structure recognition (TSR) aims to convert tabular images into a machine-readable format, where a visual encoder extracts image features and a textual decoder generates table-representing tokens. Existing approaches use classic convolutional neural network (CNN) backbones for the visual encoder and transformers for the textual decoder. However, this hybrid CNN-Transformer architecture introduces a complex visual encoder that accounts for nearly half of the total model parameters, markedly",
    "arxiv_url": "https://arxiv.org/abs/2311.05565v1",
    "pdf_url": "https://arxiv.org/pdf/2311.05565v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2311.05565",
    "arxiv_authors": [
      "ShengYun Peng",
      "Seongmin Lee",
      "Xiaojing Wang",
      "Rajarajeswari Balasubramaniyan",
      "Duen Horng Chau"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=High-Performance+Transformers+for+Table+Structure+Recognition+Need+Early+Convolutions+ShengYun+Peng+Seongmin+Lee+Xiaojing+Wang+Rajarajeswari+Balasubramaniyan+Duen+Horng+Chau",
    "gs_search_success": false,
    "gs_authors": null,
    "citation_count": null,
    "error_type": "fetch_failed"
  },
  {
    "arxiv_id": "2411.14774",
    "title": "Resolution-Agnostic Transformer-based Climate Downscaling",
    "year": 2024,
    "published": "2024-11-22T07:32:11Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Understanding future weather changes at regional and local scales is crucial for planning and decision-making, particularly in the context of extreme weather events, as well as for broader applications in agriculture, insurance, and infrastructure development. However, the computational cost of downscaling Global Climate Models (GCMs) to the fine resolutions needed for such applications presents a significant barrier. Drawing on advancements in weather forecasting models, this study introduces a",
    "arxiv_url": "https://arxiv.org/abs/2411.14774v2",
    "pdf_url": "https://arxiv.org/pdf/2411.14774v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2411.14774",
    "arxiv_authors": [
      "Declan Curran",
      "Hira Saleem",
      "Sanaa Hobeichi",
      "Flora Salim"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Resolution-Agnostic+Transformer-based+Climate+Downscaling+Declan+Curran+Hira+Saleem+Sanaa+Hobeichi+Flora+Salim",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "D Curran",
        "id": "xRsNRdcAAAAJ"
      },
      {
        "name": "H Saleem",
        "id": "gDsqQzIAAAAJ"
      },
      {
        "name": "S Hobeichi",
        "id": "aLYPHpgAAAAJ"
      },
      {
        "name": "F Salim -",
        "id": null
      }
    ],
    "citation_count": 6
  },
  {
    "arxiv_id": "2403.08108",
    "title": "TaskCLIP: Extend Large Vision-Language Model for Task Oriented Object Detection",
    "year": 2024,
    "published": "2024-03-12T22:33:02Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Task-oriented object detection aims to find objects suitable for accomplishing specific tasks. As a challenging task, it requires simultaneous visual data processing and reasoning under ambiguous semantics. Recent solutions are mainly all-in-one models. However, the object detection backbones are pre-trained without text supervision. Thus, to incorporate task requirements, their intricate models undergo extensive learning on a highly imbalanced and scarce dataset, resulting in capped performance",
    "arxiv_url": "https://arxiv.org/abs/2403.08108v2",
    "pdf_url": "https://arxiv.org/pdf/2403.08108v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2403.08108",
    "arxiv_authors": [
      "Hanning Chen",
      "Wenjun Huang",
      "Yang Ni",
      "Sanggeon Yun",
      "Yezi Liu",
      "Fei Wen",
      "Alvaro Velasquez",
      "Hugo Latapie",
      "Mohsen Imani"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=TaskCLIP%3A+Extend+Large+Vision-Language+Model+for+Task+Oriented+Object+Detection+Hanning+Chen+Wenjun+Huang+Yang+Ni+Sanggeon+Yun+Yezi+Liu",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "H Chen",
        "id": "i_CamtAAAAAJ"
      },
      {
        "name": "W Huang",
        "id": "pO3spAkAAAAJ"
      },
      {
        "name": "Y Ni",
        "id": "F_FgI4gAAAAJ"
      },
      {
        "name": "S Yun",
        "id": "Qw2sWuMAAAAJ"
      },
      {
        "name": "Y Liu",
        "id": "bufbDK0AAAAJ"
      },
      {
        "name": "F Wen",
        "id": "v2CLq4QAAAAJ"
      },
      {
        "name": "A Velasquez",
        "id": "1g3pA4cAAAAJ"
      },
      {
        "name": "H Latapie",
        "id": "9Nx0hhAAAAAJ"
      },
      {
        "name": "M ImaniEuropean",
        "id": null
      }
    ],
    "citation_count": 37
  },
  {
    "arxiv_id": "2303.12218",
    "title": "Compositional 3D Scene Generation using Locally Conditioned Diffusion",
    "year": 2023,
    "published": "2023-03-21T22:37:16Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Designing complex 3D scenes has been a tedious, manual process requiring domain expertise. Emerging text-to-3D generative models show great promise for making this task more intuitive, but existing approaches are limited to object-level generation. We introduce \\textbf{locally conditioned diffusion} as an approach to compositional scene diffusion, providing control over semantic parts using text prompts and bounding boxes while ensuring seamless transitions between these parts. We demonstrate a ",
    "arxiv_url": "https://arxiv.org/abs/2303.12218v2",
    "pdf_url": "https://arxiv.org/pdf/2303.12218v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2303.12218",
    "arxiv_authors": [
      "Ryan Po",
      "Gordon Wetzstein"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Compositional+3D+Scene+Generation+using+Locally+Conditioned+Diffusion+Ryan+Po+Gordon+Wetzstein",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "R Po",
        "id": "cyreyCUAAAAJ"
      },
      {
        "name": "G Wetzstein -",
        "id": null
      }
    ],
    "citation_count": 120
  },
  {
    "arxiv_id": "2408.16647",
    "title": "DriveGenVLM: Real-world Video Generation for Vision Language Model based Autonomous Driving",
    "year": 2024,
    "published": "2024-08-29T15:52:56Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "The advancement of autonomous driving technologies necessitates increasingly sophisticated methods for understanding and predicting real-world scenarios. Vision language models (VLMs) are emerging as revolutionary tools with significant potential to influence autonomous driving. In this paper, we propose the DriveGenVLM framework to generate driving videos and use VLMs to understand them. To achieve this, we employ a video generation framework grounded in denoising diffusion probabilistic models",
    "arxiv_url": "https://arxiv.org/abs/2408.16647v1",
    "pdf_url": "https://arxiv.org/pdf/2408.16647v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2408.16647",
    "arxiv_authors": [
      "Yongjie Fu",
      "Anmol Jain",
      "Xuan Di",
      "Xu Chen",
      "Zhaobin Mo"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=DriveGenVLM%3A+Real-world+Video+Generation+for+Vision+Language+Model+based+Autonomous+Driving+Yongjie+Fu+Anmol+Jain+Xuan+Di+Xu+Chen+Zhaobin+Mo",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "Y Fu",
        "id": "DvdbrvwAAAAJ"
      },
      {
        "name": "A Jain",
        "id": "2whfj1AAAAAJ"
      },
      {
        "name": "X Chen",
        "id": "MU4xWsYAAAAJ"
      },
      {
        "name": "Z Mo",
        "id": "PoExlP0AAAAJ"
      },
      {
        "name": "X Di -",
        "id": null
      }
    ],
    "citation_count": 10
  },
  {
    "arxiv_id": "2408.09567",
    "title": "Enhancing ASL Recognition with GCNs and Successive Residual Connections",
    "year": 2024,
    "published": "2024-08-18T18:40:30Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "This study presents a novel approach for enhancing American Sign Language (ASL) recognition using Graph Convolutional Networks (GCNs) integrated with successive residual connections. The method leverages the MediaPipe framework to extract key landmarks from each hand gesture, which are then used to construct graph representations. A robust preprocessing pipeline, including translational and scale normalization techniques, ensures consistency across the dataset. The constructed graphs are fed int",
    "arxiv_url": "https://arxiv.org/abs/2408.09567v1",
    "pdf_url": "https://arxiv.org/pdf/2408.09567v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2408.09567",
    "arxiv_authors": [
      "Ushnish Sarkar",
      "Archisman Chakraborti",
      "Tapas Samanta",
      "Sarbajit Pal",
      "Amitabha Das"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Enhancing+ASL+Recognition+with+GCNs+and+Successive+Residual+Connections+Ushnish+Sarkar+Archisman+Chakraborti+Tapas+Samanta+Sarbajit+Pal+Amitabha+Das",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "U Sarkar",
        "id": "P5Rb-IkAAAAJ"
      },
      {
        "name": "A Chakraborti",
        "id": null
      },
      {
        "name": "T Samanta",
        "id": null
      },
      {
        "name": "S Pal",
        "id": "4hDQHAQAAAAJ"
      },
      {
        "name": "A DasInternational",
        "id": null
      }
    ],
    "citation_count": 1
  },
  {
    "arxiv_id": "2402.03315",
    "title": "RTHDet: Rotate Table Area and Head Detection in images",
    "year": 2023,
    "published": "2023-12-31T07:34:31Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Traditional models focus on horizontal table detection but struggle in rotating contexts, limiting progress in table recognition. This paper introduces a new task: detecting table regions and localizing head-tail parts in rotation scenarios. We propose corresponding datasets, evaluation metrics, and methods. Our novel method, 'Adaptively Bounded Rotation,' addresses dataset scarcity in detecting rotated tables and their head-tail parts. We produced 'TRR360D,' a dataset incorporating semantic inf",
    "arxiv_url": "https://arxiv.org/abs/2402.03315v1",
    "pdf_url": "https://arxiv.org/pdf/2402.03315v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2402.03315",
    "arxiv_authors": [
      "Wenxing Hu",
      "Minglei Tong"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=RTHDet%3A+Rotate+Table+Area+and+Head+Detection+in+images+Wenxing+Hu+Minglei+Tong",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "W Hu",
        "id": "pcQD53MAAAAJ"
      },
      {
        "name": "M Tong -",
        "id": null
      }
    ],
    "citation_count": 1
  },
  {
    "arxiv_id": "2404.02394",
    "title": "Cohort-Individual Cooperative Learning for Multimodal Cancer Survival Analysis",
    "year": 2024,
    "published": "2024-04-03T01:36:27Z",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "abstract": "Recently, we have witnessed impressive achievements in cancer survival analysis by integrating multimodal data, e.g., pathology images and genomic profiles. However, the heterogeneity and high dimensionality of these modalities pose significant challenges for extracting discriminative representations while maintaining good generalization. In this paper, we propose a Cohort-individual Cooperative Learning (CCL) framework to advance cancer survival analysis by collaborating knowledge decomposition",
    "arxiv_url": "https://arxiv.org/abs/2404.02394v2",
    "pdf_url": "https://arxiv.org/pdf/2404.02394v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2404.02394",
    "arxiv_authors": [
      "Huajun Zhou",
      "Fengtao Zhou",
      "Hao Chen"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Cohort-Individual+Cooperative+Learning+for+Multimodal+Cancer+Survival+Analysis+Huajun+Zhou+Fengtao+Zhou+Hao+Chen",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "H Zhou",
        "id": "3flTFBQAAAAJ"
      },
      {
        "name": "F Zhou",
        "id": "neX6hWsAAAAJ"
      },
      {
        "name": "H Chen - IEEE Transactions on Medical",
        "id": null
      }
    ],
    "citation_count": 25
  },
  {
    "arxiv_id": "2503.12605",
    "title": "Multimodal Chain-of-Thought Reasoning: A Comprehensive Survey",
    "year": 2025,
    "published": "2025-03-16T18:39:13Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "By extending the advantage of chain-of-thought (CoT) reasoning in human-like step-by-step processes to multimodal contexts, multimodal CoT (MCoT) reasoning has recently garnered significant research attention, especially in the integration with multimodal large language models (MLLMs). Existing MCoT studies design various methodologies and innovative reasoning paradigms to address the unique challenges of image, video, speech, audio, 3D, and structured data across different modalities, achieving",
    "arxiv_url": "https://arxiv.org/abs/2503.12605v2",
    "pdf_url": "https://arxiv.org/pdf/2503.12605v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2503.12605",
    "arxiv_authors": [
      "Yaoting Wang",
      "Shengqiong Wu",
      "Yuecheng Zhang",
      "Shuicheng Yan",
      "Ziwei Liu",
      "Jiebo Luo",
      "Hao Fei"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Multimodal+Chain-of-Thought+Reasoning%3A+A+Comprehensive+Survey+Yaoting+Wang+Shengqiong+Wu+Yuecheng+Zhang+Shuicheng+Yan+Ziwei+Liu",
    "gs_search_success": false,
    "gs_authors": null,
    "citation_count": null,
    "error_type": "fetch_failed"
  },
  {
    "arxiv_id": "2312.09988",
    "title": "Towards Architecture-Agnostic Untrained Network Priors for Image Reconstruction with Frequency Regularization",
    "year": 2023,
    "published": "2023-12-15T18:01:47Z",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "abstract": "Untrained networks inspired by deep image priors have shown promising capabilities in recovering high-quality images from noisy or partial measurements without requiring training sets. Their success is widely attributed to implicit regularization due to the spectral bias of suitable network architectures. However, the application of such network-based priors often entails superfluous architectural decisions, risks of overfitting, and lengthy optimization processes, all of which hinder their prac",
    "arxiv_url": "https://arxiv.org/abs/2312.09988v3",
    "pdf_url": "https://arxiv.org/pdf/2312.09988v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2312.09988",
    "arxiv_authors": [
      "Yilin Liu",
      "Yunkui Pang",
      "Jiang Li",
      "Yong Chen",
      "Pew-Thian Yap"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Towards+Architecture-Agnostic+Untrained+Network+Priors+for+Image+Reconstruction+with+Frequency+Regularization+Yilin+Liu+Yunkui+Pang+Jiang+Li+Yong+Chen+Pew-Thian+Yap",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "Y Liu",
        "id": "IbKMM6EAAAAJ"
      },
      {
        "name": "Y Pang",
        "id": null
      },
      {
        "name": "J Li",
        "id": null
      },
      {
        "name": "Y Chen",
        "id": "HDBAq-YAAAAJ"
      },
      {
        "name": "PT Yap - European",
        "id": null
      }
    ],
    "citation_count": null
  },
  {
    "arxiv_id": "2501.00300",
    "title": "Research on vehicle detection based on improved YOLOv8 network",
    "year": 2024,
    "published": "2024-12-31T06:19:26Z",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "The key to ensuring the safe obstacle avoidance function of autonomous driving systems lies in the use of extremely accurate vehicle recognition techniques. However, the variability of the actual road environment and the diverse characteristics of vehicles and pedestrians together constitute a huge obstacle to improving detection accuracy, posing a serious challenge to the realization of this goal. To address the above issues, this paper proposes an improved YOLOv8 vehicle detection method. Spec",
    "arxiv_url": "https://arxiv.org/abs/2501.00300v1",
    "pdf_url": "https://arxiv.org/pdf/2501.00300v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2501.00300",
    "arxiv_authors": [
      "Haocheng Guo",
      "Yaqiong Zhang",
      "Lieyang Chen",
      "Arfat Ahmad Khan"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Research+on+vehicle+detection+based+on+improved+YOLOv8+network+Haocheng+Guo+Yaqiong+Zhang+Lieyang+Chen+Arfat+Ahmad+Khan",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "H Guo",
        "id": null
      },
      {
        "name": "Y Zhang",
        "id": "MaFBt2oAAAAJ"
      },
      {
        "name": "L Chen",
        "id": "8BkA-90AAAAJ"
      },
      {
        "name": "AA Khan -",
        "id": null
      }
    ],
    "citation_count": 74
  },
  {
    "arxiv_id": "2404.00257",
    "title": "YOLOOC: YOLO-based Open-Class Incremental Object Detection with Novel Class Discovery",
    "year": 2024,
    "published": "2024-03-30T06:17:39Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "eess.IV"
    ],
    "abstract": "Because of its use in practice, open-world object detection (OWOD) has gotten a lot of attention recently. The challenge is how can a model detect novel classes and then incrementally learn them without forgetting previously known classes. Previous approaches hinge on strongly-supervised or weakly-supervised novel-class data for novel-class detection, which may not apply to real applications. We construct a new benchmark that novel classes are only encountered at the inference stage. And we prop",
    "arxiv_url": "https://arxiv.org/abs/2404.00257v2",
    "pdf_url": "https://arxiv.org/pdf/2404.00257v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2404.00257",
    "arxiv_authors": [
      "Qian Wan",
      "Xiang Xiang",
      "Qinhao Zhou"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=YOLOOC%3A+YOLO-based+Open-Class+Incremental+Object+Detection+with+Novel+Class+Discovery+Qian+Wan+Xiang+Xiang+Qinhao+Zhou",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "Q Wan",
        "id": null
      },
      {
        "name": "X Xiang",
        "id": "-D5k5ioAAAAJ"
      },
      {
        "name": "Q Zhou -",
        "id": null
      }
    ],
    "citation_count": 4
  },
  {
    "arxiv_id": "2306.03287",
    "title": "ICDAR 2023 Competition on Structured Text Extraction from Visually-Rich Document Images",
    "year": 2023,
    "published": "2023-06-05T22:20:52Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Structured text extraction is one of the most valuable and challenging application directions in the field of Document AI. However, the scenarios of past benchmarks are limited, and the corresponding evaluation protocols usually focus on the submodules of the structured text extraction scheme. In order to eliminate these problems, we organized the ICDAR 2023 competition on Structured text extraction from Visually-Rich Document images (SVRD). We set up two tracks for SVRD including Track 1: HUST-",
    "arxiv_url": "https://arxiv.org/abs/2306.03287v1",
    "pdf_url": "https://arxiv.org/pdf/2306.03287v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2306.03287",
    "arxiv_authors": [
      "Wenwen Yu",
      "Chengquan Zhang",
      "Haoyu Cao",
      "Wei Hua",
      "Bohan Li",
      "Huang Chen",
      "Mingyu Liu",
      "Mingrui Chen",
      "Jianfeng Kuang",
      "Mengjun Cheng",
      "Yuning Du",
      "Shikun Feng",
      "Xiaoguang Hu",
      "Pengyuan Lyu",
      "Kun Yao",
      "Yuechen Yu",
      "Yuliang Liu",
      "Wanxiang Che",
      "Errui Ding",
      "Cheng-Lin Liu",
      "Jiebo Luo",
      "Shuicheng Yan",
      "Min Zhang",
      "Dimosthenis Karatzas",
      "Xing Sun",
      "Jingdong Wang",
      "Xiang Bai"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=ICDAR+2023+Competition+on+Structured+Text+Extraction+from+Visually-Rich+Document+Images+Wenwen+Yu+Chengquan+Zhang+Haoyu+Cao+Wei+Hua+Bohan+Li",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "W Yu",
        "id": "gEXBFvYAAAAJ"
      },
      {
        "name": "C Zhang",
        "id": null
      },
      {
        "name": "H Cao",
        "id": "LV8ejn8AAAAJ"
      },
      {
        "name": "W Hua",
        "id": "Z5OOSMkAAAAJ"
      },
      {
        "name": "B Li",
        "id": "txzTrMsAAAAJ"
      },
      {
        "name": "H Chen",
        "id": "QAXYEKAAAAAJ"
      },
      {
        "name": "M Liu",
        "id": "1b_peoQAAAAJ"
      },
      {
        "name": "M Chen",
        "id": "EJ9pB5kAAAAJ"
      },
      {
        "name": "J Kuang",
        "id": null
      },
      {
        "name": "M Cheng",
        "id": "29u6QskAAAAJ"
      },
      {
        "name": "Y Du",
        "id": null
      }
    ],
    "citation_count": 17
  },
  {
    "arxiv_id": "2311.17256",
    "title": "Pattern retrieval of traffic congestion using graph-based associations of traffic domain-specific features",
    "year": 2023,
    "published": "2023-11-28T22:33:22Z",
    "categories": [
      "cs.CV",
      "eess.IV"
    ],
    "abstract": "The fast-growing amount of traffic data brings many opportunities for revealing more insightful information about traffic dynamics. However, it also demands an effective database management system in which information retrieval is arguably an important feature. The ability to locate similar patterns in big datasets potentially paves the way for further valuable analyses in traffic management. This paper proposes a content-based retrieval system for spatiotemporal patterns of highway traffic cong",
    "arxiv_url": "https://arxiv.org/abs/2311.17256v1",
    "pdf_url": "https://arxiv.org/pdf/2311.17256v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2311.17256",
    "arxiv_authors": [
      "Tin T. Nguyen",
      "Simeon C. Calvert",
      "Guopeng Li",
      "Hans van Lint"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Pattern+retrieval+of+traffic+congestion+using+graph-based+associations+of+traffic+domain-specific+features+Tin+T.+Nguyen+Simeon+C.+Calvert+Guopeng+Li+Hans+van+Lint",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "TT Nguyen",
        "id": "-sHmknMAAAAJ"
      },
      {
        "name": "SC Calvert",
        "id": "pwVbHHEAAAAJ"
      },
      {
        "name": "G Li",
        "id": "FRYYcAUAAAAJ"
      },
      {
        "name": "H van Lint -",
        "id": null
      }
    ],
    "citation_count": null
  },
  {
    "arxiv_id": "2505.05528",
    "title": "X-Transfer Attacks: Towards Super Transferable Adversarial Attacks on CLIP",
    "year": 2025,
    "published": "2025-05-08T11:59:13Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "abstract": "As Contrastive Language-Image Pre-training (CLIP) models are increasingly adopted for diverse downstream tasks and integrated into large vision-language models (VLMs), their susceptibility to adversarial perturbations has emerged as a critical concern. In this work, we introduce \\textbf{X-Transfer}, a novel attack method that exposes a universal adversarial vulnerability in CLIP. X-Transfer generates a Universal Adversarial Perturbation (UAP) capable of deceiving various CLIP encoders and downst",
    "arxiv_url": "https://arxiv.org/abs/2505.05528v3",
    "pdf_url": "https://arxiv.org/pdf/2505.05528v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2505.05528",
    "arxiv_authors": [
      "Hanxun Huang",
      "Sarah Erfani",
      "Yige Li",
      "Xingjun Ma",
      "James Bailey"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=X-Transfer+Attacks%3A+Towards+Super+Transferable+Adversarial+Attacks+on+CLIP+Hanxun+Huang+Sarah+Erfani+Yige+Li+Xingjun+Ma+James+Bailey",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "H Huang",
        "id": "8CxZe3IAAAAJ"
      },
      {
        "name": "S Erfani",
        "id": "Jq9ocx4AAAAJ"
      },
      {
        "name": "Y Li",
        "id": "h0cS2nQAAAAJ"
      },
      {
        "name": "X Ma",
        "id": "XQViiyYAAAAJ"
      },
      {
        "name": "J Bailey -",
        "id": null
      }
    ],
    "citation_count": 7
  },
  {
    "arxiv_id": "2503.10886",
    "title": "Taxonomic Reasoning for Rare Arthropods: Combining Dense Image Captioning and RAG for Interpretable Classification",
    "year": 2025,
    "published": "2025-03-13T21:18:10Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.IR",
      "cs.LG",
      "q-bio.PE"
    ],
    "abstract": "In the context of pressing climate change challenges and the significant biodiversity loss among arthropods, automated taxonomic classification from organismal images is a subject of intense research. However, traditional AI pipelines based on deep neural visual architectures such as CNNs or ViTs face limitations such as degraded performance on the long-tail of classes and the inability to reason about their predictions. We integrate image captioning and retrieval-augmented generation (RAG) with",
    "arxiv_url": "https://arxiv.org/abs/2503.10886v1",
    "pdf_url": "https://arxiv.org/pdf/2503.10886v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2503.10886",
    "arxiv_authors": [
      "Nathaniel Lesperance",
      "Sujeevan Ratnasingham",
      "Graham W. Taylor"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Taxonomic+Reasoning+for+Rare+Arthropods%3A+Combining+Dense+Image+Captioning+and+RAG+for+Interpretable+Classification+Nathaniel+Lesperance+Sujeevan+Ratnasingham+Graham+W.+Taylor",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "N Lesperance",
        "id": null
      },
      {
        "name": "S Ratnasingham",
        "id": "5JNqG1EAAAAJ"
      },
      {
        "name": "GW Taylor -",
        "id": null
      }
    ],
    "citation_count": 1
  },
  {
    "arxiv_id": "2410.09592",
    "title": "ControLRM: Fast and Controllable 3D Generation via Large Reconstruction Model",
    "year": 2024,
    "published": "2024-10-12T16:47:20Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Despite recent advancements in 3D generation methods, achieving controllability still remains a challenging issue. Current approaches utilizing score-distillation sampling are hindered by laborious procedures that consume a significant amount of time. Furthermore, the process of first generating 2D representations and then mapping them to 3D lacks internal alignment between the two forms of representation. To address these challenges, we introduce ControLRM, an end-to-end feed-forward model desi",
    "arxiv_url": "https://arxiv.org/abs/2410.09592v1",
    "pdf_url": "https://arxiv.org/pdf/2410.09592v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2410.09592",
    "arxiv_authors": [
      "Hongbin Xu",
      "Weitao Chen",
      "Zhipeng Zhou",
      "Feng Xiao",
      "Baigui Sun",
      "Mike Zheng Shou",
      "Wenxiong Kang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=ControLRM%3A+Fast+and+Controllable+3D+Generation+via+Large+Reconstruction+Model+Hongbin+Xu+Weitao+Chen+Zhipeng+Zhou+Feng+Xiao+Baigui+Sun",
    "gs_search_success": false,
    "gs_authors": null,
    "citation_count": null,
    "error_type": "fetch_failed"
  },
  {
    "arxiv_id": "2504.13690",
    "title": "Analysing the Robustness of Vision-Language-Models to Common Corruptions",
    "year": 2025,
    "published": "2025-04-18T13:46:32Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Vision-language models (VLMs) have demonstrated impressive capabilities in understanding and reasoning about visual and textual content. However, their robustness to common image corruptions remains under-explored. In this work, we present the first comprehensive analysis of VLM robustness across 19 corruption types from the ImageNet-C benchmark, spanning four categories: noise, blur, weather, and digital distortions. We introduce two new benchmarks, TextVQA-C and GQA-C, to systematically evalua",
    "arxiv_url": "https://arxiv.org/abs/2504.13690v2",
    "pdf_url": "https://arxiv.org/pdf/2504.13690v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2504.13690",
    "arxiv_authors": [
      "Muhammad Usama",
      "Syeda Aishah Asim",
      "Syed Bilal Ali",
      "Syed Talal Wasim",
      "Umair Bin Mansoor"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Analysing+the+Robustness+of+Vision-Language-Models+to+Common+Corruptions+Muhammad+Usama+Syeda+Aishah+Asim+Syed+Bilal+Ali+Syed+Talal+Wasim+Umair+Bin+Mansoor",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "M Usama",
        "id": "hkGWNFMAAAAJ"
      },
      {
        "name": "SA Asim",
        "id": null
      },
      {
        "name": "SB Ali",
        "id": "ukJ7qbQAAAAJ"
      },
      {
        "name": "ST Wasim",
        "id": "uHySarAAAAAJ"
      },
      {
        "name": "UB Mansoor",
        "id": null
      }
    ],
    "citation_count": 5
  },
  {
    "arxiv_id": "2503.20998",
    "title": "CoMapGS: Covisibility Map-based Gaussian Splatting for Sparse Novel View Synthesis",
    "year": 2025,
    "published": "2025-03-25T12:05:25Z",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "abstract": "We propose Covisibility Map-based Gaussian Splatting (CoMapGS), designed to recover underrepresented sparse regions in sparse novel view synthesis. CoMapGS addresses both high- and low-uncertainty regions by constructing covisibility maps, enhancing initial point clouds, and applying uncertainty-aware weighted supervision using a proximity classifier. Our contributions are threefold: (1) CoMapGS reframes novel view synthesis by leveraging covisibility maps as a core component to address region-s",
    "arxiv_url": "https://arxiv.org/abs/2503.20998v1",
    "pdf_url": "https://arxiv.org/pdf/2503.20998v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2503.20998",
    "arxiv_authors": [
      "Youngkyoon Jang",
      "Eduardo P√©rez-Pellitero"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=CoMapGS%3A+Covisibility+Map-based+Gaussian+Splatting+for+Sparse+Novel+View+Synthesis+Youngkyoon+Jang+Eduardo+P%C3%A9rez-Pellitero",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "Y Jang",
        "id": "yQiSin8AAAAJ"
      },
      {
        "name": "E P√©rez-Pellitero -",
        "id": null
      }
    ],
    "citation_count": 3
  },
  {
    "arxiv_id": "2406.07284",
    "title": "Unsupervised Object Detection with Theoretical Guarantees",
    "year": 2024,
    "published": "2024-06-11T14:12:31Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Unsupervised object detection using deep neural networks is typically a difficult problem with few to no guarantees about the learned representation. In this work we present the first unsupervised object detection method that is theoretically guaranteed to recover the true object positions up to quantifiable small shifts. We develop an unsupervised object detection architecture and prove that the learned variables correspond to the true object positions up to small shifts related to the encoder ",
    "arxiv_url": "https://arxiv.org/abs/2406.07284v2",
    "pdf_url": "https://arxiv.org/pdf/2406.07284v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2406.07284",
    "arxiv_authors": [
      "Marian Longa",
      "Jo√£o F. Henriques"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Unsupervised+Object+Detection+with+Theoretical+Guarantees+Marian+Longa+Jo%C3%A3o+F.+Henriques",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "M Longa",
        "id": null
      },
      {
        "name": "JF Henriques - Advances in Neural Information",
        "id": null
      }
    ],
    "citation_count": 1
  },
  {
    "arxiv_id": "2406.20076",
    "title": "EVF-SAM: Early Vision-Language Fusion for Text-Prompted Segment Anything Model",
    "year": 2024,
    "published": "2024-06-28T17:38:18Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Segment Anything Model (SAM) has attracted widespread attention for its superior interactive segmentation capabilities with visual prompts while lacking further exploration of text prompts. In this paper, we empirically investigate what text prompt encoders (e.g., CLIP or LLM) are good for adapting SAM for referring expression segmentation and introduce the Early Vision-language Fusion-based SAM (EVF-SAM). EVF-SAM is a simple yet effective referring segmentation method which exploits multimodal ",
    "arxiv_url": "https://arxiv.org/abs/2406.20076v5",
    "pdf_url": "https://arxiv.org/pdf/2406.20076v5",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2406.20076",
    "arxiv_authors": [
      "Yuxuan Zhang",
      "Tianheng Cheng",
      "Lianghui Zhu",
      "Rui Hu",
      "Lei Liu",
      "Heng Liu",
      "Longjin Ran",
      "Xiaoxin Chen",
      "Wenyu Liu",
      "Xinggang Wang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=EVF-SAM%3A+Early+Vision-Language+Fusion+for+Text-Prompted+Segment+Anything+Model+Yuxuan+Zhang+Tianheng+Cheng+Lianghui+Zhu+Rui+Hu+Lei+Liu",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "Y Zhang",
        "id": "8TYFxugAAAAJ"
      },
      {
        "name": "T Cheng",
        "id": "PH8rJHYAAAAJ"
      },
      {
        "name": "L Zhu",
        "id": "NvMHcs0AAAAJ"
      },
      {
        "name": "R Hu",
        "id": null
      },
      {
        "name": "L Liu",
        "id": "D7jDk7gAAAAJ"
      },
      {
        "name": "H Liu",
        "id": null
      },
      {
        "name": "L Ran",
        "id": null
      },
      {
        "name": "X Chen",
        "id": null
      },
      {
        "name": "W Liu",
        "id": "SmHr4W4AAAAJ"
      },
      {
        "name": "X Wang",
        "id": "qNCTLV0AAAAJ"
      }
    ],
    "citation_count": 51
  },
  {
    "arxiv_id": "2410.12988",
    "title": "Risk Assessment for Autonomous Landing in Urban Environments using Semantic Segmentation",
    "year": 2024,
    "published": "2024-10-16T19:34:03Z",
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "abstract": "In this paper, we address the vision-based autonomous landing problem in complex urban environments using deep neural networks for semantic segmentation and risk assessment. We propose employing the SegFormer, a state-of-the-art visual transformer network, for the semantic segmentation of complex, unstructured urban environments. This approach yields valuable information that can be utilized in smart autonomous landing missions, particularly in emergency landing scenarios resulting from system f",
    "arxiv_url": "https://arxiv.org/abs/2410.12988v1",
    "pdf_url": "https://arxiv.org/pdf/2410.12988v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2410.12988",
    "arxiv_authors": [
      "Jes√∫s Alejandro Loera-Ponce",
      "Diego A. Mercado-Ravell",
      "Israel Becerra-Dur√°n",
      "Luis Manuel Valentin-Coronado"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Risk+Assessment+for+Autonomous+Landing+in+Urban+Environments+using+Semantic+Segmentation+Jes%C3%BAs+Alejandro+Loera-Ponce+Diego+A.+Mercado-Ravell+Israel+Becerra-Dur%C3%A1n+Luis+Manuel+Valentin-Coronado",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "JA Loera-Ponce",
        "id": null
      },
      {
        "name": "DA Mercado-Ravell",
        "id": "effDVocAAAAJ"
      },
      {
        "name": "I Becerra",
        "id": "Ng1bxjYAAAAJ"
      },
      {
        "name": "LM Valentin-CoronadoIbero-American",
        "id": null
      }
    ],
    "citation_count": 2
  },
  {
    "arxiv_id": "2308.07070",
    "title": "A Local Iterative Approach for the Extraction of 2D Manifolds from Strongly Curved and Folded Thin-Layer Structures",
    "year": 2023,
    "published": "2023-08-14T11:05:37Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Ridge surfaces represent important features for the analysis of 3-dimensional (3D) datasets in diverse applications and are often derived from varying underlying data including flow fields, geological fault data, and point data, but they can also be present in the original scalar images acquired using a plethora of imaging techniques. Our work is motivated by the analysis of image data acquired using micro-computed tomography (Micro-CT) of ancient, rolled and folded thin-layer structures such as",
    "arxiv_url": "https://arxiv.org/abs/2308.07070v1",
    "pdf_url": "https://arxiv.org/pdf/2308.07070v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2308.07070",
    "arxiv_authors": [
      "Nicolas Klenert",
      "Verena Lepper",
      "Daniel Baum"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=A+Local+Iterative+Approach+for+the+Extraction+of+2D+Manifolds+from+Strongly+Curved+and+Folded+Thin-Layer+Structures+Nicolas+Klenert+Verena+Lepper+Daniel+Baum",
    "gs_search_success": false,
    "gs_authors": null,
    "citation_count": null,
    "error_type": "fetch_failed"
  },
  {
    "arxiv_id": "2504.09601",
    "title": "Mixture-of-Shape-Experts (MoSE): End-to-End Shape Dictionary Framework to Prompt SAM for Generalizable Medical Segmentation",
    "year": 2025,
    "published": "2025-04-13T14:34:52Z",
    "categories": [
      "cs.CV",
      "cs.LG",
      "cs.MM",
      "eess.IV",
      "physics.med-ph"
    ],
    "abstract": "Single domain generalization (SDG) has recently attracted growing attention in medical image segmentation. One promising strategy for SDG is to leverage consistent semantic shape priors across different imaging protocols, scanner vendors, and clinical sites. However, existing dictionary learning methods that encode shape priors often suffer from limited representational power with a small set of offline computed shape elements, or overfitting when the dictionary size grows. Moreover, they are no",
    "arxiv_url": "https://arxiv.org/abs/2504.09601v1",
    "pdf_url": "https://arxiv.org/pdf/2504.09601v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2504.09601",
    "arxiv_authors": [
      "Jia Wei",
      "Xiaoqi Zhao",
      "Jonghye Woo",
      "Jinsong Ouyang",
      "Georges El Fakhri",
      "Qingyu Chen",
      "Xiaofeng Liu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Mixture-of-Shape-Experts+%28MoSE%29%3A+End-to-End+Shape+Dictionary+Framework+to+Prompt+SAM+for+Generalizable+Medical+Segmentation+Jia+Wei+Xiaoqi+Zhao+Jonghye+Woo+Jinsong+Ouyang+Georges+El+Fakhri",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "J Wei",
        "id": "jLLrtFQAAAAJ"
      },
      {
        "name": "X Zhao",
        "id": "0EKcLI4AAAAJ"
      },
      {
        "name": "J Woo",
        "id": "lgLXQYkAAAAJ"
      },
      {
        "name": "J Ouyang",
        "id": "bp7V1bYAAAAJ"
      },
      {
        "name": "G El Fakhri",
        "id": "QIablCYAAAAJ"
      },
      {
        "name": "Q Chen",
        "id": "FSLotiMAAAAJ"
      },
      {
        "name": "X Liu",
        "id": "VighnTUAAAAJ"
      }
    ],
    "citation_count": null
  },
  {
    "arxiv_id": "2308.02776",
    "title": "Dual Degradation-Inspired Deep Unfolding Network for Low-Light Image Enhancement",
    "year": 2023,
    "published": "2023-08-05T03:07:11Z",
    "categories": [
      "cs.CV",
      "eess.IV"
    ],
    "abstract": "Although low-light image enhancement has achieved great stride based on deep enhancement models, most of them mainly stress on enhancement performance via an elaborated black-box network and rarely explore the physical significance of enhancement models. Towards this issue, we propose a Dual degrAdation-inSpired deep Unfolding network, termed DASUNet, for low-light image enhancement. Specifically, we construct a dual degradation model (DDM) to explicitly simulate the deterioration mechanism of l",
    "arxiv_url": "https://arxiv.org/abs/2308.02776v2",
    "pdf_url": "https://arxiv.org/pdf/2308.02776v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2308.02776",
    "arxiv_authors": [
      "Huake Wang",
      "Xingsong Hou",
      "Chengcu Liu",
      "Kaibing Zhang",
      "Xiangyong Cao",
      "Xueming Qian"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Dual+Degradation-Inspired+Deep+Unfolding+Network+for+Low-Light+Image+Enhancement+Huake+Wang+Xingsong+Hou+Chengcu+Liu+Kaibing+Zhang+Xiangyong+Cao",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "H Wang",
        "id": null
      },
      {
        "name": "X Hou",
        "id": null
      },
      {
        "name": "X Yan -",
        "id": null
      }
    ],
    "citation_count": 5
  },
  {
    "arxiv_id": "2303.11610",
    "title": "Novel Class Discovery for 3D Point Cloud Semantic Segmentation",
    "year": 2023,
    "published": "2023-03-21T06:10:39Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Novel class discovery (NCD) for semantic segmentation is the task of learning a model that can segment unlabelled (novel) classes using only the supervision from labelled (base) classes. This problem has recently been pioneered for 2D image data, but no work exists for 3D point cloud data. In fact, the assumptions made for 2D are loosely applicable to 3D in this case. This paper is presented to advance the state of the art on point cloud data analysis in four directions. Firstly, we address the ",
    "arxiv_url": "https://arxiv.org/abs/2303.11610v1",
    "pdf_url": "https://arxiv.org/pdf/2303.11610v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2303.11610",
    "arxiv_authors": [
      "Luigi Riz",
      "Cristiano Saltori",
      "Elisa Ricci",
      "Fabio Poiesi"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Novel+Class+Discovery+for+3D+Point+Cloud+Semantic+Segmentation+Luigi+Riz+Cristiano+Saltori+Elisa+Ricci+Fabio+Poiesi",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "L Riz",
        "id": "djO2pVUAAAAJ"
      },
      {
        "name": "C Saltori",
        "id": null
      },
      {
        "name": "E Ricci",
        "id": "xf1T870AAAAJ"
      },
      {
        "name": "F Poiesi -",
        "id": null
      }
    ],
    "citation_count": 314
  },
  {
    "arxiv_id": "2311.00401",
    "title": "A Spatial-Temporal Transformer based Framework For Human Pose Assessment And Correction in Education Scenarios",
    "year": 2023,
    "published": "2023-11-01T09:53:38Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Human pose assessment and correction play a crucial role in applications across various fields, including computer vision, robotics, sports analysis, healthcare, and entertainment. In this paper, we propose a Spatial-Temporal Transformer based Framework (STTF) for human pose assessment and correction in education scenarios such as physical exercises and science experiment. The framework comprising skeletal tracking, pose estimation, posture assessment, and posture correction modules to educate s",
    "arxiv_url": "https://arxiv.org/abs/2311.00401v1",
    "pdf_url": "https://arxiv.org/pdf/2311.00401v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2311.00401",
    "arxiv_authors": [
      "Wenyang Hu",
      "Kai Liu",
      "Libin Liu",
      "Huiliang Shang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=A+Spatial-Temporal+Transformer+based+Framework+For+Human+Pose+Assessment+And+Correction+in+Education+Scenarios+Wenyang+Hu+Kai+Liu+Libin+Liu+Huiliang+Shang",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "W Hu",
        "id": null
      },
      {
        "name": "K Liu",
        "id": null
      },
      {
        "name": "L Liu",
        "id": null
      },
      {
        "name": "H Shang -",
        "id": null
      }
    ],
    "citation_count": 1
  },
  {
    "arxiv_id": "2403.16481",
    "title": "REFRAME: Reflective Surface Real-Time Rendering for Mobile Devices",
    "year": 2024,
    "published": "2024-03-25T07:07:50Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "This work tackles the challenging task of achieving real-time novel view synthesis for reflective surfaces across various scenes. Existing real-time rendering methods, especially those based on meshes, often have subpar performance in modeling surfaces with rich view-dependent appearances. Our key idea lies in leveraging meshes for rendering acceleration while incorporating a novel approach to parameterize view-dependent information. We decompose the color into diffuse and specular, and model th",
    "arxiv_url": "https://arxiv.org/abs/2403.16481v2",
    "pdf_url": "https://arxiv.org/pdf/2403.16481v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2403.16481",
    "arxiv_authors": [
      "Chaojie Ji",
      "Yufeng Li",
      "Yiyi Liao"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=REFRAME%3A+Reflective+Surface+Real-Time+Rendering+for+Mobile+Devices+Chaojie+Ji+Yufeng+Li+Yiyi+Liao",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "C Ji",
        "id": "TNFNsWMAAAAJ"
      },
      {
        "name": "Y Li",
        "id": "c7KN8nsAAAAJ"
      },
      {
        "name": "Y Liao - European",
        "id": null
      }
    ],
    "citation_count": null
  },
  {
    "arxiv_id": "2402.18527",
    "title": "Defect Detection in Tire X-Ray Images: Conventional Methods Meet Deep Structures",
    "year": 2024,
    "published": "2024-02-28T18:07:47Z",
    "categories": [
      "cs.CV",
      "cs.LG",
      "eess.IV"
    ],
    "abstract": "This paper introduces a robust approach for automated defect detection in tire X-ray images by harnessing traditional feature extraction methods such as Local Binary Pattern (LBP) and Gray Level Co-Occurrence Matrix (GLCM) features, as well as Fourier and Wavelet-based features, complemented by advanced machine learning techniques. Recognizing the challenges inherent in the complex patterns and textures of tire X-ray images, the study emphasizes the significance of feature engineering to enhance",
    "arxiv_url": "https://arxiv.org/abs/2402.18527v1",
    "pdf_url": "https://arxiv.org/pdf/2402.18527v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2402.18527",
    "arxiv_authors": [
      "Andrei Cozma",
      "Landon Harris",
      "Hairong Qi",
      "Ping Ji",
      "Wenpeng Guo",
      "Song Yuan"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Defect+Detection+in+Tire+X-Ray+Images%3A+Conventional+Methods+Meet+Deep+Structures+Andrei+Cozma+Landon+Harris+Hairong+Qi+Ping+Ji+Wenpeng+Guo",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "A Cozma",
        "id": "uPEs-KwAAAAJ"
      },
      {
        "name": "L Harris",
        "id": null
      },
      {
        "name": "H Qi",
        "id": "GqnNG-kAAAAJ"
      },
      {
        "name": "P Ji",
        "id": null
      },
      {
        "name": "W Guo",
        "id": null
      },
      {
        "name": "S Yuan",
        "id": null
      }
    ],
    "citation_count": 4
  },
  {
    "arxiv_id": "2312.16279",
    "title": "Cloud-Device Collaborative Learning for Multimodal Large Language Models",
    "year": 2023,
    "published": "2023-12-26T18:46:14Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "The burgeoning field of Multimodal Large Language Models (MLLMs) has exhibited remarkable performance in diverse tasks such as captioning, commonsense reasoning, and visual scene understanding. However, the deployment of these large-scale MLLMs on client devices is hindered by their extensive model parameters, leading to a notable decline in generalization capabilities when these models are compressed for device deployment. Addressing this challenge, we introduce a Cloud-Device Collaborative Con",
    "arxiv_url": "https://arxiv.org/abs/2312.16279v1",
    "pdf_url": "https://arxiv.org/pdf/2312.16279v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2312.16279",
    "arxiv_authors": [
      "Guanqun Wang",
      "Jiaming Liu",
      "Chenxuan Li",
      "Junpeng Ma",
      "Yuan Zhang",
      "Xinyu Wei",
      "Kevin Zhang",
      "Maurice Chong",
      "Ray Zhang",
      "Yijiang Liu",
      "Shanghang Zhang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Cloud-Device+Collaborative+Learning+for+Multimodal+Large+Language+Models+Guanqun+Wang+Jiaming+Liu+Chenxuan+Li+Junpeng+Ma+Yuan+Zhang",
    "gs_search_success": false,
    "gs_authors": null,
    "citation_count": null,
    "error_type": "fetch_failed"
  },
  {
    "arxiv_id": "2308.13343",
    "title": "Squeeze aggregated excitation network",
    "year": 2023,
    "published": "2023-08-25T12:30:48Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Convolutional neural networks have spatial representations which read patterns in the vision tasks. Squeeze and excitation links the channel wise representations by explicitly modeling on channel level. Multi layer perceptrons learn global representations and in most of the models it is used often at the end after all convolutional layers to gather all the information learned before classification. We propose a method of inducing the global representations within channels to have better performa",
    "arxiv_url": "https://arxiv.org/abs/2308.13343v1",
    "pdf_url": "https://arxiv.org/pdf/2308.13343v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2308.13343",
    "arxiv_authors": [
      "Mahendran N"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Squeeze+aggregated+excitation+network+Mahendran+N",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "N Mahendran -",
        "id": null
      }
    ],
    "citation_count": 3
  },
  {
    "arxiv_id": "2301.13741",
    "title": "UPop: Unified and Progressive Pruning for Compressing Vision-Language Transformers",
    "year": 2023,
    "published": "2023-01-31T16:18:52Z",
    "categories": [
      "cs.CV",
      "cs.CL",
      "cs.LG"
    ],
    "abstract": "Real-world data contains a vast amount of multimodal information, among which vision and language are the two most representative modalities. Moreover, increasingly heavier models, \\textit{e}.\\textit{g}., Transformers, have attracted the attention of researchers to model compression. However, how to compress multimodal models, especially vison-language Transformers, is still under-explored. This paper proposes the \\textbf{U}nified and \\textbf{P}r\\textbf{o}gressive \\textbf{P}runing (\\textbf{\\emph",
    "arxiv_url": "https://arxiv.org/abs/2301.13741v3",
    "pdf_url": "https://arxiv.org/pdf/2301.13741v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2301.13741",
    "arxiv_authors": [
      "Dachuan Shi",
      "Chaofan Tao",
      "Ying Jin",
      "Zhendong Yang",
      "Chun Yuan",
      "Jiaqi Wang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=UPop%3A+Unified+and+Progressive+Pruning+for+Compressing+Vision-Language+Transformers+Dachuan+Shi+Chaofan+Tao+Ying+Jin+Zhendong+Yang+Chun+Yuan",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "D Shi",
        "id": "ejECvlYAAAAJ"
      },
      {
        "name": "C Tao",
        "id": "gjmfLroAAAAJ"
      },
      {
        "name": "Y Jin",
        "id": "RSqGfysAAAAJ"
      },
      {
        "name": "Z Yang",
        "id": "M9qKrogAAAAJ"
      },
      {
        "name": "C Yuan",
        "id": "fYdxi2sAAAAJ"
      },
      {
        "name": "J WangInternational",
        "id": null
      }
    ],
    "citation_count": 54
  },
  {
    "arxiv_id": "2312.04028",
    "title": "ImFace++: A Sophisticated Nonlinear 3D Morphable Face Model with Implicit Neural Representations",
    "year": 2023,
    "published": "2023-12-07T03:53:53Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Accurate representations of 3D faces are of paramount importance in various computer vision and graphics applications. However, the challenges persist due to the limitations imposed by data discretization and model linearity, which hinder the precise capture of identity and expression clues in current studies. This paper presents a novel 3D morphable face model, named ImFace++, to learn a sophisticated and continuous space with implicit neural representations. ImFace++ first constructs two expli",
    "arxiv_url": "https://arxiv.org/abs/2312.04028v3",
    "pdf_url": "https://arxiv.org/pdf/2312.04028v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2312.04028",
    "arxiv_authors": [
      "Mingwu Zheng",
      "Haiyu Zhang",
      "Hongyu Yang",
      "Liming Chen",
      "Di Huang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=ImFace%2B%2B%3A+A+Sophisticated+Nonlinear+3D+Morphable+Face+Model+with+Implicit+Neural+Representations+Mingwu+Zheng+Haiyu+Zhang+Hongyu+Yang+Liming+Chen+Di+Huang",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "M Zheng",
        "id": "MdizB60AAAAJ"
      },
      {
        "name": "H Zhang",
        "id": "9B1t_zIAAAAJ"
      },
      {
        "name": "H Yang",
        "id": "dnbjaWIAAAAJ"
      },
      {
        "name": "L Chen",
        "id": "VOPW5YYAAAAJ"
      },
      {
        "name": "D HuangIEEE Transactions on Pattern Analysis and Machine Intelligence",
        "id": null
      }
    ],
    "citation_count": 7
  },
  {
    "arxiv_id": "2407.09946",
    "title": "Low-Rank Interconnected Adaptation across Layers",
    "year": 2024,
    "published": "2024-07-13T17:03:16Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Low-rank adaptation (LoRA) is a widely used parameter-efficient fine-tuning (PEFT) method that learns weight updates $ŒîW = AB$ for pretrained weights $W$ through low-rank adapters $A$ and $B$. While LoRA ensures hardware efficiency, its low-rank weight updates limit adaptation performance. In this paper, we propose low-rank interconnected adaptation across layers (Lily), a novel PEFT method that introduces an interconnected framework with locally shared $A$ and globally shared $B$ experts. This ",
    "arxiv_url": "https://arxiv.org/abs/2407.09946v3",
    "pdf_url": "https://arxiv.org/pdf/2407.09946v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2407.09946",
    "arxiv_authors": [
      "Yibo Zhong",
      "Jinman Zhao",
      "Yao Zhou"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Low-Rank+Interconnected+Adaptation+across+Layers+Yibo+Zhong+Jinman+Zhao+Yao+Zhou",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "Y Zhong",
        "id": null
      },
      {
        "name": "J Zhao",
        "id": "33L5AEUAAAAJ"
      },
      {
        "name": "Y Zhou - Findings of the Association for",
        "id": null
      }
    ],
    "citation_count": 7
  },
  {
    "arxiv_id": "2307.03602",
    "title": "Depth Estimation Analysis of Orthogonally Divergent Fisheye Cameras with Distortion Removal",
    "year": 2023,
    "published": "2023-07-07T13:44:12Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Stereo vision systems have become popular in computer vision applications, such as 3D reconstruction, object tracking, and autonomous navigation. However, traditional stereo vision systems that use rectilinear lenses may not be suitable for certain scenarios due to their limited field of view. This has led to the popularity of vision systems based on one or multiple fisheye cameras in different orientations, which can provide a field of view of 180x180 degrees or more. However, fisheye cameras i",
    "arxiv_url": "https://arxiv.org/abs/2307.03602v1",
    "pdf_url": "https://arxiv.org/pdf/2307.03602v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2307.03602",
    "arxiv_authors": [
      "Matvei Panteleev",
      "Houari Bettahar"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Depth+Estimation+Analysis+of+Orthogonally+Divergent+Fisheye+Cameras+with+Distortion+Removal+Matvei+Panteleev+Houari+Bettahar",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "M Panteleev",
        "id": null
      },
      {
        "name": "H Bettahar -",
        "id": null
      }
    ],
    "citation_count": 1
  },
  {
    "arxiv_id": "2309.16257",
    "title": "Nondestructive chicken egg fertility detection using CNN-transfer learning algorithms",
    "year": 2023,
    "published": "2023-09-28T08:50:19Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "eess.IV"
    ],
    "abstract": "This study explored the application of CNN-Transfer Learning for nondestructive chicken egg fertility detection for precision poultry hatchery practices. Four models, VGG16, ResNet50, InceptionNet, and MobileNet, were trained and evaluated on a dataset (200 single egg images) using augmented images (rotation, flip, scale, translation, and reflection). Although the training results demonstrated that all models achieved high accuracy, indicating their ability to accurately learn and classify chick",
    "arxiv_url": "https://arxiv.org/abs/2309.16257v1",
    "pdf_url": "https://arxiv.org/pdf/2309.16257v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2309.16257",
    "arxiv_authors": [
      "Shoffan Saifullah",
      "Rafal Drezewski",
      "Anton Yudhana",
      "Andri Pranolo",
      "Wilis Kaswijanti",
      "Andiko Putro Suryotomo",
      "Seno Aji Putra",
      "Alin Khaliduzzaman",
      "Anton Satria Prabuwono",
      "Nathalie Japkowicz"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Nondestructive+chicken+egg+fertility+detection+using+CNN-transfer+learning+algorithms+Shoffan+Saifullah+Rafal+Drezewski+Anton+Yudhana+Andri+Pranolo+Wilis+Kaswijanti",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "S Saifullah",
        "id": "OvnHru0AAAAJ"
      },
      {
        "name": "R Drezewski",
        "id": "8oJfP0UAAAAJ"
      },
      {
        "name": "A Yudhana",
        "id": "JmfofJ4AAAAJ"
      },
      {
        "name": "A Pranolo",
        "id": "0jxWyZMAAAAJ"
      },
      {
        "name": "W Kaswijanti",
        "id": "gtdYsskAAAAJ"
      },
      {
        "name": "AP Suryotomo",
        "id": "rG79LDwAAAAJ"
      },
      {
        "name": "SA Putra",
        "id": null
      }
    ],
    "citation_count": 30
  },
  {
    "arxiv_id": "2310.01845",
    "title": "Zero-Shot Refinement of Buildings' Segmentation Models using SAM",
    "year": 2023,
    "published": "2023-10-03T07:19:59Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "abstract": "Foundation models have excelled in various tasks but are often evaluated on general benchmarks. The adaptation of these models for specific domains, such as remote sensing imagery, remains an underexplored area. In remote sensing, precise building instance segmentation is vital for applications like urban planning. While Convolutional Neural Networks (CNNs) perform well, their generalization can be limited. For this aim, we present a novel approach to adapt foundation models to address existing ",
    "arxiv_url": "https://arxiv.org/abs/2310.01845v2",
    "pdf_url": "https://arxiv.org/pdf/2310.01845v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2310.01845",
    "arxiv_authors": [
      "Ali Mayladan",
      "Hasan Nasrallah",
      "Hasan Moughnieh",
      "Mustafa Shukor",
      "Ali J. Ghandour"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Zero-Shot+Refinement+of+Buildings%27+Segmentation+Models+using+SAM+Ali+Mayladan+Hasan+Nasrallah+Hasan+Moughnieh+Mustafa+Shukor+Ali+J.+Ghandour",
    "gs_search_success": false,
    "gs_authors": null,
    "citation_count": null,
    "error_type": "fetch_failed"
  },
  {
    "arxiv_id": "2301.11558",
    "title": "Accelerating Guided Diffusion Sampling with Splitting Numerical Methods",
    "year": 2023,
    "published": "2023-01-27T06:48:29Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Guided diffusion is a technique for conditioning the output of a diffusion model at sampling time without retraining the network for each specific task. One drawback of diffusion models, however, is their slow sampling process. Recent techniques can accelerate unguided sampling by applying high-order numerical methods to the sampling process when viewed as differential equations. On the contrary, we discover that the same techniques do not work for guided sampling, and little has been explored a",
    "arxiv_url": "https://arxiv.org/abs/2301.11558v1",
    "pdf_url": "https://arxiv.org/pdf/2301.11558v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2301.11558",
    "arxiv_authors": [
      "Suttisak Wizadwongsa",
      "Supasorn Suwajanakorn"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Accelerating+Guided+Diffusion+Sampling+with+Splitting+Numerical+Methods+Suttisak+Wizadwongsa+Supasorn+Suwajanakorn",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "S Wizadwongsa",
        "id": "DMuuBQ4AAAAJ"
      },
      {
        "name": "S Suwajanakorn -",
        "id": null
      }
    ],
    "citation_count": 22
  },
  {
    "arxiv_id": "2408.00083",
    "title": "Localized Gaussian Splatting Editing with Contextual Awareness",
    "year": 2024,
    "published": "2024-07-31T18:00:45Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Recent text-guided generation of individual 3D object has achieved great success using diffusion priors. However, these methods are not suitable for object insertion and replacement tasks as they do not consider the background, leading to illumination mismatches within the environment. To bridge the gap, we introduce an illumination-aware 3D scene editing pipeline for 3D Gaussian Splatting (3DGS) representation. Our key observation is that inpainting by the state-of-the-art conditional 2D diffus",
    "arxiv_url": "https://arxiv.org/abs/2408.00083v2",
    "pdf_url": "https://arxiv.org/pdf/2408.00083v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2408.00083",
    "arxiv_authors": [
      "Hanyuan Xiao",
      "Yingshu Chen",
      "Huajian Huang",
      "Haolin Xiong",
      "Jing Yang",
      "Pratusha Prasad",
      "Yajie Zhao"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Localized+Gaussian+Splatting+Editing+with+Contextual+Awareness+Hanyuan+Xiao+Yingshu+Chen+Huajian+Huang+Haolin+Xiong+Jing+Yang",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "H Xiao",
        "id": "bKdDoYMAAAAJ"
      },
      {
        "name": "Y Chen",
        "id": "t-tYhTMAAAAJ"
      },
      {
        "name": "H Huang",
        "id": "rOhG9NoAAAAJ"
      },
      {
        "name": "H Xiong",
        "id": "fysda1gAAAAJ"
      },
      {
        "name": "J Yang",
        "id": "DFo1j88AAAAJ"
      },
      {
        "name": "P Prasad",
        "id": null
      },
      {
        "name": "Y Zhao2025 IEEE/CVF Winter",
        "id": null
      }
    ],
    "citation_count": 5
  },
  {
    "arxiv_id": "2311.05464",
    "title": "3DStyle-Diffusion: Pursuing Fine-grained Text-driven 3D Stylization with 2D Diffusion Models",
    "year": 2023,
    "published": "2023-11-09T15:51:27Z",
    "categories": [
      "cs.CV",
      "cs.MM"
    ],
    "abstract": "3D content creation via text-driven stylization has played a fundamental challenge to multimedia and graphics community. Recent advances of cross-modal foundation models (e.g., CLIP) have made this problem feasible. Those approaches commonly leverage CLIP to align the holistic semantics of stylized mesh with the given text prompt. Nevertheless, it is not trivial to enable more controllable stylization of fine-grained details in 3D meshes solely based on such semantic-level cross-modal supervisio",
    "arxiv_url": "https://arxiv.org/abs/2311.05464v1",
    "pdf_url": "https://arxiv.org/pdf/2311.05464v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2311.05464",
    "arxiv_authors": [
      "Haibo Yang",
      "Yang Chen",
      "Yingwei Pan",
      "Ting Yao",
      "Zhineng Chen",
      "Tao Mei"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=3DStyle-Diffusion%3A+Pursuing+Fine-grained+Text-driven+3D+Stylization+with+2D+Diffusion+Models+Haibo+Yang+Yang+Chen+Yingwei+Pan+Ting+Yao+Zhineng+Chen",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "H Yang",
        "id": "mW7YL-IAAAAJ"
      },
      {
        "name": "Y Chen",
        "id": "l20jCUQAAAAJ"
      },
      {
        "name": "Y Pan",
        "id": "2RxXFPoAAAAJ"
      },
      {
        "name": "T Yao",
        "id": "7Yc6yssAAAAJ"
      },
      {
        "name": "Z Chen",
        "id": "RS4jR14AAAAJ"
      },
      {
        "name": "T Mei",
        "id": "7Yq4wf4AAAAJ"
      }
    ],
    "citation_count": 27
  },
  {
    "arxiv_id": "2407.00362",
    "title": "JSCDS: A Core Data Selection Method with Jason-Shannon Divergence for Caries RGB Images-Efficient Learning",
    "year": 2024,
    "published": "2024-06-29T08:19:25Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Deep learning-based RGB caries detection improves the efficiency of caries identification and is crucial for preventing oral diseases. The performance of deep learning models depends on high-quality data and requires substantial training resources, making efficient deployment challenging. Core data selection, by eliminating low-quality and confusing data, aims to enhance training efficiency without significantly compromising model performance. However, distance-based data selection methods strug",
    "arxiv_url": "https://arxiv.org/abs/2407.00362v2",
    "pdf_url": "https://arxiv.org/pdf/2407.00362v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2407.00362",
    "arxiv_authors": [
      "Peiliang Zhang",
      "Yujia Tong",
      "Chenghu Du",
      "Chao Che",
      "Yongjun Zhu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=JSCDS%3A+A+Core+Data+Selection+Method+with+Jason-Shannon+Divergence+for+Caries+RGB+Images-Efficient+Learning+Peiliang+Zhang+Yujia+Tong+Chenghu+Du+Chao+Che+Yongjun+Zhu",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "P Zhang",
        "id": "KiRnrXgAAAAJ"
      },
      {
        "name": "Y Tong",
        "id": "RmJxq_8AAAAJ"
      },
      {
        "name": "C Du",
        "id": "pKh8bTwAAAAJ"
      },
      {
        "name": "C Che",
        "id": null
      },
      {
        "name": "Y Zhu -",
        "id": null
      }
    ],
    "citation_count": 2
  },
  {
    "arxiv_id": "2403.04634",
    "title": "Pix2Gif: Motion-Guided Diffusion for GIF Generation",
    "year": 2024,
    "published": "2024-03-07T16:18:28Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "We present Pix2Gif, a motion-guided diffusion model for image-to-GIF (video) generation. We tackle this problem differently by formulating the task as an image translation problem steered by text and motion magnitude prompts, as shown in teaser fig. To ensure that the model adheres to motion guidance, we propose a new motion-guided warping module to spatially transform the features of the source image conditioned on the two types of prompts. Furthermore, we introduce a perceptual loss to ensure ",
    "arxiv_url": "https://arxiv.org/abs/2403.04634v2",
    "pdf_url": "https://arxiv.org/pdf/2403.04634v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2403.04634",
    "arxiv_authors": [
      "Hitesh Kandala",
      "Jianfeng Gao",
      "Jianwei Yang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Pix2Gif%3A+Motion-Guided+Diffusion+for+GIF+Generation+Hitesh+Kandala+Jianfeng+Gao+Jianwei+Yang",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "H Kandala",
        "id": "Z36s8YMAAAAJ"
      },
      {
        "name": "J Gao",
        "id": "CQ1cqKkAAAAJ"
      },
      {
        "name": "J Yang - European",
        "id": null
      }
    ],
    "citation_count": 9
  },
  {
    "arxiv_id": "2405.01673",
    "title": "ShadowNav: Autonomous Global Localization for Lunar Navigation in Darkness",
    "year": 2024,
    "published": "2024-05-02T18:59:53Z",
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "abstract": "The ability to determine the pose of a rover in an inertial frame autonomously is a crucial capability necessary for the next generation of surface rover missions on other planetary bodies. Currently, most on-going rover missions utilize ground-in-the-loop interventions to manually correct for drift in the pose estimate and this human supervision bottlenecks the distance over which rovers can operate autonomously and carry out scientific measurements. In this paper, we present ShadowNav, an auto",
    "arxiv_url": "https://arxiv.org/abs/2405.01673v3",
    "pdf_url": "https://arxiv.org/pdf/2405.01673v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2405.01673",
    "arxiv_authors": [
      "Deegan Atha",
      "R. Michael Swan",
      "Abhishek Cauligi",
      "Anne Bettens",
      "Edwin Goh",
      "Dima Kogan",
      "Larry Matthies",
      "Masahiro Ono"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=ShadowNav%3A+Autonomous+Global+Localization+for+Lunar+Navigation+in+Darkness+Deegan+Atha+R.+Michael+Swan+Abhishek+Cauligi+Anne+Bettens+Edwin+Goh",
    "gs_search_success": false,
    "gs_authors": null,
    "citation_count": null,
    "error_type": "fetch_failed"
  },
  {
    "arxiv_id": "2408.14764",
    "title": "SynthDoc: Bilingual Documents Synthesis for Visual Document Understanding",
    "year": 2024,
    "published": "2024-08-27T03:31:24Z",
    "categories": [
      "cs.CV",
      "cs.MM"
    ],
    "abstract": "This paper introduces SynthDoc, a novel synthetic document generation pipeline designed to enhance Visual Document Understanding (VDU) by generating high-quality, diverse datasets that include text, images, tables, and charts. Addressing the challenges of data acquisition and the limitations of existing datasets, SynthDoc leverages publicly available corpora and advanced rendering tools to create a comprehensive and versatile dataset. Our experiments, conducted using the Donut model, demonstrate",
    "arxiv_url": "https://arxiv.org/abs/2408.14764v1",
    "pdf_url": "https://arxiv.org/pdf/2408.14764v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2408.14764",
    "arxiv_authors": [
      "Chuanghao Ding",
      "Xuejing Liu",
      "Wei Tang",
      "Juan Li",
      "Xiaoliang Wang",
      "Rui Zhao",
      "Cam-Tu Nguyen",
      "Fei Tan"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=SynthDoc%3A+Bilingual+Documents+Synthesis+for+Visual+Document+Understanding+Chuanghao+Ding+Xuejing+Liu+Wei+Tang+Juan+Li+Xiaoliang+Wang",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "C Ding",
        "id": null
      },
      {
        "name": "X Liu",
        "id": "SVQYcYcAAAAJ"
      },
      {
        "name": "W Tang",
        "id": "D-27eLIAAAAJ"
      },
      {
        "name": "J Li",
        "id": null
      },
      {
        "name": "X Wang",
        "id": null
      },
      {
        "name": "R Zhao",
        "id": "1c9oQNMAAAAJ"
      },
      {
        "name": "CT Nguyen",
        "id": "qaSjJ4wAAAAJ"
      },
      {
        "name": "F Tan",
        "id": "IhYATC0AAAAJ"
      }
    ],
    "citation_count": 3
  },
  {
    "arxiv_id": "2303.05068",
    "title": "Toward Unsupervised Realistic Visual Question Answering",
    "year": 2023,
    "published": "2023-03-09T06:58:29Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "The problem of realistic VQA (RVQA), where a model has to reject unanswerable questions (UQs) and answer answerable ones (AQs), is studied. We first point out 2 drawbacks in current RVQA research, where (1) datasets contain too many unchallenging UQs and (2) a large number of annotated UQs are required for training. To resolve the first drawback, we propose a new testing dataset, RGQA, which combines AQs from an existing VQA dataset with around 29K human-annotated UQs. These UQs consist of both ",
    "arxiv_url": "https://arxiv.org/abs/2303.05068v1",
    "pdf_url": "https://arxiv.org/pdf/2303.05068v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2303.05068",
    "arxiv_authors": [
      "Yuwei Zhang",
      "Chih-Hui Ho",
      "Nuno Vasconcelos"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Toward+Unsupervised+Realistic+Visual+Question+Answering+Yuwei+Zhang+Chih-Hui+Ho+Nuno+Vasconcelos",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "Y Zhang",
        "id": "nQyS0asAAAAJ"
      },
      {
        "name": "CH Ho",
        "id": "9GFLGgUAAAAJ"
      },
      {
        "name": "N Vasconcelos",
        "id": "Fykyo9gAAAAJ"
      }
    ],
    "citation_count": 5
  },
  {
    "arxiv_id": "2407.20563",
    "title": "Pyramid Coder: Hierarchical Code Generator for Compositional Visual Question Answering",
    "year": 2024,
    "published": "2024-07-30T05:36:43Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Visual question answering (VQA) is the task of providing accurate answers to natural language questions based on visual input. Programmatic VQA (PVQA) models have been gaining attention recently. These use large language models (LLMs) to formulate executable programs that address questions requiring complex visual reasoning. However, there are challenges in enabling LLMs to comprehend the usage of image processing modules and generate relevant code. To overcome these challenges, this paper intro",
    "arxiv_url": "https://arxiv.org/abs/2407.20563v1",
    "pdf_url": "https://arxiv.org/pdf/2407.20563v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2407.20563",
    "arxiv_authors": [
      "Ruoyue Shen",
      "Nakamasa Inoue",
      "Koichi Shinoda"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Pyramid+Coder%3A+Hierarchical+Code+Generator+for+Compositional+Visual+Question+Answering+Ruoyue+Shen+Nakamasa+Inoue+Koichi+Shinoda",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "R Shen",
        "id": "mc6y5JQAAAAJ"
      },
      {
        "name": "N Inoue",
        "id": null
      },
      {
        "name": "K Shinoda -",
        "id": null
      }
    ],
    "citation_count": 4
  },
  {
    "arxiv_id": "2310.20175",
    "title": "LFAA: Crafting Transferable Targeted Adversarial Examples with Low-Frequency Perturbations",
    "year": 2023,
    "published": "2023-10-31T04:54:55Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Deep neural networks are susceptible to adversarial attacks, which pose a significant threat to their security and reliability in real-world applications. The most notable adversarial attacks are transfer-based attacks, where an adversary crafts an adversarial example to fool one model, which can also fool other models. While previous research has made progress in improving the transferability of untargeted adversarial examples, the generation of targeted adversarial examples that can transfer b",
    "arxiv_url": "https://arxiv.org/abs/2310.20175v2",
    "pdf_url": "https://arxiv.org/pdf/2310.20175v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2310.20175",
    "arxiv_authors": [
      "Kunyu Wang",
      "Juluan Shi",
      "Wenxuan Wang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=LFAA%3A+Crafting+Transferable+Targeted+Adversarial+Examples+with+Low-Frequency+Perturbations+Kunyu+Wang+Juluan+Shi+Wenxuan+Wang",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "K Wang",
        "id": null
      },
      {
        "name": "J Shi",
        "id": "V1ylXBAAAAAJ"
      },
      {
        "name": "W Wang -",
        "id": null
      }
    ],
    "citation_count": 10
  },
  {
    "arxiv_id": "2304.05047",
    "title": "Semi-Supervised Relational Contrastive Learning",
    "year": 2023,
    "published": "2023-04-11T08:14:30Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Disease diagnosis from medical images via supervised learning is usually dependent on tedious, error-prone, and costly image labeling by medical experts. Alternatively, semi-supervised learning and self-supervised learning offer effectiveness through the acquisition of valuable insights from readily available unlabeled images. We present Semi-Supervised Relational Contrastive Learning (SRCL), a novel semi-supervised learning model that leverages self-supervised contrastive loss and sample relati",
    "arxiv_url": "https://arxiv.org/abs/2304.05047v2",
    "pdf_url": "https://arxiv.org/pdf/2304.05047v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2304.05047",
    "arxiv_authors": [
      "Attiano Purpura-Pontoniere",
      "Demetri Terzopoulos",
      "Adam Wang",
      "Abdullah-Al-Zubaer Imran"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Semi-Supervised+Relational+Contrastive+Learning+Attiano+Purpura-Pontoniere+Demetri+Terzopoulos+Adam+Wang+Abdullah-Al-Zubaer+Imran",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "A Purpura-Pontoniere",
        "id": null
      },
      {
        "name": "D Terzopoulos",
        "id": "pKuBFaQAAAAJ"
      },
      {
        "name": "A Wang",
        "id": "-YkSTCkAAAAJ"
      },
      {
        "name": "AAZ Imran",
        "id": "HI7UKCIAAAAJ"
      }
    ],
    "citation_count": 1
  },
  {
    "arxiv_id": "2411.05898",
    "title": "Integrating Object Detection Modality into Visual Language Model for Enhanced Autonomous Driving Agent",
    "year": 2024,
    "published": "2024-11-08T15:50:30Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.RO"
    ],
    "abstract": "In this paper, we propose a novel framework for enhancing visual comprehension in autonomous driving systems by integrating visual language models (VLMs) with additional visual perception module specialised in object detection. We extend the Llama-Adapter architecture by incorporating a YOLOS-based detection network alongside the CLIP perception network, addressing limitations in object detection and localisation. Our approach introduces camera ID-separators to improve multi-view processing, cru",
    "arxiv_url": "https://arxiv.org/abs/2411.05898v1",
    "pdf_url": "https://arxiv.org/pdf/2411.05898v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2411.05898",
    "arxiv_authors": [
      "Linfeng He",
      "Yiming Sun",
      "Sihao Wu",
      "Jiaxu Liu",
      "Xiaowei Huang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Integrating+Object+Detection+Modality+into+Visual+Language+Model+for+Enhanced+Autonomous+Driving+Agent+Linfeng+He+Yiming+Sun+Sihao+Wu+Jiaxu+Liu+Xiaowei+Huang",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "L He",
        "id": null
      },
      {
        "name": "Y Sun",
        "id": null
      },
      {
        "name": "S Wu",
        "id": "30KcfSMAAAAJ"
      },
      {
        "name": "J Liu",
        "id": "qVaHUdQAAAAJ"
      },
      {
        "name": "X Huang -",
        "id": null
      }
    ],
    "citation_count": 3
  },
  {
    "arxiv_id": "2407.12292",
    "title": "Any Target Can be Offense: Adversarial Example Generation via Generalized Latent Infection",
    "year": 2024,
    "published": "2024-07-17T03:24:09Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Targeted adversarial attack, which aims to mislead a model to recognize any image as a target object by imperceptible perturbations, has become a mainstream tool for vulnerability assessment of deep neural networks (DNNs). Since existing targeted attackers only learn to attack known target classes, they cannot generalize well to unknown classes. To tackle this issue, we propose $\\bf{G}$eneralized $\\bf{A}$dversarial attac$\\bf{KER}$ ($\\bf{GAKer}$), which is able to construct adversarial examples t",
    "arxiv_url": "https://arxiv.org/abs/2407.12292v1",
    "pdf_url": "https://arxiv.org/pdf/2407.12292v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2407.12292",
    "arxiv_authors": [
      "Youheng Sun",
      "Shengming Yuan",
      "Xuanhan Wang",
      "Lianli Gao",
      "Jingkuan Song"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Any+Target+Can+be+Offense%3A+Adversarial+Example+Generation+via+Generalized+Latent+Infection+Youheng+Sun+Shengming+Yuan+Xuanhan+Wang+Lianli+Gao+Jingkuan+Song",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "Y Sun",
        "id": null
      },
      {
        "name": "S Yuan",
        "id": "QjyQOJ8AAAAJ"
      },
      {
        "name": "X Wang",
        "id": "qMpygM0AAAAJ"
      },
      {
        "name": "L Gao",
        "id": "zsm2dpYAAAAJ"
      },
      {
        "name": "J Song - European",
        "id": null
      }
    ],
    "citation_count": 7
  },
  {
    "arxiv_id": "2306.08877",
    "title": "Linguistic Binding in Diffusion Models: Enhancing Attribute Correspondence through Attention Map Alignment",
    "year": 2023,
    "published": "2023-06-15T06:21:44Z",
    "categories": [
      "cs.CL",
      "cs.CV"
    ],
    "abstract": "Text-conditioned image generation models often generate incorrect associations between entities and their visual attributes. This reflects an impaired mapping between linguistic binding of entities and modifiers in the prompt and visual binding of the corresponding elements in the generated image. As one notable example, a query like \"a pink sunflower and a yellow flamingo\" may incorrectly produce an image of a yellow sunflower and a pink flamingo. To remedy this issue, we propose SynGen, an app",
    "arxiv_url": "https://arxiv.org/abs/2306.08877v3",
    "pdf_url": "https://arxiv.org/pdf/2306.08877v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2306.08877",
    "arxiv_authors": [
      "Royi Rassin",
      "Eran Hirsch",
      "Daniel Glickman",
      "Shauli Ravfogel",
      "Yoav Goldberg",
      "Gal Chechik"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Linguistic+Binding+in+Diffusion+Models%3A+Enhancing+Attribute+Correspondence+through+Attention+Map+Alignment+Royi+Rassin+Eran+Hirsch+Daniel+Glickman+Shauli+Ravfogel+Yoav+Goldberg",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "R Rassin",
        "id": "_6g4hxsAAAAJ"
      },
      {
        "name": "E Hirsch",
        "id": "GPsTrDEAAAAJ"
      },
      {
        "name": "D Glickman",
        "id": "ZYnU64UAAAAJ"
      },
      {
        "name": "S Ravfogel",
        "id": "x09r-T8AAAAJ"
      },
      {
        "name": "Y Goldberg",
        "id": "0rskDKgAAAAJ"
      },
      {
        "name": "G ChechikAdvances in Neural Information Processing Systems",
        "id": null
      }
    ],
    "citation_count": 145
  },
  {
    "arxiv_id": "2305.01939",
    "title": "Where We Have Arrived in Proving the Emergence of Sparse Symbolic Concepts in AI Models",
    "year": 2023,
    "published": "2023-05-03T07:32:28Z",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "abstract": "This study aims to prove the emergence of symbolic concepts (or more precisely, sparse primitive inference patterns) in well-trained deep neural networks (DNNs). Specifically, we prove the following three conditions for the emergence. (i) The high-order derivatives of the network output with respect to the input variables are all zero. (ii) The DNN can be used on occluded samples and when the input sample is less occluded, the DNN will yield higher confidence. (iii) The confidence of the DNN doe",
    "arxiv_url": "https://arxiv.org/abs/2305.01939v2",
    "pdf_url": "https://arxiv.org/pdf/2305.01939v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2305.01939",
    "arxiv_authors": [
      "Qihan Ren",
      "Jiayang Gao",
      "Wen Shen",
      "Quanshi Zhang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Where+We+Have+Arrived+in+Proving+the+Emergence+of+Sparse+Symbolic+Concepts+in+AI+Models+Qihan+Ren+Jiayang+Gao+Wen+Shen+Quanshi+Zhang",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "Q Ren",
        "id": "ybTy_DwAAAAJ"
      },
      {
        "name": "J Gao",
        "id": null
      },
      {
        "name": "W Shen",
        "id": null
      },
      {
        "name": "Q Zhang -",
        "id": null
      }
    ],
    "citation_count": 12
  },
  {
    "arxiv_id": "2404.11884",
    "title": "Seeing Motion at Nighttime with an Event Camera",
    "year": 2024,
    "published": "2024-04-18T03:58:27Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "We focus on a very challenging task: imaging at nighttime dynamic scenes. Most previous methods rely on the low-light enhancement of a conventional RGB camera. However, they would inevitably face a dilemma between the long exposure time of nighttime and the motion blur of dynamic scenes. Event cameras react to dynamic changes with higher temporal resolution (microsecond) and higher dynamic range (120dB), offering an alternative solution. In this work, we present a novel nighttime dynamic imaging",
    "arxiv_url": "https://arxiv.org/abs/2404.11884v1",
    "pdf_url": "https://arxiv.org/pdf/2404.11884v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2404.11884",
    "arxiv_authors": [
      "Haoyue Liu",
      "Shihan Peng",
      "Lin Zhu",
      "Yi Chang",
      "Hanyu Zhou",
      "Luxin Yan"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Seeing+Motion+at+Nighttime+with+an+Event+Camera+Haoyue+Liu+Shihan+Peng+Lin+Zhu+Yi+Chang+Hanyu+Zhou",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "H Liu",
        "id": "DadbHdAAAAAJ"
      },
      {
        "name": "S Peng",
        "id": "Xd6FVrcAAAAJ"
      },
      {
        "name": "L Zhu",
        "id": "32d6xfEAAAAJ"
      },
      {
        "name": "Y Chang",
        "id": "I1nZ67YAAAAJ"
      },
      {
        "name": "H Zhou",
        "id": "bRXguCgAAAAJ"
      },
      {
        "name": "L Yan",
        "id": null
      }
    ],
    "citation_count": 36
  },
  {
    "arxiv_id": "2501.09815",
    "title": "Lossy Compression with Pretrained Diffusion Models",
    "year": 2025,
    "published": "2025-01-16T20:02:13Z",
    "categories": [
      "cs.CV",
      "eess.IV"
    ],
    "abstract": "We apply the DiffC algorithm (Theis et al. 2022) to Stable Diffusion 1.5, 2.1, XL, and Flux-dev, and demonstrate that these pretrained models are remarkably capable lossy image compressors. A principled algorithm for lossy compression using pretrained diffusion models has been understood since at least Ho et al. 2020, but challenges in reverse-channel coding have prevented such algorithms from ever being fully implemented. We introduce simple workarounds that lead to the first complete implement",
    "arxiv_url": "https://arxiv.org/abs/2501.09815v1",
    "pdf_url": "https://arxiv.org/pdf/2501.09815v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2501.09815",
    "arxiv_authors": [
      "Jeremy Vonderfecht",
      "Feng Liu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Lossy+Compression+with+Pretrained+Diffusion+Models+Jeremy+Vonderfecht+Feng+Liu",
    "gs_search_success": false,
    "gs_authors": null,
    "citation_count": null,
    "error_type": "fetch_failed"
  },
  {
    "arxiv_id": "2304.00837",
    "title": "Disorder-invariant Implicit Neural Representation",
    "year": 2023,
    "published": "2023-04-03T09:28:48Z",
    "categories": [
      "cs.CV",
      "eess.SP"
    ],
    "abstract": "Implicit neural representation (INR) characterizes the attributes of a signal as a function of corresponding coordinates which emerges as a sharp weapon for solving inverse problems. However, the expressive power of INR is limited by the spectral bias in the network training. In this paper, we find that such a frequency-related problem could be greatly solved by re-arranging the coordinates of the input signal, for which we propose the disorder-invariant implicit neural representation (DINER) by",
    "arxiv_url": "https://arxiv.org/abs/2304.00837v1",
    "pdf_url": "https://arxiv.org/pdf/2304.00837v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2304.00837",
    "arxiv_authors": [
      "Hao Zhu",
      "Shaowen Xie",
      "Zhen Liu",
      "Fengyi Liu",
      "Qi Zhang",
      "You Zhou",
      "Yi Lin",
      "Zhan Ma",
      "Xun Cao"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Disorder-invariant+Implicit+Neural+Representation+Hao+Zhu+Shaowen+Xie+Zhen+Liu+Fengyi+Liu+Qi+Zhang",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "H Zhu",
        "id": null
      },
      {
        "name": "S Xie",
        "id": null
      },
      {
        "name": "Z Liu",
        "id": "d-S9cjIAAAAJ"
      },
      {
        "name": "F Liu",
        "id": null
      },
      {
        "name": "Q Zhang",
        "id": "2vFjhHMAAAAJ"
      },
      {
        "name": "Y Zhou",
        "id": "eAEaH6IAAAAJ"
      },
      {
        "name": "Y Lin",
        "id": null
      },
      {
        "name": "Z Ma",
        "id": "78KxtRMAAAAJ"
      },
      {
        "name": "X CaoIEEE Transactions on Pattern Analysis and Machine Intelligence",
        "id": null
      }
    ],
    "citation_count": 27
  },
  {
    "arxiv_id": "2406.17450",
    "title": "Pseudo Labelling for Enhanced Masked Autoencoders",
    "year": 2024,
    "published": "2024-06-25T10:41:45Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Masked Image Modeling (MIM)-based models, such as SdAE, CAE, GreenMIM, and MixAE, have explored different strategies to enhance the performance of Masked Autoencoders (MAE) by modifying prediction, loss functions, or incorporating additional architectural components. In this paper, we propose an enhanced approach that boosts MAE performance by integrating pseudo labelling for both class and data tokens, alongside replacing the traditional pixel-level reconstruction with token-level reconstructio",
    "arxiv_url": "https://arxiv.org/abs/2406.17450v1",
    "pdf_url": "https://arxiv.org/pdf/2406.17450v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2406.17450",
    "arxiv_authors": [
      "Srinivasa Rao Nandam",
      "Sara Atito",
      "Zhenhua Feng",
      "Josef Kittler",
      "Muhammad Awais"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Pseudo+Labelling+for+Enhanced+Masked+Autoencoders+Srinivasa+Rao+Nandam+Sara+Atito+Zhenhua+Feng+Josef+Kittler+Muhammad+Awais",
    "gs_search_success": false,
    "gs_authors": null,
    "citation_count": null,
    "error_type": "multiple_matches"
  },
  {
    "arxiv_id": "2503.15625",
    "title": "EarthScape: A Multimodal Dataset for Surficial Geologic Mapping and Earth Surface Analysis",
    "year": 2025,
    "published": "2025-03-19T18:23:48Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Surficial geologic mapping is essential for understanding Earth surface processes, addressing modern challenges such as climate change and national security, and supporting common applications in engineering and resource management. However, traditional mapping methods are labor-intensive, limiting spatial coverage and introducing potential biases. To address these limitations, we introduce EarthScape, a novel, AI-ready multimodal dataset specifically designed for surficial geologic mapping and ",
    "arxiv_url": "https://arxiv.org/abs/2503.15625v1",
    "pdf_url": "https://arxiv.org/pdf/2503.15625v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2503.15625",
    "arxiv_authors": [
      "Matthew Massey",
      "Abdullah-Al-Zubaer Imran"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=EarthScape%3A+A+Multimodal+Dataset+for+Surficial+Geologic+Mapping+and+Earth+Surface+Analysis+Matthew+Massey+Abdullah-Al-Zubaer+Imran",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "M Massey",
        "id": "UPvHGbQAAAAJ"
      },
      {
        "name": "AAZ Imran -",
        "id": null
      }
    ],
    "citation_count": null
  },
  {
    "arxiv_id": "2411.01547",
    "title": "Decoupling Dark Knowledge via Block-wise Logit Distillation for Feature-level Alignment",
    "year": 2024,
    "published": "2024-11-03T12:42:16Z",
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "abstract": "Knowledge Distillation (KD), a learning manner with a larger teacher network guiding a smaller student network, transfers dark knowledge from the teacher to the student via logits or intermediate features, with the aim of producing a well-performed lightweight model. Notably, many subsequent feature-based KD methods outperformed the earliest logit-based KD method and iteratively generated numerous state-of-the-art distillation methods. Nevertheless, recent work has uncovered the potential of the",
    "arxiv_url": "https://arxiv.org/abs/2411.01547v2",
    "pdf_url": "https://arxiv.org/pdf/2411.01547v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2411.01547",
    "arxiv_authors": [
      "Chengting Yu",
      "Fengzhao Zhang",
      "Ruizhe Chen",
      "Aili Wang",
      "Zuozhu Liu",
      "Shurun Tan",
      "Er-Ping Li"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Decoupling+Dark+Knowledge+via+Block-wise+Logit+Distillation+for+Feature-level+Alignment+Chengting+Yu+Fengzhao+Zhang+Ruizhe+Chen+Aili+Wang+Zuozhu+Liu",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "C Yu",
        "id": "7oeTzcwAAAAJ"
      },
      {
        "name": "F Zhang",
        "id": null
      },
      {
        "name": "R Chen",
        "id": "Wr2K2sMAAAAJ"
      },
      {
        "name": "A Wang",
        "id": "WurRhGMAAAAJ"
      },
      {
        "name": "Z Liu",
        "id": "h602wLIAAAAJ"
      },
      {
        "name": "S Tan",
        "id": "2YKA6JcAAAAJ"
      },
      {
        "name": "EP LiIEEE Transactions on Artificial Intelligence",
        "id": null
      }
    ],
    "citation_count": 5
  },
  {
    "arxiv_id": "2304.10465",
    "title": "Implicit Temporal Modeling with Learnable Alignment for Video Recognition",
    "year": 2023,
    "published": "2023-04-20T17:11:01Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Contrastive language-image pretraining (CLIP) has demonstrated remarkable success in various image tasks. However, how to extend CLIP with effective temporal modeling is still an open and crucial problem. Existing factorized or joint spatial-temporal modeling trades off between the efficiency and performance. While modeling temporal information within straight through tube is widely adopted in literature, we find that simple frame alignment already provides enough essence without temporal attent",
    "arxiv_url": "https://arxiv.org/abs/2304.10465v2",
    "pdf_url": "https://arxiv.org/pdf/2304.10465v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2304.10465",
    "arxiv_authors": [
      "Shuyuan Tu",
      "Qi Dai",
      "Zuxuan Wu",
      "Zhi-Qi Cheng",
      "Han Hu",
      "Yu-Gang Jiang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Implicit+Temporal+Modeling+with+Learnable+Alignment+for+Video+Recognition+Shuyuan+Tu+Qi+Dai+Zuxuan+Wu+Zhi-Qi+Cheng+Han+Hu",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "S Tu",
        "id": "nVND1VMAAAAJ"
      },
      {
        "name": "Q Dai",
        "id": "NSJY12IAAAAJ"
      },
      {
        "name": "Z Wu",
        "id": "7t12hVkAAAAJ"
      },
      {
        "name": "ZQ Cheng",
        "id": "uB2He2UAAAAJ"
      },
      {
        "name": "H Hu",
        "id": null
      },
      {
        "name": "YG Jiang",
        "id": "f3_FP8AAAAAJ"
      }
    ],
    "citation_count": 69
  },
  {
    "arxiv_id": "2409.07904",
    "title": "FACT: Feature Adaptive Continual-learning Tracker for Multiple Object Tracking",
    "year": 2024,
    "published": "2024-09-12T10:14:48Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Multiple object tracking (MOT) involves identifying multiple targets and assigning them corresponding IDs within a video sequence, where occlusions are often encountered. Recent methods address occlusions using appearance cues through online learning techniques to improve adaptivity or offline learning techniques to utilize temporal information from videos. However, most existing online learning-based MOT methods are unable to learn from all past tracking information to improve adaptivity on lon",
    "arxiv_url": "https://arxiv.org/abs/2409.07904v1",
    "pdf_url": "https://arxiv.org/pdf/2409.07904v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2409.07904",
    "arxiv_authors": [
      "Rongzihan Song",
      "Zhenyu Weng",
      "Huiping Zhuang",
      "Jinchang Ren",
      "Yongming Chen",
      "Zhiping Lin"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=FACT%3A+Feature+Adaptive+Continual-learning+Tracker+for+Multiple+Object+Tracking+Rongzihan+Song+Zhenyu+Weng+Huiping+Zhuang+Jinchang+Ren+Yongming+Chen",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "R Song",
        "id": null
      },
      {
        "name": "Z Weng",
        "id": "76QVIroAAAAJ"
      },
      {
        "name": "H Zhuang",
        "id": "vCXxuLkAAAAJ"
      },
      {
        "name": "J Ren",
        "id": "Vsx9P-gAAAAJ"
      },
      {
        "name": "Y Chen",
        "id": null
      },
      {
        "name": "Z Lin",
        "id": "92D0pkwAAAAJ"
      }
    ],
    "citation_count": null
  },
  {
    "arxiv_id": "2303.10839",
    "title": "MXM-CLR: A Unified Framework for Contrastive Learning of Multifold Cross-Modal Representations",
    "year": 2023,
    "published": "2023-03-20T02:51:53Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Multifold observations are common for different data modalities, e.g., a 3D shape can be represented by multi-view images and an image can be described with different captions. Existing cross-modal contrastive representation learning (XM-CLR) methods such as CLIP are not fully suitable for multifold data as they only consider one positive pair and treat other pairs as negative when computing the contrastive loss. In this paper, we propose MXM-CLR, a unified framework for contrastive learning of ",
    "arxiv_url": "https://arxiv.org/abs/2303.10839v2",
    "pdf_url": "https://arxiv.org/pdf/2303.10839v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2303.10839",
    "arxiv_authors": [
      "Ye Wang",
      "Bowei Jiang",
      "Changqing Zou",
      "Rui Ma"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=MXM-CLR%3A+A+Unified+Framework+for+Contrastive+Learning+of+Multifold+Cross-Modal+Representations+Ye+Wang+Bowei+Jiang+Changqing+Zou+Rui+Ma",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "Y Wang",
        "id": "F3Xye5cAAAAJ"
      },
      {
        "name": "B Jiang",
        "id": null
      },
      {
        "name": "C Zou",
        "id": "kj5HiGgAAAAJ"
      },
      {
        "name": "R Ma -",
        "id": null
      }
    ],
    "citation_count": 6
  },
  {
    "arxiv_id": "2304.10448",
    "title": "ReLight My NeRF: A Dataset for Novel View Synthesis and Relighting of Real World Objects",
    "year": 2023,
    "published": "2023-04-20T16:43:58Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "In this paper, we focus on the problem of rendering novel views from a Neural Radiance Field (NeRF) under unobserved light conditions. To this end, we introduce a novel dataset, dubbed ReNe (Relighting NeRF), framing real world objects under one-light-at-time (OLAT) conditions, annotated with accurate ground-truth camera and light poses. Our acquisition pipeline leverages two robotic arms holding, respectively, a camera and an omni-directional point-wise light source. We release a total of 20 sc",
    "arxiv_url": "https://arxiv.org/abs/2304.10448v1",
    "pdf_url": "https://arxiv.org/pdf/2304.10448v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2304.10448",
    "arxiv_authors": [
      "Marco Toschi",
      "Riccardo De Matteo",
      "Riccardo Spezialetti",
      "Daniele De Gregorio",
      "Luigi Di Stefano",
      "Samuele Salti"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=ReLight+My+NeRF%3A+A+Dataset+for+Novel+View+Synthesis+and+Relighting+of+Real+World+Objects+Marco+Toschi+Riccardo+De+Matteo+Riccardo+Spezialetti+Daniele+De+Gregorio+Luigi+Di+Stefano",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "M Toschi",
        "id": null
      },
      {
        "name": "R De Matteo",
        "id": "kHEwpcIAAAAJ"
      },
      {
        "name": "R Spezialetti",
        "id": "DYADxJAAAAAJ"
      },
      {
        "name": "D De Gregorio",
        "id": "3T5NT24AAAAJ"
      },
      {
        "name": "L Di Stefano",
        "id": "xZVTzyAAAAAJ"
      },
      {
        "name": "S Salti",
        "id": "1kcIJG0AAAAJ"
      }
    ],
    "citation_count": 58
  },
  {
    "arxiv_id": "2502.06893",
    "title": "A New Hybrid Intelligent Approach for Multimodal Detection of Suspected Disinformation on TikTok",
    "year": 2025,
    "published": "2025-02-09T12:37:48Z",
    "categories": [
      "cs.CV",
      "cs.CL",
      "cs.MM",
      "cs.SC"
    ],
    "abstract": "In the context of the rapid dissemination of multimedia content, identifying disinformation on social media platforms such as TikTok represents a significant challenge. This study introduces a hybrid framework that combines the computational power of deep learning with the interpretability of fuzzy logic to detect suspected disinformation in TikTok videos. The methodology is comprised of two core components: a multimodal feature analyser that extracts and evaluates data from text, audio, and vid",
    "arxiv_url": "https://arxiv.org/abs/2502.06893v1",
    "pdf_url": "https://arxiv.org/pdf/2502.06893v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2502.06893",
    "arxiv_authors": [
      "Jared D. T. Guerrero-Sosa",
      "Andres Montoro-Montarroso",
      "Francisco P. Romero",
      "Jesus Serrano-Guerrero",
      "Jose A. Olivas"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=A+New+Hybrid+Intelligent+Approach+for+Multimodal+Detection+of+Suspected+Disinformation+on+TikTok+Jared+D.+T.+Guerrero-Sosa+Andres+Montoro-Montarroso+Francisco+P.+Romero+Jesus+Serrano-Guerrero+Jose+A.+Olivas",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "JDT Guerrero-Sosa",
        "id": "ST8DBbgAAAAJ"
      },
      {
        "name": "A Montoro-Montarroso",
        "id": "dfQibTMAAAAJ"
      },
      {
        "name": "FP Romero",
        "id": "N-SipHEAAAAJ"
      },
      {
        "name": "J Serrano-Guerrero",
        "id": "uTtzV1UAAAAJ"
      },
      {
        "name": "JA Olivas",
        "id": "Lx3Dc9cAAAAJ"
      }
    ],
    "citation_count": 1
  },
  {
    "arxiv_id": "2402.08333",
    "title": "Scribble-based fast weak-supervision and interactive corrections for segmenting whole slide images",
    "year": 2024,
    "published": "2024-02-13T09:57:35Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "This paper proposes a dynamic interactive and weakly supervised segmentation method with minimal user interactions to address two major challenges in the segmentation of whole slide histopathology images. First, the lack of hand-annotated datasets to train algorithms. Second, the lack of interactive paradigms to enable a dialogue between the pathologist and the machine, which can be a major obstacle for use in clinical routine.   We therefore propose a fast and user oriented method to bridge thi",
    "arxiv_url": "https://arxiv.org/abs/2402.08333v1",
    "pdf_url": "https://arxiv.org/pdf/2402.08333v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2402.08333",
    "arxiv_authors": [
      "Antoine Habis",
      "Roy Rosman Nathanson",
      "Vannary Meas-Yedid",
      "Elsa D. Angelini",
      "Jean-Christophe Olivo-Marin"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Scribble-based+fast+weak-supervision+and+interactive+corrections+for+segmenting+whole+slide+images+Antoine+Habis+Roy+Rosman+Nathanson+Vannary+Meas-Yedid+Elsa+D.+Angelini+Jean-Christophe+Olivo-Marin",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "A Habis",
        "id": "DPf-rjEAAAAJ"
      },
      {
        "name": "RR Nathanson",
        "id": null
      },
      {
        "name": "V Meas-Yedid",
        "id": "njjc9OgAAAAJ"
      },
      {
        "name": "ED Angelini",
        "id": "Fvy_0I4AAAAJ"
      },
      {
        "name": "JC Olivo-Marin",
        "id": "UuEMMkIAAAAJ"
      }
    ],
    "citation_count": null
  },
  {
    "arxiv_id": "2503.13265",
    "title": "FlexWorld: Progressively Expanding 3D Scenes for Flexiable-View Synthesis",
    "year": 2025,
    "published": "2025-03-17T15:18:38Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Generating flexible-view 3D scenes, including 360¬∞ rotation and zooming, from single images is challenging due to a lack of 3D data. To this end, we introduce FlexWorld, a novel framework consisting of two key components: (1) a strong video-to-video (V2V) diffusion model to generate high-quality novel view images from incomplete input rendered from a coarse scene, and (2) a progressive expansion process to construct a complete 3D scene. In particular, leveraging an advanced pre-trained video mod",
    "arxiv_url": "https://arxiv.org/abs/2503.13265v2",
    "pdf_url": "https://arxiv.org/pdf/2503.13265v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2503.13265",
    "arxiv_authors": [
      "Luxi Chen",
      "Zihan Zhou",
      "Min Zhao",
      "Yikai Wang",
      "Ge Zhang",
      "Wenhao Huang",
      "Hao Sun",
      "Ji-Rong Wen",
      "Chongxuan Li"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=FlexWorld%3A+Progressively+Expanding+3D+Scenes+for+Flexiable-View+Synthesis+Luxi+Chen+Zihan+Zhou+Min+Zhao+Yikai+Wang+Ge+Zhang",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "L Chen",
        "id": "Il0nwZMAAAAJ"
      },
      {
        "name": "Z Zhou",
        "id": null
      },
      {
        "name": "M Zhao",
        "id": "ExIZrLAAAAAJ"
      },
      {
        "name": "Y Wang",
        "id": "MnW5aegAAAAJ"
      },
      {
        "name": "G Zhang",
        "id": "qyTrq4kAAAAJ"
      },
      {
        "name": "W Huang",
        "id": null
      },
      {
        "name": "H Sun",
        "id": null
      },
      {
        "name": "JR Wen",
        "id": "tbxCHJgAAAAJ"
      },
      {
        "name": "C Li",
        "id": "UKMcQn4AAAAJ"
      }
    ],
    "citation_count": 8
  },
  {
    "arxiv_id": "2406.06946",
    "title": "Sparse Bayesian Networks: Efficient Uncertainty Quantification in Medical Image Analysis",
    "year": 2024,
    "published": "2024-06-11T05:12:00Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Efficiently quantifying predictive uncertainty in medical images remains a challenge. While Bayesian neural networks (BNN) offer predictive uncertainty, they require substantial computational resources to train. Although Bayesian approximations such as ensembles have shown promise, they still suffer from high training and inference costs. Existing approaches mainly address the costs of BNN inference post-training, with little focus on improving training efficiency and reducing parameter complexi",
    "arxiv_url": "https://arxiv.org/abs/2406.06946v1",
    "pdf_url": "https://arxiv.org/pdf/2406.06946v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2406.06946",
    "arxiv_authors": [
      "Zeinab Abboud",
      "Herve Lombaert",
      "Samuel Kadoury"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Sparse+Bayesian+Networks%3A+Efficient+Uncertainty+Quantification+in+Medical+Image+Analysis+Zeinab+Abboud+Herve+Lombaert+Samuel+Kadoury",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "Z Abboud",
        "id": "YnBsT_QAAAAJ"
      },
      {
        "name": "H Lombaert",
        "id": "KQbyRzIAAAAJ"
      },
      {
        "name": "S Kadoury - International",
        "id": null
      }
    ],
    "citation_count": 4
  },
  {
    "arxiv_id": "2303.07240",
    "title": "PMC-CLIP: Contrastive Language-Image Pre-training using Biomedical Documents",
    "year": 2023,
    "published": "2023-03-13T16:13:16Z",
    "categories": [
      "cs.CV",
      "cs.CL",
      "cs.LG",
      "cs.MM"
    ],
    "abstract": "Foundation models trained on large-scale dataset gain a recent surge in CV and NLP. In contrast, development in biomedical domain lags far behind due to data scarcity. To address this issue, we build and release PMC-OA, a biomedical dataset with 1.6M image-caption pairs collected from PubMedCentral's OpenAccess subset, which is 8 times larger than before. PMC-OA covers diverse modalities or diseases, with majority of the image-caption samples aligned at finer-grained level, i.e., subfigure and s",
    "arxiv_url": "https://arxiv.org/abs/2303.07240v1",
    "pdf_url": "https://arxiv.org/pdf/2303.07240v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2303.07240",
    "arxiv_authors": [
      "Weixiong Lin",
      "Ziheng Zhao",
      "Xiaoman Zhang",
      "Chaoyi Wu",
      "Ya Zhang",
      "Yanfeng Wang",
      "Weidi Xie"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=PMC-CLIP%3A+Contrastive+Language-Image+Pre-training+using+Biomedical+Documents+Weixiong+Lin+Ziheng+Zhao+Xiaoman+Zhang+Chaoyi+Wu+Ya+Zhang",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "W Lin",
        "id": "Ka57qcUAAAAJ"
      },
      {
        "name": "Z Zhao",
        "id": "NYNKEhYAAAAJ"
      },
      {
        "name": "X Zhang",
        "id": "Zno4WggAAAAJ"
      },
      {
        "name": "C Wu",
        "id": "ZLHTzHEAAAAJ"
      },
      {
        "name": "Y Zhang",
        "id": "pbjw9sMAAAAJ"
      },
      {
        "name": "Y Wang",
        "id": "x_sgJskAAAAJ"
      },
      {
        "name": "W XieInternational",
        "id": null
      }
    ],
    "citation_count": 297
  },
  {
    "arxiv_id": "2305.16269",
    "title": "UDPM: Upsampling Diffusion Probabilistic Models",
    "year": 2023,
    "published": "2023-05-25T17:25:14Z",
    "categories": [
      "cs.CV",
      "cs.LG",
      "eess.IV"
    ],
    "abstract": "Denoising Diffusion Probabilistic Models (DDPM) have recently gained significant attention. DDPMs compose a Markovian process that begins in the data domain and gradually adds noise until reaching pure white noise. DDPMs generate high-quality samples from complex data distributions by defining an inverse process and training a deep neural network to learn this mapping. However, these models are inefficient because they require many diffusion steps to produce aesthetically pleasing samples. Addit",
    "arxiv_url": "https://arxiv.org/abs/2305.16269v3",
    "pdf_url": "https://arxiv.org/pdf/2305.16269v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2305.16269",
    "arxiv_authors": [
      "Shady Abu-Hussein",
      "Raja Giryes"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=UDPM%3A+Upsampling+Diffusion+Probabilistic+Models+Shady+Abu-Hussein+Raja+Giryes",
    "gs_search_success": false,
    "gs_authors": null,
    "citation_count": null,
    "error_type": "multiple_matches"
  },
  {
    "arxiv_id": "2411.07544",
    "title": "Depthwise Separable Convolutions with Deep Residual Convolutions",
    "year": 2024,
    "published": "2024-11-12T04:47:32Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "The recent advancement of edge computing enables researchers to optimize various deep learning architectures to employ them in edge devices. In this study, we aim to optimize Xception architecture which is one of the most popular deep learning algorithms for computer vision applications. The Xception architecture is highly effective for object detection tasks. However, it comes with a significant computational cost. The computational complexity of Xception sometimes hinders its deployment on res",
    "arxiv_url": "https://arxiv.org/abs/2411.07544v1",
    "pdf_url": "https://arxiv.org/pdf/2411.07544v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2411.07544",
    "arxiv_authors": [
      "Md Arid Hasan",
      "Krishno Dey"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Depthwise+Separable+Convolutions+with+Deep+Residual+Convolutions+Md+Arid+Hasan+Krishno+Dey",
    "gs_search_success": false,
    "gs_authors": null,
    "citation_count": null,
    "error_type": "multiple_matches"
  },
  {
    "arxiv_id": "2401.15293",
    "title": "SkipViT: Speeding Up Vision Transformers with a Token-Level Skip Connection",
    "year": 2024,
    "published": "2024-01-27T04:24:49Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "abstract": "Vision transformers are known to be more computationally and data-intensive than CNN models. These transformer models such as ViT, require all the input image tokens to learn the relationship among them. However, many of these tokens are not informative and may contain irrelevant information such as unrelated background or unimportant scenery. These tokens are overlooked by the multi-head self-attention (MHSA), resulting in many redundant and unnecessary computations in MHSA and the feed-forward",
    "arxiv_url": "https://arxiv.org/abs/2401.15293v1",
    "pdf_url": "https://arxiv.org/pdf/2401.15293v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2401.15293",
    "arxiv_authors": [
      "Foozhan Ataiefard",
      "Walid Ahmed",
      "Habib Hajimolahoseini",
      "Saina Asani",
      "Farnoosh Javadi",
      "Mohammad Hassanpour",
      "Omar Mohamed Awad",
      "Austin Wen",
      "Kangling Liu",
      "Yang Liu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=SkipViT%3A+Speeding+Up+Vision+Transformers+with+a+Token-Level+Skip+Connection+Foozhan+Ataiefard+Walid+Ahmed+Habib+Hajimolahoseini+Saina+Asani+Farnoosh+Javadi",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "F Ataiefard",
        "id": "WQu1w8YAAAAJ"
      },
      {
        "name": "W Ahmed",
        "id": "fyO3QUUAAAAJ"
      },
      {
        "name": "H Hajimolahoseini",
        "id": "ECIUuhoAAAAJ"
      },
      {
        "name": "S Asani",
        "id": "4ypC0-UAAAAJ"
      },
      {
        "name": "F Javadi",
        "id": "xuJtV6UAAAAJ"
      },
      {
        "name": "M Hassanpour",
        "id": "nkX6eKwAAAAJ"
      },
      {
        "name": "OM Awad",
        "id": "oP_KjvoAAAAJ"
      }
    ],
    "citation_count": 5
  },
  {
    "arxiv_id": "2311.02719",
    "title": "Uncertainty Estimation for Safety-critical Scene Segmentation via Fine-grained Reward Maximization",
    "year": 2023,
    "published": "2023-11-05T17:43:37Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Uncertainty estimation plays an important role for future reliable deployment of deep segmentation models in safety-critical scenarios such as medical applications. However, existing methods for uncertainty estimation have been limited by the lack of explicit guidance for calibrating the prediction risk and model confidence. In this work, we propose a novel fine-grained reward maximization (FGRM) framework, to address uncertainty estimation by directly utilizing an uncertainty metric related rew",
    "arxiv_url": "https://arxiv.org/abs/2311.02719v1",
    "pdf_url": "https://arxiv.org/pdf/2311.02719v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2311.02719",
    "arxiv_authors": [
      "Hongzheng Yang",
      "Cheng Chen",
      "Yueyao Chen",
      "Markus Scheppach",
      "Hon Chi Yip",
      "Qi Dou"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Uncertainty+Estimation+for+Safety-critical+Scene+Segmentation+via+Fine-grained+Reward+Maximization+Hongzheng+Yang+Cheng+Chen+Yueyao+Chen+Markus+Scheppach+Hon+Chi+Yip",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "H Yang",
        "id": null
      },
      {
        "name": "C Chen",
        "id": "bRe3FlcAAAAJ"
      },
      {
        "name": "Y Chen",
        "id": null
      },
      {
        "name": "S Scheppach",
        "id": null
      },
      {
        "name": "HC Yip",
        "id": null
      },
      {
        "name": "DOU QIAdvances in neural information processing systems",
        "id": null
      }
    ],
    "citation_count": 13
  },
  {
    "arxiv_id": "2401.00420",
    "title": "SynCDR : Training Cross Domain Retrieval Models with Synthetic Data",
    "year": 2023,
    "published": "2023-12-31T08:06:53Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "In cross-domain retrieval, a model is required to identify images from the same semantic category across two visual domains. For instance, given a sketch of an object, a model needs to retrieve a real image of it from an online store's catalog. A standard approach for such a problem is learning a feature space of images where Euclidean distances reflect similarity. Even without human annotations, which may be expensive to acquire, prior methods function reasonably well using unlabeled images for",
    "arxiv_url": "https://arxiv.org/abs/2401.00420v2",
    "pdf_url": "https://arxiv.org/pdf/2401.00420v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2401.00420",
    "arxiv_authors": [
      "Samarth Mishra",
      "Carlos D. Castillo",
      "Hongcheng Wang",
      "Kate Saenko",
      "Venkatesh Saligrama"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=SynCDR+%3A+Training+Cross+Domain+Retrieval+Models+with+Synthetic+Data+Samarth+Mishra+Carlos+D.+Castillo+Hongcheng+Wang+Kate+Saenko+Venkatesh+Saligrama",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "S Mishra",
        "id": "Vxk4TM4AAAAJ"
      },
      {
        "name": "CD Castillo",
        "id": "jxf3Qv0AAAAJ"
      },
      {
        "name": "H Wang",
        "id": "RKp9_nQAAAAJ"
      },
      {
        "name": "K Saenko",
        "id": "9xDADY4AAAAJ"
      },
      {
        "name": "V Saligrama",
        "id": "S4z3uzMAAAAJ"
      }
    ],
    "citation_count": 1
  },
  {
    "arxiv_id": "2411.19950",
    "title": "AlphaTablets: A Generic Plane Representation for 3D Planar Reconstruction from Monocular Videos",
    "year": 2024,
    "published": "2024-11-29T18:59:52Z",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "We introduce AlphaTablets, a novel and generic representation of 3D planes that features continuous 3D surface and precise boundary delineation. By representing 3D planes as rectangles with alpha channels, AlphaTablets combine the advantages of current 2D and 3D plane representations, enabling accurate, consistent and flexible modeling of 3D planes. We derive differentiable rasterization on top of AlphaTablets to efficiently render 3D planes into images, and propose a novel bottom-up pipeline fo",
    "arxiv_url": "https://arxiv.org/abs/2411.19950v1",
    "pdf_url": "https://arxiv.org/pdf/2411.19950v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2411.19950",
    "arxiv_authors": [
      "Yuze He",
      "Wang Zhao",
      "Shaohui Liu",
      "Yubin Hu",
      "Yushi Bai",
      "Yu-Hui Wen",
      "Yong-Jin Liu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=AlphaTablets%3A+A+Generic+Plane+Representation+for+3D+Planar+Reconstruction+from+Monocular+Videos+Yuze+He+Wang+Zhao+Shaohui+Liu+Yubin+Hu+Yushi+Bai",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "Y He",
        "id": "bYeKwD8AAAAJ"
      },
      {
        "name": "W Zhao",
        "id": "oKqr-ZQAAAAJ"
      },
      {
        "name": "S Liu",
        "id": null
      },
      {
        "name": "Y Hu",
        "id": "swN2J1QAAAAJ"
      },
      {
        "name": "Y Bai",
        "id": "E6cma10AAAAJ"
      }
    ],
    "citation_count": 5
  },
  {
    "arxiv_id": "2503.00397",
    "title": "Floorplan-SLAM: A Real-Time, High-Accuracy, and Long-Term Multi-Session Point-Plane SLAM for Efficient Floorplan Reconstruction",
    "year": 2025,
    "published": "2025-03-01T08:18:11Z",
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "abstract": "Floorplan reconstruction provides structural priors essential for reliable indoor robot navigation and high-level scene understanding. However, existing approaches either require time-consuming offline processing with a complete map, or rely on expensive sensors and substantial computational resources. To address the problems, we propose Floorplan-SLAM, which incorporates floorplan reconstruction tightly into a multi-session SLAM system by seamlessly interacting with plane extraction, pose estim",
    "arxiv_url": "https://arxiv.org/abs/2503.00397v3",
    "pdf_url": "https://arxiv.org/pdf/2503.00397v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2503.00397",
    "arxiv_authors": [
      "Haolin Wang",
      "Zeren Lv",
      "Hao Wei",
      "Haijiang Zhu",
      "Yihong Wu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Floorplan-SLAM%3A+A+Real-Time%2C+High-Accuracy%2C+and+Long-Term+Multi-Session+Point-Plane+SLAM+for+Efficient+Floorplan+Reconstruction+Haolin+Wang+Zeren+Lv+Hao+Wei+Haijiang+Zhu+Yihong+Wu",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "H Wang",
        "id": null
      },
      {
        "name": "Z Lv",
        "id": null
      },
      {
        "name": "H Wei",
        "id": "wnwa354AAAAJ"
      },
      {
        "name": "H Zhu",
        "id": null
      },
      {
        "name": "Y Wu -",
        "id": null
      }
    ],
    "citation_count": null
  },
  {
    "arxiv_id": "2309.17002",
    "title": "Understanding and Mitigating the Label Noise in Pre-training on Downstream Tasks",
    "year": 2023,
    "published": "2023-09-29T06:18:15Z",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "abstract": "Pre-training on large-scale datasets and then fine-tuning on downstream tasks have become a standard practice in deep learning. However, pre-training data often contain label noise that may adversely affect the generalization of the model. This paper aims to understand the nature of noise in pre-training datasets and to mitigate its impact on downstream tasks. More specifically, through extensive experiments of supervised pre-training models on synthetic noisy ImageNet-1K and YFCC15M datasets, w",
    "arxiv_url": "https://arxiv.org/abs/2309.17002v2",
    "pdf_url": "https://arxiv.org/pdf/2309.17002v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2309.17002",
    "arxiv_authors": [
      "Hao Chen",
      "Jindong Wang",
      "Ankit Shah",
      "Ran Tao",
      "Hongxin Wei",
      "Xing Xie",
      "Masashi Sugiyama",
      "Bhiksha Raj"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Understanding+and+Mitigating+the+Label+Noise+in+Pre-training+on+Downstream+Tasks+Hao+Chen+Jindong+Wang+Ankit+Shah+Ran+Tao+Hongxin+Wei",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "H Chen",
        "id": "tktqkhwAAAAJ"
      },
      {
        "name": "J Wang",
        "id": "hBZ_tKsAAAAJ"
      },
      {
        "name": "A Shah",
        "id": "TqG1H4cAAAAJ"
      },
      {
        "name": "R Tao",
        "id": "EOTrLrgAAAAJ"
      },
      {
        "name": "H Wei",
        "id": "cABH034AAAAJ"
      },
      {
        "name": "X Xie",
        "id": "5EQfAFIAAAAJ"
      },
      {
        "name": "M Sugiyama",
        "id": "GkYIrlIAAAAJ"
      },
      {
        "name": "B Raj",
        "id": "IWcGY98AAAAJ"
      }
    ],
    "citation_count": 41
  },
  {
    "arxiv_id": "2305.05356",
    "title": "Learning Dynamic Point Cloud Compression via Hierarchical Inter-frame Block Matching",
    "year": 2023,
    "published": "2023-05-09T11:44:13Z",
    "categories": [
      "cs.CV",
      "cs.MM",
      "eess.IV"
    ],
    "abstract": "3D dynamic point cloud (DPC) compression relies on mining its temporal context, which faces significant challenges due to DPC's sparsity and non-uniform structure. Existing methods are limited in capturing sufficient temporal dependencies. Therefore, this paper proposes a learning-based DPC compression framework via hierarchical block-matching-based inter-prediction module to compensate and compress the DPC geometry in latent space. Specifically, we propose a hierarchical motion estimation and m",
    "arxiv_url": "https://arxiv.org/abs/2305.05356v2",
    "pdf_url": "https://arxiv.org/pdf/2305.05356v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2305.05356",
    "arxiv_authors": [
      "Shuting Xia",
      "Tingyu Fan",
      "Yiling Xu",
      "Jenq-Neng Hwang",
      "Zhu Li"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Learning+Dynamic+Point+Cloud+Compression+via+Hierarchical+Inter-frame+Block+Matching+Shuting+Xia+Tingyu+Fan+Yiling+Xu+Jenq-Neng+Hwang+Zhu+Li",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "S Xia",
        "id": null
      },
      {
        "name": "T Fan",
        "id": "FdjEd1UAAAAJ"
      },
      {
        "name": "Y Xu",
        "id": "638kRwkAAAAJ"
      },
      {
        "name": "JN Hwang",
        "id": "4OAHy-kAAAAJ"
      },
      {
        "name": "Z Li -",
        "id": null
      }
    ],
    "citation_count": 21
  },
  {
    "arxiv_id": "2310.01617",
    "title": "Dynamic Spatio-Temporal Summarization using Information Based Fusion",
    "year": 2023,
    "published": "2023-10-02T20:21:43Z",
    "categories": [
      "cs.CV",
      "cs.IT"
    ],
    "abstract": "In the era of burgeoning data generation, managing and storing large-scale time-varying datasets poses significant challenges. With the rise of supercomputing capabilities, the volume of data produced has soared, intensifying storage and I/O overheads. To address this issue, we propose a dynamic spatio-temporal data summarization technique that identifies informative features in key timesteps and fuses less informative ones. This approach minimizes storage requirements while preserving data dyna",
    "arxiv_url": "https://arxiv.org/abs/2310.01617v1",
    "pdf_url": "https://arxiv.org/pdf/2310.01617v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2310.01617",
    "arxiv_authors": [
      "Humayra Tasnim",
      "Soumya Dutta",
      "Melanie Moses"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Dynamic+Spatio-Temporal+Summarization+using+Information+Based+Fusion+Humayra+Tasnim+Soumya+Dutta+Melanie+Moses",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "H Tasnim",
        "id": "Q-vo1KQAAAAJ"
      },
      {
        "name": "S Dutta",
        "id": "7CGMUB8AAAAJ"
      },
      {
        "name": "M Moses -",
        "id": null
      }
    ],
    "citation_count": null
  },
  {
    "arxiv_id": "2409.09593",
    "title": "One-Shot Learning for Pose-Guided Person Image Synthesis in the Wild",
    "year": 2024,
    "published": "2024-09-15T02:42:25Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Current Pose-Guided Person Image Synthesis (PGPIS) methods depend heavily on large amounts of labeled triplet data to train the generator in a supervised manner. However, they often falter when applied to in-the-wild samples, primarily due to the distribution gap between the training datasets and real-world test samples. While some researchers aim to enhance model generalizability through sophisticated training procedures, advanced architectures, or by creating more diverse datasets, we adopt th",
    "arxiv_url": "https://arxiv.org/abs/2409.09593v1",
    "pdf_url": "https://arxiv.org/pdf/2409.09593v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2409.09593",
    "arxiv_authors": [
      "Dongqi Fan",
      "Tao Chen",
      "Mingjie Wang",
      "Rui Ma",
      "Qiang Tang",
      "Zili Yi",
      "Qian Wang",
      "Liang Chang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=One-Shot+Learning+for+Pose-Guided+Person+Image+Synthesis+in+the+Wild+Dongqi+Fan+Tao+Chen+Mingjie+Wang+Rui+Ma+Qiang+Tang",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "D Fan",
        "id": "P7gt5XYAAAAJ"
      },
      {
        "name": "T Chen",
        "id": null
      },
      {
        "name": "M Wang",
        "id": "MfsLOqcAAAAJ"
      },
      {
        "name": "R Ma",
        "id": null
      },
      {
        "name": "Q Tang",
        "id": "mwOkBvIAAAAJ"
      },
      {
        "name": "Z Yi",
        "id": "XC6Z56AAAAAJ"
      },
      {
        "name": "Q Wang",
        "id": null
      },
      {
        "name": "L ChangICASSP",
        "id": null
      }
    ],
    "citation_count": 3
  },
  {
    "arxiv_id": "2410.04128",
    "title": "Optimizing Medical Image Segmentation with Advanced Decoder Design",
    "year": 2024,
    "published": "2024-10-05T11:47:13Z",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "abstract": "U-Net is widely used in medical image segmentation due to its simple and flexible architecture design. To address the challenges of scale and complexity in medical tasks, several variants of U-Net have been proposed. In particular, methods based on Vision Transformer (ViT), represented by Swin UNETR, have gained widespread attention in recent years. However, these improvements often focus on the encoder, overlooking the crucial role of the decoder in optimizing segmentation details. This design ",
    "arxiv_url": "https://arxiv.org/abs/2410.04128v1",
    "pdf_url": "https://arxiv.org/pdf/2410.04128v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2410.04128",
    "arxiv_authors": [
      "Weibin Yang",
      "Zhiqi Dong",
      "Mingyuan Xu",
      "Longwei Xu",
      "Dehua Geng",
      "Yusong Li",
      "Pengwei Wang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Optimizing+Medical+Image+Segmentation+with+Advanced+Decoder+Design+Weibin+Yang+Zhiqi+Dong+Mingyuan+Xu+Longwei+Xu+Dehua+Geng",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "W Yang",
        "id": null
      },
      {
        "name": "Z Dong",
        "id": null
      },
      {
        "name": "M Xu",
        "id": null
      },
      {
        "name": "L Xu",
        "id": "NQYHY9YAAAAJ"
      },
      {
        "name": "D Geng",
        "id": null
      },
      {
        "name": "Y Li",
        "id": null
      },
      {
        "name": "P Wang",
        "id": "2xR6P5AAAAAJ"
      }
    ],
    "citation_count": 1
  },
  {
    "arxiv_id": "2303.14457",
    "title": "Diverse Motion In-betweening with Dual Posture Stitching",
    "year": 2023,
    "published": "2023-03-25T12:36:46Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR"
    ],
    "abstract": "In-betweening is a technique for generating transitions given initial and target character states. The majority of existing works require multiple (often $>$10) frames as input, which are not always accessible. Our work deals with a focused yet challenging problem: to generate the transition when given exactly two frames (only the first and last). To cope with this challenging scenario, we implement our bi-directional scheme which generates forward and backward transitions from the start and end",
    "arxiv_url": "https://arxiv.org/abs/2303.14457v1",
    "pdf_url": "https://arxiv.org/pdf/2303.14457v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2303.14457",
    "arxiv_authors": [
      "Tianxiang Ren",
      "Jubo Yu",
      "Shihui Guo",
      "Ying Ma",
      "Yutao Ouyang",
      "Zijiao Zeng",
      "Yazhan Zhang",
      "Yipeng Qin"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Diverse+Motion+In-betweening+with+Dual+Posture+Stitching+Tianxiang+Ren+Jubo+Yu+Shihui+Guo+Ying+Ma+Yutao+Ouyang",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "T Ren",
        "id": null
      },
      {
        "name": "J Yu",
        "id": null
      },
      {
        "name": "S Guo",
        "id": null
      },
      {
        "name": "Y Ma",
        "id": null
      },
      {
        "name": "Y Ouyang",
        "id": null
      },
      {
        "name": "Z Zeng",
        "id": null
      },
      {
        "name": "Y Zhang",
        "id": "y6JT9doAAAAJ"
      },
      {
        "name": "Y Qin",
        "id": "ojgWPpgAAAAJ"
      }
    ],
    "citation_count": 3
  },
  {
    "arxiv_id": "2406.13272",
    "title": "AniFaceDiff: Animating Stylized Avatars via Parametric Conditioned Diffusion Models",
    "year": 2024,
    "published": "2024-06-19T07:08:48Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Animating stylized avatars with dynamic poses and expressions has attracted increasing attention for its broad range of applications. Previous research has made significant progress by training controllable generative models to synthesize animations based on reference characteristics, pose, and expression conditions. However, the mechanisms used in these methods to control pose and expression often inadvertently introduce unintended features from the target motion, while also causing a loss of e",
    "arxiv_url": "https://arxiv.org/abs/2406.13272v2",
    "pdf_url": "https://arxiv.org/pdf/2406.13272v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2406.13272",
    "arxiv_authors": [
      "Ken Chen",
      "Sachith Seneviratne",
      "Wei Wang",
      "Dongting Hu",
      "Sanjay Saha",
      "Md. Tarek Hasan",
      "Sanka Rasnayaka",
      "Tamasha Malepathirana",
      "Mingming Gong",
      "Saman Halgamuge"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=AniFaceDiff%3A+Animating+Stylized+Avatars+via+Parametric+Conditioned+Diffusion+Models+Ken+Chen+Sachith+Seneviratne+Wei+Wang+Dongting+Hu+Sanjay+Saha",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "K Chen",
        "id": "bnNm_KwAAAAJ"
      },
      {
        "name": "S Seneviratne",
        "id": "nvv8iZEAAAAJ"
      },
      {
        "name": "W Wang",
        "id": null
      },
      {
        "name": "D Hu",
        "id": "0CVmdVEAAAAJ"
      },
      {
        "name": "S Saha",
        "id": "prPfgTIAAAAJ"
      },
      {
        "name": "MT Hasan",
        "id": "SFfUSOIAAAAJ"
      },
      {
        "name": "S Rasnayaka",
        "id": "H_FARq0AAAAJ"
      },
      {
        "name": "T Malepathirana",
        "id": "oZNxrKcAAAAJ"
      }
    ],
    "citation_count": null
  },
  {
    "arxiv_id": "2406.05477",
    "title": "Attri-Net: A Globally and Locally Inherently Interpretable Model for Multi-Label Classification Using Class-Specific Counterfactuals",
    "year": 2024,
    "published": "2024-06-08T13:52:02Z",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "Interpretability is crucial for machine learning algorithms in high-stakes medical applications. However, high-performing neural networks typically cannot explain their predictions. Post-hoc explanation methods provide a way to understand neural networks but have been shown to suffer from conceptual problems. Moreover, current research largely focuses on providing local explanations for individual samples rather than global explanations for the model itself. In this paper, we propose Attri-Net, ",
    "arxiv_url": "https://arxiv.org/abs/2406.05477v2",
    "pdf_url": "https://arxiv.org/pdf/2406.05477v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2406.05477",
    "arxiv_authors": [
      "Susu Sun",
      "Stefano Woerner",
      "Andreas Maier",
      "Lisa M. Koch",
      "Christian F. Baumgartner"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Attri-Net%3A+A+Globally+and+Locally+Inherently+Interpretable+Model+for+Multi-Label+Classification+Using+Class-Specific+Counterfactuals+Susu+Sun+Stefano+Woerner+Andreas+Maier+Lisa+M.+Koch+Christian+F.+Baumgartner",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "S Sun",
        "id": "cyGvnh8AAAAJ"
      },
      {
        "name": "S Woerner",
        "id": "T1cgW4gAAAAJ"
      },
      {
        "name": "A Maier",
        "id": "MA6SDuEAAAAJ"
      },
      {
        "name": "LM Koch",
        "id": "R0iwuiIAAAAJ"
      },
      {
        "name": "CF Baumgartner",
        "id": "9MSArZUAAAAJ"
      }
    ],
    "citation_count": 2
  },
  {
    "arxiv_id": "2404.11669",
    "title": "Factorized Motion Fields for Fast Sparse Input Dynamic View Synthesis",
    "year": 2024,
    "published": "2024-04-17T18:08:00Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Designing a 3D representation of a dynamic scene for fast optimization and rendering is a challenging task. While recent explicit representations enable fast learning and rendering of dynamic radiance fields, they require a dense set of input viewpoints. In this work, we focus on learning a fast representation for dynamic radiance fields with sparse input viewpoints. However, the optimization with sparse input is under-constrained and necessitates the use of motion priors to constrain the learni",
    "arxiv_url": "https://arxiv.org/abs/2404.11669v3",
    "pdf_url": "https://arxiv.org/pdf/2404.11669v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2404.11669",
    "arxiv_authors": [
      "Nagabhushan Somraj",
      "Kapil Choudhary",
      "Sai Harsha Mupparaju",
      "Rajiv Soundararajan"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Factorized+Motion+Fields+for+Fast+Sparse+Input+Dynamic+View+Synthesis+Nagabhushan+Somraj+Kapil+Choudhary+Sai+Harsha+Mupparaju+Rajiv+Soundararajan",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "N Somraj",
        "id": "kFbFZHIAAAAJ"
      },
      {
        "name": "K Choudhary",
        "id": null
      },
      {
        "name": "SH Mupparaju",
        "id": "N2Gp2gYAAAAJ"
      },
      {
        "name": "R SoundararajanACM SIGGRAPH",
        "id": null
      }
    ],
    "citation_count": 9
  },
  {
    "arxiv_id": "2405.04390",
    "title": "DriveWorld: 4D Pre-trained Scene Understanding via World Models for Autonomous Driving",
    "year": 2024,
    "published": "2024-05-07T15:14:20Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Vision-centric autonomous driving has recently raised wide attention due to its lower cost. Pre-training is essential for extracting a universal representation. However, current vision-centric pre-training typically relies on either 2D or 3D pre-text tasks, overlooking the temporal characteristics of autonomous driving as a 4D scene understanding task. In this paper, we address this challenge by introducing a world model-based autonomous driving 4D representation learning framework, dubbed \\emph",
    "arxiv_url": "https://arxiv.org/abs/2405.04390v1",
    "pdf_url": "https://arxiv.org/pdf/2405.04390v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2405.04390",
    "arxiv_authors": [
      "Chen Min",
      "Dawei Zhao",
      "Liang Xiao",
      "Jian Zhao",
      "Xinli Xu",
      "Zheng Zhu",
      "Lei Jin",
      "Jianshu Li",
      "Yulan Guo",
      "Junliang Xing",
      "Liping Jing",
      "Yiming Nie",
      "Bin Dai"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=DriveWorld%3A+4D+Pre-trained+Scene+Understanding+via+World+Models+for+Autonomous+Driving+Chen+Min+Dawei+Zhao+Liang+Xiao+Jian+Zhao+Xinli+Xu",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "C Min",
        "id": "pE9gTMQAAAAJ"
      },
      {
        "name": "D Zhao",
        "id": "zdhRJCkAAAAJ"
      },
      {
        "name": "L Xiao",
        "id": "hvxSnzoAAAAJ"
      },
      {
        "name": "J Zhao",
        "id": "yYpQrj0AAAAJ"
      },
      {
        "name": "X Xu",
        "id": "lrgPuBUAAAAJ"
      },
      {
        "name": "Z Zhu",
        "id": "NmwjI0AAAAAJ"
      },
      {
        "name": "L Jin",
        "id": "aZQLyoEAAAAJ"
      },
      {
        "name": "J Li",
        "id": null
      },
      {
        "name": "Y Guo",
        "id": "WQRNvdsAAAAJ"
      },
      {
        "name": "J Xing",
        "id": "jSwNd3MAAAAJ"
      },
      {
        "name": "L Jing",
        "id": "zStEDu4AAAAJ"
      },
      {
        "name": "Y Nie",
        "id": null
      },
      {
        "name": "B Dai",
        "id": null
      }
    ],
    "citation_count": 61
  },
  {
    "arxiv_id": "2412.16200",
    "title": "Robust Spectral Anomaly Detection in EELS Spectral Images via Three Dimensional Convolutional Variational Autoencoders",
    "year": 2024,
    "published": "2024-12-16T23:55:08Z",
    "categories": [
      "cs.CV",
      "cond-mat.mtrl-sci",
      "cs.LG"
    ],
    "abstract": "We introduce a Three-Dimensional Convolutional Variational Autoencoder (3D-CVAE) for automated anomaly detection in Electron Energy Loss Spectroscopy Spectrum Imaging (EELS-SI) data. Our approach leverages the full three-dimensional structure of EELS-SI data to detect subtle spectral anomalies while preserving both spatial and spectral correlations across the datacube. By employing negative log-likelihood loss and training on bulk spectra, the model learns to reconstruct bulk features characteri",
    "arxiv_url": "https://arxiv.org/abs/2412.16200v1",
    "pdf_url": "https://arxiv.org/pdf/2412.16200v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2412.16200",
    "arxiv_authors": [
      "Seyfal Sultanov",
      "James P Buban",
      "Robert F Klie"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Robust+Spectral+Anomaly+Detection+in+EELS+Spectral+Images+via+Three+Dimensional+Convolutional+Variational+Autoencoders+Seyfal+Sultanov+James+P+Buban+Robert+F+Klie",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "S Sultanov",
        "id": "yzyzFGcAAAAJ"
      },
      {
        "name": "JP Buban",
        "id": "xd-ZcWEAAAAJ"
      },
      {
        "name": "RF Klie -",
        "id": null
      }
    ],
    "citation_count": null
  },
  {
    "arxiv_id": "2312.01919",
    "title": "COTR: Compact Occupancy TRansformer for Vision-based 3D Occupancy Prediction",
    "year": 2023,
    "published": "2023-12-04T14:23:18Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "The autonomous driving community has shown significant interest in 3D occupancy prediction, driven by its exceptional geometric perception and general object recognition capabilities. To achieve this, current works try to construct a Tri-Perspective View (TPV) or Occupancy (OCC) representation extending from the Bird-Eye-View perception. However, compressed views like TPV representation lose 3D geometry information while raw and sparse OCC representation requires heavy but redundant computationa",
    "arxiv_url": "https://arxiv.org/abs/2312.01919v2",
    "pdf_url": "https://arxiv.org/pdf/2312.01919v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2312.01919",
    "arxiv_authors": [
      "Qihang Ma",
      "Xin Tan",
      "Yanyun Qu",
      "Lizhuang Ma",
      "Zhizhong Zhang",
      "Yuan Xie"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=COTR%3A+Compact+Occupancy+TRansformer+for+Vision-based+3D+Occupancy+Prediction+Qihang+Ma+Xin+Tan+Yanyun+Qu+Lizhuang+Ma+Zhizhong+Zhang",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "Q Ma",
        "id": "MAfVfFsAAAAJ"
      },
      {
        "name": "X Tan",
        "id": "UY4NCdcAAAAJ"
      },
      {
        "name": "Y Qu",
        "id": "idiP90sAAAAJ"
      },
      {
        "name": "L Ma",
        "id": "yd58y_0AAAAJ"
      },
      {
        "name": "Z Zhang",
        "id": "CXZciFAAAAAJ"
      },
      {
        "name": "Y Xie",
        "id": "RN1QMPgAAAAJ"
      }
    ],
    "citation_count": 78
  },
  {
    "arxiv_id": "2505.17501",
    "title": "RoHyDR: Robust Hybrid Diffusion Recovery for Incomplete Multimodal Emotion Recognition",
    "year": 2025,
    "published": "2025-05-23T05:52:17Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "abstract": "Multimodal emotion recognition analyzes emotions by combining data from multiple sources. However, real-world noise or sensor failures often cause missing or corrupted data, creating the Incomplete Multimodal Emotion Recognition (IMER) challenge. In this paper, we propose Robust Hybrid Diffusion Recovery (RoHyDR), a novel framework that performs missing-modality recovery at unimodal, multimodal, feature, and semantic levels. For unimodal representation recovery of missing modalities, RoHyDR expl",
    "arxiv_url": "https://arxiv.org/abs/2505.17501v1",
    "pdf_url": "https://arxiv.org/pdf/2505.17501v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2505.17501",
    "arxiv_authors": [
      "Yuehan Jin",
      "Xiaoqing Liu",
      "Yiyuan Yang",
      "Zhiwen Yu",
      "Tong Zhang",
      "Kaixiang Yang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=RoHyDR%3A+Robust+Hybrid+Diffusion+Recovery+for+Incomplete+Multimodal+Emotion+Recognition+Yuehan+Jin+Xiaoqing+Liu+Yiyuan+Yang+Zhiwen+Yu+Tong+Zhang",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "Y Jin",
        "id": null
      },
      {
        "name": "X Liu",
        "id": null
      },
      {
        "name": "Y Yang",
        "id": "FUuGvZIAAAAJ"
      },
      {
        "name": "Z Yu",
        "id": "uawKm4wAAAAJ"
      },
      {
        "name": "T Zhang",
        "id": "-neWbpUAAAAJ"
      },
      {
        "name": "K Yang -",
        "id": null
      }
    ],
    "citation_count": null
  },
  {
    "arxiv_id": "2402.11530",
    "title": "Efficient Multimodal Learning from Data-centric Perspective",
    "year": 2024,
    "published": "2024-02-18T10:09:10Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Multimodal Large Language Models (MLLMs) have demonstrated notable capabilities in general visual understanding and reasoning tasks. However, their deployment is hindered by substantial computational costs in both training and inference, limiting accessibility to the broader research and user communities. A straightforward solution is to leverage smaller pre-trained vision and language models, which inevitably cause significant performance drops. In this paper, we demonstrate the possibility of ",
    "arxiv_url": "https://arxiv.org/abs/2402.11530v3",
    "pdf_url": "https://arxiv.org/pdf/2402.11530v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2402.11530",
    "arxiv_authors": [
      "Muyang He",
      "Yexin Liu",
      "Boya Wu",
      "Jianhao Yuan",
      "Yueze Wang",
      "Tiejun Huang",
      "Bo Zhao"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Efficient+Multimodal+Learning+from+Data-centric+Perspective+Muyang+He+Yexin+Liu+Boya+Wu+Jianhao+Yuan+Yueze+Wang",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "M He",
        "id": "Q0Xn7i4AAAAJ"
      },
      {
        "name": "Y Liu",
        "id": "Y8zBpcoAAAAJ"
      },
      {
        "name": "B Wu",
        "id": "OQOtZXgAAAAJ"
      },
      {
        "name": "J Yuan",
        "id": "BUJPCegAAAAJ"
      },
      {
        "name": "Y Wang",
        "id": "ga2MKaMAAAAJ"
      },
      {
        "name": "T Huang",
        "id": "knvEK4AAAAAJ"
      },
      {
        "name": "B Zhao",
        "id": "R3_AR5EAAAAJ"
      }
    ],
    "citation_count": 143
  },
  {
    "arxiv_id": "2309.06142",
    "title": "Towards Reliable Domain Generalization: A New Dataset and Evaluations",
    "year": 2023,
    "published": "2023-09-12T11:29:12Z",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "There are ubiquitous distribution shifts in the real world. However, deep neural networks (DNNs) are easily biased towards the training set, which causes severe performance degradation when they receive out-of-distribution data. Many methods are studied to train models that generalize under various distribution shifts in the literature of domain generalization (DG). However, the recent DomainBed and WILDS benchmarks challenged the effectiveness of these methods. Aiming at the problems in the exi",
    "arxiv_url": "https://arxiv.org/abs/2309.06142v1",
    "pdf_url": "https://arxiv.org/pdf/2309.06142v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2309.06142",
    "arxiv_authors": [
      "Jiao Zhang",
      "Xu-Yao Zhang",
      "Cheng-Lin Liu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Towards+Reliable+Domain+Generalization%3A+A+New+Dataset+and+Evaluations+Jiao+Zhang+Xu-Yao+Zhang+Cheng-Lin+Liu",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "J Zhang",
        "id": null
      },
      {
        "name": "XY Zhang",
        "id": null
      },
      {
        "name": "CL Liu -",
        "id": null
      }
    ],
    "citation_count": null
  },
  {
    "arxiv_id": "2404.02614",
    "title": "Vestibular schwannoma growth prediction from longitudinal MRI by time conditioned neural fields",
    "year": 2024,
    "published": "2024-04-03T10:01:23Z",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "abstract": "Vestibular schwannomas (VS) are benign tumors that are generally managed by active surveillance with MRI examination. To further assist clinical decision-making and avoid overtreatment, an accurate prediction of tumor growth based on longitudinal imaging is highly desirable. In this paper, we introduce DeepGrowth, a deep learning method that incorporates neural fields and recurrent neural networks for prospective tumor growth prediction. In the proposed method, each tumor is represented as a sig",
    "arxiv_url": "https://arxiv.org/abs/2404.02614v2",
    "pdf_url": "https://arxiv.org/pdf/2404.02614v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2404.02614",
    "arxiv_authors": [
      "Yunjie Chen",
      "Jelmer M. Wolterink",
      "Olaf M. Neve",
      "Stephan R. Romeijn",
      "Berit M. Verbist",
      "Erik F. Hensen",
      "Qian Tao",
      "Marius Staring"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Vestibular+schwannoma+growth+prediction+from+longitudinal+MRI+by+time+conditioned+neural+fields+Yunjie+Chen+Jelmer+M.+Wolterink+Olaf+M.+Neve+Stephan+R.+Romeijn+Berit+M.+Verbist",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "Y Chen",
        "id": "eAEN2p4AAAAJ"
      },
      {
        "name": "JM Wolterink",
        "id": "xzlXuWoAAAAJ"
      },
      {
        "name": "OM Neve",
        "id": null
      },
      {
        "name": "SR Romeijn",
        "id": null
      },
      {
        "name": "BM Verbist",
        "id": null
      },
      {
        "name": "EF Hensen",
        "id": "Hf-1Q6gAAAAJ"
      },
      {
        "name": "Q Tao",
        "id": "djCHmzsAAAAJ"
      },
      {
        "name": "M StaringInternational",
        "id": null
      }
    ],
    "citation_count": 2
  },
  {
    "arxiv_id": "2306.08961",
    "title": "Self-Knowledge Distillation for Surgical Phase Recognition",
    "year": 2023,
    "published": "2023-06-15T08:55:00Z",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "abstract": "Purpose: Advances in surgical phase recognition are generally led by training deeper networks. Rather than going further with a more complex solution, we believe that current models can be exploited better. We propose a self-knowledge distillation framework that can be integrated into current state-of-the-art (SOTA) models without requiring any extra complexity to the models or annotations.   Methods: Knowledge distillation is a framework for network regularization where knowledge is distilled f",
    "arxiv_url": "https://arxiv.org/abs/2306.08961v1",
    "pdf_url": "https://arxiv.org/pdf/2306.08961v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2306.08961",
    "arxiv_authors": [
      "Jinglu Zhang",
      "Santiago Barbarisi",
      "Abdolrahim Kadkhodamohammadi",
      "Danail Stoyanov",
      "Imanol Luengo"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Self-Knowledge+Distillation+for+Surgical+Phase+Recognition+Jinglu+Zhang+Santiago+Barbarisi+Abdolrahim+Kadkhodamohammadi+Danail+Stoyanov+Imanol+Luengo",
    "gs_search_success": false,
    "gs_authors": null,
    "citation_count": null,
    "error_type": "fetch_failed"
  },
  {
    "arxiv_id": "2303.05734",
    "title": "Generative Model Based Noise Robust Training for Unsupervised Domain Adaptation",
    "year": 2023,
    "published": "2023-03-10T06:43:55Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Target domain pseudo-labelling has shown effectiveness in unsupervised domain adaptation (UDA). However, pseudo-labels of unlabeled target domain data are inevitably noisy due to the distribution shift between source and target domains. This paper proposes a Generative model-based Noise-Robust Training method (GeNRT), which eliminates domain shift while mitigating label noise. GeNRT incorporates a Distribution-based Class-wise Feature Augmentation (D-CFA) and a Generative-Discriminative classifi",
    "arxiv_url": "https://arxiv.org/abs/2303.05734v1",
    "pdf_url": "https://arxiv.org/pdf/2303.05734v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2303.05734",
    "arxiv_authors": [
      "Zhongying Deng",
      "Da Li",
      "Junjun He",
      "Yi-Zhe Song",
      "Tao Xiang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Generative+Model+Based+Noise+Robust+Training+for+Unsupervised+Domain+Adaptation+Zhongying+Deng+Da+Li+Junjun+He+Yi-Zhe+Song+Tao+Xiang",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "Z Deng",
        "id": "E2o62dQAAAAJ"
      },
      {
        "name": "D Li",
        "id": "RPvaE3oAAAAJ"
      },
      {
        "name": "J He",
        "id": null
      },
      {
        "name": "YZ Song",
        "id": "irZFP_AAAAAJ"
      },
      {
        "name": "T Xiang -",
        "id": null
      }
    ],
    "citation_count": 1
  },
  {
    "arxiv_id": "2309.04183",
    "title": "Stereo Matching in Time: 100+ FPS Video Stereo Matching for Extended Reality",
    "year": 2023,
    "published": "2023-09-08T07:53:58Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Real-time Stereo Matching is a cornerstone algorithm for many Extended Reality (XR) applications, such as indoor 3D understanding, video pass-through, and mixed-reality games. Despite significant advancements in deep stereo methods, achieving real-time depth inference with high accuracy on a low-power device remains a major challenge. One of the major difficulties is the lack of high-quality indoor video stereo training datasets captured by head-mounted VR/AR glasses. To address this issue, we i",
    "arxiv_url": "https://arxiv.org/abs/2309.04183v1",
    "pdf_url": "https://arxiv.org/pdf/2309.04183v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2309.04183",
    "arxiv_authors": [
      "Ziang Cheng",
      "Jiayu Yang",
      "Hongdong Li"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Stereo+Matching+in+Time%3A+100%2B+FPS+Video+Stereo+Matching+for+Extended+Reality+Ziang+Cheng+Jiayu+Yang+Hongdong+Li",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "Z Cheng",
        "id": null
      },
      {
        "name": "J Yang",
        "id": "xe6Uv3gAAAAJ"
      },
      {
        "name": "H Li -",
        "id": null
      }
    ],
    "citation_count": 12
  },
  {
    "arxiv_id": "2504.19075",
    "title": "HoloDx: Knowledge- and Data-Driven Multimodal Diagnosis of Alzheimer's Disease",
    "year": 2025,
    "published": "2025-04-27T02:10:12Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Accurate diagnosis of Alzheimer's disease (AD) requires effectively integrating multimodal data and clinical expertise. However, existing methods often struggle to fully utilize multimodal information and lack structured mechanisms to incorporate dynamic domain knowledge. To address these limitations, we propose HoloDx, a knowledge- and data-driven framework that enhances AD diagnosis by aligning domain knowledge with multimodal clinical data. HoloDx incorporates a knowledge injection module wit",
    "arxiv_url": "https://arxiv.org/abs/2504.19075v2",
    "pdf_url": "https://arxiv.org/pdf/2504.19075v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2504.19075",
    "arxiv_authors": [
      "Qiuhui Chen",
      "Jintao Wang",
      "Gang Wang",
      "Yi Hong"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=HoloDx%3A+Knowledge-+and+Data-Driven+Multimodal+Diagnosis+of+Alzheimer%27s+Disease+Qiuhui+Chen+Jintao+Wang+Gang+Wang+Yi+Hong",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "Q Chen",
        "id": "rU1cXEgAAAAJ"
      },
      {
        "name": "J Wang",
        "id": null
      },
      {
        "name": "G Wang",
        "id": null
      },
      {
        "name": "Y Hong -",
        "id": null
      }
    ],
    "citation_count": null
  },
  {
    "arxiv_id": "2408.10349",
    "title": "AIR: Analytic Imbalance Rectifier for Continual Learning",
    "year": 2024,
    "published": "2024-08-19T18:42:00Z",
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "abstract": "Continual learning enables AI models to learn new data sequentially without retraining in real-world scenarios. Most existing methods assume the training data are balanced, aiming to reduce the catastrophic forgetting problem that models tend to forget previously generated data. However, data imbalance and the mixture of new and old data in real-world scenarios lead the model to ignore categories with fewer training samples. To solve this problem, we propose an analytic imbalance rectifier algor",
    "arxiv_url": "https://arxiv.org/abs/2408.10349v1",
    "pdf_url": "https://arxiv.org/pdf/2408.10349v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2408.10349",
    "arxiv_authors": [
      "Di Fang",
      "Yinan Zhu",
      "Runze Fang",
      "Cen Chen",
      "Ziqian Zeng",
      "Huiping Zhuang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=AIR%3A+Analytic+Imbalance+Rectifier+for+Continual+Learning+Di+Fang+Yinan+Zhu+Runze+Fang+Cen+Chen+Ziqian+Zeng",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "D Fang",
        "id": "YR-Cnw8AAAAJ"
      },
      {
        "name": "Y Zhu",
        "id": null
      },
      {
        "name": "R Fang",
        "id": null
      },
      {
        "name": "C Chen",
        "id": null
      },
      {
        "name": "Z Zeng",
        "id": null
      },
      {
        "name": "H Zhuang",
        "id": "vCXxuLkAAAAJ"
      }
    ],
    "citation_count": 5
  },
  {
    "arxiv_id": "2503.14550",
    "title": "Novel AI-Based Quantification of Breast Arterial Calcification to Predict Cardiovascular Risk",
    "year": 2025,
    "published": "2025-03-17T19:38:17Z",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "Women are underdiagnosed and undertreated for cardiovascular disease. Automatic quantification of breast arterial calcification on screening mammography can identify women at risk for cardiovascular disease and enable earlier treatment and management of disease. In this retrospective study of 116,135 women from two healthcare systems, a transformer-based neural network quantified BAC severity (no BAC, mild, moderate, and severe) on screening mammograms. Outcomes included major adverse cardiovasc",
    "arxiv_url": "https://arxiv.org/abs/2503.14550v1",
    "pdf_url": "https://arxiv.org/pdf/2503.14550v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2503.14550",
    "arxiv_authors": [
      "Theodorus Dapamede",
      "Aisha Urooj",
      "Vedant Joshi",
      "Gabrielle Gershon",
      "Frank Li",
      "Mohammadreza Chavoshi",
      "Beatrice Brown-Mulry",
      "Rohan Satya Isaac",
      "Aawez Mansuri",
      "Chad Robichaux",
      "Chadi Ayoub",
      "Reza Arsanjani",
      "Laurence Sperling",
      "Judy Gichoya",
      "Marly van Assen",
      "Charles W. ONeill",
      "Imon Banerjee",
      "Hari Trivedi"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Novel+AI-Based+Quantification+of+Breast+Arterial+Calcification+to+Predict+Cardiovascular+Risk+Theodorus+Dapamede+Aisha+Urooj+Vedant+Joshi+Gabrielle+Gershon+Frank+Li",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "T Dapamede",
        "id": "bgn8HTQAAAAJ"
      },
      {
        "name": "A Urooj",
        "id": "ceiuCp4AAAAJ"
      },
      {
        "name": "V Joshi",
        "id": "HKiftDoAAAAJ"
      },
      {
        "name": "G Gershon",
        "id": "xc6irCkAAAAJ"
      },
      {
        "name": "F Li",
        "id": "TJCzKVUAAAAJ"
      },
      {
        "name": "M Chavoshi",
        "id": "iSBWHPwAAAAJ"
      },
      {
        "name": "B Brown-Mulry",
        "id": "onvWbDAAAAAJ"
      },
      {
        "name": "RS Isaac",
        "id": "bMNBgY8AAAAJ"
      }
    ],
    "citation_count": 1
  },
  {
    "arxiv_id": "2307.16508",
    "title": "Towards General Low-Light Raw Noise Synthesis and Modeling",
    "year": 2023,
    "published": "2023-07-31T09:10:10Z",
    "categories": [
      "cs.CV",
      "cs.MM",
      "eess.IV"
    ],
    "abstract": "Modeling and synthesizing low-light raw noise is a fundamental problem for computational photography and image processing applications. Although most recent works have adopted physics-based models to synthesize noise, the signal-independent noise in low-light conditions is far more complicated and varies dramatically across camera sensors, which is beyond the description of these models. To address this issue, we introduce a new perspective to synthesize the signal-independent noise by a generat",
    "arxiv_url": "https://arxiv.org/abs/2307.16508v2",
    "pdf_url": "https://arxiv.org/pdf/2307.16508v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2307.16508",
    "arxiv_authors": [
      "Feng Zhang",
      "Bin Xu",
      "Zhiqiang Li",
      "Xinran Liu",
      "Qingbo Lu",
      "Changxin Gao",
      "Nong Sang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Towards+General+Low-Light+Raw+Noise+Synthesis+and+Modeling+Feng+Zhang+Bin+Xu+Zhiqiang+Li+Xinran+Liu+Qingbo+Lu",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "F Zhang",
        "id": "iJ0tdroAAAAJ"
      },
      {
        "name": "B Xu",
        "id": null
      },
      {
        "name": "Z Li",
        "id": null
      },
      {
        "name": "X Liu",
        "id": null
      },
      {
        "name": "Q Lu",
        "id": null
      },
      {
        "name": "C Gao",
        "id": "4tku-lwAAAAJ"
      },
      {
        "name": "N Sang",
        "id": "ky_ZowEAAAAJ"
      }
    ],
    "citation_count": 30
  },
  {
    "arxiv_id": "2310.11867",
    "title": "Evaluating the Fairness of Discriminative Foundation Models in Computer Vision",
    "year": 2023,
    "published": "2023-10-18T10:32:39Z",
    "categories": [
      "cs.CV",
      "cs.CY",
      "cs.LG"
    ],
    "abstract": "We propose a novel taxonomy for bias evaluation of discriminative foundation models, such as Contrastive Language-Pretraining (CLIP), that are used for labeling tasks. We then systematically evaluate existing methods for mitigating bias in these models with respect to our taxonomy. Specifically, we evaluate OpenAI's CLIP and OpenCLIP models for key applications, such as zero-shot classification, image retrieval and image captioning. We categorize desired behaviors based around three axes: (i) if",
    "arxiv_url": "https://arxiv.org/abs/2310.11867v1",
    "pdf_url": "https://arxiv.org/pdf/2310.11867v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2310.11867",
    "arxiv_authors": [
      "Junaid Ali",
      "Matthaeus Kleindessner",
      "Florian Wenzel",
      "Kailash Budhathoki",
      "Volkan Cevher",
      "Chris Russell"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Evaluating+the+Fairness+of+Discriminative+Foundation+Models+in+Computer+Vision+Junaid+Ali+Matthaeus+Kleindessner+Florian+Wenzel+Kailash+Budhathoki+Volkan+Cevher",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "J Ali",
        "id": "9oglaaIAAAAJ"
      },
      {
        "name": "M Kleindessner",
        "id": "I_2j2bUAAAAJ"
      },
      {
        "name": "F Wenzel",
        "id": "6TmK--IAAAAJ"
      },
      {
        "name": "K Budhathoki",
        "id": "O5yaQbgAAAAJ"
      },
      {
        "name": "V Cevher",
        "id": "hlWhzU8AAAAJ"
      },
      {
        "name": "C Russell",
        "id": "RM2sHhYAAAAJ"
      }
    ],
    "citation_count": 18
  },
  {
    "arxiv_id": "2307.00811",
    "title": "Review helps learn better: Temporal Supervised Knowledge Distillation",
    "year": 2023,
    "published": "2023-07-03T07:51:08Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Reviewing plays an important role when learning knowledge. The knowledge acquisition at a certain time point may be strongly inspired with the help of previous experience. Thus the knowledge growing procedure should show strong relationship along the temporal dimension. In our research, we find that during the network training, the evolution of feature map follows temporal sequence property. A proper temporal supervision may further improve the network training performance. Inspired by this obse",
    "arxiv_url": "https://arxiv.org/abs/2307.00811v3",
    "pdf_url": "https://arxiv.org/pdf/2307.00811v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2307.00811",
    "arxiv_authors": [
      "Dongwei Wang",
      "Zhi Han",
      "Yanmei Wang",
      "Xiai Chen",
      "Baichen Liu",
      "Yandong Tang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Review+helps+learn+better%3A+Temporal+Supervised+Knowledge+Distillation+Dongwei+Wang+Zhi+Han+Yanmei+Wang+Xiai+Chen+Baichen+Liu",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "D Wang",
        "id": null
      },
      {
        "name": "Z Han",
        "id": "Zl54a74AAAAJ"
      },
      {
        "name": "Y Wang",
        "id": null
      },
      {
        "name": "X Chen",
        "id": "zcpFs-QAAAAJ"
      },
      {
        "name": "B Liu",
        "id": null
      },
      {
        "name": "Y Tang",
        "id": "CaglmKAAAAAJ"
      }
    ],
    "citation_count": null
  },
  {
    "arxiv_id": "2401.10224",
    "title": "The Manga Whisperer: Automatically Generating Transcriptions for Comics",
    "year": 2024,
    "published": "2024-01-18T18:59:09Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "In the past few decades, Japanese comics, commonly referred to as Manga, have transcended both cultural and linguistic boundaries to become a true worldwide sensation. Yet, the inherent reliance on visual cues and illustration within manga renders it largely inaccessible to individuals with visual impairments. In this work, we seek to address this substantial barrier, with the aim of ensuring that manga can be appreciated and actively engaged by everyone. Specifically, we tackle the problem of d",
    "arxiv_url": "https://arxiv.org/abs/2401.10224v3",
    "pdf_url": "https://arxiv.org/pdf/2401.10224v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2401.10224",
    "arxiv_authors": [
      "Ragav Sachdeva",
      "Andrew Zisserman"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=The+Manga+Whisperer%3A+Automatically+Generating+Transcriptions+for+Comics+Ragav+Sachdeva+Andrew+Zisserman",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "R Sachdeva",
        "id": "js1EQ8oAAAAJ"
      },
      {
        "name": "A Zisserman -",
        "id": null
      }
    ],
    "citation_count": 24
  },
  {
    "arxiv_id": "2308.14861",
    "title": "Evaluation of Key Spatiotemporal Learners for Print Track Anomaly Classification Using Melt Pool Image Streams",
    "year": 2023,
    "published": "2023-08-28T19:31:53Z",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "abstract": "Recent applications of machine learning in metal additive manufacturing (MAM) have demonstrated significant potential in addressing critical barriers to the widespread adoption of MAM technology. Recent research in this field emphasizes the importance of utilizing melt pool signatures for real-time defect prediction. While high-quality melt pool image data holds the promise of enabling precise predictions, there has been limited exploration into the utilization of cutting-edge spatiotemporal mod",
    "arxiv_url": "https://arxiv.org/abs/2308.14861v1",
    "pdf_url": "https://arxiv.org/pdf/2308.14861v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2308.14861",
    "arxiv_authors": [
      "Lynn Cherif",
      "Mutahar Safdar",
      "Guy Lamouche",
      "Priti Wanjara",
      "Padma Paul",
      "Gentry Wood",
      "Max Zimmermann",
      "Florian Hannesen",
      "Yaoyao Fiona Zhao"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Evaluation+of+Key+Spatiotemporal+Learners+for+Print+Track+Anomaly+Classification+Using+Melt+Pool+Image+Streams+Lynn+Cherif+Mutahar+Safdar+Guy+Lamouche+Priti+Wanjara+Padma+Paul",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "L Cherif",
        "id": "QfmZ_bEAAAAJ"
      },
      {
        "name": "M Safdar",
        "id": "H-C9MGcAAAAJ"
      },
      {
        "name": "G Lamouche",
        "id": "DpahJuYAAAAJ"
      },
      {
        "name": "P Wanjara",
        "id": "FKgO-F0AAAAJ"
      },
      {
        "name": "P Paul",
        "id": null
      }
    ],
    "citation_count": 6
  },
  {
    "arxiv_id": "2411.00916",
    "title": "Enhancing Osteoporosis Detection: An Explainable Multi-Modal Learning Framework with Feature Fusion and Variable Clustering",
    "year": 2024,
    "published": "2024-11-01T13:58:15Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Osteoporosis is a common condition that increases fracture risk, especially in older adults. Early diagnosis is vital for preventing fractures, reducing treatment costs, and preserving mobility. However, healthcare providers face challenges like limited labeled data and difficulties in processing medical images. This study presents a novel multi-modal learning framework that integrates clinical and imaging data to improve diagnostic accuracy and model interpretability. The model utilizes three p",
    "arxiv_url": "https://arxiv.org/abs/2411.00916v3",
    "pdf_url": "https://arxiv.org/pdf/2411.00916v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2411.00916",
    "arxiv_authors": [
      "Mehdi Hosseini Chagahi",
      "Saeed Mohammadi Dashtaki",
      "Niloufar Delfan",
      "Nadia Mohammadi",
      "Farshid Rostami Pouria",
      "Behzad Moshiri",
      "Md. Jalil Piran",
      "Oliver Faust"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Enhancing+Osteoporosis+Detection%3A+An+Explainable+Multi-Modal+Learning+Framework+with+Feature+Fusion+and+Variable+Clustering+Mehdi+Hosseini+Chagahi+Saeed+Mohammadi+Dashtaki+Niloufar+Delfan+Nadia+Mohammadi+Farshid+Rostami+Pouria",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "MH Chagahi",
        "id": "SFj74hAAAAAJ"
      },
      {
        "name": "SM Dashtaki",
        "id": "WCwL-LsAAAAJ"
      },
      {
        "name": "N Delfan",
        "id": null
      },
      {
        "name": "N Mohammadi",
        "id": null
      },
      {
        "name": "FR Pouria",
        "id": null
      },
      {
        "name": "B Moshiri",
        "id": "SmL7-6YAAAAJ"
      },
      {
        "name": "MJ Piran",
        "id": "Vfi6CSYAAAAJ"
      }
    ],
    "citation_count": 7
  },
  {
    "arxiv_id": "2302.10260",
    "title": "Unsupervised Learning on a DIET: Datum IndEx as Target Free of Self-Supervision, Reconstruction, Projector Head",
    "year": 2023,
    "published": "2023-02-20T19:46:54Z",
    "categories": [
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "Costly, noisy, and over-specialized, labels are to be set aside in favor of unsupervised learning if we hope to learn cheap, reliable, and transferable models. To that end, spectral embedding, self-supervised learning, or generative modeling have offered competitive solutions. Those methods however come with numerous challenges \\textit{e.g.} estimating geodesic distances, specifying projector architectures and anti-collapse losses, or specifying decoder architectures and reconstruction losses. I",
    "arxiv_url": "https://arxiv.org/abs/2302.10260v1",
    "pdf_url": "https://arxiv.org/pdf/2302.10260v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2302.10260",
    "arxiv_authors": [
      "Randall Balestriero"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Unsupervised+Learning+on+a+DIET%3A+Datum+IndEx+as+Target+Free+of+Self-Supervision%2C+Reconstruction%2C+Projector+Head+Randall+Balestriero",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "R Balestriero -",
        "id": null
      }
    ],
    "citation_count": 2
  },
  {
    "arxiv_id": "2405.20141",
    "title": "OpenDAS: Open-Vocabulary Domain Adaptation for 2D and 3D Segmentation",
    "year": 2024,
    "published": "2024-05-30T15:16:06Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Recently, Vision-Language Models (VLMs) have advanced segmentation techniques by shifting from the traditional segmentation of a closed-set of predefined object classes to open-vocabulary segmentation (OVS), allowing users to segment novel classes and concepts unseen during training of the segmentation model. However, this flexibility comes with a trade-off: fully-supervised closed-set methods still outperform OVS methods on base classes, that is on classes on which they have been explicitly tra",
    "arxiv_url": "https://arxiv.org/abs/2405.20141v4",
    "pdf_url": "https://arxiv.org/pdf/2405.20141v4",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2405.20141",
    "arxiv_authors": [
      "Gonca Yilmaz",
      "Songyou Peng",
      "Marc Pollefeys",
      "Francis Engelmann",
      "Hermann Blum"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=OpenDAS%3A+Open-Vocabulary+Domain+Adaptation+for+2D+and+3D+Segmentation+Gonca+Yilmaz+Songyou+Peng+Marc+Pollefeys+Francis+Engelmann+Hermann+Blum",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "G Yilmaz",
        "id": "kS1SJIkAAAAJ"
      },
      {
        "name": "S Peng",
        "id": "eNypkO0AAAAJ"
      },
      {
        "name": "M Pollefeys",
        "id": "YYH0BjEAAAAJ"
      },
      {
        "name": "F Engelmann",
        "id": "-xOsXi8AAAAJ"
      },
      {
        "name": "H Blum",
        "id": "2Pxx8QIAAAAJ"
      }
    ],
    "citation_count": 3
  },
  {
    "arxiv_id": "2401.09471",
    "title": "Brain Tumor Radiogenomic Classification",
    "year": 2024,
    "published": "2024-01-11T10:30:09Z",
    "categories": [
      "eess.IV",
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "The RSNA-MICCAI brain tumor radiogenomic classification challenge aimed to predict MGMT biomarker status in glioblastoma through binary classification on Multi parameter mpMRI scans: T1w, T1wCE, T2w and FLAIR. The dataset is splitted into three main cohorts: training set, validation set which were used during training, and the testing were only used during final evaluation. Images were either in a DICOM format or in Png format. different architectures were used to investigate the problem includi",
    "arxiv_url": "https://arxiv.org/abs/2401.09471v1",
    "pdf_url": "https://arxiv.org/pdf/2401.09471v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2401.09471",
    "arxiv_authors": [
      "Amr Mohamed",
      "Mahmoud Rabea",
      "Aya Sameh",
      "Ehab Kamal"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Brain+Tumor+Radiogenomic+Classification+Amr+Mohamed+Mahmoud+Rabea+Aya+Sameh+Ehab+Kamal",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "A Mohamed",
        "id": "9Ijh2M8AAAAJ"
      },
      {
        "name": "M Rabea",
        "id": "LbT6zU4AAAAJ"
      },
      {
        "name": "A Sameh",
        "id": null
      },
      {
        "name": "E Kamal -",
        "id": null
      }
    ],
    "citation_count": 9
  },
  {
    "arxiv_id": "2404.08515",
    "title": "ChatGPT and general-purpose AI count fruits in pictures surprisingly well",
    "year": 2024,
    "published": "2024-04-12T14:54:34Z",
    "categories": [
      "cs.CV",
      "eess.IV"
    ],
    "abstract": "Object counting is a popular task in deep learning applications in various domains, including agriculture. A conventional deep learning approach requires a large amount of training data, often a logistic problem in a real-world application. To address this issue, we examined how well ChatGPT (GPT4V) and a general-purpose AI (foundation model for object counting, T-Rex) can count the number of fruit bodies (coffee cherries) in 100 images. The foundation model with few-shot learning outperformed t",
    "arxiv_url": "https://arxiv.org/abs/2404.08515v1",
    "pdf_url": "https://arxiv.org/pdf/2404.08515v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2404.08515",
    "arxiv_authors": [
      "Konlavach Mengsuwan",
      "Juan Camilo Rivera Palacio",
      "Masahiro Ryo"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=ChatGPT+and+general-purpose+AI+count+fruits+in+pictures+surprisingly+well+Konlavach+Mengsuwan+Juan+Camilo+Rivera+Palacio+Masahiro+Ryo",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "K Mengsuwan",
        "id": "Uf8uv0UAAAAJ"
      },
      {
        "name": "JCR Palacio",
        "id": "4lFIZq8AAAAJ"
      },
      {
        "name": "M Ryo -",
        "id": null
      }
    ],
    "citation_count": 1
  },
  {
    "arxiv_id": "2408.15450",
    "title": "Avoiding Generative Model Writer's Block With Embedding Nudging",
    "year": 2024,
    "published": "2024-08-28T00:07:51Z",
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "abstract": "Generative image models, since introduction, have become a global phenomenon. From new arts becoming possible to new vectors of abuse, many new capabilities have become available. One of the challenging issues with generative models is controlling the generation process specially to prevent specific generations classes or instances . There are several reasons why one may want to control the output of generative models, ranging from privacy and safety concerns to application limitations or user p",
    "arxiv_url": "https://arxiv.org/abs/2408.15450v1",
    "pdf_url": "https://arxiv.org/pdf/2408.15450v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2408.15450",
    "arxiv_authors": [
      "Ali Zand",
      "Milad Nasr"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Avoiding+Generative+Model+Writer%27s+Block+With+Embedding+Nudging+Ali+Zand+Milad+Nasr",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "A Zand",
        "id": "229b7dkAAAAJ"
      },
      {
        "name": "M Nasr -",
        "id": null
      }
    ],
    "citation_count": null
  },
  {
    "arxiv_id": "2309.03750",
    "title": "PBP: Path-based Trajectory Prediction for Autonomous Driving",
    "year": 2023,
    "published": "2023-09-07T14:45:41Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Trajectory prediction plays a crucial role in the autonomous driving stack by enabling autonomous vehicles to anticipate the motion of surrounding agents. Goal-based prediction models have gained traction in recent years for addressing the multimodal nature of future trajectories. Goal-based prediction models simplify multimodal prediction by first predicting 2D goal locations of agents and then predicting trajectories conditioned on each goal. However, a single 2D goal location serves as a weak",
    "arxiv_url": "https://arxiv.org/abs/2309.03750v2",
    "pdf_url": "https://arxiv.org/pdf/2309.03750v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2309.03750",
    "arxiv_authors": [
      "Sepideh Afshar",
      "Nachiket Deo",
      "Akshay Bhagat",
      "Titas Chakraborty",
      "Yunming Shao",
      "Balarama Raju Buddharaju",
      "Adwait Deshpande",
      "Henggang Cui"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=PBP%3A+Path-based+Trajectory+Prediction+for+Autonomous+Driving+Sepideh+Afshar+Nachiket+Deo+Akshay+Bhagat+Titas+Chakraborty+Yunming+Shao",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "S Afshar",
        "id": null
      },
      {
        "name": "N Deo",
        "id": "Jek-NhkAAAAJ"
      },
      {
        "name": "A Bhagat",
        "id": "miYtpNIAAAAJ"
      },
      {
        "name": "T Chakraborty",
        "id": "OPsy9P4AAAAJ"
      },
      {
        "name": "Y Shao",
        "id": "1VHHJjQAAAAJ"
      },
      {
        "name": "BR Buddharaju",
        "id": null
      },
      {
        "name": "A Deshpande",
        "id": "7sOrFjQAAAAJ"
      }
    ],
    "citation_count": 12
  },
  {
    "arxiv_id": "2503.04953",
    "title": "Spectral Informed Mamba for Robust Point Cloud Processing",
    "year": 2025,
    "published": "2025-03-06T20:32:59Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "State space models have shown significant promise in Natural Language Processing (NLP) and, more recently, computer vision. This paper introduces a new methodology leveraging Mamba and Masked Autoencoder networks for point cloud data in both supervised and self-supervised learning. We propose three key contributions to enhance Mamba's capability in processing complex point cloud structures. First, we exploit the spectrum of a graph Laplacian to capture patch connectivity, defining an isometry-in",
    "arxiv_url": "https://arxiv.org/abs/2503.04953v2",
    "pdf_url": "https://arxiv.org/pdf/2503.04953v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2503.04953",
    "arxiv_authors": [
      "Ali Bahri",
      "Moslem Yazdanpanah",
      "Mehrdad Noori",
      "Sahar Dastani",
      "Milad Cheraghalikhani",
      "David Osowiechi",
      "Gustavo Adolfo Vargas Hakim",
      "Farzad Beizaee",
      "Ismail Ben Ayed",
      "Christian Desrosiers"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Spectral+Informed+Mamba+for+Robust+Point+Cloud+Processing+Ali+Bahri+Moslem+Yazdanpanah+Mehrdad+Noori+Sahar+Dastani+Milad+Cheraghalikhani",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "A Bahri",
        "id": "0gSvAv8AAAAJ"
      },
      {
        "name": "M Yazdanpanah",
        "id": "lt7_RKkAAAAJ"
      },
      {
        "name": "M Noori",
        "id": "01l47WEAAAAJ"
      },
      {
        "name": "S Dastani",
        "id": "wxbHuCsAAAAJ"
      },
      {
        "name": "M Cheraghalikhani",
        "id": "MWafinQAAAAJ"
      },
      {
        "name": "GAV Hakim",
        "id": "9q4ltOcAAAAJ"
      },
      {
        "name": "D Osowiechi",
        "id": "Rzb_YhgAAAAJ"
      }
    ],
    "citation_count": 5
  },
  {
    "arxiv_id": "2503.05051",
    "title": "Accelerated Patient-specific Non-Cartesian MRI Reconstruction using Implicit Neural Representations",
    "year": 2025,
    "published": "2025-03-07T00:05:43Z",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "abstract": "The scanning time for a fully sampled MRI can be undesirably lengthy. Compressed sensing has been developed to minimize image artifacts in accelerated scans, but the required iterative reconstruction is computationally complex and difficult to generalize on new cases. Image-domain-based deep learning methods (e.g., convolutional neural networks) emerged as a faster alternative but face challenges in modeling continuous k-space, a problem amplified with non-Cartesian sampling commonly used in acc",
    "arxiv_url": "https://arxiv.org/abs/2503.05051v1",
    "pdf_url": "https://arxiv.org/pdf/2503.05051v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2503.05051",
    "arxiv_authors": [
      "Di Xu",
      "Hengjie Liu",
      "Xin Miao",
      "Daniel O'Connor",
      "Jessica E. Scholey",
      "Wensha Yang",
      "Mary Feng",
      "Michael Ohliger",
      "Hui Lin",
      "Dan Ruan",
      "Yang Yang",
      "Ke Sheng"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Accelerated+Patient-specific+Non-Cartesian+MRI+Reconstruction+using+Implicit+Neural+Representations+Di+Xu+Hengjie+Liu+Xin+Miao+Daniel+O%27Connor+Jessica+E.+Scholey",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "D Xu",
        "id": "2jrPdDgAAAAJ"
      },
      {
        "name": "H Liu",
        "id": "LbXcdmsAAAAJ"
      },
      {
        "name": "X Miao",
        "id": null
      },
      {
        "name": "D O'Connor",
        "id": "gK_sUR8AAAAJ"
      },
      {
        "name": "JE Scholey",
        "id": "gwSqxwEAAAAJ"
      }
    ],
    "citation_count": null
  },
  {
    "arxiv_id": "2308.08276",
    "title": "Computer vision-enriched discrete choice models, with an application to residential location choice",
    "year": 2023,
    "published": "2023-08-16T10:33:24Z",
    "categories": [
      "cs.CV",
      "econ.EM"
    ],
    "abstract": "Visual imagery is indispensable to many multi-attribute decision situations. Examples of such decision situations in travel behaviour research include residential location choices, vehicle choices, tourist destination choices, and various safety-related choices. However, current discrete choice models cannot handle image data and thus cannot incorporate information embedded in images into their representations of choice behaviour. This gap between discrete choice models' capabilities and the rea",
    "arxiv_url": "https://arxiv.org/abs/2308.08276v1",
    "pdf_url": "https://arxiv.org/pdf/2308.08276v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2308.08276",
    "arxiv_authors": [
      "Sander van Cranenburgh",
      "Francisco Garrido-Valenzuela"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Computer+vision-enriched+discrete+choice+models%2C+with+an+application+to+residential+location+choice+Sander+van+Cranenburgh+Francisco+Garrido-Valenzuela",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "S van Cranenburgh",
        "id": "-FNYwWYAAAAJ"
      },
      {
        "name": "F Garrido-Valenzuela - Transportation Research Part",
        "id": null
      }
    ],
    "citation_count": 6
  },
  {
    "arxiv_id": "2402.18213",
    "title": "Multi-objective Differentiable Neural Architecture Search",
    "year": 2024,
    "published": "2024-02-28T10:09:04Z",
    "categories": [
      "cs.LG",
      "cs.CV",
      "stat.ML"
    ],
    "abstract": "Pareto front profiling in multi-objective optimization (MOO), i.e., finding a diverse set of Pareto optimal solutions, is challenging, especially with expensive objectives that require training a neural network. Typically, in MOO for neural architecture search (NAS), we aim to balance performance and hardware metrics across devices. Prior NAS approaches simplify this task by incorporating hardware constraints into the objective function, but profiling the Pareto front necessitates a computationa",
    "arxiv_url": "https://arxiv.org/abs/2402.18213v3",
    "pdf_url": "https://arxiv.org/pdf/2402.18213v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2402.18213",
    "arxiv_authors": [
      "Rhea Sanjay Sukthanker",
      "Arber Zela",
      "Benedikt Staffler",
      "Samuel Dooley",
      "Josif Grabocka",
      "Frank Hutter"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Multi-objective+Differentiable+Neural+Architecture+Search+Rhea+Sanjay+Sukthanker+Arber+Zela+Benedikt+Staffler+Samuel+Dooley+Josif+Grabocka",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "RS Sukthanker",
        "id": "OsamqmMAAAAJ"
      },
      {
        "name": "A Zela",
        "id": "hD_6YioAAAAJ"
      },
      {
        "name": "B Staffler",
        "id": "kByZ7WYAAAAJ"
      },
      {
        "name": "S Dooley",
        "id": "BNZlRMsAAAAJ"
      },
      {
        "name": "J Grabocka",
        "id": "KRy27XcAAAAJ"
      },
      {
        "name": "F Hutter",
        "id": "YUrxwrkAAAAJ"
      }
    ],
    "citation_count": 10
  },
  {
    "arxiv_id": "2302.06874",
    "title": "Robust Representation Learning with Self-Distillation for Domain Generalization",
    "year": 2023,
    "published": "2023-02-14T07:39:37Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Despite the recent success of deep neural networks, there remains a need for effective methods to enhance domain generalization using vision transformers. In this paper, we propose a novel domain generalization technique called Robust Representation Learning with Self-Distillation (RRLD) comprising i) intermediate-block self-distillation and ii) augmentation-guided self-distillation to improve the generalization capabilities of transformer-based models on unseen domains. This approach enables th",
    "arxiv_url": "https://arxiv.org/abs/2302.06874v2",
    "pdf_url": "https://arxiv.org/pdf/2302.06874v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2302.06874",
    "arxiv_authors": [
      "Ankur Singh",
      "Senthilnath Jayavelu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Robust+Representation+Learning+with+Self-Distillation+for+Domain+Generalization+Ankur+Singh+Senthilnath+Jayavelu",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "A Singh",
        "id": "qepKadQAAAAJ"
      },
      {
        "name": "S Jayavelu -",
        "id": null
      }
    ],
    "citation_count": 3
  },
  {
    "arxiv_id": "2403.01440",
    "title": "Pyramid Feature Attention Network for Monocular Depth Prediction",
    "year": 2024,
    "published": "2024-03-03T08:33:23Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Deep convolutional neural networks (DCNNs) have achieved great success in monocular depth estimation (MDE). However, few existing works take the contributions for MDE of different levels feature maps into account, leading to inaccurate spatial layout, ambiguous boundaries and discontinuous object surface in the prediction. To better tackle these problems, we propose a Pyramid Feature Attention Network (PFANet) to improve the high-level context features and low-level spatial features. In the prop",
    "arxiv_url": "https://arxiv.org/abs/2403.01440v1",
    "pdf_url": "https://arxiv.org/pdf/2403.01440v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2403.01440",
    "arxiv_authors": [
      "Yifang Xu",
      "Chenglei Peng",
      "Ming Li",
      "Yang Li",
      "Sidan Du"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Pyramid+Feature+Attention+Network+for+Monocular+Depth+Prediction+Yifang+Xu+Chenglei+Peng+Ming+Li+Yang+Li+Sidan+Du",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "Y Xu",
        "id": "iD7FL6UAAAAJ"
      },
      {
        "name": "C Peng",
        "id": "UjoxWJYAAAAJ"
      },
      {
        "name": "M Li",
        "id": "0zLyGOUAAAAJ"
      },
      {
        "name": "Y Li",
        "id": "OE_KNZgAAAAJ"
      },
      {
        "name": "S Du -",
        "id": null
      }
    ],
    "citation_count": 27
  },
  {
    "arxiv_id": "2412.01672",
    "title": "Gen-SIS: Generative Self-augmentation Improves Self-supervised Learning",
    "year": 2024,
    "published": "2024-12-02T16:20:59Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Self-supervised learning (SSL) methods have emerged as strong visual representation learners by training an image encoder to maximize similarity between features of different views of the same image. To perform this view-invariance task, current SSL algorithms rely on hand-crafted augmentations such as random cropping and color jittering to create multiple views of an image. Recently, generative diffusion models have been shown to improve SSL by providing a wider range of data augmentations. How",
    "arxiv_url": "https://arxiv.org/abs/2412.01672v1",
    "pdf_url": "https://arxiv.org/pdf/2412.01672v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2412.01672",
    "arxiv_authors": [
      "Varun Belagali",
      "Srikar Yellapragada",
      "Alexandros Graikos",
      "Saarthak Kapse",
      "Zilinghan Li",
      "Tarak Nath Nandi",
      "Ravi K Madduri",
      "Prateek Prasanna",
      "Joel Saltz",
      "Dimitris Samaras"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Gen-SIS%3A+Generative+Self-augmentation+Improves+Self-supervised+Learning+Varun+Belagali+Srikar+Yellapragada+Alexandros+Graikos+Saarthak+Kapse+Zilinghan+Li",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "V Belagali",
        "id": "tqEHnLUAAAAJ"
      },
      {
        "name": "S Yellapragada",
        "id": "DVMnHboAAAAJ"
      },
      {
        "name": "A Graikos",
        "id": "1J7ZAUAAAAAJ"
      },
      {
        "name": "S Kapse",
        "id": "WgDf0fcAAAAJ"
      },
      {
        "name": "Z Li",
        "id": "4JbL29YAAAAJ"
      },
      {
        "name": "TN Nandi",
        "id": "bOi9SDUAAAAJ"
      },
      {
        "name": "RK Madduri",
        "id": "1NoINjMAAAAJ"
      },
      {
        "name": "P Prasanna",
        "id": "uyA1Q18AAAAJ"
      }
    ],
    "citation_count": 7
  },
  {
    "arxiv_id": "2412.00283",
    "title": "Hyperspectral Images Efficient Spatial and Spectral non-Linear Model with Bidirectional Feature Learning",
    "year": 2024,
    "published": "2024-11-29T23:32:26Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Classifying hyperspectral images (HSIs) is a complex task in remote sensing due to the high-dimensional nature and volume of data involved. To address these challenges, we propose the Spectral-Spatial non-Linear Model, a novel framework that significantly reduces data volume while enhancing classification accuracy. Our model employs a bidirectional reversed convolutional neural network (CNN) to efficiently extract spectral features, complemented by a specialized block for spatial feature analysi",
    "arxiv_url": "https://arxiv.org/abs/2412.00283v2",
    "pdf_url": "https://arxiv.org/pdf/2412.00283v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2412.00283",
    "arxiv_authors": [
      "Judy X Yang",
      "Jing Wang",
      "Zekun Long",
      "Chenhong Sui",
      "Jun Zhou"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Hyperspectral+Images+Efficient+Spatial+and+Spectral+non-Linear+Model+with+Bidirectional+Feature+Learning+Judy+X+Yang+Jing+Wang+Zekun+Long+Chenhong+Sui+Jun+Zhou",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "JX Yang",
        "id": null
      },
      {
        "name": "J Wang",
        "id": null
      },
      {
        "name": "Z Long",
        "id": "jV9HkjkAAAAJ"
      },
      {
        "name": "C Sui",
        "id": null
      },
      {
        "name": "J Zhou -",
        "id": null
      }
    ],
    "citation_count": null
  },
  {
    "arxiv_id": "2401.08541",
    "title": "Scalable Pre-training of Large Autoregressive Image Models",
    "year": 2024,
    "published": "2024-01-16T18:03:37Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "This paper introduces AIM, a collection of vision models pre-trained with an autoregressive objective. These models are inspired by their textual counterparts, i.e., Large Language Models (LLMs), and exhibit similar scaling properties. Specifically, we highlight two key findings: (1) the performance of the visual features scale with both the model capacity and the quantity of data, (2) the value of the objective function correlates with the performance of the model on downstream tasks. We illust",
    "arxiv_url": "https://arxiv.org/abs/2401.08541v1",
    "pdf_url": "https://arxiv.org/pdf/2401.08541v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2401.08541",
    "arxiv_authors": [
      "Alaaeldin El-Nouby",
      "Michal Klein",
      "Shuangfei Zhai",
      "Miguel Angel Bautista",
      "Alexander Toshev",
      "Vaishaal Shankar",
      "Joshua M Susskind",
      "Armand Joulin"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Scalable+Pre-training+of+Large+Autoregressive+Image+Models+Alaaeldin+El-Nouby+Michal+Klein+Shuangfei+Zhai+Miguel+Angel+Bautista+Alexander+Toshev",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "A El-Nouby",
        "id": "jxpBMwwAAAAJ"
      },
      {
        "name": "M Klein",
        "id": "zByzdzcAAAAJ"
      },
      {
        "name": "S Zhai",
        "id": "G6vdBYsAAAAJ"
      },
      {
        "name": "MA Bautista",
        "id": "ZrRs-qoAAAAJ"
      },
      {
        "name": "A Toshev",
        "id": "T6PbwPIAAAAJ"
      },
      {
        "name": "V Shankar",
        "id": "cUkE7OgAAAAJ"
      },
      {
        "name": "JM Susskind",
        "id": "Sv2TGqsAAAAJ"
      },
      {
        "name": "A Joulin",
        "id": "kRJkDakAAAAJ"
      }
    ],
    "citation_count": 111
  },
  {
    "arxiv_id": "2302.03868",
    "title": "A Generalized Surface Loss for Reducing the Hausdorff Distance in Medical Imaging Segmentation",
    "year": 2023,
    "published": "2023-02-08T04:01:42Z",
    "categories": [
      "eess.IV",
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "Within medical imaging segmentation, the Dice coefficient and Hausdorff-based metrics are standard measures of success for deep learning models. However, modern loss functions for medical image segmentation often only consider the Dice coefficient or similar region-based metrics during training. As a result, segmentation architectures trained over such loss functions run the risk of achieving high accuracy for the Dice coefficient but low accuracy for Hausdorff-based metrics. Low accuracy on Hau",
    "arxiv_url": "https://arxiv.org/abs/2302.03868v3",
    "pdf_url": "https://arxiv.org/pdf/2302.03868v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2302.03868",
    "arxiv_authors": [
      "Adrian Celaya",
      "Beatrice Riviere",
      "David Fuentes"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=A+Generalized+Surface+Loss+for+Reducing+the+Hausdorff+Distance+in+Medical+Imaging+Segmentation+Adrian+Celaya+Beatrice+Riviere+David+Fuentes",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "A Celaya",
        "id": "IaiEG0AAAAAJ"
      },
      {
        "name": "B Riviere",
        "id": "o1KZnG0AAAAJ"
      },
      {
        "name": "D Fuentes -",
        "id": null
      }
    ],
    "citation_count": 16
  },
  {
    "arxiv_id": "2406.18996",
    "title": "Zero-shot domain adaptation based on dual-level mix and contrast",
    "year": 2024,
    "published": "2024-06-27T08:37:26Z",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "Zero-shot domain adaptation (ZSDA) is a domain adaptation problem in the situation that labeled samples for a target task (task of interest) are only available from the source domain at training time, but for a task different from the task of interest (irrelevant task), labeled samples are available from both source and target domains. In this situation, classical domain adaptation techniques can only learn domain-invariant features in the irrelevant task. However, due to the difference in sampl",
    "arxiv_url": "https://arxiv.org/abs/2406.18996v1",
    "pdf_url": "https://arxiv.org/pdf/2406.18996v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2406.18996",
    "arxiv_authors": [
      "Yu Zhe",
      "Jun Sakuma"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Zero-shot+domain+adaptation+based+on+dual-level+mix+and+contrast+Yu+Zhe+Jun+Sakuma",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "Y Zhe",
        "id": "sn0atvMAAAAJ"
      },
      {
        "name": "J Sakuma -",
        "id": null
      }
    ],
    "citation_count": null
  },
  {
    "arxiv_id": "2404.16346",
    "title": "Light-weight Retinal Layer Segmentation with Global Reasoning",
    "year": 2024,
    "published": "2024-04-25T05:42:41Z",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "abstract": "Automatic retinal layer segmentation with medical images, such as optical coherence tomography (OCT) images, serves as an important tool for diagnosing ophthalmic diseases. However, it is challenging to achieve accurate segmentation due to low contrast and blood flow noises presented in the images. In addition, the algorithm should be light-weight to be deployed for practical clinical applications. Therefore, it is desired to design a light-weight network with high performance for retinal layer ",
    "arxiv_url": "https://arxiv.org/abs/2404.16346v1",
    "pdf_url": "https://arxiv.org/pdf/2404.16346v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2404.16346",
    "arxiv_authors": [
      "Xiang He",
      "Weiye Song",
      "Yiming Wang",
      "Fabio Poiesi",
      "Ji Yi",
      "Manishi Desai",
      "Quanqing Xu",
      "Kongzheng Yang",
      "Yi Wan"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Light-weight+Retinal+Layer+Segmentation+with+Global+Reasoning+Xiang+He+Weiye+Song+Yiming+Wang+Fabio+Poiesi+Ji+Yi",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "X He",
        "id": "0MW09ocAAAAJ"
      },
      {
        "name": "W Song",
        "id": "Ybvxp5oAAAAJ"
      },
      {
        "name": "Y Wang",
        "id": "KBZ3zrEAAAAJ"
      },
      {
        "name": "F Poiesi",
        "id": "BQ7li6AAAAAJ"
      },
      {
        "name": "J Yi",
        "id": "0SKC6Y7Ki3EC"
      },
      {
        "name": "M Desai",
        "id": null
      },
      {
        "name": "Q Xu",
        "id": null
      },
      {
        "name": "K Yang",
        "id": null
      },
      {
        "name": "Y WanIEEE transactions on instrumentation and measurement",
        "id": null
      }
    ],
    "citation_count": 23
  },
  {
    "arxiv_id": "2304.14045",
    "title": "Interweaved Graph and Attention Network for 3D Human Pose Estimation",
    "year": 2023,
    "published": "2023-04-27T09:21:15Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "abstract": "Despite substantial progress in 3D human pose estimation from a single-view image, prior works rarely explore global and local correlations, leading to insufficient learning of human skeleton representations. To address this issue, we propose a novel Interweaved Graph and Attention Network (IGANet) that allows bidirectional communications between graph convolutional networks (GCNs) and attentions. Specifically, we introduce an IGA module, where attentions are provided with local information from",
    "arxiv_url": "https://arxiv.org/abs/2304.14045v1",
    "pdf_url": "https://arxiv.org/pdf/2304.14045v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2304.14045",
    "arxiv_authors": [
      "Ti Wang",
      "Hong Liu",
      "Runwei Ding",
      "Wenhao Li",
      "Yingxuan You",
      "Xia Li"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Interweaved+Graph+and+Attention+Network+for+3D+Human+Pose+Estimation+Ti+Wang+Hong+Liu+Runwei+Ding+Wenhao+Li+Yingxuan+You",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "T Wang",
        "id": "ksbjqusAAAAJ"
      },
      {
        "name": "H Liu",
        "id": "WLMUAjsAAAAJ"
      },
      {
        "name": "R Ding",
        "id": "RZorbGsAAAAJ"
      },
      {
        "name": "W Li",
        "id": "U5Y7BjkAAAAJ"
      },
      {
        "name": "Y You",
        "id": "T2e15wkAAAAJ"
      },
      {
        "name": "X LiICASSP",
        "id": null
      }
    ],
    "citation_count": 21
  },
  {
    "arxiv_id": "2503.05639",
    "title": "VideoPainter: Any-length Video Inpainting and Editing with Plug-and-Play Context Control",
    "year": 2025,
    "published": "2025-03-07T17:59:46Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.MM"
    ],
    "abstract": "Video inpainting, which aims to restore corrupted video content, has experienced substantial progress. Despite these advances, existing methods, whether propagating unmasked region pixels through optical flow and receptive field priors, or extending image-inpainting models temporally, face challenges in generating fully masked objects or balancing the competing objectives of background context preservation and foreground generation in one model, respectively. To address these limitations, we pro",
    "arxiv_url": "https://arxiv.org/abs/2503.05639v3",
    "pdf_url": "https://arxiv.org/pdf/2503.05639v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2503.05639",
    "arxiv_authors": [
      "Yuxuan Bian",
      "Zhaoyang Zhang",
      "Xuan Ju",
      "Mingdeng Cao",
      "Liangbin Xie",
      "Ying Shan",
      "Qiang Xu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=VideoPainter%3A+Any-length+Video+Inpainting+and+Editing+with+Plug-and-Play+Context+Control+Yuxuan+Bian+Zhaoyang+Zhang+Xuan+Ju+Mingdeng+Cao+Liangbin+Xie",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "Y Bian",
        "id": "HzemVzoAAAAJ"
      },
      {
        "name": "Z Zhang",
        "id": "Pf6o7uAAAAAJ"
      },
      {
        "name": "X Ju",
        "id": "pWzvK20AAAAJ"
      },
      {
        "name": "M Cao",
        "id": "EcS0L5sAAAAJ"
      },
      {
        "name": "L Xie",
        "id": "auQhf5EAAAAJ"
      },
      {
        "name": "Y Shan",
        "id": "4oXBp9UAAAAJ"
      },
      {
        "name": "Q Xu",
        "id": "eSiKPqUAAAAJ"
      }
    ],
    "citation_count": 20
  },
  {
    "arxiv_id": "2302.14237",
    "title": "Towards Surgical Context Inference and Translation to Gestures",
    "year": 2023,
    "published": "2023-02-28T01:39:36Z",
    "categories": [
      "cs.CV",
      "cs.LG",
      "cs.RO"
    ],
    "abstract": "Manual labeling of gestures in robot-assisted surgery is labor intensive, prone to errors, and requires expertise or training. We propose a method for automated and explainable generation of gesture transcripts that leverages the abundance of data for image segmentation. Surgical context is detected using segmentation masks by examining the distances and intersections between the tools and objects. Next, context labels are translated into gesture transcripts using knowledge-based Finite State Ma",
    "arxiv_url": "https://arxiv.org/abs/2302.14237v2",
    "pdf_url": "https://arxiv.org/pdf/2302.14237v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2302.14237",
    "arxiv_authors": [
      "Kay Hutchinson",
      "Zongyu Li",
      "Ian Reyes",
      "Homa Alemzadeh"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Towards+Surgical+Context+Inference+and+Translation+to+Gestures+Kay+Hutchinson+Zongyu+Li+Ian+Reyes+Homa+Alemzadeh",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "K Hutchinson",
        "id": null
      },
      {
        "name": "Z Li",
        "id": null
      },
      {
        "name": "I Reyes",
        "id": "SEoiA-kAAAAJ"
      },
      {
        "name": "H Alemzadeh -",
        "id": null
      }
    ],
    "citation_count": 6
  },
  {
    "arxiv_id": "2411.16969",
    "title": "ZoomLDM: Latent Diffusion Model for multi-scale image generation",
    "year": 2024,
    "published": "2024-11-25T22:39:22Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Diffusion models have revolutionized image generation, yet several challenges restrict their application to large-image domains, such as digital pathology and satellite imagery. Given that it is infeasible to directly train a model on 'whole' images from domains with potential gigapixel sizes, diffusion-based generative methods have focused on synthesizing small, fixed-size patches extracted from these images. However, generating small patches has limited applicability since patch-based models f",
    "arxiv_url": "https://arxiv.org/abs/2411.16969v2",
    "pdf_url": "https://arxiv.org/pdf/2411.16969v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2411.16969",
    "arxiv_authors": [
      "Srikar Yellapragada",
      "Alexandros Graikos",
      "Kostas Triaridis",
      "Prateek Prasanna",
      "Rajarsi R. Gupta",
      "Joel Saltz",
      "Dimitris Samaras"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=ZoomLDM%3A+Latent+Diffusion+Model+for+multi-scale+image+generation+Srikar+Yellapragada+Alexandros+Graikos+Kostas+Triaridis+Prateek+Prasanna+Rajarsi+R.+Gupta",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "S Yellapragada",
        "id": "DVMnHboAAAAJ"
      },
      {
        "name": "A Graikos",
        "id": "1J7ZAUAAAAAJ"
      },
      {
        "name": "K Triaridis",
        "id": "VAiMaeUAAAAJ"
      },
      {
        "name": "P Prasanna",
        "id": "uyA1Q18AAAAJ"
      },
      {
        "name": "R Gupta",
        "id": "LcEPA3cAAAAJ"
      },
      {
        "name": "J Saltz",
        "id": "_0dkufgAAAAJ"
      },
      {
        "name": "D Samaras",
        "id": "BxbKTYkAAAAJ"
      }
    ],
    "citation_count": 5
  },
  {
    "arxiv_id": "2411.01916",
    "title": "Masked Autoencoders are Parameter-Efficient Federated Continual Learners",
    "year": 2024,
    "published": "2024-11-04T09:28:18Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Federated learning is a specific distributed learning paradigm in which a central server aggregates updates from multiple clients' local models, thereby enabling the server to learn without requiring clients to upload their private data, maintaining data privacy. While existing federated learning methods are primarily designed for static data, real-world applications often require clients to learn new categories over time. This challenge necessitates the integration of continual learning techniq",
    "arxiv_url": "https://arxiv.org/abs/2411.01916v3",
    "pdf_url": "https://arxiv.org/pdf/2411.01916v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2411.01916",
    "arxiv_authors": [
      "Yuchen He",
      "Xiangfeng Wang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Masked+Autoencoders+are+Parameter-Efficient+Federated+Continual+Learners+Yuchen+He+Xiangfeng+Wang",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "Y He",
        "id": "PK6Sn94AAAAJ"
      },
      {
        "name": "X Wang -",
        "id": null
      }
    ],
    "citation_count": 1
  },
  {
    "arxiv_id": "2501.08072",
    "title": "Evaluating Human Perception of Novel View Synthesis: Subjective Quality Assessment of Gaussian Splatting and NeRF in Dynamic Scenes",
    "year": 2025,
    "published": "2025-01-13T10:01:27Z",
    "categories": [
      "cs.CV",
      "eess.IV"
    ],
    "abstract": "Gaussian Splatting (GS) and Neural Radiance Fields (NeRF) are two groundbreaking technologies that have revolutionized the field of Novel View Synthesis (NVS), enabling immersive photorealistic rendering and user experiences by synthesizing multiple viewpoints from a set of images of sparse views. The potential applications of NVS, such as high-quality virtual and augmented reality, detailed 3D modeling, and realistic medical organ imaging, underscore the importance of quality assessment of NVS ",
    "arxiv_url": "https://arxiv.org/abs/2501.08072v1",
    "pdf_url": "https://arxiv.org/pdf/2501.08072v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2501.08072",
    "arxiv_authors": [
      "Yuhang Zhang",
      "Joshua Maraval",
      "Zhengyu Zhang",
      "Nicolas Ramin",
      "Shishun Tian",
      "Lu Zhang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Evaluating+Human+Perception+of+Novel+View+Synthesis%3A+Subjective+Quality+Assessment+of+Gaussian+Splatting+and+NeRF+in+Dynamic+Scenes+Yuhang+Zhang+Joshua+Maraval+Zhengyu+Zhang+Nicolas+Ramin+Shishun+Tian",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "Y Zhang",
        "id": "C-ZGR84AAAAJ"
      },
      {
        "name": "J Maraval",
        "id": null
      },
      {
        "name": "Z Zhang",
        "id": "WZYvDkAAAAAJ"
      },
      {
        "name": "N Ramin",
        "id": null
      },
      {
        "name": "S Tian",
        "id": null
      },
      {
        "name": "L Zhang",
        "id": null
      }
    ],
    "citation_count": 9
  },
  {
    "arxiv_id": "2312.15103",
    "title": "Energy-based learning algorithms for analog computing: a comparative study",
    "year": 2023,
    "published": "2023-12-22T22:49:58Z",
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "abstract": "Energy-based learning algorithms have recently gained a surge of interest due to their compatibility with analog (post-digital) hardware. Existing algorithms include contrastive learning (CL), equilibrium propagation (EP) and coupled learning (CpL), all consisting in contrasting two states, and differing in the type of perturbation used to obtain the second state from the first one. However, these algorithms have never been explicitly compared on equal footing with same models and datasets, maki",
    "arxiv_url": "https://arxiv.org/abs/2312.15103v1",
    "pdf_url": "https://arxiv.org/pdf/2312.15103v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2312.15103",
    "arxiv_authors": [
      "Benjamin Scellier",
      "Maxence Ernoult",
      "Jack Kendall",
      "Suhas Kumar"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Energy-based+learning+algorithms+for+analog+computing%3A+a+comparative+study+Benjamin+Scellier+Maxence+Ernoult+Jack+Kendall+Suhas+Kumar",
    "gs_search_success": false,
    "gs_authors": null,
    "citation_count": null,
    "error_type": "fetch_failed"
  },
  {
    "arxiv_id": "2305.12529",
    "title": "DreamWaltz: Make a Scene with Complex 3D Animatable Avatars",
    "year": 2023,
    "published": "2023-05-21T17:59:39Z",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "We present DreamWaltz, a novel framework for generating and animating complex 3D avatars given text guidance and parametric human body prior. While recent methods have shown encouraging results for text-to-3D generation of common objects, creating high-quality and animatable 3D avatars remains challenging. To create high-quality 3D avatars, DreamWaltz proposes 3D-consistent occlusion-aware Score Distillation Sampling (SDS) to optimize implicit neural representations with canonical poses. It prov",
    "arxiv_url": "https://arxiv.org/abs/2305.12529v3",
    "pdf_url": "https://arxiv.org/pdf/2305.12529v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2305.12529",
    "arxiv_authors": [
      "Yukun Huang",
      "Jianan Wang",
      "Ailing Zeng",
      "He Cao",
      "Xianbiao Qi",
      "Yukai Shi",
      "Zheng-Jun Zha",
      "Lei Zhang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=DreamWaltz%3A+Make+a+Scene+with+Complex+3D+Animatable+Avatars+Yukun+Huang+Jianan+Wang+Ailing+Zeng+He+Cao+Xianbiao+Qi",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "Y Huang",
        "id": "lHb5gzoAAAAJ"
      },
      {
        "name": "J Wang",
        "id": "mt5mvZ8AAAAJ"
      },
      {
        "name": "A Zeng",
        "id": "Tn7fzS8AAAAJ"
      },
      {
        "name": "H Cao",
        "id": "tLZ2V2kAAAAJ"
      },
      {
        "name": "X Qi",
        "id": null
      },
      {
        "name": "Y Shi",
        "id": "oQXfkSQAAAAJ"
      },
      {
        "name": "ZJ Zha",
        "id": "gDnBC1gAAAAJ"
      },
      {
        "name": "L ZhangAdvances in Neural Information Processing Systems",
        "id": null
      }
    ],
    "citation_count": 94
  },
  {
    "arxiv_id": "2302.02181",
    "title": "Model Stitching and Visualization How GAN Generators can Invert Networks in Real-Time",
    "year": 2023,
    "published": "2023-02-04T15:28:49Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "eess.IV"
    ],
    "abstract": "In this work, we propose a fast and accurate method to reconstruct activations of classification and semantic segmentation networks by stitching them with a GAN generator utilizing a 1x1 convolution. We test our approach on images of animals from the AFHQ wild dataset, ImageNet1K, and real-world digital pathology scans of stained tissue samples. Our results show comparable performance to established gradient descent methods but with a processing time that is two orders of magnitude faster, makin",
    "arxiv_url": "https://arxiv.org/abs/2302.02181v2",
    "pdf_url": "https://arxiv.org/pdf/2302.02181v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2302.02181",
    "arxiv_authors": [
      "Rudolf Herdt",
      "Maximilian Schmidt",
      "Daniel Otero Baguer",
      "Jean Le'Clerc Arrastia",
      "Peter Maass"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Model+Stitching+and+Visualization+How+GAN+Generators+can+Invert+Networks+in+Real-Time+Rudolf+Herdt+Maximilian+Schmidt+Daniel+Otero+Baguer+Jean+Le%27Clerc+Arrastia+Peter+Maass",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "R Herdt",
        "id": "C4PzA5wAAAAJ"
      },
      {
        "name": "M Schmidt",
        "id": "AjvROjsAAAAJ"
      },
      {
        "name": "DO Baguer",
        "id": "PAMCTYgAAAAJ"
      },
      {
        "name": "JLC Arrastia",
        "id": "Iudsh30AAAAJ"
      }
    ],
    "citation_count": 3
  },
  {
    "arxiv_id": "2406.07754",
    "title": "HOI-Swap: Swapping Objects in Videos with Hand-Object Interaction Awareness",
    "year": 2024,
    "published": "2024-06-11T22:31:29Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "We study the problem of precisely swapping objects in videos, with a focus on those interacted with by hands, given one user-provided reference object image. Despite the great advancements that diffusion models have made in video editing recently, these models often fall short in handling the intricacies of hand-object interactions (HOI), failing to produce realistic edits -- especially when object swapping results in object shape or functionality changes. To bridge this gap, we present HOI-Swap",
    "arxiv_url": "https://arxiv.org/abs/2406.07754v2",
    "pdf_url": "https://arxiv.org/pdf/2406.07754v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2406.07754",
    "arxiv_authors": [
      "Zihui Xue",
      "Mi Luo",
      "Changan Chen",
      "Kristen Grauman"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=HOI-Swap%3A+Swapping+Objects+in+Videos+with+Hand-Object+Interaction+Awareness+Zihui+Xue+Mi+Luo+Changan+Chen+Kristen+Grauman",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "ZS Xue",
        "id": "JCV9BQ0AAAAJ"
      },
      {
        "name": "R Luo",
        "id": "eL-xIlAAAAAJ"
      },
      {
        "name": "C Chen",
        "id": "9Uxf0ikAAAAJ"
      },
      {
        "name": "K GraumanAdvances in Neural Information Processing Systems",
        "id": null
      }
    ],
    "citation_count": 17
  },
  {
    "arxiv_id": "2503.23468",
    "title": "Internal Organ Localization Using Depth Images",
    "year": 2025,
    "published": "2025-03-30T14:55:23Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Automated patient positioning is a crucial step in streamlining MRI workflows and enhancing patient throughput. RGB-D camera-based systems offer a promising approach to automate this process by leveraging depth information to estimate internal organ positions. This paper investigates the feasibility of a learning-based framework to infer approximate internal organ positions from the body surface. Our approach utilizes a large-scale dataset of MRI scans to train a deep learning model capable of a",
    "arxiv_url": "https://arxiv.org/abs/2503.23468v1",
    "pdf_url": "https://arxiv.org/pdf/2503.23468v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2503.23468",
    "arxiv_authors": [
      "Eytan Kats",
      "Kai Gei√üler",
      "Jochen G. Hirsch",
      "Stefan Heldman",
      "Mattias P. Heinrich"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Internal+Organ+Localization+Using+Depth+Images+Eytan+Kats+Kai+Gei%C3%9Fler+Jochen+G.+Hirsch+Stefan+Heldman+Mattias+P.+Heinrich",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "E Kats",
        "id": "PcuZ9sIAAAAJ"
      },
      {
        "name": "K Gei√üler",
        "id": "6rOe2p0AAAAJ"
      },
      {
        "name": "JG Hirsch",
        "id": null
      },
      {
        "name": "S Heldman",
        "id": null
      },
      {
        "name": "MP Heinrich - BVM",
        "id": null
      }
    ],
    "citation_count": null
  },
  {
    "arxiv_id": "2505.24558",
    "title": "Optimal Weighted Convolution for Classification and Denosing",
    "year": 2025,
    "published": "2025-05-30T13:10:46Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "We introduce a novel weighted convolution operator that enhances traditional convolutional neural networks (CNNs) by integrating a spatial density function into the convolution operator. This extension enables the network to differentially weight neighbouring pixels based on their relative position to the reference pixel, improving spatial characterisation and feature extraction. The proposed operator maintains the same number of trainable parameters and is fully compatible with existing CNN arc",
    "arxiv_url": "https://arxiv.org/abs/2505.24558v1",
    "pdf_url": "https://arxiv.org/pdf/2505.24558v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2505.24558",
    "arxiv_authors": [
      "Simone Cammarasana",
      "Giuseppe Patan√®"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Optimal+Weighted+Convolution+for+Classification+and+Denosing+Simone+Cammarasana+Giuseppe+Patan%C3%A8",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "S Cammarasana",
        "id": "q6lcBoAAAAAJ"
      },
      {
        "name": "G Patan√® -",
        "id": null
      }
    ],
    "citation_count": null
  },
  {
    "arxiv_id": "2404.01580",
    "title": "Learning Temporal Cues by Predicting Objects Move for Multi-camera 3D Object Detection",
    "year": 2024,
    "published": "2024-04-02T02:20:47Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "In autonomous driving and robotics, there is a growing interest in utilizing short-term historical data to enhance multi-camera 3D object detection, leveraging the continuous and correlated nature of input video streams. Recent work has focused on spatially aligning BEV-based features over timesteps. However, this is often limited as its gain does not scale well with long-term past observations. To address this, we advocate for supervising a model to predict objects' poses given past observation",
    "arxiv_url": "https://arxiv.org/abs/2404.01580v1",
    "pdf_url": "https://arxiv.org/pdf/2404.01580v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2404.01580",
    "arxiv_authors": [
      "Seokha Moon",
      "Hongbeen Park",
      "Jungphil Kwon",
      "Jaekoo Lee",
      "Jinkyu Kim"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Learning+Temporal+Cues+by+Predicting+Objects+Move+for+Multi-camera+3D+Object+Detection+Seokha+Moon+Hongbeen+Park+Jungphil+Kwon+Jaekoo+Lee+Jinkyu+Kim",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "S Moon",
        "id": "HhvS9d4AAAAJ"
      },
      {
        "name": "H Park",
        "id": "yFw-VdcAAAAJ"
      },
      {
        "name": "J Lee",
        "id": "89CnwkcAAAAJ"
      },
      {
        "name": "J Kim -",
        "id": null
      }
    ],
    "citation_count": null
  },
  {
    "arxiv_id": "2304.09748",
    "title": "Reference-based Image Composition with Sketch via Structure-aware Diffusion Model",
    "year": 2023,
    "published": "2023-03-31T06:12:58Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Recent remarkable improvements in large-scale text-to-image generative models have shown promising results in generating high-fidelity images. To further enhance editability and enable fine-grained generation, we introduce a multi-input-conditioned image composition model that incorporates a sketch as a novel modal, alongside a reference image. Thanks to the edge-level controllability using sketches, our method enables a user to edit or complete an image sub-part with a desired structure (i.e., ",
    "arxiv_url": "https://arxiv.org/abs/2304.09748v1",
    "pdf_url": "https://arxiv.org/pdf/2304.09748v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2304.09748",
    "arxiv_authors": [
      "Kangyeol Kim",
      "Sunghyun Park",
      "Junsoo Lee",
      "Jaegul Choo"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Reference-based+Image+Composition+with+Sketch+via+Structure-aware+Diffusion+Model+Kangyeol+Kim+Sunghyun+Park+Junsoo+Lee+Jaegul+Choo",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "K Kim",
        "id": "1r7xoDEAAAAJ"
      },
      {
        "name": "S Park",
        "id": "dYJaOh8AAAAJ"
      },
      {
        "name": "J Lee",
        "id": "Ww1oh28AAAAJ"
      },
      {
        "name": "J Choo -",
        "id": null
      }
    ],
    "citation_count": 22
  },
  {
    "arxiv_id": "2307.12239",
    "title": "Learning Dynamic Query Combinations for Transformer-based Object Detection and Segmentation",
    "year": 2023,
    "published": "2023-07-23T06:26:27Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Transformer-based detection and segmentation methods use a list of learned detection queries to retrieve information from the transformer network and learn to predict the location and category of one specific object from each query. We empirically find that random convex combinations of the learned queries are still good for the corresponding models. We then propose to learn a convex combination with dynamic coefficients based on the high-level semantics of the image. The generated dynamic queri",
    "arxiv_url": "https://arxiv.org/abs/2307.12239v2",
    "pdf_url": "https://arxiv.org/pdf/2307.12239v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2307.12239",
    "arxiv_authors": [
      "Yiming Cui",
      "Linjie Yang",
      "Haichao Yu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Learning+Dynamic+Query+Combinations+for+Transformer-based+Object+Detection+and+Segmentation+Yiming+Cui+Linjie+Yang+Haichao+Yu",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "Y Cui",
        "id": null
      },
      {
        "name": "L Yang",
        "id": "XptEO8oAAAAJ"
      },
      {
        "name": "H Yu - International",
        "id": null
      }
    ],
    "citation_count": 17
  },
  {
    "arxiv_id": "2411.01748",
    "title": "Rotation Perturbation Robustness in Point Cloud Analysis: A Perspective of Manifold Distillation",
    "year": 2024,
    "published": "2024-11-04T02:13:41Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Point cloud is often regarded as a discrete sampling of Riemannian manifold and plays a pivotal role in the 3D image interpretation. Particularly, rotation perturbation, an unexpected small change in rotation caused by various factors (like equipment offset, system instability, measurement errors and so on), can easily lead to the inferior results in point cloud learning tasks. However, classical point cloud learning methods are sensitive to rotation perturbation, and the existing networks with ",
    "arxiv_url": "https://arxiv.org/abs/2411.01748v1",
    "pdf_url": "https://arxiv.org/pdf/2411.01748v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2411.01748",
    "arxiv_authors": [
      "Xinyu Xu",
      "Huazhen Liu",
      "Feiming Wei",
      "Huilin Xiong",
      "Wenxian Yu",
      "Tao Zhang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Rotation+Perturbation+Robustness+in+Point+Cloud+Analysis%3A+A+Perspective+of+Manifold+Distillation+Xinyu+Xu+Huazhen+Liu+Feiming+Wei+Huilin+Xiong+Wenxian+Yu",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "X Xu",
        "id": null
      },
      {
        "name": "H Liu",
        "id": null
      },
      {
        "name": "F Wei",
        "id": null
      },
      {
        "name": "H Xiong",
        "id": null
      },
      {
        "name": "W Yu",
        "id": null
      },
      {
        "name": "T Zhang -",
        "id": null
      }
    ],
    "citation_count": null
  },
  {
    "arxiv_id": "2503.09122",
    "title": "Training Data Provenance Verification: Did Your Model Use Synthetic Data from My Generative Model for Training?",
    "year": 2025,
    "published": "2025-03-12T07:15:16Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "High-quality open-source text-to-image models have lowered the threshold for obtaining photorealistic images significantly, but also face potential risks of misuse. Specifically, suspects may use synthetic data generated by these generative models to train models for specific tasks without permission, when lacking real data resources especially. Protecting these generative models is crucial for the well-being of their owners. In this work, we propose the first method to this important yet unreso",
    "arxiv_url": "https://arxiv.org/abs/2503.09122v1",
    "pdf_url": "https://arxiv.org/pdf/2503.09122v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2503.09122",
    "arxiv_authors": [
      "Yuechen Xie",
      "Jie Song",
      "Huiqiong Wang",
      "Mingli Song"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Training+Data+Provenance+Verification%3A+Did+Your+Model+Use+Synthetic+Data+from+My+Generative+Model+for+Training%3F+Yuechen+Xie+Jie+Song+Huiqiong+Wang+Mingli+Song",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "Y Xie",
        "id": "qmupQ84AAAAJ"
      },
      {
        "name": "J Song",
        "id": "4OjO-WYAAAAJ"
      },
      {
        "name": "H Wang",
        "id": null
      },
      {
        "name": "M Song -",
        "id": null
      }
    ],
    "citation_count": 2
  },
  {
    "arxiv_id": "2408.09663",
    "title": "3D-Consistent Human Avatars with Sparse Inputs via Gaussian Splatting and Contrastive Learning",
    "year": 2024,
    "published": "2024-08-19T02:46:23Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Existing approaches for human avatar generation--both NeRF-based and 3D Gaussian Splatting (3DGS) based--struggle with maintaining 3D consistency and exhibit degraded detail reconstruction, particularly when training with sparse inputs. To address this challenge, we propose CHASE, a novel framework that achieves dense-input-level performance using only sparse inputs through two key innovations: cross-pose intrinsic 3D consistency supervision and 3D geometry contrastive learning. Building upon pr",
    "arxiv_url": "https://arxiv.org/abs/2408.09663v3",
    "pdf_url": "https://arxiv.org/pdf/2408.09663v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2408.09663",
    "arxiv_authors": [
      "Haoyu Zhao",
      "Hao Wang",
      "Chen Yang",
      "Wei Shen"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=3D-Consistent+Human+Avatars+with+Sparse+Inputs+via+Gaussian+Splatting+and+Contrastive+Learning+Haoyu+Zhao+Hao+Wang+Chen+Yang+Wei+Shen",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "H Zhao",
        "id": null
      },
      {
        "name": "H Wang",
        "id": "StdXTR8AAAAJ"
      },
      {
        "name": "C Yang",
        "id": "LxjxwpgAAAAJ"
      },
      {
        "name": "W Shen -",
        "id": null
      }
    ],
    "citation_count": null
  },
  {
    "arxiv_id": "2312.16902",
    "title": "Joint Learning for Scattered Point Cloud Understanding with Hierarchical Self-Distillation",
    "year": 2023,
    "published": "2023-12-28T08:51:04Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Numerous point-cloud understanding techniques focus on whole entities and have succeeded in obtaining satisfactory results and limited sparsity tolerance. However, these methods are generally sensitive to incomplete point clouds that are scanned with flaws or large gaps. In this paper, we propose an end-to-end architecture that compensates for and identifies partial point clouds on the fly. First, we propose a cascaded solution that integrates both the upstream masked autoencoder (MAE) and downs",
    "arxiv_url": "https://arxiv.org/abs/2312.16902v3",
    "pdf_url": "https://arxiv.org/pdf/2312.16902v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2312.16902",
    "arxiv_authors": [
      "Kaiyue Zhou",
      "Ming Dong",
      "Peiyuan Zhi",
      "Shengjin Wang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Joint+Learning+for+Scattered+Point+Cloud+Understanding+with+Hierarchical+Self-Distillation+Kaiyue+Zhou+Ming+Dong+Peiyuan+Zhi+Shengjin+Wang",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "K Zhou",
        "id": "tS287loAAAAJ"
      },
      {
        "name": "M Dong",
        "id": "yCUFD2wAAAAJ"
      },
      {
        "name": "P Zhi",
        "id": null
      },
      {
        "name": "S Wang - IEEE Sensors",
        "id": null
      }
    ],
    "citation_count": 1
  },
  {
    "arxiv_id": "2311.12855",
    "title": "Contextualised Out-of-Distribution Detection using Pattern Identication",
    "year": 2023,
    "published": "2023-10-24T07:55:09Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "In this work, we propose CODE, an extension of existing work from the field of explainable AI that identifies class-specific recurring patterns to build a robust Out-of-Distribution (OoD) detection method for visual classifiers. CODE does not require any classifier retraining and is OoD-agnostic, i.e., tuned directly to the training dataset. Crucially, pattern identification allows us to provide images from the In-Distribution (ID) dataset as reference data to provide additional context to the c",
    "arxiv_url": "https://arxiv.org/abs/2311.12855v1",
    "pdf_url": "https://arxiv.org/pdf/2311.12855v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2311.12855",
    "arxiv_authors": [
      "Romain Xu-Darme",
      "Julien Girard-Satabin",
      "Darryl Hond",
      "Gabriele Incorvaia",
      "Zakaria Chihani"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Contextualised+Out-of-Distribution+Detection+using+Pattern+Identication+Romain+Xu-Darme+Julien+Girard-Satabin+Darryl+Hond+Gabriele+Incorvaia+Zakaria+Chihani",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "R Xu-Darme",
        "id": "QB4YkI0AAAAJ"
      },
      {
        "name": "J Girard-Satabin",
        "id": "erWN5TwAAAAJ"
      },
      {
        "name": "D Hond",
        "id": null
      },
      {
        "name": "G Incorvaia",
        "id": null
      },
      {
        "name": "Z Chihani",
        "id": "mgzCh30AAAAJ"
      }
    ],
    "citation_count": null
  },
  {
    "arxiv_id": "2405.05502",
    "title": "Towards Accurate and Robust Architectures via Neural Architecture Search",
    "year": 2024,
    "published": "2024-05-09T02:16:50Z",
    "categories": [
      "cs.CV",
      "cs.CR",
      "cs.LG"
    ],
    "abstract": "To defend deep neural networks from adversarial attacks, adversarial training has been drawing increasing attention for its effectiveness. However, the accuracy and robustness resulting from the adversarial training are limited by the architecture, because adversarial training improves accuracy and robustness by adjusting the weight connection affiliated to the architecture. In this work, we propose ARNAS to search for accurate and robust architectures for adversarial training. First we design a",
    "arxiv_url": "https://arxiv.org/abs/2405.05502v1",
    "pdf_url": "https://arxiv.org/pdf/2405.05502v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2405.05502",
    "arxiv_authors": [
      "Yuwei Ou",
      "Yuqi Feng",
      "Yanan Sun"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Towards+Accurate+and+Robust+Architectures+via+Neural+Architecture+Search+Yuwei+Ou+Yuqi+Feng+Yanan+Sun",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "Y Ou",
        "id": null
      },
      {
        "name": "Y Feng",
        "id": "sN0x6dgAAAAJ"
      },
      {
        "name": "Y Sun -",
        "id": null
      }
    ],
    "citation_count": 12
  },
  {
    "arxiv_id": "2401.10215",
    "title": "GPAvatar: Generalizable and Precise Head Avatar from Image(s)",
    "year": 2024,
    "published": "2024-01-18T18:56:34Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Head avatar reconstruction, crucial for applications in virtual reality, online meetings, gaming, and film industries, has garnered substantial attention within the computer vision community. The fundamental objective of this field is to faithfully recreate the head avatar and precisely control expressions and postures. Existing methods, categorized into 2D-based warping, mesh-based, and neural rendering approaches, present challenges in maintaining multi-view consistency, incorporating non-faci",
    "arxiv_url": "https://arxiv.org/abs/2401.10215v1",
    "pdf_url": "https://arxiv.org/pdf/2401.10215v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2401.10215",
    "arxiv_authors": [
      "Xuangeng Chu",
      "Yu Li",
      "Ailing Zeng",
      "Tianyu Yang",
      "Lijian Lin",
      "Yunfei Liu",
      "Tatsuya Harada"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=GPAvatar%3A+Generalizable+and+Precise+Head+Avatar+from+Image%28s%29+Xuangeng+Chu+Yu+Li+Ailing+Zeng+Tianyu+Yang+Lijian+Lin",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "X Chu",
        "id": "yr4kSUsAAAAJ"
      },
      {
        "name": "Y Li",
        "id": "j9lwU7kAAAAJ"
      },
      {
        "name": "A Zeng",
        "id": "Tn7fzS8AAAAJ"
      },
      {
        "name": "T Yang",
        "id": "BXsWsf8AAAAJ"
      },
      {
        "name": "L Lin",
        "id": "Xf5_TfcAAAAJ"
      },
      {
        "name": "Y Liu",
        "id": "B1Z1vTMAAAAJ"
      },
      {
        "name": "T Harada",
        "id": "k8rlJ8AAAAAJ"
      }
    ],
    "citation_count": 42
  },
  {
    "arxiv_id": "2405.16008",
    "title": "Intensity and Texture Correction of Omnidirectional Image Using Camera Images for Indirect Augmented Reality",
    "year": 2024,
    "published": "2024-05-25T02:14:07Z",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "abstract": "Augmented reality (AR) using camera images in mobile devices is becoming popular for tourism promotion. However, obstructions such as tourists appearing in the camera images may cause the camera pose estimation error, resulting in CG misalignment and reduced visibility of the contents. To avoid this problem, Indirect AR (IAR), which does not use real-time camera images, has been proposed. In this method, an omnidirectional image is captured and virtual objects are synthesized on the image in adv",
    "arxiv_url": "https://arxiv.org/abs/2405.16008v1",
    "pdf_url": "https://arxiv.org/pdf/2405.16008v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2405.16008",
    "arxiv_authors": [
      "Hakim Ikebayashi",
      "Norihiko Kawai"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Intensity+and+Texture+Correction+of+Omnidirectional+Image+Using+Camera+Images+for+Indirect+Augmented+Reality+Hakim+Ikebayashi+Norihiko+Kawai",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "H Ikebayashi",
        "id": null
      },
      {
        "name": "N Kawai -",
        "id": null
      }
    ],
    "citation_count": null
  },
  {
    "arxiv_id": "2409.08772",
    "title": "The Practice of Averaging Rate-Distortion Curves over Testsets to Compare Learned Video Codecs Can Cause Misleading Conclusions",
    "year": 2024,
    "published": "2024-09-13T12:30:15Z",
    "categories": [
      "cs.MM",
      "cs.CV",
      "eess.IV"
    ],
    "abstract": "This paper aims to demonstrate how the prevalent practice in the learned video compression community of averaging rate-distortion (RD) curves across a test video set can lead to misleading conclusions in evaluating codec performance. Through analytical analysis of a simple case and experimental results with two recent learned video codecs, we show how averaged RD curves can mislead comparative evaluation of different codecs, particularly when videos in a dataset have varying characteristics and ",
    "arxiv_url": "https://arxiv.org/abs/2409.08772v2",
    "pdf_url": "https://arxiv.org/pdf/2409.08772v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2409.08772",
    "arxiv_authors": [
      "M. Akin Yilmaz",
      "Onur Kele≈ü",
      "A. Murat Tekalp"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=The+Practice+of+Averaging+Rate-Distortion+Curves+over+Testsets+to+Compare+Learned+Video+Codecs+Can+Cause+Misleading+Conclusions+M.+Akin+Yilmaz+Onur+Kele%C5%9F+A.+Murat+Tekalp",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "MA Yilmaz",
        "id": "gRgNm7oAAAAJ"
      },
      {
        "name": "O Kele≈ü",
        "id": "epVJoggAAAAJ"
      },
      {
        "name": "AM Tekalp -",
        "id": null
      }
    ],
    "citation_count": null
  },
  {
    "arxiv_id": "2501.12115",
    "title": "Meta-Sparsity: Learning Optimal Sparse Structures in Multi-task Networks through Meta-learning",
    "year": 2025,
    "published": "2025-01-21T13:25:32Z",
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "abstract": "This paper presents meta-sparsity, a framework for learning model sparsity, basically learning the parameter that controls the degree of sparsity, that allows deep neural networks (DNNs) to inherently generate optimal sparse shared structures in multi-task learning (MTL) setting. This proposed approach enables the dynamic learning of sparsity patterns across a variety of tasks, unlike traditional sparsity methods that rely heavily on manual hyperparameter tuning. Inspired by Model Agnostic Meta-",
    "arxiv_url": "https://arxiv.org/abs/2501.12115v1",
    "pdf_url": "https://arxiv.org/pdf/2501.12115v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2501.12115",
    "arxiv_authors": [
      "Richa Upadhyay",
      "Ronald Phlypo",
      "Rajkumar Saini",
      "Marcus Liwicki"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Meta-Sparsity%3A+Learning+Optimal+Sparse+Structures+in+Multi-task+Networks+through+Meta-learning+Richa+Upadhyay+Ronald+Phlypo+Rajkumar+Saini+Marcus+Liwicki",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "R Upadhyay",
        "id": "Z1L6-o8AAAAJ"
      },
      {
        "name": "R Phlypo",
        "id": "hZIeqGMAAAAJ"
      },
      {
        "name": "R Saini",
        "id": "J191UeEAAAAJ"
      },
      {
        "name": "M Liwicki -",
        "id": null
      }
    ],
    "citation_count": 2
  },
  {
    "arxiv_id": "2402.06841",
    "title": "Point cloud-based registration and image fusion between cardiac SPECT MPI and CTA",
    "year": 2024,
    "published": "2024-02-10T00:00:40Z",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "abstract": "A method was proposed for the point cloud-based registration and image fusion between cardiac single photon emission computed tomography (SPECT) myocardial perfusion images (MPI) and cardiac computed tomography angiograms (CTA). Firstly, the left ventricle (LV) epicardial regions (LVERs) in SPECT and CTA images were segmented by using different U-Net neural networks trained to generate the point clouds of the LV epicardial contours (LVECs). Secondly, according to the characteristics of cardiac a",
    "arxiv_url": "https://arxiv.org/abs/2402.06841v1",
    "pdf_url": "https://arxiv.org/pdf/2402.06841v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2402.06841",
    "arxiv_authors": [
      "Shaojie Tang",
      "Penpen Miao",
      "Xingyu Gao",
      "Yu Zhong",
      "Dantong Zhu",
      "Haixing Wen",
      "Zhihui Xu",
      "Qiuyue Wei",
      "Hongping Yao",
      "Xin Huang",
      "Rui Gao",
      "Chen Zhao",
      "Weihua Zhou"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Point+cloud-based+registration+and+image+fusion+between+cardiac+SPECT+MPI+and+CTA+Shaojie+Tang+Penpen+Miao+Xingyu+Gao+Yu+Zhong+Dantong+Zhu",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "S Tang",
        "id": null
      },
      {
        "name": "P Miao",
        "id": null
      },
      {
        "name": "X Gao",
        "id": null
      },
      {
        "name": "Y Zhong",
        "id": null
      },
      {
        "name": "D Zhu",
        "id": null
      },
      {
        "name": "H Wen",
        "id": null
      },
      {
        "name": "Z Xu",
        "id": null
      },
      {
        "name": "Q Wei",
        "id": null
      },
      {
        "name": "H Yao",
        "id": null
      },
      {
        "name": "X Huang",
        "id": null
      },
      {
        "name": "R Gao",
        "id": null
      }
    ],
    "citation_count": 1
  },
  {
    "arxiv_id": "2408.11227",
    "title": "OCTCube-M: A 3D multimodal optical coherence tomography foundation model for retinal and systemic diseases with cross-cohort and cross-device validation",
    "year": 2024,
    "published": "2024-08-20T22:55:19Z",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "abstract": "We present OCTCube-M, a 3D OCT-based multi-modal foundation model for jointly analyzing OCT and en face images. OCTCube-M first developed OCTCube, a 3D foundation model pre-trained on 26,685 3D OCT volumes encompassing 1.62 million 2D OCT images. It then exploits a novel multi-modal contrastive learning framework COEP to integrate other retinal imaging modalities, such as fundus autofluorescence and infrared retinal imaging, into OCTCube, efficiently extending it into multi-modal foundation mode",
    "arxiv_url": "https://arxiv.org/abs/2408.11227v2",
    "pdf_url": "https://arxiv.org/pdf/2408.11227v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2408.11227",
    "arxiv_authors": [
      "Zixuan Liu",
      "Hanwen Xu",
      "Addie Woicik",
      "Linda G. Shapiro",
      "Marian Blazes",
      "Yue Wu",
      "Verena Steffen",
      "Catherine Cukras",
      "Cecilia S. Lee",
      "Miao Zhang",
      "Aaron Y. Lee",
      "Sheng Wang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=OCTCube-M%3A+A+3D+multimodal+optical+coherence+tomography+foundation+model+for+retinal+and+systemic+diseases+with+cross-cohort+and+cross-device+validation+Zixuan+Liu+Hanwen+Xu+Addie+Woicik+Linda+G.+Shapiro+Marian+Blazes",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "Z Liu",
        "id": null
      },
      {
        "name": "H Xu",
        "id": "HwO7L5sAAAAJ"
      },
      {
        "name": "A Woicik",
        "id": null
      },
      {
        "name": "LG Shapiro",
        "id": "nBwaXUsAAAAJ"
      },
      {
        "name": "M Blazes",
        "id": "x5RVnAkAAAAJ"
      },
      {
        "name": "Y Wu",
        "id": "P85B8xsAAAAJ"
      },
      {
        "name": "V Steffen",
        "id": null
      },
      {
        "name": "C Cukras",
        "id": "p5iMoq4AAAAJ"
      },
      {
        "name": "CS Lee",
        "id": "Ds-OxHwAAAAJ"
      },
      {
        "name": "M Zhang",
        "id": "dJHW9V8AAAAJ"
      }
    ],
    "citation_count": 2
  },
  {
    "arxiv_id": "2405.12646",
    "title": "PoseGravity: Pose Estimation from Points and Lines with Axis Prior",
    "year": 2024,
    "published": "2024-05-21T09:55:56Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "This paper presents a new algorithm to estimate absolute camera pose given an axis of the camera's rotation matrix. Current algorithms solve the problem via algebraic solutions on limited input domains. This paper shows that the problem can be solved efficiently by finding the intersection points of a hyperbola and the unit circle. The solution can flexibly accommodate combinations of point and line features in minimal and overconstrained configurations. In addition, the two special cases of pla",
    "arxiv_url": "https://arxiv.org/abs/2405.12646v2",
    "pdf_url": "https://arxiv.org/pdf/2405.12646v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2405.12646",
    "arxiv_authors": [
      "Akshay Chandrasekhar"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=PoseGravity%3A+Pose+Estimation+from+Points+and+Lines+with+Axis+Prior+Akshay+Chandrasekhar",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "A Chandrasekhar -",
        "id": null
      }
    ],
    "citation_count": 1
  },
  {
    "arxiv_id": "2505.04088",
    "title": "SMMT: Siamese Motion Mamba with Self-attention for Thermal Infrared Target Tracking",
    "year": 2025,
    "published": "2025-05-07T03:02:04Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Thermal infrared (TIR) object tracking often suffers from challenges such as target occlusion, motion blur, and background clutter, which significantly degrade the performance of trackers. To address these issues, this paper pro-poses a novel Siamese Motion Mamba Tracker (SMMT), which integrates a bidirectional state-space model and a self-attention mechanism. Specifically, we introduce the Motion Mamba module into the Siamese architecture to ex-tract motion features and recover overlooked edge ",
    "arxiv_url": "https://arxiv.org/abs/2505.04088v3",
    "pdf_url": "https://arxiv.org/pdf/2505.04088v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2505.04088",
    "arxiv_authors": [
      "Shang Zhang",
      "Huanbin Zhang",
      "Dali Feng",
      "Yujie Cui",
      "Ruoyan Xiong",
      "Cen He"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=SMMT%3A+Siamese+Motion+Mamba+with+Self-attention+for+Thermal+Infrared+Target+Tracking+Shang+Zhang+Huanbin+Zhang+Dali+Feng+Yujie+Cui+Ruoyan+Xiong",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "S Zhang",
        "id": null
      },
      {
        "name": "H Zhang",
        "id": null
      },
      {
        "name": "Y Cui",
        "id": null
      },
      {
        "name": "R Xiong",
        "id": "x8NRa8kAAAAJ"
      },
      {
        "name": "C He",
        "id": null
      },
      {
        "name": "D FengInternational",
        "id": null
      }
    ],
    "citation_count": 1
  },
  {
    "arxiv_id": "2410.08184",
    "title": "Scaling Laws For Diffusion Transformers",
    "year": 2024,
    "published": "2024-10-10T17:56:03Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Diffusion transformers (DiT) have already achieved appealing synthesis and scaling properties in content recreation, e.g., image and video generation. However, scaling laws of DiT are less explored, which usually offer precise predictions regarding optimal model size and data requirements given a specific compute budget. Therefore, experiments across a broad range of compute budgets, from 1e17 to 6e18 FLOPs are conducted to confirm the existence of scaling laws in DiT for the first time. Concret",
    "arxiv_url": "https://arxiv.org/abs/2410.08184v1",
    "pdf_url": "https://arxiv.org/pdf/2410.08184v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2410.08184",
    "arxiv_authors": [
      "Zhengyang Liang",
      "Hao He",
      "Ceyuan Yang",
      "Bo Dai"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Scaling+Laws+For+Diffusion+Transformers+Zhengyang+Liang+Hao+He+Ceyuan+Yang+Bo+Dai",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "Z Liang",
        "id": null
      },
      {
        "name": "H He",
        "id": "kdbmt6QAAAAJ"
      },
      {
        "name": "C Yang",
        "id": "Rfj4jWoAAAAJ"
      },
      {
        "name": "B Dai -",
        "id": null
      }
    ],
    "citation_count": 20
  },
  {
    "arxiv_id": "2405.15758",
    "title": "InstructAvatar: Text-Guided Emotion and Motion Control for Avatar Generation",
    "year": 2024,
    "published": "2024-05-24T17:53:54Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Recent talking avatar generation models have made strides in achieving realistic and accurate lip synchronization with the audio, but often fall short in controlling and conveying detailed expressions and emotions of the avatar, making the generated video less vivid and controllable. In this paper, we propose a novel text-guided approach for generating emotionally expressive 2D avatars, offering fine-grained control, improved interactivity, and generalizability to the resulting video. Our framew",
    "arxiv_url": "https://arxiv.org/abs/2405.15758v1",
    "pdf_url": "https://arxiv.org/pdf/2405.15758v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2405.15758",
    "arxiv_authors": [
      "Yuchi Wang",
      "Junliang Guo",
      "Jianhong Bai",
      "Runyi Yu",
      "Tianyu He",
      "Xu Tan",
      "Xu Sun",
      "Jiang Bian"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=InstructAvatar%3A+Text-Guided+Emotion+and+Motion+Control+for+Avatar+Generation+Yuchi+Wang+Junliang+Guo+Jianhong+Bai+Runyi+Yu+Tianyu+He",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "Y Wang",
        "id": "RxuU_0YAAAAJ"
      },
      {
        "name": "J Guo",
        "id": "S88C9ewAAAAJ"
      },
      {
        "name": "J Bai",
        "id": "U926UgYAAAAJ"
      },
      {
        "name": "R Yu",
        "id": "jUSqsWkAAAAJ"
      },
      {
        "name": "T He",
        "id": "P08KU1YAAAAJ"
      },
      {
        "name": "X Tan",
        "id": "tob-U1oAAAAJ"
      },
      {
        "name": "X Sun",
        "id": "tpXiQkYAAAAJ"
      },
      {
        "name": "J Bian",
        "id": "pZBEnY8AAAAJ"
      }
    ],
    "citation_count": 23
  },
  {
    "arxiv_id": "2412.12788",
    "title": "RA-SGG: Retrieval-Augmented Scene Graph Generation Framework via Multi-Prototype Learning",
    "year": 2024,
    "published": "2024-12-17T10:47:13Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Scene Graph Generation (SGG) research has suffered from two fundamental challenges: the long-tailed predicate distribution and semantic ambiguity between predicates. These challenges lead to a bias towards head predicates in SGG models, favoring dominant general predicates while overlooking fine-grained predicates. In this paper, we address the challenges of SGG by framing it as multi-label classification problem with partial annotation, where relevant labels of fine-grained predicates are missi",
    "arxiv_url": "https://arxiv.org/abs/2412.12788v1",
    "pdf_url": "https://arxiv.org/pdf/2412.12788v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2412.12788",
    "arxiv_authors": [
      "Kanghoon Yoon",
      "Kibum Kim",
      "Jaehyung Jeon",
      "Yeonjun In",
      "Donghyun Kim",
      "Chanyoung Park"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=RA-SGG%3A+Retrieval-Augmented+Scene+Graph+Generation+Framework+via+Multi-Prototype+Learning+Kanghoon+Yoon+Kibum+Kim+Jaehyung+Jeon+Yeonjun+In+Donghyun+Kim",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "K Yoon",
        "id": null
      },
      {
        "name": "K Kim",
        "id": "PF6y7awAAAAJ"
      },
      {
        "name": "J Jeon",
        "id": "_uSWJ6gAAAAJ"
      },
      {
        "name": "Y In",
        "id": "bBJXfnUAAAAJ"
      },
      {
        "name": "D Kim",
        "id": "UsqNPH4AAAAJ"
      },
      {
        "name": "C Park",
        "id": "lWk2LtQAAAAJ"
      }
    ],
    "citation_count": 1
  },
  {
    "arxiv_id": "2501.04750",
    "title": "Efficient License Plate Recognition in Videos Using Visual Rhythm and Accumulative Line Analysis",
    "year": 2025,
    "published": "2025-01-08T16:17:05Z",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "Video-based Automatic License Plate Recognition (ALPR) involves extracting vehicle license plate text information from video captures. Traditional systems typically rely heavily on high-end computing resources and utilize multiple frames to recognize license plates, leading to increased computational overhead. In this paper, we propose two methods capable of efficiently extracting exactly one frame per vehicle and recognizing its license plate characters from this single image, thus significantl",
    "arxiv_url": "https://arxiv.org/abs/2501.04750v1",
    "pdf_url": "https://arxiv.org/pdf/2501.04750v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2501.04750",
    "arxiv_authors": [
      "Victor Nascimento Ribeiro",
      "Nina S. T. Hirata"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Efficient+License+Plate+Recognition+in+Videos+Using+Visual+Rhythm+and+Accumulative+Line+Analysis+Victor+Nascimento+Ribeiro+Nina+S.+T.+Hirata",
    "gs_search_success": false,
    "gs_authors": null,
    "citation_count": null,
    "error_type": "multiple_matches"
  },
  {
    "arxiv_id": "2304.13991",
    "title": "Vision Conformer: Incorporating Convolutions into Vision Transformer Layers",
    "year": 2023,
    "published": "2023-04-27T07:27:44Z",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "Transformers are popular neural network models that use layers of self-attention and fully-connected nodes with embedded tokens. Vision Transformers (ViT) adapt transformers for image recognition tasks. In order to do this, the images are split into patches and used as tokens. One issue with ViT is the lack of inductive bias toward image structures. Because ViT was adapted for image data from language modeling, the network does not explicitly handle issues such as local translations, pixel infor",
    "arxiv_url": "https://arxiv.org/abs/2304.13991v1",
    "pdf_url": "https://arxiv.org/pdf/2304.13991v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2304.13991",
    "arxiv_authors": [
      "Brian Kenji Iwana",
      "Akihiro Kusuda"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Vision+Conformer%3A+Incorporating+Convolutions+into+Vision+Transformer+Layers+Brian+Kenji+Iwana+Akihiro+Kusuda",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "BK Iwana",
        "id": "azIV5VkAAAAJ"
      },
      {
        "name": "A Kusuda - International",
        "id": null
      }
    ],
    "citation_count": 4
  },
  {
    "arxiv_id": "2503.07120",
    "title": "FEB-Cache: Frequency-Guided Exposure Bias Reduction for Enhancing Diffusion Transformer Caching",
    "year": 2025,
    "published": "2025-03-10T09:49:18Z",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "Diffusion Transformer (DiT) has exhibited impressive generation capabilities but faces great challenges due to its high computational complexity. To address this issue, various methods, notably feature caching, have been introduced. However, these approaches focus on aligning non-cache diffusion without analyzing why caching damage the generation processes. In this paper, we first confirm that the cache greatly amplifies the exposure bias, resulting in a decline in the generation quality. Howeve",
    "arxiv_url": "https://arxiv.org/abs/2503.07120v3",
    "pdf_url": "https://arxiv.org/pdf/2503.07120v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2503.07120",
    "arxiv_authors": [
      "Zhen Zou",
      "Feng Zhao"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=FEB-Cache%3A+Frequency-Guided+Exposure+Bias+Reduction+for+Enhancing+Diffusion+Transformer+Caching+Zhen+Zou+Feng+Zhao",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "Z Zou",
        "id": "ofRms6MAAAAJ"
      },
      {
        "name": "J Huang",
        "id": null
      },
      {
        "name": "H Yu",
        "id": null
      },
      {
        "name": "F Zhao - Available at SSRN 5584552 -",
        "id": null
      }
    ],
    "citation_count": 1
  },
  {
    "arxiv_id": "2412.19645",
    "title": "VideoMaker: Zero-shot Customized Video Generation with the Inherent Force of Video Diffusion Models",
    "year": 2024,
    "published": "2024-12-27T13:49:25Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Zero-shot customized video generation has gained significant attention due to its substantial application potential. Existing methods rely on additional models to extract and inject reference subject features, assuming that the Video Diffusion Model (VDM) alone is insufficient for zero-shot customized video generation. However, these methods often struggle to maintain consistent subject appearance due to suboptimal feature extraction and injection techniques. In this paper, we reveal that VDM in",
    "arxiv_url": "https://arxiv.org/abs/2412.19645v2",
    "pdf_url": "https://arxiv.org/pdf/2412.19645v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2412.19645",
    "arxiv_authors": [
      "Tao Wu",
      "Yong Zhang",
      "Xiaodong Cun",
      "Zhongang Qi",
      "Junfu Pu",
      "Huanzhang Dou",
      "Guangcong Zheng",
      "Ying Shan",
      "Xi Li"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=VideoMaker%3A+Zero-shot+Customized+Video+Generation+with+the+Inherent+Force+of+Video+Diffusion+Models+Tao+Wu+Yong+Zhang+Xiaodong+Cun+Zhongang+Qi+Junfu+Pu",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "T Wu",
        "id": "F4lns_oAAAAJ"
      },
      {
        "name": "Y Zhang",
        "id": null
      },
      {
        "name": "X Cun",
        "id": "p42qwXcAAAAJ"
      },
      {
        "name": "Z Qi",
        "id": "zJvrrusAAAAJ"
      },
      {
        "name": "J Pu",
        "id": "G31SX1IAAAAJ"
      },
      {
        "name": "H Dou",
        "id": "N0kaKcwAAAAJ"
      },
      {
        "name": "G Zheng",
        "id": "i1PqK7kAAAAJ"
      },
      {
        "name": "Y Shan",
        "id": "4oXBp9UAAAAJ"
      },
      {
        "name": "X Li",
        "id": "TYNPJQMAAAAJ"
      }
    ],
    "citation_count": 12
  },
  {
    "arxiv_id": "2401.14626",
    "title": "Towards Lifelong Scene Graph Generation with Knowledge-ware In-context Prompt Learning",
    "year": 2024,
    "published": "2024-01-26T03:43:22Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Scene graph generation (SGG) endeavors to predict visual relationships between pairs of objects within an image. Prevailing SGG methods traditionally assume a one-off learning process for SGG. This conventional paradigm may necessitate repetitive training on all previously observed samples whenever new relationships emerge, mitigating the risk of forgetting previously acquired knowledge. This work seeks to address this pitfall inherent in a suite of prior relationship predictions. Motivated by t",
    "arxiv_url": "https://arxiv.org/abs/2401.14626v1",
    "pdf_url": "https://arxiv.org/pdf/2401.14626v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2401.14626",
    "arxiv_authors": [
      "Tao He",
      "Tongtong Wu",
      "Dongyang Zhang",
      "Guiduo Duan",
      "Ke Qin",
      "Yuan-Fang Li"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Towards+Lifelong+Scene+Graph+Generation+with+Knowledge-ware+In-context+Prompt+Learning+Tao+He+Tongtong+Wu+Dongyang+Zhang+Guiduo+Duan+Ke+Qin",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "T He",
        "id": "nhbZOLcAAAAJ"
      },
      {
        "name": "T Wu",
        "id": "u1Qp8lUAAAAJ"
      },
      {
        "name": "D Zhang",
        "id": "SsjcBbwAAAAJ"
      },
      {
        "name": "G Duan",
        "id": null
      },
      {
        "name": "K Qin",
        "id": "YevGUDgAAAAJ"
      },
      {
        "name": "YF Li -",
        "id": null
      }
    ],
    "citation_count": 3
  },
  {
    "arxiv_id": "2405.09883",
    "title": "RoScenes: A Large-scale Multi-view 3D Dataset for Roadside Perception",
    "year": 2024,
    "published": "2024-05-16T08:06:52Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "We introduce RoScenes, the largest multi-view roadside perception dataset, which aims to shed light on the development of vision-centric Bird's Eye View (BEV) approaches for more challenging traffic scenes. The highlights of RoScenes include significantly large perception area, full scene coverage and crowded traffic. More specifically, our dataset achieves surprising 21.13M 3D annotations within 64,000 $m^2$. To relieve the expensive costs of roadside 3D labeling, we present a novel BEV-to-3D j",
    "arxiv_url": "https://arxiv.org/abs/2405.09883v4",
    "pdf_url": "https://arxiv.org/pdf/2405.09883v4",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2405.09883",
    "arxiv_authors": [
      "Xiaosu Zhu",
      "Hualian Sheng",
      "Sijia Cai",
      "Bing Deng",
      "Shaopeng Yang",
      "Qiao Liang",
      "Ken Chen",
      "Lianli Gao",
      "Jingkuan Song",
      "Jieping Ye"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=RoScenes%3A+A+Large-scale+Multi-view+3D+Dataset+for+Roadside+Perception+Xiaosu+Zhu+Hualian+Sheng+Sijia+Cai+Bing+Deng+Shaopeng+Yang",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "X Zhu",
        "id": "2DihiQ0AAAAJ"
      },
      {
        "name": "H Sheng",
        "id": "73JaDUQAAAAJ"
      },
      {
        "name": "S Cai",
        "id": "LMVeRVAAAAAJ"
      },
      {
        "name": "B Deng",
        "id": "VQp_ye4AAAAJ"
      },
      {
        "name": "S Yang",
        "id": null
      },
      {
        "name": "Q Liang",
        "id": "-Hv_dPkAAAAJ"
      },
      {
        "name": "K Chen",
        "id": "fYbset4AAAAJ"
      },
      {
        "name": "L Gao",
        "id": null
      },
      {
        "name": "J Song",
        "id": "F5Zy9V4AAAAJ"
      },
      {
        "name": "J YeEuropean",
        "id": null
      }
    ],
    "citation_count": 9
  },
  {
    "arxiv_id": "2411.10564",
    "title": "Vision Eagle Attention: a new lens for advancing image classification",
    "year": 2024,
    "published": "2024-11-15T20:21:59Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "In computer vision tasks, the ability to focus on relevant regions within an image is crucial for improving model performance, particularly when key features are small, subtle, or spatially dispersed. Convolutional neural networks (CNNs) typically treat all regions of an image equally, which can lead to inefficient feature extraction. To address this challenge, I have introduced Vision Eagle Attention, a novel attention mechanism that enhances visual feature extraction using convolutional spatia",
    "arxiv_url": "https://arxiv.org/abs/2411.10564v2",
    "pdf_url": "https://arxiv.org/pdf/2411.10564v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2411.10564",
    "arxiv_authors": [
      "Mahmudul Hasan"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Vision+Eagle+Attention%3A+a+new+lens+for+advancing+image+classification+Mahmudul+Hasan",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "M Hasan -",
        "id": null
      }
    ],
    "citation_count": 2
  },
  {
    "arxiv_id": "2407.03836",
    "title": "ADAPT: Multimodal Learning for Detecting Physiological Changes under Missing Modalities",
    "year": 2024,
    "published": "2024-07-04T11:05:14Z",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "Multimodality has recently gained attention in the medical domain, where imaging or video modalities may be integrated with biomedical signals or health records. Yet, two challenges remain: balancing the contributions of modalities, especially in cases with a limited amount of data available, and tackling missing modalities. To address both issues, in this paper, we introduce the AnchoreD multimodAl Physiological Transformer (ADAPT), a multimodal, scalable framework with two key components: (i) ",
    "arxiv_url": "https://arxiv.org/abs/2407.03836v1",
    "pdf_url": "https://arxiv.org/pdf/2407.03836v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2407.03836",
    "arxiv_authors": [
      "Julie Mordacq",
      "Leo Milecki",
      "Maria Vakalopoulou",
      "Steve Oudot",
      "Vicky Kalogeiton"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=ADAPT%3A+Multimodal+Learning+for+Detecting+Physiological+Changes+under+Missing+Modalities+Julie+Mordacq+Leo+Milecki+Maria+Vakalopoulou+Steve+Oudot+Vicky+Kalogeiton",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "J Mordacq",
        "id": "s2ACBa0AAAAJ"
      },
      {
        "name": "L Milecki",
        "id": "R8ko8mYAAAAJ"
      },
      {
        "name": "M Vakalopoulou",
        "id": "FKUHYqMAAAAJ"
      },
      {
        "name": "S Oudot",
        "id": "tYki-xwAAAAJ"
      },
      {
        "name": "V Kalogeiton",
        "id": "gIRvhKkAAAAJ"
      }
    ],
    "citation_count": 7
  },
  {
    "arxiv_id": "2410.03987",
    "title": "Mamba Capsule Routing Towards Part-Whole Relational Camouflaged Object Detection",
    "year": 2024,
    "published": "2024-10-05T00:20:22Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "The part-whole relational property endowed by Capsule Networks (CapsNets) has been known successful for camouflaged object detection due to its segmentation integrity. However, the previous Expectation Maximization (EM) capsule routing algorithm with heavy computation and large parameters obstructs this trend. The primary attribution behind lies in the pixel-level capsule routing. Alternatively, in this paper, we propose a novel mamba capsule routing at the type level. Specifically, we first ext",
    "arxiv_url": "https://arxiv.org/abs/2410.03987v1",
    "pdf_url": "https://arxiv.org/pdf/2410.03987v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2410.03987",
    "arxiv_authors": [
      "Dingwen Zhang",
      "Liangbo Cheng",
      "Yi Liu",
      "Xinggang Wang",
      "Junwei Han"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Mamba+Capsule+Routing+Towards+Part-Whole+Relational+Camouflaged+Object+Detection+Dingwen+Zhang+Liangbo+Cheng+Yi+Liu+Xinggang+Wang+Junwei+Han",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "D Zhang",
        "id": null
      },
      {
        "name": "L Cheng",
        "id": null
      },
      {
        "name": "Y Liu",
        "id": "1EoCoNYAAAAJ"
      },
      {
        "name": "X Wang",
        "id": "qNCTLV0AAAAJ"
      },
      {
        "name": "J Han - International",
        "id": null
      }
    ],
    "citation_count": 17
  },
  {
    "arxiv_id": "2404.09111",
    "title": "Exploring Generative AI for Sim2Real in Driving Data Synthesis",
    "year": 2024,
    "published": "2024-04-14T01:23:19Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Datasets are essential for training and testing vehicle perception algorithms. However, the collection and annotation of real-world images is time-consuming and expensive. Driving simulators offer a solution by automatically generating various driving scenarios with corresponding annotations, but the simulation-to-reality (Sim2Real) domain gap remains a challenge. While most of the Generative Artificial Intelligence (AI) follows the de facto Generative Adversarial Nets (GANs)-based methods, the ",
    "arxiv_url": "https://arxiv.org/abs/2404.09111v1",
    "pdf_url": "https://arxiv.org/pdf/2404.09111v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2404.09111",
    "arxiv_authors": [
      "Haonan Zhao",
      "Yiting Wang",
      "Thomas Bashford-Rogers",
      "Valentina Donzella",
      "Kurt Debattista"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Exploring+Generative+AI+for+Sim2Real+in+Driving+Data+Synthesis+Haonan+Zhao+Yiting+Wang+Thomas+Bashford-Rogers+Valentina+Donzella+Kurt+Debattista",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "H Zhao",
        "id": "M7BJcU0AAAAJ"
      },
      {
        "name": "Y Wang",
        "id": "xbQjJgYAAAAJ"
      },
      {
        "name": "T Bashford-Rogers",
        "id": "axeVg8IAAAAJ"
      },
      {
        "name": "V Donzella",
        "id": "lV1A_88AAAAJ"
      },
      {
        "name": "K Debattista2024 IEEE Intelligent Vehicles",
        "id": null
      }
    ],
    "citation_count": 15
  },
  {
    "arxiv_id": "2304.01842",
    "title": "Evaluating Synthetic Pre-Training for Handwriting Processing Tasks",
    "year": 2023,
    "published": "2023-04-04T14:50:52Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "In this work, we explore massive pre-training on synthetic word images for enhancing the performance on four benchmark downstream handwriting analysis tasks. To this end, we build a large synthetic dataset of word images rendered in several handwriting fonts, which offers a complete supervision signal. We use it to train a simple convolutional neural network (ConvNet) with a fully supervised objective. The vector representations of the images obtained from the pre-trained ConvNet can then be con",
    "arxiv_url": "https://arxiv.org/abs/2304.01842v1",
    "pdf_url": "https://arxiv.org/pdf/2304.01842v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2304.01842",
    "arxiv_authors": [
      "Vittorio Pippi",
      "Silvia Cascianelli",
      "Lorenzo Baraldi",
      "Rita Cucchiara"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Evaluating+Synthetic+Pre-Training+for+Handwriting+Processing+Tasks+Vittorio+Pippi+Silvia+Cascianelli+Lorenzo+Baraldi+Rita+Cucchiara",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "V Pippi",
        "id": "OHmt2vUAAAAJ"
      },
      {
        "name": "S Cascianelli",
        "id": "utmt89wAAAAJ"
      },
      {
        "name": "L Baraldi",
        "id": "V4RuMvsAAAAJ"
      },
      {
        "name": "R Cucchiara - Pattern Recognition Letters",
        "id": null
      }
    ],
    "citation_count": 8
  },
  {
    "arxiv_id": "2404.08531",
    "title": "Text Prompt with Normality Guidance for Weakly Supervised Video Anomaly Detection",
    "year": 2024,
    "published": "2024-04-12T15:18:25Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Weakly supervised video anomaly detection (WSVAD) is a challenging task. Generating fine-grained pseudo-labels based on weak-label and then self-training a classifier is currently a promising solution. However, since the existing methods use only RGB visual modality and the utilization of category text information is neglected, thus limiting the generation of more accurate pseudo-labels and affecting the performance of self-training. Inspired by the manual labeling process based on the event des",
    "arxiv_url": "https://arxiv.org/abs/2404.08531v1",
    "pdf_url": "https://arxiv.org/pdf/2404.08531v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2404.08531",
    "arxiv_authors": [
      "Zhiwei Yang",
      "Jing Liu",
      "Peng Wu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Text+Prompt+with+Normality+Guidance+for+Weakly+Supervised+Video+Anomaly+Detection+Zhiwei+Yang+Jing+Liu+Peng+Wu",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "Z Yang",
        "id": "CBcPdc8AAAAJ"
      },
      {
        "name": "J Liu",
        "id": "kqRxf3MAAAAJ"
      },
      {
        "name": "P Wu -",
        "id": null
      }
    ],
    "citation_count": 89
  },
  {
    "arxiv_id": "2309.14538",
    "title": "Dynamic Scene Graph Representation for Surgical Video",
    "year": 2023,
    "published": "2023-09-25T21:28:14Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Surgical videos captured from microscopic or endoscopic imaging devices are rich but complex sources of information, depicting different tools and anatomical structures utilized during an extended amount of time. Despite containing crucial workflow information and being commonly recorded in many procedures, usage of surgical videos for automated surgical workflow understanding is still limited.   In this work, we exploit scene graphs as a more holistic, semantically meaningful and human-readable",
    "arxiv_url": "https://arxiv.org/abs/2309.14538v2",
    "pdf_url": "https://arxiv.org/pdf/2309.14538v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2309.14538",
    "arxiv_authors": [
      "Felix Holm",
      "Ghazal Ghazaei",
      "Tobias Czempiel",
      "Ege √ñzsoy",
      "Stefan Saur",
      "Nassir Navab"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Dynamic+Scene+Graph+Representation+for+Surgical+Video+Felix+Holm+Ghazal+Ghazaei+Tobias+Czempiel+Ege+%C3%96zsoy+Stefan+Saur",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "F Holm",
        "id": "87NxU6AAAAAJ"
      },
      {
        "name": "G Ghazaei",
        "id": "T7BXBYkAAAAJ"
      },
      {
        "name": "T Czempiel",
        "id": "p1Cvve0AAAAJ"
      },
      {
        "name": "E √ñzsoy",
        "id": "XrQVGwYAAAAJ"
      },
      {
        "name": "S Saur",
        "id": "xzUWl2MAAAAJ"
      },
      {
        "name": "N Navab",
        "id": "kzoVUPYAAAAJ"
      }
    ],
    "citation_count": 29
  },
  {
    "arxiv_id": "2408.01446",
    "title": "Estimating Environmental Cost Throughout Model's Adaptive Life Cycle",
    "year": 2024,
    "published": "2024-07-23T03:58:06Z",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "With the rapid increase in the research, development, and application of neural networks in the current era, there is a proportional increase in the energy needed to train and use models. Crucially, this is accompanied by the increase in carbon emissions into the environment. A sustainable and socially beneficial approach to reducing the carbon footprint and rising energy demands associated with the modern age of AI/deep learning is the adaptive and continuous reuse of models with regard to chan",
    "arxiv_url": "https://arxiv.org/abs/2408.01446v1",
    "pdf_url": "https://arxiv.org/pdf/2408.01446v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2408.01446",
    "arxiv_authors": [
      "Vishwesh Sangarya",
      "Richard Bradford",
      "Jung-Eun Kim"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Estimating+Environmental+Cost+Throughout+Model%27s+Adaptive+Life+Cycle+Vishwesh+Sangarya+Richard+Bradford+Jung-Eun+Kim",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "V Sangarya",
        "id": null
      },
      {
        "name": "R Bradford",
        "id": null
      },
      {
        "name": "JE Kim -",
        "id": null
      }
    ],
    "citation_count": 3
  },
  {
    "arxiv_id": "2505.16151",
    "title": "Training-Free Reasoning and Reflection in MLLMs",
    "year": 2025,
    "published": "2025-05-22T02:51:12Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Recent advances in Reasoning LLMs (e.g., DeepSeek-R1 and OpenAI-o1) have showcased impressive reasoning capabilities via reinforcement learning. However, extending these capabilities to Multimodal LLMs (MLLMs) is hampered by the prohibitive costs of retraining and the scarcity of high-quality, verifiable multimodal reasoning datasets. This paper introduces FRANK Model, a training-FRee ANd r1-liKe MLLM that imbues off-the-shelf MLLMs with reasoning and reflection abilities, without any gradient u",
    "arxiv_url": "https://arxiv.org/abs/2505.16151v1",
    "pdf_url": "https://arxiv.org/pdf/2505.16151v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2505.16151",
    "arxiv_authors": [
      "Hongchen Wei",
      "Zhenzhong Chen"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Training-Free+Reasoning+and+Reflection+in+MLLMs+Hongchen+Wei+Zhenzhong+Chen",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "H Wei",
        "id": "CCQ3YsEAAAAJ"
      },
      {
        "name": "Z Chen -",
        "id": null
      }
    ],
    "citation_count": 1
  },
  {
    "arxiv_id": "2404.09509",
    "title": "Fuse after Align: Improving Face-Voice Association Learning via Multimodal Encoder",
    "year": 2024,
    "published": "2024-04-15T07:05:14Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Today, there have been many achievements in learning the association between voice and face. However, most previous work models rely on cosine similarity or L2 distance to evaluate the likeness of voices and faces following contrastive learning, subsequently applied to retrieval and matching tasks. This method only considers the embeddings as high-dimensional vectors, utilizing a minimal scope of available information. This paper introduces a novel framework within an unsupervised setting for le",
    "arxiv_url": "https://arxiv.org/abs/2404.09509v1",
    "pdf_url": "https://arxiv.org/pdf/2404.09509v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2404.09509",
    "arxiv_authors": [
      "Chong Peng",
      "Liqiang He",
      "Dan Su"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Fuse+after+Align%3A+Improving+Face-Voice+Association+Learning+via+Multimodal+Encoder+Chong+Peng+Liqiang+He+Dan+Su",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "C Peng",
        "id": "_w3IAc8AAAAJ"
      },
      {
        "name": "L He",
        "id": null
      },
      {
        "name": "D Su -",
        "id": null
      }
    ],
    "citation_count": 2
  },
  {
    "arxiv_id": "2407.13750",
    "title": "Pose-guided multi-task video transformer for driver action recognition",
    "year": 2024,
    "published": "2024-07-18T17:53:51Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "We investigate the task of identifying situations of distracted driving through analysis of in-car videos. To tackle this challenge we introduce a multi-task video transformer that predicts both distracted actions and driver pose. Leveraging VideoMAEv2, a large pre-trained architecture, our approach incorporates semantic information from human keypoint locations to enhance action recognition and decrease computational overhead by minimizing the number of spatio-temporal tokens. By guiding token ",
    "arxiv_url": "https://arxiv.org/abs/2407.13750v1",
    "pdf_url": "https://arxiv.org/pdf/2407.13750v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2407.13750",
    "arxiv_authors": [
      "Ricardo Pizarro",
      "Roberto Valle",
      "Luis Miguel Bergasa",
      "Jos√© M. Buenaposada",
      "Luis Baumela"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Pose-guided+multi-task+video+transformer+for+driver+action+recognition+Ricardo+Pizarro+Roberto+Valle+Luis+Miguel+Bergasa+Jos%C3%A9+M.+Buenaposada+Luis+Baumela",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "R Pizarro",
        "id": "DI7ivS8AAAAJ"
      },
      {
        "name": "R Valle",
        "id": "cuoJUhQAAAAJ"
      },
      {
        "name": "LM Bergasa",
        "id": "uEBILewAAAAJ"
      },
      {
        "name": "JM Buenaposada",
        "id": "gtCI9YwAAAAJ"
      },
      {
        "name": "L Baumela",
        "id": "HQsRT00AAAAJ"
      }
    ],
    "citation_count": 1
  },
  {
    "arxiv_id": "2308.06715",
    "title": "StairNetV3: Depth-aware Stair Modeling using Deep Learning",
    "year": 2023,
    "published": "2023-08-13T08:11:40Z",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "abstract": "Vision-based stair perception can help autonomous mobile robots deal with the challenge of climbing stairs, especially in unfamiliar environments. To address the problem that current monocular vision methods are difficult to model stairs accurately without depth information, this paper proposes a depth-aware stair modeling method for monocular vision. Specifically, we take the extraction of stair geometric features and the prediction of depth images as joint tasks in a convolutional neural netwo",
    "arxiv_url": "https://arxiv.org/abs/2308.06715v1",
    "pdf_url": "https://arxiv.org/pdf/2308.06715v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2308.06715",
    "arxiv_authors": [
      "Chen Wang",
      "Zhongcai Pei",
      "Shuang Qiu",
      "Yachun Wang",
      "Zhiyong Tang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=StairNetV3%3A+Depth-aware+Stair+Modeling+using+Deep+Learning+Chen+Wang+Zhongcai+Pei+Shuang+Qiu+Yachun+Wang+Zhiyong+Tang",
    "gs_search_success": false,
    "gs_authors": null,
    "citation_count": null,
    "error_type": "fetch_failed"
  },
  {
    "arxiv_id": "2304.11718",
    "title": "No Free Lunch in Self Supervised Representation Learning",
    "year": 2023,
    "published": "2023-04-23T18:14:19Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Self-supervised representation learning in computer vision relies heavily on hand-crafted image transformations to learn meaningful and invariant features. However few extensive explorations of the impact of transformation design have been conducted in the literature. In particular, the dependence of downstream performances to transformation design has been established, but not studied in depth. In this work, we explore this relationship, its impact on a domain other than natural images, and sho",
    "arxiv_url": "https://arxiv.org/abs/2304.11718v1",
    "pdf_url": "https://arxiv.org/pdf/2304.11718v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2304.11718",
    "arxiv_authors": [
      "Ihab Bendidi",
      "Adrien Bardes",
      "Ethan Cohen",
      "Alexis Lamiable",
      "Guillaume Bollot",
      "Auguste Genovesio"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=No+Free+Lunch+in+Self+Supervised+Representation+Learning+Ihab+Bendidi+Adrien+Bardes+Ethan+Cohen+Alexis+Lamiable+Guillaume+Bollot",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "I Bendidi",
        "id": "8u5Ay84AAAAJ"
      },
      {
        "name": "A Bardes",
        "id": "SvRU8F8AAAAJ"
      },
      {
        "name": "E Cohen",
        "id": "WdUFxGYAAAAJ"
      },
      {
        "name": "A Lamiable",
        "id": "Z2QsFAEAAAAJ"
      },
      {
        "name": "G Bollot",
        "id": null
      },
      {
        "name": "A Genovesio",
        "id": "hE2StEAAAAAJ"
      }
    ],
    "citation_count": 12
  },
  {
    "arxiv_id": "2309.04399",
    "title": "MaskDiffusion: Boosting Text-to-Image Consistency with Conditional Mask",
    "year": 2023,
    "published": "2023-09-08T15:53:37Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Recent advancements in diffusion models have showcased their impressive capacity to generate visually striking images. Nevertheless, ensuring a close match between the generated image and the given prompt remains a persistent challenge. In this work, we identify that a crucial factor leading to the text-image mismatch issue is the inadequate cross-modality relation learning between the prompt and the output image. To better align the prompt and image content, we advance the cross-attention with ",
    "arxiv_url": "https://arxiv.org/abs/2309.04399v1",
    "pdf_url": "https://arxiv.org/pdf/2309.04399v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2309.04399",
    "arxiv_authors": [
      "Yupeng Zhou",
      "Daquan Zhou",
      "Zuo-Liang Zhu",
      "Yaxing Wang",
      "Qibin Hou",
      "Jiashi Feng"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=MaskDiffusion%3A+Boosting+Text-to-Image+Consistency+with+Conditional+Mask+Yupeng+Zhou+Daquan+Zhou+Zuo-Liang+Zhu+Yaxing+Wang+Qibin+Hou",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "Y Zhou",
        "id": "zQTdfUIAAAAJ"
      },
      {
        "name": "D Zhou",
        "id": "DdCAbWwAAAAJ"
      },
      {
        "name": "Y Wang",
        "id": null
      },
      {
        "name": "J Feng",
        "id": "Q8iay0gAAAAJ"
      },
      {
        "name": "Q Hou - International",
        "id": null
      }
    ],
    "citation_count": 14
  },
  {
    "arxiv_id": "2504.13406",
    "title": "LangCoop: Collaborative Driving with Language",
    "year": 2025,
    "published": "2025-04-18T02:03:14Z",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CL",
      "cs.CV"
    ],
    "abstract": "Multi-agent collaboration holds great promise for enhancing the safety, reliability, and mobility of autonomous driving systems by enabling information sharing among multiple connected agents. However, existing multi-agent communication approaches are hindered by limitations of existing communication media, including high bandwidth demands, agent heterogeneity, and information loss. To address these challenges, we introduce LangCoop, a new paradigm for collaborative autonomous driving that lever",
    "arxiv_url": "https://arxiv.org/abs/2504.13406v2",
    "pdf_url": "https://arxiv.org/pdf/2504.13406v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2504.13406",
    "arxiv_authors": [
      "Xiangbo Gao",
      "Yuheng Wu",
      "Rujia Wang",
      "Chenxi Liu",
      "Yang Zhou",
      "Zhengzhong Tu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=LangCoop%3A+Collaborative+Driving+with+Language+Xiangbo+Gao+Yuheng+Wu+Rujia+Wang+Chenxi+Liu+Yang+Zhou",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "X Gao",
        "id": "bSpZc84AAAAJ"
      },
      {
        "name": "Y Wu",
        "id": "bAhIqYwAAAAJ"
      },
      {
        "name": "R Wang",
        "id": null
      },
      {
        "name": "C Liu",
        "id": "UFvM01gAAAAJ"
      },
      {
        "name": "Y Zhou",
        "id": "0xCRTw0AAAAJ"
      },
      {
        "name": "Z Tu",
        "id": "9ajdZaEAAAAJ"
      }
    ],
    "citation_count": 15
  },
  {
    "arxiv_id": "2312.06071",
    "title": "Precipitation Downscaling with Spatiotemporal Video Diffusion",
    "year": 2023,
    "published": "2023-12-11T02:38:07Z",
    "categories": [
      "cs.CV",
      "cs.LG",
      "physics.ao-ph",
      "stat.ML"
    ],
    "abstract": "In climate science and meteorology, high-resolution local precipitation (rain and snowfall) predictions are limited by the computational costs of simulation-based methods. Statistical downscaling, or super-resolution, is a common workaround where a low-resolution prediction is improved using statistical approaches. Unlike traditional computer vision tasks, weather and climate applications require capturing the accurate conditional distribution of high-resolution given low-resolution patterns to ",
    "arxiv_url": "https://arxiv.org/abs/2312.06071v3",
    "pdf_url": "https://arxiv.org/pdf/2312.06071v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2312.06071",
    "arxiv_authors": [
      "Prakhar Srivastava",
      "Ruihan Yang",
      "Gavin Kerrigan",
      "Gideon Dresdner",
      "Jeremy McGibbon",
      "Christopher Bretherton",
      "Stephan Mandt"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Precipitation+Downscaling+with+Spatiotemporal+Video+Diffusion+Prakhar+Srivastava+Ruihan+Yang+Gavin+Kerrigan+Gideon+Dresdner+Jeremy+McGibbon",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "P Srivastava",
        "id": "E8RKhwYAAAAJ"
      },
      {
        "name": "R Yang",
        "id": "mWEXfLwAAAAJ"
      },
      {
        "name": "G Kerrigan",
        "id": "2F2XCy8AAAAJ"
      },
      {
        "name": "G Dresdner",
        "id": null
      },
      {
        "name": "J McGibbon",
        "id": "3_s_vKoAAAAJ"
      },
      {
        "name": "CS Bretherton",
        "id": "dGxT7WcAAAAJ"
      },
      {
        "name": "S MandtAdvances in Neural Information Processing Systems",
        "id": null
      }
    ],
    "citation_count": 13
  },
  {
    "arxiv_id": "2302.04578",
    "title": "Adversarial Example Does Good: Preventing Painting Imitation from Diffusion Models via Adversarial Examples",
    "year": 2023,
    "published": "2023-02-09T11:36:39Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CR",
      "cs.LG"
    ],
    "abstract": "Recently, Diffusion Models (DMs) boost a wave in AI for Art yet raise new copyright concerns, where infringers benefit from using unauthorized paintings to train DMs to generate novel paintings in a similar style. To address these emerging copyright violations, in this paper, we are the first to explore and propose to utilize adversarial examples for DMs to protect human-created artworks. Specifically, we first build a theoretical framework to define and evaluate the adversarial examples for DMs",
    "arxiv_url": "https://arxiv.org/abs/2302.04578v2",
    "pdf_url": "https://arxiv.org/pdf/2302.04578v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2302.04578",
    "arxiv_authors": [
      "Chumeng Liang",
      "Xiaoyu Wu",
      "Yang Hua",
      "Jiaru Zhang",
      "Yiming Xue",
      "Tao Song",
      "Zhengui Xue",
      "Ruhui Ma",
      "Haibing Guan"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Adversarial+Example+Does+Good%3A+Preventing+Painting+Imitation+from+Diffusion+Models+via+Adversarial+Examples+Chumeng+Liang+Xiaoyu+Wu+Yang+Hua+Jiaru+Zhang+Yiming+Xue",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "C Liang",
        "id": "4S0PYJYAAAAJ"
      },
      {
        "name": "X Wu",
        "id": "VzQkSAkAAAAJ"
      },
      {
        "name": "Y Hua",
        "id": "N0tFi8MAAAAJ"
      },
      {
        "name": "J Zhang",
        "id": "d6q4zkMAAAAJ"
      },
      {
        "name": "Y Xue",
        "id": null
      },
      {
        "name": "T Song",
        "id": "tIjK-3QAAAAJ"
      },
      {
        "name": "Z Xue",
        "id": null
      },
      {
        "name": "R Ma",
        "id": "PcrtqDsAAAAJ"
      },
      {
        "name": "H Guan",
        "id": null
      }
    ],
    "citation_count": 195
  },
  {
    "arxiv_id": "2504.10727",
    "title": "Foundation Models for Remote Sensing: An Analysis of MLLMs for Object Localization",
    "year": 2025,
    "published": "2025-04-14T21:34:06Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Multimodal large language models (MLLMs) have altered the landscape of computer vision, obtaining impressive results across a wide range of tasks, especially in zero-shot settings. Unfortunately, their strong performance does not always transfer to out-of-distribution domains, such as earth observation (EO) imagery. Prior work has demonstrated that MLLMs excel at some EO tasks, such as image captioning and scene understanding, while failing at tasks that require more fine-grained spatial reasoni",
    "arxiv_url": "https://arxiv.org/abs/2504.10727v1",
    "pdf_url": "https://arxiv.org/pdf/2504.10727v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2504.10727",
    "arxiv_authors": [
      "Darryl Hannan",
      "John Cooper",
      "Dylan White",
      "Timothy Doster",
      "Henry Kvinge",
      "Yijing Watkins"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Foundation+Models+for+Remote+Sensing%3A+An+Analysis+of+MLLMs+for+Object+Localization+Darryl+Hannan+John+Cooper+Dylan+White+Timothy+Doster+Henry+Kvinge",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "D Hannan",
        "id": "WjRBBYYAAAAJ"
      },
      {
        "name": "J Cooper",
        "id": null
      },
      {
        "name": "D White",
        "id": null
      },
      {
        "name": "T Doster",
        "id": "5kJfT1gAAAAJ"
      },
      {
        "name": "H Kvinge",
        "id": "vfFn_QsAAAAJ"
      },
      {
        "name": "Y Watkins",
        "id": null
      }
    ],
    "citation_count": null
  },
  {
    "arxiv_id": "2305.04790",
    "title": "MultiModal-GPT: A Vision and Language Model for Dialogue with Humans",
    "year": 2023,
    "published": "2023-05-08T15:45:42Z",
    "categories": [
      "cs.CV",
      "cs.CL"
    ],
    "abstract": "We present a vision and language model named MultiModal-GPT to conduct multi-round dialogue with humans. MultiModal-GPT can follow various instructions from humans, such as generating a detailed caption, counting the number of interested objects, and answering general questions from users. MultiModal-GPT is parameter-efficiently fine-tuned from OpenFlamingo, with Low-rank Adapter (LoRA) added both in the cross-attention part and the self-attention part of the language model. We first construct i",
    "arxiv_url": "https://arxiv.org/abs/2305.04790v3",
    "pdf_url": "https://arxiv.org/pdf/2305.04790v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2305.04790",
    "arxiv_authors": [
      "Tao Gong",
      "Chengqi Lyu",
      "Shilong Zhang",
      "Yudong Wang",
      "Miao Zheng",
      "Qian Zhao",
      "Kuikun Liu",
      "Wenwei Zhang",
      "Ping Luo",
      "Kai Chen"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=MultiModal-GPT%3A+A+Vision+and+Language+Model+for+Dialogue+with+Humans+Tao+Gong+Chengqi+Lyu+Shilong+Zhang+Yudong+Wang+Miao+Zheng",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "T Gong",
        "id": "_JhW9D0AAAAJ"
      },
      {
        "name": "C Lyu",
        "id": "kV3WvXcAAAAJ"
      },
      {
        "name": "S Zhang",
        "id": "s1NMu_UAAAAJ"
      },
      {
        "name": "Y Wang",
        "id": "O9Tzr1EAAAAJ"
      },
      {
        "name": "M Zheng",
        "id": null
      },
      {
        "name": "Q Zhao",
        "id": null
      },
      {
        "name": "K Liu",
        "id": "lcSm6RoAAAAJ"
      },
      {
        "name": "W Zhang",
        "id": "QDXADSEAAAAJ"
      },
      {
        "name": "P Luo",
        "id": "aXdjxb4AAAAJ"
      },
      {
        "name": "K Chen",
        "id": "eGD0b7IAAAAJ"
      }
    ],
    "citation_count": 339
  },
  {
    "arxiv_id": "2505.03401",
    "title": "DDaTR: Dynamic Difference-aware Temporal Residual Network for Longitudinal Radiology Report Generation",
    "year": 2025,
    "published": "2025-05-06T10:29:23Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Radiology Report Generation (RRG) automates the creation of radiology reports from medical imaging, enhancing the efficiency of the reporting process. Longitudinal Radiology Report Generation (LRRG) extends RRG by incorporating the ability to compare current and prior exams, facilitating the tracking of temporal changes in clinical findings. Existing LRRG approaches only extract features from prior and current images using a visual pre-trained encoder, which are then concatenated to generate the",
    "arxiv_url": "https://arxiv.org/abs/2505.03401v2",
    "pdf_url": "https://arxiv.org/pdf/2505.03401v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2505.03401",
    "arxiv_authors": [
      "Shanshan Song",
      "Hui Tang",
      "Honglong Yang",
      "Xiaomeng Li"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=DDaTR%3A+Dynamic+Difference-aware+Temporal+Residual+Network+for+Longitudinal+Radiology+Report+Generation+Shanshan+Song+Hui+Tang+Honglong+Yang+Xiaomeng+Li",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "S Song",
        "id": "EoNWyTcAAAAJ"
      },
      {
        "name": "H Tang",
        "id": "eqVvhiQAAAAJ"
      },
      {
        "name": "H Yang",
        "id": "3BPUjoQAAAAJ"
      },
      {
        "name": "X Li -",
        "id": null
      }
    ],
    "citation_count": 2
  },
  {
    "arxiv_id": "2311.05146",
    "title": "OW-SLR: Overlapping Windows on Semi-Local Region for Image Super-Resolution",
    "year": 2023,
    "published": "2023-11-09T05:06:55Z",
    "categories": [
      "cs.CV",
      "eess.IV"
    ],
    "abstract": "There has been considerable progress in implicit neural representation to upscale an image to any arbitrary resolution. However, existing methods are based on defining a function to predict the Red, Green and Blue (RGB) value from just four specific loci. Relying on just four loci is insufficient as it leads to losing fine details from the neighboring region(s). We show that by taking into account the semi-local region leads to an improvement in performance. In this paper, we propose applying a ",
    "arxiv_url": "https://arxiv.org/abs/2311.05146v2",
    "pdf_url": "https://arxiv.org/pdf/2311.05146v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2311.05146",
    "arxiv_authors": [
      "Rishav Bhardwaj",
      "Janarthanam Jothi Balaji",
      "Vasudevan Lakshminarayanan"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=OW-SLR%3A+Overlapping+Windows+on+Semi-Local+Region+for+Image+Super-Resolution+Rishav+Bhardwaj+Janarthanam+Jothi+Balaji+Vasudevan+Lakshminarayanan",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "R Bhardwaj",
        "id": "IyNYE68AAAAJ"
      },
      {
        "name": "J Jothi Balaji",
        "id": "e9gob5IAAAAJ"
      },
      {
        "name": "V Lakshminarayanan -",
        "id": null
      }
    ],
    "citation_count": null
  },
  {
    "arxiv_id": "2307.08263",
    "title": "Hierarchical Spatiotemporal Transformers for Video Object Segmentation",
    "year": 2023,
    "published": "2023-07-17T06:12:26Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "This paper presents a novel framework called HST for semi-supervised video object segmentation (VOS). HST extracts image and video features using the latest Swin Transformer and Video Swin Transformer to inherit their inductive bias for the spatiotemporal locality, which is essential for temporally coherent VOS. To take full advantage of the image and video features, HST casts image and video features as a query and memory, respectively. By applying efficient memory read operations at multiple s",
    "arxiv_url": "https://arxiv.org/abs/2307.08263v1",
    "pdf_url": "https://arxiv.org/pdf/2307.08263v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2307.08263",
    "arxiv_authors": [
      "Jun-Sang Yoo",
      "Hongjae Lee",
      "Seung-Won Jung"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Hierarchical+Spatiotemporal+Transformers+for+Video+Object+Segmentation+Jun-Sang+Yoo+Hongjae+Lee+Seung-Won+Jung",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "JS Yoo",
        "id": "PNm8ZUYAAAAJ"
      },
      {
        "name": "H Lee",
        "id": "gh526vMAAAAJ"
      },
      {
        "name": "SW Jung -",
        "id": null
      }
    ],
    "citation_count": 4
  },
  {
    "arxiv_id": "2404.10163",
    "title": "EyeFormer: Predicting Personalized Scanpaths with Transformer-Guided Reinforcement Learning",
    "year": 2024,
    "published": "2024-04-15T22:26:27Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.HC",
      "cs.LG"
    ],
    "abstract": "From a visual perception perspective, modern graphical user interfaces (GUIs) comprise a complex graphics-rich two-dimensional visuospatial arrangement of text, images, and interactive objects such as buttons and menus. While existing models can accurately predict regions and objects that are likely to attract attention ``on average'', so far there is no scanpath model capable of predicting scanpaths for an individual. To close this gap, we introduce EyeFormer, which leverages a Transformer arch",
    "arxiv_url": "https://arxiv.org/abs/2404.10163v2",
    "pdf_url": "https://arxiv.org/pdf/2404.10163v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2404.10163",
    "arxiv_authors": [
      "Yue Jiang",
      "Zixin Guo",
      "Hamed Rezazadegan Tavakoli",
      "Luis A. Leiva",
      "Antti Oulasvirta"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=EyeFormer%3A+Predicting+Personalized+Scanpaths+with+Transformer-Guided+Reinforcement+Learning+Yue+Jiang+Zixin+Guo+Hamed+Rezazadegan+Tavakoli+Luis+A.+Leiva+Antti+Oulasvirta",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "Y Jiang",
        "id": "KkdOIRoAAAAJ"
      },
      {
        "name": "Z Guo",
        "id": "XnrVKRoAAAAJ"
      },
      {
        "name": "H Rezazadegan Tavakoli",
        "id": "auVrkIkAAAAJ"
      },
      {
        "name": "LA Leiva",
        "id": "N0E7MlUAAAAJ"
      },
      {
        "name": "A Oulasvirta",
        "id": "_z41TLwAAAAJ"
      }
    ],
    "citation_count": 18
  },
  {
    "arxiv_id": "2407.08113",
    "title": "FYI: Flip Your Images for Dataset Distillation",
    "year": 2024,
    "published": "2024-07-11T01:08:41Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Dataset distillation synthesizes a small set of images from a large-scale real dataset such that synthetic and real images share similar behavioral properties (e.g, distributions of gradients or features) during a training process. Through extensive analyses on current methods and real datasets, together with empirical observations, we provide in this paper two important things to share for dataset distillation. First, object parts that appear on one side of a real image are highly likely to app",
    "arxiv_url": "https://arxiv.org/abs/2407.08113v1",
    "pdf_url": "https://arxiv.org/pdf/2407.08113v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2407.08113",
    "arxiv_authors": [
      "Byunggwan Son",
      "Youngmin Oh",
      "Donghyeon Baek",
      "Bumsub Ham"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=FYI%3A+Flip+Your+Images+for+Dataset+Distillation+Byunggwan+Son+Youngmin+Oh+Donghyeon+Baek+Bumsub+Ham",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "B Son",
        "id": "OdeJEGsAAAAJ"
      },
      {
        "name": "Y Oh",
        "id": "Ends2WoAAAAJ"
      },
      {
        "name": "D Baek",
        "id": "Br1Zs30AAAAJ"
      },
      {
        "name": "B Ham - European",
        "id": null
      }
    ],
    "citation_count": 4
  },
  {
    "arxiv_id": "2401.14111",
    "title": "Image Synthesis with Graph Conditioning: CLIP-Guided Diffusion Models for Scene Graphs",
    "year": 2024,
    "published": "2024-01-25T11:46:31Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Advancements in generative models have sparked significant interest in generating images while adhering to specific structural guidelines. Scene graph to image generation is one such task of generating images which are consistent with the given scene graph. However, the complexity of visual scenes poses a challenge in accurately aligning objects based on specified relations within the scene graph. Existing methods approach this task by first predicting a scene layout and generating images from t",
    "arxiv_url": "https://arxiv.org/abs/2401.14111v3",
    "pdf_url": "https://arxiv.org/pdf/2401.14111v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2401.14111",
    "arxiv_authors": [
      "Rameshwar Mishra",
      "A V Subramanyam"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Image+Synthesis+with+Graph+Conditioning%3A+CLIP-Guided+Diffusion+Models+for+Scene+Graphs+Rameshwar+Mishra+A+V+Subramanyam",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "R Mishra",
        "id": "593c8sAAAAAJ"
      },
      {
        "name": "AV Subramanyam -",
        "id": null
      }
    ],
    "citation_count": 7
  },
  {
    "arxiv_id": "2308.12416",
    "title": "Reframing the Brain Age Prediction Problem to a More Interpretable and Quantitative Approach",
    "year": 2023,
    "published": "2023-08-23T20:33:22Z",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV",
      "q-bio.QM"
    ],
    "abstract": "Deep learning models have achieved state-of-the-art results in estimating brain age, which is an important brain health biomarker, from magnetic resonance (MR) images. However, most of these models only provide a global age prediction, and rely on techniques, such as saliency maps to interpret their results. These saliency maps highlight regions in the input image that were significant for the model's predictions, but they are hard to be interpreted, and saliency map values are not directly comp",
    "arxiv_url": "https://arxiv.org/abs/2308.12416v1",
    "pdf_url": "https://arxiv.org/pdf/2308.12416v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2308.12416",
    "arxiv_authors": [
      "Neha Gianchandani",
      "Mahsa Dibaji",
      "Mariana Bento",
      "Ethan MacDonald",
      "Roberto Souza"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Reframing+the+Brain+Age+Prediction+Problem+to+a+More+Interpretable+and+Quantitative+Approach+Neha+Gianchandani+Mahsa+Dibaji+Mariana+Bento+Ethan+MacDonald+Roberto+Souza",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "N Gianchandani",
        "id": "zW3pptAAAAAJ"
      },
      {
        "name": "M Dibaji",
        "id": "OhCYnIcAAAAJ"
      },
      {
        "name": "M Bento",
        "id": "3DxVbpcAAAAJ"
      },
      {
        "name": "E MacDonald",
        "id": "3ipTl3cAAAAJ"
      },
      {
        "name": "R Souza",
        "id": "G2V4oBIAAAAJ"
      }
    ],
    "citation_count": 2
  },
  {
    "arxiv_id": "2311.18531",
    "title": "Dataset Distillation via the Wasserstein Metric",
    "year": 2023,
    "published": "2023-11-30T13:15:28Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "abstract": "Dataset Distillation (DD) aims to generate a compact synthetic dataset that enables models to achieve performance comparable to training on the full large dataset, significantly reducing computational costs. Drawing from optimal transport theory, we introduce WMDD (Wasserstein Metric-based Dataset Distillation), a straightforward yet powerful method that employs the Wasserstein metric to enhance distribution matching.   We compute the Wasserstein barycenter of features from a pretrained classifi",
    "arxiv_url": "https://arxiv.org/abs/2311.18531v3",
    "pdf_url": "https://arxiv.org/pdf/2311.18531v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2311.18531",
    "arxiv_authors": [
      "Haoyang Liu",
      "Yijiang Li",
      "Tiancheng Xing",
      "Peiran Wang",
      "Vibhu Dalal",
      "Luwei Li",
      "Jingrui He",
      "Haohan Wang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Dataset+Distillation+via+the+Wasserstein+Metric+Haoyang+Liu+Yijiang+Li+Tiancheng+Xing+Peiran+Wang+Vibhu+Dalal",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "H Liu",
        "id": "1jLWSKAAAAAJ"
      },
      {
        "name": "Y Li",
        "id": "Dx3z0m8AAAAJ"
      },
      {
        "name": "T Xing",
        "id": "j_9YdekAAAAJ"
      },
      {
        "name": "P Wang",
        "id": null
      },
      {
        "name": "V Dalal",
        "id": "Tu4xqMoAAAAJ"
      },
      {
        "name": "L Li",
        "id": null
      },
      {
        "name": "J He",
        "id": "hXpZynkAAAAJ"
      },
      {
        "name": "H Wang",
        "id": "nZxJGeUAAAAJ"
      }
    ],
    "citation_count": 22
  },
  {
    "arxiv_id": "2504.15118",
    "title": "Improving Sound Source Localization with Joint Slot Attention on Image and Audio",
    "year": 2025,
    "published": "2025-04-21T14:16:46Z",
    "categories": [
      "cs.CV",
      "cs.SD"
    ],
    "abstract": "Sound source localization (SSL) is the task of locating the source of sound within an image. Due to the lack of localization labels, the de facto standard in SSL has been to represent an image and audio as a single embedding vector each, and use them to learn SSL via contrastive learning. To this end, previous work samples one of local image features as the image embedding and aggregates all local audio features to obtain the audio embedding, which is far from optimal due to the presence of nois",
    "arxiv_url": "https://arxiv.org/abs/2504.15118v2",
    "pdf_url": "https://arxiv.org/pdf/2504.15118v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2504.15118",
    "arxiv_authors": [
      "Inho Kim",
      "Youngkil Song",
      "Jicheol Park",
      "Won Hwa Kim",
      "Suha Kwak"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Improving+Sound+Source+Localization+with+Joint+Slot+Attention+on+Image+and+Audio+Inho+Kim+Youngkil+Song+Jicheol+Park+Won+Hwa+Kim+Suha+Kwak",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "I Kim",
        "id": "yxEes8wAAAAJ"
      },
      {
        "name": "Y Song",
        "id": "s6XYYUMAAAAJ"
      },
      {
        "name": "J Park",
        "id": "AU4LPjUAAAAJ"
      },
      {
        "name": "WH Kim",
        "id": "aWPSHNwAAAAJ"
      },
      {
        "name": "S Kwak",
        "id": "-gscDIEAAAAJ"
      }
    ],
    "citation_count": 2
  },
  {
    "arxiv_id": "2410.03536",
    "title": "Computer Vision Intelligence Test Modeling and Generation: A Case Study on Smart OCR",
    "year": 2024,
    "published": "2024-09-14T23:33:28Z",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.CV"
    ],
    "abstract": "AI-based systems possess distinctive characteristics and introduce challenges in quality evaluation at the same time. Consequently, ensuring and validating AI software quality is of critical importance. In this paper, we present an effective AI software functional testing model to address this challenge. Specifically, we first present a comprehensive literature review of previous work, covering key facets of AI software testing processes. We then introduce a 3D classification model to systematic",
    "arxiv_url": "https://arxiv.org/abs/2410.03536v1",
    "pdf_url": "https://arxiv.org/pdf/2410.03536v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2410.03536",
    "arxiv_authors": [
      "Jing Shu",
      "Bing-Jiun Miu",
      "Eugene Chang",
      "Jerry Gao",
      "Jun Liu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Computer+Vision+Intelligence+Test+Modeling+and+Generation%3A+A+Case+Study+on+Smart+OCR+Jing+Shu+Bing-Jiun+Miu+Eugene+Chang+Jerry+Gao+Jun+Liu",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "J Shu",
        "id": null
      },
      {
        "name": "BJ Miu",
        "id": null
      },
      {
        "name": "E Chang",
        "id": null
      },
      {
        "name": "J Gao",
        "id": "vMi9grgAAAAJ"
      },
      {
        "name": "J Liu2024 IEEE International",
        "id": null
      }
    ],
    "citation_count": 4
  },
  {
    "arxiv_id": "2410.08469",
    "title": "Semantic Token Reweighting for Interpretable and Controllable Text Embeddings in CLIP",
    "year": 2024,
    "published": "2024-10-11T02:42:13Z",
    "categories": [
      "cs.LG",
      "cs.CL",
      "cs.CV"
    ],
    "abstract": "A text encoder within Vision-Language Models (VLMs) like CLIP plays a crucial role in translating textual input into an embedding space shared with images, thereby facilitating the interpretative analysis of vision tasks through natural language. Despite the varying significance of different textual elements within a sentence depending on the context, efforts to account for variation of importance in constructing text embeddings have been lacking. We propose a framework of Semantic Token Reweigh",
    "arxiv_url": "https://arxiv.org/abs/2410.08469v2",
    "pdf_url": "https://arxiv.org/pdf/2410.08469v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2410.08469",
    "arxiv_authors": [
      "Eunji Kim",
      "Kyuhong Shim",
      "Simyung Chang",
      "Sungroh Yoon"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Semantic+Token+Reweighting+for+Interpretable+and+Controllable+Text+Embeddings+in+CLIP+Eunji+Kim+Kyuhong+Shim+Simyung+Chang+Sungroh+Yoon",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "E Kim",
        "id": "nShf6cgAAAAJ"
      },
      {
        "name": "K Shim",
        "id": "msFkCLEAAAAJ"
      },
      {
        "name": "S Chang",
        "id": "0-tF1dwAAAAJ"
      },
      {
        "name": "S Yoon -",
        "id": null
      }
    ],
    "citation_count": 2
  },
  {
    "arxiv_id": "2308.05137",
    "title": "Discrepancy-based Active Learning for Weakly Supervised Bleeding Segmentation in Wireless Capsule Endoscopy Images",
    "year": 2023,
    "published": "2023-08-09T15:04:17Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Weakly supervised methods, such as class activation maps (CAM) based, have been applied to achieve bleeding segmentation with low annotation efforts in Wireless Capsule Endoscopy (WCE) images. However, the CAM labels tend to be extremely noisy, and there is an irreparable gap between CAM labels and ground truths for medical images. This paper proposes a new Discrepancy-basEd Active Learning (DEAL) approach to bridge the gap between CAMs and ground truths with a few annotations. Specifically, to ",
    "arxiv_url": "https://arxiv.org/abs/2308.05137v1",
    "pdf_url": "https://arxiv.org/pdf/2308.05137v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2308.05137",
    "arxiv_authors": [
      "Fan Bai",
      "Xiaohan Xing",
      "Yutian Shen",
      "Han Ma",
      "Max Q. -H. Meng"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Discrepancy-based+Active+Learning+for+Weakly+Supervised+Bleeding+Segmentation+in+Wireless+Capsule+Endoscopy+Images+Fan+Bai+Xiaohan+Xing+Yutian+Shen+Han+Ma+Max+Q.+-H.+Meng",
    "gs_search_success": false,
    "gs_authors": null,
    "citation_count": null,
    "error_type": "fetch_failed"
  },
  {
    "arxiv_id": "2304.05675",
    "title": "Semantic-Aware Mixup for Domain Generalization",
    "year": 2023,
    "published": "2023-04-12T07:49:52Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Deep neural networks (DNNs) have shown exciting performance in various tasks, yet suffer generalization failures when meeting unknown target domains. One of the most promising approaches to achieve domain generalization (DG) is generating unseen data, e.g., mixup, to cover the unknown target data. However, existing works overlook the challenges induced by the simultaneous appearance of changes in both the semantic and distribution space. Accordingly, such a challenge makes source distributions h",
    "arxiv_url": "https://arxiv.org/abs/2304.05675v1",
    "pdf_url": "https://arxiv.org/pdf/2304.05675v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2304.05675",
    "arxiv_authors": [
      "Chengchao Xu",
      "Xinmei Tian"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Semantic-Aware+Mixup+for+Domain+Generalization+Chengchao+Xu+Xinmei+Tian",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "C Xu",
        "id": "HubAG7kAAAAJ"
      },
      {
        "name": "X Tian -",
        "id": null
      }
    ],
    "citation_count": 8
  },
  {
    "arxiv_id": "2505.07396",
    "title": "TUM2TWIN: Introducing the Large-Scale Multimodal Urban Digital Twin Benchmark Dataset",
    "year": 2025,
    "published": "2025-05-12T09:48:32Z",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "Urban Digital Twins (UDTs) have become essential for managing cities and integrating complex, heterogeneous data from diverse sources. Creating UDTs involves challenges at multiple process stages, including acquiring accurate 3D source data, reconstructing high-fidelity 3D models, maintaining models' updates, and ensuring seamless interoperability to downstream tasks. Current datasets are usually limited to one part of the processing chain, hampering comprehensive UDTs validation. To address the",
    "arxiv_url": "https://arxiv.org/abs/2505.07396v2",
    "pdf_url": "https://arxiv.org/pdf/2505.07396v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2505.07396",
    "arxiv_authors": [
      "Olaf Wysocki",
      "Benedikt Schwab",
      "Manoj Kumar Biswanath",
      "Michael Greza",
      "Qilin Zhang",
      "Jingwei Zhu",
      "Thomas Froech",
      "Medhini Heeramaglore",
      "Ihab Hijazi",
      "Khaoula Kanna",
      "Mathias Pechinger",
      "Zhaiyu Chen",
      "Yao Sun",
      "Alejandro Rueda Segura",
      "Ziyang Xu",
      "Omar AbdelGafar",
      "Mansour Mehranfar",
      "Chandan Yeshwanth",
      "Yueh-Cheng Liu",
      "Hadi Yazdi",
      "Jiapan Wang",
      "Stefan Auer",
      "Katharina Anders",
      "Klaus Bogenberger",
      "Andre Borrmann",
      "Angela Dai",
      "Ludwig Hoegner",
      "Christoph Holst",
      "Thomas H. Kolbe",
      "Ferdinand Ludwig",
      "Matthias Nie√üner",
      "Frank Petzold",
      "Xiao Xiang Zhu",
      "Boris Jutzi"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=TUM2TWIN%3A+Introducing+the+Large-Scale+Multimodal+Urban+Digital+Twin+Benchmark+Dataset+Olaf+Wysocki+Benedikt+Schwab+Manoj+Kumar+Biswanath+Michael+Greza+Qilin+Zhang",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "O Wysocki",
        "id": "9xQhtFcAAAAJ"
      },
      {
        "name": "B Schwab",
        "id": "tp27X3cAAAAJ"
      },
      {
        "name": "MK Biswanath",
        "id": null
      },
      {
        "name": "M Greza",
        "id": null
      },
      {
        "name": "Q Zhang",
        "id": "Eh75_GcAAAAJ"
      },
      {
        "name": "J Zhu",
        "id": "UZ5b7VAAAAAJ"
      },
      {
        "name": "T Froech",
        "id": "BVFCf9oAAAAJ"
      },
      {
        "name": "M Heeramaglore",
        "id": null
      }
    ],
    "citation_count": 4
  },
  {
    "arxiv_id": "2411.16222",
    "title": "UltraSam: A Foundation Model for Ultrasound using Large Open-Access Segmentation Datasets",
    "year": 2024,
    "published": "2024-11-25T09:33:44Z",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "abstract": "Purpose: Automated ultrasound image analysis is challenging due to anatomical complexity and limited annotated data. To tackle this, we take a data-centric approach, assembling the largest public ultrasound segmentation dataset and training a versatile visual foundation model tailored for ultrasound.   Methods: We compile US-43d, a large-scale collection of 43 open-access ultrasound datasets with over 280,000 images and segmentation masks for more than 50 anatomical structures. We then introduce",
    "arxiv_url": "https://arxiv.org/abs/2411.16222v2",
    "pdf_url": "https://arxiv.org/pdf/2411.16222v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2411.16222",
    "arxiv_authors": [
      "Adrien Meyer",
      "Aditya Murali",
      "Farahdiba Zarin",
      "Didier Mutter",
      "Nicolas Padoy"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=UltraSam%3A+A+Foundation+Model+for+Ultrasound+using+Large+Open-Access+Segmentation+Datasets+Adrien+Meyer+Aditya+Murali+Farahdiba+Zarin+Didier+Mutter+Nicolas+Padoy",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "A Meyer",
        "id": "bofF1fcAAAAJ"
      },
      {
        "name": "A Murali",
        "id": "yN5fTGEAAAAJ"
      },
      {
        "name": "F Zarin",
        "id": "9ly0UPIAAAAJ"
      },
      {
        "name": "D Mutter",
        "id": "xwd0d-EAAAAJ"
      },
      {
        "name": "N PadoyInternational",
        "id": null
      }
    ],
    "citation_count": 9
  },
  {
    "arxiv_id": "2308.03349",
    "title": "SciGraphQA: A Large-Scale Synthetic Multi-Turn Question-Answering Dataset for Scientific Graphs",
    "year": 2023,
    "published": "2023-08-07T07:03:49Z",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV"
    ],
    "abstract": "In this work, we present SciGraphQA, a synthetic multi-turn question-answer dataset related to academic graphs. SciGraphQA is 13 times larger than ChartVQA, the previously largest chart-visual question-answering dataset. It is also the largest open-sourced chart VQA dataset with non-synthetic charts. To build our dataset, we selected 290,000 Computer Science or Machine Learning ArXiv papers published between 2010 and 2020, and then used Palm-2 to generate 295K samples of open-vocabulary multi-tu",
    "arxiv_url": "https://arxiv.org/abs/2308.03349v1",
    "pdf_url": "https://arxiv.org/pdf/2308.03349v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2308.03349",
    "arxiv_authors": [
      "Shengzhi Li",
      "Nima Tajbakhsh"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=SciGraphQA%3A+A+Large-Scale+Synthetic+Multi-Turn+Question-Answering+Dataset+for+Scientific+Graphs+Shengzhi+Li+Nima+Tajbakhsh",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "S Li",
        "id": "UBxhmfIAAAAJ"
      },
      {
        "name": "N Tajbakhsh -",
        "id": null
      }
    ],
    "citation_count": 76
  },
  {
    "arxiv_id": "2305.18769",
    "title": "DualVAE: Controlling Colours of Generated and Real Images",
    "year": 2023,
    "published": "2023-05-30T06:04:30Z",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "Colour controlled image generation and manipulation are of interest to artists and graphic designers. Vector Quantised Variational AutoEncoders (VQ-VAEs) with autoregressive (AR) prior are able to produce high quality images, but lack an explicit representation mechanism to control colour attributes. We introduce DualVAE, a hybrid representation model that provides such control by learning disentangled representations for colour and geometry. The geometry is represented by an image intensity map",
    "arxiv_url": "https://arxiv.org/abs/2305.18769v1",
    "pdf_url": "https://arxiv.org/pdf/2305.18769v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2305.18769",
    "arxiv_authors": [
      "Keerth Rathakumar",
      "David Liebowitz",
      "Christian Walder",
      "Kristen Moore",
      "Salil S. Kanhere"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=DualVAE%3A+Controlling+Colours+of+Generated+and+Real+Images+Keerth+Rathakumar+David+Liebowitz+Christian+Walder+Kristen+Moore+Salil+S.+Kanhere",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "K Rathakumar",
        "id": "5hhvj-EAAAAJ"
      },
      {
        "name": "D Liebowitz",
        "id": null
      },
      {
        "name": "C Walder",
        "id": "ugH_Wg4AAAAJ"
      },
      {
        "name": "K Moore",
        "id": "uI20HykAAAAJ"
      },
      {
        "name": "SS Kanhere",
        "id": "sgqmaPMAAAAJ"
      }
    ],
    "citation_count": null
  },
  {
    "arxiv_id": "2409.00353",
    "title": "RI-MAE: Rotation-Invariant Masked AutoEncoders for Self-Supervised Point Cloud Representation Learning",
    "year": 2024,
    "published": "2024-08-31T05:17:55Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Masked point modeling methods have recently achieved great success in self-supervised learning for point cloud data. However, these methods are sensitive to rotations and often exhibit sharp performance drops when encountering rotational variations. In this paper, we propose a novel Rotation-Invariant Masked AutoEncoders (RI-MAE) to address two major challenges: 1) achieving rotation-invariant latent representations, and 2) facilitating self-supervised reconstruction in a rotation-invariant mann",
    "arxiv_url": "https://arxiv.org/abs/2409.00353v2",
    "pdf_url": "https://arxiv.org/pdf/2409.00353v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2409.00353",
    "arxiv_authors": [
      "Kunming Su",
      "Qiuxia Wu",
      "Panpan Cai",
      "Xiaogang Zhu",
      "Xuequan Lu",
      "Zhiyong Wang",
      "Kun Hu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=RI-MAE%3A+Rotation-Invariant+Masked+AutoEncoders+for+Self-Supervised+Point+Cloud+Representation+Learning+Kunming+Su+Qiuxia+Wu+Panpan+Cai+Xiaogang+Zhu+Xuequan+Lu",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "K Su",
        "id": "AXeFtfYAAAAJ"
      },
      {
        "name": "Q Wu",
        "id": null
      },
      {
        "name": "P Cai",
        "id": null
      },
      {
        "name": "X Zhu",
        "id": "rav4c-0AAAAJ"
      },
      {
        "name": "X Lu",
        "id": "cG_WXywAAAAJ"
      },
      {
        "name": "Z Wang",
        "id": "Sqou_P0AAAAJ"
      },
      {
        "name": "K Hu",
        "id": "_UiH_xgAAAAJ"
      }
    ],
    "citation_count": 7
  },
  {
    "arxiv_id": "2312.09038",
    "title": "Object Recognition from Scientific Document based on Compartment Refinement Framework",
    "year": 2023,
    "published": "2023-12-14T15:36:49Z",
    "categories": [
      "cs.CV",
      "cs.DL",
      "cs.LG"
    ],
    "abstract": "With the rapid development of the internet in the past decade, it has become increasingly important to extract valuable information from vast resources efficiently, which is crucial for establishing a comprehensive digital ecosystem, particularly in the context of research surveys and comprehension. The foundation of these tasks focuses on accurate extraction and deep mining of data from scientific documents, which are essential for building a robust data infrastructure. However, parsing raw dat",
    "arxiv_url": "https://arxiv.org/abs/2312.09038v4",
    "pdf_url": "https://arxiv.org/pdf/2312.09038v4",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2312.09038",
    "arxiv_authors": [
      "Jinghong Li",
      "Wen Gu",
      "Koichi Ota",
      "Shinobu Hasegawa"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Object+Recognition+from+Scientific+Document+based+on+Compartment+Refinement+Framework+Jinghong+Li+Wen+Gu+Koichi+Ota+Shinobu+Hasegawa",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "J Li",
        "id": "EUbqW08AAAAJ"
      },
      {
        "name": "W Gu",
        "id": "deamBAYAAAAJ"
      },
      {
        "name": "K Ota",
        "id": null
      }
    ],
    "citation_count": 3
  },
  {
    "arxiv_id": "2306.00272",
    "title": "Accelerated Fingerprint Enhancement: A GPU-Optimized Mixed Architecture Approach",
    "year": 2023,
    "published": "2023-06-01T01:19:32Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "This document presents a preliminary approach to latent fingerprint enhancement, fundamentally designed around a mixed Unet architecture. It combines the capabilities of the Resnet-101 network and Unet encoder, aiming to form a potentially powerful composite. This combination, enhanced with attention mechanisms and forward skip connections, is intended to optimize the enhancement of ridge and minutiae features in fingerprints. One innovative element of this approach includes a novel Fingerprint ",
    "arxiv_url": "https://arxiv.org/abs/2306.00272v1",
    "pdf_url": "https://arxiv.org/pdf/2306.00272v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2306.00272",
    "arxiv_authors": [
      "Andr√© Brasil Vieira Wyzykowski",
      "Anil K. Jain"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Accelerated+Fingerprint+Enhancement%3A+A+GPU-Optimized+Mixed+Architecture+Approach+Andr%C3%A9+Brasil+Vieira+Wyzykowski+Anil+K.+Jain",
    "gs_search_success": false,
    "gs_authors": null,
    "citation_count": null,
    "error_type": "multiple_matches"
  },
  {
    "arxiv_id": "2403.05087",
    "title": "SplattingAvatar: Realistic Real-Time Human Avatars with Mesh-Embedded Gaussian Splatting",
    "year": 2024,
    "published": "2024-03-08T06:28:09Z",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "abstract": "We present SplattingAvatar, a hybrid 3D representation of photorealistic human avatars with Gaussian Splatting embedded on a triangle mesh, which renders over 300 FPS on a modern GPU and 30 FPS on a mobile device. We disentangle the motion and appearance of a virtual human with explicit mesh geometry and implicit appearance modeling with Gaussian Splatting. The Gaussians are defined by barycentric coordinates and displacement on a triangle mesh as Phong surfaces. We extend lifted optimization to",
    "arxiv_url": "https://arxiv.org/abs/2403.05087v1",
    "pdf_url": "https://arxiv.org/pdf/2403.05087v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2403.05087",
    "arxiv_authors": [
      "Zhijing Shao",
      "Zhaolong Wang",
      "Zhuang Li",
      "Duotun Wang",
      "Xiangru Lin",
      "Yu Zhang",
      "Mingming Fan",
      "Zeyu Wang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=SplattingAvatar%3A+Realistic+Real-Time+Human+Avatars+with+Mesh-Embedded+Gaussian+Splatting+Zhijing+Shao+Zhaolong+Wang+Zhuang+Li+Duotun+Wang+Xiangru+Lin",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "Z Shao",
        "id": "UUrJKSkAAAAJ"
      },
      {
        "name": "Z Wang",
        "id": "q7NLPG0AAAAJ"
      },
      {
        "name": "Z Li",
        "id": null
      },
      {
        "name": "D Wang",
        "id": "S-N5ZgIAAAAJ"
      },
      {
        "name": "X Lin",
        "id": "br97cK0AAAAJ"
      },
      {
        "name": "Y Zhang",
        "id": null
      },
      {
        "name": "M Fan",
        "id": "GAqZYGcAAAAJ"
      },
      {
        "name": "Z Wang",
        "id": "q7NLPG0AAAAJ"
      }
    ],
    "citation_count": 172
  },
  {
    "arxiv_id": "2505.17908",
    "title": "ComfyMind: Toward General-Purpose Generation via Tree-Based Planning and Reactive Feedback",
    "year": 2025,
    "published": "2025-05-23T13:53:03Z",
    "categories": [
      "cs.AI",
      "cs.CV"
    ],
    "abstract": "With the rapid advancement of generative models, general-purpose generation has gained increasing attention as a promising approach to unify diverse tasks across modalities within a single system. Despite this progress, existing open-source frameworks often remain fragile and struggle to support complex real-world applications due to the lack of structured workflow planning and execution-level feedback. To address these limitations, we present ComfyMind, a collaborative AI system designed to ena",
    "arxiv_url": "https://arxiv.org/abs/2505.17908v1",
    "pdf_url": "https://arxiv.org/pdf/2505.17908v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2505.17908",
    "arxiv_authors": [
      "Litao Guo",
      "Xinli Xu",
      "Luozhou Wang",
      "Jiantao Lin",
      "Jinsong Zhou",
      "Zixin Zhang",
      "Bolan Su",
      "Ying-Cong Chen"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=ComfyMind%3A+Toward+General-Purpose+Generation+via+Tree-Based+Planning+and+Reactive+Feedback+Litao+Guo+Xinli+Xu+Luozhou+Wang+Jiantao+Lin+Jinsong+Zhou",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "L Guo",
        "id": "efdm760AAAAJ"
      },
      {
        "name": "X Xu",
        "id": "lrgPuBUAAAAJ"
      },
      {
        "name": "L Wang",
        "id": "FMoFIBUAAAAJ"
      },
      {
        "name": "J Lin",
        "id": "ri-snP0AAAAJ"
      },
      {
        "name": "J Zhou",
        "id": "9GlGW1MAAAAJ"
      },
      {
        "name": "Z Zhang",
        "id": "BbZ0mwoAAAAJ"
      },
      {
        "name": "B Su",
        "id": null
      },
      {
        "name": "YC Chen",
        "id": "n7j4bJUAAAAJ"
      }
    ],
    "citation_count": 2
  },
  {
    "arxiv_id": "2503.18393",
    "title": "PDDM: Pseudo Depth Diffusion Model for RGB-PD Semantic Segmentation Based in Complex Indoor Scenes",
    "year": 2025,
    "published": "2025-03-24T07:05:31Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "The integration of RGB and depth modalities significantly enhances the accuracy of segmenting complex indoor scenes, with depth data from RGB-D cameras playing a crucial role in this improvement. However, collecting an RGB-D dataset is more expensive than an RGB dataset due to the need for specialized depth sensors. Aligning depth and RGB images also poses challenges due to sensor positioning and issues like missing data and noise. In contrast, Pseudo Depth (PD) from high-precision depth estimat",
    "arxiv_url": "https://arxiv.org/abs/2503.18393v1",
    "pdf_url": "https://arxiv.org/pdf/2503.18393v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2503.18393",
    "arxiv_authors": [
      "Xinhua Xu",
      "Hong Liu",
      "Jianbing Wu",
      "Jinfu Liu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=PDDM%3A+Pseudo+Depth+Diffusion+Model+for+RGB-PD+Semantic+Segmentation+Based+in+Complex+Indoor+Scenes+Xinhua+Xu+Hong+Liu+Jianbing+Wu+Jinfu+Liu",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "X Xu",
        "id": "Pn5QchoAAAAJ"
      },
      {
        "name": "H Liu",
        "id": "WLMUAjsAAAAJ"
      },
      {
        "name": "J Wu",
        "id": null
      },
      {
        "name": "J Liu -",
        "id": null
      }
    ],
    "citation_count": 1
  },
  {
    "arxiv_id": "2311.10356",
    "title": "Garment Recovery with Shape and Deformation Priors",
    "year": 2023,
    "published": "2023-11-17T07:06:21Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "While modeling people wearing tight-fitting clothing has made great strides in recent years, loose-fitting clothing remains a challenge. We propose a method that delivers realistic garment models from real-world images, regardless of garment shape or deformation. To this end, we introduce a fitting approach that utilizes shape and deformation priors learned from synthetic data to accurately capture garment shapes and deformations, including large ones. Not only does our approach recover the garm",
    "arxiv_url": "https://arxiv.org/abs/2311.10356v2",
    "pdf_url": "https://arxiv.org/pdf/2311.10356v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2311.10356",
    "arxiv_authors": [
      "Ren Li",
      "Corentin Dumery",
      "Beno√Æt Guillard",
      "Pascal Fua"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Garment+Recovery+with+Shape+and+Deformation+Priors+Ren+Li+Corentin+Dumery+Beno%C3%AEt+Guillard+Pascal+Fua",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "R Li",
        "id": "dZU-_FgAAAAJ"
      },
      {
        "name": "C Dumery",
        "id": "lUXqyiMAAAAJ"
      },
      {
        "name": "B Guillard",
        "id": "9c5ruhsAAAAJ"
      },
      {
        "name": "P Fua",
        "id": "kzFmAkYAAAAJ"
      }
    ],
    "citation_count": 17
  },
  {
    "arxiv_id": "2404.14247",
    "title": "From Modalities to Styles: Rethinking the Domain Gap in Heterogeneous Face Recognition",
    "year": 2024,
    "published": "2024-04-22T15:00:51Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Heterogeneous Face Recognition (HFR) focuses on matching faces from different domains, for instance, thermal to visible images, making Face Recognition (FR) systems more versatile for challenging scenarios. However, the domain gap between these domains and the limited large-scale datasets in the target HFR modalities make it challenging to develop robust HFR models from scratch. In our work, we view different modalities as distinct styles and propose a method to modulate feature maps of the targ",
    "arxiv_url": "https://arxiv.org/abs/2404.14247v1",
    "pdf_url": "https://arxiv.org/pdf/2404.14247v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2404.14247",
    "arxiv_authors": [
      "Anjith George",
      "Sebastien Marcel"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=From+Modalities+to+Styles%3A+Rethinking+the+Domain+Gap+in+Heterogeneous+Face+Recognition+Anjith+George+Sebastien+Marcel",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "A George",
        "id": "4CCrDp0AAAAJ"
      },
      {
        "name": "S MarcelIEEE Transactions on Biometrics",
        "id": null
      },
      {
        "name": "Behavior",
        "id": null
      },
      {
        "name": "and Identity Science",
        "id": null
      }
    ],
    "citation_count": 2
  },
  {
    "arxiv_id": "2405.15843",
    "title": "SpotNet: An Image Centric, Lidar Anchored Approach To Long Range Perception",
    "year": 2024,
    "published": "2024-05-24T17:25:48Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "In this paper, we propose SpotNet: a fast, single stage, image-centric but LiDAR anchored approach for long range 3D object detection. We demonstrate that our approach to LiDAR/image sensor fusion, combined with the joint learning of 2D and 3D detection tasks, can lead to accurate 3D object detection with very sparse LiDAR support. Unlike more recent bird's-eye-view (BEV) sensor-fusion methods which scale with range $r$ as $O(r^2)$, SpotNet scales as $O(1)$ with range. We argue that such an arch",
    "arxiv_url": "https://arxiv.org/abs/2405.15843v1",
    "pdf_url": "https://arxiv.org/pdf/2405.15843v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2405.15843",
    "arxiv_authors": [
      "Louis Foucard",
      "Samar Khanna",
      "Yi Shi",
      "Chi-Kuei Liu",
      "Quinn Z Shen",
      "Thuyen Ngo",
      "Zi-Xiang Xia"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=SpotNet%3A+An+Image+Centric%2C+Lidar+Anchored+Approach+To+Long+Range+Perception+Louis+Foucard+Samar+Khanna+Yi+Shi+Chi-Kuei+Liu+Quinn+Z+Shen",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "L Foucard",
        "id": null
      },
      {
        "name": "S Khanna",
        "id": "DPHEQsMAAAAJ"
      },
      {
        "name": "Y Shi",
        "id": null
      },
      {
        "name": "CK Liu",
        "id": null
      },
      {
        "name": "QZ Shen",
        "id": null
      },
      {
        "name": "T Ngo",
        "id": null
      },
      {
        "name": "ZX Xia",
        "id": "SGsNEFUAAAAJ"
      }
    ],
    "citation_count": 2
  },
  {
    "arxiv_id": "2304.03937",
    "title": "Delving into Discrete Normalizing Flows on SO(3) Manifold for Probabilistic Rotation Modeling",
    "year": 2023,
    "published": "2023-04-08T06:52:02Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Normalizing flows (NFs) provide a powerful tool to construct an expressive distribution by a sequence of trackable transformations of a base distribution and form a probabilistic model of underlying data. Rotation, as an important quantity in computer vision, graphics, and robotics, can exhibit many ambiguities when occlusion and symmetry occur and thus demands such probabilistic models. Though much progress has been made for NFs in Euclidean space, there are no effective normalizing flows witho",
    "arxiv_url": "https://arxiv.org/abs/2304.03937v1",
    "pdf_url": "https://arxiv.org/pdf/2304.03937v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2304.03937",
    "arxiv_authors": [
      "Yulin Liu",
      "Haoran Liu",
      "Yingda Yin",
      "Yang Wang",
      "Baoquan Chen",
      "He Wang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Delving+into+Discrete+Normalizing+Flows+on+SO%283%29+Manifold+for+Probabilistic+Rotation+Modeling+Yulin+Liu+Haoran+Liu+Yingda+Yin+Yang+Wang+Baoquan+Chen",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "Y Liu",
        "id": "Irlb8M8AAAAJ"
      },
      {
        "name": "H Liu",
        "id": null
      },
      {
        "name": "Y Yin",
        "id": null
      },
      {
        "name": "Y Wang",
        "id": "g_TxcboAAAAJ"
      },
      {
        "name": "B Chen",
        "id": "iHWtrEAAAAAJ"
      },
      {
        "name": "H Wang",
        "id": "roCAWkoAAAAJ"
      }
    ],
    "citation_count": 20
  },
  {
    "arxiv_id": "2411.18667",
    "title": "Point Cloud Unsupervised Pre-training via 3D Gaussian Splatting",
    "year": 2024,
    "published": "2024-11-27T16:11:45Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Pre-training on large-scale unlabeled datasets contribute to the model achieving powerful performance on 3D vision tasks, especially when annotations are limited. However, existing rendering-based self-supervised frameworks are computationally demanding and memory-intensive during pre-training due to the inherent nature of volume rendering. In this paper, we propose an efficient framework named GS$^3$ to learn point cloud representation, which seamlessly integrates fast 3D Gaussian Splatting int",
    "arxiv_url": "https://arxiv.org/abs/2411.18667v1",
    "pdf_url": "https://arxiv.org/pdf/2411.18667v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2411.18667",
    "arxiv_authors": [
      "Hao Liu",
      "Minglin Chen",
      "Yanni Ma",
      "Haihong Xiao",
      "Ying He"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Point+Cloud+Unsupervised+Pre-training+via+3D+Gaussian+Splatting+Hao+Liu+Minglin+Chen+Yanni+Ma+Haihong+Xiao+Ying+He",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "H Liu",
        "id": "CEEtEnoAAAAJ"
      },
      {
        "name": "M Chen",
        "id": "G4oCNmUAAAAJ"
      },
      {
        "name": "Y Ma",
        "id": null
      },
      {
        "name": "H Xiao",
        "id": "8AV-HUYAAAAJ"
      },
      {
        "name": "Y He -",
        "id": null
      }
    ],
    "citation_count": 1
  },
  {
    "arxiv_id": "2501.06014",
    "title": "Pose-independent 3D Anthropometry from Sparse Data",
    "year": 2025,
    "published": "2025-01-10T14:50:00Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "3D digital anthropometry is the study of estimating human body measurements from 3D scans. Precise body measurements are important health indicators in the medical industry, and guiding factors in the fashion, ergonomic and entertainment industries. The measuring protocol consists of scanning the whole subject in the static A-pose, which is maintained without breathing or movement during the scanning process. However, the A-pose is not easy to maintain during the whole scanning process, which ca",
    "arxiv_url": "https://arxiv.org/abs/2501.06014v1",
    "pdf_url": "https://arxiv.org/pdf/2501.06014v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2501.06014",
    "arxiv_authors": [
      "David Bojaniƒá",
      "Stefanie Wuhrer",
      "Tomislav Petkoviƒá",
      "Tomislav Pribaniƒá"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Pose-independent+3D+Anthropometry+from+Sparse+Data+David+Bojani%C4%87+Stefanie+Wuhrer+Tomislav+Petkovi%C4%87+Tomislav+Pribani%C4%87",
    "gs_search_success": false,
    "gs_authors": null,
    "citation_count": null,
    "error_type": "fetch_failed"
  },
  {
    "arxiv_id": "2502.14418",
    "title": "Role of the Pretraining and the Adaptation data sizes for low-resource real-time MRI video segmentation",
    "year": 2025,
    "published": "2025-02-20T10:15:43Z",
    "categories": [
      "eess.AS",
      "cs.CV",
      "eess.SP"
    ],
    "abstract": "Real-time Magnetic Resonance Imaging (rtMRI) is frequently used in speech production studies as it provides a complete view of the vocal tract during articulation. This study investigates the effectiveness of rtMRI in analyzing vocal tract movements by employing the SegNet and UNet models for Air-Tissue Boundary (ATB)segmentation tasks. We conducted pretraining of a few base models using increasing numbers of subjects and videos, to assess performance on two datasets. First, consisting of unseen",
    "arxiv_url": "https://arxiv.org/abs/2502.14418v1",
    "pdf_url": "https://arxiv.org/pdf/2502.14418v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2502.14418",
    "arxiv_authors": [
      "Masoud Thajudeen Tholan",
      "Vinayaka Hegde",
      "Chetan Sharma",
      "Prasanta Kumar Ghosh"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Role+of+the+Pretraining+and+the+Adaptation+data+sizes+for+low-resource+real-time+MRI+video+segmentation+Masoud+Thajudeen+Tholan+Vinayaka+Hegde+Chetan+Sharma+Prasanta+Kumar+Ghosh",
    "gs_search_success": false,
    "gs_authors": null,
    "citation_count": null,
    "error_type": "multiple_matches"
  },
  {
    "arxiv_id": "2312.16240",
    "title": "Merging Vision Transformers from Different Tasks and Domains",
    "year": 2023,
    "published": "2023-12-25T09:32:28Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "This work targets to merge various Vision Transformers (ViTs) trained on different tasks (i.e., datasets with different object categories) or domains (i.e., datasets with the same categories but different environments) into one unified model, yielding still good performance on each task or domain. Previous model merging works focus on either CNNs or NLP models, leaving the ViTs merging research untouched. To fill this gap, we first explore and find that existing model merging methods cannot well",
    "arxiv_url": "https://arxiv.org/abs/2312.16240v1",
    "pdf_url": "https://arxiv.org/pdf/2312.16240v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2312.16240",
    "arxiv_authors": [
      "Peng Ye",
      "Chenyu Huang",
      "Mingzhu Shen",
      "Tao Chen",
      "Yongqi Huang",
      "Yuning Zhang",
      "Wanli Ouyang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Merging+Vision+Transformers+from+Different+Tasks+and+Domains+Peng+Ye+Chenyu+Huang+Mingzhu+Shen+Tao+Chen+Yongqi+Huang",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "P Ye",
        "id": "UEZZP5QAAAAJ"
      },
      {
        "name": "C Huang",
        "id": "EKto99gAAAAJ"
      },
      {
        "name": "M Shen",
        "id": "o7vrw6IAAAAJ"
      },
      {
        "name": "T Chen",
        "id": "w3OoFL0AAAAJ"
      },
      {
        "name": "Y Huang",
        "id": null
      },
      {
        "name": "Y Zhang",
        "id": null
      },
      {
        "name": "W Ouyang",
        "id": "pw_0Z_UAAAAJ"
      }
    ],
    "citation_count": 19
  },
  {
    "arxiv_id": "2312.04861",
    "title": "Exploring Radar Data Representations in Autonomous Driving: A Comprehensive Review",
    "year": 2023,
    "published": "2023-12-08T06:31:19Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "With the rapid advancements of sensor technology and deep learning, autonomous driving systems are providing safe and efficient access to intelligent vehicles as well as intelligent transportation. Among these equipped sensors, the radar sensor plays a crucial role in providing robust perception information in diverse environmental conditions. This review focuses on exploring different radar data representations utilized in autonomous driving systems. Firstly, we introduce the capabilities and l",
    "arxiv_url": "https://arxiv.org/abs/2312.04861v3",
    "pdf_url": "https://arxiv.org/pdf/2312.04861v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2312.04861",
    "arxiv_authors": [
      "Shanliang Yao",
      "Runwei Guan",
      "Zitian Peng",
      "Chenhang Xu",
      "Yilu Shi",
      "Weiping Ding",
      "Eng Gee Lim",
      "Yong Yue",
      "Hyungjoon Seo",
      "Ka Lok Man",
      "Jieming Ma",
      "Xiaohui Zhu",
      "Yutao Yue"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Exploring+Radar+Data+Representations+in+Autonomous+Driving%3A+A+Comprehensive+Review+Shanliang+Yao+Runwei+Guan+Zitian+Peng+Chenhang+Xu+Yilu+Shi",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "S Yao",
        "id": "SHLZ-cYAAAAJ"
      },
      {
        "name": "R Guan",
        "id": "Fjo72tUAAAAJ"
      },
      {
        "name": "Z Peng",
        "id": "3aik9gIAAAAJ"
      },
      {
        "name": "C Xu",
        "id": null
      },
      {
        "name": "Y Shi",
        "id": null
      },
      {
        "name": "W Ding",
        "id": null
      },
      {
        "name": "EG Lim",
        "id": "zHw8eegAAAAJ"
      },
      {
        "name": "Y Yue",
        "id": "HXU7M0kAAAAJ"
      },
      {
        "name": "H Seo",
        "id": "9FOIHmYAAAAJ"
      },
      {
        "name": "KL Man",
        "id": "Pa_xqn8AAAAJ"
      },
      {
        "name": "J Ma",
        "id": "Z25_r3kAAAAJ"
      },
      {
        "name": "X Zhu",
        "id": "Ug_UiIoAAAAJ"
      }
    ],
    "citation_count": 35
  },
  {
    "arxiv_id": "2405.04175",
    "title": "Topicwise Separable Sentence Retrieval for Medical Report Generation",
    "year": 2024,
    "published": "2024-05-07T10:21:23Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Automated radiology reporting holds immense clinical potential in alleviating the burdensome workload of radiologists and mitigating diagnostic bias. Recently, retrieval-based report generation methods have garnered increasing attention due to their inherent advantages in terms of the quality and consistency of generated reports. However, due to the long-tail distribution of the training data, these models tend to learn frequently occurring sentences and topics, overlooking the rare topics. Regr",
    "arxiv_url": "https://arxiv.org/abs/2405.04175v1",
    "pdf_url": "https://arxiv.org/pdf/2405.04175v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2405.04175",
    "arxiv_authors": [
      "Junting Zhao",
      "Yang Zhou",
      "Zhihao Chen",
      "Huazhu Fu",
      "Liang Wan"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Topicwise+Separable+Sentence+Retrieval+for+Medical+Report+Generation+Junting+Zhao+Yang+Zhou+Zhihao+Chen+Huazhu+Fu+Liang+Wan",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "J Zhao",
        "id": "nRWOB4QAAAAJ"
      },
      {
        "name": "Y Zhou",
        "id": "_-cbldUAAAAJ"
      },
      {
        "name": "Z Chen",
        "id": null
      },
      {
        "name": "H Fu",
        "id": "jCvUBYMAAAAJ"
      },
      {
        "name": "L WanIEEE Transactions on Medical Imaging",
        "id": null
      }
    ],
    "citation_count": 4
  },
  {
    "arxiv_id": "2408.04224",
    "title": "Cross-View Meets Diffusion: Aerial Image Synthesis with Geometry and Text Guidance",
    "year": 2024,
    "published": "2024-08-08T05:17:27Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Aerial imagery analysis is critical for many research fields. However, obtaining frequent high-quality aerial images is not always accessible due to its high effort and cost requirements. One solution is to use the Ground-to-Aerial (G2A) technique to synthesize aerial images from easily collectible ground images. However, G2A is rarely studied, because of its challenges, including but not limited to, the drastic view changes, occlusion, and range of visibility. In this paper, we present a novel ",
    "arxiv_url": "https://arxiv.org/abs/2408.04224v2",
    "pdf_url": "https://arxiv.org/pdf/2408.04224v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2408.04224",
    "arxiv_authors": [
      "Ahmad Arrabi",
      "Xiaohan Zhang",
      "Waqas Sultani",
      "Chen Chen",
      "Safwan Wshah"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Cross-View+Meets+Diffusion%3A+Aerial+Image+Synthesis+with+Geometry+and+Text+Guidance+Ahmad+Arrabi+Xiaohan+Zhang+Waqas+Sultani+Chen+Chen+Safwan+Wshah",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "A Arrabi",
        "id": "rpg3l8QAAAAJ"
      },
      {
        "name": "X Zhang",
        "id": "hnEUxZ4AAAAJ"
      },
      {
        "name": "W Sultani",
        "id": "SqcjV8EAAAAJ"
      },
      {
        "name": "C Chen",
        "id": "TuEwcZ0AAAAJ"
      },
      {
        "name": "S Wshah2025 IEEE/CVF Winter",
        "id": null
      }
    ],
    "citation_count": 7
  },
  {
    "arxiv_id": "2310.07322",
    "title": "A webcam-based machine learning approach for three-dimensional range of motion evaluation",
    "year": 2023,
    "published": "2023-10-11T09:12:42Z",
    "categories": [
      "cs.HC",
      "cs.CV"
    ],
    "abstract": "Background. Joint range of motion (ROM) is an important quantitative measure for physical therapy. Commonly relying on a goniometer, accurate and reliable ROM measurement requires extensive training and practice. This, in turn, imposes a significant barrier for those who have limited in-person access to healthcare.   Objective. The current study presents and evaluates an alternative machine learning-based ROM evaluation method that could be remotely accessed via a webcam.   Methods. To evaluate ",
    "arxiv_url": "https://arxiv.org/abs/2310.07322v1",
    "pdf_url": "https://arxiv.org/pdf/2310.07322v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2310.07322",
    "arxiv_authors": [
      "Xiaoye Michael Wang",
      "Derek T. Smith",
      "Qin Zhu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=A+webcam-based+machine+learning+approach+for+three-dimensional+range+of+motion+evaluation+Xiaoye+Michael+Wang+Derek+T.+Smith+Qin+Zhu",
    "gs_search_success": false,
    "gs_authors": null,
    "citation_count": null,
    "error_type": "fetch_failed"
  },
  {
    "arxiv_id": "2312.13016",
    "title": "DiffPortrait3D: Controllable Diffusion for Zero-Shot Portrait View Synthesis",
    "year": 2023,
    "published": "2023-12-20T13:31:11Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "We present DiffPortrait3D, a conditional diffusion model that is capable of synthesizing 3D-consistent photo-realistic novel views from as few as a single in-the-wild portrait. Specifically, given a single RGB input, we aim to synthesize plausible but consistent facial details rendered from novel camera views with retained both identity and facial expression. In lieu of time-consuming optimization and fine-tuning, our zero-shot method generalizes well to arbitrary face portraits with unposed cam",
    "arxiv_url": "https://arxiv.org/abs/2312.13016v4",
    "pdf_url": "https://arxiv.org/pdf/2312.13016v4",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2312.13016",
    "arxiv_authors": [
      "Yuming Gu",
      "You Xie",
      "Hongyi Xu",
      "Guoxian Song",
      "Yichun Shi",
      "Di Chang",
      "Jing Yang",
      "Linjie Luo"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=DiffPortrait3D%3A+Controllable+Diffusion+for+Zero-Shot+Portrait+View+Synthesis+Yuming+Gu+You+Xie+Hongyi+Xu+Guoxian+Song+Yichun+Shi",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "Y Gu",
        "id": "qvciWWMAAAAJ"
      },
      {
        "name": "H Xu",
        "id": "gqtTGD4AAAAJ"
      },
      {
        "name": "Y Xie",
        "id": "FV0eXhQAAAAJ"
      },
      {
        "name": "G Song",
        "id": "EMyFIYgAAAAJ"
      },
      {
        "name": "Y Shi",
        "id": "RXZChV0AAAAJ"
      },
      {
        "name": "D Chang",
        "id": "68wkMTgAAAAJ"
      },
      {
        "name": "J Yang",
        "id": "DFo1j88AAAAJ"
      },
      {
        "name": "L Luo",
        "id": "fqubyX0AAAAJ"
      }
    ],
    "citation_count": 24
  },
  {
    "arxiv_id": "2401.06143",
    "title": "Redefining Recon: Bridging Gaps with UAVs, 360 degree Cameras, and Neural Radiance Fields",
    "year": 2023,
    "published": "2023-11-30T14:21:29Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "In the realm of digital situational awareness during disaster situations, accurate digital representations, like 3D models, play an indispensable role. To ensure the safety of rescue teams, robotic platforms are often deployed to generate these models. In this paper, we introduce an innovative approach that synergizes the capabilities of compact Unmaned Arial Vehicles (UAVs), smaller than 30 cm, equipped with 360 degree cameras and the advances of Neural Radiance Fields (NeRFs). A NeRF, a specia",
    "arxiv_url": "https://arxiv.org/abs/2401.06143v1",
    "pdf_url": "https://arxiv.org/pdf/2401.06143v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2401.06143",
    "arxiv_authors": [
      "Hartmut Surmann",
      "Niklas Digakis",
      "Jan-Nicklas Kremer",
      "Julien Meine",
      "Max Schulte",
      "Niklas Voigt"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Redefining+Recon%3A+Bridging+Gaps+with+UAVs%2C+360+degree+Cameras%2C+and+Neural+Radiance+Fields+Hartmut+Surmann+Niklas+Digakis+Jan-Nicklas+Kremer+Julien+Meine+Max+Schulte",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "H Surmann",
        "id": "j8XrW-wAAAAJ"
      },
      {
        "name": "N Digakis",
        "id": null
      },
      {
        "name": "JN Kremer",
        "id": null
      },
      {
        "name": "J Meine",
        "id": null
      },
      {
        "name": "M Schulte",
        "id": null
      },
      {
        "name": "N Voigt",
        "id": null
      }
    ],
    "citation_count": null
  },
  {
    "arxiv_id": "2407.04688",
    "title": "Enhancing Vehicle Re-identification and Matching for Weaving Analysis",
    "year": 2024,
    "published": "2024-07-05T17:50:35Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Vehicle weaving on highways contributes to traffic congestion, raises safety issues, and underscores the need for sophisticated traffic management systems. Current tools are inadequate in offering precise and comprehensive data on lane-specific weaving patterns. This paper introduces an innovative method for collecting non-overlapping video data in weaving zones, enabling the generation of quantitative insights into lane-specific weaving behaviors. Our experimental results confirm the efficacy o",
    "arxiv_url": "https://arxiv.org/abs/2407.04688v1",
    "pdf_url": "https://arxiv.org/pdf/2407.04688v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2407.04688",
    "arxiv_authors": [
      "Mei Qiu",
      "Wei Lin",
      "Stanley Chien",
      "Lauren Christopher",
      "Yaobin Chen",
      "Shu Hu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Enhancing+Vehicle+Re-identification+and+Matching+for+Weaving+Analysis+Mei+Qiu+Wei+Lin+Stanley+Chien+Lauren+Christopher+Yaobin+Chen",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "M Qiu",
        "id": "waoRJJsAAAAJ"
      },
      {
        "name": "W Lin",
        "id": null
      },
      {
        "name": "S Chien",
        "id": "KqmXGE0AAAAJ"
      },
      {
        "name": "L Christopher",
        "id": "0sil2KEAAAAJ"
      },
      {
        "name": "Y Chen",
        "id": null
      },
      {
        "name": "S Hu2024 IEEE International",
        "id": null
      }
    ],
    "citation_count": 3
  },
  {
    "arxiv_id": "2411.09265",
    "title": "BEARD: Benchmarking the Adversarial Robustness for Dataset Distillation",
    "year": 2024,
    "published": "2024-11-14T08:05:34Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Dataset Distillation (DD) is an emerging technique that compresses large-scale datasets into significantly smaller synthesized datasets while preserving high test performance and enabling the efficient training of large models. However, current research primarily focuses on enhancing evaluation accuracy under limited compression ratios, often overlooking critical security concerns such as adversarial robustness. A key challenge in evaluating this robustness lies in the complex interactions betwe",
    "arxiv_url": "https://arxiv.org/abs/2411.09265v1",
    "pdf_url": "https://arxiv.org/pdf/2411.09265v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2411.09265",
    "arxiv_authors": [
      "Zheng Zhou",
      "Wenquan Feng",
      "Shuchang Lyu",
      "Guangliang Cheng",
      "Xiaowei Huang",
      "Qi Zhao"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=BEARD%3A+Benchmarking+the+Adversarial+Robustness+for+Dataset+Distillation+Zheng+Zhou+Wenquan+Feng+Shuchang+Lyu+Guangliang+Cheng+Xiaowei+Huang",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "Z Zhou",
        "id": "L5o4LTcAAAAJ"
      },
      {
        "name": "W Feng",
        "id": null
      },
      {
        "name": "S Lyu",
        "id": null
      },
      {
        "name": "G Cheng",
        "id": "FToOC-wAAAAJ"
      },
      {
        "name": "X Huang",
        "id": "X4fLCCIAAAAJ"
      },
      {
        "name": "Q Zhao",
        "id": null
      }
    ],
    "citation_count": 2
  },
  {
    "arxiv_id": "2309.11131",
    "title": "Locate and Verify: A Two-Stream Network for Improved Deepfake Detection",
    "year": 2023,
    "published": "2023-09-20T08:25:19Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Deepfake has taken the world by storm, triggering a trust crisis. Current deepfake detection methods are typically inadequate in generalizability, with a tendency to overfit to image contents such as the background, which are frequently occurring but relatively unimportant in the training dataset. Furthermore, current methods heavily rely on a few dominant forgery regions and may ignore other equally important regions, leading to inadequate uncovering of forgery cues. In this paper, we strive to",
    "arxiv_url": "https://arxiv.org/abs/2309.11131v1",
    "pdf_url": "https://arxiv.org/pdf/2309.11131v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2309.11131",
    "arxiv_authors": [
      "Chao Shuai",
      "Jieming Zhong",
      "Shuang Wu",
      "Feng Lin",
      "Zhibo Wang",
      "Zhongjie Ba",
      "Zhenguang Liu",
      "Lorenzo Cavallaro",
      "Kui Ren"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Locate+and+Verify%3A+A+Two-Stream+Network+for+Improved+Deepfake+Detection+Chao+Shuai+Jieming+Zhong+Shuang+Wu+Feng+Lin+Zhibo+Wang",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "C Shuai",
        "id": "xpNpnhQAAAAJ"
      },
      {
        "name": "J Zhong",
        "id": "RSNixgMAAAAJ"
      },
      {
        "name": "S Wu",
        "id": "_IlTlTsAAAAJ"
      },
      {
        "name": "F Lin",
        "id": "gyBxQOAAAAAJ"
      },
      {
        "name": "Z Wang",
        "id": "0ox7zDkAAAAJ"
      },
      {
        "name": "Z Ba",
        "id": "dO2kc6kAAAAJ"
      },
      {
        "name": "Z Liu",
        "id": "OP2ySB8AAAAJ"
      },
      {
        "name": "L Cavallaro",
        "id": "oWT7fIYAAAAJ"
      },
      {
        "name": "K Ren",
        "id": "uuQA_rcAAAAJ"
      }
    ],
    "citation_count": 62
  },
  {
    "arxiv_id": "2310.12982",
    "title": "Putting the Object Back into Video Object Segmentation",
    "year": 2023,
    "published": "2023-10-19T17:59:56Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "We present Cutie, a video object segmentation (VOS) network with object-level memory reading, which puts the object representation from memory back into the video object segmentation result. Recent works on VOS employ bottom-up pixel-level memory reading which struggles due to matching noise, especially in the presence of distractors, resulting in lower performance in more challenging data. In contrast, Cutie performs top-down object-level memory reading by adapting a small set of object queries",
    "arxiv_url": "https://arxiv.org/abs/2310.12982v2",
    "pdf_url": "https://arxiv.org/pdf/2310.12982v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2310.12982",
    "arxiv_authors": [
      "Ho Kei Cheng",
      "Seoung Wug Oh",
      "Brian Price",
      "Joon-Young Lee",
      "Alexander Schwing"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Putting+the+Object+Back+into+Video+Object+Segmentation+Ho+Kei+Cheng+Seoung+Wug+Oh+Brian+Price+Joon-Young+Lee+Alexander+Schwing",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "HK Cheng",
        "id": "-ONsrGAAAAAJ"
      },
      {
        "name": "SW Oh",
        "id": "BWME3BoAAAAJ"
      },
      {
        "name": "B Price",
        "id": "ntGll74AAAAJ"
      },
      {
        "name": "JY Lee",
        "id": "6ajse1YAAAAJ"
      },
      {
        "name": "A Schwing",
        "id": "3B2c31wAAAAJ"
      }
    ],
    "citation_count": 197
  },
  {
    "arxiv_id": "2503.04501",
    "title": "IMFine: 3D Inpainting via Geometry-guided Multi-view Refinement",
    "year": 2025,
    "published": "2025-03-06T14:50:17Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Current 3D inpainting and object removal methods are largely limited to front-facing scenes, facing substantial challenges when applied to diverse, \"unconstrained\" scenes where the camera orientation and trajectory are unrestricted. To bridge this gap, we introduce a novel approach that produces inpainted 3D scenes with consistent visual quality and coherent underlying geometry across both front-facing and unconstrained scenes. Specifically, we propose a robust 3D inpainting pipeline that incorp",
    "arxiv_url": "https://arxiv.org/abs/2503.04501v1",
    "pdf_url": "https://arxiv.org/pdf/2503.04501v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2503.04501",
    "arxiv_authors": [
      "Zhihao Shi",
      "Dong Huo",
      "Yuhongze Zhou",
      "Kejia Yin",
      "Yan Min",
      "Juwei Lu",
      "Xinxin Zuo"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=IMFine%3A+3D+Inpainting+via+Geometry-guided+Multi-view+Refinement+Zhihao+Shi+Dong+Huo+Yuhongze+Zhou+Kejia+Yin+Yan+Min",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "Z Shi",
        "id": "xFAV1X8AAAAJ"
      },
      {
        "name": "D Huo",
        "id": "UludUtAAAAAJ"
      },
      {
        "name": "Y Zhou",
        "id": null
      },
      {
        "name": "Y Min",
        "id": "8bSWUAgAAAAJ"
      },
      {
        "name": "J Lu",
        "id": "Asz24wcAAAAJ"
      },
      {
        "name": "X Zuo",
        "id": "lv0UjhIAAAAJ"
      }
    ],
    "citation_count": 4
  },
  {
    "arxiv_id": "2310.15913",
    "title": "Mitigate Domain Shift by Primary-Auxiliary Objectives Association for Generalizing Person ReID",
    "year": 2023,
    "published": "2023-10-24T15:15:57Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "While deep learning has significantly improved ReID model accuracy under the independent and identical distribution (IID) assumption, it has also become clear that such models degrade notably when applied to an unseen novel domain due to unpredictable/unknown domain shift. Contemporary domain generalization (DG) ReID models struggle in learning domain-invariant representation solely through training on an instance classification objective. We consider that a deep learning model is heavily influe",
    "arxiv_url": "https://arxiv.org/abs/2310.15913v1",
    "pdf_url": "https://arxiv.org/pdf/2310.15913v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2310.15913",
    "arxiv_authors": [
      "Qilei Li",
      "Shaogang Gong"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Mitigate+Domain+Shift+by+Primary-Auxiliary+Objectives+Association+for+Generalizing+Person+ReID+Qilei+Li+Shaogang+Gong",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "Q Li",
        "id": "BIUlY6AAAAAJ"
      },
      {
        "name": "S Gong -",
        "id": null
      }
    ],
    "citation_count": 8
  },
  {
    "arxiv_id": "2502.04074",
    "title": "3D Prior is All You Need: Cross-Task Few-shot 2D Gaze Estimation",
    "year": 2025,
    "published": "2025-02-06T13:37:09Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "3D and 2D gaze estimation share the fundamental objective of capturing eye movements but are traditionally treated as two distinct research domains. In this paper, we introduce a novel cross-task few-shot 2D gaze estimation approach, aiming to adapt a pre-trained 3D gaze estimation network for 2D gaze prediction on unseen devices using only a few training images. This task is highly challenging due to the domain gap between 3D and 2D gaze, unknown screen poses, and limited training data. To addr",
    "arxiv_url": "https://arxiv.org/abs/2502.04074v3",
    "pdf_url": "https://arxiv.org/pdf/2502.04074v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2502.04074",
    "arxiv_authors": [
      "Yihua Cheng",
      "Hengfei Wang",
      "Zhongqun Zhang",
      "Yang Yue",
      "Bo Eun Kim",
      "Feng Lu",
      "Hyung Jin Chang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=3D+Prior+is+All+You+Need%3A+Cross-Task+Few-shot+2D+Gaze+Estimation+Yihua+Cheng+Hengfei+Wang+Zhongqun+Zhang+Yang+Yue+Bo+Eun+Kim",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "Y Cheng",
        "id": "cGn8lUAAAAAJ"
      },
      {
        "name": "H Wang",
        "id": "5fcCaV4AAAAJ"
      },
      {
        "name": "Z Zhang",
        "id": "B1mu6ugAAAAJ"
      },
      {
        "name": "Y Yue",
        "id": null
      },
      {
        "name": "B Kim",
        "id": "o0JYrXUAAAAJ"
      },
      {
        "name": "F Lu",
        "id": "9ggbm0QAAAAJ"
      },
      {
        "name": "HJ Chang",
        "id": "3TggrEkAAAAJ"
      }
    ],
    "citation_count": 1
  },
  {
    "arxiv_id": "2502.17709",
    "title": "Contrastive Visual Data Augmentation",
    "year": 2025,
    "published": "2025-02-24T23:05:31Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "cs.MM"
    ],
    "abstract": "Large multimodal models (LMMs) often struggle to recognize novel concepts, as they rely on pre-trained knowledge and have limited ability to capture subtle visual details. Domain-specific knowledge gaps in training also make them prone to confusing visually similar, commonly misrepresented, or low-resource concepts. To help LMMs better align nuanced visual features with language, improving their ability to recognize and reason about novel or rare concepts, we propose a Contrastive visual Data Au",
    "arxiv_url": "https://arxiv.org/abs/2502.17709v2",
    "pdf_url": "https://arxiv.org/pdf/2502.17709v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2502.17709",
    "arxiv_authors": [
      "Yu Zhou",
      "Bingxuan Li",
      "Mohan Tang",
      "Xiaomeng Jin",
      "Te-Lin Wu",
      "Kuan-Hao Huang",
      "Heng Ji",
      "Kai-Wei Chang",
      "Nanyun Peng"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Contrastive+Visual+Data+Augmentation+Yu+Zhou+Bingxuan+Li+Mohan+Tang+Xiaomeng+Jin+Te-Lin+Wu",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "Y Zhou",
        "id": "61rJc-YAAAAJ"
      },
      {
        "name": "B Li",
        "id": "l5Qe_mkAAAAJ"
      },
      {
        "name": "M Tang",
        "id": null
      },
      {
        "name": "X Jin",
        "id": "Jd_tsuEAAAAJ"
      },
      {
        "name": "TL Wu",
        "id": "UyWUO9IAAAAJ"
      },
      {
        "name": "KH Huang",
        "id": "PIWnCdYAAAAJ"
      },
      {
        "name": "H Ji",
        "id": "z7GCqT4AAAAJ"
      },
      {
        "name": "KW Chang",
        "id": "fqDBtzYAAAAJ"
      },
      {
        "name": "N Peng",
        "id": "XxRXvX0AAAAJ"
      }
    ],
    "citation_count": 1
  },
  {
    "arxiv_id": "2306.02878",
    "title": "Single-Stage 3D Geometry-Preserving Depth Estimation Model Training on Dataset Mixtures with Uncalibrated Stereo Data",
    "year": 2023,
    "published": "2023-06-05T13:49:24Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Nowadays, robotics, AR, and 3D modeling applications attract considerable attention to single-view depth estimation (SVDE) as it allows estimating scene geometry from a single RGB image. Recent works have demonstrated that the accuracy of an SVDE method hugely depends on the diversity and volume of the training data. However, RGB-D datasets obtained via depth capturing or 3D reconstruction are typically small, synthetic datasets are not photorealistic enough, and all these datasets lack diversit",
    "arxiv_url": "https://arxiv.org/abs/2306.02878v1",
    "pdf_url": "https://arxiv.org/pdf/2306.02878v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2306.02878",
    "arxiv_authors": [
      "Nikolay Patakin",
      "Mikhail Romanov",
      "Anna Vorontsova",
      "Mikhail Artemyev",
      "Anton Konushin"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Single-Stage+3D+Geometry-Preserving+Depth+Estimation+Model+Training+on+Dataset+Mixtures+with+Uncalibrated+Stereo+Data+Nikolay+Patakin+Mikhail+Romanov+Anna+Vorontsova+Mikhail+Artemyev+Anton+Konushin",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "N Patakin",
        "id": "ShNU3WAAAAAJ"
      },
      {
        "name": "A Vorontsova",
        "id": "HiVoQCIAAAAJ"
      },
      {
        "name": "M Artemyev",
        "id": "lYrPHWsAAAAJ"
      },
      {
        "name": "A Konushin",
        "id": "ZT_k-wMAAAAJ"
      }
    ],
    "citation_count": 9
  },
  {
    "arxiv_id": "2407.03575",
    "title": "DGR-MIL: Exploring Diverse Global Representation in Multiple Instance Learning for Whole Slide Image Classification",
    "year": 2024,
    "published": "2024-07-04T01:58:30Z",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "abstract": "Multiple instance learning (MIL) stands as a powerful approach in weakly supervised learning, regularly employed in histological whole slide image (WSI) classification for detecting tumorous lesions. However, existing mainstream MIL methods focus on modeling correlation between instances while overlooking the inherent diversity among instances. However, few MIL methods have aimed at diversity modeling, which empirically show inferior performance but with a high computational cost. To bridge this",
    "arxiv_url": "https://arxiv.org/abs/2407.03575v1",
    "pdf_url": "https://arxiv.org/pdf/2407.03575v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2407.03575",
    "arxiv_authors": [
      "Wenhui Zhu",
      "Xiwen Chen",
      "Peijie Qiu",
      "Aristeidis Sotiras",
      "Abolfazl Razi",
      "Yalin Wang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=DGR-MIL%3A+Exploring+Diverse+Global+Representation+in+Multiple+Instance+Learning+for+Whole+Slide+Image+Classification+Wenhui+Zhu+Xiwen+Chen+Peijie+Qiu+Aristeidis+Sotiras+Abolfazl+Razi",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "W Zhu",
        "id": "Se8aIO4YIp8C"
      },
      {
        "name": "X Chen",
        "id": "9jKJmcgAAAAJ"
      },
      {
        "name": "P Qiu",
        "id": "7HLmlHMAAAAJ"
      },
      {
        "name": "A Sotiras",
        "id": "MsNwZ-IAAAAJ"
      },
      {
        "name": "A Razi",
        "id": "DhwC8gsAAAAJ"
      },
      {
        "name": "Y WangEuropean",
        "id": null
      }
    ],
    "citation_count": 26
  },
  {
    "arxiv_id": "2409.12318",
    "title": "A large-scale study of performance and equity of commercial remote identity verification technologies across demographics",
    "year": 2024,
    "published": "2024-09-18T21:15:31Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "As more types of transactions move online, there is an increasing need to verify someone's identity remotely. Remote identity verification (RIdV) technologies have emerged to fill this need. RIdV solutions typically use a smart device to validate an identity document like a driver's license by comparing a face selfie to the face photo on the document. Recent research has been focused on ensuring that biometric systems work fairly across demographic groups. This study assesses five commercial RId",
    "arxiv_url": "https://arxiv.org/abs/2409.12318v1",
    "pdf_url": "https://arxiv.org/pdf/2409.12318v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2409.12318",
    "arxiv_authors": [
      "Kaniz Fatima",
      "Michael Schuckers",
      "Gerardo Cruz-Ortiz",
      "Daqing Hou",
      "Sandip Purnapatra",
      "Tiffany Andrews",
      "Ambuj Neupane",
      "Brandeis Marshall",
      "Stephanie Schuckers"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=A+large-scale+study+of+performance+and+equity+of+commercial+remote+identity+verification+technologies+across+demographics+Kaniz+Fatima+Michael+Schuckers+Gerardo+Cruz-Ortiz+Daqing+Hou+Sandip+Purnapatra",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "K Fatima",
        "id": "pj1hmcIAAAAJ"
      },
      {
        "name": "M Schuckers",
        "id": "vmzOZbcAAAAJ"
      },
      {
        "name": "G Cruz-Ortiz",
        "id": null
      },
      {
        "name": "D Hou",
        "id": "7EeqlwUAAAAJ"
      },
      {
        "name": "S Purnapatra",
        "id": "w2JJ_FoAAAAJ"
      },
      {
        "name": "T Andrews",
        "id": null
      },
      {
        "name": "A Neupane",
        "id": null
      }
    ],
    "citation_count": 5
  },
  {
    "arxiv_id": "2412.08545",
    "title": "Improving Satellite Imagery Masking using Multi-task and Transfer Learning",
    "year": 2024,
    "published": "2024-12-11T17:00:51Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Many remote sensing applications employ masking of pixels in satellite imagery for subsequent measurements. For example, estimating water quality variables, such as Suspended Sediment Concentration (SSC) requires isolating pixels depicting water bodies unaffected by clouds, their shadows, terrain shadows, and snow and ice formation. A significant bottleneck is the reliance on a variety of data products (e.g., satellite imagery, elevation maps), and a lack of precision in individual steps affecti",
    "arxiv_url": "https://arxiv.org/abs/2412.08545v1",
    "pdf_url": "https://arxiv.org/pdf/2412.08545v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2412.08545",
    "arxiv_authors": [
      "Rangel Daroya",
      "Luisa Vieira Lucchese",
      "Travis Simmons",
      "Punwath Prum",
      "Tamlin Pavelsky",
      "John Gardner",
      "Colin J. Gleason",
      "Subhransu Maji"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Improving+Satellite+Imagery+Masking+using+Multi-task+and+Transfer+Learning+Rangel+Daroya+Luisa+Vieira+Lucchese+Travis+Simmons+Punwath+Prum+Tamlin+Pavelsky",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "R Daroya",
        "id": "ZqpMqk0AAAAJ"
      },
      {
        "name": "LV Lucchese",
        "id": "unlfSuYAAAAJ"
      },
      {
        "name": "T Simmons",
        "id": null
      },
      {
        "name": "P Prum",
        "id": "h0Q203sAAAAJ"
      },
      {
        "name": "T Pavelsky",
        "id": "PNUjwowrnfUC"
      },
      {
        "name": "J Gardner",
        "id": "QK8XsCcAAAAJ"
      },
      {
        "name": "CJ Gleason",
        "id": "c_ezMJ0AAAAJ"
      },
      {
        "name": "S MajiIEEE",
        "id": null
      }
    ],
    "citation_count": 3
  },
  {
    "arxiv_id": "2304.11862",
    "title": "Universal Domain Adaptation via Compressive Attention Matching",
    "year": 2023,
    "published": "2023-04-24T07:16:54Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Universal domain adaptation (UniDA) aims to transfer knowledge from the source domain to the target domain without any prior knowledge about the label set. The challenge lies in how to determine whether the target samples belong to common categories. The mainstream methods make judgments based on the sample features, which overemphasizes global information while ignoring the most crucial local objects in the image, resulting in limited accuracy. To address this issue, we propose a Universal Atte",
    "arxiv_url": "https://arxiv.org/abs/2304.11862v4",
    "pdf_url": "https://arxiv.org/pdf/2304.11862v4",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2304.11862",
    "arxiv_authors": [
      "Didi Zhu",
      "Yincuan Li",
      "Junkun Yuan",
      "Zexi Li",
      "Kun Kuang",
      "Chao Wu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Universal+Domain+Adaptation+via+Compressive+Attention+Matching+Didi+Zhu+Yincuan+Li+Junkun+Yuan+Zexi+Li+Kun+Kuang",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "D Zhu",
        "id": "gthqIqIAAAAJ"
      },
      {
        "name": "Y Li",
        "id": "M6YfuCTSaKsC"
      },
      {
        "name": "J Yuan",
        "id": "j3iFVPsAAAAJ"
      },
      {
        "name": "Z Li",
        "id": "6lMg5eoAAAAJ"
      },
      {
        "name": "K Kuang",
        "id": "FOsNiMQAAAAJ"
      },
      {
        "name": "C Wu",
        "id": "gpTPt58AAAAJ"
      }
    ],
    "citation_count": 29
  },
  {
    "arxiv_id": "2402.08427",
    "title": "Leveraging Self-Supervised Instance Contrastive Learning for Radar Object Detection",
    "year": 2024,
    "published": "2024-02-13T12:53:33Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "In recent years, driven by the need for safer and more autonomous transport systems, the automotive industry has shifted toward integrating a growing number of Advanced Driver Assistance Systems (ADAS). Among the array of sensors employed for object recognition tasks, radar sensors have emerged as a formidable contender due to their abilities in adverse weather conditions or low-light scenarios and their robustness in maintaining consistent performance across diverse environments. However, the s",
    "arxiv_url": "https://arxiv.org/abs/2402.08427v1",
    "pdf_url": "https://arxiv.org/pdf/2402.08427v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2402.08427",
    "arxiv_authors": [
      "Colin Decourt",
      "Rufin VanRullen",
      "Didier Salle",
      "Thomas Oberlin"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Leveraging+Self-Supervised+Instance+Contrastive+Learning+for+Radar+Object+Detection+Colin+Decourt+Rufin+VanRullen+Didier+Salle+Thomas+Oberlin",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "C Decourt",
        "id": null
      },
      {
        "name": "R VanRullen",
        "id": "1pwyaYgAAAAJ"
      },
      {
        "name": "D Salle",
        "id": null
      },
      {
        "name": "T Oberlin -",
        "id": null
      }
    ],
    "citation_count": 2
  },
  {
    "arxiv_id": "2412.12755",
    "title": "Progressive Monitoring of Generative Model Training Evolution",
    "year": 2024,
    "published": "2024-12-17T10:20:29Z",
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "abstract": "While deep generative models (DGMs) have gained popularity, their susceptibility to biases and other inefficiencies that lead to undesirable outcomes remains an issue. With their growing complexity, there is a critical need for early detection of issues to achieve desired results and optimize resources. Hence, we introduce a progressive analysis framework to monitor the training process of DGMs. Our method utilizes dimensionality reduction techniques to facilitate the inspection of latent repres",
    "arxiv_url": "https://arxiv.org/abs/2412.12755v1",
    "pdf_url": "https://arxiv.org/pdf/2412.12755v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2412.12755",
    "arxiv_authors": [
      "Vidya Prasad",
      "Anna Vilanova",
      "Nicola Pezzotti"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Progressive+Monitoring+of+Generative+Model+Training+Evolution+Vidya+Prasad+Anna+Vilanova+Nicola+Pezzotti",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "V Prasad",
        "id": "dX9k_UkAAAAJ"
      },
      {
        "name": "A Vilanova",
        "id": "P83cpEEAAAAJ"
      },
      {
        "name": "N Pezzotti -",
        "id": null
      }
    ],
    "citation_count": null
  },
  {
    "arxiv_id": "2303.04334",
    "title": "Corner Detection Based on Multi-directional Gabor Filters with Multi-scales",
    "year": 2023,
    "published": "2023-03-08T02:11:54Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Gabor wavelet is an essential tool for image analysis and computer vision tasks. Local structure tensors with multiple scales are widely used in local feature extraction. Our research indicates that the current corner detection method based on Gabor wavelets can not effectively apply to complex scenes. In this work, the capability of the Gabor function to discriminate the intensity changes of step edges, L-shaped corners, Y-shaped or T-shaped corners, X-shaped corners, and star-shaped corners ar",
    "arxiv_url": "https://arxiv.org/abs/2303.04334v1",
    "pdf_url": "https://arxiv.org/pdf/2303.04334v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2303.04334",
    "arxiv_authors": [
      "Huaqing Wang",
      "Junfeng Jing",
      "Ning Li",
      "Weichuan Zhang",
      "Chao Liu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Corner+Detection+Based+on+Multi-directional+Gabor+Filters+with+Multi-scales+Huaqing+Wang+Junfeng+Jing+Ning+Li+Weichuan+Zhang+Chao+Liu",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "H Wang",
        "id": null
      },
      {
        "name": "J Jing",
        "id": "rS4ZH18AAAAJ"
      },
      {
        "name": "N Li",
        "id": null
      },
      {
        "name": "W Zhang",
        "id": "BUyQFIoAAAAJ"
      },
      {
        "name": "C Liu -",
        "id": null
      }
    ],
    "citation_count": null
  },
  {
    "arxiv_id": "2303.16940",
    "title": "T-FFTRadNet: Object Detection with Swin Vision Transformers from Raw ADC Radar Signals",
    "year": 2023,
    "published": "2023-03-29T18:04:19Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Object detection utilizing Frequency Modulated Continous Wave radar is becoming increasingly popular in the field of autonomous systems. Radar does not possess the same drawbacks seen by other emission-based sensors such as LiDAR, primarily the degradation or loss of return signals due to weather conditions such as rain or snow. However, radar does possess traits that make it unsuitable for standard emission-based deep learning representations such as point clouds. Radar point clouds tend to be ",
    "arxiv_url": "https://arxiv.org/abs/2303.16940v1",
    "pdf_url": "https://arxiv.org/pdf/2303.16940v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2303.16940",
    "arxiv_authors": [
      "James Giroux",
      "Martin Bouchard",
      "Robert Laganiere"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=T-FFTRadNet%3A+Object+Detection+with+Swin+Vision+Transformers+from+Raw+ADC+Radar+Signals+James+Giroux+Martin+Bouchard+Robert+Laganiere",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "J Giroux",
        "id": "HKK82K8AAAAJ"
      },
      {
        "name": "M Bouchard",
        "id": "Lb-zsiIAAAAJ"
      }
    ],
    "citation_count": 35
  },
  {
    "arxiv_id": "2305.11566",
    "title": "StereoVAE: A lightweight stereo-matching system using embedded GPUs",
    "year": 2023,
    "published": "2023-05-19T10:08:39Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.MM",
      "cs.RO"
    ],
    "abstract": "We present a lightweight system for stereo matching through embedded GPUs. It breaks the trade-off between accuracy and processing speed in stereo matching, enabling our embedded system to further improve the matching accuracy while ensuring real-time processing. The main idea of our method is to construct a tiny neural network based on variational auto-encoder (VAE) to upsample and refinement a small size of coarse disparity map, which is first generated by a traditional matching method. The pr",
    "arxiv_url": "https://arxiv.org/abs/2305.11566v3",
    "pdf_url": "https://arxiv.org/pdf/2305.11566v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2305.11566",
    "arxiv_authors": [
      "Qiong Chang",
      "Xiang Li",
      "Xin Xu",
      "Xin Liu",
      "Yun Li",
      "Miyazaki Jun"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=StereoVAE%3A+A+lightweight+stereo-matching+system+using+embedded+GPUs+Qiong+Chang+Xiang+Li+Xin+Xu+Xin+Liu+Yun+Li",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "Q Chang",
        "id": "GkfJ564AAAAJ"
      },
      {
        "name": "X Li",
        "id": "cPh-vn0AAAAJ"
      },
      {
        "name": "X Xu",
        "id": null
      },
      {
        "name": "X Liu",
        "id": "wviRTP0AAAAJ"
      },
      {
        "name": "Y Li",
        "id": "pog7XscAAAAJ"
      },
      {
        "name": "J Miyazaki2023 IEEE International",
        "id": null
      }
    ],
    "citation_count": 10
  },
  {
    "arxiv_id": "2305.14298",
    "title": "MOTRv3: Release-Fetch Supervision for End-to-End Multi-Object Tracking",
    "year": 2023,
    "published": "2023-05-23T17:40:13Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Although end-to-end multi-object trackers like MOTR enjoy the merits of simplicity, they suffer from the conflict between detection and association seriously, resulting in unsatisfactory convergence dynamics. While MOTRv2 partly addresses this problem, it demands an additional detection network for assistance. In this work, we serve as the first to reveal that this conflict arises from the unfair label assignment between detect queries and track queries during training, where these detect querie",
    "arxiv_url": "https://arxiv.org/abs/2305.14298v1",
    "pdf_url": "https://arxiv.org/pdf/2305.14298v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2305.14298",
    "arxiv_authors": [
      "En Yu",
      "Tiancai Wang",
      "Zhuoling Li",
      "Yuang Zhang",
      "Xiangyu Zhang",
      "Wenbing Tao"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=MOTRv3%3A+Release-Fetch+Supervision+for+End-to-End+Multi-Object+Tracking+En+Yu+Tiancai+Wang+Zhuoling+Li+Yuang+Zhang+Xiangyu+Zhang",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "E Yu",
        "id": "rWCQMNgAAAAJ"
      },
      {
        "name": "T Wang",
        "id": "YI0sRroAAAAJ"
      },
      {
        "name": "Z Li",
        "id": "2r6ejykAAAAJ"
      },
      {
        "name": "Y Zhang",
        "id": "pP5WG9wAAAAJ"
      },
      {
        "name": "X Zhang",
        "id": "yuB-cfoAAAAJ"
      },
      {
        "name": "W Tao",
        "id": "jRDPE2AAAAAJ"
      }
    ],
    "citation_count": 61
  },
  {
    "arxiv_id": "2501.03006",
    "title": "TransPixeler: Advancing Text-to-Video Generation with Transparency",
    "year": 2025,
    "published": "2025-01-06T13:32:16Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Text-to-video generative models have made significant strides, enabling diverse applications in entertainment, advertising, and education. However, generating RGBA video, which includes alpha channels for transparency, remains a challenge due to limited datasets and the difficulty of adapting existing models. Alpha channels are crucial for visual effects (VFX), allowing transparent elements like smoke and reflections to blend seamlessly into scenes. We introduce TransPixeler, a method to extend ",
    "arxiv_url": "https://arxiv.org/abs/2501.03006v2",
    "pdf_url": "https://arxiv.org/pdf/2501.03006v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2501.03006",
    "arxiv_authors": [
      "Luozhou Wang",
      "Yijun Li",
      "Zhifei Chen",
      "Jui-Hsien Wang",
      "Zhifei Zhang",
      "He Zhang",
      "Zhe Lin",
      "Yingcong Chen"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=TransPixeler%3A+Advancing+Text-to-Video+Generation+with+Transparency+Luozhou+Wang+Yijun+Li+Zhifei+Chen+Jui-Hsien+Wang+Zhifei+Zhang",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "L Wang",
        "id": "FMoFIBUAAAAJ"
      },
      {
        "name": "Y Li",
        "id": "nrsWSt4AAAAJ"
      },
      {
        "name": "Z Chen",
        "id": "RZv77XwAAAAJ"
      },
      {
        "name": "JH Wang",
        "id": "DSUfEqMAAAAJ"
      },
      {
        "name": "Z Zhang",
        "id": "HuerflQAAAAJ"
      },
      {
        "name": "H Zhang",
        "id": "HZLiJt0AAAAJ"
      },
      {
        "name": "Z Lin",
        "id": "R0bnqaAAAAAJ"
      },
      {
        "name": "YC Chen",
        "id": "n7j4bJUAAAAJ"
      }
    ],
    "citation_count": 7
  },
  {
    "arxiv_id": "2309.04147",
    "title": "Robot Localization and Mapping Final Report -- Sequential Adversarial Learning for Self-Supervised Deep Visual Odometry",
    "year": 2023,
    "published": "2023-09-08T06:24:17Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Visual odometry (VO) and SLAM have been using multi-view geometry via local structure from motion for decades. These methods have a slight disadvantage in challenging scenarios such as low-texture images, dynamic scenarios, etc. Meanwhile, use of deep neural networks to extract high level features is ubiquitous in computer vision. For VO, we can use these deep networks to extract depth and pose estimates using these high level features. The visual odometry task then can be modeled as an image ge",
    "arxiv_url": "https://arxiv.org/abs/2309.04147v1",
    "pdf_url": "https://arxiv.org/pdf/2309.04147v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2309.04147",
    "arxiv_authors": [
      "Akankshya Kar",
      "Sajal Maheshwari",
      "Shamit Lal",
      "Vinay Sameer Raja Kad"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Robot+Localization+and+Mapping+Final+Report+--+Sequential+Adversarial+Learning+for+Self-Supervised+Deep+Visual+Odometry+Akankshya+Kar+Sajal+Maheshwari+Shamit+Lal+Vinay+Sameer+Raja+Kad",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "A Kar",
        "id": null
      },
      {
        "name": "S Maheshwari",
        "id": "MlQg6KoAAAAJ"
      },
      {
        "name": "S Lal",
        "id": null
      },
      {
        "name": "VSR Kad -",
        "id": null
      }
    ],
    "citation_count": null
  },
  {
    "arxiv_id": "2505.20862",
    "title": "AVCD: Mitigating Hallucinations in Audio-Visual Large Language Models through Contrastive Decoding",
    "year": 2025,
    "published": "2025-05-27T08:13:57Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Hallucination remains a major challenge in multimodal large language models (MLLMs). To address this, various contrastive decoding (CD) methods have been proposed that contrasts original logits with hallucinated logits generated from perturbed inputs. While CD has shown promise in vision-language models (VLMs), it is not well-suited for AV-LLMs, where hallucinations often emerge from both unimodal and cross-modal combinations involving audio, video, and language. These intricate interactions cal",
    "arxiv_url": "https://arxiv.org/abs/2505.20862v2",
    "pdf_url": "https://arxiv.org/pdf/2505.20862v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2505.20862",
    "arxiv_authors": [
      "Chaeyoung Jung",
      "Youngjoon Jang",
      "Joon Son Chung"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=AVCD%3A+Mitigating+Hallucinations+in+Audio-Visual+Large+Language+Models+through+Contrastive+Decoding+Chaeyoung+Jung+Youngjoon+Jang+Joon+Son+Chung",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "C Jung",
        "id": "LooPec8AAAAJ"
      },
      {
        "name": "Y Jang",
        "id": "oB5AOQQAAAAJ"
      },
      {
        "name": "JS Chung -",
        "id": null
      }
    ],
    "citation_count": 1
  },
  {
    "arxiv_id": "2410.04135",
    "title": "IceCloudNet: 3D reconstruction of cloud ice from Meteosat SEVIRI",
    "year": 2024,
    "published": "2024-10-05T12:15:38Z",
    "categories": [
      "physics.ao-ph",
      "cs.AI",
      "cs.CV"
    ],
    "abstract": "IceCloudNet is a novel method based on machine learning able to predict high-quality vertically resolved cloud ice water contents (IWC) and ice crystal number concentrations (N$_\\textrm{ice}$). The predictions come at the spatio-temporal coverage and resolution of geostationary satellite observations (SEVIRI) and the vertical resolution of active satellite retrievals (DARDAR). IceCloudNet consists of a ConvNeXt-based U-Net and a 3D PatchGAN discriminator model and is trained by predicting DARDAR",
    "arxiv_url": "https://arxiv.org/abs/2410.04135v1",
    "pdf_url": "https://arxiv.org/pdf/2410.04135v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2410.04135",
    "arxiv_authors": [
      "Kai Jeggle",
      "Mikolaj Czerkawski",
      "Federico Serva",
      "Bertrand Le Saux",
      "David Neubauer",
      "Ulrike Lohmann"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=IceCloudNet%3A+3D+reconstruction+of+cloud+ice+from+Meteosat+SEVIRI+Kai+Jeggle+Mikolaj+Czerkawski+Federico+Serva+Bertrand+Le+Saux+David+Neubauer",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "K Jeggle",
        "id": "5_fiN-sAAAAJ"
      },
      {
        "name": "M Czerkawski",
        "id": "6KVrra8AAAAJ"
      },
      {
        "name": "F Serva",
        "id": null
      },
      {
        "name": "B Le Saux",
        "id": "SiGd2-YAAAAJ"
      },
      {
        "name": "D Neubauer",
        "id": null
      },
      {
        "name": "U LohmannArtificial Intelligence for the Earth Systems",
        "id": null
      }
    ],
    "citation_count": 4
  },
  {
    "arxiv_id": "2403.16384",
    "title": "Residual Dense Swin Transformer for Continuous Depth-Independent Ultrasound Imaging",
    "year": 2024,
    "published": "2024-03-25T03:01:53Z",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "abstract": "Ultrasound imaging is crucial for evaluating organ morphology and function, yet depth adjustment can degrade image quality and field-of-view, presenting a depth-dependent dilemma. Traditional interpolation-based zoom-in techniques often sacrifice detail and introduce artifacts. Motivated by the potential of arbitrary-scale super-resolution to naturally address these inherent challenges, we present the Residual Dense Swin Transformer Network (RDSTN), designed to capture the non-local characterist",
    "arxiv_url": "https://arxiv.org/abs/2403.16384v1",
    "pdf_url": "https://arxiv.org/pdf/2403.16384v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2403.16384",
    "arxiv_authors": [
      "Jintong Hu",
      "Hui Che",
      "Zishuo Li",
      "Wenming Yang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Residual+Dense+Swin+Transformer+for+Continuous+Depth-Independent+Ultrasound+Imaging+Jintong+Hu+Hui+Che+Zishuo+Li+Wenming+Yang",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "J Hu",
        "id": "cyRKzncAAAAJ"
      },
      {
        "name": "H Che",
        "id": "d33RudQAAAAJ"
      },
      {
        "name": "Z Li",
        "id": null
      },
      {
        "name": "W Yang - ICASSP",
        "id": null
      }
    ],
    "citation_count": 4
  },
  {
    "arxiv_id": "2412.17807",
    "title": "Cross-View Referring Multi-Object Tracking",
    "year": 2024,
    "published": "2024-12-23T18:58:39Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Referring Multi-Object Tracking (RMOT) is an important topic in the current tracking field. Its task form is to guide the tracker to track objects that match the language description. Current research mainly focuses on referring multi-object tracking under single-view, which refers to a view sequence or multiple unrelated view sequences. However, in the single-view, some appearances of objects are easily invisible, resulting in incorrect matching of objects with the language description. In this",
    "arxiv_url": "https://arxiv.org/abs/2412.17807v1",
    "pdf_url": "https://arxiv.org/pdf/2412.17807v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2412.17807",
    "arxiv_authors": [
      "Sijia Chen",
      "En Yu",
      "Wenbing Tao"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Cross-View+Referring+Multi-Object+Tracking+Sijia+Chen+En+Yu+Wenbing+Tao",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "S Chen",
        "id": "G2k32_gAAAAJ"
      },
      {
        "name": "E Yu",
        "id": "rWCQMNgAAAAJ"
      },
      {
        "name": "W Tao -",
        "id": null
      }
    ],
    "citation_count": 8
  },
  {
    "arxiv_id": "2410.10701",
    "title": "Early Diagnosis of Acute Lymphoblastic Leukemia Using YOLOv8 and YOLOv11 Deep Learning Models",
    "year": 2024,
    "published": "2024-10-14T16:42:07Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "abstract": "Leukemia, a severe form of blood cancer, claims thousands of lives each year. This study focuses on the detection of Acute Lymphoblastic Leukemia (ALL) using advanced image processing and deep learning techniques. By leveraging recent advancements in artificial intelligence, the research evaluates the reliability of these methods in practical, real-world scenarios. Specifically, it examines the performance of state-of-the-art YOLO models, including YOLOv8 and YOLOv11, to distinguish between mali",
    "arxiv_url": "https://arxiv.org/abs/2410.10701v2",
    "pdf_url": "https://arxiv.org/pdf/2410.10701v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2410.10701",
    "arxiv_authors": [
      "Alaa Awad",
      "Salah A. Aly"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Early+Diagnosis+of+Acute+Lymphoblastic+Leukemia+Using+YOLOv8+and+YOLOv11+Deep+Learning+Models+Alaa+Awad+Salah+A.+Aly",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "A Awad",
        "id": "SN6rlcQAAAAJ"
      },
      {
        "name": "SA Aly -",
        "id": null
      }
    ],
    "citation_count": 21
  },
  {
    "arxiv_id": "2310.02492",
    "title": "FairVision: Equitable Deep Learning for Eye Disease Screening via Fair Identity Scaling",
    "year": 2023,
    "published": "2023-10-03T23:44:35Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Equity in AI for healthcare is crucial due to its direct impact on human well-being. Despite advancements in 2D medical imaging fairness, the fairness of 3D models remains underexplored, hindered by the small sizes of 3D fairness datasets. Since 3D imaging surpasses 2D imaging in SOTA clinical care, it is critical to understand the fairness of these 3D models. To address this research gap, we conduct the first comprehensive study on the fairness of 3D medical imaging models across multiple prote",
    "arxiv_url": "https://arxiv.org/abs/2310.02492v3",
    "pdf_url": "https://arxiv.org/pdf/2310.02492v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2310.02492",
    "arxiv_authors": [
      "Yan Luo",
      "Muhammad Osama Khan",
      "Yu Tian",
      "Min Shi",
      "Zehao Dou",
      "Tobias Elze",
      "Yi Fang",
      "Mengyu Wang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=FairVision%3A+Equitable+Deep+Learning+for+Eye+Disease+Screening+via+Fair+Identity+Scaling+Yan+Luo+Muhammad+Osama+Khan+Yu+Tian+Min+Shi+Zehao+Dou",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "Y Luo",
        "id": "LSgqnKQAAAAJ"
      },
      {
        "name": "MO Khan",
        "id": "TVWgFWkAAAAJ"
      },
      {
        "name": "Y Tian",
        "id": "knptLuEAAAAJ"
      },
      {
        "name": "M Shi",
        "id": "NoA9KFIAAAAJ"
      },
      {
        "name": "Z Dou",
        "id": "CypbdCkAAAAJ"
      },
      {
        "name": "T Elze",
        "id": "IaDNG2wAAAAJ"
      },
      {
        "name": "Y Fang",
        "id": "j-cyhzwAAAAJ"
      },
      {
        "name": "M Wang",
        "id": "i9B02k4AAAAJ"
      }
    ],
    "citation_count": 11
  },
  {
    "arxiv_id": "2411.15653",
    "title": "OCDet: Object Center Detection via Bounding Box-Aware Heatmap Prediction on Edge Devices with NPUs",
    "year": 2024,
    "published": "2024-11-23T21:17:35Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Real-time object localization on edge devices is fundamental for numerous applications, ranging from surveillance to industrial automation. Traditional frameworks, such as object detection, segmentation, and keypoint detection, struggle in resource-constrained environments, often resulting in substantial target omissions. To address these challenges, we introduce OCDet, a lightweight Object Center Detection framework optimized for edge devices with NPUs. OCDet predicts heatmaps representing obje",
    "arxiv_url": "https://arxiv.org/abs/2411.15653v1",
    "pdf_url": "https://arxiv.org/pdf/2411.15653v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2411.15653",
    "arxiv_authors": [
      "Chen Xin",
      "Thomas Motz",
      "Andreas Hartel",
      "Enkelejda Kasneci"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=OCDet%3A+Object+Center+Detection+via+Bounding+Box-Aware+Heatmap+Prediction+on+Edge+Devices+with+NPUs+Chen+Xin+Thomas+Motz+Andreas+Hartel+Enkelejda+Kasneci",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "C Xin",
        "id": "vL3u70UAAAAJ"
      },
      {
        "name": "T Motz",
        "id": null
      },
      {
        "name": "A Hartel",
        "id": "ILTNvi4AAAAJ"
      },
      {
        "name": "E Kasneci -",
        "id": null
      }
    ],
    "citation_count": 2
  },
  {
    "arxiv_id": "2406.16481",
    "title": "Improving Quaternion Neural Networks with Quaternionic Activation Functions",
    "year": 2024,
    "published": "2024-06-24T09:36:58Z",
    "categories": [
      "cs.LG",
      "cs.CV",
      "cs.NE"
    ],
    "abstract": "In this paper, we propose novel quaternion activation functions where we modify either the quaternion magnitude or the phase, as an alternative to the commonly used split activation functions. We define criteria that are relevant for quaternion activation functions, and subsequently we propose our novel activation functions based on this analysis. Instead of applying a known activation function like the ReLU or Tanh on the quaternion elements separately, these activation functions consider the q",
    "arxiv_url": "https://arxiv.org/abs/2406.16481v1",
    "pdf_url": "https://arxiv.org/pdf/2406.16481v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2406.16481",
    "arxiv_authors": [
      "Johannes P√∂ppelbaum",
      "Andreas Schwung"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Improving+Quaternion+Neural+Networks+with+Quaternionic+Activation+Functions+Johannes+P%C3%B6ppelbaum+Andreas+Schwung",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "J P√∂ppelbaum",
        "id": null
      },
      {
        "name": "A Schwung - Knowledge-Based Systems",
        "id": null
      }
    ],
    "citation_count": 2
  },
  {
    "arxiv_id": "2302.12301",
    "title": "An Aligned Multi-Temporal Multi-Resolution Satellite Image Dataset for Change Detection Research",
    "year": 2023,
    "published": "2023-02-23T19:43:20Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "This paper presents an aligned multi-temporal and multi-resolution satellite image dataset for research in change detection. We expect our dataset to be useful to researchers who want to fuse information from multiple satellites for detecting changes on the surface of the earth that may not be fully visible in any single satellite. The dataset we present was created by augmenting the SpaceNet-7 dataset with temporally parallel stacks of Landsat and Sentinel images. The SpaceNet-7 dataset consist",
    "arxiv_url": "https://arxiv.org/abs/2302.12301v2",
    "pdf_url": "https://arxiv.org/pdf/2302.12301v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2302.12301",
    "arxiv_authors": [
      "Rahul Deshmukh",
      "Constantine J. Roros",
      "Amith Kashyap",
      "Avinash C. Kak"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=An+Aligned+Multi-Temporal+Multi-Resolution+Satellite+Image+Dataset+for+Change+Detection+Research+Rahul+Deshmukh+Constantine+J.+Roros+Amith+Kashyap+Avinash+C.+Kak",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "R Deshmukh",
        "id": "eELNH3wAAAAJ"
      },
      {
        "name": "CJ Roros",
        "id": "UirFjo0AAAAJ"
      },
      {
        "name": "A Kashyap",
        "id": "ogT5t80AAAAJ"
      },
      {
        "name": "AC Kak -",
        "id": null
      }
    ],
    "citation_count": 3
  },
  {
    "arxiv_id": "2410.17434",
    "title": "LongVU: Spatiotemporal Adaptive Compression for Long Video-Language Understanding",
    "year": 2024,
    "published": "2024-10-22T21:21:37Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Multimodal Large Language Models (MLLMs) have shown promising progress in understanding and analyzing video content. However, processing long videos remains a significant challenge constrained by LLM's context size. To address this limitation, we propose LongVU, a spatiotemporal adaptive compression mechanism thats reduces the number of video tokens while preserving visual details of long videos. Our idea is based on leveraging cross-modal query and inter-frame dependencies to adaptively reduce ",
    "arxiv_url": "https://arxiv.org/abs/2410.17434v1",
    "pdf_url": "https://arxiv.org/pdf/2410.17434v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2410.17434",
    "arxiv_authors": [
      "Xiaoqian Shen",
      "Yunyang Xiong",
      "Changsheng Zhao",
      "Lemeng Wu",
      "Jun Chen",
      "Chenchen Zhu",
      "Zechun Liu",
      "Fanyi Xiao",
      "Balakrishnan Varadarajan",
      "Florian Bordes",
      "Zhuang Liu",
      "Hu Xu",
      "Hyunwoo J. Kim",
      "Bilge Soran",
      "Raghuraman Krishnamoorthi",
      "Mohamed Elhoseiny",
      "Vikas Chandra"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=LongVU%3A+Spatiotemporal+Adaptive+Compression+for+Long+Video-Language+Understanding+Xiaoqian+Shen+Yunyang+Xiong+Changsheng+Zhao+Lemeng+Wu+Jun+Chen",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "X Shen",
        "id": "uToGtIwAAAAJ"
      },
      {
        "name": "Y Xiong",
        "id": "k5FaRwcAAAAJ"
      },
      {
        "name": "C Zhao",
        "id": "bXnrlyAAAAAJ"
      },
      {
        "name": "L Wu",
        "id": "PCDSl2sAAAAJ"
      },
      {
        "name": "J Chen",
        "id": "9G2OQmkAAAAJ"
      },
      {
        "name": "C Zhu",
        "id": "MlyN118AAAAJ"
      },
      {
        "name": "Z Liu",
        "id": "7OTD-LEAAAAJ"
      },
      {
        "name": "F Xiao",
        "id": "cuqP0dYAAAAJ"
      },
      {
        "name": "B Varadarajan",
        "id": null
      },
      {
        "name": "F Bordes",
        "id": "OADfWhUAAAAJ"
      },
      {
        "name": "Z Liu",
        "id": "7OTD-LEAAAAJ"
      }
    ],
    "citation_count": 145
  },
  {
    "arxiv_id": "2309.05782",
    "title": "Blendshapes GHUM: Real-time Monocular Facial Blendshape Prediction",
    "year": 2023,
    "published": "2023-09-11T19:29:26Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "We present Blendshapes GHUM, an on-device ML pipeline that predicts 52 facial blendshape coefficients at 30+ FPS on modern mobile phones, from a single monocular RGB image and enables facial motion capture applications like virtual avatars. Our main contributions are: i) an annotation-free offline method for obtaining blendshape coefficients from real-world human scans, ii) a lightweight real-time model that predicts blendshape coefficients based on facial landmarks.",
    "arxiv_url": "https://arxiv.org/abs/2309.05782v1",
    "pdf_url": "https://arxiv.org/pdf/2309.05782v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2309.05782",
    "arxiv_authors": [
      "Ivan Grishchenko",
      "Geng Yan",
      "Eduard Gabriel Bazavan",
      "Andrei Zanfir",
      "Nikolai Chinaev",
      "Karthik Raveendran",
      "Matthias Grundmann",
      "Cristian Sminchisescu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Blendshapes+GHUM%3A+Real-time+Monocular+Facial+Blendshape+Prediction+Ivan+Grishchenko+Geng+Yan+Eduard+Gabriel+Bazavan+Andrei+Zanfir+Nikolai+Chinaev",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "I Grishchenko",
        "id": "FWLtAaAAAAAJ"
      },
      {
        "name": "G Yan",
        "id": null
      },
      {
        "name": "EG Bazavan",
        "id": "1PZUz5AAAAAJ"
      },
      {
        "name": "A Zanfir",
        "id": "8lmzWycAAAAJ"
      },
      {
        "name": "N Chinaev",
        "id": null
      },
      {
        "name": "K Raveendran",
        "id": "r24MYdgAAAAJ"
      },
      {
        "name": "M Grundmann",
        "id": "_8SObXwAAAAJ"
      }
    ],
    "citation_count": null
  },
  {
    "arxiv_id": "2404.05828",
    "title": "Privacy-Preserving Deep Learning Using Deformable Operators for Secure Task Learning",
    "year": 2024,
    "published": "2024-04-08T19:46:20Z",
    "categories": [
      "cs.CV",
      "cs.CR",
      "eess.IV"
    ],
    "abstract": "In the era of cloud computing and data-driven applications, it is crucial to protect sensitive information to maintain data privacy, ensuring truly reliable systems. As a result, preserving privacy in deep learning systems has become a critical concern. Existing methods for privacy preservation rely on image encryption or perceptual transformation approaches. However, they often suffer from reduced task performance and high computational costs. To address these challenges, we propose a novel Pri",
    "arxiv_url": "https://arxiv.org/abs/2404.05828v1",
    "pdf_url": "https://arxiv.org/pdf/2404.05828v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2404.05828",
    "arxiv_authors": [
      "Fabian Perez",
      "Jhon Lopez",
      "Henry Arguello"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Privacy-Preserving+Deep+Learning+Using+Deformable+Operators+for+Secure+Task+Learning+Fabian+Perez+Jhon+Lopez+Henry+Arguello",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "F Perez",
        "id": "59gy5p8AAAAJ"
      },
      {
        "name": "J Lopez",
        "id": "d1QOBbYAAAAJ"
      },
      {
        "name": "H Arguello - ICASSP",
        "id": null
      }
    ],
    "citation_count": 3
  },
  {
    "arxiv_id": "2308.14391",
    "title": "FIRE: Food Image to REcipe generation",
    "year": 2023,
    "published": "2023-08-28T08:14:20Z",
    "categories": [
      "cs.CV",
      "cs.CL"
    ],
    "abstract": "Food computing has emerged as a prominent multidisciplinary field of research in recent years. An ambitious goal of food computing is to develop end-to-end intelligent systems capable of autonomously producing recipe information for a food image. Current image-to-recipe methods are retrieval-based and their success depends heavily on the dataset size and diversity, as well as the quality of learned embeddings. Meanwhile, the emergence of powerful attention-based vision and language models presen",
    "arxiv_url": "https://arxiv.org/abs/2308.14391v2",
    "pdf_url": "https://arxiv.org/pdf/2308.14391v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2308.14391",
    "arxiv_authors": [
      "Prateek Chhikara",
      "Dhiraj Chaurasia",
      "Yifan Jiang",
      "Omkar Masur",
      "Filip Ilievski"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=FIRE%3A+Food+Image+to+REcipe+generation+Prateek+Chhikara+Dhiraj+Chaurasia+Yifan+Jiang+Omkar+Masur+Filip+Ilievski",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "P Chhikara",
        "id": "RQTJ_aIAAAAJ"
      },
      {
        "name": "D Chaurasia",
        "id": "6c5TMZ8AAAAJ"
      },
      {
        "name": "Y Jiang",
        "id": "npRM7lYAAAAJ"
      },
      {
        "name": "O Masur",
        "id": "CXpHoIIAAAAJ"
      },
      {
        "name": "F Ilievski",
        "id": "4ZScBc0AAAAJ"
      }
    ],
    "citation_count": 44
  },
  {
    "arxiv_id": "2502.15188",
    "title": "Interleaved Block-based Learned Image Compression with Feature Enhancement and Quantization Error Compensation",
    "year": 2025,
    "published": "2025-02-21T03:40:27Z",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "abstract": "In recent years, learned image compression (LIC) methods have achieved significant performance improvements. However, obtaining a more compact latent representation and reducing the impact of quantization errors remain key challenges in the field of LIC. To address these challenges, we propose a feature extraction module, a feature refinement module, and a feature enhancement module. Our feature extraction module shuffles the pixels in the image, splits the resulting image into sub-images, and e",
    "arxiv_url": "https://arxiv.org/abs/2502.15188v1",
    "pdf_url": "https://arxiv.org/pdf/2502.15188v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2502.15188",
    "arxiv_authors": [
      "Shiqi Jiang",
      "Hui Yuan",
      "Shuai Li",
      "Raouf Hamzaoui",
      "Xu Wang",
      "Junyan Huo"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Interleaved+Block-based+Learned+Image+Compression+with+Feature+Enhancement+and+Quantization+Error+Compensation+Shiqi+Jiang+Hui+Yuan+Shuai+Li+Raouf+Hamzaoui+Xu+Wang",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "S Jiang",
        "id": null
      },
      {
        "name": "H Yuan",
        "id": "u58FkyUAAAAJ"
      },
      {
        "name": "S Li",
        "id": "GY1t5OYAAAAJ"
      },
      {
        "name": "R Hamzaoui",
        "id": "aEQSVDgAAAAJ"
      },
      {
        "name": "X Wang",
        "id": null
      },
      {
        "name": "J Huo",
        "id": "u2fj024AAAAJ"
      }
    ],
    "citation_count": null
  },
  {
    "arxiv_id": "2304.02265",
    "title": "Deep Perceptual Similarity is Adaptable to Ambiguous Contexts",
    "year": 2023,
    "published": "2023-04-05T07:31:44Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "The concept of image similarity is ambiguous, and images can be similar in one context and not in another. This ambiguity motivates the creation of metrics for specific contexts. This work explores the ability of deep perceptual similarity (DPS) metrics to adapt to a given context. DPS metrics use the deep features of neural networks for comparing images. These metrics have been successful on datasets that leverage the average human perception in limited settings. But the question remains if the",
    "arxiv_url": "https://arxiv.org/abs/2304.02265v2",
    "pdf_url": "https://arxiv.org/pdf/2304.02265v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2304.02265",
    "arxiv_authors": [
      "Gustav Grund Pihlgren",
      "Fredrik Sandin",
      "Marcus Liwicki"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Deep+Perceptual+Similarity+is+Adaptable+to+Ambiguous+Contexts+Gustav+Grund+Pihlgren+Fredrik+Sandin+Marcus+Liwicki",
    "gs_search_success": false,
    "gs_authors": null,
    "citation_count": null,
    "error_type": "multiple_matches"
  },
  {
    "arxiv_id": "2307.07511",
    "title": "NIFTY: Neural Object Interaction Fields for Guided Human Motion Synthesis",
    "year": 2023,
    "published": "2023-07-14T17:59:38Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "We address the problem of generating realistic 3D motions of humans interacting with objects in a scene. Our key idea is to create a neural interaction field attached to a specific object, which outputs the distance to the valid interaction manifold given a human pose as input. This interaction field guides the sampling of an object-conditioned human motion diffusion model, so as to encourage plausible contacts and affordance semantics. To support interactions with scarcely available data, we pr",
    "arxiv_url": "https://arxiv.org/abs/2307.07511v1",
    "pdf_url": "https://arxiv.org/pdf/2307.07511v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2307.07511",
    "arxiv_authors": [
      "Nilesh Kulkarni",
      "Davis Rempe",
      "Kyle Genova",
      "Abhijit Kundu",
      "Justin Johnson",
      "David Fouhey",
      "Leonidas Guibas"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=NIFTY%3A+Neural+Object+Interaction+Fields+for+Guided+Human+Motion+Synthesis+Nilesh+Kulkarni+Davis+Rempe+Kyle+Genova+Abhijit+Kundu+Justin+Johnson",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "N Kulkarni",
        "id": "tLrLu1cAAAAJ"
      },
      {
        "name": "D Rempe",
        "id": "BVde3Y0AAAAJ"
      },
      {
        "name": "K Genova",
        "id": "73HIeWcAAAAJ"
      },
      {
        "name": "A Kundu",
        "id": "GGcjtCsAAAAJ"
      },
      {
        "name": "J Johnson",
        "id": "mS5k4CYAAAAJ"
      },
      {
        "name": "D Fouhey",
        "id": "FLcpd34AAAAJ"
      },
      {
        "name": "L Guibas",
        "id": "5JlEyTAAAAAJ"
      }
    ],
    "citation_count": 76
  },
  {
    "arxiv_id": "2402.19479",
    "title": "Panda-70M: Captioning 70M Videos with Multiple Cross-Modality Teachers",
    "year": 2024,
    "published": "2024-02-29T18:59:50Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "The quality of the data and annotation upper-bounds the quality of a downstream model. While there exist large text corpora and image-text pairs, high-quality video-text data is much harder to collect. First of all, manual labeling is more time-consuming, as it requires an annotator to watch an entire video. Second, videos have a temporal dimension, consisting of several scenes stacked together, and showing multiple actions. Accordingly, to establish a video dataset with high-quality captions, w",
    "arxiv_url": "https://arxiv.org/abs/2402.19479v1",
    "pdf_url": "https://arxiv.org/pdf/2402.19479v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2402.19479",
    "arxiv_authors": [
      "Tsai-Shien Chen",
      "Aliaksandr Siarohin",
      "Willi Menapace",
      "Ekaterina Deyneka",
      "Hsiang-wei Chao",
      "Byung Eun Jeon",
      "Yuwei Fang",
      "Hsin-Ying Lee",
      "Jian Ren",
      "Ming-Hsuan Yang",
      "Sergey Tulyakov"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Panda-70M%3A+Captioning+70M+Videos+with+Multiple+Cross-Modality+Teachers+Tsai-Shien+Chen+Aliaksandr+Siarohin+Willi+Menapace+Ekaterina+Deyneka+Hsiang-wei+Chao",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "TS Chen",
        "id": "jRsSebwAAAAJ"
      },
      {
        "name": "A Siarohin",
        "id": "uMl5-k4AAAAJ"
      },
      {
        "name": "W Menapace",
        "id": "31ha1LgAAAAJ"
      },
      {
        "name": "E Deyneka",
        "id": "pXhjMcoAAAAJ"
      },
      {
        "name": "H Chao",
        "id": null
      },
      {
        "name": "BE Jeon",
        "id": null
      },
      {
        "name": "Y Fang",
        "id": "Om_-hHsAAAAJ"
      },
      {
        "name": "HY Lee",
        "id": "SeozinYAAAAJ"
      },
      {
        "name": "J Ren",
        "id": "vDALiU4AAAAJ"
      }
    ],
    "citation_count": 329
  },
  {
    "arxiv_id": "2410.07124",
    "title": "Cross-Task Pretraining for Cross-Organ Cross-Scanner Adenocarcinoma Segmentation",
    "year": 2024,
    "published": "2024-09-20T20:52:20Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "This short abstract describes a solution to the COSAS 2024 competition on Cross-Organ and Cross-Scanner Adenocarcinoma Segmentation from histopathological image patches. The main challenge in the task of segmenting this type of cancer is a noticeable domain shift encountered when changing acquisition devices (microscopes) and also when tissue comes from different organs. The two tasks proposed in COSAS were to train on a dataset of images from three different organs, and then predict segmentatio",
    "arxiv_url": "https://arxiv.org/abs/2410.07124v1",
    "pdf_url": "https://arxiv.org/pdf/2410.07124v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2410.07124",
    "arxiv_authors": [
      "Adrian Galdran"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Cross-Task+Pretraining+for+Cross-Organ+Cross-Scanner+Adenocarcinoma+Segmentation+Adrian+Galdran",
    "gs_search_success": false,
    "gs_authors": null,
    "citation_count": null,
    "error_type": "fetch_failed"
  },
  {
    "arxiv_id": "2405.08487",
    "title": "Semantic Contextualization of Face Forgery: A New Definition, Dataset, and Detection Method",
    "year": 2024,
    "published": "2024-05-14T10:24:19Z",
    "categories": [
      "cs.CV",
      "cs.CR"
    ],
    "abstract": "In recent years, deep learning has greatly streamlined the process of manipulating photographic face images. Aware of the potential dangers, researchers have developed various tools to spot these counterfeits. Yet, none asks the fundamental question: What digital manipulations make a real photographic face image fake, while others do not? In this paper, we put face forgery in a semantic context and define that computational methods that alter semantic face attributes to exceed human discriminati",
    "arxiv_url": "https://arxiv.org/abs/2405.08487v3",
    "pdf_url": "https://arxiv.org/pdf/2405.08487v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2405.08487",
    "arxiv_authors": [
      "Mian Zou",
      "Baosheng Yu",
      "Yibing Zhan",
      "Siwei Lyu",
      "Kede Ma"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Semantic+Contextualization+of+Face+Forgery%3A+A+New+Definition%2C+Dataset%2C+and+Detection+Method+Mian+Zou+Baosheng+Yu+Yibing+Zhan+Siwei+Lyu+Kede+Ma",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "M Zou",
        "id": "zkE70XAAAAAJ"
      },
      {
        "name": "B Yu",
        "id": "fjzIdMQAAAAJ"
      },
      {
        "name": "Y Zhan",
        "id": "rjd977cAAAAJ"
      },
      {
        "name": "S Lyu",
        "id": "wefAEM4AAAAJ"
      },
      {
        "name": "K Ma - IEEE Transactions on",
        "id": null
      }
    ],
    "citation_count": 6
  },
  {
    "arxiv_id": "2404.03618",
    "title": "DeViDe: Faceted medical knowledge for improved medical vision-language pre-training",
    "year": 2024,
    "published": "2024-04-04T17:40:06Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Vision-language pre-training for chest X-rays has made significant strides, primarily by utilizing paired radiographs and radiology reports. However, existing approaches often face challenges in encoding medical knowledge effectively. While radiology reports provide insights into the current disease manifestation, medical definitions (as used by contemporary methods) tend to be overly abstract, creating a gap in knowledge. To address this, we propose DeViDe, a novel transformer-based method that",
    "arxiv_url": "https://arxiv.org/abs/2404.03618v1",
    "pdf_url": "https://arxiv.org/pdf/2404.03618v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2404.03618",
    "arxiv_authors": [
      "Haozhe Luo",
      "Ziyu Zhou",
      "Corentin Royer",
      "Anjany Sekuboyina",
      "Bjoern Menze"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=DeViDe%3A+Faceted+medical+knowledge+for+improved+medical+vision-language+pre-training+Haozhe+Luo+Ziyu+Zhou+Corentin+Royer+Anjany+Sekuboyina+Bjoern+Menze",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "H Luo",
        "id": "0rlCz-cAAAAJ"
      },
      {
        "name": "Z Zhou",
        "id": "nvAfKnsAAAAJ"
      },
      {
        "name": "C Royer",
        "id": "f0QrWSgAAAAJ"
      },
      {
        "name": "A Sekuboyina",
        "id": "_xbYusYAAAAJ"
      },
      {
        "name": "B Menze",
        "id": "Kv2QrQgAAAAJ"
      }
    ],
    "citation_count": 19
  },
  {
    "arxiv_id": "2403.07113",
    "title": "Class Imbalance in Object Detection: An Experimental Diagnosis and Study of Mitigation Strategies",
    "year": 2024,
    "published": "2024-03-11T19:06:04Z",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "Object detection, a pivotal task in computer vision, is frequently hindered by dataset imbalances, particularly the under-explored issue of foreground-foreground class imbalance. This lack of attention to foreground-foreground class imbalance becomes even more pronounced in the context of single-stage detectors. This study introduces a benchmarking framework utilizing the YOLOv5 single-stage detector to address the problem of foreground-foreground class imbalance. We crafted a novel 10-class lon",
    "arxiv_url": "https://arxiv.org/abs/2403.07113v1",
    "pdf_url": "https://arxiv.org/pdf/2403.07113v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2403.07113",
    "arxiv_authors": [
      "Nieves Crasto"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Class+Imbalance+in+Object+Detection%3A+An+Experimental+Diagnosis+and+Study+of+Mitigation+Strategies+Nieves+Crasto",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "N Crasto -",
        "id": null
      }
    ],
    "citation_count": 18
  },
  {
    "arxiv_id": "2404.09931",
    "title": "Zero-shot detection of buildings in mobile LiDAR using Language Vision Model",
    "year": 2024,
    "published": "2024-04-15T16:56:58Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Recent advances have demonstrated that Language Vision Models (LVMs) surpass the existing State-of-the-Art (SOTA) in two-dimensional (2D) computer vision tasks, motivating attempts to apply LVMs to three-dimensional (3D) data. While LVMs are efficient and effective in addressing various downstream 2D vision tasks without training, they face significant challenges when it comes to point clouds, a representative format for representing 3D data. It is more difficult to extract features from 3D data",
    "arxiv_url": "https://arxiv.org/abs/2404.09931v1",
    "pdf_url": "https://arxiv.org/pdf/2404.09931v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2404.09931",
    "arxiv_authors": [
      "June Moh Goo",
      "Zichao Zeng",
      "Jan Boehm"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Zero-shot+detection+of+buildings+in+mobile+LiDAR+using+Language+Vision+Model+June+Moh+Goo+Zichao+Zeng+Jan+Boehm",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "JM Goo",
        "id": "kFdOqXMAAAAJ"
      },
      {
        "name": "Z Zeng",
        "id": "WPOWP6gAAAAJ"
      },
      {
        "name": "J Boehm -",
        "id": null
      }
    ],
    "citation_count": 3
  },
  {
    "arxiv_id": "2309.13240",
    "title": "NeRF-Enhanced Outpainting for Faithful Field-of-View Extrapolation",
    "year": 2023,
    "published": "2023-09-23T03:16:58Z",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "abstract": "In various applications, such as robotic navigation and remote visual assistance, expanding the field of view (FOV) of the camera proves beneficial for enhancing environmental perception. Unlike image outpainting techniques aimed solely at generating aesthetically pleasing visuals, these applications demand an extended view that faithfully represents the scene. To achieve this, we formulate a new problem of faithful FOV extrapolation that utilizes a set of pre-captured images as prior knowledge ",
    "arxiv_url": "https://arxiv.org/abs/2309.13240v1",
    "pdf_url": "https://arxiv.org/pdf/2309.13240v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2309.13240",
    "arxiv_authors": [
      "Rui Yu",
      "Jiachen Liu",
      "Zihan Zhou",
      "Sharon X. Huang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=NeRF-Enhanced+Outpainting+for+Faithful+Field-of-View+Extrapolation+Rui+Yu+Jiachen+Liu+Zihan+Zhou+Sharon+X.+Huang",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "R Yu",
        "id": "wTAyQhIAAAAJ"
      },
      {
        "name": "J Liu",
        "id": "NSeLtc0AAAAJ"
      },
      {
        "name": "Z Zhou",
        "id": "vEcgp3AAAAAJ"
      },
      {
        "name": "SX Huang -",
        "id": null
      }
    ],
    "citation_count": 2
  },
  {
    "arxiv_id": "2307.09267",
    "title": "Distilling Coarse-to-Fine Semantic Matching Knowledge for Weakly Supervised 3D Visual Grounding",
    "year": 2023,
    "published": "2023-07-18T13:49:49Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "3D visual grounding involves finding a target object in a 3D scene that corresponds to a given sentence query. Although many approaches have been proposed and achieved impressive performance, they all require dense object-sentence pair annotations in 3D point clouds, which are both time-consuming and expensive. To address the problem that fine-grained annotated data is difficult to obtain, we propose to leverage weakly supervised annotations to learn the 3D visual grounding model, i.e., only coa",
    "arxiv_url": "https://arxiv.org/abs/2307.09267v1",
    "pdf_url": "https://arxiv.org/pdf/2307.09267v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2307.09267",
    "arxiv_authors": [
      "Zehan Wang",
      "Haifeng Huang",
      "Yang Zhao",
      "Linjun Li",
      "Xize Cheng",
      "Yichen Zhu",
      "Aoxiong Yin",
      "Zhou Zhao"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Distilling+Coarse-to-Fine+Semantic+Matching+Knowledge+for+Weakly+Supervised+3D+Visual+Grounding+Zehan+Wang+Haifeng+Huang+Yang+Zhao+Linjun+Li+Xize+Cheng",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "Z Wang",
        "id": "euXK0lkAAAAJ"
      },
      {
        "name": "H Huang",
        "id": "oUm2gZUAAAAJ"
      },
      {
        "name": "Y Zhao",
        "id": "uPmTOHAAAAAJ"
      },
      {
        "name": "L Li",
        "id": "pwQ0QMgAAAAJ"
      },
      {
        "name": "X Cheng",
        "id": "7w1U0l4AAAAJ"
      },
      {
        "name": "Y Zhu",
        "id": "9K3a7T8AAAAJ"
      },
      {
        "name": "A Yin",
        "id": "Ay5aiWwAAAAJ"
      },
      {
        "name": "Z Zhao",
        "id": "IIoFY90AAAAJ"
      }
    ],
    "citation_count": 30
  },
  {
    "arxiv_id": "2505.21531",
    "title": "How Much Do Large Language Models Know about Human Motion? A Case Study in 3D Avatar Control",
    "year": 2025,
    "published": "2025-05-23T16:01:08Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.RO"
    ],
    "abstract": "We explore the human motion knowledge of Large Language Models (LLMs) through 3D avatar control. Given a motion instruction, we prompt LLMs to first generate a high-level movement plan with consecutive steps (High-level Planning), then specify body part positions in each step (Low-level Planning), which we linearly interpolate into avatar animations. Using 20 representative motion instructions that cover fundamental movements and balance body part usage, we conduct comprehensive evaluations, inc",
    "arxiv_url": "https://arxiv.org/abs/2505.21531v2",
    "pdf_url": "https://arxiv.org/pdf/2505.21531v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2505.21531",
    "arxiv_authors": [
      "Kunhang Li",
      "Jason Naradowsky",
      "Yansong Feng",
      "Yusuke Miyao"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=How+Much+Do+Large+Language+Models+Know+about+Human+Motion%3F+A+Case+Study+in+3D+Avatar+Control+Kunhang+Li+Jason+Naradowsky+Yansong+Feng+Yusuke+Miyao",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "K Li",
        "id": "hoOkhAMAAAAJ"
      },
      {
        "name": "J Naradowsky",
        "id": null
      },
      {
        "name": "Y Feng",
        "id": "67qAw_wAAAAJ"
      },
      {
        "name": "Y Miyao -",
        "id": null
      }
    ],
    "citation_count": null
  },
  {
    "arxiv_id": "2404.00132",
    "title": "FetalDiffusion: Pose-Controllable 3D Fetal MRI Synthesis with Conditional Diffusion Model",
    "year": 2024,
    "published": "2024-03-29T19:58:13Z",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "abstract": "The quality of fetal MRI is significantly affected by unpredictable and substantial fetal motion, leading to the introduction of artifacts even when fast acquisition sequences are employed. The development of 3D real-time fetal pose estimation approaches on volumetric EPI fetal MRI opens up a promising avenue for fetal motion monitoring and prediction. Challenges arise in fetal pose estimation due to limited number of real scanned fetal MR training images, hindering model generalization when the",
    "arxiv_url": "https://arxiv.org/abs/2404.00132v1",
    "pdf_url": "https://arxiv.org/pdf/2404.00132v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2404.00132",
    "arxiv_authors": [
      "Molin Zhang",
      "Polina Golland",
      "Patricia Ellen Grant",
      "Elfar Adalsteinsson"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=FetalDiffusion%3A+Pose-Controllable+3D+Fetal+MRI+Synthesis+with+Conditional+Diffusion+Model+Molin+Zhang+Polina+Golland+Patricia+Ellen+Grant+Elfar+Adalsteinsson",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "M Zhang",
        "id": "M6LlDA4AAAAJ"
      },
      {
        "name": "P Golland",
        "id": "4GpKQUIAAAAJ"
      },
      {
        "name": "PE Grant",
        "id": "W4dqZ7EAAAAJ"
      },
      {
        "name": "E Adalsteinsson",
        "id": null
      }
    ],
    "citation_count": 2
  },
  {
    "arxiv_id": "2501.07296",
    "title": "Event-based Video Person Re-identification via Cross-Modality and Temporal Collaboration",
    "year": 2025,
    "published": "2025-01-13T13:03:28Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Video-based person re-identification (ReID) has become increasingly important due to its applications in video surveillance applications. By employing events in video-based person ReID, more motion information can be provided between continuous frames to improve recognition accuracy. Previous approaches have assisted by introducing event data into the video person ReID task, but they still cannot avoid the privacy leakage problem caused by RGB images. In order to avoid privacy attacks and to tak",
    "arxiv_url": "https://arxiv.org/abs/2501.07296v1",
    "pdf_url": "https://arxiv.org/pdf/2501.07296v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2501.07296",
    "arxiv_authors": [
      "Renkai Li",
      "Xin Yuan",
      "Wei Liu",
      "Xin Xu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Event-based+Video+Person+Re-identification+via+Cross-Modality+and+Temporal+Collaboration+Renkai+Li+Xin+Yuan+Wei+Liu+Xin+Xu",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "R Li",
        "id": null
      },
      {
        "name": "X Yuan",
        "id": "Cp5JZsoAAAAJ"
      },
      {
        "name": "W Liu",
        "id": "EibZ-p8AAAAJ"
      },
      {
        "name": "X Xu -",
        "id": null
      }
    ],
    "citation_count": 3
  },
  {
    "arxiv_id": "2307.08417",
    "title": "Divide&Classify: Fine-Grained Classification for City-Wide Visual Place Recognition",
    "year": 2023,
    "published": "2023-07-17T11:57:04Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Visual Place recognition is commonly addressed as an image retrieval problem. However, retrieval methods are impractical to scale to large datasets, densely sampled from city-wide maps, since their dimension impact negatively on the inference time. Using approximate nearest neighbour search for retrieval helps to mitigate this issue, at the cost of a performance drop. In this paper we investigate whether we can effectively approach this task as a classification problem, thus bypassing the need f",
    "arxiv_url": "https://arxiv.org/abs/2307.08417v2",
    "pdf_url": "https://arxiv.org/pdf/2307.08417v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2307.08417",
    "arxiv_authors": [
      "Gabriele Trivigno",
      "Gabriele Berton",
      "Juan Aragon",
      "Barbara Caputo",
      "Carlo Masone"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Divide%26Classify%3A+Fine-Grained+Classification+for+City-Wide+Visual+Place+Recognition+Gabriele+Trivigno+Gabriele+Berton+Juan+Aragon+Barbara+Caputo+Carlo+Masone",
    "gs_search_success": false,
    "gs_authors": null,
    "citation_count": null,
    "error_type": "fetch_failed"
  },
  {
    "arxiv_id": "2306.15515",
    "title": "Abdominal organ segmentation via deep diffeomorphic mesh deformations",
    "year": 2023,
    "published": "2023-06-27T14:41:18Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Abdominal organ segmentation from CT and MRI is an essential prerequisite for surgical planning and computer-aided navigation systems. It is challenging due to the high variability in the shape, size, and position of abdominal organs. Three-dimensional numeric representations of abdominal shapes with point-wise correspondence to a template are further important for quantitative and statistical analyses thereof. Recently, template-based surface extraction methods have shown promising advances for",
    "arxiv_url": "https://arxiv.org/abs/2306.15515v2",
    "pdf_url": "https://arxiv.org/pdf/2306.15515v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2306.15515",
    "arxiv_authors": [
      "Fabian Bongratz",
      "Anne-Marie Rickmann",
      "Christian Wachinger"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Abdominal+organ+segmentation+via+deep+diffeomorphic+mesh+deformations+Fabian+Bongratz+Anne-Marie+Rickmann+Christian+Wachinger",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "F Bongratz",
        "id": "aQzZhi8AAAAJ"
      },
      {
        "name": "AM Rickmann",
        "id": "NDf-2OMAAAAJ"
      },
      {
        "name": "C Wachinger - Scientific reports",
        "id": null
      }
    ],
    "citation_count": 16
  },
  {
    "arxiv_id": "2407.01894",
    "title": "Adaptive Modality Balanced Online Knowledge Distillation for Brain-Eye-Computer based Dim Object Detection",
    "year": 2024,
    "published": "2024-07-02T02:30:23Z",
    "categories": [
      "cs.CV",
      "cs.HC"
    ],
    "abstract": "Advanced cognition can be extracted from the human brain using brain-computer interfaces. Integrating these interfaces with computer vision techniques, which possess efficient feature extraction capabilities, can achieve more robust and accurate detection of dim targets in aerial images. However, existing target detection methods primarily concentrate on homogeneous data, lacking efficient and versatile processing capabilities for heterogeneous multimodal data. In this paper, we first build a br",
    "arxiv_url": "https://arxiv.org/abs/2407.01894v3",
    "pdf_url": "https://arxiv.org/pdf/2407.01894v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2407.01894",
    "arxiv_authors": [
      "Zixing Li",
      "Chao Yan",
      "Zhen Lan",
      "Xiaojia Xiang",
      "Han Zhou",
      "Jun Lai",
      "Dengqing Tang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Adaptive+Modality+Balanced+Online+Knowledge+Distillation+for+Brain-Eye-Computer+based+Dim+Object+Detection+Zixing+Li+Chao+Yan+Zhen+Lan+Xiaojia+Xiang+Han+Zhou",
    "gs_search_success": false,
    "gs_authors": null,
    "citation_count": null,
    "error_type": "fetch_failed"
  },
  {
    "arxiv_id": "2303.03598",
    "title": "Guided Image-to-Image Translation by Discriminator-Generator Communication",
    "year": 2023,
    "published": "2023-03-07T02:29:36Z",
    "categories": [
      "cs.CV",
      "eess.IV"
    ],
    "abstract": "The goal of Image-to-image (I2I) translation is to transfer an image from a source domain to a target domain, which has recently drawn increasing attention. One major branch of this research is to formulate I2I translation based on Generative Adversarial Network (GAN). As a zero-sum game, GAN can be reformulated as a Partially-observed Markov Decision Process (POMDP) for generators, where generators cannot access full state information of their environments. This formulation illustrates the info",
    "arxiv_url": "https://arxiv.org/abs/2303.03598v1",
    "pdf_url": "https://arxiv.org/pdf/2303.03598v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2303.03598",
    "arxiv_authors": [
      "Yuanjiang Cao",
      "Lina Yao",
      "Le Pan",
      "Quan Z. Sheng",
      "Xiaojun Chang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Guided+Image-to-Image+Translation+by+Discriminator-Generator+Communication+Yuanjiang+Cao+Lina+Yao+Le+Pan+Quan+Z.+Sheng+Xiaojun+Chang",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "Y Cao",
        "id": "K5w-48IAAAAJ"
      },
      {
        "name": "L Yao",
        "id": "EU3snBgAAAAJ"
      },
      {
        "name": "L Pan",
        "id": "1l6NH9MAAAAJ"
      },
      {
        "name": "QZ Sheng",
        "id": "lwy2C5YAAAAJ"
      },
      {
        "name": "X ChangIEEE Transactions on Multimedia",
        "id": null
      }
    ],
    "citation_count": 6
  },
  {
    "arxiv_id": "2303.04315",
    "title": "A Threefold Review on Deep Semantic Segmentation: Efficiency-oriented, Temporal and Depth-aware design",
    "year": 2023,
    "published": "2023-03-08T01:29:55Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Semantic image and video segmentation stand among the most important tasks in computer vision nowadays, since they provide a complete and meaningful representation of the environment by means of a dense classification of the pixels in a given scene. Recently, Deep Learning, and more precisely Convolutional Neural Networks, have boosted semantic segmentation to a new level in terms of performance and generalization capabilities. However, designing Deep Semantic Segmentation models is a complex ta",
    "arxiv_url": "https://arxiv.org/abs/2303.04315v1",
    "pdf_url": "https://arxiv.org/pdf/2303.04315v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2303.04315",
    "arxiv_authors": [
      "Felipe Manfio Barbosa",
      "Fernando Santos Os√≥rio"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=A+Threefold+Review+on+Deep+Semantic+Segmentation%3A+Efficiency-oriented%2C+Temporal+and+Depth-aware+design+Felipe+Manfio+Barbosa+Fernando+Santos+Os%C3%B3rio",
    "gs_search_success": false,
    "gs_authors": null,
    "citation_count": null,
    "error_type": "multiple_matches"
  },
  {
    "arxiv_id": "2305.13623",
    "title": "Validating Multimedia Content Moderation Software via Semantic Fusion",
    "year": 2023,
    "published": "2023-05-23T02:44:15Z",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.CL",
      "cs.CV",
      "cs.MM"
    ],
    "abstract": "The exponential growth of social media platforms, such as Facebook and TikTok, has revolutionized communication and content publication in human society. Users on these platforms can publish multimedia content that delivers information via the combination of text, audio, images, and video. Meanwhile, the multimedia content release facility has been increasingly exploited to propagate toxic content, such as hate speech, malicious advertisements, and pornography. To this end, content moderation so",
    "arxiv_url": "https://arxiv.org/abs/2305.13623v1",
    "pdf_url": "https://arxiv.org/pdf/2305.13623v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2305.13623",
    "arxiv_authors": [
      "Wenxuan Wang",
      "Jingyuan Huang",
      "Chang Chen",
      "Jiazhen Gu",
      "Jianping Zhang",
      "Weibin Wu",
      "Pinjia He",
      "Michael Lyu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Validating+Multimedia+Content+Moderation+Software+via+Semantic+Fusion+Wenxuan+Wang+Jingyuan+Huang+Chang+Chen+Jiazhen+Gu+Jianping+Zhang",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "W Wang",
        "id": "4v5x0bUAAAAJ"
      },
      {
        "name": "J Huang",
        "id": "MfoZS_AAAAAJ"
      },
      {
        "name": "C Chen",
        "id": "h8C-_8sAAAAJ"
      },
      {
        "name": "J Gu",
        "id": "SO_MCpIAAAAJ"
      },
      {
        "name": "J Zhang",
        "id": "2o7L9GwAAAAJ"
      },
      {
        "name": "W Wu",
        "id": "6mtEjCEAAAAJ"
      },
      {
        "name": "P He",
        "id": "vg0moI0AAAAJ"
      },
      {
        "name": "M Lyu",
        "id": "uQnBgK0AAAAJ"
      }
    ],
    "citation_count": 15
  },
  {
    "arxiv_id": "2502.00700",
    "title": "S2CFormer: Revisiting the RD-Latency Trade-off in Transformer-based Learned Image Compression",
    "year": 2025,
    "published": "2025-02-02T07:15:51Z",
    "categories": [
      "cs.CV",
      "eess.IV"
    ],
    "abstract": "Transformer-based Learned Image Compression (LIC) suffers from a suboptimal trade-off between decoding latency and rate-distortion (R-D) performance. Moreover, the critical role of the FeedForward Network (FFN)-based channel aggregation module has been largely overlooked. Our research reveals that efficient channel aggregation-rather than complex and time-consuming spatial operations-is the key to achieving competitive LIC models. Based on this insight, we initiate the ``S2CFormer'' paradigm, a ",
    "arxiv_url": "https://arxiv.org/abs/2502.00700v3",
    "pdf_url": "https://arxiv.org/pdf/2502.00700v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2502.00700",
    "arxiv_authors": [
      "Yunuo Chen",
      "Qian Li",
      "Bing He",
      "Donghui Feng",
      "Ronghua Wu",
      "Qi Wang",
      "Li Song",
      "Guo Lu",
      "Wenjun Zhang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=S2CFormer%3A+Revisiting+the+RD-Latency+Trade-off+in+Transformer-based+Learned+Image+Compression+Yunuo+Chen+Qian+Li+Bing+He+Donghui+Feng+Ronghua+Wu",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "Y Chen",
        "id": null
      },
      {
        "name": "Q Li",
        "id": null
      },
      {
        "name": "B He",
        "id": null
      },
      {
        "name": "D Feng",
        "id": null
      },
      {
        "name": "R Wu",
        "id": null
      },
      {
        "name": "Q Wang",
        "id": null
      },
      {
        "name": "L Song",
        "id": "jKIoTVoAAAAJ"
      },
      {
        "name": "G Lu",
        "id": "R9iwlJcAAAAJ"
      },
      {
        "name": "W Zhang",
        "id": null
      }
    ],
    "citation_count": null
  },
  {
    "arxiv_id": "2304.05065",
    "title": "Artificial intelligence based prediction on lung cancer risk factors using deep learning",
    "year": 2023,
    "published": "2023-04-11T08:57:15Z",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV",
      "q-bio.SC"
    ],
    "abstract": "In this proposed work, we identified the significant research issues on lung cancer risk factors. Capturing and defining symptoms at an early stage is one of the most difficult phases for patients. Based on the history of patients records, we reviewed a number of current research studies on lung cancer and its various stages. We identified that lung cancer is one of the significant research issues in predicting the early stages of cancer disease. This research aimed to develop a model that can d",
    "arxiv_url": "https://arxiv.org/abs/2304.05065v1",
    "pdf_url": "https://arxiv.org/pdf/2304.05065v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2304.05065",
    "arxiv_authors": [
      "Muhammad Sohaib",
      "Mary Adewunmi"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Artificial+intelligence+based+prediction+on+lung+cancer+risk+factors+using+deep+learning+Muhammad+Sohaib+Mary+Adewunmi",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "M Sohaib",
        "id": "0EeIox4AAAAJ"
      },
      {
        "name": "M Adewunmi -",
        "id": null
      }
    ],
    "citation_count": 5
  },
  {
    "arxiv_id": "2504.01024",
    "title": "Gaze-Guided 3D Hand Motion Prediction for Detecting Intent in Egocentric Grasping Tasks",
    "year": 2025,
    "published": "2025-03-27T15:26:41Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.RO"
    ],
    "abstract": "Human intention detection with hand motion prediction is critical to drive the upper-extremity assistive robots in neurorehabilitation applications. However, the traditional methods relying on physiological signal measurement are restrictive and often lack environmental context. We propose a novel approach that predicts future sequences of both hand poses and joint positions. This method integrates gaze information, historical hand motion sequences, and environmental object data, adapting dynami",
    "arxiv_url": "https://arxiv.org/abs/2504.01024v1",
    "pdf_url": "https://arxiv.org/pdf/2504.01024v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2504.01024",
    "arxiv_authors": [
      "Yufei He",
      "Xucong Zhang",
      "Arno H. A. Stienen"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Gaze-Guided+3D+Hand+Motion+Prediction+for+Detecting+Intent+in+Egocentric+Grasping+Tasks+Yufei+He+Xucong+Zhang+Arno+H.+A.+Stienen",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "Y He",
        "id": "pg1VHoQAAAAJ"
      },
      {
        "name": "X Zhang",
        "id": "lDfmDk4AAAAJ"
      },
      {
        "name": "AHA Stienen -",
        "id": null
      }
    ],
    "citation_count": null
  },
  {
    "arxiv_id": "2302.08058",
    "title": "Learning Non-Local Spatial-Angular Correlation for Light Field Image Super-Resolution",
    "year": 2023,
    "published": "2023-02-16T03:40:40Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Exploiting spatial-angular correlation is crucial to light field (LF) image super-resolution (SR), but is highly challenging due to its non-local property caused by the disparities among LF images. Although many deep neural networks (DNNs) have been developed for LF image SR and achieved continuously improved performance, existing methods cannot well leverage the long-range spatial-angular correlation and thus suffer a significant performance drop when handling scenes with large disparity variat",
    "arxiv_url": "https://arxiv.org/abs/2302.08058v3",
    "pdf_url": "https://arxiv.org/pdf/2302.08058v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2302.08058",
    "arxiv_authors": [
      "Zhengyu Liang",
      "Yingqian Wang",
      "Longguang Wang",
      "Jungang Yang",
      "Shilin Zhou",
      "Yulan Guo"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Learning+Non-Local+Spatial-Angular+Correlation+for+Light+Field+Image+Super-Resolution+Zhengyu+Liang+Yingqian+Wang+Longguang+Wang+Jungang+Yang+Shilin+Zhou",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "Z Liang",
        "id": "-Kvo988AAAAJ"
      },
      {
        "name": "Y Wang",
        "id": "tBA4alMAAAAJ"
      },
      {
        "name": "L Wang",
        "id": "gbBAujsAAAAJ"
      },
      {
        "name": "J Yang",
        "id": null
      },
      {
        "name": "S Zhou",
        "id": null
      },
      {
        "name": "Y Guo",
        "id": "WQRNvdsAAAAJ"
      }
    ],
    "citation_count": 92
  },
  {
    "arxiv_id": "2311.01091",
    "title": "Enriching Phrases with Coupled Pixel and Object Contexts for Panoptic Narrative Grounding",
    "year": 2023,
    "published": "2023-11-02T08:55:28Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Panoptic narrative grounding (PNG) aims to segment things and stuff objects in an image described by noun phrases of a narrative caption. As a multimodal task, an essential aspect of PNG is the visual-linguistic interaction between image and caption. The previous two-stage method aggregates visual contexts from offline-generated mask proposals to phrase features, which tend to be noisy and fragmentary. The recent one-stage method aggregates only pixel contexts from image features to phrase featu",
    "arxiv_url": "https://arxiv.org/abs/2311.01091v2",
    "pdf_url": "https://arxiv.org/pdf/2311.01091v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2311.01091",
    "arxiv_authors": [
      "Tianrui Hui",
      "Zihan Ding",
      "Junshi Huang",
      "Xiaoming Wei",
      "Xiaolin Wei",
      "Jiao Dai",
      "Jizhong Han",
      "Si Liu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Enriching+Phrases+with+Coupled+Pixel+and+Object+Contexts+for+Panoptic+Narrative+Grounding+Tianrui+Hui+Zihan+Ding+Junshi+Huang+Xiaoming+Wei+Xiaolin+Wei",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "T Hui",
        "id": "ArjkrTkAAAAJ"
      },
      {
        "name": "Z Ding",
        "id": "czvpD10AAAAJ"
      },
      {
        "name": "J Huang",
        "id": "FFB6lzQAAAAJ"
      },
      {
        "name": "X Wei",
        "id": "s5b7lU4AAAAJ"
      },
      {
        "name": "X Wei",
        "id": "s5b7lU4AAAAJ"
      },
      {
        "name": "J Dai",
        "id": null
      },
      {
        "name": "J Han",
        "id": "0b_BPiMAAAAJ"
      },
      {
        "name": "S Liu",
        "id": null
      }
    ],
    "citation_count": 6
  },
  {
    "arxiv_id": "2302.02255",
    "title": "Human-Imperceptible Identification with Learnable Lensless Imaging",
    "year": 2023,
    "published": "2023-02-04T22:58:46Z",
    "categories": [
      "cs.CV",
      "eess.IV"
    ],
    "abstract": "Lensless imaging protects visual privacy by capturing heavily blurred images that are imperceptible for humans to recognize the subject but contain enough information for machines to infer information. Unfortunately, protecting visual privacy comes with a reduction in recognition accuracy and vice versa. We propose a learnable lensless imaging framework that protects visual privacy while maintaining recognition accuracy. To make captured images imperceptible to humans, we designed several loss f",
    "arxiv_url": "https://arxiv.org/abs/2302.02255v1",
    "pdf_url": "https://arxiv.org/pdf/2302.02255v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2302.02255",
    "arxiv_authors": [
      "Thuong Nguyen Canh",
      "Trung Thanh Ngo",
      "Hajime Nagahara"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Human-Imperceptible+Identification+with+Learnable+Lensless+Imaging+Thuong+Nguyen+Canh+Trung+Thanh+Ngo+Hajime+Nagahara",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "TN Canh",
        "id": "btY_JpQAAAAJ"
      },
      {
        "name": "TT Ngo",
        "id": "i5UfXBIAAAAJ"
      },
      {
        "name": "H Nagahara - IEEE access",
        "id": null
      }
    ],
    "citation_count": 6
  },
  {
    "arxiv_id": "2412.03451",
    "title": "PlanarSplatting: Accurate Planar Surface Reconstruction in 3 Minutes",
    "year": 2024,
    "published": "2024-12-04T16:38:07Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "This paper presents PlanarSplatting, an ultra-fast and accurate surface reconstruction approach for multiview indoor images. We take the 3D planes as the main objective due to their compactness and structural expressiveness in indoor scenes, and develop an explicit optimization framework that learns to fit the expected surface of indoor scenes by splatting the 3D planes into 2.5D depth and normal maps. As our PlanarSplatting operates directly on the 3D plane primitives, it eliminates the depende",
    "arxiv_url": "https://arxiv.org/abs/2412.03451v1",
    "pdf_url": "https://arxiv.org/pdf/2412.03451v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2412.03451",
    "arxiv_authors": [
      "Bin Tan",
      "Rui Yu",
      "Yujun Shen",
      "Nan Xue"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=PlanarSplatting%3A+Accurate+Planar+Surface+Reconstruction+in+3+Minutes+Bin+Tan+Rui+Yu+Yujun+Shen+Nan+Xue",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "B Tan",
        "id": "V391L8sAAAAJ"
      },
      {
        "name": "R Yu",
        "id": "wTAyQhIAAAAJ"
      },
      {
        "name": "Y Shen",
        "id": "u76xfogAAAAJ"
      },
      {
        "name": "N Xue -",
        "id": null
      }
    ],
    "citation_count": 9
  },
  {
    "arxiv_id": "2302.10237",
    "title": "SceneHGN: Hierarchical Graph Networks for 3D Indoor Scene Generation with Fine-Grained Geometry",
    "year": 2023,
    "published": "2023-02-16T15:31:59Z",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "abstract": "3D indoor scenes are widely used in computer graphics, with applications ranging from interior design to gaming to virtual and augmented reality. They also contain rich information, including room layout, as well as furniture type, geometry, and placement. High-quality 3D indoor scenes are highly demanded while it requires expertise and is time-consuming to design high-quality 3D indoor scenes manually. Existing research only addresses partial problems: some works learn to generate room layout, ",
    "arxiv_url": "https://arxiv.org/abs/2302.10237v1",
    "pdf_url": "https://arxiv.org/pdf/2302.10237v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2302.10237",
    "arxiv_authors": [
      "Lin Gao",
      "Jia-Mu Sun",
      "Kaichun Mo",
      "Yu-Kun Lai",
      "Leonidas J. Guibas",
      "Jie Yang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=SceneHGN%3A+Hierarchical+Graph+Networks+for+3D+Indoor+Scene+Generation+with+Fine-Grained+Geometry+Lin+Gao+Jia-Mu+Sun+Kaichun+Mo+Yu-Kun+Lai+Leonidas+J.+Guibas",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "L Gao",
        "id": "_RIupXUAAAAJ"
      },
      {
        "name": "JM Sun",
        "id": "B6Gqx7EAAAAJ"
      },
      {
        "name": "K Mo",
        "id": "pL7JsOsAAAAJ"
      },
      {
        "name": "YK Lai",
        "id": "0i-Nzv0AAAAJ"
      },
      {
        "name": "LJ Guibas",
        "id": "5JlEyTAAAAAJ"
      },
      {
        "name": "J YangIEEE Transactions on Pattern Analysis and Machine Intelligence",
        "id": null
      }
    ],
    "citation_count": 68
  },
  {
    "arxiv_id": "2304.08492",
    "title": "STRAP: Structured Object Affordance Segmentation with Point Supervision",
    "year": 2023,
    "published": "2023-04-17T17:59:49Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "With significant annotation savings, point supervision has been proven effective for numerous 2D and 3D scene understanding problems. This success is primarily attributed to the structured output space; i.e., samples with high spatial affinity tend to share the same labels. Sharing this spirit, we study affordance segmentation with point supervision, wherein the setting inherits an unexplored dual affinity-spatial affinity and label affinity. By label affinity, we refer to affordance segmentatio",
    "arxiv_url": "https://arxiv.org/abs/2304.08492v1",
    "pdf_url": "https://arxiv.org/pdf/2304.08492v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2304.08492",
    "arxiv_authors": [
      "Leiyao Cui",
      "Xiaoxue Chen",
      "Hao Zhao",
      "Guyue Zhou",
      "Yixin Zhu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=STRAP%3A+Structured+Object+Affordance+Segmentation+with+Point+Supervision+Leiyao+Cui+Xiaoxue+Chen+Hao+Zhao+Guyue+Zhou+Yixin+Zhu",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "L Cui",
        "id": "PqS_VMYAAAAJ"
      },
      {
        "name": "X Chen",
        "id": "_tz64W0AAAAJ"
      },
      {
        "name": "H Zhao",
        "id": "ygQznUQAAAAJ"
      },
      {
        "name": "G Zhou",
        "id": null
      },
      {
        "name": "Y Zhu -",
        "id": null
      }
    ],
    "citation_count": 11
  },
  {
    "arxiv_id": "2308.06129",
    "title": "Uncertainty Quantification for Image-based Traffic Prediction across Cities",
    "year": 2023,
    "published": "2023-08-11T13:35:52Z",
    "categories": [
      "cs.CV",
      "cs.LG",
      "stat.ML"
    ],
    "abstract": "Despite the strong predictive performance of deep learning models for traffic prediction, their widespread deployment in real-world intelligent transportation systems has been restrained by a lack of interpretability. Uncertainty quantification (UQ) methods provide an approach to induce probabilistic reasoning, improve decision-making and enhance model deployment potential. To gain a comprehensive picture of the usefulness of existing UQ methods for traffic prediction and the relation between ob",
    "arxiv_url": "https://arxiv.org/abs/2308.06129v1",
    "pdf_url": "https://arxiv.org/pdf/2308.06129v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2308.06129",
    "arxiv_authors": [
      "Alexander Timans",
      "Nina Wiedemann",
      "Nishant Kumar",
      "Ye Hong",
      "Martin Raubal"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Uncertainty+Quantification+for+Image-based+Traffic+Prediction+across+Cities+Alexander+Timans+Nina+Wiedemann+Nishant+Kumar+Ye+Hong+Martin+Raubal",
    "gs_search_success": false,
    "gs_authors": null,
    "citation_count": null,
    "error_type": "fetch_failed"
  },
  {
    "arxiv_id": "2312.07327",
    "title": "Adaptive Confidence Multi-View Hashing for Multimedia Retrieval",
    "year": 2023,
    "published": "2023-12-12T14:43:09Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "The multi-view hash method converts heterogeneous data from multiple views into binary hash codes, which is one of the critical technologies in multimedia retrieval. However, the current methods mainly explore the complementarity among multiple views while lacking confidence learning and fusion. Moreover, in practical application scenarios, the single-view data contain redundant noise. To conduct the confidence learning and eliminate unnecessary noise, we propose a novel Adaptive Confidence Mult",
    "arxiv_url": "https://arxiv.org/abs/2312.07327v2",
    "pdf_url": "https://arxiv.org/pdf/2312.07327v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2312.07327",
    "arxiv_authors": [
      "Jian Zhu",
      "Yu Cui",
      "Zhangmin Huang",
      "Xingyu Li",
      "Lei Liu",
      "Lingfang Zeng",
      "Li-Rong Dai"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Adaptive+Confidence+Multi-View+Hashing+for+Multimedia+Retrieval+Jian+Zhu+Yu+Cui+Zhangmin+Huang+Xingyu+Li+Lei+Liu",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "J Zhu",
        "id": "zYrKCHIAAAAJ"
      },
      {
        "name": "Y Cui",
        "id": null
      },
      {
        "name": "Z Huang",
        "id": "kmAwd80AAAAJ"
      },
      {
        "name": "X Li",
        "id": null
      },
      {
        "name": "L Liu",
        "id": null
      },
      {
        "name": "L Zeng",
        "id": "74KJItoAAAAJ"
      },
      {
        "name": "LR DaiICASSP",
        "id": null
      }
    ],
    "citation_count": 14
  },
  {
    "arxiv_id": "2312.12848",
    "title": "Quantum Annealing for Computer Vision Minimization Problems",
    "year": 2023,
    "published": "2023-12-20T08:56:35Z",
    "categories": [
      "quant-ph",
      "cs.CV"
    ],
    "abstract": "Computer Vision (CV) labelling algorithms play a pivotal role in the domain of low-level vision. For decades, it has been known that these problems can be elegantly formulated as discrete energy minimization problems derived from probabilistic graphical models (such as Markov Random Fields). Despite recent advances in inference algorithms (such as graph-cut and message-passing algorithms), the resulting energy minimization problems are generally viewed as intractable. The emergence of quantum co",
    "arxiv_url": "https://arxiv.org/abs/2312.12848v1",
    "pdf_url": "https://arxiv.org/pdf/2312.12848v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2312.12848",
    "arxiv_authors": [
      "Shahrokh Heidari",
      "Michael J. Dinneen",
      "Patrice Delmas"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Quantum+Annealing+for+Computer+Vision+Minimization+Problems+Shahrokh+Heidari+Michael+J.+Dinneen+Patrice+Delmas",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "S Heidari",
        "id": "3qtK8QwAAAAJ"
      },
      {
        "name": "MJ Dinneen",
        "id": "NspX1ZIAAAAJ"
      }
    ],
    "citation_count": 8
  },
  {
    "arxiv_id": "2306.11051",
    "title": "Concavity-Induced Distance for Unoriented Point Cloud Decomposition",
    "year": 2023,
    "published": "2023-06-19T16:35:09Z",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "abstract": "We propose Concavity-induced Distance (CID) as a novel way to measure the dissimilarity between a pair of points in an unoriented point cloud. CID indicates the likelihood of two points or two sets of points belonging to different convex parts of an underlying shape represented as a point cloud. After analyzing its properties, we demonstrate how CID can benefit point cloud analysis without the need for meshing or normal estimation, which is beneficial for robotics applications when dealing with ",
    "arxiv_url": "https://arxiv.org/abs/2306.11051v1",
    "pdf_url": "https://arxiv.org/pdf/2306.11051v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2306.11051",
    "arxiv_authors": [
      "Ruoyu Wang",
      "Yanfei Xue",
      "Bharath Surianarayanan",
      "Dong Tian",
      "Chen Feng"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Concavity-Induced+Distance+for+Unoriented+Point+Cloud+Decomposition+Ruoyu+Wang+Yanfei+Xue+Bharath+Surianarayanan+Dong+Tian+Chen+Feng",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "R Wang",
        "id": null
      },
      {
        "name": "Y Xue",
        "id": "e---aawAAAAJ"
      },
      {
        "name": "B Surianarayanan",
        "id": null
      },
      {
        "name": "D Tian",
        "id": "VsGLu-QAAAAJ"
      },
      {
        "name": "C FengIEEE Robotics and Automation Letters",
        "id": null
      }
    ],
    "citation_count": null
  },
  {
    "arxiv_id": "2302.09919",
    "title": "Interactive Face Video Coding: A Generative Compression Framework",
    "year": 2023,
    "published": "2023-02-20T11:24:23Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "In this paper, we propose a novel framework for Interactive Face Video Coding (IFVC), which allows humans to interact with the intrinsic visual representations instead of the signals. The proposed solution enjoys several distinct advantages, including ultra-compact representation, low delay interaction, and vivid expression/headpose animation. In particular, we propose the Internal Dimension Increase (IDI) based representation, greatly enhancing the fidelity and flexibility in rendering the appe",
    "arxiv_url": "https://arxiv.org/abs/2302.09919v2",
    "pdf_url": "https://arxiv.org/pdf/2302.09919v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2302.09919",
    "arxiv_authors": [
      "Bolin Chen",
      "Zhao Wang",
      "Binzhe Li",
      "Shurun Wang",
      "Shiqi Wang",
      "Yan Ye"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Interactive+Face+Video+Coding%3A+A+Generative+Compression+Framework+Bolin+Chen+Zhao+Wang+Binzhe+Li+Shurun+Wang+Shiqi+Wang",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "B Chen",
        "id": "Z30kLzgAAAAJ"
      },
      {
        "name": "Z Wang",
        "id": "7JxrlB0AAAAJ"
      },
      {
        "name": "B Li",
        "id": "cdXlvmsAAAAJ"
      },
      {
        "name": "S Wang",
        "id": "Pr7s2VUAAAAJ"
      },
      {
        "name": "S Wang",
        "id": "Pr7s2VUAAAAJ"
      },
      {
        "name": "Y YeIEEE Transactions on Image Processing",
        "id": null
      }
    ],
    "citation_count": 28
  },
  {
    "arxiv_id": "2312.08822",
    "title": "Planning and Rendering: Towards Product Poster Generation with Diffusion Models",
    "year": 2023,
    "published": "2023-12-14T11:11:50Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Product poster generation significantly optimizes design efficiency and reduces production costs. Prevailing methods predominantly rely on image-inpainting methods to generate clean background images for given products. Subsequently, poster layout generation methods are employed to produce corresponding layout results. However, the background images may not be suitable for accommodating textual content due to their complexity, and the fixed location of products limits the diversity of layout res",
    "arxiv_url": "https://arxiv.org/abs/2312.08822v2",
    "pdf_url": "https://arxiv.org/pdf/2312.08822v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2312.08822",
    "arxiv_authors": [
      "Zhaochen Li",
      "Fengheng Li",
      "Wei Feng",
      "Honghe Zhu",
      "Yaoyu Li",
      "Zheng Zhang",
      "Jingjing Lv",
      "Junjie Shen",
      "Zhangang Lin",
      "Jingping Shao",
      "Zhenglu Yang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Planning+and+Rendering%3A+Towards+Product+Poster+Generation+with+Diffusion+Models+Zhaochen+Li+Fengheng+Li+Wei+Feng+Honghe+Zhu+Yaoyu+Li",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "Z Li",
        "id": null
      },
      {
        "name": "F Li",
        "id": null
      },
      {
        "name": "W Feng",
        "id": "FvIAct4AAAAJ"
      },
      {
        "name": "H Zhu",
        "id": null
      },
      {
        "name": "Y Li",
        "id": null
      },
      {
        "name": "Z Zhang",
        "id": null
      },
      {
        "name": "J Lv",
        "id": null
      },
      {
        "name": "J Shen",
        "id": null
      },
      {
        "name": "Z Lin",
        "id": null
      },
      {
        "name": "J Shao",
        "id": null
      },
      {
        "name": "Z Yang",
        "id": "u5LzNHcAAAAJ"
      }
    ],
    "citation_count": 6
  },
  {
    "arxiv_id": "2301.10241",
    "title": "K-Planes: Explicit Radiance Fields in Space, Time, and Appearance",
    "year": 2023,
    "published": "2023-01-24T18:59:08Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "We introduce k-planes, a white-box model for radiance fields in arbitrary dimensions. Our model uses d choose 2 planes to represent a d-dimensional scene, providing a seamless way to go from static (d=3) to dynamic (d=4) scenes. This planar factorization makes adding dimension-specific priors easy, e.g. temporal smoothness and multi-resolution spatial structure, and induces a natural decomposition of static and dynamic components of a scene. We use a linear feature decoder with a learned color b",
    "arxiv_url": "https://arxiv.org/abs/2301.10241v2",
    "pdf_url": "https://arxiv.org/pdf/2301.10241v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2301.10241",
    "arxiv_authors": [
      "Sara Fridovich-Keil",
      "Giacomo Meanti",
      "Frederik Warburg",
      "Benjamin Recht",
      "Angjoo Kanazawa"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=K-Planes%3A+Explicit+Radiance+Fields+in+Space%2C+Time%2C+and+Appearance+Sara+Fridovich-Keil+Giacomo+Meanti+Frederik+Warburg+Benjamin+Recht+Angjoo+Kanazawa",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "S Fridovich-Keil",
        "id": "9xF7M6wAAAAJ"
      },
      {
        "name": "G Meanti",
        "id": "g1n-MOEAAAAJ"
      },
      {
        "name": "FR Warburg",
        "id": "0Ozzy4IAAAAJ"
      },
      {
        "name": "B Recht",
        "id": "a_dbdxAAAAAJ"
      },
      {
        "name": "A Kanazawa",
        "id": "Ci-_QYIAAAAJ"
      }
    ],
    "citation_count": 808
  },
  {
    "arxiv_id": "2301.06043",
    "title": "Unsupervised Cardiac Segmentation Utilizing Synthesized Images from Anatomical Labels",
    "year": 2023,
    "published": "2023-01-15T08:28:33Z",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "abstract": "Cardiac segmentation is in great demand for clinical practice. Due to the enormous labor of manual delineation, unsupervised segmentation is desired. The ill-posed optimization problem of this task is inherently challenging, requiring well-designed constraints. In this work, we propose an unsupervised framework for multi-class segmentation with both intensity and shape constraints. Firstly, we extend a conventional non-convex energy function as an intensity constraint and implement it with U-Net",
    "arxiv_url": "https://arxiv.org/abs/2301.06043v1",
    "pdf_url": "https://arxiv.org/pdf/2301.06043v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2301.06043",
    "arxiv_authors": [
      "Sihan Wang",
      "Fuping Wu",
      "Lei Li",
      "Zheyao Gao",
      "Byung-Woo Hong",
      "Xiahai Zhuang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Unsupervised+Cardiac+Segmentation+Utilizing+Synthesized+Images+from+Anatomical+Labels+Sihan+Wang+Fuping+Wu+Lei+Li+Zheyao+Gao+Byung-Woo+Hong",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "S Wang",
        "id": null
      },
      {
        "name": "F Wu",
        "id": "iRjRlWQAAAAJ"
      },
      {
        "name": "L Li",
        "id": "--CYiuwAAAAJ"
      },
      {
        "name": "Z Gao",
        "id": "6f5ARf4AAAAJ"
      },
      {
        "name": "BW Hong",
        "id": "gYk4eAgAAAAJ"
      },
      {
        "name": "X ZhuangInternational",
        "id": null
      }
    ],
    "citation_count": 3
  },
  {
    "arxiv_id": "2502.15478",
    "title": "CondiQuant: Condition Number Based Low-Bit Quantization for Image Super-Resolution",
    "year": 2025,
    "published": "2025-02-21T14:04:30Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Low-bit model quantization for image super-resolution (SR) is a longstanding task that is renowned for its surprising compression and acceleration ability. However, accuracy degradation is inevitable when compressing the full-precision (FP) model to ultra-low bit widths (2~4 bits). Experimentally, we observe that the degradation of quantization is mainly attributed to the quantization of activation instead of model weights. In numerical analysis, the condition number of weights could measure how",
    "arxiv_url": "https://arxiv.org/abs/2502.15478v1",
    "pdf_url": "https://arxiv.org/pdf/2502.15478v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2502.15478",
    "arxiv_authors": [
      "Kai Liu",
      "Dehui Wang",
      "Zhiteng Li",
      "Zheng Chen",
      "Yong Guo",
      "Wenbo Li",
      "Linghe Kong",
      "Yulun Zhang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=CondiQuant%3A+Condition+Number+Based+Low-Bit+Quantization+for+Image+Super-Resolution+Kai+Liu+Dehui+Wang+Zhiteng+Li+Zheng+Chen+Yong+Guo",
    "gs_search_success": false,
    "gs_authors": null,
    "citation_count": null,
    "error_type": "fetch_failed"
  },
  {
    "arxiv_id": "2505.04850",
    "title": "ORXE: Orchestrating Experts for Dynamically Configurable Efficiency",
    "year": 2025,
    "published": "2025-05-07T23:16:56Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "This paper presents ORXE, a modular and adaptable framework for achieving real-time configurable efficiency in AI models. By leveraging a collection of pre-trained experts with diverse computational costs and performance levels, ORXE dynamically adjusts inference pathways based on the complexity of input samples. Unlike conventional approaches that require complex metamodel training, ORXE achieves high efficiency and flexibility without complicating the development process. The proposed system u",
    "arxiv_url": "https://arxiv.org/abs/2505.04850v1",
    "pdf_url": "https://arxiv.org/pdf/2505.04850v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2505.04850",
    "arxiv_authors": [
      "Qingyuan Wang",
      "Guoxin Wang",
      "Barry Cardiff",
      "Deepu John"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=ORXE%3A+Orchestrating+Experts+for+Dynamically+Configurable+Efficiency+Qingyuan+Wang+Guoxin+Wang+Barry+Cardiff+Deepu+John",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "Q Wang",
        "id": "-G-F-m4AAAAJ"
      },
      {
        "name": "G Wang",
        "id": "yo7t7CIAAAAJ"
      },
      {
        "name": "B Cardiff",
        "id": "QnOwZmoAAAAJ"
      },
      {
        "name": "D John -",
        "id": null
      }
    ],
    "citation_count": null
  },
  {
    "arxiv_id": "2406.17899",
    "title": "Entity Augmentation for Efficient Classification of Vertically Partitioned Data with Limited Overlap",
    "year": 2024,
    "published": "2024-06-25T19:20:10Z",
    "categories": [
      "cs.LG",
      "cs.CV",
      "cs.DC"
    ],
    "abstract": "Vertical Federated Learning (VFL) is a machine learning paradigm for learning from vertically partitioned data (i.e. features for each input are distributed across multiple \"guest\" clients and an aggregating \"host\" server owns labels) without communicating raw data. Traditionally, VFL involves an \"entity resolution\" phase where the host identifies and serializes the unique entities known to all guests. This is followed by private set intersection to find common entities, and an \"entity alignment",
    "arxiv_url": "https://arxiv.org/abs/2406.17899v1",
    "pdf_url": "https://arxiv.org/pdf/2406.17899v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2406.17899",
    "arxiv_authors": [
      "Avi Amalanshu",
      "Viswesh Nagaswamy",
      "G. V. S. S. Prudhvi",
      "Yash Sirvi",
      "Debashish Chakravarty"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Entity+Augmentation+for+Efficient+Classification+of+Vertically+Partitioned+Data+with+Limited+Overlap+Avi+Amalanshu+Viswesh+Nagaswamy+G.+V.+S.+S.+Prudhvi+Yash+Sirvi+Debashish+Chakravarty",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "A Amalanshu",
        "id": "uxaD7gUAAAAJ"
      },
      {
        "name": "V Nagaswamy",
        "id": "KmmNF30AAAAJ"
      },
      {
        "name": "G Prudhvi",
        "id": "GNbMs_8AAAAJ"
      },
      {
        "name": "Y SirviInternational Joint",
        "id": null
      }
    ],
    "citation_count": null
  },
  {
    "arxiv_id": "2312.08600",
    "title": "CartoMark: a benchmark dataset for map pattern recognition and 1 map content retrieval with machine intelligence",
    "year": 2023,
    "published": "2023-12-14T01:54:38Z",
    "categories": [
      "cs.CV",
      "cs.MM"
    ],
    "abstract": "Maps are fundamental medium to visualize and represent the real word in a simple and 16 philosophical way. The emergence of the 3rd wave information has made a proportion of maps are available to be generated ubiquitously, which would significantly enrich the dimensions and perspectives to understand the characteristics of the real world. However, a majority of map dataset have never been discovered, acquired and effectively used, and the map data used in many applications might not be completel",
    "arxiv_url": "https://arxiv.org/abs/2312.08600v1",
    "pdf_url": "https://arxiv.org/pdf/2312.08600v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2312.08600",
    "arxiv_authors": [
      "Xiran Zhou",
      "Yi Wen",
      "Honghao Li",
      "Kaiyuan Li",
      "Zhenfeng Shao",
      "Zhigang Yan",
      "Xiao Xie"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=CartoMark%3A+a+benchmark+dataset+for+map+pattern+recognition+and+1+map+content+retrieval+with+machine+intelligence+Xiran+Zhou+Yi+Wen+Honghao+Li+Kaiyuan+Li+Zhenfeng+Shao",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "X Zhou",
        "id": "dwmtDfIAAAAJ"
      },
      {
        "name": "Y Wen",
        "id": null
      },
      {
        "name": "Z Shao",
        "id": "lsz8fJoAAAAJ"
      },
      {
        "name": "W Li",
        "id": "nlBeAO4AAAAJ"
      },
      {
        "name": "K Li",
        "id": null
      },
      {
        "name": "H Li",
        "id": null
      },
      {
        "name": "X Xie",
        "id": "CF0uevcAAAAJ"
      },
      {
        "name": "Z Yan - Scientific Data",
        "id": null
      }
    ],
    "citation_count": 1
  },
  {
    "arxiv_id": "2403.13261",
    "title": "Self-Supervised Class-Agnostic Motion Prediction with Spatial and Temporal Consistency Regularizations",
    "year": 2024,
    "published": "2024-03-20T02:58:45Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "The perception of motion behavior in a dynamic environment holds significant importance for autonomous driving systems, wherein class-agnostic motion prediction methods directly predict the motion of the entire point cloud. While most existing methods rely on fully-supervised learning, the manual labeling of point cloud data is laborious and time-consuming. Therefore, several annotation-efficient methods have been proposed to address this challenge. Although effective, these methods rely on weak",
    "arxiv_url": "https://arxiv.org/abs/2403.13261v2",
    "pdf_url": "https://arxiv.org/pdf/2403.13261v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2403.13261",
    "arxiv_authors": [
      "Kewei Wang",
      "Yizheng Wu",
      "Jun Cen",
      "Zhiyu Pan",
      "Xingyi Li",
      "Zhe Wang",
      "Zhiguo Cao",
      "Guosheng Lin"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Self-Supervised+Class-Agnostic+Motion+Prediction+with+Spatial+and+Temporal+Consistency+Regularizations+Kewei+Wang+Yizheng+Wu+Jun+Cen+Zhiyu+Pan+Xingyi+Li",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "K Wang",
        "id": "fW7pUGMAAAAJ"
      },
      {
        "name": "Y Wu",
        "id": "0_iF4jMAAAAJ"
      },
      {
        "name": "J Cen",
        "id": "7SKAhBwAAAAJ"
      },
      {
        "name": "Z Pan",
        "id": "X1AP9ZEAAAAJ"
      },
      {
        "name": "X Li",
        "id": "XDKQsvUAAAAJ"
      },
      {
        "name": "Z Wang",
        "id": null
      },
      {
        "name": "Z Cao",
        "id": "396o2BAAAAAJ"
      },
      {
        "name": "G Lin",
        "id": "ZudEhvcAAAAJ"
      }
    ],
    "citation_count": 6
  },
  {
    "arxiv_id": "2307.03175",
    "title": "Push Past Green: Learning to Look Behind Plant Foliage by Moving It",
    "year": 2023,
    "published": "2023-07-06T17:55:28Z",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "Autonomous agriculture applications (e.g., inspection, phenotyping, plucking fruits) require manipulating the plant foliage to look behind the leaves and the branches. Partial visibility, extreme clutter, thin structures, and unknown geometry and dynamics for plants make such manipulation challenging. We tackle these challenges through data-driven methods. We use self-supervision to train SRPNet, a neural network that predicts what space is revealed on execution of a candidate action on a given ",
    "arxiv_url": "https://arxiv.org/abs/2307.03175v2",
    "pdf_url": "https://arxiv.org/pdf/2307.03175v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2307.03175",
    "arxiv_authors": [
      "Xiaoyu Zhang",
      "Saurabh Gupta"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Push+Past+Green%3A+Learning+to+Look+Behind+Plant+Foliage+by+Moving+It+Xiaoyu+Zhang+Saurabh+Gupta",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "X Zhang",
        "id": "jXfleiEAAAAJ"
      },
      {
        "name": "S Gupta -",
        "id": null
      }
    ],
    "citation_count": 4
  },
  {
    "arxiv_id": "2403.14235",
    "title": "RG-CAT: Detection Pipeline and Catalogue of Radio Galaxies in the EMU Pilot Survey",
    "year": 2024,
    "published": "2024-03-21T08:52:39Z",
    "categories": [
      "astro-ph.GA",
      "astro-ph.CO",
      "astro-ph.IM",
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "We present source detection and catalogue construction pipelines to build the first catalogue of radio galaxies from the 270 $\\rm deg^2$ pilot survey of the Evolutionary Map of the Universe (EMU-PS) conducted with the Australian Square Kilometre Array Pathfinder (ASKAP) telescope. The detection pipeline uses Gal-DINO computer-vision networks (Gupta et al., 2024) to predict the categories of radio morphology and bounding boxes for radio sources, as well as their potential infrared host positions.",
    "arxiv_url": "https://arxiv.org/abs/2403.14235v1",
    "pdf_url": "https://arxiv.org/pdf/2403.14235v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2403.14235",
    "arxiv_authors": [
      "Nikhel Gupta",
      "Ray P. Norris",
      "Zeeshan Hayder",
      "Minh Huynh",
      "Lars Petersson",
      "X. Rosalind Wang",
      "Andrew M. Hopkins",
      "Heinz Andernach",
      "Yjan Gordon",
      "Simone Riggi",
      "Miranda Yew",
      "Evan J. Crawford",
      "B√§rbel Koribalski",
      "Miroslav D. Filipoviƒá",
      "Anna D. Kapin≈õka",
      "Stanislav Shabala",
      "Tessa Vernstrom",
      "Joshua R. Marvil"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=RG-CAT%3A+Detection+Pipeline+and+Catalogue+of+Radio+Galaxies+in+the+EMU+Pilot+Survey+Nikhel+Gupta+Ray+P.+Norris+Zeeshan+Hayder+Minh+Huynh+Lars+Petersson",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "N Gupta",
        "id": "bWAN13QAAAAJ"
      },
      {
        "name": "RP Norris",
        "id": "4iJ_JWwAAAAJ"
      },
      {
        "name": "Z Hayder",
        "id": "K2INPyYAAAAJ"
      },
      {
        "name": "M Huynh",
        "id": null
      },
      {
        "name": "L Petersson",
        "id": "32RHN4oAAAAJ"
      },
      {
        "name": "XR Wang",
        "id": "8K-JTZIAAAAJ"
      },
      {
        "name": "AM Hopkins",
        "id": "Tzr6Z1AAAAAJ"
      },
      {
        "name": "H Andernach",
        "id": null
      }
    ],
    "citation_count": 17
  },
  {
    "arxiv_id": "2405.19957",
    "title": "PLA4D: Pixel-Level Alignments for Text-to-4D Gaussian Splatting",
    "year": 2024,
    "published": "2024-05-30T11:23:01Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Previous text-to-4D methods have leveraged multiple Score Distillation Sampling (SDS) techniques, combining motion priors from video-based diffusion models (DMs) with geometric priors from multiview DMs to implicitly guide 4D renderings. However, differences in these priors result in conflicting gradient directions during optimization, causing trade-offs between motion fidelity and geometry accuracy, and requiring substantial optimization time to reconcile the models. In this paper, we introduce",
    "arxiv_url": "https://arxiv.org/abs/2405.19957v4",
    "pdf_url": "https://arxiv.org/pdf/2405.19957v4",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2405.19957",
    "arxiv_authors": [
      "Qiaowei Miao",
      "JinSheng Quan",
      "Kehan Li",
      "Yawei Luo"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=PLA4D%3A+Pixel-Level+Alignments+for+Text-to-4D+Gaussian+Splatting+Qiaowei+Miao+JinSheng+Quan+Kehan+Li+Yawei+Luo",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "Q Miao",
        "id": "BBzxAl4AAAAJ"
      },
      {
        "name": "J Quan",
        "id": "hRarFkEAAAAJ"
      },
      {
        "name": "K Li",
        "id": "ATru1qQAAAAJ"
      },
      {
        "name": "Y Luo -",
        "id": null
      }
    ],
    "citation_count": 14
  },
  {
    "arxiv_id": "2312.02366",
    "title": "Evaluating General Purpose Vision Foundation Models for Medical Image Analysis: An Experimental Study of DINOv2 on Radiology Benchmarks",
    "year": 2023,
    "published": "2023-12-04T21:47:10Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "The integration of deep learning systems into healthcare has been hindered by the resource-intensive process of data annotation and the inability of these systems to generalize to different data distributions. Foundation models, which are models pre-trained on large datasets, have emerged as a solution to reduce reliance on annotated data and enhance model generalizability and robustness. DINOv2 is an open-source foundation model pre-trained with self-supervised learning on 142 million curated n",
    "arxiv_url": "https://arxiv.org/abs/2312.02366v4",
    "pdf_url": "https://arxiv.org/pdf/2312.02366v4",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2312.02366",
    "arxiv_authors": [
      "Mohammed Baharoon",
      "Waseem Qureshi",
      "Jiahong Ouyang",
      "Yanwu Xu",
      "Abdulrhman Aljouie",
      "Wei Peng"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Evaluating+General+Purpose+Vision+Foundation+Models+for+Medical+Image+Analysis%3A+An+Experimental+Study+of+DINOv2+on+Radiology+Benchmarks+Mohammed+Baharoon+Waseem+Qureshi+Jiahong+Ouyang+Yanwu+Xu+Abdulrhman+Aljouie",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "M Baharoon",
        "id": "U-LgNOwAAAAJ"
      },
      {
        "name": "W Qureshi",
        "id": null
      },
      {
        "name": "J Ouyang",
        "id": "B_D7cCAAAAAJ"
      },
      {
        "name": "Y Xu",
        "id": "NOEyacoAAAAJ"
      },
      {
        "name": "A Aljouie",
        "id": "w8nyAIAAAAAJ"
      },
      {
        "name": "W Peng",
        "id": null
      }
    ],
    "citation_count": 19
  },
  {
    "arxiv_id": "2312.02437",
    "title": "GDN: A Stacking Network Used for Skin Cancer Diagnosis",
    "year": 2023,
    "published": "2023-12-05T02:33:55Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Skin cancer, the primary type of cancer that can be identified by visual recognition, requires an automatic identification system that can accurately classify different types of lesions. This paper presents GoogLe-Dense Network (GDN), which is an image-classification model to identify two types of skin cancer, Basal Cell Carcinoma, and Melanoma. GDN uses stacking of different networks to enhance the model performance. Specifically, GDN consists of two sequential levels in its structure. The firs",
    "arxiv_url": "https://arxiv.org/abs/2312.02437v1",
    "pdf_url": "https://arxiv.org/pdf/2312.02437v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2312.02437",
    "arxiv_authors": [
      "Jingmin Wei",
      "Haoyang Shen",
      "Ziyi Wang",
      "Ziqian Zhang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=GDN%3A+A+Stacking+Network+Used+for+Skin+Cancer+Diagnosis+Jingmin+Wei+Haoyang+Shen+Ziyi+Wang+Ziqian+Zhang",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "J Wei",
        "id": null
      },
      {
        "name": "H Shen",
        "id": null
      },
      {
        "name": "Z Wang",
        "id": null
      },
      {
        "name": "Z Zhang -",
        "id": null
      }
    ],
    "citation_count": null
  },
  {
    "arxiv_id": "2412.13749",
    "title": "Multi-Exposure Image Fusion via Distilled 3D LUT Grid with Editable Mode",
    "year": 2024,
    "published": "2024-12-18T11:33:16Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "With the rising imaging resolution of handheld devices, existing multi-exposure image fusion algorithms struggle to generate a high dynamic range image with ultra-high resolution in real-time. Apart from that, there is a trend to design a manageable and editable algorithm as the different needs of real application scenarios. To tackle these issues, we introduce 3D LUT technology, which can enhance images with ultra-high-definition (UHD) resolution in real time on resource-constrained devices. Ho",
    "arxiv_url": "https://arxiv.org/abs/2412.13749v1",
    "pdf_url": "https://arxiv.org/pdf/2412.13749v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2412.13749",
    "arxiv_authors": [
      "Xin Su",
      "Zhuoran Zheng"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Multi-Exposure+Image+Fusion+via+Distilled+3D+LUT+Grid+with+Editable+Mode+Xin+Su+Zhuoran+Zheng",
    "gs_search_success": false,
    "gs_authors": null,
    "citation_count": null,
    "error_type": "multiple_matches"
  },
  {
    "arxiv_id": "2304.10066",
    "title": "Recognizability Embedding Enhancement for Very Low-Resolution Face Recognition and Quality Estimation",
    "year": 2023,
    "published": "2023-04-20T03:18:03Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Very low-resolution face recognition (VLRFR) poses unique challenges, such as tiny regions of interest and poor resolution due to extreme standoff distance or wide viewing angle of the acquisition devices. In this paper, we study principled approaches to elevate the recognizability of a face in the embedding space instead of the visual quality. We first formulate a robust learning-based face recognizability measure, namely recognizability index (RI), based on two criteria: (i) proximity of each ",
    "arxiv_url": "https://arxiv.org/abs/2304.10066v1",
    "pdf_url": "https://arxiv.org/pdf/2304.10066v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2304.10066",
    "arxiv_authors": [
      "Jacky Chen Long Chai",
      "Tiong-Sik Ng",
      "Cheng-Yaw Low",
      "Jaewoo Park",
      "Andrew Beng Jin Teoh"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Recognizability+Embedding+Enhancement+for+Very+Low-Resolution+Face+Recognition+and+Quality+Estimation+Jacky+Chen+Long+Chai+Tiong-Sik+Ng+Cheng-Yaw+Low+Jaewoo+Park+Andrew+Beng+Jin+Teoh",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "JCL Chai",
        "id": null
      },
      {
        "name": "TS Ng",
        "id": "oT2_EAEAAAAJ"
      },
      {
        "name": "CY Low",
        "id": "NgRN_6kAAAAJ"
      },
      {
        "name": "J Park",
        "id": "LpIDsOwAAAAJ"
      },
      {
        "name": "ABJ Teoh",
        "id": "ueRkvQMAAAAJ"
      }
    ],
    "citation_count": 40
  },
  {
    "arxiv_id": "2311.05230",
    "title": "ConRad: Image Constrained Radiance Fields for 3D Generation from a Single Image",
    "year": 2023,
    "published": "2023-11-09T09:17:10Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "We present a novel method for reconstructing 3D objects from a single RGB image. Our method leverages the latest image generation models to infer the hidden 3D structure while remaining faithful to the input image. While existing methods obtain impressive results in generating 3D models from text prompts, they do not provide an easy approach for conditioning on input RGB data. Na√Øve extensions of these methods often lead to improper alignment in appearance between the input image and the 3D reco",
    "arxiv_url": "https://arxiv.org/abs/2311.05230v1",
    "pdf_url": "https://arxiv.org/pdf/2311.05230v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2311.05230",
    "arxiv_authors": [
      "Senthil Purushwalkam",
      "Nikhil Naik"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=ConRad%3A+Image+Constrained+Radiance+Fields+for+3D+Generation+from+a+Single+Image+Senthil+Purushwalkam+Nikhil+Naik",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "S Purushwalkam",
        "id": "T3Tt0S8AAAAJ"
      },
      {
        "name": "N Naik - Advances in Neural Information",
        "id": null
      }
    ],
    "citation_count": 6
  },
  {
    "arxiv_id": "2502.02029",
    "title": "MORPH-LER: Log-Euclidean Regularization for Population-Aware Image Registration",
    "year": 2025,
    "published": "2025-02-04T05:39:13Z",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "Spatial transformations that capture population-level morphological statistics are critical for medical image analysis. Commonly used smoothness regularizers for image registration fail to integrate population statistics, leading to anatomically inconsistent transformations. Inverse consistency regularizers promote geometric consistency but lack population morphometrics integration. Regularizers that constrain deformation to low-dimensional manifold methods address this. However, they prioritize",
    "arxiv_url": "https://arxiv.org/abs/2502.02029v2",
    "pdf_url": "https://arxiv.org/pdf/2502.02029v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2502.02029",
    "arxiv_authors": [
      "Mokshagna Sai Teja Karanam",
      "Krithika Iyer",
      "Sarang Joshi",
      "Shireen Elhabian"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=MORPH-LER%3A+Log-Euclidean+Regularization+for+Population-Aware+Image+Registration+Mokshagna+Sai+Teja+Karanam+Krithika+Iyer+Sarang+Joshi+Shireen+Elhabian",
    "gs_search_success": false,
    "gs_authors": null,
    "citation_count": null,
    "error_type": "multiple_matches"
  },
  {
    "arxiv_id": "2408.10710",
    "title": "Coarse-to-Fine Detection of Multiple Seams for Robotic Welding",
    "year": 2024,
    "published": "2024-08-20T10:24:59Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Efficiently detecting target weld seams while ensuring sub-millimeter accuracy has always been an important challenge in autonomous welding, which has significant application in industrial practice. Previous works mostly focused on recognizing and localizing welding seams one by one, leading to inferior efficiency in modeling the workpiece. This paper proposes a novel framework capable of multiple weld seams extraction using both RGB images and 3D point clouds. The RGB image is used to obtain th",
    "arxiv_url": "https://arxiv.org/abs/2408.10710v1",
    "pdf_url": "https://arxiv.org/pdf/2408.10710v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2408.10710",
    "arxiv_authors": [
      "Pengkun Wei",
      "Shuo Cheng",
      "Dayou Li",
      "Ran Song",
      "Yipeng Zhang",
      "Wei Zhang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Coarse-to-Fine+Detection+of+Multiple+Seams+for+Robotic+Welding+Pengkun+Wei+Shuo+Cheng+Dayou+Li+Ran+Song+Yipeng+Zhang",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "P Wei",
        "id": "EN77cFoAAAAJ"
      },
      {
        "name": "S Cheng",
        "id": null
      },
      {
        "name": "D Li",
        "id": "8wV8cnEAAAAJ"
      },
      {
        "name": "R Song",
        "id": null
      },
      {
        "name": "Y Zhang",
        "id": "VESaC8wAAAAJ"
      },
      {
        "name": "W Zhang2024 IEEE/RSJ International",
        "id": null
      }
    ],
    "citation_count": 2
  },
  {
    "arxiv_id": "2409.10473",
    "title": "MacDiff: Unified Skeleton Modeling with Masked Conditional Diffusion",
    "year": 2024,
    "published": "2024-09-16T17:06:10Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Self-supervised learning has proved effective for skeleton-based human action understanding. However, previous works either rely on contrastive learning that suffers false negative problems or are based on reconstruction that learns too much unessential low-level clues, leading to limited representations for downstream tasks. Recently, great advances have been made in generative learning, which is naturally a challenging yet meaningful pretext task to model the general underlying data distributi",
    "arxiv_url": "https://arxiv.org/abs/2409.10473v1",
    "pdf_url": "https://arxiv.org/pdf/2409.10473v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2409.10473",
    "arxiv_authors": [
      "Lehong Wu",
      "Lilang Lin",
      "Jiahang Zhang",
      "Yiyang Ma",
      "Jiaying Liu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=MacDiff%3A+Unified+Skeleton+Modeling+with+Masked+Conditional+Diffusion+Lehong+Wu+Lilang+Lin+Jiahang+Zhang+Yiyang+Ma+Jiaying+Liu",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "L Wu",
        "id": null
      },
      {
        "name": "L Lin",
        "id": null
      },
      {
        "name": "J Zhang",
        "id": "lzsANdYAAAAJ"
      },
      {
        "name": "Y Ma",
        "id": "cjZ0vJMAAAAJ"
      },
      {
        "name": "J Liu - European",
        "id": null
      }
    ],
    "citation_count": 9
  },
  {
    "arxiv_id": "2306.05442",
    "title": "FlowFormer: A Transformer Architecture and Its Masked Cost Volume Autoencoding for Optical Flow",
    "year": 2023,
    "published": "2023-06-08T12:24:04Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "This paper introduces a novel transformer-based network architecture, FlowFormer, along with the Masked Cost Volume AutoEncoding (MCVA) for pretraining it to tackle the problem of optical flow estimation. FlowFormer tokenizes the 4D cost-volume built from the source-target image pair and iteratively refines flow estimation with a cost-volume encoder-decoder architecture. The cost-volume encoder derives a cost memory with alternate-group transformer~(AGT) layers in a latent space and the decoder ",
    "arxiv_url": "https://arxiv.org/abs/2306.05442v1",
    "pdf_url": "https://arxiv.org/pdf/2306.05442v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2306.05442",
    "arxiv_authors": [
      "Zhaoyang Huang",
      "Xiaoyu Shi",
      "Chao Zhang",
      "Qiang Wang",
      "Yijin Li",
      "Hongwei Qin",
      "Jifeng Dai",
      "Xiaogang Wang",
      "Hongsheng Li"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=FlowFormer%3A+A+Transformer+Architecture+and+Its+Masked+Cost+Volume+Autoencoding+for+Optical+Flow+Zhaoyang+Huang+Xiaoyu+Shi+Chao+Zhang+Qiang+Wang+Yijin+Li",
    "gs_search_success": false,
    "gs_authors": null,
    "citation_count": null,
    "error_type": "fetch_failed"
  },
  {
    "arxiv_id": "2309.09907",
    "title": "Quantum Vision Clustering",
    "year": 2023,
    "published": "2023-09-18T16:15:16Z",
    "categories": [
      "quant-ph",
      "cs.CV"
    ],
    "abstract": "Unsupervised visual clustering has garnered significant attention in recent times, aiming to characterize distributions of unlabeled visual images through clustering based on a parameterized appearance approach. Alternatively, clustering algorithms can be viewed as assignment problems, often characterized as NP-hard, yet precisely solvable for small instances on contemporary hardware. Adiabatic quantum computing (AQC) emerges as a promising solution, poised to deliver substantial speedups for a ",
    "arxiv_url": "https://arxiv.org/abs/2309.09907v3",
    "pdf_url": "https://arxiv.org/pdf/2309.09907v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2309.09907",
    "arxiv_authors": [
      "Xuan Bac Nguyen",
      "Hugh Churchill",
      "Khoa Luu",
      "Samee U. Khan"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Quantum+Vision+Clustering+Xuan+Bac+Nguyen+Hugh+Churchill+Khoa+Luu+Samee+U.+Khan",
    "gs_search_success": false,
    "gs_authors": null,
    "citation_count": null,
    "error_type": "multiple_matches"
  },
  {
    "arxiv_id": "2307.09696",
    "title": "Towards Saner Deep Image Registration",
    "year": 2023,
    "published": "2023-07-19T00:41:39Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "With recent advances in computing hardware and surges of deep-learning architectures, learning-based deep image registration methods have surpassed their traditional counterparts, in terms of metric performance and inference time. However, these methods focus on improving performance measurements such as Dice, resulting in less attention given to model behaviors that are equally desirable for registrations, especially for medical imaging. This paper investigates these behaviors for popular learn",
    "arxiv_url": "https://arxiv.org/abs/2307.09696v3",
    "pdf_url": "https://arxiv.org/pdf/2307.09696v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2307.09696",
    "arxiv_authors": [
      "Bin Duan",
      "Ming Zhong",
      "Yan Yan"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Towards+Saner+Deep+Image+Registration+Bin+Duan+Ming+Zhong+Yan+Yan",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "B Duan",
        "id": "04SJDVoAAAAJ"
      },
      {
        "name": "M Zhong",
        "id": "saQCXowAAAAJ"
      },
      {
        "name": "Y Yan -",
        "id": null
      }
    ],
    "citation_count": 6
  },
  {
    "arxiv_id": "2304.06306",
    "title": "Efficient Multimodal Fusion via Interactive Prompting",
    "year": 2023,
    "published": "2023-04-13T07:31:51Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Large-scale pre-training has brought unimodal fields such as computer vision and natural language processing to a new era. Following this trend, the size of multi-modal learning models constantly increases, leading to an urgent need to reduce the massive computational cost of finetuning these models for downstream tasks. In this paper, we propose an efficient and flexible multimodal fusion method, namely PMF, tailored for fusing unimodally pre-trained transformers. Specifically, we first present",
    "arxiv_url": "https://arxiv.org/abs/2304.06306v2",
    "pdf_url": "https://arxiv.org/pdf/2304.06306v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2304.06306",
    "arxiv_authors": [
      "Yaowei Li",
      "Ruijie Quan",
      "Linchao Zhu",
      "Yi Yang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Efficient+Multimodal+Fusion+via+Interactive+Prompting+Yaowei+Li+Ruijie+Quan+Linchao+Zhu+Yi+Yang",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "Y Li",
        "id": "PgLlLU4AAAAJ"
      },
      {
        "name": "R Quan",
        "id": "WKLRPsAAAAAJ"
      },
      {
        "name": "L Zhu",
        "id": "9ZukE28AAAAJ"
      },
      {
        "name": "Y Yang -",
        "id": null
      }
    ],
    "citation_count": 83
  },
  {
    "arxiv_id": "2409.00020",
    "title": "A novel fusion of Sentinel-1 and Sentinel-2 with climate data for crop phenology estimation using Machine Learning",
    "year": 2024,
    "published": "2024-08-16T13:44:35Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Crop phenology describes the physiological development stages of crops from planting to harvest which is valuable information for decision makers to plan and adapt agricultural management strategies. In the era of big Earth observation data ubiquity, attempts have been made to accurately detect crop phenology using Remote Sensing (RS) and high resolution weather data. However, most studies have focused on large scale predictions of phenology or developed methods which are not adequate to help cr",
    "arxiv_url": "https://arxiv.org/abs/2409.00020v2",
    "pdf_url": "https://arxiv.org/pdf/2409.00020v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2409.00020",
    "arxiv_authors": [
      "Shahab Aldin Shojaeezadeh",
      "Abdelrazek Elnashar",
      "Tobias Karl David Weber"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=A+novel+fusion+of+Sentinel-1+and+Sentinel-2+with+climate+data+for+crop+phenology+estimation+using+Machine+Learning+Shahab+Aldin+Shojaeezadeh+Abdelrazek+Elnashar+Tobias+Karl+David+Weber",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "SA Shojaeezadeh",
        "id": "5wfrVUwAAAAJ"
      },
      {
        "name": "A Elnashar",
        "id": "mIlN0DoAAAAJ"
      },
      {
        "name": "TKD Weber - Science of Remote Sensing",
        "id": null
      }
    ],
    "citation_count": 3
  },
  {
    "arxiv_id": "2301.09339",
    "title": "Computer Vision for a Camel-Vehicle Collision Mitigation System",
    "year": 2023,
    "published": "2023-01-23T09:45:31Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "As the population grows and more land is being used for urbanization, ecosystems are disrupted by our roads and cars. This expansion of infrastructure cuts through wildlife territories, leading to many instances of Wildlife-Vehicle Collision (WVC). These instances of WVC are a global issue that is having a global socio-economic impact, resulting in billions of dollars in property damage and, at times, fatalities for vehicle occupants. In Saudi Arabia, this issue is similar, with instances of Cam",
    "arxiv_url": "https://arxiv.org/abs/2301.09339v1",
    "pdf_url": "https://arxiv.org/pdf/2301.09339v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2301.09339",
    "arxiv_authors": [
      "Khalid Alnujaidi",
      "Ghadah Alhabib"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Computer+Vision+for+a+Camel-Vehicle+Collision+Mitigation+System+Khalid+Alnujaidi+Ghadah+Alhabib",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "K Alnujaidi",
        "id": null
      },
      {
        "name": "G AlHabib -",
        "id": null
      }
    ],
    "citation_count": 2
  },
  {
    "arxiv_id": "2505.20858",
    "title": "ProBA: Probabilistic Bundle Adjustment with the Bhattacharyya Coefficient",
    "year": 2025,
    "published": "2025-05-27T08:07:00Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Classical Bundle Adjustment (BA) methods require accurate initial estimates for convergence and typically assume known camera intrinsics, which limits their applicability when such information is uncertain or unavailable. We propose a novel probabilistic formulation of BA (ProBA) that explicitly models and propagates uncertainty in both the 2D observations and the 3D scene structure, enabling optimization without any prior knowledge of camera poses or focal length. Our method uses 3D Gaussians i",
    "arxiv_url": "https://arxiv.org/abs/2505.20858v1",
    "pdf_url": "https://arxiv.org/pdf/2505.20858v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2505.20858",
    "arxiv_authors": [
      "Jason Chui",
      "Daniel Cremers"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=ProBA%3A+Probabilistic+Bundle+Adjustment+with+the+Bhattacharyya+Coefficient+Jason+Chui+Daniel+Cremers",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "J Chui",
        "id": null
      },
      {
        "name": "D Cremers -",
        "id": null
      }
    ],
    "citation_count": null
  },
  {
    "arxiv_id": "2305.09121",
    "title": "A Conditional Denoising Diffusion Probabilistic Model for Radio Interferometric Image Reconstruction",
    "year": 2023,
    "published": "2023-05-16T03:00:04Z",
    "categories": [
      "astro-ph.IM",
      "astro-ph.GA",
      "cs.CV",
      "cs.LG",
      "eess.IV"
    ],
    "abstract": "In radio astronomy, signals from radio telescopes are transformed into images of observed celestial objects, or sources. However, these images, called dirty images, contain real sources as well as artifacts due to signal sparsity and other factors. Therefore, radio interferometric image reconstruction is performed on dirty images, aiming to produce clean images in which artifacts are reduced and real sources are recovered. So far, existing methods have limited success on recovering faint sources",
    "arxiv_url": "https://arxiv.org/abs/2305.09121v2",
    "pdf_url": "https://arxiv.org/pdf/2305.09121v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2305.09121",
    "arxiv_authors": [
      "Ruoqi Wang",
      "Zhuoyang Chen",
      "Qiong Luo",
      "Feng Wang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=A+Conditional+Denoising+Diffusion+Probabilistic+Model+for+Radio+Interferometric+Image+Reconstruction+Ruoqi+Wang+Zhuoyang+Chen+Qiong+Luo+Feng+Wang",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "R Wang",
        "id": "AHk1W2EAAAAJ"
      },
      {
        "name": "Z Chen",
        "id": "n_Ez9OcAAAAJ"
      },
      {
        "name": "Q Luo",
        "id": "SqAsppUAAAAJ"
      },
      {
        "name": "F Wang -",
        "id": null
      }
    ],
    "citation_count": 9
  },
  {
    "arxiv_id": "2405.05538",
    "title": "A Survey on Personalized Content Synthesis with Diffusion Models",
    "year": 2024,
    "published": "2024-05-09T04:36:04Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Recent advancements in diffusion models have significantly impacted content creation, leading to the emergence of Personalized Content Synthesis (PCS). By utilizing a small set of user-provided examples featuring the same subject, PCS aims to tailor this subject to specific user-defined prompts. Over the past two years, more than 150 methods have been introduced in this area. However, existing surveys primarily focus on text-to-image generation, with few providing up-to-date summaries on PCS. Th",
    "arxiv_url": "https://arxiv.org/abs/2405.05538v4",
    "pdf_url": "https://arxiv.org/pdf/2405.05538v4",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2405.05538",
    "arxiv_authors": [
      "Xulu Zhang",
      "Xiaoyong Wei",
      "Wentao Hu",
      "Jinlin Wu",
      "Jiaxin Wu",
      "Wengyu Zhang",
      "Zhaoxiang Zhang",
      "Zhen Lei",
      "Qing Li"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=A+Survey+on+Personalized+Content+Synthesis+with+Diffusion+Models+Xulu+Zhang+Xiaoyong+Wei+Wentao+Hu+Jinlin+Wu+Jiaxin+Wu",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "X Zhang",
        "id": "4UqJoGMAAAAJ"
      },
      {
        "name": "X Wei",
        "id": "8kxWTokAAAAJ"
      },
      {
        "name": "W Hu",
        "id": "e-Das3gAAAAJ"
      },
      {
        "name": "J Wu",
        "id": "PRjnSUwAAAAJ"
      },
      {
        "name": "J Wu",
        "id": "PRjnSUwAAAAJ"
      },
      {
        "name": "W Zhang",
        "id": "zgV2AIAAAAAJ"
      },
      {
        "name": "Z Zhang",
        "id": "qxWfV6cAAAAJ"
      },
      {
        "name": "Z Lei",
        "id": "cuJ3QG8AAAAJ"
      },
      {
        "name": "Q LiMachine Intelligence Research",
        "id": null
      }
    ],
    "citation_count": 26
  },
  {
    "arxiv_id": "2305.15367",
    "title": "SAMScore: A Content Structural Similarity Metric for Image Translation Evaluation",
    "year": 2023,
    "published": "2023-05-24T17:22:39Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Image translation has wide applications, such as style transfer and modality conversion, usually aiming to generate images having both high degrees of realism and faithfulness. These problems remain difficult, especially when it is important to preserve content structures. Traditional image-level similarity metrics are of limited use, since the content structures of an image are high-level, and not strongly governed by pixel-wise faithfulness to an original image. To fill this gap, we introduce ",
    "arxiv_url": "https://arxiv.org/abs/2305.15367v2",
    "pdf_url": "https://arxiv.org/pdf/2305.15367v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2305.15367",
    "arxiv_authors": [
      "Yunxiang Li",
      "Meixu Chen",
      "Kai Wang",
      "Jun Ma",
      "Alan C. Bovik",
      "You Zhang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=SAMScore%3A+A+Content+Structural+Similarity+Metric+for+Image+Translation+Evaluation+Yunxiang+Li+Meixu+Chen+Kai+Wang+Jun+Ma+Alan+C.+Bovik",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "Y Li",
        "id": "evbcKz8AAAAJ"
      },
      {
        "name": "M Chen",
        "id": "oXK_smwAAAAJ"
      },
      {
        "name": "K Wang",
        "id": "lawzt94AAAAJ"
      },
      {
        "name": "J Ma",
        "id": null
      },
      {
        "name": "AC Bovik",
        "id": "p-PC50wAAAAJ"
      },
      {
        "name": "Y ZhangIEEE Transactions on Artificial Intelligence",
        "id": null
      }
    ],
    "citation_count": 2
  },
  {
    "arxiv_id": "2407.17428",
    "title": "Vision Language Model-Empowered Contract Theory for AIGC Task Allocation in Teleoperation",
    "year": 2024,
    "published": "2024-07-09T20:08:26Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Integrating low-light image enhancement techniques, in which diffusion-based AI-generated content (AIGC) models are promising, is necessary to enhance nighttime teleoperation. Remarkably, the AIGC model is computation-intensive, thus necessitating the allocation of AIGC tasks to edge servers with ample computational resources. Given the distinct cost of the AIGC model trained with varying-sized datasets and AIGC tasks possessing disparate demand, it is imperative to formulate a differential pric",
    "arxiv_url": "https://arxiv.org/abs/2407.17428v1",
    "pdf_url": "https://arxiv.org/pdf/2407.17428v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2407.17428",
    "arxiv_authors": [
      "Zijun Zhan",
      "Yaxian Dong",
      "Yuqing Hu",
      "Shuai Li",
      "Shaohua Cao",
      "Zhu Han"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Vision+Language+Model-Empowered+Contract+Theory+for+AIGC+Task+Allocation+in+Teleoperation+Zijun+Zhan+Yaxian+Dong+Yuqing+Hu+Shuai+Li+Shaohua+Cao",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "Z Zhan",
        "id": "H9BBihcAAAAJ"
      },
      {
        "name": "Y Dong",
        "id": "TqeAX70AAAAJ"
      },
      {
        "name": "DM Doe",
        "id": "B1O7Ih4AAAAJ"
      },
      {
        "name": "Y Hu",
        "id": "EYeNkVgAAAAJ"
      },
      {
        "name": "S Li",
        "id": "s0JTnKsAAAAJ"
      },
      {
        "name": "S Cao",
        "id": "3jxRwDgAAAAJ"
      }
    ],
    "citation_count": 6
  },
  {
    "arxiv_id": "2404.16348",
    "title": "Dual Expert Distillation Network for Generalized Zero-Shot Learning",
    "year": 2024,
    "published": "2024-04-25T05:59:42Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Zero-shot learning has consistently yielded remarkable progress via modeling nuanced one-to-one visual-attribute correlation. Existing studies resort to refining a uniform mapping function to align and correlate the sample regions and subattributes, ignoring two crucial issues: 1) the inherent asymmetry of attributes; and 2) the unutilized channel information. This paper addresses these issues by introducing a simple yet effective approach, dubbed Dual Expert Distillation Network (DEDN), where t",
    "arxiv_url": "https://arxiv.org/abs/2404.16348v2",
    "pdf_url": "https://arxiv.org/pdf/2404.16348v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2404.16348",
    "arxiv_authors": [
      "Zhijie Rao",
      "Jingcai Guo",
      "Xiaocheng Lu",
      "Jingming Liang",
      "Jie Zhang",
      "Haozhao Wang",
      "Kang Wei",
      "Xiaofeng Cao"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Dual+Expert+Distillation+Network+for+Generalized+Zero-Shot+Learning+Zhijie+Rao+Jingcai+Guo+Xiaocheng+Lu+Jingming+Liang+Jie+Zhang",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "Z Rao",
        "id": "M8fUIVIAAAAJ"
      },
      {
        "name": "J Guo",
        "id": "YjSHPjcAAAAJ"
      },
      {
        "name": "X Lu",
        "id": null
      },
      {
        "name": "J Liang",
        "id": null
      },
      {
        "name": "J Zhang",
        "id": null
      },
      {
        "name": "H Wang",
        "id": "yFrOuMEAAAAJ"
      },
      {
        "name": "K Wei",
        "id": "CYqbLN8AAAAJ"
      },
      {
        "name": "X Cao",
        "id": null
      }
    ],
    "citation_count": 19
  },
  {
    "arxiv_id": "2411.09361",
    "title": "Time-to-Event Pretraining for 3D Medical Imaging",
    "year": 2024,
    "published": "2024-11-14T11:08:54Z",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "With the rise of medical foundation models and the growing availability of imaging data, scalable pretraining techniques offer a promising way to identify imaging biomarkers predictive of future disease risk. While current self-supervised methods for 3D medical imaging models capture local structural features like organ morphology, they fail to link pixel biomarkers with long-term health outcomes due to a missing context problem. Current approaches lack the temporal context necessary to identify",
    "arxiv_url": "https://arxiv.org/abs/2411.09361v2",
    "pdf_url": "https://arxiv.org/pdf/2411.09361v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2411.09361",
    "arxiv_authors": [
      "Zepeng Huo",
      "Jason Alan Fries",
      "Alejandro Lozano",
      "Jeya Maria Jose Valanarasu",
      "Ethan Steinberg",
      "Louis Blankemeier",
      "Akshay S. Chaudhari",
      "Curtis Langlotz",
      "Nigam H. Shah"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Time-to-Event+Pretraining+for+3D+Medical+Imaging+Zepeng+Huo+Jason+Alan+Fries+Alejandro+Lozano+Jeya+Maria+Jose+Valanarasu+Ethan+Steinberg",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "Z Huo",
        "id": "1DM_ZVIAAAAJ"
      },
      {
        "name": "JA Fries",
        "id": "wywWmwoAAAAJ"
      },
      {
        "name": "A Lozano",
        "id": "EBPnTFMAAAAJ"
      },
      {
        "name": "JMJ Valanarasu",
        "id": null
      },
      {
        "name": "E Steinberg",
        "id": null
      },
      {
        "name": "L Blankemeier",
        "id": "z4SZH7MAAAAJ"
      },
      {
        "name": "AS Chaudhari",
        "id": "08Y4NhMAAAAJ"
      }
    ],
    "citation_count": 3
  },
  {
    "arxiv_id": "2404.13146",
    "title": "DeepFake-O-Meter v2.0: An Open Platform for DeepFake Detection",
    "year": 2024,
    "published": "2024-04-19T19:24:20Z",
    "categories": [
      "cs.CR",
      "cs.CV"
    ],
    "abstract": "Deepfakes, as AI-generated media, have increasingly threatened media integrity and personal privacy with realistic yet fake digital content. In this work, we introduce an open-source and user-friendly online platform, DeepFake-O-Meter v2.0, that integrates state-of-the-art methods for detecting Deepfake images, videos, and audio. Built upon DeepFake-O-Meter v1.0, we have made significant upgrades and improvements in platform architecture design, including user interaction, detector integration, ",
    "arxiv_url": "https://arxiv.org/abs/2404.13146v2",
    "pdf_url": "https://arxiv.org/pdf/2404.13146v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2404.13146",
    "arxiv_authors": [
      "Yan Ju",
      "Chengzhe Sun",
      "Shan Jia",
      "Shuwei Hou",
      "Zhaofeng Si",
      "Soumyya Kanti Datta",
      "Lipeng Ke",
      "Riky Zhou",
      "Anita Nikolich",
      "Siwei Lyu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=DeepFake-O-Meter+v2.0%3A+An+Open+Platform+for+DeepFake+Detection+Yan+Ju+Chengzhe+Sun+Shan+Jia+Shuwei+Hou+Zhaofeng+Si",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "Y Ju",
        "id": "pHvea8kAAAAJ"
      },
      {
        "name": "C Sun",
        "id": "HFVroDAAAAAJ"
      },
      {
        "name": "S Jia",
        "id": "YTAkshQAAAAJ"
      },
      {
        "name": "S Hou",
        "id": "9TqgL0UAAAAJ"
      },
      {
        "name": "Z Si",
        "id": "auO4764AAAAJ"
      },
      {
        "name": "SK Datta",
        "id": "R-SyM4YAAAAJ"
      },
      {
        "name": "L Ke",
        "id": "qzlM2bMAAAAJ"
      },
      {
        "name": "R Zhou",
        "id": null
      },
      {
        "name": "A Nikolich",
        "id": null
      },
      {
        "name": "S Lyu2024 IEEE 7th International",
        "id": null
      }
    ],
    "citation_count": 5
  },
  {
    "arxiv_id": "2505.22400",
    "title": "STDR: Spatio-Temporal Decoupling for Real-Time Dynamic Scene Rendering",
    "year": 2025,
    "published": "2025-05-28T14:26:41Z",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "abstract": "Although dynamic scene reconstruction has long been a fundamental challenge in 3D vision, the recent emergence of 3D Gaussian Splatting (3DGS) offers a promising direction by enabling high-quality, real-time rendering through explicit Gaussian primitives. However, existing 3DGS-based methods for dynamic reconstruction often suffer from \\textit{spatio-temporal incoherence} during initialization, where canonical Gaussians are constructed by aggregating observations from multiple frames without tem",
    "arxiv_url": "https://arxiv.org/abs/2505.22400v1",
    "pdf_url": "https://arxiv.org/pdf/2505.22400v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2505.22400",
    "arxiv_authors": [
      "Zehao Li",
      "Hao Jiang",
      "Yujun Cai",
      "Jianing Chen",
      "Baolong Bi",
      "Shuqin Gao",
      "Honglong Zhao",
      "Yiwei Wang",
      "Tianlu Mao",
      "Zhaoqi Wang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=STDR%3A+Spatio-Temporal+Decoupling+for+Real-Time+Dynamic+Scene+Rendering+Zehao+Li+Hao+Jiang+Yujun+Cai+Jianing+Chen+Baolong+Bi",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "Z Li",
        "id": "ZTxkDr4AAAAJ"
      },
      {
        "name": "H Jiang",
        "id": "xcctmG8AAAAJ"
      },
      {
        "name": "Y Cai",
        "id": "TE7lbQwAAAAJ"
      },
      {
        "name": "J Chen",
        "id": "5LrKbhcAAAAJ"
      },
      {
        "name": "B Bi",
        "id": "Pdu35PIAAAAJ"
      },
      {
        "name": "S Gao",
        "id": null
      },
      {
        "name": "H Zhao",
        "id": null
      },
      {
        "name": "Y Wang",
        "id": null
      },
      {
        "name": "T Mao",
        "id": "851N-WEAAAAJ"
      },
      {
        "name": "Z Wang",
        "id": null
      }
    ],
    "citation_count": 2
  },
  {
    "arxiv_id": "2412.03572",
    "title": "Navigation World Models",
    "year": 2024,
    "published": "2024-12-04T18:59:45Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "cs.RO"
    ],
    "abstract": "Navigation is a fundamental skill of agents with visual-motor capabilities. We introduce a Navigation World Model (NWM), a controllable video generation model that predicts future visual observations based on past observations and navigation actions. To capture complex environment dynamics, NWM employs a Conditional Diffusion Transformer (CDiT), trained on a diverse collection of egocentric videos of both human and robotic agents, and scaled up to 1 billion parameters. In familiar environments, ",
    "arxiv_url": "https://arxiv.org/abs/2412.03572v2",
    "pdf_url": "https://arxiv.org/pdf/2412.03572v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2412.03572",
    "arxiv_authors": [
      "Amir Bar",
      "Gaoyue Zhou",
      "Danny Tran",
      "Trevor Darrell",
      "Yann LeCun"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Navigation+World+Models+Amir+Bar+Gaoyue+Zhou+Danny+Tran+Trevor+Darrell+Yann+LeCun",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "A Bar",
        "id": "L__n1LUAAAAJ"
      },
      {
        "name": "G Zhou",
        "id": "-1iyBukAAAAJ"
      },
      {
        "name": "D Tran",
        "id": "azjJi_8AAAAJ"
      },
      {
        "name": "T Darrell",
        "id": "bh-uRFMAAAAJ"
      },
      {
        "name": "Y LeCun",
        "id": "WLN3QrAAAAAJ"
      }
    ],
    "citation_count": 102
  },
  {
    "arxiv_id": "2402.17316",
    "title": "Towards Robust and Efficient Cloud-Edge Elastic Model Adaptation via Selective Entropy Distillation",
    "year": 2024,
    "published": "2024-02-27T08:47:19Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "The conventional deep learning paradigm often involves training a deep model on a server and then deploying the model or its distilled ones to resource-limited edge devices. Usually, the models shall remain fixed once deployed (at least for some period) due to the potential high cost of model adaptation for both the server and edge sides. However, in many real-world scenarios, the test environments may change dynamically (known as distribution shifts), which often results in degraded performance",
    "arxiv_url": "https://arxiv.org/abs/2402.17316v3",
    "pdf_url": "https://arxiv.org/pdf/2402.17316v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2402.17316",
    "arxiv_authors": [
      "Yaofo Chen",
      "Shuaicheng Niu",
      "Yaowei Wang",
      "Shoukai Xu",
      "Hengjie Song",
      "Mingkui Tan"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Towards+Robust+and+Efficient+Cloud-Edge+Elastic+Model+Adaptation+via+Selective+Entropy+Distillation+Yaofo+Chen+Shuaicheng+Niu+Yaowei+Wang+Shoukai+Xu+Hengjie+Song",
    "gs_search_success": false,
    "gs_authors": null,
    "citation_count": null,
    "error_type": "fetch_failed"
  },
  {
    "arxiv_id": "2503.11439",
    "title": "COIN: Confidence Score-Guided Distillation for Annotation-Free Cell Segmentation",
    "year": 2025,
    "published": "2025-03-14T14:27:24Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Cell instance segmentation (CIS) is crucial for identifying individual cell morphologies in histopathological images, providing valuable insights for biological and medical research. While unsupervised CIS (UCIS) models aim to reduce the heavy reliance on labor-intensive image annotations, they fail to accurately capture cell boundaries, causing missed detections and poor performance. Recognizing the absence of error-free instances as a key limitation, we present COIN (COnfidence score-guided IN",
    "arxiv_url": "https://arxiv.org/abs/2503.11439v4",
    "pdf_url": "https://arxiv.org/pdf/2503.11439v4",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2503.11439",
    "arxiv_authors": [
      "Sanghyun Jo",
      "Seo Jin Lee",
      "Seungwoo Lee",
      "Seohyung Hong",
      "Hyungseok Seo",
      "Kyungsu Kim"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=COIN%3A+Confidence+Score-Guided+Distillation+for+Annotation-Free+Cell+Segmentation+Sanghyun+Jo+Seo+Jin+Lee+Seungwoo+Lee+Seohyung+Hong+Hyungseok+Seo",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "S Jo",
        "id": "xgP6q2YAAAAJ"
      },
      {
        "name": "SJ Lee",
        "id": null
      },
      {
        "name": "S Lee",
        "id": null
      },
      {
        "name": "S Hong",
        "id": null
      },
      {
        "name": "H Seo",
        "id": null
      },
      {
        "name": "K Kim",
        "id": "RbJDbtgAAAAJ"
      }
    ],
    "citation_count": 1
  },
  {
    "arxiv_id": "2403.17223",
    "title": "Co-Occurring of Object Detection and Identification towards unlabeled object discovery",
    "year": 2024,
    "published": "2024-03-25T21:53:36Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "abstract": "In this paper, we propose a novel deep learning based approach for identifying co-occurring objects in conjunction with base objects in multilabel object categories. Nowadays, with the advancement in computer vision based techniques we need to know about co-occurring objects with respect to base object for various purposes. The pipeline of the proposed work is composed of two stages: in the first stage of the proposed model we detect all the bounding boxes present in the image and their correspo",
    "arxiv_url": "https://arxiv.org/abs/2403.17223v1",
    "pdf_url": "https://arxiv.org/pdf/2403.17223v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2403.17223",
    "arxiv_authors": [
      "Binay Kumar Singh",
      "Niels Da Vitoria Lobo"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Co-Occurring+of+Object+Detection+and+Identification+towards+unlabeled+object+discovery+Binay+Kumar+Singh+Niels+Da+Vitoria+Lobo",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "BK Singh",
        "id": null
      },
      {
        "name": "NDV Lobo -",
        "id": null
      }
    ],
    "citation_count": null
  },
  {
    "arxiv_id": "2309.08760",
    "title": "Biased Attention: Do Vision Transformers Amplify Gender Bias More than Convolutional Neural Networks?",
    "year": 2023,
    "published": "2023-09-15T20:59:12Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Deep neural networks used in computer vision have been shown to exhibit many social biases such as gender bias. Vision Transformers (ViTs) have become increasingly popular in computer vision applications, outperforming Convolutional Neural Networks (CNNs) in many tasks such as image classification. However, given that research on mitigating bias in computer vision has primarily focused on CNNs, it is important to evaluate the effect of a different network architecture on the potential for bias a",
    "arxiv_url": "https://arxiv.org/abs/2309.08760v1",
    "pdf_url": "https://arxiv.org/pdf/2309.08760v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2309.08760",
    "arxiv_authors": [
      "Abhishek Mandal",
      "Susan Leavy",
      "Suzanne Little"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Biased+Attention%3A+Do+Vision+Transformers+Amplify+Gender+Bias+More+than+Convolutional+Neural+Networks%3F+Abhishek+Mandal+Susan+Leavy+Suzanne+Little",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "A Mandal",
        "id": "umf2YEQAAAAJ"
      },
      {
        "name": "S Leavy",
        "id": "DM5dr7YAAAAJ"
      },
      {
        "name": "S Little -",
        "id": null
      }
    ],
    "citation_count": 11
  },
  {
    "arxiv_id": "2505.12903",
    "title": "Towards Low-Latency Event Stream-based Visual Object Tracking: A Slow-Fast Approach",
    "year": 2025,
    "published": "2025-05-19T09:37:23Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Existing tracking algorithms typically rely on low-frame-rate RGB cameras coupled with computationally intensive deep neural network architectures to achieve effective tracking. However, such frame-based methods inherently face challenges in achieving low-latency performance and often fail in resource-constrained environments. Visual object tracking using bio-inspired event cameras has emerged as a promising research direction in recent years, offering distinct advantages for low-latency applica",
    "arxiv_url": "https://arxiv.org/abs/2505.12903v1",
    "pdf_url": "https://arxiv.org/pdf/2505.12903v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2505.12903",
    "arxiv_authors": [
      "Shiao Wang",
      "Xiao Wang",
      "Liye Jin",
      "Bo Jiang",
      "Lin Zhu",
      "Lan Chen",
      "Yonghong Tian",
      "Bin Luo"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Towards+Low-Latency+Event+Stream-based+Visual+Object+Tracking%3A+A+Slow-Fast+Approach+Shiao+Wang+Xiao+Wang+Liye+Jin+Bo+Jiang+Lin+Zhu",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "S Wang",
        "id": "U8-1OLYAAAAJ"
      },
      {
        "name": "X Wang",
        "id": "oq9awGMAAAAJ"
      },
      {
        "name": "L Jin",
        "id": null
      },
      {
        "name": "B Jiang",
        "id": null
      },
      {
        "name": "L Zhu",
        "id": "32d6xfEAAAAJ"
      },
      {
        "name": "L Chen",
        "id": null
      },
      {
        "name": "Y Tian",
        "id": "fn6hJx0AAAAJ"
      },
      {
        "name": "B Luo",
        "id": "0qaDapcAAAAJ"
      }
    ],
    "citation_count": 2
  },
  {
    "arxiv_id": "2311.09355",
    "title": "Privacy Threats in Stable Diffusion Models",
    "year": 2023,
    "published": "2023-11-15T20:31:40Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "abstract": "This paper introduces a novel approach to membership inference attacks (MIA) targeting stable diffusion computer vision models, specifically focusing on the highly sophisticated Stable Diffusion V2 by StabilityAI. MIAs aim to extract sensitive information about a model's training data, posing significant privacy concerns. Despite its advancements in image synthesis, our research reveals privacy vulnerabilities in the stable diffusion models' outputs. Exploiting this information, we devise a blac",
    "arxiv_url": "https://arxiv.org/abs/2311.09355v1",
    "pdf_url": "https://arxiv.org/pdf/2311.09355v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2311.09355",
    "arxiv_authors": [
      "Thomas Cilloni",
      "Charles Fleming",
      "Charles Walter"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Privacy+Threats+in+Stable+Diffusion+Models+Thomas+Cilloni+Charles+Fleming+Charles+Walter",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "T Cilloni",
        "id": "DUcwAAsAAAAJ"
      },
      {
        "name": "C Fleming",
        "id": null
      },
      {
        "name": "C Walter -",
        "id": null
      }
    ],
    "citation_count": 4
  },
  {
    "arxiv_id": "2404.13000",
    "title": "RadRotator: 3D Rotation of Radiographs with Diffusion Models",
    "year": 2024,
    "published": "2024-04-19T16:55:12Z",
    "categories": [
      "eess.IV",
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "Transforming two-dimensional (2D) images into three-dimensional (3D) volumes is a well-known yet challenging problem for the computer vision community. In the medical domain, a few previous studies attempted to convert two or more input radiographs into computed tomography (CT) volumes. Following their effort, we introduce a diffusion model-based technology that can rotate the anatomical content of any input radiograph in 3D space, potentially enabling the visualization of the entire anatomical ",
    "arxiv_url": "https://arxiv.org/abs/2404.13000v1",
    "pdf_url": "https://arxiv.org/pdf/2404.13000v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2404.13000",
    "arxiv_authors": [
      "Pouria Rouzrokh",
      "Bardia Khosravi",
      "Shahriar Faghani",
      "Kellen L. Mulford",
      "Michael J. Taunton",
      "Bradley J. Erickson",
      "Cody C. Wyles"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=RadRotator%3A+3D+Rotation+of+Radiographs+with+Diffusion+Models+Pouria+Rouzrokh+Bardia+Khosravi+Shahriar+Faghani+Kellen+L.+Mulford+Michael+J.+Taunton",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "P Rouzrokh",
        "id": "Ksv9I0sAAAAJ"
      },
      {
        "name": "B Khosravi",
        "id": "cSPQpGQAAAAJ"
      },
      {
        "name": "S Faghani",
        "id": "6HV5eJAAAAAJ"
      },
      {
        "name": "KL Mulford",
        "id": "p51IVCsAAAAJ"
      },
      {
        "name": "MJ Taunton",
        "id": "Te0rilEAAAAJ"
      },
      {
        "name": "BJ Erickson",
        "id": "28JnjFEAAAAJ"
      },
      {
        "name": "CC Wyles",
        "id": "HWnRTogAAAAJ"
      }
    ],
    "citation_count": 1
  },
  {
    "arxiv_id": "2307.06701",
    "title": "S-HR-VQVAE: Sequential Hierarchical Residual Learning Vector Quantized Variational Autoencoder for Video Prediction",
    "year": 2023,
    "published": "2023-07-13T11:58:27Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "abstract": "We address the video prediction task by putting forth a novel model that combines (i) a novel hierarchical residual learning vector quantized variational autoencoder (HR-VQVAE), and (ii) a novel autoregressive spatiotemporal predictive model (AST-PM). We refer to this approach as a sequential hierarchical residual learning vector quantized variational autoencoder (S-HR-VQVAE). By leveraging the intrinsic capabilities of HR-VQVAE at modeling still images with a parsimonious representation, combin",
    "arxiv_url": "https://arxiv.org/abs/2307.06701v3",
    "pdf_url": "https://arxiv.org/pdf/2307.06701v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2307.06701",
    "arxiv_authors": [
      "Mohammad Adiban",
      "Kalin Stefanov",
      "Sabato Marco Siniscalchi",
      "Giampiero Salvi"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=S-HR-VQVAE%3A+Sequential+Hierarchical+Residual+Learning+Vector+Quantized+Variational+Autoencoder+for+Video+Prediction+Mohammad+Adiban+Kalin+Stefanov+Sabato+Marco+Siniscalchi+Giampiero+Salvi",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "M Adiban",
        "id": "C_0FlbcAAAAJ"
      },
      {
        "name": "K Stefanov",
        "id": "0ZjgqkAAAAAJ"
      },
      {
        "name": "SM Siniscalchi",
        "id": "iHhGIcEAAAAJ"
      },
      {
        "name": "G SalviIEEE Transactions on Multimedia",
        "id": null
      }
    ],
    "citation_count": 5
  },
  {
    "arxiv_id": "2411.03576",
    "title": "Hybrid Attention for Robust RGB-T Pedestrian Detection in Real-World Conditions",
    "year": 2024,
    "published": "2024-11-06T00:34:26Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Multispectral pedestrian detection has gained significant attention in recent years, particularly in autonomous driving applications. To address the challenges posed by adversarial illumination conditions, the combination of thermal and visible images has demonstrated its advantages. However, existing fusion methods rely on the critical assumption that the RGB-Thermal (RGB-T) image pairs are fully overlapping. These assumptions often do not hold in real-world applications, where only partial ove",
    "arxiv_url": "https://arxiv.org/abs/2411.03576v1",
    "pdf_url": "https://arxiv.org/pdf/2411.03576v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2411.03576",
    "arxiv_authors": [
      "Arunkumar Rathinam",
      "Leo Pauly",
      "Abd El Rahman Shabayek",
      "Wassim Rharbaoui",
      "Anis Kacem",
      "Vincent Gaudilli√®re",
      "Djamila Aouada"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Hybrid+Attention+for+Robust+RGB-T+Pedestrian+Detection+in+Real-World+Conditions+Arunkumar+Rathinam+Leo+Pauly+Abd+El+Rahman+Shabayek+Wassim+Rharbaoui+Anis+Kacem",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "A Rathinam",
        "id": "zC2Ri2MAAAAJ"
      },
      {
        "name": "L Pauly",
        "id": "KbSrv50AAAAJ"
      },
      {
        "name": "W Rharbaoui",
        "id": "1OKAqtsAAAAJ"
      },
      {
        "name": "A Kacem",
        "id": "K3EWusMAAAAJ"
      },
      {
        "name": "V Gaudilli√®re",
        "id": "izegIRsAAAAJ"
      },
      {
        "name": "D AouadaIEEE Robotics and Automation Letters",
        "id": null
      }
    ],
    "citation_count": 1
  },
  {
    "arxiv_id": "2505.00063",
    "title": "GDI-Bench: A Benchmark for General Document Intelligence with Vision and Reasoning Decoupling",
    "year": 2025,
    "published": "2025-04-30T15:46:46Z",
    "categories": [
      "cs.CL",
      "cs.CV"
    ],
    "abstract": "The rapid advancement of multimodal large language models (MLLMs) has profoundly impacted the document domain, creating a wide array of application scenarios. This progress highlights the need for a comprehensive benchmark to evaluate these models' capabilities across various document-specific tasks. However, existing benchmarks often fail to locate specific model weaknesses or guide systematic improvements. To bridge this gap, we introduce a General Document Intelligence Benchmark (GDI-Bench), ",
    "arxiv_url": "https://arxiv.org/abs/2505.00063v2",
    "pdf_url": "https://arxiv.org/pdf/2505.00063v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2505.00063",
    "arxiv_authors": [
      "Siqi Li",
      "Yufan Shen",
      "Xiangnan Chen",
      "Jiayi Chen",
      "Hengwei Ju",
      "Haodong Duan",
      "Song Mao",
      "Hongbin Zhou",
      "Bo Zhang",
      "Bin Fu",
      "Pinlong Cai",
      "Licheng Wen",
      "Botian Shi",
      "Yong Liu",
      "Xinyu Cai",
      "Yu Qiao"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=GDI-Bench%3A+A+Benchmark+for+General+Document+Intelligence+with+Vision+and+Reasoning+Decoupling+Siqi+Li+Yufan+Shen+Xiangnan+Chen+Jiayi+Chen+Hengwei+Ju",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "S Li",
        "id": null
      },
      {
        "name": "Y Shen",
        "id": "BnGLjlgAAAAJ"
      },
      {
        "name": "X Chen",
        "id": null
      },
      {
        "name": "J Chen",
        "id": null
      },
      {
        "name": "H Ju",
        "id": null
      },
      {
        "name": "H Duan",
        "id": "vi3W-m8AAAAJ"
      },
      {
        "name": "S Mao",
        "id": "BaqGkQQAAAAJ"
      },
      {
        "name": "H Zhou",
        "id": "fb_WgAEAAAAJ"
      },
      {
        "name": "B Zhang",
        "id": "qoP6b3oAAAAJ"
      },
      {
        "name": "B Fu",
        "id": "9WhK1y4AAAAJ"
      },
      {
        "name": "P Cai",
        "id": "H6mQGfAAAAAJ"
      },
      {
        "name": "L Wen",
        "id": "RNnjXTkAAAAJ"
      }
    ],
    "citation_count": 3
  },
  {
    "arxiv_id": "2411.12789",
    "title": "Efficient Physics Simulation for 3D Scenes via MLLM-Guided Gaussian Splatting",
    "year": 2024,
    "published": "2024-11-19T12:52:21Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Recent advancements in 3D generation models have opened new possibilities for simulating dynamic 3D object movements and customizing behaviors, yet creating this content remains challenging. Current methods often require manual assignment of precise physical properties for simulations or rely on video generation models to predict them, which is computationally intensive. In this paper, we rethink the usage of multi-modal large language model (MLLM) in physics-based simulation, and present Sim An",
    "arxiv_url": "https://arxiv.org/abs/2411.12789v3",
    "pdf_url": "https://arxiv.org/pdf/2411.12789v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2411.12789",
    "arxiv_authors": [
      "Haoyu Zhao",
      "Hao Wang",
      "Xingyue Zhao",
      "Hao Fei",
      "Hongqiu Wang",
      "Chengjiang Long",
      "Hua Zou"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Efficient+Physics+Simulation+for+3D+Scenes+via+MLLM-Guided+Gaussian+Splatting+Haoyu+Zhao+Hao+Wang+Xingyue+Zhao+Hao+Fei+Hongqiu+Wang",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "H Zhao",
        "id": null
      },
      {
        "name": "H Wang",
        "id": "nt3qQqkAAAAJ"
      },
      {
        "name": "X Zhao",
        "id": "fZuqWe0AAAAJ"
      },
      {
        "name": "H Fei",
        "id": null
      },
      {
        "name": "H Wang",
        "id": "nt3qQqkAAAAJ"
      },
      {
        "name": "C Long",
        "id": "k0XkeiAAAAAJ"
      },
      {
        "name": "H Zou",
        "id": null
      }
    ],
    "citation_count": 2
  },
  {
    "arxiv_id": "2406.12736",
    "title": "Beyond Visual Appearances: Privacy-sensitive Objects Identification via Hybrid Graph Reasoning",
    "year": 2024,
    "published": "2024-06-18T15:58:22Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "The Privacy-sensitive Object Identification (POI) task allocates bounding boxes for privacy-sensitive objects in a scene. The key to POI is settling an object's privacy class (privacy-sensitive or non-privacy-sensitive). In contrast to conventional object classes which are determined by the visual appearance of an object, one object's privacy class is derived from the scene contexts and is subject to various implicit factors beyond its visual appearance. That is, visually similar objects may be ",
    "arxiv_url": "https://arxiv.org/abs/2406.12736v2",
    "pdf_url": "https://arxiv.org/pdf/2406.12736v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2406.12736",
    "arxiv_authors": [
      "Zhuohang Jiang",
      "Bingkui Tong",
      "Xia Du",
      "Ahmed Alhammadi",
      "Jizhe Zhou"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Beyond+Visual+Appearances%3A+Privacy-sensitive+Objects+Identification+via+Hybrid+Graph+Reasoning+Zhuohang+Jiang+Bingkui+Tong+Xia+Du+Ahmed+Alhammadi+Jizhe+Zhou",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "Z Jiang",
        "id": "h3vEhrgAAAAJ"
      },
      {
        "name": "B Tong",
        "id": null
      },
      {
        "name": "X Du",
        "id": "JheiwGAAAAAJ"
      },
      {
        "name": "A Alhammadi",
        "id": "m6BPDY0AAAAJ"
      },
      {
        "name": "J Zhou -",
        "id": null
      }
    ],
    "citation_count": 1
  },
  {
    "arxiv_id": "2308.14575",
    "title": "Referring Image Segmentation Using Text Supervision",
    "year": 2023,
    "published": "2023-08-28T13:40:47Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Existing Referring Image Segmentation (RIS) methods typically require expensive pixel-level or box-level annotations for supervision. In this paper, we observe that the referring texts used in RIS already provide sufficient information to localize the target object. Hence, we propose a novel weakly-supervised RIS framework to formulate the target localization problem as a classification process to differentiate between positive and negative text expressions. While the referring text expressions ",
    "arxiv_url": "https://arxiv.org/abs/2308.14575v1",
    "pdf_url": "https://arxiv.org/pdf/2308.14575v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2308.14575",
    "arxiv_authors": [
      "Fang Liu",
      "Yuhao Liu",
      "Yuqiu Kong",
      "Ke Xu",
      "Lihe Zhang",
      "Baocai Yin",
      "Gerhard Hancke",
      "Rynson Lau"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Referring+Image+Segmentation+Using+Text+Supervision+Fang+Liu+Yuhao+Liu+Yuqiu+Kong+Ke+Xu+Lihe+Zhang",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "F Liu",
        "id": "qrQkfxYAAAAJ"
      },
      {
        "name": "Y Liu",
        "id": "eHWiGU8AAAAJ"
      },
      {
        "name": "Y Kong",
        "id": "nKrhk4UAAAAJ"
      },
      {
        "name": "K Xu",
        "id": "2meBhbQAAAAJ"
      },
      {
        "name": "L Zhang",
        "id": "XGPdQbIAAAAJ"
      },
      {
        "name": "B Yin",
        "id": "uXtYy_8AAAAJ"
      },
      {
        "name": "G Hancke",
        "id": "C2iR3xUAAAAJ"
      },
      {
        "name": "R Lau",
        "id": "KilQqKYAAAAJ"
      }
    ],
    "citation_count": 55
  },
  {
    "arxiv_id": "2403.15378",
    "title": "Long-CLIP: Unlocking the Long-Text Capability of CLIP",
    "year": 2024,
    "published": "2024-03-22T17:58:16Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Contrastive Language-Image Pre-training (CLIP) has been the cornerstone for zero-shot classification, text-image retrieval, and text-image generation by aligning image and text modalities. Despite its widespread adoption, a significant limitation of CLIP lies in the inadequate length of text input. The length of the text token is restricted to 77, and an empirical study shows the actual effective length is even less than 20. This prevents CLIP from handling detailed descriptions, limiting its ap",
    "arxiv_url": "https://arxiv.org/abs/2403.15378v3",
    "pdf_url": "https://arxiv.org/pdf/2403.15378v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2403.15378",
    "arxiv_authors": [
      "Beichen Zhang",
      "Pan Zhang",
      "Xiaoyi Dong",
      "Yuhang Zang",
      "Jiaqi Wang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Long-CLIP%3A+Unlocking+the+Long-Text+Capability+of+CLIP+Beichen+Zhang+Pan+Zhang+Xiaoyi+Dong+Yuhang+Zang+Jiaqi+Wang",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "B Zhang",
        "id": "qlxc34gAAAAJ"
      },
      {
        "name": "P Zhang",
        "id": "u61Pe1QAAAAJ"
      },
      {
        "name": "X Dong",
        "id": "FscToE0AAAAJ"
      },
      {
        "name": "Y Zang",
        "id": "hW23VKIAAAAJ"
      },
      {
        "name": "J Wang - European",
        "id": null
      }
    ],
    "citation_count": 295
  },
  {
    "arxiv_id": "2403.07593",
    "title": "MinkUNeXt: Point Cloud-based Large-scale Place Recognition using 3D Sparse Convolutions",
    "year": 2024,
    "published": "2024-03-12T12:25:54Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "This paper presents MinkUNeXt, an effective and efficient architecture for place-recognition from point clouds entirely based on the new 3D MinkNeXt Block, a residual block composed of 3D sparse convolutions that follows the philosophy established by recent Transformers but purely using simple 3D convolutions. Feature extraction is performed at different scales by a U-Net encoder-decoder network and the feature aggregation of those features into a single descriptor is carried out by a Generalize",
    "arxiv_url": "https://arxiv.org/abs/2403.07593v2",
    "pdf_url": "https://arxiv.org/pdf/2403.07593v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2403.07593",
    "arxiv_authors": [
      "J. J. Cabrera",
      "A. Santo",
      "A. Gil",
      "C. Viegas",
      "L. Pay√°"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=MinkUNeXt%3A+Point+Cloud-based+Large-scale+Place+Recognition+using+3D+Sparse+Convolutions+J.+J.+Cabrera+A.+Santo+A.+Gil+C.+Viegas+L.+Pay%C3%A1",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "JJ Cabrera",
        "id": "kUAmbQMAAAAJ"
      },
      {
        "name": "A Santo",
        "id": "5zgGWHoAAAAJ"
      },
      {
        "name": "A Gil",
        "id": null
      },
      {
        "name": "C Viegas",
        "id": "b8bU8BoAAAAJ"
      },
      {
        "name": "L Pay√° - Array",
        "id": null
      }
    ],
    "citation_count": 7
  },
  {
    "arxiv_id": "2503.06501",
    "title": "TextInPlace: Indoor Visual Place Recognition in Repetitive Structures with Scene Text Spotting and Verification",
    "year": 2025,
    "published": "2025-03-09T08:03:41Z",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "abstract": "Visual Place Recognition (VPR) is a crucial capability for long-term autonomous robots, enabling them to identify previously visited locations using visual information. However, existing methods remain limited in indoor settings due to the highly repetitive structures inherent in such environments. We observe that scene texts frequently appear in indoor spaces and can help distinguish visually similar but different places. This inspires us to propose TextInPlace, a simple yet effective VPR frame",
    "arxiv_url": "https://arxiv.org/abs/2503.06501v2",
    "pdf_url": "https://arxiv.org/pdf/2503.06501v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2503.06501",
    "arxiv_authors": [
      "Huaqi Tao",
      "Bingxi Liu",
      "Calvin Chen",
      "Tingjun Huang",
      "He Li",
      "Jinqiang Cui",
      "Hong Zhang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=TextInPlace%3A+Indoor+Visual+Place+Recognition+in+Repetitive+Structures+with+Scene+Text+Spotting+and+Verification+Huaqi+Tao+Bingxi+Liu+Calvin+Chen+Tingjun+Huang+He+Li",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "H Tao",
        "id": "XIo7-wYAAAAJ"
      },
      {
        "name": "B Liu",
        "id": "T2_7muEAAAAJ"
      },
      {
        "name": "C Chen",
        "id": "9vZuu1QAAAAJ"
      },
      {
        "name": "T Huang",
        "id": null
      },
      {
        "name": "H Li",
        "id": null
      },
      {
        "name": "J Cui",
        "id": null
      },
      {
        "name": "H Zhang",
        "id": null
      }
    ],
    "citation_count": 1
  },
  {
    "arxiv_id": "2303.04378",
    "title": "SGDViT: Saliency-Guided Dynamic Vision Transformer for UAV Tracking",
    "year": 2023,
    "published": "2023-03-08T05:01:00Z",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "abstract": "Vision-based object tracking has boosted extensive autonomous applications for unmanned aerial vehicles (UAVs). However, the dynamic changes in flight maneuver and viewpoint encountered in UAV tracking pose significant difficulties, e.g. , aspect ratio change, and scale variation. The conventional cross-correlation operation, while commonly used, has limitations in effectively capturing perceptual similarity and incorporates extraneous background information. To mitigate these limitations, this ",
    "arxiv_url": "https://arxiv.org/abs/2303.04378v1",
    "pdf_url": "https://arxiv.org/pdf/2303.04378v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2303.04378",
    "arxiv_authors": [
      "Liangliang Yao",
      "Changhong Fu",
      "Sihang Li",
      "Guangze Zheng",
      "Junjie Ye"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=SGDViT%3A+Saliency-Guided+Dynamic+Vision+Transformer+for+UAV+Tracking+Liangliang+Yao+Changhong+Fu+Sihang+Li+Guangze+Zheng+Junjie+Ye",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "L Yao",
        "id": "DipDDOwAAAAJ"
      },
      {
        "name": "C Fu",
        "id": "zmbMZ4kAAAAJ"
      },
      {
        "name": "S Li",
        "id": "90IoeJsAAAAJ"
      },
      {
        "name": "G Zheng",
        "id": "-kcZWRQAAAAJ"
      },
      {
        "name": "J Ye -",
        "id": null
      }
    ],
    "citation_count": 63
  },
  {
    "arxiv_id": "2401.14832",
    "title": "Text Image Inpainting via Global Structure-Guided Diffusion Models",
    "year": 2024,
    "published": "2024-01-26T13:01:28Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Real-world text can be damaged by corrosion issues caused by environmental or human factors, which hinder the preservation of the complete styles of texts, e.g., texture and structure. These corrosion issues, such as graffiti signs and incomplete signatures, bring difficulties in understanding the texts, thereby posing significant challenges to downstream applications, e.g., scene text recognition and signature identification. Notably, current inpainting techniques often fail to adequately addre",
    "arxiv_url": "https://arxiv.org/abs/2401.14832v3",
    "pdf_url": "https://arxiv.org/pdf/2401.14832v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2401.14832",
    "arxiv_authors": [
      "Shipeng Zhu",
      "Pengfei Fang",
      "Chenjie Zhu",
      "Zuoyan Zhao",
      "Qiang Xu",
      "Hui Xue"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Text+Image+Inpainting+via+Global+Structure-Guided+Diffusion+Models+Shipeng+Zhu+Pengfei+Fang+Chenjie+Zhu+Zuoyan+Zhao+Qiang+Xu",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "S Zhu",
        "id": "D0T1TFAAAAAJ"
      },
      {
        "name": "P Fang",
        "id": "Fk4A13IAAAAJ"
      },
      {
        "name": "C Zhu",
        "id": "-VW8XWcAAAAJ"
      },
      {
        "name": "Z Zhao",
        "id": "Zny4LKYAAAAJ"
      },
      {
        "name": "Q Xu",
        "id": null
      },
      {
        "name": "H Xue",
        "id": null
      }
    ],
    "citation_count": 17
  },
  {
    "arxiv_id": "2402.13144",
    "title": "Neural Network Diffusion",
    "year": 2024,
    "published": "2024-02-20T16:59:03Z",
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "abstract": "Diffusion models have achieved remarkable success in image and video generation. In this work, we demonstrate that diffusion models can also \\textit{generate high-performing neural network parameters}. Our approach is simple, utilizing an autoencoder and a diffusion model. The autoencoder extracts latent representations of a subset of the trained neural network parameters. Next, a diffusion model is trained to synthesize these latent representations from random noise. This model then generates n",
    "arxiv_url": "https://arxiv.org/abs/2402.13144v3",
    "pdf_url": "https://arxiv.org/pdf/2402.13144v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2402.13144",
    "arxiv_authors": [
      "Kai Wang",
      "Dongwen Tang",
      "Boya Zeng",
      "Yida Yin",
      "Zhaopan Xu",
      "Yukun Zhou",
      "Zelin Zang",
      "Trevor Darrell",
      "Zhuang Liu",
      "Yang You"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Neural+Network+Diffusion+Kai+Wang+Dongwen+Tang+Boya+Zeng+Yida+Yin+Zhaopan+Xu",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "K Wang",
        "id": "i2II0XIAAAAJ"
      },
      {
        "name": "D Tang",
        "id": "9lKm_5IAAAAJ"
      },
      {
        "name": "B Zeng",
        "id": "gYSdq-IAAAAJ"
      },
      {
        "name": "Y Yin",
        "id": "F3yrMeUAAAAJ"
      },
      {
        "name": "Z Xu",
        "id": null
      },
      {
        "name": "Y Zhou",
        "id": "c0WCD74AAAAJ"
      },
      {
        "name": "Z Zang",
        "id": "foERjnQAAAAJ"
      },
      {
        "name": "T Darrell",
        "id": "bh-uRFMAAAAJ"
      },
      {
        "name": "Z Liu",
        "id": "7OTD-LEAAAAJ"
      },
      {
        "name": "Y You",
        "id": "jF4dPZwAAAAJ"
      }
    ],
    "citation_count": 60
  },
  {
    "arxiv_id": "2305.04203",
    "title": "Unlocking the Power of Open Set : A New Perspective for Open-Set Noisy Label Learning",
    "year": 2023,
    "published": "2023-05-07T06:55:28Z",
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "abstract": "Learning from noisy data has attracted much attention, where most methods focus on closed-set label noise. However, a more common scenario in the real world is the presence of both open-set and closed-set noise. Existing methods typically identify and handle these two types of label noise separately by designing a specific strategy for each type. However, in many real-world scenarios, it would be challenging to identify open-set examples, especially when the dataset has been severely corrupted. ",
    "arxiv_url": "https://arxiv.org/abs/2305.04203v2",
    "pdf_url": "https://arxiv.org/pdf/2305.04203v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2305.04203",
    "arxiv_authors": [
      "Wenhai Wan",
      "Xinrui Wang",
      "Ming-Kun Xie",
      "Shao-Yuan Li",
      "Sheng-Jun Huang",
      "Songcan Chen"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Unlocking+the+Power+of+Open+Set+%3A+A+New+Perspective+for+Open-Set+Noisy+Label+Learning+Wenhai+Wan+Xinrui+Wang+Ming-Kun+Xie+Shao-Yuan+Li+Sheng-Jun+Huang",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "W Wan",
        "id": null
      },
      {
        "name": "X Wang",
        "id": "3juyXgQAAAAJ"
      },
      {
        "name": "MK Xie",
        "id": "DxUxwnMAAAAJ"
      },
      {
        "name": "SY Li",
        "id": "GeazSBAAAAAJ"
      },
      {
        "name": "SJ Huang",
        "id": "-mAks_AAAAAJ"
      },
      {
        "name": "S Chen",
        "id": "SdPinGIAAAAJ"
      }
    ],
    "citation_count": 17
  },
  {
    "arxiv_id": "2310.08071",
    "title": "Learning Transferable Conceptual Prototypes for Interpretable Unsupervised Domain Adaptation",
    "year": 2023,
    "published": "2023-10-12T06:36:41Z",
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "abstract": "Despite the great progress of unsupervised domain adaptation (UDA) with the deep neural networks, current UDA models are opaque and cannot provide promising explanations, limiting their applications in the scenarios that require safe and controllable model decisions. At present, a surge of work focuses on designing deep interpretable methods with adequate data annotations and only a few methods consider the distributional shift problem. Most existing interpretable UDA methods are post-hoc ones, ",
    "arxiv_url": "https://arxiv.org/abs/2310.08071v1",
    "pdf_url": "https://arxiv.org/pdf/2310.08071v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2310.08071",
    "arxiv_authors": [
      "Junyu Gao",
      "Xinhong Ma",
      "Changsheng Xu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Learning+Transferable+Conceptual+Prototypes+for+Interpretable+Unsupervised+Domain+Adaptation+Junyu+Gao+Xinhong+Ma+Changsheng+Xu",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "J Gao",
        "id": "y1nOY24AAAAJ"
      },
      {
        "name": "X Ma",
        "id": "a44XUMQAAAAJ"
      },
      {
        "name": "C Xu - IEEE Transactions on Image Processing",
        "id": null
      }
    ],
    "citation_count": 16
  },
  {
    "arxiv_id": "2301.06567",
    "title": "Scalable Surface Water Mapping up to Fine-scale using Geometric Features of Water from Topographic Airborne LiDAR Data",
    "year": 2023,
    "published": "2023-01-16T19:04:23Z",
    "categories": [
      "cs.CV",
      "eess.IV"
    ],
    "abstract": "Despite substantial technological advancements, the comprehensive mapping of surface water, particularly smaller bodies (<1ha), continues to be a challenge due to a lack of robust, scalable methods. Standard methods require either training labels or site-specific parameter tuning, which complicates automated mapping and introduces biases related to training data and parameters. The reliance on water's reflectance properties, including LiDAR intensity, further complicates the matter, as higher-re",
    "arxiv_url": "https://arxiv.org/abs/2301.06567v2",
    "pdf_url": "https://arxiv.org/pdf/2301.06567v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2301.06567",
    "arxiv_authors": [
      "Hunsoo Song",
      "Jinha Jung"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Scalable+Surface+Water+Mapping+up+to+Fine-scale+using+Geometric+Features+of+Water+from+Topographic+Airborne+LiDAR+Data+Hunsoo+Song+Jinha+Jung",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "H Song",
        "id": "7YYuRPAAAAAJ"
      },
      {
        "name": "J Jung -",
        "id": null
      }
    ],
    "citation_count": 5
  },
  {
    "arxiv_id": "2505.03299",
    "title": "Towards Efficient Benchmarking of Foundation Models in Remote Sensing: A Capabilities Encoding Approach",
    "year": 2025,
    "published": "2025-05-06T08:29:18Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Foundation models constitute a significant advancement in computer vision: after a single, albeit costly, training phase, they can address a wide array of tasks. In the field of Earth observation, over 75 remote sensing vision foundation models have been developed in the past four years. However, none has consistently outperformed the others across all available downstream tasks. To facilitate their comparison, we propose a cost-effective method for predicting a model's performance on multiple d",
    "arxiv_url": "https://arxiv.org/abs/2505.03299v1",
    "pdf_url": "https://arxiv.org/pdf/2505.03299v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2505.03299",
    "arxiv_authors": [
      "Pierre Adorni",
      "Minh-Tan Pham",
      "St√©phane May",
      "S√©bastien Lef√®vre"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Towards+Efficient+Benchmarking+of+Foundation+Models+in+Remote+Sensing%3A+A+Capabilities+Encoding+Approach+Pierre+Adorni+Minh-Tan+Pham+St%C3%A9phane+May+S%C3%A9bastien+Lef%C3%A8vre",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "P Adorni",
        "id": "yB4hyb0AAAAJ"
      },
      {
        "name": "MT Pham",
        "id": "mnNKKlAAAAAJ"
      },
      {
        "name": "S May",
        "id": null
      },
      {
        "name": "S Lef√®vre",
        "id": "C_8NI7IAAAAJ"
      }
    ],
    "citation_count": 5
  },
  {
    "arxiv_id": "2409.14747",
    "title": "Distribution-Level Feature Distancing for Machine Unlearning: Towards a Better Trade-off Between Model Utility and Forgetting",
    "year": 2024,
    "published": "2024-09-23T06:51:10Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "With the explosive growth of deep learning applications and increasing privacy concerns, the right to be forgotten has become a critical requirement in various AI industries. For example, given a facial recognition system, some individuals may wish to remove their personal data that might have been used in the training phase. Unfortunately, deep neural networks sometimes unexpectedly leak personal identities, making this removal challenging. While recent machine unlearning algorithms aim to enab",
    "arxiv_url": "https://arxiv.org/abs/2409.14747v5",
    "pdf_url": "https://arxiv.org/pdf/2409.14747v5",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2409.14747",
    "arxiv_authors": [
      "Dasol Choi",
      "Dongbin Na"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Distribution-Level+Feature+Distancing+for+Machine+Unlearning%3A+Towards+a+Better+Trade-off+Between+Model+Utility+and+Forgetting+Dasol+Choi+Dongbin+Na",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "D Choi",
        "id": "qEQMMuoAAAAJ"
      },
      {
        "name": "D Na -",
        "id": null
      }
    ],
    "citation_count": 6
  },
  {
    "arxiv_id": "2308.01390",
    "title": "OpenFlamingo: An Open-Source Framework for Training Large Autoregressive Vision-Language Models",
    "year": 2023,
    "published": "2023-08-02T19:10:23Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "abstract": "We introduce OpenFlamingo, a family of autoregressive vision-language models ranging from 3B to 9B parameters. OpenFlamingo is an ongoing effort to produce an open-source replication of DeepMind's Flamingo models. On seven vision-language datasets, OpenFlamingo models average between 80 - 89% of corresponding Flamingo performance. This technical report describes our models, training data, hyperparameters, and evaluation suite. We share our models and code at https://github.com/mlfoundations/open",
    "arxiv_url": "https://arxiv.org/abs/2308.01390v2",
    "pdf_url": "https://arxiv.org/pdf/2308.01390v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2308.01390",
    "arxiv_authors": [
      "Anas Awadalla",
      "Irena Gao",
      "Josh Gardner",
      "Jack Hessel",
      "Yusuf Hanafy",
      "Wanrong Zhu",
      "Kalyani Marathe",
      "Yonatan Bitton",
      "Samir Gadre",
      "Shiori Sagawa",
      "Jenia Jitsev",
      "Simon Kornblith",
      "Pang Wei Koh",
      "Gabriel Ilharco",
      "Mitchell Wortsman",
      "Ludwig Schmidt"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=OpenFlamingo%3A+An+Open-Source+Framework+for+Training+Large+Autoregressive+Vision-Language+Models+Anas+Awadalla+Irena+Gao+Josh+Gardner+Jack+Hessel+Yusuf+Hanafy",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "A Awadalla",
        "id": "gMOjp_oAAAAJ"
      },
      {
        "name": "I Gao",
        "id": "yAyAvssAAAAJ"
      },
      {
        "name": "J Gardner",
        "id": "SSq1t_YAAAAJ"
      },
      {
        "name": "J Hessel",
        "id": "SxQQ1msAAAAJ"
      },
      {
        "name": "Y Hanafy",
        "id": null
      },
      {
        "name": "W Zhu",
        "id": "xNWgry0AAAAJ"
      },
      {
        "name": "K Marathe",
        "id": "gCxlvdcAAAAJ"
      },
      {
        "name": "Y Bitton",
        "id": "P9Fpf4sAAAAJ"
      },
      {
        "name": "S Gadre",
        "id": "oAhlg9gAAAAJ"
      }
    ],
    "citation_count": 805
  },
  {
    "arxiv_id": "2307.14127",
    "title": "Creative Birds: Self-Supervised Single-View 3D Style Transfer",
    "year": 2023,
    "published": "2023-07-26T11:47:44Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "In this paper, we propose a novel method for single-view 3D style transfer that generates a unique 3D object with both shape and texture transfer. Our focus lies primarily on birds, a popular subject in 3D reconstruction, for which no existing single-view 3D transfer methods have been developed.The method we propose seeks to generate a 3D mesh shape and texture of a bird from two single-view images. To achieve this, we introduce a novel shape transfer generator that comprises a dual residual gat",
    "arxiv_url": "https://arxiv.org/abs/2307.14127v2",
    "pdf_url": "https://arxiv.org/pdf/2307.14127v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2307.14127",
    "arxiv_authors": [
      "Renke Wang",
      "Guimin Que",
      "Shuo Chen",
      "Xiang Li",
      "Jun Li",
      "Jian Yang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Creative+Birds%3A+Self-Supervised+Single-View+3D+Style+Transfer+Renke+Wang+Guimin+Que+Shuo+Chen+Xiang+Li+Jun+Li",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "R Wang",
        "id": "BZG8NcEAAAAJ"
      },
      {
        "name": "G Que",
        "id": null
      },
      {
        "name": "S Chen",
        "id": "vlu_3ksAAAAJ"
      },
      {
        "name": "X Li",
        "id": "oamjJdYAAAAJ"
      },
      {
        "name": "J Li",
        "id": "iGPEwQsAAAAJ"
      },
      {
        "name": "J Yang",
        "id": "6CIDtZQAAAAJ"
      }
    ],
    "citation_count": 10
  },
  {
    "arxiv_id": "2311.12608",
    "title": "Density-Guided Dense Pseudo Label Selection For Semi-supervised Oriented Object Detection",
    "year": 2023,
    "published": "2023-11-21T13:49:28Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Recently, dense pseudo-label, which directly selects pseudo labels from the original output of the teacher model without any complicated post-processing steps, has received considerable attention in semi-supervised object detection (SSOD). However, for the multi-oriented and dense objects that are common in aerial scenes, existing dense pseudo-label selection methods are inefficient because they ignore the significant density difference. Therefore, we propose Density-Guided Dense Pseudo Label Se",
    "arxiv_url": "https://arxiv.org/abs/2311.12608v2",
    "pdf_url": "https://arxiv.org/pdf/2311.12608v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2311.12608",
    "arxiv_authors": [
      "Tong Zhao",
      "Qiang Fang",
      "Shuohao Shi",
      "Xin Xu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Density-Guided+Dense+Pseudo+Label+Selection+For+Semi-supervised+Oriented+Object+Detection+Tong+Zhao+Qiang+Fang+Shuohao+Shi+Xin+Xu",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "T Zhao",
        "id": null
      },
      {
        "name": "Q Fang",
        "id": null
      },
      {
        "name": "S Shi",
        "id": null
      },
      {
        "name": "X Xu -",
        "id": null
      }
    ],
    "citation_count": 4
  },
  {
    "arxiv_id": "2409.13477",
    "title": "A Plug-and-Play Method for Guided Multi-contrast MRI Reconstruction based on Content/Style Modeling",
    "year": 2024,
    "published": "2024-09-20T13:08:51Z",
    "categories": [
      "eess.IV",
      "cs.CV",
      "physics.med-ph"
    ],
    "abstract": "Since multiple MRI contrasts of the same anatomy contain redundant information, one contrast can guide the reconstruction of an undersampled subsequent contrast. To this end, several end-to-end learning-based guided reconstruction methods have been proposed. However, a key challenge is the requirement of large paired training datasets comprising raw data and aligned reference images. We propose a modular two-stage approach addressing this issue, additionally providing an explanatory framework fo",
    "arxiv_url": "https://arxiv.org/abs/2409.13477v4",
    "pdf_url": "https://arxiv.org/pdf/2409.13477v4",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2409.13477",
    "arxiv_authors": [
      "Chinmay Rao",
      "Matthias van Osch",
      "Nicola Pezzotti",
      "Jeroen de Bresser",
      "Mark van Buchem",
      "Laurens Beljaards",
      "Jakob Meineke",
      "Elwin de Weerdt",
      "Huangling Lu",
      "Mariya Doneva",
      "Marius Staring"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=A+Plug-and-Play+Method+for+Guided+Multi-contrast+MRI+Reconstruction+based+on+Content%2FStyle+Modeling+Chinmay+Rao+Matthias+van+Osch+Nicola+Pezzotti+Jeroen+de+Bresser+Mark+van+Buchem",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "C Rao",
        "id": "aNaTofQAAAAJ"
      },
      {
        "name": "M van Osch",
        "id": "gWrILsYAAAAJ"
      },
      {
        "name": "N Pezzotti",
        "id": "61To93wAAAAJ"
      },
      {
        "name": "J de Bresser",
        "id": "Jxvf9ygAAAAJ"
      },
      {
        "name": "M van Buchem",
        "id": null
      },
      {
        "name": "L Beljaards",
        "id": null
      },
      {
        "name": "J Meineke",
        "id": null
      }
    ],
    "citation_count": 2
  },
  {
    "arxiv_id": "2411.18968",
    "title": "Perception of Visual Content: Differences Between Humans and Foundation Models",
    "year": 2024,
    "published": "2024-11-28T07:37:04Z",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "Human-annotated content is often used to train machine learning (ML) models. However, recently, language and multi-modal foundational models have been used to replace and scale-up human annotator's efforts. This study explores the similarity between human-generated and ML-generated annotations of images across diverse socio-economic contexts (RQ1) and their impact on ML model performance and bias (RQ2). We aim to understand differences in perception and identify potential biases in content inter",
    "arxiv_url": "https://arxiv.org/abs/2411.18968v3",
    "pdf_url": "https://arxiv.org/pdf/2411.18968v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2411.18968",
    "arxiv_authors": [
      "Nardiena A. Pratama",
      "Shaoyang Fan",
      "Gianluca Demartini"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Perception+of+Visual+Content%3A+Differences+Between+Humans+and+Foundation+Models+Nardiena+A.+Pratama+Shaoyang+Fan+Gianluca+Demartini",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "NA Pratama",
        "id": "Jl2S_UoAAAAJ"
      },
      {
        "name": "S Fan",
        "id": "uF87D5kAAAAJ"
      },
      {
        "name": "G Demartini -",
        "id": null
      }
    ],
    "citation_count": null
  },
  {
    "arxiv_id": "2411.03723",
    "title": "Zero-shot Dynamic MRI Reconstruction with Global-to-local Diffusion Model",
    "year": 2024,
    "published": "2024-11-06T07:40:27Z",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "abstract": "Diffusion models have recently demonstrated considerable advancement in the generation and reconstruction of magnetic resonance imaging (MRI) data. These models exhibit great potential in handling unsampled data and reducing noise, highlighting their promise as generative models. However, their application in dynamic MRI remains relatively underexplored. This is primarily due to the substantial amount of fully-sampled data typically required for training, which is difficult to obtain in dynamic ",
    "arxiv_url": "https://arxiv.org/abs/2411.03723v1",
    "pdf_url": "https://arxiv.org/pdf/2411.03723v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2411.03723",
    "arxiv_authors": [
      "Yu Guan",
      "Kunlong Zhang",
      "Qi Qi",
      "Dong Wang",
      "Ziwen Ke",
      "Shaoyu Wang",
      "Dong Liang",
      "Qiegen Liu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Zero-shot+Dynamic+MRI+Reconstruction+with+Global-to-local+Diffusion+Model+Yu+Guan+Kunlong+Zhang+Qi+Qi+Dong+Wang+Ziwen+Ke",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "Y Guan",
        "id": null
      },
      {
        "name": "K Zhang",
        "id": null
      },
      {
        "name": "Q Qi",
        "id": null
      },
      {
        "name": "D Wang",
        "id": null
      },
      {
        "name": "Z Ke",
        "id": "mh0DHTAAAAAJ"
      },
      {
        "name": "S Wang",
        "id": null
      },
      {
        "name": "D Liang",
        "id": null
      },
      {
        "name": "Q LiuNMR in Biomedicine",
        "id": null
      }
    ],
    "citation_count": null
  },
  {
    "arxiv_id": "2304.07169",
    "title": "A Comparative Study on Generative Models for High Resolution Solar Observation Imaging",
    "year": 2023,
    "published": "2023-04-14T14:40:32Z",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "Solar activity is one of the main drivers of variability in our solar system and the key source of space weather phenomena that affect Earth and near Earth space. The extensive record of high resolution extreme ultraviolet (EUV) observations from the Solar Dynamics Observatory (SDO) offers an unprecedented, very large dataset of solar images. In this work, we make use of this comprehensive dataset to investigate capabilities of current state-of-the-art generative models to accurately capture the",
    "arxiv_url": "https://arxiv.org/abs/2304.07169v1",
    "pdf_url": "https://arxiv.org/pdf/2304.07169v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2304.07169",
    "arxiv_authors": [
      "Mehdi Cherti",
      "Alexander Czernik",
      "Stefan Kesselheim",
      "Frederic Effenberger",
      "Jenia Jitsev"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=A+Comparative+Study+on+Generative+Models+for+High+Resolution+Solar+Observation+Imaging+Mehdi+Cherti+Alexander+Czernik+Stefan+Kesselheim+Frederic+Effenberger+Jenia+Jitsev",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "M Cherti",
        "id": "JgOyYi8AAAAJ"
      },
      {
        "name": "A Czernik",
        "id": null
      },
      {
        "name": "S Kesselheim",
        "id": "5k2rdewAAAAJ"
      },
      {
        "name": "F Effenberger",
        "id": "_URnXIIAAAAJ"
      },
      {
        "name": "J Jitsev",
        "id": "p1FuAMkAAAAJ"
      }
    ],
    "citation_count": null
  },
  {
    "arxiv_id": "2501.05460",
    "title": "Efficiently Serving Large Multimodal Models Using EPD Disaggregation",
    "year": 2024,
    "published": "2024-12-25T10:11:31Z",
    "categories": [
      "cs.DC",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "Large Multimodal Models (LMMs) extend Large Language Models (LLMs) by handling diverse inputs such as images, audio, and video, but at the cost of adding a multimodal encoding stage that increases both computational and memory overhead. This step negatively affects key Service Level Objectives (SLOs), such as time to first token (TTFT) and time per output token (TPOT). We introduce Encode-Prefill-Decode (EPD) Disaggregation, a novel framework that separates the encoding, prefill, and decode stag",
    "arxiv_url": "https://arxiv.org/abs/2501.05460v4",
    "pdf_url": "https://arxiv.org/pdf/2501.05460v4",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2501.05460",
    "arxiv_authors": [
      "Gursimran Singh",
      "Xinglu Wang",
      "Yifan Hu",
      "Timothy Yu",
      "Linzi Xing",
      "Wei Jiang",
      "Zhefeng Wang",
      "Xiaolong Bai",
      "Yi Li",
      "Ying Xiong",
      "Yong Zhang",
      "Zhenan Fan"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Efficiently+Serving+Large+Multimodal+Models+Using+EPD+Disaggregation+Gursimran+Singh+Xinglu+Wang+Yifan+Hu+Timothy+Yu+Linzi+Xing",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "G Singh",
        "id": "SHHfhvEAAAAJ"
      },
      {
        "name": "X Wang",
        "id": null
      },
      {
        "name": "Y Hu",
        "id": null
      },
      {
        "name": "T Yu",
        "id": "fR2ZNkoAAAAJ"
      },
      {
        "name": "L Xing",
        "id": "CXghWHMAAAAJ"
      },
      {
        "name": "W Jiang",
        "id": null
      },
      {
        "name": "Z Wang",
        "id": "t22ZUJ4AAAAJ"
      },
      {
        "name": "X Bai",
        "id": null
      },
      {
        "name": "Y Li",
        "id": null
      },
      {
        "name": "Y Xiong",
        "id": null
      },
      {
        "name": "Y Zhang",
        "id": "K2zamrwAAAAJ"
      },
      {
        "name": "Z Fan",
        "id": "un31cHUAAAAJ"
      }
    ],
    "citation_count": 8
  },
  {
    "arxiv_id": "2306.17319",
    "title": "ReMaX: Relaxing for Better Training on Efficient Panoptic Segmentation",
    "year": 2023,
    "published": "2023-06-29T22:05:56Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "This paper presents a new mechanism to facilitate the training of mask transformers for efficient panoptic segmentation, democratizing its deployment. We observe that due to its high complexity, the training objective of panoptic segmentation will inevitably lead to much higher false positive penalization. Such unbalanced loss makes the training process of the end-to-end mask-transformer based architectures difficult, especially for efficient models. In this paper, we present ReMaX that adds rel",
    "arxiv_url": "https://arxiv.org/abs/2306.17319v1",
    "pdf_url": "https://arxiv.org/pdf/2306.17319v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2306.17319",
    "arxiv_authors": [
      "Shuyang Sun",
      "Weijun Wang",
      "Qihang Yu",
      "Andrew Howard",
      "Philip Torr",
      "Liang-Chieh Chen"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=ReMaX%3A+Relaxing+for+Better+Training+on+Efficient+Panoptic+Segmentation+Shuyang+Sun+Weijun+Wang+Qihang+Yu+Andrew+Howard+Philip+Torr",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "S Sun",
        "id": "PoAvGRMAAAAJ"
      },
      {
        "name": "W Wang",
        "id": "K-BSWr4AAAAJ"
      },
      {
        "name": "A Howard",
        "id": "_9l8vD8AAAAJ"
      },
      {
        "name": "Q Yu",
        "id": "7zZdZxsAAAAJ"
      },
      {
        "name": "P Torr",
        "id": "kPxa2w0AAAAJ"
      },
      {
        "name": "LC ChenAdvances in Neural Information Processing Systems",
        "id": null
      }
    ],
    "citation_count": 18
  },
  {
    "arxiv_id": "2311.01216",
    "title": "Convergent plug-and-play with proximal denoiser and unconstrained regularization parameter",
    "year": 2023,
    "published": "2023-11-02T13:18:39Z",
    "categories": [
      "math.OC",
      "cs.CV"
    ],
    "abstract": "In this work, we present new proofs of convergence for Plug-and-Play (PnP) algorithms. PnP methods are efficient iterative algorithms for solving image inverse problems where regularization is performed by plugging a pre-trained denoiser in a proximal algorithm, such as Proximal Gradient Descent (PGD) or Douglas-Rachford Splitting (DRS). Recent research has explored convergence by incorporating a denoiser that writes exactly as a proximal operator. However, the corresponding PnP algorithm has th",
    "arxiv_url": "https://arxiv.org/abs/2311.01216v1",
    "pdf_url": "https://arxiv.org/pdf/2311.01216v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2311.01216",
    "arxiv_authors": [
      "Samuel Hurault",
      "Antonin Chambolle",
      "Arthur Leclaire",
      "Nicolas Papadakis"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Convergent+plug-and-play+with+proximal+denoiser+and+unconstrained+regularization+parameter+Samuel+Hurault+Antonin+Chambolle+Arthur+Leclaire+Nicolas+Papadakis",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "S Hurault",
        "id": "f_rtYCAAAAAJ"
      },
      {
        "name": "A Chambolle",
        "id": "eGLUzVEAAAAJ"
      },
      {
        "name": "A Leclaire",
        "id": "sm-xR4wAAAAJ"
      },
      {
        "name": "N Papadakis",
        "id": "hfyLiLYAAAAJ"
      }
    ],
    "citation_count": 14
  },
  {
    "arxiv_id": "2503.22346",
    "title": "ArchCAD-400K: A Large-Scale CAD drawings Dataset and New Baseline for Panoptic Symbol Spotting",
    "year": 2025,
    "published": "2025-03-28T11:40:53Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Recognizing symbols in architectural CAD drawings is critical for various advanced engineering applications. In this paper, we propose a novel CAD data annotation engine that leverages intrinsic attributes from systematically archived CAD drawings to automatically generate high-quality annotations, thus significantly reducing manual labeling efforts. Utilizing this engine, we construct ArchCAD-400K, a large-scale CAD dataset consisting of 413,062 chunks from 5538 highly standardized drawings, ma",
    "arxiv_url": "https://arxiv.org/abs/2503.22346v3",
    "pdf_url": "https://arxiv.org/pdf/2503.22346v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2503.22346",
    "arxiv_authors": [
      "Ruifeng Luo",
      "Zhengjie Liu",
      "Tianxiao Cheng",
      "Jie Wang",
      "Tongjie Wang",
      "Xingguang Wei",
      "Haomin Wang",
      "YanPeng Li",
      "Fu Chai",
      "Fei Cheng",
      "Shenglong Ye",
      "Wenhai Wang",
      "Yanting Zhang",
      "Yu Qiao",
      "Hongjie Zhang",
      "Xianzhong Zhao"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=ArchCAD-400K%3A+A+Large-Scale+CAD+drawings+Dataset+and+New+Baseline+for+Panoptic+Symbol+Spotting+Ruifeng+Luo+Zhengjie+Liu+Tianxiao+Cheng+Jie+Wang+Tongjie+Wang",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "Z Liu",
        "id": null
      },
      {
        "name": "T Cheng",
        "id": null
      },
      {
        "name": "J Wang",
        "id": "WM0OglcAAAAJ"
      },
      {
        "name": "T Wang",
        "id": null
      },
      {
        "name": "X Wei",
        "id": "7OcAXh8AAAAJ"
      },
      {
        "name": "H Wang",
        "id": "EkfrzcYAAAAJ"
      },
      {
        "name": "YP Li",
        "id": null
      },
      {
        "name": "F Chai",
        "id": "BScFoPoAAAAJ"
      },
      {
        "name": "F Cheng",
        "id": null
      },
      {
        "name": "S Ye",
        "id": "IX_lIFIAAAAJ"
      }
    ],
    "citation_count": 1
  },
  {
    "arxiv_id": "2411.12440",
    "title": "Beyond Gaussians: Fast and High-Fidelity 3D Splatting with Linear Kernels",
    "year": 2024,
    "published": "2024-11-19T11:59:54Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Recent advancements in 3D Gaussian Splatting (3DGS) have substantially improved novel view synthesis, enabling high-quality reconstruction and real-time rendering. However, blurring artifacts, such as floating primitives and over-reconstruction, remain challenging. Current methods address these issues by refining scene structure, enhancing geometric representations, addressing blur in training images, improving rendering consistency, and optimizing density control, yet the role of kernel design ",
    "arxiv_url": "https://arxiv.org/abs/2411.12440v3",
    "pdf_url": "https://arxiv.org/pdf/2411.12440v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2411.12440",
    "arxiv_authors": [
      "Haodong Chen",
      "Runnan Chen",
      "Qiang Qu",
      "Zhaoqing Wang",
      "Tongliang Liu",
      "Xiaoming Chen",
      "Yuk Ying Chung"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Beyond+Gaussians%3A+Fast+and+High-Fidelity+3D+Splatting+with+Linear+Kernels+Haodong+Chen+Runnan+Chen+Qiang+Qu+Zhaoqing+Wang+Tongliang+Liu",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "H Chen",
        "id": null
      },
      {
        "name": "R Chen",
        "id": "Uq2DuzkAAAAJ"
      },
      {
        "name": "Q Qu",
        "id": "tQkMLgoAAAAJ"
      },
      {
        "name": "Z Wang",
        "id": "ZqOjPKQAAAAJ"
      },
      {
        "name": "T Liu",
        "id": "EiLdZ_YAAAAJ"
      },
      {
        "name": "X Chen",
        "id": "-2TD5ewAAAAJ"
      },
      {
        "name": "YY Chung",
        "id": "bxURGE4AAAAJ"
      }
    ],
    "citation_count": 9
  },
  {
    "arxiv_id": "2503.01845",
    "title": "Denoising Functional Maps: Diffusion Models for Shape Correspondence",
    "year": 2025,
    "published": "2025-03-03T18:59:56Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Estimating correspondences between pairs of deformable shapes remains a challenging problem. Despite substantial progress, existing methods lack broad generalization capabilities and require category-specific training data. To address these limitations, we propose a fundamentally new approach to shape correspondence based on denoising diffusion models. In our method, a diffusion model learns to directly predict the functional map, a low-dimensional representation of a point-wise map between shap",
    "arxiv_url": "https://arxiv.org/abs/2503.01845v2",
    "pdf_url": "https://arxiv.org/pdf/2503.01845v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2503.01845",
    "arxiv_authors": [
      "Aleksei Zhuravlev",
      "Zorah L√§hner",
      "Vladislav Golyanik"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Denoising+Functional+Maps%3A+Diffusion+Models+for+Shape+Correspondence+Aleksei+Zhuravlev+Zorah+L%C3%A4hner+Vladislav+Golyanik",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "A Zhuravlev",
        "id": "0yDzRT0AAAAJ"
      },
      {
        "name": "Z L√§hner",
        "id": "fMc479MAAAAJ"
      },
      {
        "name": "V Golyanik",
        "id": "Aona9YwAAAAJ"
      }
    ],
    "citation_count": 6
  },
  {
    "arxiv_id": "2504.05795",
    "title": "Robust Fusion Controller: Degradation-aware Image Fusion with Fine-grained Language Instructions",
    "year": 2025,
    "published": "2025-04-08T08:22:55Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Current image fusion methods struggle to adapt to real-world environments encompassing diverse degradations with spatially varying characteristics. To address this challenge, we propose a robust fusion controller (RFC) capable of achieving degradation-aware image fusion through fine-grained language instructions, ensuring its reliable application in adverse environments. Specifically, RFC first parses language instructions to innovatively derive the functional condition and the spatial condition",
    "arxiv_url": "https://arxiv.org/abs/2504.05795v2",
    "pdf_url": "https://arxiv.org/pdf/2504.05795v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2504.05795",
    "arxiv_authors": [
      "Hao Zhang",
      "Yanping Zha",
      "Qingwei Zhuang",
      "Zhenfeng Shao",
      "Jiayi Ma"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Robust+Fusion+Controller%3A+Degradation-aware+Image+Fusion+with+Fine-grained+Language+Instructions+Hao+Zhang+Yanping+Zha+Qingwei+Zhuang+Zhenfeng+Shao+Jiayi+Ma",
    "gs_search_success": false,
    "gs_authors": null,
    "citation_count": null,
    "error_type": "fetch_failed"
  },
  {
    "arxiv_id": "2502.18197",
    "title": "VCT: Training Consistency Models with Variational Noise Coupling",
    "year": 2025,
    "published": "2025-02-25T13:38:04Z",
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "abstract": "Consistency Training (CT) has recently emerged as a strong alternative to diffusion models for image generation. However, non-distillation CT often suffers from high variance and instability, motivating ongoing research into its training dynamics. We propose Variational Consistency Training (VCT), a flexible and effective framework compatible with various forward kernels, including those in flow matching. Its key innovation is a learned noise-data coupling scheme inspired by Variational Autoenco",
    "arxiv_url": "https://arxiv.org/abs/2502.18197v2",
    "pdf_url": "https://arxiv.org/pdf/2502.18197v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2502.18197",
    "arxiv_authors": [
      "Gianluigi Silvestri",
      "Luca Ambrogioni",
      "Chieh-Hsin Lai",
      "Yuhta Takida",
      "Yuki Mitsufuji"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=VCT%3A+Training+Consistency+Models+with+Variational+Noise+Coupling+Gianluigi+Silvestri+Luca+Ambrogioni+Chieh-Hsin+Lai+Yuhta+Takida+Yuki+Mitsufuji",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "G Silvestri",
        "id": "BKRLVogAAAAJ"
      },
      {
        "name": "L Ambrogioni",
        "id": "J9IABpQAAAAJ"
      },
      {
        "name": "CH Lai",
        "id": "KDnKGu8AAAAJ"
      },
      {
        "name": "Y Takida",
        "id": "ahqdEYUAAAAJ"
      },
      {
        "name": "Y Mitsufuji",
        "id": "GMytI10AAAAJ"
      }
    ],
    "citation_count": 7
  },
  {
    "arxiv_id": "2409.09788",
    "title": "Reasoning Paths with Reference Objects Elicit Quantitative Spatial Reasoning in Large Vision-Language Models",
    "year": 2024,
    "published": "2024-09-15T16:45:42Z",
    "categories": [
      "cs.CV",
      "cs.CL"
    ],
    "abstract": "Despite recent advances demonstrating vision-language models' (VLMs) abilities to describe complex relationships in images using natural language, their capability to quantitatively reason about object sizes and distances remains underexplored. In this work, we introduce a manually annotated benchmark, Q-Spatial Bench, with 271 questions across five categories designed for quantitative spatial reasoning and systematically investigate the performance of state-of-the-art VLMs on this task. Our ana",
    "arxiv_url": "https://arxiv.org/abs/2409.09788v1",
    "pdf_url": "https://arxiv.org/pdf/2409.09788v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2409.09788",
    "arxiv_authors": [
      "Yuan-Hong Liao",
      "Rafid Mahmood",
      "Sanja Fidler",
      "David Acuna"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Reasoning+Paths+with+Reference+Objects+Elicit+Quantitative+Spatial+Reasoning+in+Large+Vision-Language+Models+Yuan-Hong+Liao+Rafid+Mahmood+Sanja+Fidler+David+Acuna",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "YH Liao",
        "id": "cL05XGsAAAAJ"
      },
      {
        "name": "R Mahmood",
        "id": "NoPweUQAAAAJ"
      },
      {
        "name": "S Fidler",
        "id": "CUlqK5EAAAAJ"
      },
      {
        "name": "D Acuna -",
        "id": null
      }
    ],
    "citation_count": 36
  },
  {
    "arxiv_id": "2306.03779",
    "title": "Performance-optimized deep neural networks are evolving into worse models of inferotemporal visual cortex",
    "year": 2023,
    "published": "2023-06-06T15:34:45Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "One of the most impactful findings in computational neuroscience over the past decade is that the object recognition accuracy of deep neural networks (DNNs) correlates with their ability to predict neural responses to natural images in the inferotemporal (IT) cortex. This discovery supported the long-held theory that object recognition is a core objective of the visual cortex, and suggested that more accurate DNNs would serve as better models of IT neuron responses to images. Since then, deep le",
    "arxiv_url": "https://arxiv.org/abs/2306.03779v1",
    "pdf_url": "https://arxiv.org/pdf/2306.03779v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2306.03779",
    "arxiv_authors": [
      "Drew Linsley",
      "Ivan F. Rodriguez",
      "Thomas Fel",
      "Michael Arcaro",
      "Saloni Sharma",
      "Margaret Livingstone",
      "Thomas Serre"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Performance-optimized+deep+neural+networks+are+evolving+into+worse+models+of+inferotemporal+visual+cortex+Drew+Linsley+Ivan+F.+Rodriguez+Thomas+Fel+Michael+Arcaro+Saloni+Sharma",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "D Linsley",
        "id": "cXZlAuQAAAAJ"
      },
      {
        "name": "IF Rodriguez Rodriguez",
        "id": "KotPiIMAAAAJ"
      },
      {
        "name": "T Fel",
        "id": "_Rsjt9kAAAAJ"
      },
      {
        "name": "M Arcaro",
        "id": "07tNFdgAAAAJ"
      },
      {
        "name": "S Sharma",
        "id": "QvYJdzMAAAAJ"
      },
      {
        "name": "M Livingstone",
        "id": "P_3rGrsAAAAJ"
      },
      {
        "name": "T SerreAdvances in Neural Information Processing Systems",
        "id": null
      }
    ],
    "citation_count": 35
  },
  {
    "arxiv_id": "2303.09803",
    "title": "Covariance properties under natural image transformations for the generalized Gaussian derivative model for visual receptive fields",
    "year": 2023,
    "published": "2023-03-17T07:08:17Z",
    "categories": [
      "q-bio.NC",
      "cs.CV",
      "eess.IV"
    ],
    "abstract": "This paper presents a theory for how geometric image transformations can be handled by a first layer of linear receptive fields, in terms of true covariance properties, which, in turn, enable geometric invariance properties at higher levels in the visual hierarchy. Specifically, we develop this theory for a generalized Gaussian derivative model for visual receptive fields, which is derived in an axiomatic manner from first principles, that reflect symmetry properties of the environment, compleme",
    "arxiv_url": "https://arxiv.org/abs/2303.09803v4",
    "pdf_url": "https://arxiv.org/pdf/2303.09803v4",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2303.09803",
    "arxiv_authors": [
      "Tony Lindeberg"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Covariance+properties+under+natural+image+transformations+for+the+generalized+Gaussian+derivative+model+for+visual+receptive+fields+Tony+Lindeberg",
    "gs_search_success": true,
    "gs_authors": [],
    "citation_count": 20
  },
  {
    "arxiv_id": "2412.10339",
    "title": "A Universal Degradation-based Bridging Technique for Domain Adaptive Semantic Segmentation",
    "year": 2024,
    "published": "2024-12-13T18:35:27Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Semantic segmentation often suffers from significant performance degradation when the trained network is applied to a different domain. To address this issue, unsupervised domain adaptation (UDA) has been extensively studied. Existing methods introduce the domain bridging techniques to mitigate substantial domain gap, which construct intermediate domains to facilitate the gradual transfer of knowledge across different domains. However, these strategies often require dataset-specific designs and ",
    "arxiv_url": "https://arxiv.org/abs/2412.10339v1",
    "pdf_url": "https://arxiv.org/pdf/2412.10339v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2412.10339",
    "arxiv_authors": [
      "Wangkai Li",
      "Rui Sun",
      "Tianzhu Zhang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=A+Universal+Degradation-based+Bridging+Technique+for+Domain+Adaptive+Semantic+Segmentation+Wangkai+Li+Rui+Sun+Tianzhu+Zhang",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "W Li",
        "id": "vgWptp0AAAAJ"
      },
      {
        "name": "R Sun",
        "id": null
      },
      {
        "name": "T Zhang -",
        "id": null
      }
    ],
    "citation_count": 4
  },
  {
    "arxiv_id": "2411.10237",
    "title": "ScribbleVS: Scribble-Supervised Medical Image Segmentation via Dynamic Competitive Pseudo Label Selection",
    "year": 2024,
    "published": "2024-11-15T14:51:30Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "In clinical medicine, precise image segmentation can provide substantial support to clinicians. However, obtaining high-quality segmentation typically demands extensive pixel-level annotations, which are labor-intensive and expensive. Scribble annotations offer a more cost-effective alternative by improving labeling efficiency. Nonetheless, using such sparse supervision for training reliable medical image segmentation models remains a significant challenge. Some studies employ pseudo-labeling to",
    "arxiv_url": "https://arxiv.org/abs/2411.10237v2",
    "pdf_url": "https://arxiv.org/pdf/2411.10237v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2411.10237",
    "arxiv_authors": [
      "Tao Wang",
      "Xinlin Zhang",
      "Zhenxuan Zhang",
      "Yuanbo Zhou",
      "Yuanbin Chen",
      "Longxuan Zhao",
      "Chaohui Xu",
      "Shun Chen",
      "Guang Yang",
      "Tong Tong"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=ScribbleVS%3A+Scribble-Supervised+Medical+Image+Segmentation+via+Dynamic+Competitive+Pseudo+Label+Selection+Tao+Wang+Xinlin+Zhang+Zhenxuan+Zhang+Yuanbo+Zhou+Yuanbin+Chen",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "T Wang",
        "id": "vdNg9q0AAAAJ"
      },
      {
        "name": "X Zhang",
        "id": "De0cZdQAAAAJ"
      },
      {
        "name": "Y Chen",
        "id": null
      },
      {
        "name": "Y Zhou",
        "id": "ZvgEfjEAAAAJ"
      },
      {
        "name": "L Zhao",
        "id": null
      },
      {
        "name": "T Tan",
        "id": "lLg3WRkAAAAJ"
      },
      {
        "name": "T Tong",
        "id": "zgFZ93sAAAAJ"
      }
    ],
    "citation_count": 1
  },
  {
    "arxiv_id": "2505.18546",
    "title": "ReflectGAN: Modeling Vegetation Effects for Soil Carbon Estimation from Satellite Imagery",
    "year": 2025,
    "published": "2025-05-24T06:26:38Z",
    "categories": [
      "eess.IV",
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "Soil organic carbon (SOC) is a critical indicator of soil health, but its accurate estimation from satellite imagery is hindered in vegetated regions due to spectral contamination from plant cover, which obscures soil reflectance and reduces model reliability. This study proposes the Reflectance Transformation Generative Adversarial Network (ReflectGAN), a novel paired GAN-based framework designed to reconstruct accurate bare soil reflectance from vegetated soil satellite observations. By learni",
    "arxiv_url": "https://arxiv.org/abs/2505.18546v1",
    "pdf_url": "https://arxiv.org/pdf/2505.18546v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2505.18546",
    "arxiv_authors": [
      "Dristi Datta",
      "Manoranjan Paul",
      "Manzur Murshed",
      "Shyh Wei Teng",
      "Leigh M. Schmidtke"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=ReflectGAN%3A+Modeling+Vegetation+Effects+for+Soil+Carbon+Estimation+from+Satellite+Imagery+Dristi+Datta+Manoranjan+Paul+Manzur+Murshed+Shyh+Wei+Teng+Leigh+M.+Schmidtke",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "D Datta",
        "id": "t2TMCtMAAAAJ"
      },
      {
        "name": "M Paul",
        "id": "dfBU3voAAAAJ"
      },
      {
        "name": "M Murshed",
        "id": "2546GSEAAAAJ"
      },
      {
        "name": "SW Teng",
        "id": null
      },
      {
        "name": "LM Schmidtke",
        "id": "X37KhLEAAAAJ"
      }
    ],
    "citation_count": null
  },
  {
    "arxiv_id": "2406.00704",
    "title": "An Optimized Toolbox for Advanced Image Processing with Tsetlin Machine Composites",
    "year": 2024,
    "published": "2024-06-02T10:52:48Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "abstract": "The Tsetlin Machine (TM) has achieved competitive results on several image classification benchmarks, including MNIST, K-MNIST, F-MNIST, and CIFAR-2. However, color image classification is arguably still in its infancy for TMs, with CIFAR-10 being a focal point for tracking progress. Over the past few years, TM's CIFAR-10 accuracy has increased from around 61% in 2020 to 75.1% in 2023 with the introduction of Drop Clause. In this paper, we leverage the recently proposed TM Composites architectur",
    "arxiv_url": "https://arxiv.org/abs/2406.00704v2",
    "pdf_url": "https://arxiv.org/pdf/2406.00704v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2406.00704",
    "arxiv_authors": [
      "Ylva Gr√∏nnings√¶ter",
      "Halvor S. Sm√∏rvik",
      "Ole-Christoffer Granmo"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=An+Optimized+Toolbox+for+Advanced+Image+Processing+with+Tsetlin+Machine+Composites+Ylva+Gr%C3%B8nnings%C3%A6ter+Halvor+S.+Sm%C3%B8rvik+Ole-Christoffer+Granmo",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "Y Gr√∏nnings√¶ter",
        "id": "BbI2CmYAAAAJ"
      },
      {
        "name": "HS Sm√∏rvik",
        "id": null
      },
      {
        "name": "OC Granmo2024 International",
        "id": null
      }
    ],
    "citation_count": 15
  },
  {
    "arxiv_id": "2409.06485",
    "title": "Mitigating Hallucination in Visual-Language Models via Re-Balancing Contrastive Decoding",
    "year": 2024,
    "published": "2024-09-10T13:13:14Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Although Visual-Language Models (VLMs) have shown impressive capabilities in tasks like visual question answering and image captioning, they still struggle with hallucinations. Analysis of attention distribution in these models shows that VLMs tend to processing textual tokens rather than visual tokens. This imbalance of attention distribution causes VLMs to favor textual knowledge in the case of multimodal knowledge conflicts, resulting in differences from the image information. In this paper, ",
    "arxiv_url": "https://arxiv.org/abs/2409.06485v1",
    "pdf_url": "https://arxiv.org/pdf/2409.06485v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2409.06485",
    "arxiv_authors": [
      "Xiaoyu Liang",
      "Jiayuan Yu",
      "Lianrui Mu",
      "Jiedong Zhuang",
      "Jiaqi Hu",
      "Yuchen Yang",
      "Jiangnan Ye",
      "Lu Lu",
      "Jian Chen",
      "Haoji Hu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Mitigating+Hallucination+in+Visual-Language+Models+via+Re-Balancing+Contrastive+Decoding+Xiaoyu+Liang+Jiayuan+Yu+Lianrui+Mu+Jiedong+Zhuang+Jiaqi+Hu",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "X Liang",
        "id": "cAitmk0AAAAJ"
      },
      {
        "name": "J Yu",
        "id": null
      },
      {
        "name": "L Mu",
        "id": "dCik-2YAAAAJ"
      },
      {
        "name": "J Zhuang",
        "id": null
      },
      {
        "name": "J Hu",
        "id": null
      },
      {
        "name": "Y Yang",
        "id": null
      },
      {
        "name": "J Ye",
        "id": null
      },
      {
        "name": "L Lu",
        "id": null
      },
      {
        "name": "J Chen",
        "id": null
      },
      {
        "name": "H HuChinese",
        "id": null
      }
    ],
    "citation_count": 1
  },
  {
    "arxiv_id": "2406.00432",
    "title": "Localize, Understand, Collaborate: Semantic-Aware Dragging via Intention Reasoner",
    "year": 2024,
    "published": "2024-06-01T13:10:43Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Flexible and accurate drag-based editing is a challenging task that has recently garnered significant attention. Current methods typically model this problem as automatically learning \"how to drag\" through point dragging and often produce one deterministic estimation, which presents two key limitations: 1) Overlooking the inherently ill-posed nature of drag-based editing, where multiple results may correspond to a given input, as illustrated in Fig.1; 2) Ignoring the constraint of image quality,",
    "arxiv_url": "https://arxiv.org/abs/2406.00432v2",
    "pdf_url": "https://arxiv.org/pdf/2406.00432v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2406.00432",
    "arxiv_authors": [
      "Xing Cui",
      "Peipei Li",
      "Zekun Li",
      "Xuannan Liu",
      "Yueying Zou",
      "Zhaofeng He"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Localize%2C+Understand%2C+Collaborate%3A+Semantic-Aware+Dragging+via+Intention+Reasoner+Xing+Cui+Peipei+Li+Zekun+Li+Xuannan+Liu+Yueying+Zou",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "X Cui",
        "id": null
      },
      {
        "name": "P Li",
        "id": "A0khpKYAAAAJ"
      },
      {
        "name": "Z Li",
        "id": "MD61m08AAAAJ"
      },
      {
        "name": "X Liu",
        "id": null
      },
      {
        "name": "Y Zou",
        "id": "parZLCgAAAAJ"
      },
      {
        "name": "Z HeAdvances in Neural Information Processing Systems",
        "id": null
      }
    ],
    "citation_count": 9
  },
  {
    "arxiv_id": "2412.06771",
    "title": "Proactive Agents for Multi-Turn Text-to-Image Generation Under Uncertainty",
    "year": 2024,
    "published": "2024-12-09T18:56:32Z",
    "categories": [
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "User prompts for generative AI models are often underspecified, leading to a misalignment between the user intent and models' understanding. As a result, users commonly have to painstakingly refine their prompts. We study this alignment problem in text-to-image (T2I) generation and propose a prototype for proactive T2I agents equipped with an interface to (1) actively ask clarification questions when uncertain, and (2) present their uncertainty about user intent as an understandable and editable",
    "arxiv_url": "https://arxiv.org/abs/2412.06771v3",
    "pdf_url": "https://arxiv.org/pdf/2412.06771v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2412.06771",
    "arxiv_authors": [
      "Meera Hahn",
      "Wenjun Zeng",
      "Nithish Kannen",
      "Rich Galt",
      "Kartikeya Badola",
      "Been Kim",
      "Zi Wang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Proactive+Agents+for+Multi-Turn+Text-to-Image+Generation+Under+Uncertainty+Meera+Hahn+Wenjun+Zeng+Nithish+Kannen+Rich+Galt+Kartikeya+Badola",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "M Hahn",
        "id": "XNXylX0AAAAJ"
      },
      {
        "name": "W Zeng",
        "id": "GKysgLsAAAAJ"
      },
      {
        "name": "N Kannen",
        "id": "nPQMsWMAAAAJ"
      },
      {
        "name": "R Galt",
        "id": null
      },
      {
        "name": "K Badola",
        "id": "1bXieIsAAAAJ"
      },
      {
        "name": "B Kim",
        "id": "aGXkhcwAAAAJ"
      },
      {
        "name": "Z Wang",
        "id": "U0egIsIAAAAJ"
      }
    ],
    "citation_count": 7
  },
  {
    "arxiv_id": "2306.10955",
    "title": "Semi-Supervised Learning for hyperspectral images by non parametrically predicting view assignment",
    "year": 2023,
    "published": "2023-06-19T14:13:56Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Hyperspectral image (HSI) classification is gaining a lot of momentum in present time because of high inherent spectral information within the images. However, these images suffer from the problem of curse of dimensionality and usually require a large number samples for tasks such as classification, especially in supervised setting. Recently, to effectively train the deep learning models with minimal labelled samples, the unlabeled samples are also being leveraged in self-supervised and semi-sup",
    "arxiv_url": "https://arxiv.org/abs/2306.10955v1",
    "pdf_url": "https://arxiv.org/pdf/2306.10955v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2306.10955",
    "arxiv_authors": [
      "Shivam Pande",
      "Nassim Ait Ali Braham",
      "Yi Wang",
      "Conrad M Albrecht",
      "Biplab Banerjee",
      "Xiao Xiang Zhu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Semi-Supervised+Learning+for+hyperspectral+images+by+non+parametrically+predicting+view+assignment+Shivam+Pande+Nassim+Ait+Ali+Braham+Yi+Wang+Conrad+M+Albrecht+Biplab+Banerjee",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "S Pande",
        "id": "PtjFjf8AAAAJ"
      },
      {
        "name": "NAA Braham",
        "id": null
      },
      {
        "name": "Y Wang",
        "id": null
      },
      {
        "name": "CM Albrecht",
        "id": null
      },
      {
        "name": "B Banerjee",
        "id": "IEcsMPAAAAAJ"
      },
      {
        "name": "XX ZhuIGARSS",
        "id": null
      }
    ],
    "citation_count": 1
  },
  {
    "arxiv_id": "2410.01089",
    "title": "FMBench: Benchmarking Fairness in Multimodal Large Language Models on Medical Tasks",
    "year": 2024,
    "published": "2024-10-01T21:38:15Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Advancements in Multimodal Large Language Models (MLLMs) have significantly improved medical task performance, such as Visual Question Answering (VQA) and Report Generation (RG). However, the fairness of these models across diverse demographic groups remains underexplored, despite its importance in healthcare. This oversight is partly due to the lack of demographic diversity in existing medical multimodal datasets, which complicates the evaluation of fairness. In response, we propose FMBench, th",
    "arxiv_url": "https://arxiv.org/abs/2410.01089v1",
    "pdf_url": "https://arxiv.org/pdf/2410.01089v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2410.01089",
    "arxiv_authors": [
      "Peiran Wu",
      "Che Liu",
      "Canyu Chen",
      "Jun Li",
      "Cosmin I. Bercea",
      "Rossella Arcucci"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=FMBench%3A+Benchmarking+Fairness+in+Multimodal+Large+Language+Models+on+Medical+Tasks+Peiran+Wu+Che+Liu+Canyu+Chen+Jun+Li+Cosmin+I.+Bercea",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "P Wu",
        "id": "t9-PPzQAAAAJ"
      },
      {
        "name": "C Liu",
        "id": null
      },
      {
        "name": "C Chen",
        "id": "iKfWNy0AAAAJ"
      },
      {
        "name": "J Li",
        "id": "t9vB5TgAAAAJ"
      },
      {
        "name": "CI Bercea",
        "id": "6M7srVcAAAAJ"
      },
      {
        "name": "R Arcucci",
        "id": "oxy2ZQoAAAAJ"
      }
    ],
    "citation_count": 11
  },
  {
    "arxiv_id": "2406.05113",
    "title": "LlavaGuard: An Open VLM-based Framework for Safeguarding Vision Datasets and Models",
    "year": 2024,
    "published": "2024-06-07T17:44:32Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "abstract": "This paper introduces LlavaGuard, a suite of VLM-based vision safeguards that address the critical need for reliable guardrails in the era of large-scale data and models. To this end, we establish a novel open framework, describing a customizable safety taxonomy, data preprocessing, augmentation, and training setup. For teaching a VLM safeguard on safety, we further create a multimodal safety dataset with high-quality human expert annotations, where each image is labeled with a safety rating, ca",
    "arxiv_url": "https://arxiv.org/abs/2406.05113v3",
    "pdf_url": "https://arxiv.org/pdf/2406.05113v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2406.05113",
    "arxiv_authors": [
      "Lukas Helff",
      "Felix Friedrich",
      "Manuel Brack",
      "Kristian Kersting",
      "Patrick Schramowski"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=LlavaGuard%3A+An+Open+VLM-based+Framework+for+Safeguarding+Vision+Datasets+and+Models+Lukas+Helff+Felix+Friedrich+Manuel+Brack+Kristian+Kersting+Patrick+Schramowski",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "L Helff",
        "id": "auJDLHgAAAAJ"
      },
      {
        "name": "F Friedrich",
        "id": "RfM9ud0AAAAJ"
      },
      {
        "name": "M Brack",
        "id": "kJ9Abf8AAAAJ"
      },
      {
        "name": "K Kersting",
        "id": "QY-earAAAAAJ"
      },
      {
        "name": "P Schramowski",
        "id": "GD481RkAAAAJ"
      }
    ],
    "citation_count": 9
  },
  {
    "arxiv_id": "2306.12929",
    "title": "Quantizable Transformers: Removing Outliers by Helping Attention Heads Do Nothing",
    "year": 2023,
    "published": "2023-06-22T14:39:04Z",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.CV"
    ],
    "abstract": "Transformer models have been widely adopted in various domains over the last years, and especially large language models have advanced the field of AI significantly. Due to their size, the capability of these networks has increased tremendously, but this has come at the cost of a significant increase in necessary compute. Quantization is one of the most effective ways to reduce the computational time and memory consumption of neural networks. Many studies have shown, however, that modern transfo",
    "arxiv_url": "https://arxiv.org/abs/2306.12929v2",
    "pdf_url": "https://arxiv.org/pdf/2306.12929v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2306.12929",
    "arxiv_authors": [
      "Yelysei Bondarenko",
      "Markus Nagel",
      "Tijmen Blankevoort"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Quantizable+Transformers%3A+Removing+Outliers+by+Helping+Attention+Heads+Do+Nothing+Yelysei+Bondarenko+Markus+Nagel+Tijmen+Blankevoort",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "Y Bondarenko",
        "id": "4mHNa28AAAAJ"
      },
      {
        "name": "M Nagel",
        "id": "akNuBBEAAAAJ"
      },
      {
        "name": "T BlankevoortAdvances in Neural Information Processing Systems",
        "id": null
      }
    ],
    "citation_count": 131
  },
  {
    "arxiv_id": "2402.05106",
    "title": "Image captioning for Brazilian Portuguese using GRIT model",
    "year": 2024,
    "published": "2024-02-07T18:57:37Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "abstract": "This work presents the early development of a model of image captioning for the Brazilian Portuguese language. We used the GRIT (Grid - and Region-based Image captioning Transformer) model to accomplish this work. GRIT is a Transformer-only neural architecture that effectively utilizes two visual features to generate better captions. The GRIT method emerged as a proposal to be a more efficient way to generate image captioning. In this work, we adapt the GRIT model to be trained in a Brazilian Po",
    "arxiv_url": "https://arxiv.org/abs/2402.05106v1",
    "pdf_url": "https://arxiv.org/pdf/2402.05106v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2402.05106",
    "arxiv_authors": [
      "Rafael Silva de Alencar",
      "William Alberto Cruz Casta√±eda",
      "Marcellus Amadeus"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Image+captioning+for+Brazilian+Portuguese+using+GRIT+model+Rafael+Silva+de+Alencar+William+Alberto+Cruz+Casta%C3%B1eda+Marcellus+Amadeus",
    "gs_search_success": false,
    "gs_authors": null,
    "citation_count": null,
    "error_type": "fetch_failed"
  },
  {
    "arxiv_id": "2402.03214",
    "title": "Organic or Diffused: Can We Distinguish Human Art from AI-generated Images?",
    "year": 2024,
    "published": "2024-02-05T17:25:04Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "abstract": "The advent of generative AI images has completely disrupted the art world. Distinguishing AI generated images from human art is a challenging problem whose impact is growing over time. A failure to address this problem allows bad actors to defraud individuals paying a premium for human art and companies whose stated policies forbid AI imagery. It is also critical for content owners to establish copyright, and for model trainers interested in curating training data in order to avoid potential mod",
    "arxiv_url": "https://arxiv.org/abs/2402.03214v3",
    "pdf_url": "https://arxiv.org/pdf/2402.03214v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2402.03214",
    "arxiv_authors": [
      "Anna Yoo Jeong Ha",
      "Josephine Passananti",
      "Ronik Bhaskar",
      "Shawn Shan",
      "Reid Southen",
      "Haitao Zheng",
      "Ben Y. Zhao"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Organic+or+Diffused%3A+Can+We+Distinguish+Human+Art+from+AI-generated+Images%3F+Anna+Yoo+Jeong+Ha+Josephine+Passananti+Ronik+Bhaskar+Shawn+Shan+Reid+Southen",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "AYJ Ha",
        "id": "SSFR7zYAAAAJ"
      },
      {
        "name": "J Passananti",
        "id": "_Gl3_5kAAAAJ"
      },
      {
        "name": "R Bhaskar",
        "id": "dqTwtegAAAAJ"
      },
      {
        "name": "S Shan",
        "id": "vonYQc4AAAAJ"
      },
      {
        "name": "R Southen",
        "id": null
      },
      {
        "name": "H Zheng",
        "id": "XrIr0nYAAAAJ"
      },
      {
        "name": "BY Zhao",
        "id": "cYReSuEAAAAJ"
      }
    ],
    "citation_count": 51
  },
  {
    "arxiv_id": "2305.05732",
    "title": "Duke Spleen Data Set: A Publicly Available Spleen MRI and CT dataset for Training Segmentation",
    "year": 2023,
    "published": "2023-05-09T19:24:09Z",
    "categories": [
      "eess.IV",
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "Spleen volumetry is primarily associated with patients suffering from chronic liver disease and portal hypertension, as they often have spleens with abnormal shapes and sizes. However, manually segmenting the spleen to obtain its volume is a time-consuming process. Deep learning algorithms have proven to be effective in automating spleen segmentation, but a suitable dataset is necessary for training such algorithms. To our knowledge, the few publicly available datasets for spleen segmentation la",
    "arxiv_url": "https://arxiv.org/abs/2305.05732v1",
    "pdf_url": "https://arxiv.org/pdf/2305.05732v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2305.05732",
    "arxiv_authors": [
      "Yuqi Wang",
      "Jacob A. Macdonald",
      "Katelyn R. Morgan",
      "Danielle Hom",
      "Sarah Cubberley",
      "Kassi Sollace",
      "Nicole Casasanto",
      "Islam H. Zaki",
      "Kyle J. Lafata",
      "Mustafa R. Bashir"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Duke+Spleen+Data+Set%3A+A+Publicly+Available+Spleen+MRI+and+CT+dataset+for+Training+Segmentation+Yuqi+Wang+Jacob+A.+Macdonald+Katelyn+R.+Morgan+Danielle+Hom+Sarah+Cubberley",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "Y Wang",
        "id": "-4XmJMEAAAAJ"
      },
      {
        "name": "JA Macdonald",
        "id": null
      },
      {
        "name": "KR Morgan",
        "id": null
      },
      {
        "name": "D Hom",
        "id": null
      },
      {
        "name": "S Cubberley",
        "id": null
      },
      {
        "name": "K Sollace",
        "id": null
      },
      {
        "name": "N Casasanto",
        "id": null
      },
      {
        "name": "IH Zaki",
        "id": "s1ySXlgAAAAJ"
      }
    ],
    "citation_count": 3
  },
  {
    "arxiv_id": "2308.03151",
    "title": "Food-500 Cap: A Fine-Grained Food Caption Benchmark for Evaluating Vision-Language Models",
    "year": 2023,
    "published": "2023-08-06T15:56:31Z",
    "categories": [
      "cs.CV",
      "cs.CL"
    ],
    "abstract": "Vision-language models (VLMs) have shown impressive performance in substantial downstream multi-modal tasks. However, only comparing the fine-tuned performance on downstream tasks leads to the poor interpretability of VLMs, which is adverse to their future improvement. Several prior works have identified this issue and used various probing methods under a zero-shot setting to detect VLMs' limitations, but they all examine VLMs using general datasets instead of specialized ones. In practical appl",
    "arxiv_url": "https://arxiv.org/abs/2308.03151v1",
    "pdf_url": "https://arxiv.org/pdf/2308.03151v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2308.03151",
    "arxiv_authors": [
      "Zheng Ma",
      "Mianzhi Pan",
      "Wenhan Wu",
      "Kanzhi Cheng",
      "Jianbing Zhang",
      "Shujian Huang",
      "Jiajun Chen"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Food-500+Cap%3A+A+Fine-Grained+Food+Caption+Benchmark+for+Evaluating+Vision-Language+Models+Zheng+Ma+Mianzhi+Pan+Wenhan+Wu+Kanzhi+Cheng+Jianbing+Zhang",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "Z Ma",
        "id": "8WfKu9UAAAAJ"
      },
      {
        "name": "M Pan",
        "id": "fcB2s-oAAAAJ"
      },
      {
        "name": "W Wu",
        "id": null
      },
      {
        "name": "K Cheng",
        "id": "S2IPVnwAAAAJ"
      },
      {
        "name": "J Zhang",
        "id": "lZ_hw9oAAAAJ"
      },
      {
        "name": "S Huang",
        "id": "HF3-E9kAAAAJ"
      },
      {
        "name": "J Chen",
        "id": null
      }
    ],
    "citation_count": 11
  },
  {
    "arxiv_id": "2406.03298",
    "title": "L-PR: Exploiting LiDAR Fiducial Marker for Unordered Low Overlap Multiview Point Cloud Registration",
    "year": 2024,
    "published": "2024-06-05T14:08:13Z",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "abstract": "Point cloud registration is a prerequisite for many applications in computer vision and robotics. Most existing methods focus on pairwise registration of two point clouds with high overlap. Although there have been some methods for low overlap cases, they struggle in degraded scenarios. This paper introduces a novel framework dubbed L-PR, designed to register unordered low overlap multiview point clouds leveraging LiDAR fiducial markers. We refer to them as LiDAR fiducial markers, but they are t",
    "arxiv_url": "https://arxiv.org/abs/2406.03298v3",
    "pdf_url": "https://arxiv.org/pdf/2406.03298v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2406.03298",
    "arxiv_authors": [
      "Yibo Liu",
      "Jinjun Shan",
      "Amaldev Haridevan",
      "Shuo Zhang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=L-PR%3A+Exploiting+LiDAR+Fiducial+Marker+for+Unordered+Low+Overlap+Multiview+Point+Cloud+Registration+Yibo+Liu+Jinjun+Shan+Amaldev+Haridevan+Shuo+Zhang",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "Y Liu",
        "id": "zqDgDQ4AAAAJ"
      },
      {
        "name": "J Shan",
        "id": "-Xt2r3DqP3AC"
      },
      {
        "name": "A Haridevan",
        "id": "LzoD85kAAAAJ"
      },
      {
        "name": "S ZhangIEEE Transactions on Instrumentation and Measurement",
        "id": null
      }
    ],
    "citation_count": 5
  },
  {
    "arxiv_id": "2307.11567",
    "title": "CortexMorph: fast cortical thickness estimation via diffeomorphic registration using VoxelMorph",
    "year": 2023,
    "published": "2023-07-21T13:18:43Z",
    "categories": [
      "eess.IV",
      "cs.CV",
      "q-bio.QM"
    ],
    "abstract": "The thickness of the cortical band is linked to various neurological and psychiatric conditions, and is often estimated through surface-based methods such as Freesurfer in MRI studies. The DiReCT method, which calculates cortical thickness using a diffeomorphic deformation of the gray-white matter interface towards the pial surface, offers an alternative to surface-based methods. Recent studies using a synthetic cortical thickness phantom have demonstrated that the combination of DiReCT and deep",
    "arxiv_url": "https://arxiv.org/abs/2307.11567v1",
    "pdf_url": "https://arxiv.org/pdf/2307.11567v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2307.11567",
    "arxiv_authors": [
      "Richard McKinley",
      "Christian Rummel"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=CortexMorph%3A+fast+cortical+thickness+estimation+via+diffeomorphic+registration+using+VoxelMorph+Richard+McKinley+Christian+Rummel",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "R McKinley",
        "id": "MVFfMZcAAAAJ"
      },
      {
        "name": "C Rummel -",
        "id": null
      }
    ],
    "citation_count": 4
  },
  {
    "arxiv_id": "2303.09187",
    "title": "PSVT: End-to-End Multi-person 3D Pose and Shape Estimation with Progressive Video Transformers",
    "year": 2023,
    "published": "2023-03-16T09:55:43Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Existing methods of multi-person video 3D human Pose and Shape Estimation (PSE) typically adopt a two-stage strategy, which first detects human instances in each frame and then performs single-person PSE with temporal model. However, the global spatio-temporal context among spatial instances can not be captured. In this paper, we propose a new end-to-end multi-person 3D Pose and Shape estimation framework with progressive Video Transformer, termed PSVT. In PSVT, a spatio-temporal encoder (STE) c",
    "arxiv_url": "https://arxiv.org/abs/2303.09187v1",
    "pdf_url": "https://arxiv.org/pdf/2303.09187v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2303.09187",
    "arxiv_authors": [
      "Zhongwei Qiu",
      "Yang Qiansheng",
      "Jian Wang",
      "Haocheng Feng",
      "Junyu Han",
      "Errui Ding",
      "Chang Xu",
      "Dongmei Fu",
      "Jingdong Wang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=PSVT%3A+End-to-End+Multi-person+3D+Pose+and+Shape+Estimation+with+Progressive+Video+Transformers+Zhongwei+Qiu+Yang+Qiansheng+Jian+Wang+Haocheng+Feng+Junyu+Han",
    "gs_search_success": true,
    "gs_authors": [
      {
        "name": "Z Qiu",
        "id": "uVV3rqcAAAAJ"
      },
      {
        "name": "Q Yang",
        "id": "4eKdyxEAAAAJ"
      },
      {
        "name": "J Wang",
        "id": "z5SPCmgAAAAJ"
      },
      {
        "name": "H Feng",
        "id": "pnuQ5UsAAAAJ"
      },
      {
        "name": "J Han",
        "id": null
      },
      {
        "name": "E Ding",
        "id": "1wzEtxcAAAAJ"
      },
      {
        "name": "C Xu",
        "id": "N4F_3eoAAAAJ"
      },
      {
        "name": "D Fu",
        "id": null
      },
      {
        "name": "J Wang",
        "id": "z5SPCmgAAAAJ"
      }
    ],
    "citation_count": 49
  }
]