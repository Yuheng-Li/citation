[
  {
    "arxiv_id": "2503.13184",
    "title": "Triad: Empowering LMM-based Anomaly Detection with Vision Expert-guided Visual Tokenizer and Manufacturing Process",
    "year": 2025,
    "published": "2025-03-17T13:56:57Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Although recent methods have tried to introduce large multimodal models (LMMs) into industrial anomaly detection (IAD), their generalization in the IAD field is far inferior to that for general purposes. We summarize the main reasons for this gap into two aspects. On one hand, general-purpose LMMs lack cognition of defects in the visual modality, thereby failing to sufficiently focus on defect areas. Therefore, we propose to modify the AnyRes structure of the LLaVA model, providing the potential",
    "arxiv_url": "https://arxiv.org/abs/2503.13184v2",
    "pdf_url": "https://arxiv.org/pdf/2503.13184v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2503.13184",
    "arxiv_authors": [
      "Yuanze Li",
      "Shihao Yuan",
      "Haolin Wang",
      "Qizhang Li",
      "Ming Liu",
      "Chen Xu",
      "Guangming Shi",
      "Wangmeng Zuo"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Triad%3A+Empowering+LMM-based+Anomaly+Detection+with+Vision+Expert-guided+Visual+Tokenizer+and+Manufacturing+Process+Yuanze+Li+Shihao+Yuan+Haolin+Wang+Qizhang+Li+Ming+Liu",
    "gs_search_success": true,
    "gs_authors": [
      "CdV5LfQAAAAJ",
      "rUOpCEYAAAAJ"
    ],
    "citation_count": 3,
    "gs_author_count": 8
  },
  {
    "arxiv_id": "2503.06382",
    "title": "X-LRM: X-ray Large Reconstruction Model for Extremely Sparse-View Computed Tomography Recovery in One Second",
    "year": 2025,
    "published": "2025-03-09T01:39:59Z",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "abstract": "Sparse-view 3D CT reconstruction aims to recover volumetric structures from a limited number of 2D X-ray projections. Existing feedforward methods are constrained by the limited capacity of CNN-based architectures and the scarcity of large-scale training datasets. In this paper, we propose an X-ray Large Reconstruction Model (X-LRM) for extremely sparse-view (<10 views) CT reconstruction. X-LRM consists of two key components: X-former and X-triplane. Our X-former can handle an arbitrary number o",
    "arxiv_url": "https://arxiv.org/abs/2503.06382v1",
    "pdf_url": "https://arxiv.org/pdf/2503.06382v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2503.06382",
    "arxiv_authors": [
      "Guofeng Zhang",
      "Ruyi Zha",
      "Hao He",
      "Yixun Liang",
      "Alan Yuille",
      "Hongdong Li",
      "Yuanhao Cai"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=X-LRM%3A+X-ray+Large+Reconstruction+Model+for+Extremely+Sparse-View+Computed+Tomography+Recovery+in+One+Second+Guofeng+Zhang+Ruyi+Zha+Hao+He+Yixun+Liang+Alan+Yuille",
    "gs_search_success": true,
    "gs_authors": [
      "rmX3lvEAAAAJ",
      "vl0mzhEAAAAJ",
      "lOEX3aUAAAAJ",
      "_5W6W7oAAAAJ",
      "FJ-huxgAAAAJ",
      "3YozQwcAAAAJ"
    ],
    "citation_count": 3,
    "gs_author_count": 7
  },
  {
    "arxiv_id": "2501.03729",
    "title": "Realistic Test-Time Adaptation of Vision-Language Models",
    "year": 2025,
    "published": "2025-01-07T12:17:25Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "The zero-shot capabilities of Vision-Language Models (VLMs) have been widely leveraged to improve predictive performance. However, previous works on transductive or test-time adaptation (TTA) often make strong assumptions about the data distribution, such as the presence of all classes. Our work challenges these favorable deployment scenarios, and introduces a more realistic evaluation framework, including: (i) a variable number of effective classes for adaptation within a single batch, and (ii)",
    "arxiv_url": "https://arxiv.org/abs/2501.03729v1",
    "pdf_url": "https://arxiv.org/pdf/2501.03729v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2501.03729",
    "arxiv_authors": [
      "Maxime Zanella",
      "Clément Fuchs",
      "Christophe De Vleeschouwer",
      "Ismail Ben Ayed"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Realistic+Test-Time+Adaptation+of+Vision-Language+Models+Maxime+Zanella+Cl%C3%A9ment+Fuchs+Christophe+De+Vleeschouwer+Ismail+Ben+Ayed",
    "gs_search_success": true,
    "gs_authors": [
      "xb3Zc3cAAAAJ",
      "FIoE9YIAAAAJ",
      "ZXWUJ4QAAAAJ",
      "29vyUccAAAAJ"
    ],
    "citation_count": 7,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2403.10452",
    "title": "Robust Shape Fitting for 3D Scene Abstraction",
    "year": 2024,
    "published": "2024-03-15T16:37:43Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Humans perceive and construct the world as an arrangement of simple parametric models. In particular, we can often describe man-made environments using volumetric primitives such as cuboids or cylinders. Inferring these primitives is important for attaining high-level, abstract scene descriptions. Previous approaches for primitive-based abstraction estimate shape parameters directly and are only able to reproduce simple objects. In contrast, we propose a robust estimator for primitive fitting, w",
    "arxiv_url": "https://arxiv.org/abs/2403.10452v1",
    "pdf_url": "https://arxiv.org/pdf/2403.10452v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2403.10452",
    "arxiv_authors": [
      "Florian Kluger",
      "Eric Brachmann",
      "Michael Ying Yang",
      "Bodo Rosenhahn"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Robust+Shape+Fitting+for+3D+Scene+Abstraction+Florian+Kluger+Eric+Brachmann+Michael+Ying+Yang+Bodo+Rosenhahn",
    "gs_search_success": true,
    "gs_authors": [
      "qq3TxtcAAAAJ",
      "cAIshsYAAAAJ",
      "lgGmYBoAAAAJ",
      "KNIzPH8AAAAJ"
    ],
    "citation_count": 6,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2501.01101",
    "title": "Deformable Gaussian Splatting for Efficient and High-Fidelity Reconstruction of Surgical Scenes",
    "year": 2025,
    "published": "2025-01-02T06:50:25Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Efficient and high-fidelity reconstruction of deformable surgical scenes is a critical yet challenging task. Building on recent advancements in 3D Gaussian splatting, current methods have seen significant improvements in both reconstruction quality and rendering speed. However, two major limitations remain: (1) difficulty in handling irreversible dynamic changes, such as tissue shearing, which are common in surgical scenes; and (2) the lack of hierarchical modeling for surgical scene deformation",
    "arxiv_url": "https://arxiv.org/abs/2501.01101v1",
    "pdf_url": "https://arxiv.org/pdf/2501.01101v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2501.01101",
    "arxiv_authors": [
      "Jiwei Shan",
      "Zeyu Cai",
      "Cheng-Tai Hsieh",
      "Shing Shin Cheng",
      "Hesheng Wang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Deformable+Gaussian+Splatting+for+Efficient+and+High-Fidelity+Reconstruction+of+Surgical+Scenes+Jiwei+Shan+Zeyu+Cai+Cheng-Tai+Hsieh+Shing+Shin+Cheng+Hesheng+Wang",
    "gs_search_success": true,
    "gs_authors": [
      "8VYntc0AAAAJ",
      "F-ucRegAAAAJ",
      "s4AaiW4AAAAJ",
      "q6AY9XsAAAAJ"
    ],
    "citation_count": 7,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2410.05301",
    "title": "Diffusion-based Unsupervised Audio-visual Speech Enhancement",
    "year": 2024,
    "published": "2024-10-04T12:22:54Z",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.CV",
      "cs.LG",
      "eess.AS",
      "eess.SP"
    ],
    "abstract": "This paper proposes a new unsupervised audio-visual speech enhancement (AVSE) approach that combines a diffusion-based audio-visual speech generative model with a non-negative matrix factorization (NMF) noise model. First, the diffusion model is pre-trained on clean speech conditioned on corresponding video data to simulate the speech generative distribution. This pre-trained model is then paired with the NMF-based noise model to estimate clean speech iteratively. Specifically, a diffusion-based",
    "arxiv_url": "https://arxiv.org/abs/2410.05301v2",
    "pdf_url": "https://arxiv.org/pdf/2410.05301v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2410.05301",
    "arxiv_authors": [
      "Jean-Eudes Ayilo",
      "Mostafa Sadeghi",
      "Romain Serizel",
      "Xavier Alameda-Pineda"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Diffusion-based+Unsupervised+Audio-visual+Speech+Enhancement+Jean-Eudes+Ayilo+Mostafa+Sadeghi+Romain+Serizel+Xavier+Alameda-Pineda",
    "gs_search_success": true,
    "gs_authors": [
      "_PXk20cAAAAJ",
      "ukI2bz8AAAAJ",
      "gSQ5xpcAAAAJ"
    ],
    "citation_count": 6,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2406.13006",
    "title": "Weighted Sum of Segmented Correlation: An Efficient Method for Spectra Matching in Hyperspectral Images",
    "year": 2024,
    "published": "2024-06-18T18:51:00Z",
    "categories": [
      "cs.CV",
      "cs.ET",
      "eess.IV"
    ],
    "abstract": "Matching a target spectrum with known spectra in a spectral library is a common method for material identification in hyperspectral imaging research. Hyperspectral spectra exhibit precise absorption features across different wavelength segments, and the unique shapes and positions of these absorptions create distinct spectral signatures for each material, aiding in their identification. Therefore, only the specific positions can be considered for material identification. This study introduces th",
    "arxiv_url": "https://arxiv.org/abs/2406.13006v1",
    "pdf_url": "https://arxiv.org/pdf/2406.13006v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2406.13006",
    "arxiv_authors": [
      "Sampriti Soor",
      "Priyanka Kumari",
      "B. S. Daya Sagar",
      "Amba Shetty"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Weighted+Sum+of+Segmented+Correlation%3A+An+Efficient+Method+for+Spectra+Matching+in+Hyperspectral+Images+Sampriti+Soor+Priyanka+Kumari+B.+S.+Daya+Sagar+Amba+Shetty",
    "gs_search_success": true,
    "gs_authors": [
      "epOfTAYAAAAJ",
      "MDqXWJAAAAAJ",
      "2_rp5LIAAAAJ",
      "kd6hGOkAAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2504.14717",
    "title": "TAPIP3D: Tracking Any Point in Persistent 3D Geometry",
    "year": 2025,
    "published": "2025-04-20T19:09:43Z",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "We introduce TAPIP3D, a novel approach for long-term 3D point tracking in monocular RGB and RGB-D videos. TAPIP3D represents videos as camera-stabilized spatio-temporal feature clouds, leveraging depth and camera motion information to lift 2D video features into a 3D world space where camera movement is effectively canceled out. Within this stabilized 3D representation, TAPIP3D iteratively refines multi-frame motion estimates, enabling robust point tracking over long time horizons. To handle the",
    "arxiv_url": "https://arxiv.org/abs/2504.14717v3",
    "pdf_url": "https://arxiv.org/pdf/2504.14717v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2504.14717",
    "arxiv_authors": [
      "Bowei Zhang",
      "Lei Ke",
      "Adam W. Harley",
      "Katerina Fragkiadaki"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=TAPIP3D%3A+Tracking+Any+Point+in+Persistent+3D+Geometry+Bowei+Zhang+Lei+Ke+Adam+W.+Harley+Katerina+Fragkiadaki",
    "gs_search_success": true,
    "gs_authors": [
      "tYH72AYAAAAJ",
      "FWp7728AAAAJ",
      "OB6vAtkAAAAJ",
      "WseeNrUAAAAJ"
    ],
    "citation_count": 17,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2407.04819",
    "title": "RPN: Reconciled Polynomial Network Towards Unifying PGMs, Kernel SVMs, MLP and KAN",
    "year": 2024,
    "published": "2024-07-05T19:00:18Z",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "cs.IT",
      "stat.ML"
    ],
    "abstract": "In this paper, we will introduce a novel deep model named Reconciled Polynomial Network (RPN) for deep function learning. RPN has a very general architecture and can be used to build models with various complexities, capacities, and levels of completeness, which all contribute to the correctness of these models. As indicated in the subtitle, RPN can also serve as the backbone to unify different base models into one canonical representation. This includes non-deep models, like probabilistic graph",
    "arxiv_url": "https://arxiv.org/abs/2407.04819v1",
    "pdf_url": "https://arxiv.org/pdf/2407.04819v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2407.04819",
    "arxiv_authors": [
      "Jiawei Zhang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=RPN%3A+Reconciled+Polynomial+Network+Towards+Unifying+PGMs%2C+Kernel+SVMs%2C+MLP+and+KAN+Jiawei+Zhang",
    "gs_search_success": true,
    "gs_authors": [
      "7AkZSJsAAAAJ"
    ],
    "citation_count": 7,
    "gs_author_count": 1
  },
  {
    "arxiv_id": "2501.10144",
    "title": "A Vision-Language Framework for Multispectral Scene Representation Using Language-Grounded Features",
    "year": 2025,
    "published": "2025-01-17T12:12:33Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Scene understanding in remote sensing often faces challenges in generating accurate representations for complex environments such as various land use areas or coastal regions, which may also include snow, clouds, or haze. To address this, we present a vision-language framework named Spectral LLaVA, which integrates multispectral data with vision-language alignment techniques to enhance scene representation and description. Using the BigEarthNet v2 dataset from Sentinel-2, we establish a baseline",
    "arxiv_url": "https://arxiv.org/abs/2501.10144v1",
    "pdf_url": "https://arxiv.org/pdf/2501.10144v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2501.10144",
    "arxiv_authors": [
      "Enes Karanfil",
      "Nevrez Imamoglu",
      "Erkut Erdem",
      "Aykut Erdem"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=A+Vision-Language+Framework+for+Multispectral+Scene+Representation+Using+Language-Grounded+Features+Enes+Karanfil+Nevrez+Imamoglu+Erkut+Erdem+Aykut+Erdem",
    "gs_search_success": true,
    "gs_authors": [
      "-xA1_OAAAAAJ",
      "VJgx61MAAAAJ",
      "eALwl74AAAAJ",
      "J7UN2eoAAAAJ"
    ],
    "citation_count": 2,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2407.11566",
    "title": "TGIF: Text-Guided Inpainting Forgery Dataset",
    "year": 2024,
    "published": "2024-07-16T10:19:14Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CR",
      "cs.MM"
    ],
    "abstract": "Digital image manipulation has become increasingly accessible and realistic with the advent of generative AI technologies. Recent developments allow for text-guided inpainting, making sophisticated image edits possible with minimal effort. This poses new challenges for digital media forensics. For example, diffusion model-based approaches could either splice the inpainted region into the original image, or regenerate the entire image. In the latter case, traditional image forgery localization (I",
    "arxiv_url": "https://arxiv.org/abs/2407.11566v2",
    "pdf_url": "https://arxiv.org/pdf/2407.11566v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2407.11566",
    "arxiv_authors": [
      "Hannes Mareen",
      "Dimitrios Karageorgiou",
      "Glenn Van Wallendael",
      "Peter Lambert",
      "Symeon Papadopoulos"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=TGIF%3A+Text-Guided+Inpainting+Forgery+Dataset+Hannes+Mareen+Dimitrios+Karageorgiou+Glenn+Van+Wallendael+Peter+Lambert+Symeon+Papadopoulos",
    "gs_search_success": true,
    "gs_authors": [
      "SNmMEJEAAAAJ",
      "GuhyORoAAAAJ",
      "0Y690dMAAAAJ",
      "kFxdDCkAAAAJ",
      "CaDakRwAAAAJ"
    ],
    "citation_count": 13,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2411.09037",
    "title": "Pay Attention to the Keys: Visual Piano Transcription Using Transformers",
    "year": 2024,
    "published": "2024-11-13T21:31:12Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Visual piano transcription (VPT) is the task of obtaining a symbolic representation of a piano performance from visual information alone (e.g., from a top-down video of the piano keyboard). In this work we propose a VPT system based on the vision transformer (ViT), which surpasses previous methods based on convolutional neural networks (CNNs). Our system is trained on the newly introduced R3 dataset, consisting of ca.~31 hours of synchronized video and MIDI recordings of piano performances. We a",
    "arxiv_url": "https://arxiv.org/abs/2411.09037v2",
    "pdf_url": "https://arxiv.org/pdf/2411.09037v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2411.09037",
    "arxiv_authors": [
      "Uros Zivanovic",
      "Ivan Pilkov",
      "Carlos Eduardo Cancino-Chacón"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Pay+Attention+to+the+Keys%3A+Visual+Piano+Transcription+Using+Transformers+Uros+Zivanovic+Ivan+Pilkov+Carlos+Eduardo+Cancino-Chac%C3%B3n",
    "gs_search_success": false,
    "gs_authors": [],
    "citation_count": null,
    "error_type": "fetch_failed",
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2501.05236",
    "title": "Automated external cervical resorption segmentation in cone-beam CT using local texture features",
    "year": 2025,
    "published": "2025-01-09T13:43:01Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "External cervical resorption (ECR) is a resorptive process affecting teeth. While in some patients, active resorption ceases and gets replaced by osseous tissue, in other cases, the resorption progresses and ultimately results in tooth loss. For proper ECR assessment, cone-beam computed tomography (CBCT) is the recommended imaging modality, enabling a 3-D characterization of these lesions. While it is possible to manually identify and measure ECR resorption in CBCT scans, this process can be tim",
    "arxiv_url": "https://arxiv.org/abs/2501.05236v1",
    "pdf_url": "https://arxiv.org/pdf/2501.05236v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2501.05236",
    "arxiv_authors": [
      "Sadhana Ravikumar",
      "Asma A. Khan",
      "Matthew C. Davis",
      "Beatriz Paniagua"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Automated+external+cervical+resorption+segmentation+in+cone-beam+CT+using+local+texture+features+Sadhana+Ravikumar+Asma+A.+Khan+Matthew+C.+Davis+Beatriz+Paniagua",
    "gs_search_success": true,
    "gs_authors": [
      "N3etvb4AAAAJ",
      "TxK5raEAAAAJ",
      "PRVRWcwAAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2310.06945",
    "title": "End-to-end Evaluation of Practical Video Analytics Systems for Face Detection and Recognition",
    "year": 2023,
    "published": "2023-10-10T19:06:10Z",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "abstract": "Practical video analytics systems that are deployed in bandwidth constrained environments like autonomous vehicles perform computer vision tasks such as face detection and recognition. In an end-to-end face analytics system, inputs are first compressed using popular video codecs like HEVC and then passed onto modules that perform face detection, alignment, and recognition sequentially. Typically, the modules of these systems are evaluated independently using task-specific imbalanced datasets tha",
    "arxiv_url": "https://arxiv.org/abs/2310.06945v1",
    "pdf_url": "https://arxiv.org/pdf/2310.06945v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2310.06945",
    "arxiv_authors": [
      "Praneet Singh",
      "Edward J. Delp",
      "Amy R. Reibman"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=End-to-end+Evaluation+of+Practical+Video+Analytics+Systems+for+Face+Detection+and+Recognition+Praneet+Singh+Edward+J.+Delp+Amy+R.+Reibman",
    "gs_search_success": true,
    "gs_authors": [
      "TUX6cg8AAAAJ",
      "owurYNEAAAAJ",
      "qxs5pc4AAAAJ"
    ],
    "citation_count": 5,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2407.20917",
    "title": "How to Choose a Reinforcement-Learning Algorithm",
    "year": 2024,
    "published": "2024-07-30T15:54:18Z",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "stat.ML"
    ],
    "abstract": "The field of reinforcement learning offers a large variety of concepts and methods to tackle sequential decision-making problems. This variety has become so large that choosing an algorithm for a task at hand can be challenging. In this work, we streamline the process of choosing reinforcement-learning algorithms and action-distribution families. We provide a structured overview of existing methods and their properties, as well as guidelines for when to choose which methods. An interactive versi",
    "arxiv_url": "https://arxiv.org/abs/2407.20917v1",
    "pdf_url": "https://arxiv.org/pdf/2407.20917v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2407.20917",
    "arxiv_authors": [
      "Fabian Bongratz",
      "Vladimir Golkov",
      "Lukas Mautner",
      "Luca Della Libera",
      "Frederik Heetmeyer",
      "Felix Czaja",
      "Julian Rodemann",
      "Daniel Cremers"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=How+to+Choose+a+Reinforcement-Learning+Algorithm+Fabian+Bongratz+Vladimir+Golkov+Lukas+Mautner+Luca+Della+Libera+Frederik+Heetmeyer",
    "gs_search_success": true,
    "gs_authors": [
      "fLjta2AAAAAJ",
      "k-vHBED9_CgC",
      "hIcjYocAAAAJ",
      "mylZOd4AAAAJ",
      "aQzZhi8AAAAJ",
      "rXQPW74AAAAJ"
    ],
    "citation_count": 2,
    "gs_author_count": 8
  },
  {
    "arxiv_id": "2404.05069",
    "title": "AirShot: Efficient Few-Shot Detection for Autonomous Exploration",
    "year": 2024,
    "published": "2024-04-07T20:39:31Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Few-shot object detection has drawn increasing attention in the field of robotic exploration, where robots are required to find unseen objects with a few online provided examples. Despite recent efforts have been made to yield online processing capabilities, slow inference speeds of low-powered robots fail to meet the demands of real-time detection-making them impractical for autonomous exploration. Existing methods still face performance and efficiency challenges, mainly due to unreliable featu",
    "arxiv_url": "https://arxiv.org/abs/2404.05069v1",
    "pdf_url": "https://arxiv.org/pdf/2404.05069v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2404.05069",
    "arxiv_authors": [
      "Zihan Wang",
      "Bowen Li",
      "Chen Wang",
      "Sebastian Scherer"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=AirShot%3A+Efficient+Few-Shot+Detection+for+Autonomous+Exploration+Zihan+Wang+Bowen+Li+Chen+Wang+Sebastian+Scherer",
    "gs_search_success": true,
    "gs_authors": [
      "VsCA3oQAAAAJ",
      "gxoPfIYAAAAJ",
      "XIAMHVMAAAAJ",
      "vZfmKl4AAAAJ"
    ],
    "citation_count": 14,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2309.05793",
    "title": "PhotoVerse: Tuning-Free Image Customization with Text-to-Image Diffusion Models",
    "year": 2023,
    "published": "2023-09-11T19:59:43Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Personalized text-to-image generation has emerged as a powerful and sought-after tool, empowering users to create customized images based on their specific concepts and prompts. However, existing approaches to personalization encounter multiple challenges, including long tuning times, large storage requirements, the necessity for multiple input images per identity, and limitations in preserving identity and editability. To address these obstacles, we present PhotoVerse, an innovative methodology",
    "arxiv_url": "https://arxiv.org/abs/2309.05793v1",
    "pdf_url": "https://arxiv.org/pdf/2309.05793v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2309.05793",
    "arxiv_authors": [
      "Li Chen",
      "Mengyi Zhao",
      "Yiheng Liu",
      "Mingxu Ding",
      "Yangyang Song",
      "Shizun Wang",
      "Xu Wang",
      "Hao Yang",
      "Jing Liu",
      "Kang Du",
      "Min Zheng"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=PhotoVerse%3A+Tuning-Free+Image+Customization+with+Text-to-Image+Diffusion+Models+Li+Chen+Mengyi+Zhao+Yiheng+Liu+Mingxu+Ding+Yangyang+Song",
    "gs_search_success": true,
    "gs_authors": [
      "vJXUa7MAAAAJ",
      "vlgvrD0AAAAJ",
      "n9rw7ZgAAAAJ",
      "fv8F6CEAAAAJ",
      "HxpOiHoAAAAJ",
      "cFS823sAAAAJ"
    ],
    "citation_count": 69,
    "gs_author_count": 9
  },
  {
    "arxiv_id": "2501.06215",
    "title": "Fitting Different Interactive Information: Joint Classification of Emotion and Intention",
    "year": 2025,
    "published": "2025-01-05T05:23:27Z",
    "categories": [
      "cs.CV",
      "cs.CL",
      "cs.LG",
      "cs.MM",
      "eess.AS"
    ],
    "abstract": "This paper is the first-place solution for ICASSP MEIJU@2025 Track I, which focuses on low-resource multimodal emotion and intention recognition. How to effectively utilize a large amount of unlabeled data, while ensuring the mutual promotion of different difficulty levels tasks in the interaction stage, these two points become the key to the competition. In this paper, pseudo-label labeling is carried out on the model trained with labeled data, and samples with high confidence and their labels ",
    "arxiv_url": "https://arxiv.org/abs/2501.06215v1",
    "pdf_url": "https://arxiv.org/pdf/2501.06215v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2501.06215",
    "arxiv_authors": [
      "Xinger Li",
      "Zhiqiang Zhong",
      "Bo Huang",
      "Yang Yang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Fitting+Different+Interactive+Information%3A+Joint+Classification+of+Emotion+and+Intention+Xinger+Li+Zhiqiang+Zhong+Bo+Huang+Yang+Yang",
    "gs_search_success": true,
    "gs_authors": [
      "_6NJip0AAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2406.01059",
    "title": "VIP: Versatile Image Outpainting Empowered by Multimodal Large Language Model",
    "year": 2024,
    "published": "2024-06-03T07:14:19Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "In this paper, we focus on resolving the problem of image outpainting, which aims to extrapolate the surrounding parts given the center contents of an image. Although recent works have achieved promising performance, the lack of versatility and customization hinders their practical applications in broader scenarios. Therefore, this work presents a novel image outpainting framework that is capable of customizing the results according to the requirement of users. First of all, we take advantage of",
    "arxiv_url": "https://arxiv.org/abs/2406.01059v3",
    "pdf_url": "https://arxiv.org/pdf/2406.01059v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2406.01059",
    "arxiv_authors": [
      "Jinze Yang",
      "Haoran Wang",
      "Zining Zhu",
      "Chenglong Liu",
      "Meng Wymond Wu",
      "Mingming Sun"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=VIP%3A+Versatile+Image+Outpainting+Empowered+by+Multimodal+Large+Language+Model+Jinze+Yang+Haoran+Wang+Zining+Zhu+Chenglong+Liu+Meng+Wymond+Wu",
    "gs_search_success": true,
    "gs_authors": [
      "xfnL2IEAAAAJ",
      "_PfM-AUAAAAJ",
      "kUqmflsAAAAJ"
    ],
    "citation_count": 8,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2412.03200",
    "title": "Fab-ME: A Vision State-Space and Attention-Enhanced Framework for Fabric Defect Detection",
    "year": 2024,
    "published": "2024-12-04T10:40:17Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Effective defect detection is critical for ensuring the quality, functionality, and economic value of textile products. However, existing methods face challenges in achieving high accuracy, real-time performance, and efficient global information extraction. To address these issues, we propose Fab-ME, an advanced framework based on YOLOv8s, specifically designed for the accurate detection of 20 fabric defect types. Our contributions include the introduction of the cross-stage partial bottleneck w",
    "arxiv_url": "https://arxiv.org/abs/2412.03200v2",
    "pdf_url": "https://arxiv.org/pdf/2412.03200v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2412.03200",
    "arxiv_authors": [
      "Shuai Wang",
      "Huiyan Kong",
      "Baotian Li",
      "Fa Zheng"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Fab-ME%3A+A+Vision+State-Space+and+Attention-Enhanced+Framework+for+Fabric+Defect+Detection+Shuai+Wang+Huiyan+Kong+Baotian+Li+Fa+Zheng",
    "gs_search_success": true,
    "gs_authors": [],
    "citation_count": 2,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2402.14371",
    "title": "HR-APR: APR-agnostic Framework with Uncertainty Estimation and Hierarchical Refinement for Camera Relocalisation",
    "year": 2024,
    "published": "2024-02-22T08:21:46Z",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "abstract": "Absolute Pose Regressors (APRs) directly estimate camera poses from monocular images, but their accuracy is unstable for different queries. Uncertainty-aware APRs provide uncertainty information on the estimated pose, alleviating the impact of these unreliable predictions. However, existing uncertainty modelling techniques are often coupled with a specific APR architecture, resulting in suboptimal performance compared to state-of-the-art (SOTA) APR methods. This work introduces a novel APR-agnos",
    "arxiv_url": "https://arxiv.org/abs/2402.14371v2",
    "pdf_url": "https://arxiv.org/pdf/2402.14371v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2402.14371",
    "arxiv_authors": [
      "Changkun Liu",
      "Shuai Chen",
      "Yukun Zhao",
      "Huajian Huang",
      "Victor Prisacariu",
      "Tristan Braud"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=HR-APR%3A+APR-agnostic+Framework+with+Uncertainty+Estimation+and+Hierarchical+Refinement+for+Camera+Relocalisation+Changkun+Liu+Shuai+Chen+Yukun+Zhao+Huajian+Huang+Victor+Prisacariu",
    "gs_search_success": true,
    "gs_authors": [
      "NcLael4AAAAJ",
      "c0xTh_YAAAAJ",
      "25npC_YAAAAJ",
      "rOhG9NoAAAAJ",
      "ZOZtoQUAAAAJ",
      "GmWA-LoAAAAJ"
    ],
    "citation_count": 14,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2403.17808",
    "title": "Annotated Biomedical Video Generation using Denoising Diffusion Probabilistic Models and Flow Fields",
    "year": 2024,
    "published": "2024-03-26T15:45:29Z",
    "categories": [
      "eess.IV",
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "The segmentation and tracking of living cells play a vital role within the biomedical domain, particularly in cancer research, drug development, and developmental biology. These are usually tedious and time-consuming tasks that are traditionally done by biomedical experts. Recently, to automatize these processes, deep learning based segmentation and tracking methods have been proposed. These methods require large-scale datasets and their full potential is constrained by the scarcity of annotated",
    "arxiv_url": "https://arxiv.org/abs/2403.17808v1",
    "pdf_url": "https://arxiv.org/pdf/2403.17808v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2403.17808",
    "arxiv_authors": [
      "Rüveyda Yilmaz",
      "Dennis Eschweiler",
      "Johannes Stegmaier"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Annotated+Biomedical+Video+Generation+using+Denoising+Diffusion+Probabilistic+Models+and+Flow+Fields+R%C3%BCveyda+Yilmaz+Dennis+Eschweiler+Johannes+Stegmaier",
    "gs_search_success": true,
    "gs_authors": [
      "JV57X-wAAAAJ",
      "gwf8A9YAAAAJ",
      "L1Pa16AAAAAJ"
    ],
    "citation_count": 6,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2409.14984",
    "title": "SocialCircle+: Learning the Angle-based Conditioned Interaction Representation for Pedestrian Trajectory Prediction",
    "year": 2024,
    "published": "2024-09-23T13:02:12Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Trajectory prediction is a crucial aspect of understanding human behaviors. Researchers have made efforts to represent socially interactive behaviors among pedestrians and utilize various networks to enhance prediction capability. Unfortunately, they still face challenges not only in fully explaining and measuring how these interactive behaviors work to modify trajectories but also in modeling pedestrians' preferences to plan or participate in social interactions in response to the changeable ph",
    "arxiv_url": "https://arxiv.org/abs/2409.14984v1",
    "pdf_url": "https://arxiv.org/pdf/2409.14984v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2409.14984",
    "arxiv_authors": [
      "Conghao Wong",
      "Beihao Xia",
      "Ziqian Zou",
      "Xinge You"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=SocialCircle%2B%3A+Learning+the+Angle-based+Conditioned+Interaction+Representation+for+Pedestrian+Trajectory+Prediction+Conghao+Wong+Beihao+Xia+Ziqian+Zou+Xinge+You",
    "gs_search_success": true,
    "gs_authors": [
      "v7bRZX8AAAAJ",
      "P-WtHKgAAAAJ",
      "Lx_iqH8AAAAJ",
      "cA_6-EwAAAAJ"
    ],
    "citation_count": 6,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2412.03407",
    "title": "Skel3D: Skeleton Guided Novel View Synthesis",
    "year": 2024,
    "published": "2024-12-04T15:45:20Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "In this paper, we present an approach for monocular open-set novel view synthesis (NVS) that leverages object skeletons to guide the underlying diffusion model. Building upon a baseline that utilizes a pre-trained 2D image generator, our method takes advantage of the Objaverse dataset, which includes animated objects with bone structures. By introducing a skeleton guide layer following the existing ray conditioning normalization (RCN) layer, our approach enhances pose accuracy and multi-view con",
    "arxiv_url": "https://arxiv.org/abs/2412.03407v1",
    "pdf_url": "https://arxiv.org/pdf/2412.03407v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2412.03407",
    "arxiv_authors": [
      "Aron Fóthi",
      "Bence Fazekas",
      "Natabara Máté Gyöngyössy",
      "Kristian Fenech"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Skel3D%3A+Skeleton+Guided+Novel+View+Synthesis+Aron+F%C3%B3thi+Bence+Fazekas+Natabara+M%C3%A1t%C3%A9+Gy%C3%B6ngy%C3%B6ssy+Kristian+Fenech",
    "gs_search_success": true,
    "gs_authors": [
      "-drqZsoAAAAJ",
      "2UlkRh0AAAAJ",
      "xpQLT_oAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2307.00038",
    "title": "Training-free Object Counting with Prompts",
    "year": 2023,
    "published": "2023-06-30T13:26:30Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "This paper tackles the problem of object counting in images. Existing approaches rely on extensive training data with point annotations for each object, making data collection labor-intensive and time-consuming. To overcome this, we propose a training-free object counter that treats the counting task as a segmentation problem. Our approach leverages the Segment Anything Model (SAM), known for its high-quality masks and zero-shot segmentation capability. However, the vanilla mask generation metho",
    "arxiv_url": "https://arxiv.org/abs/2307.00038v2",
    "pdf_url": "https://arxiv.org/pdf/2307.00038v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2307.00038",
    "arxiv_authors": [
      "Zenglin Shi",
      "Ying Sun",
      "Mengmi Zhang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Training-free+Object+Counting+with+Prompts+Zenglin+Shi+Ying+Sun+Mengmi+Zhang",
    "gs_search_success": true,
    "gs_authors": [
      "G2sVOhcAAAAJ",
      "gGK6E38AAAAJ",
      "AJReaQ8AAAAJ"
    ],
    "citation_count": 45,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2401.04750",
    "title": "DedustNet: A Frequency-dominated Swin Transformer-based Wavelet Network for Agricultural Dust Removal",
    "year": 2024,
    "published": "2024-01-09T13:40:01Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "While dust significantly affects the environmental perception of automated agricultural machines, the existing deep learning-based methods for dust removal require further research and improvement in this area to improve the performance and reliability of automated agricultural machines in agriculture. We propose an end-to-end trainable learning network (DedustNet) to solve the real-world agricultural dust removal task. To our knowledge, DedustNet is the first time Swin Transformer-based units h",
    "arxiv_url": "https://arxiv.org/abs/2401.04750v1",
    "pdf_url": "https://arxiv.org/pdf/2401.04750v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2401.04750",
    "arxiv_authors": [
      "Shengli Zhang",
      "Zhiyong Tao",
      "Sen Lin"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=DedustNet%3A+A+Frequency-dominated+Swin+Transformer-based+Wavelet+Network+for+Agricultural+Dust+Removal+Shengli+Zhang+Zhiyong+Tao+Sen+Lin",
    "gs_search_success": true,
    "gs_authors": [
      "R71r-pwAAAAJ",
      "qCY7Af8AAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2309.11382",
    "title": "Discuss Before Moving: Visual Language Navigation via Multi-expert Discussions",
    "year": 2023,
    "published": "2023-09-20T15:04:49Z",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CL",
      "cs.CV"
    ],
    "abstract": "Visual language navigation (VLN) is an embodied task demanding a wide range of skills encompassing understanding, perception, and planning. For such a multifaceted challenge, previous VLN methods totally rely on one model's own thinking to make predictions within one round. However, existing models, even the most advanced large language model GPT4, still struggle with dealing with multiple tasks by single-round self-thinking. In this work, drawing inspiration from the expert consultation meeting",
    "arxiv_url": "https://arxiv.org/abs/2309.11382v1",
    "pdf_url": "https://arxiv.org/pdf/2309.11382v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2309.11382",
    "arxiv_authors": [
      "Yuxing Long",
      "Xiaoqi Li",
      "Wenzhe Cai",
      "Hao Dong"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Discuss+Before+Moving%3A+Visual+Language+Navigation+via+Multi-expert+Discussions+Yuxing+Long+Xiaoqi+Li+Wenzhe+Cai+Hao+Dong",
    "gs_search_success": true,
    "gs_authors": [
      "UqQ41BIAAAAJ",
      "NHQcCyAAAAAJ",
      "vkQ5_LIAAAAJ",
      "xLFL4sMAAAAJ"
    ],
    "citation_count": 112,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2309.15048",
    "title": "Class Incremental Learning via Likelihood Ratio Based Task Prediction",
    "year": 2023,
    "published": "2023-09-26T16:25:57Z",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "abstract": "Class incremental learning (CIL) is a challenging setting of continual learning, which learns a series of tasks sequentially. Each task consists of a set of unique classes. The key feature of CIL is that no task identifier (or task-id) is provided at test time. Predicting the task-id for each test sample is a challenging problem. An emerging theory-guided approach (called TIL+OOD) is to train a task-specific model for each task in a shared network for all tasks based on a task-incremental learni",
    "arxiv_url": "https://arxiv.org/abs/2309.15048v4",
    "pdf_url": "https://arxiv.org/pdf/2309.15048v4",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2309.15048",
    "arxiv_authors": [
      "Haowei Lin",
      "Yijia Shao",
      "Weinan Qian",
      "Ningxin Pan",
      "Yiduo Guo",
      "Bing Liu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Class+Incremental+Learning+via+Likelihood+Ratio+Based+Task+Prediction+Haowei+Lin+Yijia+Shao+Weinan+Qian+Ningxin+Pan+Yiduo+Guo",
    "gs_search_success": true,
    "gs_authors": [
      "ov-Cb2kAAAAJ",
      "Ng-DmJgAAAAJ",
      "Kt1bjZoAAAAJ",
      "H0zcQh4AAAAJ"
    ],
    "citation_count": 23,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2310.19820",
    "title": "NetDistiller: Empowering Tiny Deep Learning via In-Situ Distillation",
    "year": 2023,
    "published": "2023-10-24T04:27:51Z",
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "abstract": "Boosting the task accuracy of tiny neural networks (TNNs) has become a fundamental challenge for enabling the deployments of TNNs on edge devices which are constrained by strict limitations in terms of memory, computation, bandwidth, and power supply. To this end, we propose a framework called NetDistiller to boost the achievable accuracy of TNNs by treating them as sub-networks of a weight-sharing teacher constructed by expanding the number of channels of the TNN. Specifically, the target TNN m",
    "arxiv_url": "https://arxiv.org/abs/2310.19820v1",
    "pdf_url": "https://arxiv.org/pdf/2310.19820v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2310.19820",
    "arxiv_authors": [
      "Shunyao Zhang",
      "Yonggan Fu",
      "Shang Wu",
      "Jyotikrishna Dass",
      "Haoran You",
      "Yingyan",
      "Lin"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=NetDistiller%3A+Empowering+Tiny+Deep+Learning+via+In-Situ+Distillation+Shunyao+Zhang+Yonggan+Fu+Shang+Wu+Jyotikrishna+Dass+Haoran+You",
    "gs_search_success": true,
    "gs_authors": [
      "xIU4snQAAAAJ",
      "fBp8EWgAAAAJ",
      "YJWCW8gAAAAJ",
      "dobDj4AAAAAJ",
      "dio8IesAAAAJ",
      "pt3GfXcAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 7
  },
  {
    "arxiv_id": "2408.14789",
    "title": "Revisiting Surgical Instrument Segmentation Without Human Intervention: A Graph Partitioning View",
    "year": 2024,
    "published": "2024-08-27T05:31:30Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Surgical instrument segmentation (SIS) on endoscopic images stands as a long-standing and essential task in the context of computer-assisted interventions for boosting minimally invasive surgery. Given the recent surge of deep learning methodologies and their data-hungry nature, training a neural predictive model based on massive expert-curated annotations has been dominating and served as an off-the-shelf approach in the field, which could, however, impose prohibitive burden to clinicians for p",
    "arxiv_url": "https://arxiv.org/abs/2408.14789v3",
    "pdf_url": "https://arxiv.org/pdf/2408.14789v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2408.14789",
    "arxiv_authors": [
      "Mingyu Sheng",
      "Jianan Fan",
      "Dongnan Liu",
      "Ron Kikinis",
      "Weidong Cai"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Revisiting+Surgical+Instrument+Segmentation+Without+Human+Intervention%3A+A+Graph+Partitioning+View+Mingyu+Sheng+Jianan+Fan+Dongnan+Liu+Ron+Kikinis+Weidong+Cai",
    "gs_search_success": true,
    "gs_authors": [
      "n01L0mEAAAAJ",
      "P7MIBuMAAAAJ",
      "N8qTc2AAAAAJ",
      "JZzb8XUAAAAJ"
    ],
    "citation_count": 3,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2401.17231",
    "title": "Achieving More Human Brain-Like Vision via Human EEG Representational Alignment",
    "year": 2024,
    "published": "2024-01-30T18:18:41Z",
    "categories": [
      "cs.CV",
      "cs.LG",
      "cs.NE",
      "q-bio.NC"
    ],
    "abstract": "Despite advancements in artificial intelligence, object recognition models still lag behind in emulating visual information processing in human brains. Recent studies have highlighted the potential of using neural data to mimic brain processing; however, these often rely on invasive neural recordings from non-human subjects, leaving a critical gap in understanding human visual perception. Addressing this gap, we present, 'Re(presentational)Al(ignment)net', a vision model aligned with human brain",
    "arxiv_url": "https://arxiv.org/abs/2401.17231v3",
    "pdf_url": "https://arxiv.org/pdf/2401.17231v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2401.17231",
    "arxiv_authors": [
      "Zitong Lu",
      "Yile Wang",
      "Julie D. Golomb"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Achieving+More+Human+Brain-Like+Vision+via+Human+EEG+Representational+Alignment+Zitong+Lu+Yile+Wang+Julie+D.+Golomb",
    "gs_search_success": true,
    "gs_authors": [
      "bE5VCKsAAAAJ",
      "bgnVj8wAAAAJ",
      "h1VR9HAAAAAJ"
    ],
    "citation_count": 8,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2305.00970",
    "title": "ArK: Augmented Reality with Knowledge Interactive Emergent Ability",
    "year": 2023,
    "published": "2023-05-01T17:57:01Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Despite the growing adoption of mixed reality and interactive AI agents, it remains challenging for these systems to generate high quality 2D/3D scenes in unseen environments. The common practice requires deploying an AI agent to collect large amounts of data for model training for every new task. This process is costly, or even impossible, for many domains. In this study, we develop an infinite agent that learns to transfer knowledge memory from general foundation models (e.g. GPT4, DALLE) to n",
    "arxiv_url": "https://arxiv.org/abs/2305.00970v1",
    "pdf_url": "https://arxiv.org/pdf/2305.00970v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2305.00970",
    "arxiv_authors": [
      "Qiuyuan Huang",
      "Jae Sung Park",
      "Abhinav Gupta",
      "Paul Bennett",
      "Ran Gong",
      "Subhojit Som",
      "Baolin Peng",
      "Owais Khan Mohammed",
      "Chris Pal",
      "Yejin Choi",
      "Jianfeng Gao"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=ArK%3A+Augmented+Reality+with+Knowledge+Interactive+Emergent+Ability+Qiuyuan+Huang+Jae+Sung+Park+Abhinav+Gupta+Paul+Bennett+Ran+Gong",
    "gs_search_success": true,
    "gs_authors": [
      "u1CNjgwAAAAJ",
      "AIncPrIAAAAJ",
      "n1gxPekAAAAJ",
      "hD2WqqcAAAAJ",
      "jAaCd7YAAAAJ",
      "1ScWJOoAAAAJ",
      "U7Mmyc8AAAAJ"
    ],
    "citation_count": 5,
    "gs_author_count": 9
  },
  {
    "arxiv_id": "2403.17869",
    "title": "To Supervise or Not to Supervise: Understanding and Addressing the Key Challenges of Point Cloud Transfer Learning",
    "year": 2024,
    "published": "2024-03-26T16:57:33Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Transfer learning has long been a key factor in the advancement of many fields including 2D image analysis. Unfortunately, its applicability in 3D data processing has been relatively limited. While several approaches for point cloud transfer learning have been proposed in recent literature, with contrastive learning gaining particular prominence, most existing methods in this domain have only been studied and evaluated in limited scenarios. Most importantly, there is currently a lack of principl",
    "arxiv_url": "https://arxiv.org/abs/2403.17869v2",
    "pdf_url": "https://arxiv.org/pdf/2403.17869v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2403.17869",
    "arxiv_authors": [
      "Souhail Hadgi",
      "Lei Li",
      "Maks Ovsjanikov"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=To+Supervise+or+Not+to+Supervise%3A+Understanding+and+Addressing+the+Key+Challenges+of+Point+Cloud+Transfer+Learning+Souhail+Hadgi+Lei+Li+Maks+Ovsjanikov",
    "gs_search_success": true,
    "gs_authors": [
      "CxK2IjYAAAAJ",
      "0IsSPNEAAAAJ",
      "1B0l7U8AAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2308.05128",
    "title": "High-Level Parallelism and Nested Features for Dynamic Inference Cost and Top-Down Attention",
    "year": 2023,
    "published": "2023-08-09T08:49:29Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "This paper introduces a novel network topology that seamlessly integrates dynamic inference cost with a top-down attention mechanism, addressing two significant gaps in traditional deep learning models. Drawing inspiration from human perception, we combine sequential processing of generic low-level features with parallelism and nesting of high-level features. This design not only reflects a finding from recent neuroscience research regarding - spatially and contextually distinct neural activatio",
    "arxiv_url": "https://arxiv.org/abs/2308.05128v3",
    "pdf_url": "https://arxiv.org/pdf/2308.05128v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2308.05128",
    "arxiv_authors": [
      "André Peter Kelm",
      "Niels Hannemann",
      "Bruno Heberle",
      "Lucas Schmidt",
      "Tim Rolff",
      "Christian Wilms",
      "Ehsan Yaghoubi",
      "Simone Frintrop"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=High-Level+Parallelism+and+Nested+Features+for+Dynamic+Inference+Cost+and+Top-Down+Attention+Andr%C3%A9+Peter+Kelm+Niels+Hannemann+Bruno+Heberle+Lucas+Schmidt+Tim+Rolff",
    "gs_search_success": true,
    "gs_authors": [
      "NynHirYAAAAJ",
      "klbfxswAAAAJ",
      "AjkH8tYAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 8
  },
  {
    "arxiv_id": "2412.17523",
    "title": "Constructing Fair Latent Space for Intersection of Fairness and Explainability",
    "year": 2024,
    "published": "2024-12-23T12:47:04Z",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "abstract": "As the use of machine learning models has increased, numerous studies have aimed to enhance fairness. However, research on the intersection of fairness and explainability remains insufficient, leading to potential issues in gaining the trust of actual users. Here, we propose a novel module that constructs a fair latent space, enabling faithful explanation while ensuring fairness. The fair latent space is constructed by disentangling and redistributing labels and sensitive attributes, allowing th",
    "arxiv_url": "https://arxiv.org/abs/2412.17523v2",
    "pdf_url": "https://arxiv.org/pdf/2412.17523v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2412.17523",
    "arxiv_authors": [
      "Hyungjun Joo",
      "Hyeonggeun Han",
      "Sehwan Kim",
      "Sangwoo Hong",
      "Jungwoo Lee"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Constructing+Fair+Latent+Space+for+Intersection+of+Fairness+and+Explainability+Hyungjun+Joo+Hyeonggeun+Han+Sehwan+Kim+Sangwoo+Hong+Jungwoo+Lee",
    "gs_search_success": true,
    "gs_authors": [
      "B6LcRbYAAAAJ",
      "zYn8YZ8AAAAJ",
      "IZ8-3PgAAAAJ"
    ],
    "citation_count": 2,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2309.15426",
    "title": "NeuRBF: A Neural Fields Representation with Adaptive Radial Basis Functions",
    "year": 2023,
    "published": "2023-09-27T06:32:05Z",
    "categories": [
      "cs.CV",
      "cs.GR",
      "cs.LG"
    ],
    "abstract": "We present a novel type of neural fields that uses general radial bases for signal representation. State-of-the-art neural fields typically rely on grid-based representations for storing local neural features and N-dimensional linear kernels for interpolating features at continuous query points. The spatial positions of their neural features are fixed on grid nodes and cannot well adapt to target signals. Our method instead builds upon general radial bases with flexible kernel position and shape",
    "arxiv_url": "https://arxiv.org/abs/2309.15426v1",
    "pdf_url": "https://arxiv.org/pdf/2309.15426v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2309.15426",
    "arxiv_authors": [
      "Zhang Chen",
      "Zhong Li",
      "Liangchen Song",
      "Lele Chen",
      "Jingyi Yu",
      "Junsong Yuan",
      "Yi Xu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=NeuRBF%3A+A+Neural+Fields+Representation+with+Adaptive+Radial+Basis+Functions+Zhang+Chen+Zhong+Li+Liangchen+Song+Lele+Chen+Jingyi+Yu",
    "gs_search_success": true,
    "gs_authors": [
      "fJ7seq0AAAAJ",
      "Kl4T9FYAAAAJ",
      "ldanjkUAAAAJ",
      "4MIbSrAAAAAJ",
      "C-wK73YAAAAJ",
      "H71yt54AAAAJ"
    ],
    "citation_count": 105,
    "gs_author_count": 7
  },
  {
    "arxiv_id": "2402.02936",
    "title": "Panoramic Image Inpainting With Gated Convolution And Contextual Reconstruction Loss",
    "year": 2024,
    "published": "2024-02-05T11:58:08Z",
    "categories": [
      "eess.IV",
      "cs.CV",
      "cs.LG",
      "cs.MM"
    ],
    "abstract": "Deep learning-based methods have demonstrated encouraging results in tackling the task of panoramic image inpainting. However, it is challenging for existing methods to distinguish valid pixels from invalid pixels and find suitable references for corrupted areas, thus leading to artifacts in the inpainted results. In response to these challenges, we propose a panoramic image inpainting framework that consists of a Face Generator, a Cube Generator, a side branch, and two discriminators. We use th",
    "arxiv_url": "https://arxiv.org/abs/2402.02936v1",
    "pdf_url": "https://arxiv.org/pdf/2402.02936v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2402.02936",
    "arxiv_authors": [
      "Li Yu",
      "Yanjun Gao",
      "Farhad Pakdaman",
      "Moncef Gabbouj"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Panoramic+Image+Inpainting+With+Gated+Convolution+And+Contextual+Reconstruction+Loss+Li+Yu+Yanjun+Gao+Farhad+Pakdaman+Moncef+Gabbouj",
    "gs_search_success": true,
    "gs_authors": [
      "HSzWxk4AAAAJ",
      "cHukfSUAAAAJ",
      "RVluyTgAAAAJ"
    ],
    "citation_count": 6,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2307.07928",
    "title": "Reinforced Disentanglement for Face Swapping without Skip Connection",
    "year": 2023,
    "published": "2023-07-16T02:44:19Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "The SOTA face swap models still suffer the problem of either target identity (i.e., shape) being leaked or the target non-identity attributes (i.e., background, hair) failing to be fully preserved in the final results. We show that this insufficient disentanglement is caused by two flawed designs that were commonly adopted in prior models: (1) counting on only one compressed encoder to represent both the semantic-level non-identity facial attributes(i.e., pose) and the pixel-level non-facial reg",
    "arxiv_url": "https://arxiv.org/abs/2307.07928v4",
    "pdf_url": "https://arxiv.org/pdf/2307.07928v4",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2307.07928",
    "arxiv_authors": [
      "Xiaohang Ren",
      "Xingyu Chen",
      "Pengfei Yao",
      "Heung-Yeung Shum",
      "Baoyuan Wang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Reinforced+Disentanglement+for+Face+Swapping+without+Skip+Connection+Xiaohang+Ren+Xingyu+Chen+Pengfei+Yao+Heung-Yeung+Shum+Baoyuan+Wang",
    "gs_search_success": true,
    "gs_authors": [
      "9akH-n8AAAAJ",
      "YurWtIEAAAAJ",
      "OWa5rOEAAAAJ"
    ],
    "citation_count": 18,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2311.02480",
    "title": "A Strictly Bounded Deep Network for Unpaired Cyclic Translation of Medical Images",
    "year": 2023,
    "published": "2023-11-04T18:43:31Z",
    "categories": [
      "eess.IV",
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "Medical image translation is an ill-posed problem. Unlike existing paired unbounded unidirectional translation networks, in this paper, we consider unpaired medical images and provide a strictly bounded network that yields a stable bidirectional translation. We propose a patch-level concatenated cyclic conditional generative adversarial network (pCCGAN) embedded with adaptive dictionary learning. It consists of two cyclically connected CGANs of 47 layers each; where both generators (each of 32 l",
    "arxiv_url": "https://arxiv.org/abs/2311.02480v1",
    "pdf_url": "https://arxiv.org/pdf/2311.02480v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2311.02480",
    "arxiv_authors": [
      "Swati Rai",
      "Jignesh S. Bhatt",
      "Sarat Kumar Patra"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=A+Strictly+Bounded+Deep+Network+for+Unpaired+Cyclic+Translation+of+Medical+Images+Swati+Rai+Jignesh+S.+Bhatt+Sarat+Kumar+Patra",
    "gs_search_success": true,
    "gs_authors": [
      "Ro5gYAwAAAAJ",
      "clxd-nsAAAAJ",
      "6vFfeb4AAAAJ"
    ],
    "citation_count": 4,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2401.16456",
    "title": "SHViT: Single-Head Vision Transformer with Memory Efficient Macro Design",
    "year": 2024,
    "published": "2024-01-29T09:12:23Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Recently, efficient Vision Transformers have shown great performance with low latency on resource-constrained devices. Conventionally, they use 4x4 patch embeddings and a 4-stage structure at the macro level, while utilizing sophisticated attention with multi-head configuration at the micro level. This paper aims to address computational redundancy at all design levels in a memory-efficient manner. We discover that using larger-stride patchify stem not only reduces memory access costs but also a",
    "arxiv_url": "https://arxiv.org/abs/2401.16456v2",
    "pdf_url": "https://arxiv.org/pdf/2401.16456v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2401.16456",
    "arxiv_authors": [
      "Seokju Yun",
      "Youngmin Ro"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=SHViT%3A+Single-Head+Vision+Transformer+with+Memory+Efficient+Macro+Design+Seokju+Yun+Youngmin+Ro",
    "gs_search_success": true,
    "gs_authors": [
      "-2MnHEIAAAAJ",
      "f5zk6PsAAAAJ"
    ],
    "citation_count": 112,
    "gs_author_count": 2
  },
  {
    "arxiv_id": "2407.11383",
    "title": "TM-PATHVQA:90000+ Textless Multilingual Questions for Medical Visual Question Answering",
    "year": 2024,
    "published": "2024-07-16T04:54:45Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "In healthcare and medical diagnostics, Visual Question Answering (VQA) mayemergeasapivotal tool in scenarios where analysis of intricate medical images becomes critical for accurate diagnoses. Current text-based VQA systems limit their utility in scenarios where hands-free interaction and accessibility are crucial while performing tasks. A speech-based VQA system may provide a better means of interaction where information can be accessed while performing tasks simultaneously. To this end, this w",
    "arxiv_url": "https://arxiv.org/abs/2407.11383v1",
    "pdf_url": "https://arxiv.org/pdf/2407.11383v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2407.11383",
    "arxiv_authors": [
      "Tonmoy Rajkhowa",
      "Amartya Roy Chowdhury",
      "Sankalp Nagaonkar",
      "Achyut Mani Tripathi"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=TM-PATHVQA%3A90000%2B+Textless+Multilingual+Questions+for+Medical+Visual+Question+Answering+Tonmoy+Rajkhowa+Amartya+Roy+Chowdhury+Sankalp+Nagaonkar+Achyut+Mani+Tripathi",
    "gs_search_success": true,
    "gs_authors": [
      "daIIpSUAAAAJ",
      "sCr-DQgAAAAJ",
      "R2iStHMAAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2312.05288",
    "title": "MotionCrafter: One-Shot Motion Customization of Diffusion Models",
    "year": 2023,
    "published": "2023-12-08T16:31:04Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "The essence of a video lies in its dynamic motions, including character actions, object movements, and camera movements. While text-to-video generative diffusion models have recently advanced in creating diverse contents, controlling specific motions through text prompts remains a significant challenge. A primary issue is the coupling of appearance and motion, often leading to overfitting on appearance. To tackle this challenge, we introduce MotionCrafter, a novel one-shot instance-guided motion",
    "arxiv_url": "https://arxiv.org/abs/2312.05288v2",
    "pdf_url": "https://arxiv.org/pdf/2312.05288v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2312.05288",
    "arxiv_authors": [
      "Yuxin Zhang",
      "Fan Tang",
      "Nisha Huang",
      "Haibin Huang",
      "Chongyang Ma",
      "Weiming Dong",
      "Changsheng Xu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=MotionCrafter%3A+One-Shot+Motion+Customization+of+Diffusion+Models+Yuxin+Zhang+Fan+Tang+Nisha+Huang+Haibin+Huang+Chongyang+Ma",
    "gs_search_success": true,
    "gs_authors": [
      "YDl1M80AAAAJ",
      "wTmPkSsAAAAJ",
      "l-ZQfpQAAAAJ",
      "hI9NRDkAAAAJ",
      "8VD0_DkAAAAJ",
      "WKGx4k8AAAAJ",
      "PdKElfwAAAAJ"
    ],
    "citation_count": 25,
    "gs_author_count": 7
  },
  {
    "arxiv_id": "2306.16142",
    "title": "Neural directional distance field object representation for uni-directional path-traced rendering",
    "year": 2023,
    "published": "2023-06-28T12:16:38Z",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "abstract": "Faster rendering of synthetic images is a core problem in the field of computer graphics. Rendering algorithms, such as path-tracing is dependent on parameters like size of the image, number of light bounces, number of samples per pixel, all of which, are fixed if one wants to obtain a image of a desired quality. It is also dependent on the size and complexity of the scene being rendered. One of the largest bottleneck in rendering, particularly when the scene is very large, is querying for objec",
    "arxiv_url": "https://arxiv.org/abs/2306.16142v1",
    "pdf_url": "https://arxiv.org/pdf/2306.16142v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2306.16142",
    "arxiv_authors": [
      "Annada Prasad Behera",
      "Subhankar Mishra"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Neural+directional+distance+field+object+representation+for+uni-directional+path-traced+rendering+Annada+Prasad+Behera+Subhankar+Mishra",
    "gs_search_success": true,
    "gs_authors": [
      "FICz_ukAAAAJ"
    ],
    "citation_count": 3,
    "gs_author_count": 2
  },
  {
    "arxiv_id": "2303.15735",
    "title": "Improving the Transferability of Adversarial Samples by Path-Augmented Method",
    "year": 2023,
    "published": "2023-03-28T05:14:04Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Deep neural networks have achieved unprecedented success on diverse vision tasks. However, they are vulnerable to adversarial noise that is imperceptible to humans. This phenomenon negatively affects their deployment in real-world scenarios, especially security-related ones. To evaluate the robustness of a target model in practice, transfer-based attacks craft adversarial samples with a local model and have attracted increasing attention from researchers due to their high efficiency. The state-o",
    "arxiv_url": "https://arxiv.org/abs/2303.15735v1",
    "pdf_url": "https://arxiv.org/pdf/2303.15735v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2303.15735",
    "arxiv_authors": [
      "Jianping Zhang",
      "Jen-tse Huang",
      "Wenxuan Wang",
      "Yichen Li",
      "Weibin Wu",
      "Xiaosen Wang",
      "Yuxin Su",
      "Michael R. Lyu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Improving+the+Transferability+of+Adversarial+Samples+by+Path-Augmented+Method+Jianping+Zhang+Jen-tse+Huang+Wenxuan+Wang+Yichen+Li+Weibin+Wu",
    "gs_search_success": true,
    "gs_authors": [
      "2o7L9GwAAAAJ",
      "uQnBgK0AAAAJ",
      "XBzDTAQAAAAJ",
      "4v5x0bUAAAAJ",
      "ckycnWkAAAAJ",
      "IrNImLsAAAAJ",
      "sVeDOcsAAAAJ",
      "6mtEjCEAAAAJ"
    ],
    "citation_count": 76,
    "gs_author_count": 8
  },
  {
    "arxiv_id": "2401.05224",
    "title": "Do Vision and Language Encoders Represent the World Similarly?",
    "year": 2024,
    "published": "2024-01-10T15:51:39Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "abstract": "Aligned text-image encoders such as CLIP have become the de facto model for vision-language tasks. Furthermore, modality-specific encoders achieve impressive performances in their respective domains. This raises a central question: does an alignment exist between uni-modal vision and language encoders since they fundamentally represent the same physical world? Analyzing the latent spaces structure of vision and language models on image-caption benchmarks using the Centered Kernel Alignment (CKA)",
    "arxiv_url": "https://arxiv.org/abs/2401.05224v2",
    "pdf_url": "https://arxiv.org/pdf/2401.05224v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2401.05224",
    "arxiv_authors": [
      "Mayug Maniparambil",
      "Raiymbek Akshulakov",
      "Yasser Abdelaziz Dahou Djilali",
      "Sanath Narayan",
      "Mohamed El Amine Seddik",
      "Karttikeya Mangalam",
      "Noel E. O'Connor"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Do+Vision+and+Language+Encoders+Represent+the+World+Similarly%3F+Mayug+Maniparambil+Raiymbek+Akshulakov+Yasser+Abdelaziz+Dahou+Djilali+Sanath+Narayan+Mohamed+El+Amine+Seddik",
    "gs_search_success": true,
    "gs_authors": [
      "2l1fWEoAAAAJ",
      "Bx7EFGoAAAAJ",
      "tCHi_7IAAAAJ",
      "f02jV7oAAAAJ",
      "85Hxd24AAAAJ"
    ],
    "citation_count": 35,
    "gs_author_count": 7
  },
  {
    "arxiv_id": "2501.01723",
    "title": "IGAF: Incremental Guided Attention Fusion for Depth Super-Resolution",
    "year": 2025,
    "published": "2025-01-03T09:27:51Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Accurate depth estimation is crucial for many fields, including robotics, navigation, and medical imaging. However, conventional depth sensors often produce low-resolution (LR) depth maps, making detailed scene perception challenging. To address this, enhancing LR depth maps to high-resolution (HR) ones has become essential, guided by HR-structured inputs like RGB or grayscale images. We propose a novel sensor fusion methodology for guided depth super-resolution (GDSR), a technique that combines",
    "arxiv_url": "https://arxiv.org/abs/2501.01723v1",
    "pdf_url": "https://arxiv.org/pdf/2501.01723v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2501.01723",
    "arxiv_authors": [
      "Athanasios Tragakis",
      "Chaitanya Kaul",
      "Kevin J. Mitchell",
      "Hang Dai",
      "Roderick Murray-Smith",
      "Daniele Faccio"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=IGAF%3A+Incremental+Guided+Attention+Fusion+for+Depth+Super-Resolution+Athanasios+Tragakis+Chaitanya+Kaul+Kevin+J.+Mitchell+Hang+Dai+Roderick+Murray-Smith",
    "gs_search_success": true,
    "gs_authors": [
      "_acS9zAAAAAJ",
      "6yvjpQQAAAAJ",
      "MsPIYAoAAAAJ",
      "eA8DbtoAAAAJ",
      "laX7LzQAAAAJ",
      "GAGMBAwAAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2304.02255",
    "title": "Topology-Guided Multi-Class Cell Context Generation for Digital Pathology",
    "year": 2023,
    "published": "2023-04-05T07:01:34Z",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "abstract": "In digital pathology, the spatial context of cells is important for cell classification, cancer diagnosis and prognosis. To model such complex cell context, however, is challenging. Cells form different mixtures, lineages, clusters and holes. To model such structural patterns in a learnable fashion, we introduce several mathematical tools from spatial statistics and topological data analysis. We incorporate such structural descriptors into a deep generative model as both conditional inputs and a",
    "arxiv_url": "https://arxiv.org/abs/2304.02255v1",
    "pdf_url": "https://arxiv.org/pdf/2304.02255v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2304.02255",
    "arxiv_authors": [
      "Shahira Abousamra",
      "Rajarsi Gupta",
      "Tahsin Kurc",
      "Dimitris Samaras",
      "Joel Saltz",
      "Chao Chen"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Topology-Guided+Multi-Class+Cell+Context+Generation+for+Digital+Pathology+Shahira+Abousamra+Rajarsi+Gupta+Tahsin+Kurc+Dimitris+Samaras+Joel+Saltz",
    "gs_search_success": true,
    "gs_authors": [
      "LcEPA3cAAAAJ",
      "J-iIIFAAAAAJ",
      "_0dkufgAAAAJ",
      "QuEQAlMAAAAJ",
      "BxbKTYkAAAAJ"
    ],
    "citation_count": 20,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2505.01113",
    "title": "NeuroLoc: Encoding Navigation Cells for 6-DOF Camera Localization",
    "year": 2025,
    "published": "2025-05-02T08:47:31Z",
    "categories": [
      "cs.RO",
      "cs.CV",
      "cs.NE"
    ],
    "abstract": "Recently, camera localization has been widely adopted in autonomous robotic navigation due to its efficiency and convenience. However, autonomous navigation in unknown environments often suffers from scene ambiguity, environmental disturbances, and dynamic object transformation in camera localization. To address this problem, inspired by the biological brain navigation mechanism (such as grid cells, place cells, and head direction cells), we propose a novel neurobiological camera location method",
    "arxiv_url": "https://arxiv.org/abs/2505.01113v1",
    "pdf_url": "https://arxiv.org/pdf/2505.01113v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2505.01113",
    "arxiv_authors": [
      "Xun Li",
      "Jian Yang",
      "Fenli Jia",
      "Muyu Wang",
      "Qi Wu",
      "Jun Wu",
      "Jinpeng Mi",
      "Jilin Hu",
      "Peidong Liang",
      "Xuan Tang",
      "Ke Li",
      "Xiong You",
      "Xian Wei"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=NeuroLoc%3A+Encoding+Navigation+Cells+for+6-DOF+Camera+Localization+Xun+Li+Jian+Yang+Fenli+Jia+Muyu+Wang+Qi+Wu",
    "gs_search_success": true,
    "gs_authors": [
      "o3m_s5YAAAAJ",
      "mFj-I10AAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 9
  },
  {
    "arxiv_id": "2304.11450",
    "title": "Dilated-UNet: A Fast and Accurate Medical Image Segmentation Approach using a Dilated Transformer and U-Net Architecture",
    "year": 2023,
    "published": "2023-04-22T17:20:13Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Medical image segmentation is crucial for the development of computer-aided diagnostic and therapeutic systems, but still faces numerous difficulties. In recent years, the commonly used encoder-decoder architecture based on CNNs has been applied effectively in medical image segmentation, but has limitations in terms of learning global context and spatial relationships. Some researchers have attempted to incorporate transformers into both the decoder and encoder components, with promising results",
    "arxiv_url": "https://arxiv.org/abs/2304.11450v1",
    "pdf_url": "https://arxiv.org/pdf/2304.11450v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2304.11450",
    "arxiv_authors": [
      "Davoud Saadati",
      "Omid Nejati Manzari",
      "Sattar Mirzakuchaki"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Dilated-UNet%3A+A+Fast+and+Accurate+Medical+Image+Segmentation+Approach+using+a+Dilated+Transformer+and+U-Net+Architecture+Davoud+Saadati+Omid+Nejati+Manzari+Sattar+Mirzakuchaki",
    "gs_search_success": true,
    "gs_authors": [
      "UvZmQzIAAAAJ"
    ],
    "citation_count": 27,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2308.09242",
    "title": "ASAG: Building Strong One-Decoder-Layer Sparse Detectors via Adaptive Sparse Anchor Generation",
    "year": 2023,
    "published": "2023-08-18T02:06:49Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Recent sparse detectors with multiple, e.g. six, decoder layers achieve promising performance but much inference time due to complex heads. Previous works have explored using dense priors as initialization and built one-decoder-layer detectors. Although they gain remarkable acceleration, their performance still lags behind their six-decoder-layer counterparts by a large margin. In this work, we aim to bridge this performance gap while retaining fast speed. We find that the architecture discrepan",
    "arxiv_url": "https://arxiv.org/abs/2308.09242v1",
    "pdf_url": "https://arxiv.org/pdf/2308.09242v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2308.09242",
    "arxiv_authors": [
      "Shenghao Fu",
      "Junkai Yan",
      "Yipeng Gao",
      "Xiaohua Xie",
      "Wei-Shi Zheng"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=ASAG%3A+Building+Strong+One-Decoder-Layer+Sparse+Detectors+via+Adaptive+Sparse+Anchor+Generation+Shenghao+Fu+Junkai+Yan+Yipeng+Gao+Xiaohua+Xie+Wei-Shi+Zheng",
    "gs_search_success": true,
    "gs_authors": [
      "m8I1ELMAAAAJ",
      "5YZ3kvoAAAAJ",
      "QMm29SwAAAAJ",
      "AwqDDGoAAAAJ",
      "G16eJVMAAAAJ"
    ],
    "citation_count": 9,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2502.09980",
    "title": "V2V-LLM: Vehicle-to-Vehicle Cooperative Autonomous Driving with Multi-Modal Large Language Models",
    "year": 2025,
    "published": "2025-02-14T08:05:41Z",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "abstract": "Current autonomous driving vehicles rely mainly on their individual sensors to understand surrounding scenes and plan for future trajectories, which can be unreliable when the sensors are malfunctioning or occluded. To address this problem, cooperative perception methods via vehicle-to-vehicle (V2V) communication have been proposed, but they have tended to focus on perception tasks like detection or tracking. How those approaches contribute to overall cooperative planning performance is still un",
    "arxiv_url": "https://arxiv.org/abs/2502.09980v3",
    "pdf_url": "https://arxiv.org/pdf/2502.09980v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2502.09980",
    "arxiv_authors": [
      "Hsu-kuang Chiu",
      "Ryo Hachiuma",
      "Chien-Yi Wang",
      "Stephen F. Smith",
      "Yu-Chiang Frank Wang",
      "Min-Hung Chen"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=V2V-LLM%3A+Vehicle-to-Vehicle+Cooperative+Autonomous+Driving+with+Multi-Modal+Large+Language+Models+Hsu-kuang+Chiu+Ryo+Hachiuma+Chien-Yi+Wang+Stephen+F.+Smith+Yu-Chiang+Frank+Wang",
    "gs_search_success": true,
    "gs_authors": [
      "fCFwERYAAAAJ",
      "ovzuxi8AAAAJ",
      "F_c5sHkAAAAJ",
      "05LW2DcAAAAJ",
      "W1KPqGEAAAAJ"
    ],
    "citation_count": 21,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2502.04640",
    "title": "Building Rome with Convex Optimization",
    "year": 2025,
    "published": "2025-02-07T03:53:46Z",
    "categories": [
      "cs.RO",
      "cs.CV",
      "math.OC"
    ],
    "abstract": "Global bundle adjustment is made easy by depth prediction and convex optimization. We (i) propose a scaled bundle adjustment (SBA) formulation that lifts 2D keypoint measurements to 3D with learned depth, (ii) design an empirically tight convex semidfinite program (SDP) relaxation that solves SBA to certfiable global optimality, (iii) solve the SDP relaxations at extreme scale with Burer-Monteiro factorization and a CUDA-based trust-region Riemannian optimizer (dubbed XM), (iv) build a structure",
    "arxiv_url": "https://arxiv.org/abs/2502.04640v4",
    "pdf_url": "https://arxiv.org/pdf/2502.04640v4",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2502.04640",
    "arxiv_authors": [
      "Haoyu Han",
      "Heng Yang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Building+Rome+with+Convex+Optimization+Haoyu+Han+Heng+Yang",
    "gs_search_success": true,
    "gs_authors": [
      "GuKEDfixZqsC"
    ],
    "citation_count": 6,
    "gs_author_count": 2
  },
  {
    "arxiv_id": "2406.19635",
    "title": "Model Predictive Simulation Using Structured Graphical Models and Transformers",
    "year": 2024,
    "published": "2024-06-28T03:46:53Z",
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "abstract": "We propose an approach to simulating trajectories of multiple interacting agents (road users) based on transformers and probabilistic graphical models (PGMs), and apply it to the Waymo SimAgents challenge. The transformer baseline is based on the MTR model, which predicts multiple future trajectories conditioned on the past trajectories and static road layout features. We then improve upon these generated trajectories using a PGM, which contains factors which encode prior knowledge, such as a pr",
    "arxiv_url": "https://arxiv.org/abs/2406.19635v1",
    "pdf_url": "https://arxiv.org/pdf/2406.19635v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2406.19635",
    "arxiv_authors": [
      "Xinghua Lou",
      "Meet Dave",
      "Shrinu Kushagra",
      "Miguel Lazaro-Gredilla",
      "Kevin Murphy"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Model+Predictive+Simulation+Using+Structured+Graphical+Models+and+Transformers+Xinghua+Lou+Meet+Dave+Shrinu+Kushagra+Miguel+Lazaro-Gredilla+Kevin+Murphy",
    "gs_search_success": true,
    "gs_authors": [
      "tqPmopoAAAAJ",
      "8RYloKYAAAAJ",
      "SFjDQk8AAAAJ",
      "MxxZkEcAAAAJ",
      "5o4N31oAAAAJ"
    ],
    "citation_count": 2,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2406.19736",
    "title": "MM-Instruct: Generated Visual Instructions for Large Multimodal Model Alignment",
    "year": 2024,
    "published": "2024-06-28T08:25:27Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "abstract": "This paper introduces MM-Instruct, a large-scale dataset of diverse and high-quality visual instruction data designed to enhance the instruction-following capabilities of large multimodal models (LMMs). While existing visual instruction datasets often focus on question-answering, they struggle to generalize to broader application scenarios such as creative writing, summarization, or image analysis. To address these limitations, we propose a novel approach to constructing MM-Instruct that leverag",
    "arxiv_url": "https://arxiv.org/abs/2406.19736v1",
    "pdf_url": "https://arxiv.org/pdf/2406.19736v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2406.19736",
    "arxiv_authors": [
      "Jihao Liu",
      "Xin Huang",
      "Jinliang Zheng",
      "Boxiao Liu",
      "Jia Wang",
      "Osamu Yoshie",
      "Yu Liu",
      "Hongsheng Li"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=MM-Instruct%3A+Generated+Visual+Instructions+for+Large+Multimodal+Model+Alignment+Jihao+Liu+Xin+Huang+Jinliang+Zheng+Boxiao+Liu+Jia+Wang",
    "gs_search_success": true,
    "gs_authors": [
      "gU5abfcAAAAJ",
      "YLA5LwEAAAAJ",
      "PP1HyToAAAAJ",
      "3j5AHFsAAAAJ",
      "VGJ1rRAAAAAJ"
    ],
    "citation_count": 5,
    "gs_author_count": 8
  },
  {
    "arxiv_id": "2412.13569",
    "title": "Multi-View Pedestrian Occupancy Prediction with a Novel Synthetic Dataset",
    "year": 2024,
    "published": "2024-12-18T07:35:42Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "We address an advanced challenge of predicting pedestrian occupancy as an extension of multi-view pedestrian detection in urban traffic. To support this, we have created a new synthetic dataset called MVP-Occ, designed for dense pedestrian scenarios in large-scale scenes. Our dataset provides detailed representations of pedestrians using voxel structures, accompanied by rich semantic scene understanding labels, facilitating visual navigation and insights into pedestrian spatial information. Furt",
    "arxiv_url": "https://arxiv.org/abs/2412.13569v1",
    "pdf_url": "https://arxiv.org/pdf/2412.13569v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2412.13569",
    "arxiv_authors": [
      "Sithu Aung",
      "Min-Cheol Sagong",
      "Junghyun Cho"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Multi-View+Pedestrian+Occupancy+Prediction+with+a+Novel+Synthetic+Dataset+Sithu+Aung+Min-Cheol+Sagong+Junghyun+Cho",
    "gs_search_success": true,
    "gs_authors": [
      "pEx-E2wAAAAJ",
      "ifybhH8AAAAJ",
      "Yj1hv8YAAAAJ"
    ],
    "citation_count": 2,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2310.05699",
    "title": "Uni3DETR: Unified 3D Detection Transformer",
    "year": 2023,
    "published": "2023-10-09T13:20:20Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Existing point cloud based 3D detectors are designed for the particular scene, either indoor or outdoor ones. Because of the substantial differences in object distribution and point density within point clouds collected from various environments, coupled with the intricate nature of 3D metrics, there is still a lack of a unified network architecture that can accommodate diverse scenes. In this paper, we propose Uni3DETR, a unified 3D detector that addresses indoor and outdoor 3D detection within",
    "arxiv_url": "https://arxiv.org/abs/2310.05699v1",
    "pdf_url": "https://arxiv.org/pdf/2310.05699v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2310.05699",
    "arxiv_authors": [
      "Zhenyu Wang",
      "Yali Li",
      "Xi Chen",
      "Hengshuang Zhao",
      "Shengjin Wang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Uni3DETR%3A+Unified+3D+Detection+Transformer+Zhenyu+Wang+Yali+Li+Xi+Chen+Hengshuang+Zhao+Shengjin+Wang",
    "gs_search_success": true,
    "gs_authors": [
      "x_-kOjoAAAAJ",
      "INISnXkAAAAJ",
      "RgzLZZsAAAAJ",
      "4uE10I0AAAAJ"
    ],
    "citation_count": 41,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2304.07522",
    "title": "ID2image: Leakage of non-ID information into face descriptors and inversion from descriptors to images",
    "year": 2023,
    "published": "2023-04-15T10:11:22Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Embedding a face image to a descriptor vector using a deep CNN is a widely used technique in face recognition. Via several possible training strategies, such embeddings are supposed to capture only identity information. Information about the environment (such as background and lighting) or changeable aspects of the face (such as pose, expression, presence of glasses, hat etc.) should be discarded since they are not useful for recognition. In this paper, we present a surprising result that this i",
    "arxiv_url": "https://arxiv.org/abs/2304.07522v1",
    "pdf_url": "https://arxiv.org/pdf/2304.07522v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2304.07522",
    "arxiv_authors": [
      "Mingrui Li",
      "William A. P. Smith",
      "Patrik Huber"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=ID2image%3A+Leakage+of+non-ID+information+into+face+descriptors+and+inversion+from+descriptors+to+images+Mingrui+Li+William+A.+P.+Smith+Patrik+Huber",
    "gs_search_success": true,
    "gs_authors": [
      "o2z0h6MAAAAJ",
      "nb_I6yUAAAAJ"
    ],
    "citation_count": 4,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2302.10915",
    "title": "Conformers are All You Need for Visual Speech Recognition",
    "year": 2023,
    "published": "2023-02-17T01:31:55Z",
    "categories": [
      "cs.LG",
      "cs.CL",
      "cs.CV",
      "cs.SD",
      "eess.AS"
    ],
    "abstract": "Visual speech recognition models extract visual features in a hierarchical manner. At the lower level, there is a visual front-end with a limited temporal receptive field that processes the raw pixels depicting the lips or faces. At the higher level, there is an encoder that attends to the embeddings produced by the front-end over a large temporal receptive field. Previous work has focused on improving the visual front-end of the model to extract more useful features for speech recognition. Surp",
    "arxiv_url": "https://arxiv.org/abs/2302.10915v2",
    "pdf_url": "https://arxiv.org/pdf/2302.10915v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2302.10915",
    "arxiv_authors": [
      "Oscar Chang",
      "Hank Liao",
      "Dmitriy Serdyuk",
      "Ankit Shah",
      "Olivier Siohan"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Conformers+are+All+You+Need+for+Visual+Speech+Recognition+Oscar+Chang+Hank+Liao+Dmitriy+Serdyuk+Ankit+Shah+Olivier+Siohan",
    "gs_search_success": false,
    "gs_authors": [],
    "citation_count": null,
    "error_type": "fetch_failed",
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2403.10722",
    "title": "Cannabis Seed Variant Detection using Faster R-CNN",
    "year": 2024,
    "published": "2024-03-15T22:49:47Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Analyzing and detecting cannabis seed variants is crucial for the agriculture industry. It enables precision breeding, allowing cultivators to selectively enhance desirable traits. Accurate identification of seed variants also ensures regulatory compliance, facilitating the cultivation of specific cannabis strains with defined characteristics, ultimately improving agricultural productivity and meeting diverse market demands. This paper presents a study on cannabis seed variant detection by emplo",
    "arxiv_url": "https://arxiv.org/abs/2403.10722v1",
    "pdf_url": "https://arxiv.org/pdf/2403.10722v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2403.10722",
    "arxiv_authors": [
      "Toqi Tahamid Sarker",
      "Taminul Islam",
      "Khaled R Ahmed"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Cannabis+Seed+Variant+Detection+using+Faster+R-CNN+Toqi+Tahamid+Sarker+Taminul+Islam+Khaled+R+Ahmed",
    "gs_search_success": true,
    "gs_authors": [
      "i1SmuwYAAAAJ",
      "FYKqgh4AAAAJ",
      "Kgo_S9sAAAAJ"
    ],
    "citation_count": 7,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2308.09380",
    "title": "Deciphering knee osteoarthritis diagnostic features with explainable artificial intelligence: A systematic review",
    "year": 2023,
    "published": "2023-08-18T08:23:47Z",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "abstract": "Existing artificial intelligence (AI) models for diagnosing knee osteoarthritis (OA) have faced criticism for their lack of transparency and interpretability, despite achieving medical-expert-like performance. This opacity makes them challenging to trust in clinical practice. Recently, explainable artificial intelligence (XAI) has emerged as a specialized technique that can provide confidence in the model's prediction by revealing how the prediction is derived, thus promoting the use of AI syste",
    "arxiv_url": "https://arxiv.org/abs/2308.09380v1",
    "pdf_url": "https://arxiv.org/pdf/2308.09380v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2308.09380",
    "arxiv_authors": [
      "Yun Xin Teoh",
      "Alice Othmani",
      "Siew Li Goh",
      "Juliana Usman",
      "Khin Wee Lai"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Deciphering+knee+osteoarthritis+diagnostic+features+with+explainable+artificial+intelligence%3A+A+systematic+review+Yun+Xin+Teoh+Alice+Othmani+Siew+Li+Goh+Juliana+Usman+Khin+Wee+Lai",
    "gs_search_success": true,
    "gs_authors": [
      "axupJGAAAAAJ",
      "vzmTHjsAAAAJ",
      "UX_D-VgAAAAJ",
      "MuTNEIUAAAAJ"
    ],
    "citation_count": 15,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2407.04245",
    "title": "Every Pixel Has its Moments: Ultra-High-Resolution Unpaired Image-to-Image Translation via Dense Normalization",
    "year": 2024,
    "published": "2024-07-05T04:14:50Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Recent advancements in ultra-high-resolution unpaired image-to-image translation have aimed to mitigate the constraints imposed by limited GPU memory through patch-wise inference. Nonetheless, existing methods often compromise between the reduction of noticeable tiling artifacts and the preservation of color and hue contrast, attributed to the reliance on global image- or patch-level statistics in the instance normalization layers. In this study, we introduce a Dense Normalization (DN) layer des",
    "arxiv_url": "https://arxiv.org/abs/2407.04245v1",
    "pdf_url": "https://arxiv.org/pdf/2407.04245v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2407.04245",
    "arxiv_authors": [
      "Ming-Yang Ho",
      "Che-Ming Wu",
      "Min-Sheng Wu",
      "Yufeng Jane Tseng"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Every+Pixel+Has+its+Moments%3A+Ultra-High-Resolution+Unpaired+Image-to-Image+Translation+via+Dense+Normalization+Ming-Yang+Ho+Che-Ming+Wu+Min-Sheng+Wu+Yufeng+Jane+Tseng",
    "gs_search_success": true,
    "gs_authors": [
      "NRmTj0sAAAAJ",
      "QOHrBUYAAAAJ",
      "CmvyshAAAAAJ",
      "PFh1h-kAAAAJ"
    ],
    "citation_count": 9,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2310.16590",
    "title": "$\\mathbb{VD}$-$\\mathbb{GR}$: Boosting $\\mathbb{V}$isual $\\mathbb{D}$ialog with Cascaded Spatial-Temporal Multi-Modal $\\mathbb{GR}$aphs",
    "year": 2023,
    "published": "2023-10-25T12:25:53Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "We propose $\\mathbb{VD}$-$\\mathbb{GR}$ - a novel visual dialog model that combines pre-trained language models (LMs) with graph neural networks (GNNs). Prior works mainly focused on one class of models at the expense of the other, thus missing out on the opportunity of combining their respective benefits. At the core of $\\mathbb{VD}$-$\\mathbb{GR}$ is a novel integration mechanism that alternates between spatial-temporal multi-modal GNNs and BERT layers, and that covers three distinct contributio",
    "arxiv_url": "https://arxiv.org/abs/2310.16590v1",
    "pdf_url": "https://arxiv.org/pdf/2310.16590v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2310.16590",
    "arxiv_authors": [
      "Adnen Abdessaied",
      "Lei Shi",
      "Andreas Bulling"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=%24%5Cmathbb%7BVD%7D%24-%24%5Cmathbb%7BGR%7D%24%3A+Boosting+%24%5Cmathbb%7BV%7D%24isual+%24%5Cmathbb%7BD%7D%24ialog+with+Cascaded+Spatial-Temporal+Multi-Modal+%24%5Cmathbb%7BGR%7D%24aphs+Adnen+Abdessaied+Lei+Shi+Andreas+Bulling",
    "gs_search_success": true,
    "gs_authors": [
      "nsuJs6QAAAAJ",
      "GRF1XoMAAAAJ",
      "QURZIzUAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2503.00897",
    "title": "A Simple and Effective Reinforcement Learning Method for Text-to-Image Diffusion Fine-tuning",
    "year": 2025,
    "published": "2025-03-02T13:43:53Z",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "abstract": "Reinforcement learning (RL)-based fine-tuning has emerged as a powerful approach for aligning diffusion models with black-box objectives. Proximal policy optimization (PPO) is the most popular choice of method for policy optimization. While effective in terms of performance, PPO is highly sensitive to hyper-parameters and involves substantial computational overhead. REINFORCE, on the other hand, mitigates some computational complexities such as high memory overhead and sensitive hyper-parameter ",
    "arxiv_url": "https://arxiv.org/abs/2503.00897v6",
    "pdf_url": "https://arxiv.org/pdf/2503.00897v6",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2503.00897",
    "arxiv_authors": [
      "Shashank Gupta",
      "Chaitanya Ahuja",
      "Tsung-Yu Lin",
      "Sreya Dutta Roy",
      "Harrie Oosterhuis",
      "Maarten de Rijke",
      "Satya Narayan Shukla"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=A+Simple+and+Effective+Reinforcement+Learning+Method+for+Text-to-Image+Diffusion+Fine-tuning+Shashank+Gupta+Chaitanya+Ahuja+Tsung-Yu+Lin+Sreya+Dutta+Roy+Harrie+Oosterhuis",
    "gs_search_success": true,
    "gs_authors": [
      "AVDkgFIAAAAJ",
      "ahFtDysAAAAJ",
      "CX8zqPoAAAAJ",
      "e9JynrAAAAAJ",
      "UvTcU-IAAAAJ",
      "l1tsmesAAAAJ"
    ],
    "citation_count": 12,
    "gs_author_count": 7
  },
  {
    "arxiv_id": "2309.00696",
    "title": "AAN: Attributes-Aware Network for Temporal Action Detection",
    "year": 2023,
    "published": "2023-09-01T18:35:31Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "The challenge of long-term video understanding remains constrained by the efficient extraction of object semantics and the modelling of their relationships for downstream tasks. Although the CLIP visual features exhibit discriminative properties for various vision tasks, particularly in object encoding, they are suboptimal for long-term video understanding. To address this issue, we present the Attributes-Aware Network (AAN), which consists of two key components: the Attributes Extractor and a G",
    "arxiv_url": "https://arxiv.org/abs/2309.00696v1",
    "pdf_url": "https://arxiv.org/pdf/2309.00696v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2309.00696",
    "arxiv_authors": [
      "Rui Dai",
      "Srijan Das",
      "Michael S. Ryoo",
      "Francois Bremond"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=AAN%3A+Attributes-Aware+Network+for+Temporal+Action+Detection+Rui+Dai+Srijan+Das+Michael+S.+Ryoo+Francois+Bremond",
    "gs_search_success": true,
    "gs_authors": [
      "vcw0TJIAAAAJ",
      "h-oGBzsAAAAJ",
      "ZDTF5AEAAAAJ",
      "V1s0tTwAAAAJ"
    ],
    "citation_count": 4,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2407.16309",
    "title": "A new visual quality metric for Evaluating the performance of multidimensional projections",
    "year": 2024,
    "published": "2024-07-23T09:02:46Z",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "abstract": "Multidimensional projections (MP) are among the most essential approaches in the visual analysis of multidimensional data. It transforms multidimensional data into two-dimensional representations that may be shown as scatter plots while preserving their similarity with the original data. Human visual perception is frequently used to evaluate the quality of MP. In this work, we propose to study and improve on a well-known map called Local Affine Multidimensional Projection (LAMP), which takes a m",
    "arxiv_url": "https://arxiv.org/abs/2407.16309v1",
    "pdf_url": "https://arxiv.org/pdf/2407.16309v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2407.16309",
    "arxiv_authors": [
      "Maniru Ibrahim",
      "Thales Vieira"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=A+new+visual+quality+metric+for+Evaluating+the+performance+of+multidimensional+projections+Maniru+Ibrahim+Thales+Vieira",
    "gs_search_success": true,
    "gs_authors": [
      "meFmL2sAAAAJ",
      "Qz6kX58AAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 2
  },
  {
    "arxiv_id": "2307.07663",
    "title": "INVE: Interactive Neural Video Editing",
    "year": 2023,
    "published": "2023-07-15T00:02:41Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "We present Interactive Neural Video Editing (INVE), a real-time video editing solution, which can assist the video editing process by consistently propagating sparse frame edits to the entire video clip. Our method is inspired by the recent work on Layered Neural Atlas (LNA). LNA, however, suffers from two major drawbacks: (1) the method is too slow for interactive editing, and (2) it offers insufficient support for some editing use cases, including direct frame editing and rigid texture trackin",
    "arxiv_url": "https://arxiv.org/abs/2307.07663v1",
    "pdf_url": "https://arxiv.org/pdf/2307.07663v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2307.07663",
    "arxiv_authors": [
      "Jiahui Huang",
      "Leonid Sigal",
      "Kwang Moo Yi",
      "Oliver Wang",
      "Joon-Young Lee"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=INVE%3A+Interactive+Neural+Video+Editing+Jiahui+Huang+Leonid+Sigal+Kwang+Moo+Yi+Oliver+Wang+Joon-Young+Lee",
    "gs_search_success": true,
    "gs_authors": [
      "qWU4y1wAAAAJ",
      "pr6rIJEAAAAJ",
      "6ajse1YAAAAJ",
      "0B8uuBkAAAAJ",
      "P2mG6rcAAAAJ"
    ],
    "citation_count": 19,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2309.09256",
    "title": "LiDAR Data Synthesis with Denoising Diffusion Probabilistic Models",
    "year": 2023,
    "published": "2023-09-17T12:26:57Z",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "abstract": "Generative modeling of 3D LiDAR data is an emerging task with promising applications for autonomous mobile robots, such as scalable simulation, scene manipulation, and sparse-to-dense completion of LiDAR point clouds. While existing approaches have demonstrated the feasibility of image-based LiDAR data generation using deep generative models, they still struggle with fidelity and training stability. In this work, we present R2DM, a novel generative model for LiDAR data that can generate diverse ",
    "arxiv_url": "https://arxiv.org/abs/2309.09256v2",
    "pdf_url": "https://arxiv.org/pdf/2309.09256v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2309.09256",
    "arxiv_authors": [
      "Kazuto Nakashima",
      "Ryo Kurazume"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=LiDAR+Data+Synthesis+with+Denoising+Diffusion+Probabilistic+Models+Kazuto+Nakashima+Ryo+Kurazume",
    "gs_search_success": true,
    "gs_authors": [
      "VnDvavYAAAAJ",
      "PGhVyVcAAAAJ"
    ],
    "citation_count": 53,
    "gs_author_count": 2
  },
  {
    "arxiv_id": "2503.21721",
    "title": "Evaluating Text-to-Image and Text-to-Video Synthesis with a Conditional Fréchet Distance",
    "year": 2025,
    "published": "2025-03-27T17:35:14Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Evaluating text-to-image and text-to-video models is challenging due to a fundamental disconnect: established metrics fail to jointly measure visual quality and semantic alignment with text, leading to a poor correlation with human judgments. To address this critical issue, we propose cFreD, a general metric based on a Conditional Fréchet Distance that unifies the assessment of visual fidelity and text-prompt consistency into a single score. Existing metrics such as Fréchet Inception Distance (F",
    "arxiv_url": "https://arxiv.org/abs/2503.21721v2",
    "pdf_url": "https://arxiv.org/pdf/2503.21721v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2503.21721",
    "arxiv_authors": [
      "Jaywon Koo",
      "Jefferson Hernandez",
      "Moayed Haji-Ali",
      "Ziyan Yang",
      "Vicente Ordonez"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Evaluating+Text-to-Image+and+Text-to-Video+Synthesis+with+a+Conditional+Fr%C3%A9chet+Distance+Jaywon+Koo+Jefferson+Hernandez+Moayed+Haji-Ali+Ziyan+Yang+Vicente+Ordonez",
    "gs_search_success": true,
    "gs_authors": [
      "TtA_j4YAAAAJ",
      "amZrKVQAAAAJ",
      "nJya9woAAAAJ",
      "Ck-mSEwAAAAJ",
      "Xk6-2C0AAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2405.10305",
    "title": "4D Panoptic Scene Graph Generation",
    "year": 2024,
    "published": "2024-05-16T17:56:55Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "We are living in a three-dimensional space while moving forward through a fourth dimension: time. To allow artificial intelligence to develop a comprehensive understanding of such a 4D environment, we introduce 4D Panoptic Scene Graph (PSG-4D), a new representation that bridges the raw visual data perceived in a dynamic 4D world and high-level visual understanding. Specifically, PSG-4D abstracts rich 4D sensory data into nodes, which represent entities with precise location and status informatio",
    "arxiv_url": "https://arxiv.org/abs/2405.10305v1",
    "pdf_url": "https://arxiv.org/pdf/2405.10305v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2405.10305",
    "arxiv_authors": [
      "Jingkang Yang",
      "Jun Cen",
      "Wenxuan Peng",
      "Shuai Liu",
      "Fangzhou Hong",
      "Xiangtai Li",
      "Kaiyang Zhou",
      "Qifeng Chen",
      "Ziwei Liu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=4D+Panoptic+Scene+Graph+Generation+Jingkang+Yang+Jun+Cen+Wenxuan+Peng+Shuai+Liu+Fangzhou+Hong",
    "gs_search_success": true,
    "gs_authors": [
      "7SKAhBwAAAAJ",
      "mhaiL5MAAAAJ",
      "lLMX9hcAAAAJ",
      "W9190BQAAAAJ",
      "lc45xlcAAAAJ",
      "I_B6IHIAAAAJ",
      "S-YjbUYAAAAJ",
      "gRIejugAAAAJ",
      "FL3ReD0AAAAJ"
    ],
    "citation_count": 29,
    "gs_author_count": 9
  },
  {
    "arxiv_id": "2505.04758",
    "title": "Lightweight RGB-D Salient Object Detection from a Speed-Accuracy Tradeoff Perspective",
    "year": 2025,
    "published": "2025-05-07T19:37:20Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Current RGB-D methods usually leverage large-scale backbones to improve accuracy but sacrifice efficiency. Meanwhile, several existing lightweight methods are difficult to achieve high-precision performance. To balance the efficiency and performance, we propose a Speed-Accuracy Tradeoff Network (SATNet) for Lightweight RGB-D SOD from three fundamental perspectives: depth quality, modality fusion, and feature representation. Concerning depth quality, we introduce the Depth Anything Model to gener",
    "arxiv_url": "https://arxiv.org/abs/2505.04758v1",
    "pdf_url": "https://arxiv.org/pdf/2505.04758v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2505.04758",
    "arxiv_authors": [
      "Songsong Duan",
      "Xi Yang",
      "Nannan Wang",
      "Xinbo Gao"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Lightweight+RGB-D+Salient+Object+Detection+from+a+Speed-Accuracy+Tradeoff+Perspective+Songsong+Duan+Xi+Yang+Nannan+Wang+Xinbo+Gao",
    "gs_search_success": true,
    "gs_authors": [
      "VZVTOOIAAAAJ",
      "W5c-LSYAAAAJ",
      "SRBn7oUAAAAJ"
    ],
    "citation_count": 7,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2407.21640",
    "title": "MSA$^2$Net: Multi-scale Adaptive Attention-guided Network for Medical Image Segmentation",
    "year": 2024,
    "published": "2024-07-31T14:41:10Z",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "abstract": "Medical image segmentation involves identifying and separating object instances in a medical image to delineate various tissues and structures, a task complicated by the significant variations in size, shape, and density of these features. Convolutional neural networks (CNNs) have traditionally been used for this task but have limitations in capturing long-range dependencies. Transformers, equipped with self-attention mechanisms, aim to address this problem. However, in medical image segmentatio",
    "arxiv_url": "https://arxiv.org/abs/2407.21640v3",
    "pdf_url": "https://arxiv.org/pdf/2407.21640v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2407.21640",
    "arxiv_authors": [
      "Sina Ghorbani Kolahi",
      "Seyed Kamal Chaharsooghi",
      "Toktam Khatibi",
      "Afshin Bozorgpour",
      "Reza Azad",
      "Moein Heidari",
      "Ilker Hacihaliloglu",
      "Dorit Merhof"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=MSA%24%5E2%24Net%3A+Multi-scale+Adaptive+Attention-guided+Network+for+Medical+Image+Segmentation+Sina+Ghorbani+Kolahi+Seyed+Kamal+Chaharsooghi+Toktam+Khatibi+Afshin+Bozorgpour+Reza+Azad",
    "gs_search_success": true,
    "gs_authors": [
      "Lxl6D2gAAAAJ",
      "IitYW5AAAAAJ"
    ],
    "citation_count": 25,
    "gs_author_count": 8
  },
  {
    "arxiv_id": "2410.03143",
    "title": "ECHOPulse: ECG controlled echocardio-grams video generation",
    "year": 2024,
    "published": "2024-10-04T04:49:56Z",
    "categories": [
      "eess.IV",
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "Echocardiography (ECHO) is essential for cardiac assessments, but its video quality and interpretation heavily relies on manual expertise, leading to inconsistent results from clinical and portable devices. ECHO video generation offers a solution by improving automated monitoring through synthetic data and generating high-quality videos from routine health data. However, existing models often face high computational costs, slow inference, and rely on complex conditional prompts that require expe",
    "arxiv_url": "https://arxiv.org/abs/2410.03143v2",
    "pdf_url": "https://arxiv.org/pdf/2410.03143v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2410.03143",
    "arxiv_authors": [
      "Yiwei Li",
      "Sekeun Kim",
      "Zihao Wu",
      "Hanqi Jiang",
      "Yi Pan",
      "Pengfei Jin",
      "Sifan Song",
      "Yucheng Shi",
      "Tianming Liu",
      "Quanzheng Li",
      "Xiang Li"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=ECHOPulse%3A+ECG+controlled+echocardio-grams+video+generation+Yiwei+Li+Sekeun+Kim+Zihao+Wu+Hanqi+Jiang+Yi+Pan",
    "gs_search_success": true,
    "gs_authors": [
      "RMvoE4sAAAAJ",
      "MjkwwiQAAAAJ",
      "rIFRHvIAAAAJ",
      "4S5I1TwAAAAJ",
      "MHq2z7oAAAAJ",
      "A-SP7VYAAAAJ",
      "92RPXm0AAAAJ",
      "sfEPWiAAAAAJ",
      "jENRqN8AAAAJ",
      "adC1a0IAAAAJ",
      "tU6pacYAAAAJ"
    ],
    "citation_count": 6,
    "gs_author_count": 9
  },
  {
    "arxiv_id": "2402.18925",
    "title": "PCDepth: Pattern-based Complementary Learning for Monocular Depth Estimation by Best of Both Worlds",
    "year": 2024,
    "published": "2024-02-29T07:31:59Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Event cameras can record scene dynamics with high temporal resolution, providing rich scene details for monocular depth estimation (MDE) even at low-level illumination. Therefore, existing complementary learning approaches for MDE fuse intensity information from images and scene details from event data for better scene understanding. However, most methods directly fuse two modalities at pixel level, ignoring that the attractive complementarity mainly impacts high-level patterns that only occupy ",
    "arxiv_url": "https://arxiv.org/abs/2402.18925v1",
    "pdf_url": "https://arxiv.org/pdf/2402.18925v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2402.18925",
    "arxiv_authors": [
      "Haotian Liu",
      "Sanqing Qu",
      "Fan Lu",
      "Zongtao Bu",
      "Florian Roehrbein",
      "Alois Knoll",
      "Guang Chen"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=PCDepth%3A+Pattern-based+Complementary+Learning+for+Monocular+Depth+Estimation+by+Best+of+Both+Worlds+Haotian+Liu+Sanqing+Qu+Fan+Lu+Zongtao+Bu+Florian+Roehrbein",
    "gs_search_success": true,
    "gs_authors": [
      "DyEUPFUAAAAJ",
      "kBhIyv4AAAAJ",
      "-CA8QgwAAAAJ",
      "IEOJBbAAAAAJ",
      "fbAyN08AAAAJ",
      "pZk-LBIAAAAJ"
    ],
    "citation_count": 8,
    "gs_author_count": 7
  },
  {
    "arxiv_id": "2411.10180",
    "title": "CART: Compositional Auto-Regressive Transformer for Image Generation",
    "year": 2024,
    "published": "2024-11-15T13:29:44Z",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "We propose a novel Auto-Regressive (AR) image generation approach that models images as hierarchical compositions of interpretable visual layers. While AR models have achieved transformative success in language modeling, replicating this success in vision tasks remains challenging due to inherent spatial dependencies in images. Addressing the unique challenges of vision tasks, our method (CART) adds image details iteratively via semantically meaningful decompositions. We demonstrate the flexibil",
    "arxiv_url": "https://arxiv.org/abs/2411.10180v3",
    "pdf_url": "https://arxiv.org/pdf/2411.10180v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2411.10180",
    "arxiv_authors": [
      "Siddharth Roheda",
      "Rohit Chowdhury",
      "Aniruddha Bala",
      "Rohan Jaiswal"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=CART%3A+Compositional+Auto-Regressive+Transformer+for+Image+Generation+Siddharth+Roheda+Rohit+Chowdhury+Aniruddha+Bala+Rohan+Jaiswal",
    "gs_search_success": true,
    "gs_authors": [
      "oajO0OMAAAAJ",
      "cmCuIeYAAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2407.10559",
    "title": "LIP-CAR: contrast agent reduction by a deep learned inverse problem",
    "year": 2024,
    "published": "2024-07-15T09:16:54Z",
    "categories": [
      "cs.CV",
      "eess.IV",
      "math.NA"
    ],
    "abstract": "The adoption of contrast agents in medical imaging protocols is crucial for accurate and timely diagnosis. While highly effective and characterized by an excellent safety profile, the use of contrast agents has its limitation, including rare risk of allergic reactions, potential environmental impact and economic burdens on patients and healthcare systems. In this work, we address the contrast agent reduction (CAR) problem, which involves reducing the administered dosage of contrast agent while p",
    "arxiv_url": "https://arxiv.org/abs/2407.10559v1",
    "pdf_url": "https://arxiv.org/pdf/2407.10559v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2407.10559",
    "arxiv_authors": [
      "Davide Bianchi",
      "Sonia Colombo Serra",
      "Davide Evangelista",
      "Pengpeng Luo",
      "Elena Morotti",
      "Giovanni Valbusa"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=LIP-CAR%3A+contrast+agent+reduction+by+a+deep+learned+inverse+problem+Davide+Bianchi+Sonia+Colombo+Serra+Davide+Evangelista+Pengpeng+Luo+Elena+Morotti",
    "gs_search_success": true,
    "gs_authors": [
      "s1GjMQMAAAAJ",
      "Ji7V1qEAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2304.10054",
    "title": "Complex Mixer for MedMNIST Classification Decathlon",
    "year": 2023,
    "published": "2023-04-20T02:34:36Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "With the development of the medical image field, researchers seek to develop a class of datasets to block the need for medical knowledge, such as \\text{MedMNIST} (v2). MedMNIST (v2) includes a large number of small-sized (28 $\\times$ 28 or 28 $\\times$ 28 $\\times$ 28) medical samples and the corresponding expert annotations (class label). The existing baseline model (Google AutoML Vision, ResNet-50+3D) can reach an average accuracy of over 70\\% on MedMNIST (v2) datasets, which is comparable to th",
    "arxiv_url": "https://arxiv.org/abs/2304.10054v1",
    "pdf_url": "https://arxiv.org/pdf/2304.10054v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2304.10054",
    "arxiv_authors": [
      "Zhuoran Zheng",
      "Xiuyi Jia"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Complex+Mixer+for+MedMNIST+Classification+Decathlon+Zhuoran+Zheng+Xiuyi+Jia",
    "gs_search_success": true,
    "gs_authors": [
      "pXzPL-sAAAAJ",
      "sEFaytgAAAAJ"
    ],
    "citation_count": 9,
    "gs_author_count": 2
  },
  {
    "arxiv_id": "2309.02435",
    "title": "Efficient RL via Disentangled Environment and Agent Representations",
    "year": 2023,
    "published": "2023-09-05T17:59:45Z",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "cs.NE",
      "cs.RO"
    ],
    "abstract": "Agents that are aware of the separation between themselves and their environments can leverage this understanding to form effective representations of visual input. We propose an approach for learning such structured representations for RL algorithms, using visual knowledge of the agent, such as its shape or mask, which is often inexpensive to obtain. This is incorporated into the RL objective using a simple auxiliary loss. We show that our method, Structured Environment-Agent Representations, o",
    "arxiv_url": "https://arxiv.org/abs/2309.02435v1",
    "pdf_url": "https://arxiv.org/pdf/2309.02435v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2309.02435",
    "arxiv_authors": [
      "Kevin Gmelin",
      "Shikhar Bahl",
      "Russell Mendonca",
      "Deepak Pathak"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Efficient+RL+via+Disentangled+Environment+and+Agent+Representations+Kevin+Gmelin+Shikhar+Bahl+Russell+Mendonca+Deepak+Pathak",
    "gs_search_success": true,
    "gs_authors": [
      "Uly5spMAAAAJ",
      "AEsPCAUAAAAJ",
      "bdHgGgEAAAAJ"
    ],
    "citation_count": 15,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2307.16256",
    "title": "3D Medical Image Segmentation with Sparse Annotation via Cross-Teaching between 3D and 2D Networks",
    "year": 2023,
    "published": "2023-07-30T15:26:17Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Medical image segmentation typically necessitates a large and precisely annotated dataset. However, obtaining pixel-wise annotation is a labor-intensive task that requires significant effort from domain experts, making it challenging to obtain in practical clinical scenarios. In such situations, reducing the amount of annotation required is a more practical approach. One feasible direction is sparse annotation, which involves annotating only a few slices, and has several advantages over traditio",
    "arxiv_url": "https://arxiv.org/abs/2307.16256v1",
    "pdf_url": "https://arxiv.org/pdf/2307.16256v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2307.16256",
    "arxiv_authors": [
      "Heng Cai",
      "Lei Qi",
      "Qian Yu",
      "Yinghuan Shi",
      "Yang Gao"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=3D+Medical+Image+Segmentation+with+Sparse+Annotation+via+Cross-Teaching+between+3D+and+2D+Networks+Heng+Cai+Lei+Qi+Qian+Yu+Yinghuan+Shi+Yang+Gao",
    "gs_search_success": true,
    "gs_authors": [
      "wlm5blIAAAAJ",
      "k0jjQo8AAAAJ",
      "m6BKDUMAAAAJ",
      "7mm8iZwAAAAJ"
    ],
    "citation_count": 25,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2304.04653",
    "title": "Do We Train on Test Data? The Impact of Near-Duplicates on License Plate Recognition",
    "year": 2023,
    "published": "2023-04-10T15:24:29Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "This work draws attention to the large fraction of near-duplicates in the training and test sets of datasets widely adopted in License Plate Recognition (LPR) research. These duplicates refer to images that, although different, show the same license plate. Our experiments, conducted on the two most popular datasets in the field, show a substantial decrease in recognition rate when six well-known models are trained and tested under fair splits, that is, in the absence of duplicates in the trainin",
    "arxiv_url": "https://arxiv.org/abs/2304.04653v2",
    "pdf_url": "https://arxiv.org/pdf/2304.04653v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2304.04653",
    "arxiv_authors": [
      "Rayson Laroca",
      "Valter Estevam",
      "Alceu S. Britto",
      "Rodrigo Minetto",
      "David Menotti"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Do+We+Train+on+Test+Data%3F+The+Impact+of+Near-Duplicates+on+License+Plate+Recognition+Rayson+Laroca+Valter+Estevam+Alceu+S.+Britto+Rodrigo+Minetto+David+Menotti",
    "gs_search_success": true,
    "gs_authors": [
      "DSZaSPcAAAAJ",
      "Ipu5_-gAAAAJ",
      "pRK8DCUAAAAJ",
      "9jt-hIMAAAAJ",
      "ntyfuEcAAAAJ"
    ],
    "citation_count": 21,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2305.09211",
    "title": "CB-HVTNet: A channel-boosted hybrid vision transformer network for lymphocyte assessment in histopathological images",
    "year": 2023,
    "published": "2023-05-16T06:40:04Z",
    "categories": [
      "eess.IV",
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "Transformers, due to their ability to learn long range dependencies, have overcome the shortcomings of convolutional neural networks (CNNs) for global perspective learning. Therefore, they have gained the focus of researchers for several vision related tasks including medical diagnosis. However, their multi-head attention module only captures global level feature representations, which is insufficient for medical images. To address this issue, we propose a Channel Boosted Hybrid Vision Transform",
    "arxiv_url": "https://arxiv.org/abs/2305.09211v3",
    "pdf_url": "https://arxiv.org/pdf/2305.09211v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2305.09211",
    "arxiv_authors": [
      "Momina Liaqat Ali",
      "Zunaira Rauf",
      "Asifullah Khan",
      "Anabia Sohail",
      "Rafi Ullah",
      "Jeonghwan Gwak"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=CB-HVTNet%3A+A+channel-boosted+hybrid+vision+transformer+network+for+lymphocyte+assessment+in+histopathological+images+Momina+Liaqat+Ali+Zunaira+Rauf+Asifullah+Khan+Anabia+Sohail+Rafi+Ullah",
    "gs_search_success": true,
    "gs_authors": [
      "d_vkseAAAAAJ",
      "FXrC-T4AAAAJ",
      "C8uhO88AAAAJ",
      "f2QH4MsAAAAJ",
      "S4f2ALkAAAAJ",
      "-mx1qCAAAAAJ"
    ],
    "citation_count": 8,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2412.00121",
    "title": "Hybrid Discriminative Attribute-Object Embedding Network for Compositional Zero-Shot Learning",
    "year": 2024,
    "published": "2024-11-28T09:50:25Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Compositional Zero-Shot Learning (CZSL) recognizes new combinations by learning from known attribute-object pairs. However, the main challenge of this task lies in the complex interactions between attributes and object visual representations, which lead to significant differences in images. In addition, the long-tail label distribution in the real world makes the recognition task more complicated. To address these problems, we propose a novel method, named Hybrid Discriminative Attribute-Object ",
    "arxiv_url": "https://arxiv.org/abs/2412.00121v1",
    "pdf_url": "https://arxiv.org/pdf/2412.00121v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2412.00121",
    "arxiv_authors": [
      "Yang Liu",
      "Xinshuo Wang",
      "Jiale Du",
      "Xinbo Gao",
      "Jungong Han"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Hybrid+Discriminative+Attribute-Object+Embedding+Network+for+Compositional+Zero-Shot+Learning+Yang+Liu+Xinshuo+Wang+Jiale+Du+Xinbo+Gao+Jungong+Han",
    "gs_search_success": true,
    "gs_authors": [
      "VZVTOOIAAAAJ",
      "hHK9Ik0AAAAJ",
      "hNi1gxAAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2412.16937",
    "title": "PINN-EMFNet: PINN-based and Enhanced Multi-Scale Feature Fusion Network for Breast Ultrasound Images Segmentation",
    "year": 2024,
    "published": "2024-12-22T09:16:00Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "With the rapid development of deep learning and computer vision technologies, medical image segmentation plays a crucial role in the early diagnosis of breast cancer. However, due to the characteristics of breast ultrasound images, such as low contrast, speckle noise, and the highly diverse morphology of tumors, existing segmentation methods exhibit significant limitations in terms of accuracy and robustness. To address these challenges, this study proposes a PINN-based and Enhanced Multi-Scale ",
    "arxiv_url": "https://arxiv.org/abs/2412.16937v1",
    "pdf_url": "https://arxiv.org/pdf/2412.16937v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2412.16937",
    "arxiv_authors": [
      "Jiajun Ding",
      "Beiyao Zhu",
      "Wenjie Wang",
      "Shurong Zhang",
      "Dian Zhua",
      "Zhao Liua"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=PINN-EMFNet%3A+PINN-based+and+Enhanced+Multi-Scale+Feature+Fusion+Network+for+Breast+Ultrasound+Images+Segmentation+Jiajun+Ding+Beiyao+Zhu+Wenjie+Wang+Shurong+Zhang+Dian+Zhua",
    "gs_search_success": true,
    "gs_authors": [],
    "citation_count": 3,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2305.14188",
    "title": "The Best Defense is a Good Offense: Adversarial Augmentation against Adversarial Attacks",
    "year": 2023,
    "published": "2023-05-23T16:07:58Z",
    "categories": [
      "cs.LG",
      "cs.CR",
      "cs.CV"
    ],
    "abstract": "Many defenses against adversarial attacks (\\eg robust classifiers, randomization, or image purification) use countermeasures put to work only after the attack has been crafted. We adopt a different perspective to introduce $A^5$ (Adversarial Augmentation Against Adversarial Attacks), a novel framework including the first certified preemptive defense against adversarial attacks. The main idea is to craft a defensive perturbation to guarantee that any attack (up to a given magnitude) towards the i",
    "arxiv_url": "https://arxiv.org/abs/2305.14188v1",
    "pdf_url": "https://arxiv.org/pdf/2305.14188v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2305.14188",
    "arxiv_authors": [
      "Iuri Frosio",
      "Jan Kautz"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=The+Best+Defense+is+a+Good+Offense%3A+Adversarial+Augmentation+against+Adversarial+Attacks+Iuri+Frosio+Jan+Kautz",
    "gs_search_success": true,
    "gs_authors": [
      "PCJJ8LkAAAAJ",
      "P9FclNEAAAAJ"
    ],
    "citation_count": 31,
    "gs_author_count": 2
  },
  {
    "arxiv_id": "2405.01483",
    "title": "MANTIS: Interleaved Multi-Image Instruction Tuning",
    "year": 2024,
    "published": "2024-05-02T17:14:57Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "abstract": "Large multimodal models (LMMs) have shown great results in single-image vision language tasks. However, their abilities to solve multi-image visual language tasks is yet to be improved. The existing LMMs like OpenFlamingo, Emu2, and Idefics gain their multi-image ability through pre-training on hundreds of millions of noisy interleaved image-text data from the web, which is neither efficient nor effective. In this paper, we aim to build strong multi-image LMMs via instruction tuning with academi",
    "arxiv_url": "https://arxiv.org/abs/2405.01483v3",
    "pdf_url": "https://arxiv.org/pdf/2405.01483v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2405.01483",
    "arxiv_authors": [
      "Dongfu Jiang",
      "Xuan He",
      "Huaye Zeng",
      "Cong Wei",
      "Max Ku",
      "Qian Liu",
      "Wenhu Chen"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=MANTIS%3A+Interleaved+Multi-Image+Instruction+Tuning+Dongfu+Jiang+Xuan+He+Huaye+Zeng+Cong+Wei+Max+Ku",
    "gs_search_success": true,
    "gs_authors": [
      "bcbeUo0AAAAJ",
      "U8ShbhUAAAAJ",
      "I7WSsuMAAAAJ",
      "y1d5C5YAAAAJ",
      "G0_RcM4AAAAJ",
      "oCFgVhUAAAAJ",
      "kciKEPUAAAAJ"
    ],
    "citation_count": 202,
    "gs_author_count": 7
  },
  {
    "arxiv_id": "2407.14418",
    "title": "Improving classification of road surface conditions via road area extraction and contrastive learning",
    "year": 2024,
    "published": "2024-07-19T15:43:16Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Maintaining roads is crucial to economic growth and citizen well-being because roads are a vital means of transportation. In various countries, the inspection of road surfaces is still done manually, however, to automate it, research interest is now focused on detecting the road surface defects via the visual data. While, previous research has been focused on deep learning methods which tend to process the entire image and leads to heavy computational cost. In this study, we focus our attention ",
    "arxiv_url": "https://arxiv.org/abs/2407.14418v1",
    "pdf_url": "https://arxiv.org/pdf/2407.14418v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2407.14418",
    "arxiv_authors": [
      "Linh Trinh",
      "Ali Anwar",
      "Siegfried Mercelis"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Improving+classification+of+road+surface+conditions+via+road+area+extraction+and+contrastive+learning+Linh+Trinh+Ali+Anwar+Siegfried+Mercelis",
    "gs_search_success": true,
    "gs_authors": [
      "8bC9-DYAAAAJ",
      "7dUIGzkAAAAJ",
      "FtcBjx8AAAAJ"
    ],
    "citation_count": 2,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2505.22673",
    "title": "Physiology-Informed Generative Multi-Task Network for Contrast-Free CT Perfusion",
    "year": 2025,
    "published": "2025-05-12T22:58:55Z",
    "categories": [
      "q-bio.TO",
      "cs.AI",
      "cs.CV"
    ],
    "abstract": "Perfusion imaging is extensively utilized to assess hemodynamic status and tissue perfusion in various organs. Computed tomography perfusion (CTP) imaging plays a key role in the early assessment and planning of stroke treatment. While CTP provides essential perfusion parameters to identify abnormal blood flow in the brain, the use of contrast agents in CTP can lead to allergic reactions and adverse side effects, along with costing USD 4.9 billion worldwide in 2022. To address these challenges, ",
    "arxiv_url": "https://arxiv.org/abs/2505.22673v1",
    "pdf_url": "https://arxiv.org/pdf/2505.22673v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2505.22673",
    "arxiv_authors": [
      "Wasif Khan",
      "Kyle B. See",
      "Simon Kato",
      "Ziqian Huang",
      "Amy Lazarte",
      "Kyle Douglas",
      "Xiangyang Lou",
      "Teng J. Peng",
      "Dhanashree Rajderkar",
      "John Rees",
      "Pina Sanelli",
      "Amita Singh",
      "Ibrahim Tuna",
      "Christina A. Wilson",
      "Ruogu Fang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Physiology-Informed+Generative+Multi-Task+Network+for+Contrast-Free+CT+Perfusion+Wasif+Khan+Kyle+B.+See+Simon+Kato+Ziqian+Huang+Amy+Lazarte",
    "gs_search_success": true,
    "gs_authors": [
      "QwJDNZYAAAAJ",
      "9CmjlcMAAAAJ",
      "h7g65d4AAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 9
  },
  {
    "arxiv_id": "2503.19283",
    "title": "ISPDiffuser: Learning RAW-to-sRGB Mappings with Texture-Aware Diffusion Models and Histogram-Guided Color Consistency",
    "year": 2025,
    "published": "2025-03-25T02:29:39Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "RAW-to-sRGB mapping, or the simulation of the traditional camera image signal processor (ISP), aims to generate DSLR-quality sRGB images from raw data captured by smartphone sensors. Despite achieving comparable results to sophisticated handcrafted camera ISP solutions, existing learning-based methods still struggle with detail disparity and color distortion. In this paper, we present ISPDiffuser, a diffusion-based decoupled framework that separates the RAW-to-sRGB mapping into detail reconstruc",
    "arxiv_url": "https://arxiv.org/abs/2503.19283v1",
    "pdf_url": "https://arxiv.org/pdf/2503.19283v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2503.19283",
    "arxiv_authors": [
      "Yang Ren",
      "Hai Jiang",
      "Menglong Yang",
      "Wei Li",
      "Shuaicheng Liu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=ISPDiffuser%3A+Learning+RAW-to-sRGB+Mappings+with+Texture-Aware+Diffusion+Models+and+Histogram-Guided+Color+Consistency+Yang+Ren+Hai+Jiang+Menglong+Yang+Wei+Li+Shuaicheng+Liu",
    "gs_search_success": true,
    "gs_authors": [
      "GxgVfyVaOEwC",
      "ahVKn4AAAAAJ",
      "1DP9DAUAAAAJ",
      "ZKBt-wgAAAAJ"
    ],
    "citation_count": 2,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2308.00799",
    "title": "Body Knowledge and Uncertainty Modeling for Monocular 3D Human Body Reconstruction",
    "year": 2023,
    "published": "2023-08-01T19:29:10Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "While 3D body reconstruction methods have made remarkable progress recently, it remains difficult to acquire the sufficiently accurate and numerous 3D supervisions required for training. In this paper, we propose \\textbf{KNOWN}, a framework that effectively utilizes body \\textbf{KNOW}ledge and u\\textbf{N}certainty modeling to compensate for insufficient 3D supervisions. KNOWN exploits a comprehensive set of generic body constraints derived from well-established body knowledge. These generic cons",
    "arxiv_url": "https://arxiv.org/abs/2308.00799v1",
    "pdf_url": "https://arxiv.org/pdf/2308.00799v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2308.00799",
    "arxiv_authors": [
      "Yufei Zhang",
      "Hanjing Wang",
      "Jeffrey O. Kephart",
      "Qiang Ji"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Body+Knowledge+and+Uncertainty+Modeling+for+Monocular+3D+Human+Body+Reconstruction+Yufei+Zhang+Hanjing+Wang+Jeffrey+O.+Kephart+Qiang+Ji",
    "gs_search_success": false,
    "gs_authors": [],
    "citation_count": null,
    "error_type": "fetch_failed",
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2502.09026",
    "title": "Billet Number Recognition Based on Test-Time Adaptation",
    "year": 2025,
    "published": "2025-02-13T07:31:03Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "During the steel billet production process, it is essential to recognize machine-printed or manually written billet numbers on moving billets in real-time. To address the issue of low recognition accuracy for existing scene text recognition methods, caused by factors such as image distortions and distribution differences between training and test data, we propose a billet number recognition method that integrates test-time adaptation with prior knowledge. First, we introduce a test-time adaptati",
    "arxiv_url": "https://arxiv.org/abs/2502.09026v1",
    "pdf_url": "https://arxiv.org/pdf/2502.09026v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2502.09026",
    "arxiv_authors": [
      "Yuan Wei",
      "Xiuzhuang Zhou"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Billet+Number+Recognition+Based+on+Test-Time+Adaptation+Yuan+Wei+Xiuzhuang+Zhou",
    "gs_search_success": true,
    "gs_authors": [],
    "citation_count": null,
    "gs_author_count": 2
  },
  {
    "arxiv_id": "2307.00717",
    "title": "SSC3OD: Sparsely Supervised Collaborative 3D Object Detection from LiDAR Point Clouds",
    "year": 2023,
    "published": "2023-07-03T02:42:14Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Collaborative 3D object detection, with its improved interaction advantage among multiple agents, has been widely explored in autonomous driving. However, existing collaborative 3D object detectors in a fully supervised paradigm heavily rely on large-scale annotated 3D bounding boxes, which is labor-intensive and time-consuming. To tackle this issue, we propose a sparsely supervised collaborative 3D object detection framework SSC3OD, which only requires each agent to randomly label one object in",
    "arxiv_url": "https://arxiv.org/abs/2307.00717v1",
    "pdf_url": "https://arxiv.org/pdf/2307.00717v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2307.00717",
    "arxiv_authors": [
      "Yushan Han",
      "Hui Zhang",
      "Honglei Zhang",
      "Yidong Li"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=SSC3OD%3A+Sparsely+Supervised+Collaborative+3D+Object+Detection+from+LiDAR+Point+Clouds+Yushan+Han+Hui+Zhang+Honglei+Zhang+Yidong+Li",
    "gs_search_success": true,
    "gs_authors": [
      "X91w9HIAAAAJ",
      "L5o3ERIAAAAJ",
      "TLXKZswAAAAJ"
    ],
    "citation_count": 4,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2311.11013",
    "title": "Implicit Event-RGBD Neural SLAM",
    "year": 2023,
    "published": "2023-11-18T08:48:58Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Implicit neural SLAM has achieved remarkable progress recently. Nevertheless, existing methods face significant challenges in non-ideal scenarios, such as motion blur or lighting variation, which often leads to issues like convergence failures, localization drifts, and distorted mapping. To address these challenges, we propose EN-SLAM, the first event-RGBD implicit neural SLAM framework, which effectively leverages the high rate and high dynamic range advantages of event data for tracking and ma",
    "arxiv_url": "https://arxiv.org/abs/2311.11013v3",
    "pdf_url": "https://arxiv.org/pdf/2311.11013v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2311.11013",
    "arxiv_authors": [
      "Delin Qu",
      "Chi Yan",
      "Dong Wang",
      "Jie Yin",
      "Dan Xu",
      "Bin Zhao",
      "Xuelong Li"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Implicit+Event-RGBD+Neural+SLAM+Delin+Qu+Chi+Yan+Dong+Wang+Jie+Yin+Dan+Xu",
    "gs_search_success": true,
    "gs_authors": [
      "ahUibskAAAAJ",
      "OuSPv-AAAAAJ",
      "8IFwRIQAAAAJ",
      "YCGcVJ0AAAAJ",
      "dasL9V4AAAAJ",
      "DQB0hqwAAAAJ",
      "Y8LVRYIAAAAJ",
      "zgiFoOwAAAAJ"
    ],
    "citation_count": 27,
    "gs_author_count": 7
  },
  {
    "arxiv_id": "2502.07160",
    "title": "HDCompression: Hybrid-Diffusion Image Compression for Ultra-Low Bitrates",
    "year": 2025,
    "published": "2025-02-11T00:56:44Z",
    "categories": [
      "cs.CV",
      "cs.MM"
    ],
    "abstract": "Image compression under ultra-low bitrates remains challenging for both conventional learned image compression (LIC) and generative vector-quantized (VQ) modeling. Conventional LIC suffers from severe artifacts due to heavy quantization, while generative VQ modeling gives poor fidelity due to the mismatch between learned generative priors and specific inputs. In this work, we propose Hybrid-Diffusion Image Compression (HDCompression), a dual-stream framework that utilizes both generative VQ-mode",
    "arxiv_url": "https://arxiv.org/abs/2502.07160v3",
    "pdf_url": "https://arxiv.org/pdf/2502.07160v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2502.07160",
    "arxiv_authors": [
      "Lei Lu",
      "Yize Li",
      "Yanzhi Wang",
      "Wei Wang",
      "Wei Jiang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=HDCompression%3A+Hybrid-Diffusion+Image+Compression+for+Ultra-Low+Bitrates+Lei+Lu+Yize+Li+Yanzhi+Wang+Wei+Wang+Wei+Jiang",
    "gs_search_success": true,
    "gs_authors": [
      "aLii7l0AAAAJ",
      "c8koDJgAAAAJ",
      "a7akgIEAAAAJ",
      "_wzQ1EgAAAAJ",
      "rxyMOeAAAAAJ"
    ],
    "citation_count": 3,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2406.05821",
    "title": "F-LMM: Grounding Frozen Large Multimodal Models",
    "year": 2024,
    "published": "2024-06-09T15:14:26Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Endowing Large Multimodal Models (LMMs) with visual grounding capability can significantly enhance AIs' understanding of the visual world and their interaction with humans. However, existing methods typically fine-tune the parameters of LMMs to learn additional segmentation tokens and overfit grounding and segmentation datasets. Such a design would inevitably cause a catastrophic diminution in the indispensable conversational capability of general AI assistants. In this paper, we comprehensively",
    "arxiv_url": "https://arxiv.org/abs/2406.05821v3",
    "pdf_url": "https://arxiv.org/pdf/2406.05821v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2406.05821",
    "arxiv_authors": [
      "Size Wu",
      "Sheng Jin",
      "Wenwei Zhang",
      "Lumin Xu",
      "Wentao Liu",
      "Wei Li",
      "Chen Change Loy"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=F-LMM%3A+Grounding+Frozen+Large+Multimodal+Models+Size+Wu+Sheng+Jin+Wenwei+Zhang+Lumin+Xu+Wentao+Liu",
    "gs_search_success": true,
    "gs_authors": [
      "41KAd6AAAAAJ",
      "wrNd--oAAAAJ",
      "KZn9NWEAAAAJ",
      "559LF80AAAAJ",
      "dkXuy54AAAAJ",
      "y2S02IcAAAAJ",
      "QDXADSEAAAAJ"
    ],
    "citation_count": 25,
    "gs_author_count": 7
  },
  {
    "arxiv_id": "2504.09852",
    "title": "GFT: Gradient Focal Transformer",
    "year": 2025,
    "published": "2025-04-14T03:49:06Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Fine-Grained Image Classification (FGIC) remains a complex task in computer vision, as it requires models to distinguish between categories with subtle localized visual differences. Well-studied CNN-based models, while strong in local feature extraction, often fail to capture the global context required for fine-grained recognition, while more recent ViT-backboned models address FGIC with attention-driven mechanisms but lack the ability to adaptively focus on truly discriminative regions. TransF",
    "arxiv_url": "https://arxiv.org/abs/2504.09852v1",
    "pdf_url": "https://arxiv.org/pdf/2504.09852v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2504.09852",
    "arxiv_authors": [
      "Boris Kriuk",
      "Simranjit Kaur Gill",
      "Shoaib Aslam",
      "Amir Fakhrutdinov"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=GFT%3A+Gradient+Focal+Transformer+Boris+Kriuk+Simranjit+Kaur+Gill+Shoaib+Aslam+Amir+Fakhrutdinov",
    "gs_search_success": true,
    "gs_authors": [
      "24vkt1oAAAAJ",
      "KaV-T2gAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2302.03689",
    "title": "PartitionVAE -- a human-interpretable VAE",
    "year": 2023,
    "published": "2023-02-04T05:22:19Z",
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "abstract": "VAEs, or variational autoencoders, are autoencoders that explicitly learn the distribution of the input image space rather than assuming no prior information about the distribution. This allows it to classify similar samples close to each other in the latent space's distribution. VAEs classically assume the latent space is normally distributed, though many distribution priors work, and they encode this assumption through a K-L divergence term in the loss function. While VAEs learn the distributi",
    "arxiv_url": "https://arxiv.org/abs/2302.03689v1",
    "pdf_url": "https://arxiv.org/pdf/2302.03689v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2302.03689",
    "arxiv_authors": [
      "Fareed Sheriff",
      "Sameer Pai"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=PartitionVAE+--+a+human-interpretable+VAE+Fareed+Sheriff+Sameer+Pai",
    "gs_search_success": true,
    "gs_authors": [],
    "citation_count": 1,
    "gs_author_count": 2
  },
  {
    "arxiv_id": "2307.10768",
    "title": "Decoding the Enigma: Benchmarking Humans and AIs on the Many Facets of Working Memory",
    "year": 2023,
    "published": "2023-07-20T10:57:02Z",
    "categories": [
      "q-bio.NC",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "Working memory (WM), a fundamental cognitive process facilitating the temporary storage, integration, manipulation, and retrieval of information, plays a vital role in reasoning and decision-making tasks. Robust benchmark datasets that capture the multifaceted nature of WM are crucial for the effective development and evaluation of AI WM models. Here, we introduce a comprehensive Working Memory (WorM) benchmark dataset for this purpose. WorM comprises 10 tasks and a total of 1 million trials, as",
    "arxiv_url": "https://arxiv.org/abs/2307.10768v2",
    "pdf_url": "https://arxiv.org/pdf/2307.10768v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2307.10768",
    "arxiv_authors": [
      "Ankur Sikarwar",
      "Mengmi Zhang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Decoding+the+Enigma%3A+Benchmarking+Humans+and+AIs+on+the+Many+Facets+of+Working+Memory+Ankur+Sikarwar+Mengmi+Zhang",
    "gs_search_success": true,
    "gs_authors": [
      "G2sVOhcAAAAJ",
      "eWDiT_wAAAAJ"
    ],
    "citation_count": 4,
    "gs_author_count": 2
  },
  {
    "arxiv_id": "2503.24382",
    "title": "Free360: Layered Gaussian Splatting for Unbounded 360-Degree View Synthesis from Extremely Sparse and Unposed Views",
    "year": 2025,
    "published": "2025-03-31T17:59:25Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Neural rendering has demonstrated remarkable success in high-quality 3D neural reconstruction and novel view synthesis with dense input views and accurate poses. However, applying it to extremely sparse, unposed views in unbounded 360° scenes remains a challenging problem. In this paper, we propose a novel neural rendering framework to accomplish the unposed and extremely sparse-view 3D reconstruction in unbounded 360° scenes. To resolve the spatial ambiguity inherent in unbounded scenes with sp",
    "arxiv_url": "https://arxiv.org/abs/2503.24382v1",
    "pdf_url": "https://arxiv.org/pdf/2503.24382v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2503.24382",
    "arxiv_authors": [
      "Chong Bao",
      "Xiyu Zhang",
      "Zehao Yu",
      "Jiale Shi",
      "Guofeng Zhang",
      "Songyou Peng",
      "Zhaopeng Cui"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Free360%3A+Layered+Gaussian+Splatting+for+Unbounded+360-Degree+View+Synthesis+from+Extremely+Sparse+and+Unposed+Views+Chong+Bao+Xiyu+Zhang+Zehao+Yu+Jiale+Shi+Guofeng+Zhang",
    "gs_search_success": true,
    "gs_authors": [
      "4daSiAwAAAAJ",
      "eNypkO0AAAAJ",
      "Z8MwnzsAAAAJ",
      "HRHCYq0AAAAJ",
      "F0xfpXAAAAAJ",
      "vwIRwDUAAAAJ",
      "vvW-UNMAAAAJ"
    ],
    "citation_count": 6,
    "gs_author_count": 7
  },
  {
    "arxiv_id": "2310.03456",
    "title": "Multi-Resolution Audio-Visual Feature Fusion for Temporal Action Localization",
    "year": 2023,
    "published": "2023-10-05T10:54:33Z",
    "categories": [
      "cs.CV",
      "cs.LG",
      "cs.MM"
    ],
    "abstract": "Temporal Action Localization (TAL) aims to identify actions' start, end, and class labels in untrimmed videos. While recent advancements using transformer networks and Feature Pyramid Networks (FPN) have enhanced visual feature recognition in TAL tasks, less progress has been made in the integration of audio features into such frameworks. This paper introduces the Multi-Resolution Audio-Visual Feature Fusion (MRAV-FF), an innovative method to merge audio-visual data across different temporal res",
    "arxiv_url": "https://arxiv.org/abs/2310.03456v1",
    "pdf_url": "https://arxiv.org/pdf/2310.03456v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2310.03456",
    "arxiv_authors": [
      "Edward Fish",
      "Jon Weinbren",
      "Andrew Gilbert"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Multi-Resolution+Audio-Visual+Feature+Fusion+for+Temporal+Action+Localization+Edward+Fish+Jon+Weinbren+Andrew+Gilbert",
    "gs_search_success": true,
    "gs_authors": [
      "oLUdxEUAAAAJ",
      "G0YGBekAAAAJ"
    ],
    "citation_count": 2,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2304.00414",
    "title": "Learning Dynamic Style Kernels for Artistic Style Transfer",
    "year": 2023,
    "published": "2023-04-02T00:26:43Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Arbitrary style transfer has been demonstrated to be efficient in artistic image generation. Previous methods either globally modulate the content feature ignoring local details, or overly focus on the local structure details leading to style leakage. In contrast to the literature, we propose a new scheme \\textit{``style kernel\"} that learns {\\em spatially adaptive kernels} for per-pixel stylization, where the convolutional kernels are dynamically generated from the global style-content aligned ",
    "arxiv_url": "https://arxiv.org/abs/2304.00414v2",
    "pdf_url": "https://arxiv.org/pdf/2304.00414v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2304.00414",
    "arxiv_authors": [
      "Wenju Xu",
      "Chengjiang Long",
      "Yongwei Nie"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Learning+Dynamic+Style+Kernels+for+Artistic+Style+Transfer+Wenju+Xu+Chengjiang+Long+Yongwei+Nie",
    "gs_search_success": true,
    "gs_authors": [
      "jVlU_oQAAAAJ",
      "k0XkeiAAAAAJ"
    ],
    "citation_count": 36,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2505.06898",
    "title": "Multi-Modal Explainable Medical AI Assistant for Trustworthy Human-AI Collaboration",
    "year": 2025,
    "published": "2025-05-11T08:32:01Z",
    "categories": [
      "cs.CV",
      "cs.CL"
    ],
    "abstract": "Generalist Medical AI (GMAI) systems have demonstrated expert-level performance in biomedical perception tasks, yet their clinical utility remains limited by inadequate multi-modal explainability and suboptimal prognostic capabilities. Here, we present XMedGPT, a clinician-centric, multi-modal AI assistant that integrates textual and visual interpretability to support transparent and trustworthy medical decision-making. XMedGPT not only produces accurate diagnostic and descriptive outputs, but a",
    "arxiv_url": "https://arxiv.org/abs/2505.06898v1",
    "pdf_url": "https://arxiv.org/pdf/2505.06898v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2505.06898",
    "arxiv_authors": [
      "Honglong Yang",
      "Shanshan Song",
      "Yi Qin",
      "Lehan Wang",
      "Haonan Wang",
      "Xinpeng Ding",
      "Qixiang Zhang",
      "Bodong Du",
      "Xiaomeng Li"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Multi-Modal+Explainable+Medical+AI+Assistant+for+Trustworthy+Human-AI+Collaboration+Honglong+Yang+Shanshan+Song+Yi+Qin+Lehan+Wang+Haonan+Wang",
    "gs_search_success": true,
    "gs_authors": [
      "3BPUjoQAAAAJ",
      "qq-o-JoAAAAJ",
      "FzR7uSsAAAAJ",
      "KDNRnW0AAAAJ",
      "uVTzPpoAAAAJ",
      "oIcu4mgAAAAJ",
      "KqePyFoAAAAJ",
      "EoNWyTcAAAAJ"
    ],
    "citation_count": 3,
    "gs_author_count": 9
  },
  {
    "arxiv_id": "2308.06701",
    "title": "Camouflaged Image Synthesis Is All You Need to Boost Camouflaged Detection",
    "year": 2023,
    "published": "2023-08-13T06:55:05Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "abstract": "Camouflaged objects that blend into natural scenes pose significant challenges for deep-learning models to detect and synthesize. While camouflaged object detection is a crucial task in computer vision with diverse real-world applications, this research topic has been constrained by limited data availability. We propose a framework for synthesizing camouflage data to enhance the detection of camouflaged objects in natural scenes. Our approach employs a generative model to produce realistic camou",
    "arxiv_url": "https://arxiv.org/abs/2308.06701v2",
    "pdf_url": "https://arxiv.org/pdf/2308.06701v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2308.06701",
    "arxiv_authors": [
      "Haichao Zhang",
      "Can Qin",
      "Yu Yin",
      "Yun Fu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Camouflaged+Image+Synthesis+Is+All+You+Need+to+Boost+Camouflaged+Detection+Haichao+Zhang+Can+Qin+Yu+Yin+Yun+Fu",
    "gs_search_success": true,
    "gs_authors": [
      "h-JEcQ8AAAAJ",
      "QQoemgQAAAAJ",
      "QCik-YcAAAAJ",
      "pY0_YNcAAAAJ"
    ],
    "citation_count": 6,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2407.01014",
    "title": "An Expectation-Maximization Algorithm for Training Clean Diffusion Models from Corrupted Observations",
    "year": 2024,
    "published": "2024-07-01T07:00:17Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Diffusion models excel in solving imaging inverse problems due to their ability to model complex image priors. However, their reliance on large, clean datasets for training limits their practical use where clean data is scarce. In this paper, we propose EMDiffusion, an expectation-maximization (EM) approach to train diffusion models from corrupted observations. Our method alternates between reconstructing clean images from corrupted data using a known diffusion model (E-step) and refining diffus",
    "arxiv_url": "https://arxiv.org/abs/2407.01014v1",
    "pdf_url": "https://arxiv.org/pdf/2407.01014v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2407.01014",
    "arxiv_authors": [
      "Weimin Bai",
      "Yifei Wang",
      "Wenzheng Chen",
      "He Sun"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=An+Expectation-Maximization+Algorithm+for+Training+Clean+Diffusion+Models+from+Corrupted+Observations+Weimin+Bai+Yifei+Wang+Wenzheng+Chen+He+Sun",
    "gs_search_success": true,
    "gs_authors": [
      "c9V5HkYAAAAJ",
      "M6bEs8IAAAAJ",
      "KzhR_TsAAAAJ"
    ],
    "citation_count": 21,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2505.24361",
    "title": "Revisiting Cross-Modal Knowledge Distillation: A Disentanglement Approach for RGBD Semantic Segmentation",
    "year": 2025,
    "published": "2025-05-30T08:53:35Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Multi-modal RGB and Depth (RGBD) data are predominant in many domains such as robotics, autonomous driving and remote sensing. The combination of these multi-modal data enhances environmental perception by providing 3D spatial context, which is absent in standard RGB images. Although RGBD multi-modal data can be available to train computer vision models, accessing all sensor modalities during the inference stage may be infeasible due to sensor failures or resource constraints, leading to a misma",
    "arxiv_url": "https://arxiv.org/abs/2505.24361v1",
    "pdf_url": "https://arxiv.org/pdf/2505.24361v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2505.24361",
    "arxiv_authors": [
      "Roger Ferrod",
      "Cássio F. Dantas",
      "Luigi Di Caro",
      "Dino Ienco"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Revisiting+Cross-Modal+Knowledge+Distillation%3A+A+Disentanglement+Approach+for+RGBD+Semantic+Segmentation+Roger+Ferrod+C%C3%A1ssio+F.+Dantas+Luigi+Di+Caro+Dino+Ienco",
    "gs_search_success": true,
    "gs_authors": [
      "YgcZQpgAAAAJ",
      "MS7sDJgAAAAJ",
      "XQAvqi4AAAAJ",
      "C8zfH3kAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2305.15001",
    "title": "Contrastive Training of Complex-Valued Autoencoders for Object Discovery",
    "year": 2023,
    "published": "2023-05-24T10:37:43Z",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "abstract": "Current state-of-the-art object-centric models use slots and attention-based routing for binding. However, this class of models has several conceptual limitations: the number of slots is hardwired; all slots have equal capacity; training has high computational cost; there are no object-level relational factors within slots. Synchrony-based models in principle can address these limitations by using complex-valued activations which store binding information in their phase components. However, work",
    "arxiv_url": "https://arxiv.org/abs/2305.15001v3",
    "pdf_url": "https://arxiv.org/pdf/2305.15001v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2305.15001",
    "arxiv_authors": [
      "Aleksandar Stanić",
      "Anand Gopalakrishnan",
      "Kazuki Irie",
      "Jürgen Schmidhuber"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Contrastive+Training+of+Complex-Valued+Autoencoders+for+Object+Discovery+Aleksandar+Stani%C4%87+Anand+Gopalakrishnan+Kazuki+Irie+J%C3%BCrgen+Schmidhuber",
    "gs_search_success": true,
    "gs_authors": [
      "gLnCTgIAAAAJ",
      "tx0opKcAAAAJ",
      "-gZ-BdwAAAAJ",
      "SsbgJ1UAAAAJ"
    ],
    "citation_count": 20,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2307.00485",
    "title": "TopicFM+: Boosting Accuracy and Efficiency of Topic-Assisted Feature Matching",
    "year": 2023,
    "published": "2023-07-02T06:14:07Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "This study tackles the challenge of image matching in difficult scenarios, such as scenes with significant variations or limited texture, with a strong emphasis on computational efficiency. Previous studies have attempted to address this challenge by encoding global scene contexts using Transformers. However, these approaches suffer from high computational costs and may not capture sufficient high-level contextual information, such as structural shapes or semantic instances. Consequently, the en",
    "arxiv_url": "https://arxiv.org/abs/2307.00485v1",
    "pdf_url": "https://arxiv.org/pdf/2307.00485v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2307.00485",
    "arxiv_authors": [
      "Khang Truong Giang",
      "Soohwan Song",
      "Sungho Jo"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=TopicFM%2B%3A+Boosting+Accuracy+and+Efficiency+of+Topic-Assisted+Feature+Matching+Khang+Truong+Giang+Soohwan+Song+Sungho+Jo",
    "gs_search_success": true,
    "gs_authors": [
      "Wy4ytA0AAAAJ",
      "mEvl5GsAAAAJ"
    ],
    "citation_count": 14,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2505.16334",
    "title": "Panoptic Captioning: An Equivalence Bridge for Image and Text",
    "year": 2025,
    "published": "2025-05-22T07:44:10Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "This work introduces panoptic captioning, a novel task striving to seek the minimum text equivalent of images, which has broad potential applications. We take the first step towards panoptic captioning by formulating it as a task of generating a comprehensive textual description for an image, which encapsulates all entities, their respective locations and attributes, relationships among entities, as well as global image state. Through an extensive evaluation, our work reveals that state-of-the-a",
    "arxiv_url": "https://arxiv.org/abs/2505.16334v3",
    "pdf_url": "https://arxiv.org/pdf/2505.16334v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2505.16334",
    "arxiv_authors": [
      "Kun-Yu Lin",
      "Hongjun Wang",
      "Weining Ren",
      "Kai Han"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Panoptic+Captioning%3A+An+Equivalence+Bridge+for+Image+and+Text+Kun-Yu+Lin+Hongjun+Wang+Weining+Ren+Kai+Han",
    "gs_search_success": false,
    "gs_authors": [],
    "citation_count": null,
    "error_type": "fetch_failed",
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2406.01187",
    "title": "Patch-Based Encoder-Decoder Architecture for Automatic Transmitted Light to Fluorescence Imaging Transition: Contribution to the LightMyCells Challenge",
    "year": 2024,
    "published": "2024-06-03T10:49:34Z",
    "categories": [
      "eess.IV",
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "Automatic prediction of fluorescently labeled organelles from label-free transmitted light input images is an important, yet difficult task. The traditional way to obtain fluorescence images is related to performing biochemical labeling which is time-consuming and costly. Therefore, an automatic algorithm to perform the task based on the label-free transmitted light microscopy could be strongly beneficial. The importance of the task motivated researchers from the France-BioImaging to organize th",
    "arxiv_url": "https://arxiv.org/abs/2406.01187v1",
    "pdf_url": "https://arxiv.org/pdf/2406.01187v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2406.01187",
    "arxiv_authors": [
      "Marek Wodzinski",
      "Henning Müller"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Patch-Based+Encoder-Decoder+Architecture+for+Automatic+Transmitted+Light+to+Fluorescence+Imaging+Transition%3A+Contribution+to+the+LightMyCells+Challenge+Marek+Wodzinski+Henning+M%C3%BCller",
    "gs_search_success": true,
    "gs_authors": [
      "YvLZ5rsAAAAJ",
      "UEZ9RlUAAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 2
  },
  {
    "arxiv_id": "2411.09968",
    "title": "Seeing Clearly by Layer Two: Enhancing Attention Heads to Alleviate Hallucination in LVLMs",
    "year": 2024,
    "published": "2024-11-15T05:51:29Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "The hallucination problem in multimodal large language models (MLLMs) remains a common issue. Although image tokens occupy a majority of the input sequence of MLLMs, there is limited research to explore the relationship between image tokens and hallucinations. In this paper, we analyze the distribution of attention scores for image tokens across each layer and head of the model, revealing an intriguing and common phenomenon: most hallucinations are closely linked to the pattern of attention sink",
    "arxiv_url": "https://arxiv.org/abs/2411.09968v1",
    "pdf_url": "https://arxiv.org/pdf/2411.09968v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2411.09968",
    "arxiv_authors": [
      "Xiaofeng Zhang",
      "Yihao Quan",
      "Chaochen Gu",
      "Chen Shen",
      "Xiaosong Yuan",
      "Shaotian Yan",
      "Hao Cheng",
      "Kaijie Wu",
      "Jieping Ye"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Seeing+Clearly+by+Layer+Two%3A+Enhancing+Attention+Heads+to+Alleviate+Hallucination+in+LVLMs+Xiaofeng+Zhang+Yihao+Quan+Chaochen+Gu+Chen+Shen+Xiaosong+Yuan",
    "gs_search_success": true,
    "gs_authors": [
      "T9AzhwcAAAAJ",
      "Y6Z5xQQAAAAJ",
      "-Fg_EuEAAAAJ",
      "agZ9flIAAAAJ",
      "sBhbb2wAAAAJ",
      "Rm69hkYAAAAJ",
      "b6vn1uMAAAAJ"
    ],
    "citation_count": 32,
    "gs_author_count": 9
  },
  {
    "arxiv_id": "2311.05410",
    "title": "Linear Gaussian Bounding Box Representation and Ring-Shaped Rotated Convolution for Oriented Object Detection",
    "year": 2023,
    "published": "2023-11-09T14:45:22Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "In oriented object detection, current representations of oriented bounding boxes (OBBs) often suffer from boundary discontinuity problem. Methods of designing continuous regression losses do not essentially solve this problem. Although Gaussian bounding box (GBB) representation avoids this problem, directly regressing GBB is susceptible to numerical instability. We propose linear GBB (LGBB), a novel OBB representation. By linearly transforming the elements of GBB, LGBB avoids the boundary discon",
    "arxiv_url": "https://arxiv.org/abs/2311.05410v2",
    "pdf_url": "https://arxiv.org/pdf/2311.05410v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2311.05410",
    "arxiv_authors": [
      "Zhen Zhou",
      "Yunkai Ma",
      "Junfeng Fan",
      "Zhaoyang Liu",
      "Fengshui Jing",
      "Min Tan"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Linear+Gaussian+Bounding+Box+Representation+and+Ring-Shaped+Rotated+Convolution+for+Oriented+Object+Detection+Zhen+Zhou+Yunkai+Ma+Junfeng+Fan+Zhaoyang+Liu+Fengshui+Jing",
    "gs_search_success": true,
    "gs_authors": [
      "jxxktc4AAAAJ",
      "lqht97EAAAAJ",
      "AOiXnrwAAAAJ",
      "NHitQ88AAAAJ"
    ],
    "citation_count": 10,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2303.12533",
    "title": "Pixel-wise Agricultural Image Time Series Classification: Comparisons and a Deformable Prototype-based Approach",
    "year": 2023,
    "published": "2023-03-22T13:06:39Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Improvements in Earth observation by satellites allow for imagery of ever higher temporal and spatial resolution. Leveraging this data for agricultural monitoring is key for addressing environmental and economic challenges. Current methods for crop segmentation using temporal data either rely on annotated data or are heavily engineered to compensate the lack of supervision. In this paper, we present and compare datasets and methods for both supervised and unsupervised pixel-wise segmentation of ",
    "arxiv_url": "https://arxiv.org/abs/2303.12533v2",
    "pdf_url": "https://arxiv.org/pdf/2303.12533v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2303.12533",
    "arxiv_authors": [
      "Elliot Vincent",
      "Jean Ponce",
      "Mathieu Aubry"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Pixel-wise+Agricultural+Image+Time+Series+Classification%3A+Comparisons+and+a+Deformable+Prototype-based+Approach+Elliot+Vincent+Jean+Ponce+Mathieu+Aubry",
    "gs_search_success": true,
    "gs_authors": [
      "vC2vywcAAAAJ",
      "0MiPsosAAAAJ",
      "AUz-y5wAAAAJ"
    ],
    "citation_count": 8,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2401.07567",
    "title": "Bias-Conflict Sample Synthesis and Adversarial Removal Debias Strategy for Temporal Sentence Grounding in Video",
    "year": 2024,
    "published": "2024-01-15T09:59:43Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Temporal Sentence Grounding in Video (TSGV) is troubled by dataset bias issue, which is caused by the uneven temporal distribution of the target moments for samples with similar semantic components in input videos or query texts. Existing methods resort to utilizing prior knowledge about bias to artificially break this uneven distribution, which only removes a limited amount of significant language biases. In this work, we propose the bias-conflict sample synthesis and adversarial removal debias",
    "arxiv_url": "https://arxiv.org/abs/2401.07567v2",
    "pdf_url": "https://arxiv.org/pdf/2401.07567v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2401.07567",
    "arxiv_authors": [
      "Zhaobo Qi",
      "Yibo Yuan",
      "Xiaowen Ruan",
      "Shuhui Wang",
      "Weigang Zhang",
      "Qingming Huang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Bias-Conflict+Sample+Synthesis+and+Adversarial+Removal+Debias+Strategy+for+Temporal+Sentence+Grounding+in+Video+Zhaobo+Qi+Yibo+Yuan+Xiaowen+Ruan+Shuhui+Wang+Weigang+Zhang",
    "gs_search_success": true,
    "gs_authors": [
      "J1vMnRgAAAAJ",
      "QZ8URKAAAAAJ",
      "h-JxBSYAAAAJ",
      "4bm5wYUAAAAJ"
    ],
    "citation_count": 12,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2310.06291",
    "title": "Three-Dimensional Medical Image Fusion with Deformable Cross-Attention",
    "year": 2023,
    "published": "2023-10-10T04:10:56Z",
    "categories": [
      "eess.IV",
      "cs.CV",
      "physics.med-ph"
    ],
    "abstract": "Multimodal medical image fusion plays an instrumental role in several areas of medical image processing, particularly in disease recognition and tumor detection. Traditional fusion methods tend to process each modality independently before combining the features and reconstructing the fusion image. However, this approach often neglects the fundamental commonalities and disparities between multimodal information. Furthermore, the prevailing methodologies are largely confined to fusing two-dimensi",
    "arxiv_url": "https://arxiv.org/abs/2310.06291v1",
    "pdf_url": "https://arxiv.org/pdf/2310.06291v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2310.06291",
    "arxiv_authors": [
      "Lin Liu",
      "Xinxin Fan",
      "Chulong Zhang",
      "Jingjing Dai",
      "Yaoqin Xie",
      "Xiaokun Liang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Three-Dimensional+Medical+Image+Fusion+with+Deformable+Cross-Attention+Lin+Liu+Xinxin+Fan+Chulong+Zhang+Jingjing+Dai+Yaoqin+Xie",
    "gs_search_success": true,
    "gs_authors": [
      "z1E45JUAAAAJ",
      "KfrVZewAAAAJ",
      "BAcuUS4AAAAJ",
      "ACrSrHkAAAAJ",
      "ndrM4NcAAAAJ",
      "ALkaB9YAAAAJ"
    ],
    "citation_count": 3,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2503.12947",
    "title": "DivCon-NeRF: Diverse and Consistent Ray Augmentation for Few-Shot NeRF",
    "year": 2025,
    "published": "2025-03-17T08:59:34Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Neural Radiance Field (NeRF) has shown remarkable performance in novel view synthesis but requires numerous multi-view images, limiting its practicality in few-shot scenarios. Ray augmentation has been proposed to alleviate overfitting caused by sparse training data by generating additional rays. However, existing methods, which generate augmented rays only near the original rays, exhibit pronounced floaters and appearance distortions due to limited viewpoints and inconsistent rays obstructed by",
    "arxiv_url": "https://arxiv.org/abs/2503.12947v2",
    "pdf_url": "https://arxiv.org/pdf/2503.12947v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2503.12947",
    "arxiv_authors": [
      "Ingyun Lee",
      "Jae Won Jang",
      "Seunghyeon Seo",
      "Nojun Kwak"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=DivCon-NeRF%3A+Diverse+and+Consistent+Ray+Augmentation+for+Few-Shot+NeRF+Ingyun+Lee+Jae+Won+Jang+Seunghyeon+Seo+Nojun+Kwak",
    "gs_search_success": true,
    "gs_authors": [
      "ChN2UUkAAAAJ",
      "h_8-1M0AAAAJ",
      "LL9u-5IAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2404.00672",
    "title": "A General and Efficient Training for Transformer via Token Expansion",
    "year": 2024,
    "published": "2024-03-31T12:44:24Z",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.CV"
    ],
    "abstract": "The remarkable performance of Vision Transformers (ViTs) typically requires an extremely large training cost. Existing methods have attempted to accelerate the training of ViTs, yet typically disregard method universality with accuracy dropping. Meanwhile, they break the training consistency of the original transformers, including the consistency of hyper-parameters, architecture, and strategy, which prevents them from being widely applied to different Transformer networks. In this paper, we pro",
    "arxiv_url": "https://arxiv.org/abs/2404.00672v1",
    "pdf_url": "https://arxiv.org/pdf/2404.00672v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2404.00672",
    "arxiv_authors": [
      "Wenxuan Huang",
      "Yunhang Shen",
      "Jiao Xie",
      "Baochang Zhang",
      "Gaoqi He",
      "Ke Li",
      "Xing Sun",
      "Shaohui Lin"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=A+General+and+Efficient+Training+for+Transformer+via+Token+Expansion+Wenxuan+Huang+Yunhang+Shen+Jiao+Xie+Baochang+Zhang+Gaoqi+He",
    "gs_search_success": true,
    "gs_authors": [
      "6Ys6HgsAAAAJ",
      "IUtix9IAAAAJ",
      "5Nt_2DYAAAAJ",
      "k8AMa1kAAAAJ",
      "29teR74AAAAJ",
      "ImJz6MsAAAAJ",
      "mfWsFM0AAAAJ"
    ],
    "citation_count": 8,
    "gs_author_count": 8
  },
  {
    "arxiv_id": "2307.09520",
    "title": "Adversarial Bayesian Augmentation for Single-Source Domain Generalization",
    "year": 2023,
    "published": "2023-07-18T18:01:30Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Generalizing to unseen image domains is a challenging problem primarily due to the lack of diverse training data, inaccessible target data, and the large domain shift that may exist in many real-world settings. As such data augmentation is a critical component of domain generalization methods that seek to address this problem. We present Adversarial Bayesian Augmentation (ABA), a novel algorithm that learns to generate image augmentations in the challenging single-source domain generalization se",
    "arxiv_url": "https://arxiv.org/abs/2307.09520v2",
    "pdf_url": "https://arxiv.org/pdf/2307.09520v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2307.09520",
    "arxiv_authors": [
      "Sheng Cheng",
      "Tejas Gokhale",
      "Yezhou Yang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Adversarial+Bayesian+Augmentation+for+Single-Source+Domain+Generalization+Sheng+Cheng+Tejas+Gokhale+Yezhou+Yang",
    "gs_search_success": true,
    "gs_authors": [
      "_ILTlEwAAAAJ",
      "k2suuZgAAAAJ",
      "TWAwdYsAAAAJ"
    ],
    "citation_count": 28,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2412.01254",
    "title": "EmojiDiff: Advanced Facial Expression Control with High Identity Preservation in Portrait Generation",
    "year": 2024,
    "published": "2024-12-02T08:24:11Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "This paper aims to bring fine-grained expression control while maintaining high-fidelity identity in portrait generation. This is challenging due to the mutual interference between expression and identity: (i) fine expression control signals inevitably introduce appearance-related semantics (e.g., facial contours, and ratio), which impact the identity of the generated portrait; (ii) even coarse-grained expression control can cause facial changes that compromise identity, since they all act on th",
    "arxiv_url": "https://arxiv.org/abs/2412.01254v2",
    "pdf_url": "https://arxiv.org/pdf/2412.01254v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2412.01254",
    "arxiv_authors": [
      "Liangwei Jiang",
      "Ruida Li",
      "Zhifeng Zhang",
      "Shuo Fang",
      "Chenguang Ma"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=EmojiDiff%3A+Advanced+Facial+Expression+Control+with+High+Identity+Preservation+in+Portrait+Generation+Liangwei+Jiang+Ruida+Li+Zhifeng+Zhang+Shuo+Fang+Chenguang+Ma",
    "gs_search_success": true,
    "gs_authors": [],
    "citation_count": 4,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2505.02025",
    "title": "A Birotation Solution for Relative Pose Problems",
    "year": 2025,
    "published": "2025-05-04T08:24:14Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Relative pose estimation, a fundamental computer vision problem, has been extensively studied for decades. Existing methods either estimate and decompose the essential matrix or directly estimate the rotation and translation to obtain the solution. In this article, we break the mold by tackling this traditional problem with a novel birotation solution. We first introduce three basis transformations, each associated with a geometric metric to quantify the distance between the relative pose to be ",
    "arxiv_url": "https://arxiv.org/abs/2505.02025v1",
    "pdf_url": "https://arxiv.org/pdf/2505.02025v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2505.02025",
    "arxiv_authors": [
      "Hongbo Zhao",
      "Ziwei Long",
      "Mengtan Zhang",
      "Hanli Wang",
      "Qijun Chen",
      "Rui Fan"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=A+Birotation+Solution+for+Relative+Pose+Problems+Hongbo+Zhao+Ziwei+Long+Mengtan+Zhang+Hanli+Wang+Qijun+Chen",
    "gs_search_success": true,
    "gs_authors": [
      "uanS4aIAAAAJ",
      "WioFu64AAAAJ",
      "P5AJTXcAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2302.06353",
    "title": "Contour-based Interactive Segmentation",
    "year": 2023,
    "published": "2023-02-13T13:35:26Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Recent advances in interactive segmentation (IS) allow speeding up and simplifying image editing and labeling greatly. The majority of modern IS approaches accept user input in the form of clicks. However, using clicks may require too many user interactions, especially when selecting small objects, minor parts of an object, or a group of objects of the same type. In this paper, we consider such a natural form of user interaction as a loose contour, and introduce a contour-based IS method. We eva",
    "arxiv_url": "https://arxiv.org/abs/2302.06353v2",
    "pdf_url": "https://arxiv.org/pdf/2302.06353v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2302.06353",
    "arxiv_authors": [
      "Danil Galeev",
      "Polina Popenova",
      "Anna Vorontsova",
      "Anton Konushin"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Contour-based+Interactive+Segmentation+Danil+Galeev+Polina+Popenova+Anna+Vorontsova+Anton+Konushin",
    "gs_search_success": true,
    "gs_authors": [
      "HiVoQCIAAAAJ",
      "HCj54P8AAAAJ",
      "ZT_k-wMAAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2410.13598",
    "title": "Let Me Finish My Sentence: Video Temporal Grounding with Holistic Text Understanding",
    "year": 2024,
    "published": "2024-10-17T14:31:02Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Video Temporal Grounding (VTG) aims to identify visual frames in a video clip that match text queries. Recent studies in VTG employ cross-attention to correlate visual frames and text queries as individual token sequences. However, these approaches overlook a crucial aspect of the problem: a holistic understanding of the query sentence. A model may capture correlations between individual word tokens and arbitrary visual frames while possibly missing out on the global meaning. To address this, we",
    "arxiv_url": "https://arxiv.org/abs/2410.13598v1",
    "pdf_url": "https://arxiv.org/pdf/2410.13598v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2410.13598",
    "arxiv_authors": [
      "Jongbhin Woo",
      "Hyeonggon Ryu",
      "Youngjoon Jang",
      "Jae Won Cho",
      "Joon Son Chung"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Let+Me+Finish+My+Sentence%3A+Video+Temporal+Grounding+with+Holistic+Text+Understanding+Jongbhin+Woo+Hyeonggon+Ryu+Youngjoon+Jang+Jae+Won+Cho+Joon+Son+Chung",
    "gs_search_success": true,
    "gs_authors": [
      "v9BcFogAAAAJ",
      "oB5AOQQAAAAJ",
      "O_l7ShIAAAAJ",
      "th4fvfIAAAAJ",
      "JJ_LQ0YAAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2411.07765",
    "title": "Novel View Synthesis with Pixel-Space Diffusion Models",
    "year": 2024,
    "published": "2024-11-12T12:58:33Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Synthesizing a novel view from a single input image is a challenging task. Traditionally, this task was approached by estimating scene depth, warping, and inpainting, with machine learning models enabling parts of the pipeline. More recently, generative models are being increasingly employed in novel view synthesis (NVS), often encompassing the entire end-to-end system. In this work, we adapt a modern diffusion model architecture for end-to-end NVS in the pixel space, substantially outperforming",
    "arxiv_url": "https://arxiv.org/abs/2411.07765v1",
    "pdf_url": "https://arxiv.org/pdf/2411.07765v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2411.07765",
    "arxiv_authors": [
      "Noam Elata",
      "Bahjat Kawar",
      "Yaron Ostrovsky-Berman",
      "Miriam Farber",
      "Ron Sokolovsky"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Novel+View+Synthesis+with+Pixel-Space+Diffusion+Models+Noam+Elata+Bahjat+Kawar+Yaron+Ostrovsky-Berman+Miriam+Farber+Ron+Sokolovsky",
    "gs_search_success": true,
    "gs_authors": [
      "36gR46QAAAAJ",
      "88l-2DcAAAAJ",
      "z005TlIAAAAJ"
    ],
    "citation_count": 5,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2408.07825",
    "title": "SSRFlow: Semantic-aware Fusion with Spatial Temporal Re-embedding for Real-world Scene Flow",
    "year": 2024,
    "published": "2024-07-31T02:28:40Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Scene flow, which provides the 3D motion field of the first frame from two consecutive point clouds, is vital for dynamic scene perception. However, contemporary scene flow methods face three major challenges. Firstly, they lack global flow embedding or only consider the context of individual point clouds before embedding, leading to embedded points struggling to perceive the consistent semantic relationship of another frame. To address this issue, we propose a novel approach called Dual Cross A",
    "arxiv_url": "https://arxiv.org/abs/2408.07825v1",
    "pdf_url": "https://arxiv.org/pdf/2408.07825v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2408.07825",
    "arxiv_authors": [
      "Zhiyang Lu",
      "Qinghan Chen",
      "Zhimin Yuan",
      "Ming Cheng"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=SSRFlow%3A+Semantic-aware+Fusion+with+Spatial+Temporal+Re-embedding+for+Real-world+Scene+Flow+Zhiyang+Lu+Qinghan+Chen+Zhimin+Yuan+Ming+Cheng",
    "gs_search_success": true,
    "gs_authors": [
      "JOoZUmUAAAAJ",
      "Ii6P4MkAAAAJ",
      "kAnv3SkAAAAJ",
      "477EMiUAAAAJ",
      "L214A8gAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2401.07061",
    "title": "Dual-View Data Hallucination with Semantic Relation Guidance for Few-Shot Image Recognition",
    "year": 2024,
    "published": "2024-01-13T12:32:29Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Learning to recognize novel concepts from just a few image samples is very challenging as the learned model is easily overfitted on the few data and results in poor generalizability. One promising but underexplored solution is to compensate the novel classes by generating plausible samples. However, most existing works of this line exploit visual information only, rendering the generated data easy to be distracted by some challenging factors contained in the few available samples. Being aware of",
    "arxiv_url": "https://arxiv.org/abs/2401.07061v2",
    "pdf_url": "https://arxiv.org/pdf/2401.07061v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2401.07061",
    "arxiv_authors": [
      "Hefeng Wu",
      "Guangzhi Ye",
      "Ziyang Zhou",
      "Ling Tian",
      "Qing Wang",
      "Liang Lin"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Dual-View+Data+Hallucination+with+Semantic+Relation+Guidance+for+Few-Shot+Image+Recognition+Hefeng+Wu+Guangzhi+Ye+Ziyang+Zhou+Ling+Tian+Qing+Wang",
    "gs_search_success": true,
    "gs_authors": [
      "Nav8m8gAAAAJ",
      "gX2pNewAAAAJ",
      "yg7JdVQAAAAJ"
    ],
    "citation_count": 7,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2407.04003",
    "title": "Fully Fine-tuned CLIP Models are Efficient Few-Shot Learners",
    "year": 2024,
    "published": "2024-07-04T15:22:54Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Prompt tuning, which involves training a small set of parameters, effectively enhances the pre-trained Vision-Language Models (VLMs) to downstream tasks. However, they often come at the cost of flexibility and adaptability when the tuned models are applied to different datasets or domains. In this paper, we explore capturing the task-specific information via meticulous refinement of entire VLMs, with minimal parameter adjustments. When fine-tuning the entire VLMs for specific tasks under limited",
    "arxiv_url": "https://arxiv.org/abs/2407.04003v1",
    "pdf_url": "https://arxiv.org/pdf/2407.04003v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2407.04003",
    "arxiv_authors": [
      "Mushui Liu",
      "Bozheng Li",
      "Yunlong Yu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Fully+Fine-tuned+CLIP+Models+are+Efficient+Few-Shot+Learners+Mushui+Liu+Bozheng+Li+Yunlong+Yu",
    "gs_search_success": true,
    "gs_authors": [
      "NB9Mn5MAAAAJ",
      "-WUyWpMAAAAJ",
      "cbC26HkAAAAJ",
      "qx1yRVEAAAAJ",
      "mCaOnp4AAAAJ"
    ],
    "citation_count": 10,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2403.05124",
    "title": "CLIP-Gaze: Towards General Gaze Estimation via Visual-Linguistic Model",
    "year": 2024,
    "published": "2024-03-08T07:37:21Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Gaze estimation methods often experience significant performance degradation when evaluated across different domains, due to the domain gap between the testing and training data. Existing methods try to address this issue using various domain generalization approaches, but with little success because of the limited diversity of gaze datasets, such as appearance, wearable, and image quality. To overcome these limitations, we propose a novel framework called CLIP-Gaze that utilizes a pre-trained v",
    "arxiv_url": "https://arxiv.org/abs/2403.05124v1",
    "pdf_url": "https://arxiv.org/pdf/2403.05124v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2403.05124",
    "arxiv_authors": [
      "Pengwei Yin",
      "Guanzhong Zeng",
      "Jingjing Wang",
      "Di Xie"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=CLIP-Gaze%3A+Towards+General+Gaze+Estimation+via+Visual-Linguistic+Model+Pengwei+Yin+Guanzhong+Zeng+Jingjing+Wang+Di+Xie",
    "gs_search_success": true,
    "gs_authors": [
      "7sxVnykAAAAJ",
      "Eyx-kX8AAAAJ",
      "muYYWO4AAAAJ"
    ],
    "citation_count": 20,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2304.01227",
    "title": "Resolution-Invariant Image Classification based on Fourier Neural Operators",
    "year": 2023,
    "published": "2023-04-02T10:23:36Z",
    "categories": [
      "cs.CV",
      "cs.LG",
      "math.NA"
    ],
    "abstract": "In this paper we investigate the use of Fourier Neural Operators (FNOs) for image classification in comparison to standard Convolutional Neural Networks (CNNs). Neural operators are a discretization-invariant generalization of neural networks to approximate operators between infinite dimensional function spaces. FNOs - which are neural operators with a specific parametrization - have been applied successfully in the context of parametric PDEs. We derive the FNO architecture as an example for con",
    "arxiv_url": "https://arxiv.org/abs/2304.01227v1",
    "pdf_url": "https://arxiv.org/pdf/2304.01227v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2304.01227",
    "arxiv_authors": [
      "Samira Kabri",
      "Tim Roith",
      "Daniel Tenbrinck",
      "Martin Burger"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Resolution-Invariant+Image+Classification+based+on+Fourier+Neural+Operators+Samira+Kabri+Tim+Roith+Daniel+Tenbrinck+Martin+Burger",
    "gs_search_success": true,
    "gs_authors": [
      "tbAlI9kAAAAJ",
      "kVi18K0AAAAJ",
      "cDDMZv4AAAAJ",
      "BKlbQTAAAAAJ"
    ],
    "citation_count": 16,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2404.12680",
    "title": "VoxAtnNet: A 3D Point Clouds Convolutional Neural Network for Generalizable Face Presentation Attack Detection",
    "year": 2024,
    "published": "2024-04-19T07:30:36Z",
    "categories": [
      "cs.CV",
      "cs.CR"
    ],
    "abstract": "Facial biometrics are an essential components of smartphones to ensure reliable and trustworthy authentication. However, face biometric systems are vulnerable to Presentation Attacks (PAs), and the availability of more sophisticated presentation attack instruments such as 3D silicone face masks will allow attackers to deceive face recognition systems easily. In this work, we propose a novel Presentation Attack Detection (PAD) algorithm based on 3D point clouds captured using the frontal camera o",
    "arxiv_url": "https://arxiv.org/abs/2404.12680v1",
    "pdf_url": "https://arxiv.org/pdf/2404.12680v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2404.12680",
    "arxiv_authors": [
      "Raghavendra Ramachandra",
      "Narayan Vetrekar",
      "Sushma Venkatesh",
      "Savita Nageshker",
      "Jag Mohan Singh",
      "R. S. Gad"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=VoxAtnNet%3A+A+3D+Point+Clouds+Convolutional+Neural+Network+for+Generalizable+Face+Presentation+Attack+Detection+Raghavendra+Ramachandra+Narayan+Vetrekar+Sushma+Venkatesh+Savita+Nageshker+Jag+Mohan+Singh",
    "gs_search_success": true,
    "gs_authors": [
      "qD0Uzq0AAAAJ",
      "OIYIrmIAAAAJ",
      "ZWjmZDcAAAAJ",
      "6c6gvqAAAAAJ"
    ],
    "citation_count": 5,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2412.13610",
    "title": "Faster and Stronger: When ANN-SNN Conversion Meets Parallel Spiking Calculation",
    "year": 2024,
    "published": "2024-12-18T08:37:13Z",
    "categories": [
      "cs.NE",
      "cs.AI",
      "cs.CV"
    ],
    "abstract": "Spiking Neural Network (SNN), as a brain-inspired and energy-efficient network, is currently facing the pivotal challenge of exploring a suitable and efficient learning framework. The predominant training methodologies, namely Spatial-Temporal Back-propagation (STBP) and ANN-SNN Conversion, are encumbered by substantial training overhead or pronounced inference latency, which impedes the advancement of SNNs in scaling to larger networks and navigating intricate application domains. In this work,",
    "arxiv_url": "https://arxiv.org/abs/2412.13610v2",
    "pdf_url": "https://arxiv.org/pdf/2412.13610v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2412.13610",
    "arxiv_authors": [
      "Zecheng Hao",
      "Qichao Ma",
      "Kang Chen",
      "Yi Zhang",
      "Zhaofei Yu",
      "Tiejun Huang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Faster+and+Stronger%3A+When+ANN-SNN+Conversion+Meets+Parallel+Spiking+Calculation+Zecheng+Hao+Qichao+Ma+Kang+Chen+Yi+Zhang+Zhaofei+Yu",
    "gs_search_success": true,
    "gs_authors": [
      "mX2poY0AAAAJ",
      "qaUgD50AAAAJ",
      "txTkX7YAAAAJ",
      "knvEK4AAAAAJ",
      "F5feBP4AAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2303.11831",
    "title": "CLADE: Cycle Loss Augmented Degradation Enhancement for Unpaired Super-Resolution of Anisotropic Medical Images",
    "year": 2023,
    "published": "2023-03-21T13:19:51Z",
    "categories": [
      "cs.CV",
      "cs.LG",
      "eess.IV",
      "physics.med-ph"
    ],
    "abstract": "Three-dimensional (3D) imaging is popular in medical applications, however, anisotropic 3D volumes with thick, low-spatial-resolution slices are often acquired to reduce scan times. Deep learning (DL) offers a solution to recover high-resolution features through super-resolution reconstruction (SRR). Unfortunately, paired training data is unavailable in many 3D medical applications and therefore we propose a novel unpaired approach; CLADE (Cycle Loss Augmented Degradation Enhancement). CLADE use",
    "arxiv_url": "https://arxiv.org/abs/2303.11831v3",
    "pdf_url": "https://arxiv.org/pdf/2303.11831v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2303.11831",
    "arxiv_authors": [
      "Michele Pascale",
      "Vivek Muthurangu",
      "Javier Montalt Tordera",
      "Heather E Fitzke",
      "Gauraang Bhatnagar",
      "Stuart Taylor",
      "Jennifer Steeden"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=CLADE%3A+Cycle+Loss+Augmented+Degradation+Enhancement+for+Unpaired+Super-Resolution+of+Anisotropic+Medical+Images+Michele+Pascale+Vivek+Muthurangu+Javier+Montalt+Tordera+Heather+E+Fitzke+Gauraang+Bhatnagar",
    "gs_search_success": true,
    "gs_authors": [
      "0dxDjuIAAAAJ",
      "hEfovLsAAAAJ",
      "GRHqMf0AAAAJ",
      "LXpgKbMAAAAJ",
      "rXjDFPYAAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 7
  },
  {
    "arxiv_id": "2303.09068",
    "title": "Vortex Feature Positioning: Bridging Tabular IIoT Data and Image-Based Deep Learning",
    "year": 2023,
    "published": "2023-03-16T04:02:17Z",
    "categories": [
      "cs.CV",
      "cs.LG",
      "eess.IV"
    ],
    "abstract": "Tabular data from IIoT devices are typically analyzed using decision tree-based machine learning techniques, which struggle with high-dimensional and numeric data. To overcome these limitations, techniques converting tabular data into images have been developed, leveraging the strengths of image-based deep learning approaches such as Convolutional Neural Networks. These methods cluster similar features into distinct image areas with fixed sizes, regardless of the number of features, resembling a",
    "arxiv_url": "https://arxiv.org/abs/2303.09068v2",
    "pdf_url": "https://arxiv.org/pdf/2303.09068v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2303.09068",
    "arxiv_authors": [
      "Jong-Ik Park",
      "Sihoon Seong",
      "JunKyu Lee",
      "Cheol-Ho Hong"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Vortex+Feature+Positioning%3A+Bridging+Tabular+IIoT+Data+and+Image-Based+Deep+Learning+Jong-Ik+Park+Sihoon+Seong+JunKyu+Lee+Cheol-Ho+Hong",
    "gs_search_success": true,
    "gs_authors": [
      "r0505CgAAAAJ",
      "4kco1LYAAAAJ",
      "iChrFn8AAAAJ",
      "lJtZwsMAAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2411.17606",
    "title": "HyperSeg: Towards Universal Visual Segmentation with Large Language Model",
    "year": 2024,
    "published": "2024-11-26T17:18:20Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "This paper aims to address universal segmentation for image and video perception with the strong reasoning ability empowered by Visual Large Language Models (VLLMs). Despite significant progress in current unified segmentation methods, limitations in adaptation to both image and video scenarios, as well as the complex reasoning segmentation, make it difficult for them to handle various challenging instructions and achieve an accurate understanding of fine-grained vision-language correlations. We",
    "arxiv_url": "https://arxiv.org/abs/2411.17606v2",
    "pdf_url": "https://arxiv.org/pdf/2411.17606v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2411.17606",
    "arxiv_authors": [
      "Cong Wei",
      "Yujie Zhong",
      "Haoxian Tan",
      "Yong Liu",
      "Zheng Zhao",
      "Jie Hu",
      "Yujiu Yang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=HyperSeg%3A+Towards+Universal+Visual+Segmentation+with+Large+Language+Model+Cong+Wei+Yujie+Zhong+Haoxian+Tan+Yong+Liu+Zheng+Zhao",
    "gs_search_success": true,
    "gs_authors": [
      "9rY7WqAAAAAJ",
      "4gH3sxsAAAAJ",
      "DAJdHnkAAAAJ",
      "i9keb3IAAAAJ"
    ],
    "citation_count": 14,
    "gs_author_count": 7
  },
  {
    "arxiv_id": "2303.04346",
    "title": "Semi-Supervised 2D Human Pose Estimation Driven by Position Inconsistency Pseudo Label Correction Module",
    "year": 2023,
    "published": "2023-03-08T02:57:05Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "In this paper, we delve into semi-supervised 2D human pose estimation. The previous method ignored two problems: (i) When conducting interactive training between large model and lightweight model, the pseudo label of lightweight model will be used to guide large models. (ii) The negative impact of noise pseudo labels on training. Moreover, the labels used for 2D human pose estimation are relatively complex: keypoint category and keypoint position. To solve the problems mentioned above, we propos",
    "arxiv_url": "https://arxiv.org/abs/2303.04346v1",
    "pdf_url": "https://arxiv.org/pdf/2303.04346v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2303.04346",
    "arxiv_authors": [
      "Linzhi Huang",
      "Yulong Li",
      "Hongbo Tian",
      "Yue Yang",
      "Xiangang Li",
      "Weihong Deng",
      "Jieping Ye"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Semi-Supervised+2D+Human+Pose+Estimation+Driven+by+Position+Inconsistency+Pseudo+Label+Correction+Module+Linzhi+Huang+Yulong+Li+Hongbo+Tian+Yue+Yang+Xiangang+Li",
    "gs_search_success": true,
    "gs_authors": [
      "T9AzhwcAAAAJ",
      "x8CoMv4AAAAJ",
      "hYQb9goAAAAJ",
      "oC75phkAAAAJ",
      "1rhBlUEAAAAJ"
    ],
    "citation_count": 25,
    "gs_author_count": 7
  },
  {
    "arxiv_id": "2409.07307",
    "title": "Data Augmentation via Latent Diffusion for Saliency Prediction",
    "year": 2024,
    "published": "2024-09-11T14:36:24Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Saliency prediction models are constrained by the limited diversity and quantity of labeled data. Standard data augmentation techniques such as rotating and cropping alter scene composition, affecting saliency. We propose a novel data augmentation method for deep saliency prediction that edits natural images while preserving the complexity and variability of real-world scenes. Since saliency depends on high-level and low-level features, our approach involves learning both by incorporating photom",
    "arxiv_url": "https://arxiv.org/abs/2409.07307v1",
    "pdf_url": "https://arxiv.org/pdf/2409.07307v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2409.07307",
    "arxiv_authors": [
      "Bahar Aydemir",
      "Deblina Bhattacharjee",
      "Tong Zhang",
      "Mathieu Salzmann",
      "Sabine Süsstrunk"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Data+Augmentation+via+Latent+Diffusion+for+Saliency+Prediction+Bahar+Aydemir+Deblina+Bhattacharjee+Tong+Zhang+Mathieu+Salzmann+Sabine+S%C3%BCsstrunk",
    "gs_search_success": true,
    "gs_authors": [
      "LurWtuYAAAAJ",
      "n-B0jr4AAAAJ",
      "EX3OYP4AAAAJ",
      "F3YYEmMAAAAJ",
      "fWVzdOcAAAAJ"
    ],
    "citation_count": 6,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2407.02004",
    "title": "SAVE: Segment Audio-Visual Easy way using Segment Anything Model",
    "year": 2024,
    "published": "2024-07-02T07:22:28Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.SD",
      "eess.AS"
    ],
    "abstract": "The primary aim of Audio-Visual Segmentation (AVS) is to precisely identify and locate auditory elements within visual scenes by accurately predicting segmentation masks at the pixel level. Achieving this involves comprehensively considering data and model aspects to address this task effectively. This study presents a lightweight approach, SAVE, which efficiently adapts the pre-trained segment anything model (SAM) to the AVS task. By incorporating an image encoder adapter into the transformer b",
    "arxiv_url": "https://arxiv.org/abs/2407.02004v2",
    "pdf_url": "https://arxiv.org/pdf/2407.02004v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2407.02004",
    "arxiv_authors": [
      "Khanh-Binh Nguyen",
      "Chae Jung Park"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=SAVE%3A+Segment+Audio-Visual+Easy+way+using+Segment+Anything+Model+Khanh-Binh+Nguyen+Chae+Jung+Park",
    "gs_search_success": true,
    "gs_authors": [
      "LoOglv4AAAAJ"
    ],
    "citation_count": 4,
    "gs_author_count": 2
  },
  {
    "arxiv_id": "2312.09365",
    "title": "SAR image segmentation algorithms based on I-divergence-TV model",
    "year": 2023,
    "published": "2023-12-09T04:14:46Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "In this paper, we propose a novel variational active contour model based on I-divergence-TV model to segment Synthetic aperture radar (SAR) images with multiplicative gamma noise, which hybrides edge-based model with region-based model. The proposed model can efficiently stop the contours at weak or blurred edges, and can automatically detect the exterior and interior boundaries of images. We incorporate the global convex segmentation method and split Bregman technique into the proposed model, a",
    "arxiv_url": "https://arxiv.org/abs/2312.09365v2",
    "pdf_url": "https://arxiv.org/pdf/2312.09365v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2312.09365",
    "arxiv_authors": [
      "Guangming Liu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=SAR+image+segmentation+algorithms+based+on+I-divergence-TV+model+Guangming+Liu",
    "gs_search_success": true,
    "gs_authors": [],
    "citation_count": 2,
    "gs_author_count": 1
  },
  {
    "arxiv_id": "2405.14014",
    "title": "RadarOcc: Robust 3D Occupancy Prediction with 4D Imaging Radar",
    "year": 2024,
    "published": "2024-05-22T21:48:17Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "cs.RO"
    ],
    "abstract": "3D occupancy-based perception pipeline has significantly advanced autonomous driving by capturing detailed scene descriptions and demonstrating strong generalizability across various object categories and shapes. Current methods predominantly rely on LiDAR or camera inputs for 3D occupancy prediction. These methods are susceptible to adverse weather conditions, limiting the all-weather deployment of self-driving cars. To improve perception robustness, we leverage the recent advances in automotiv",
    "arxiv_url": "https://arxiv.org/abs/2405.14014v4",
    "pdf_url": "https://arxiv.org/pdf/2405.14014v4",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2405.14014",
    "arxiv_authors": [
      "Fangqiang Ding",
      "Xiangyu Wen",
      "Yunzhou Zhu",
      "Yiming Li",
      "Chris Xiaoxuan Lu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=RadarOcc%3A+Robust+3D+Occupancy+Prediction+with+4D+Imaging+Radar+Fangqiang+Ding+Xiangyu+Wen+Yunzhou+Zhu+Yiming+Li+Chris+Xiaoxuan+Lu",
    "gs_search_success": true,
    "gs_authors": [
      "i_aajNoAAAAJ",
      "6SVBFSwAAAAJ",
      "Ja8dgh8AAAAJ",
      "idu78-EAAAAJ",
      "WxgdNyAAAAAJ"
    ],
    "citation_count": 32,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2303.13913",
    "title": "GarmentTracking: Category-Level Garment Pose Tracking",
    "year": 2023,
    "published": "2023-03-24T10:59:17Z",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "abstract": "Garments are important to humans. A visual system that can estimate and track the complete garment pose can be useful for many downstream tasks and real-world applications. In this work, we present a complete package to address the category-level garment pose tracking task: (1) A recording system VR-Garment, with which users can manipulate virtual garment models in simulation through a VR interface. (2) A large-scale dataset VR-Folding, with complex garment pose configurations in manipulation li",
    "arxiv_url": "https://arxiv.org/abs/2303.13913v2",
    "pdf_url": "https://arxiv.org/pdf/2303.13913v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2303.13913",
    "arxiv_authors": [
      "Han Xue",
      "Wenqiang Xu",
      "Jieyi Zhang",
      "Tutian Tang",
      "Yutong Li",
      "Wenxin Du",
      "Ruolin Ye",
      "Cewu Lu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=GarmentTracking%3A+Category-Level+Garment+Pose+Tracking+Han+Xue+Wenqiang+Xu+Jieyi+Zhang+Tutian+Tang+Yutong+Li",
    "gs_search_success": true,
    "gs_authors": [
      "kQMFnUkAAAAJ",
      "fCy8NOUAAAAJ",
      "PdzO-4YAAAAJ",
      "xyanDrMAAAAJ",
      "QZVQEWAAAAAJ",
      "KSv942gAAAAJ",
      "j861KLoAAAAJ"
    ],
    "citation_count": 20,
    "gs_author_count": 8
  },
  {
    "arxiv_id": "2402.18213",
    "title": "Multi-objective Differentiable Neural Architecture Search",
    "year": 2024,
    "published": "2024-02-28T10:09:04Z",
    "categories": [
      "cs.LG",
      "cs.CV",
      "stat.ML"
    ],
    "abstract": "Pareto front profiling in multi-objective optimization (MOO), i.e., finding a diverse set of Pareto optimal solutions, is challenging, especially with expensive objectives that require training a neural network. Typically, in MOO for neural architecture search (NAS), we aim to balance performance and hardware metrics across devices. Prior NAS approaches simplify this task by incorporating hardware constraints into the objective function, but profiling the Pareto front necessitates a computationa",
    "arxiv_url": "https://arxiv.org/abs/2402.18213v3",
    "pdf_url": "https://arxiv.org/pdf/2402.18213v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2402.18213",
    "arxiv_authors": [
      "Rhea Sanjay Sukthanker",
      "Arber Zela",
      "Benedikt Staffler",
      "Samuel Dooley",
      "Josif Grabocka",
      "Frank Hutter"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Multi-objective+Differentiable+Neural+Architecture+Search+Rhea+Sanjay+Sukthanker+Arber+Zela+Benedikt+Staffler+Samuel+Dooley+Josif+Grabocka",
    "gs_search_success": true,
    "gs_authors": [
      "hD_6YioAAAAJ",
      "OsamqmMAAAAJ",
      "kByZ7WYAAAAJ",
      "YUrxwrkAAAAJ",
      "KRy27XcAAAAJ",
      "BNZlRMsAAAAJ"
    ],
    "citation_count": 10,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2402.05917",
    "title": "Point-VOS: Pointing Up Video Object Segmentation",
    "year": 2024,
    "published": "2024-02-08T18:52:23Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Current state-of-the-art Video Object Segmentation (VOS) methods rely on dense per-object mask annotations both during training and testing. This requires time-consuming and costly video annotation mechanisms. We propose a novel Point-VOS task with a spatio-temporally sparse point-wise annotation scheme that substantially reduces the annotation effort. We apply our annotation scheme to two large-scale video datasets with text descriptions and annotate over 19M points across 133K objects in 32K v",
    "arxiv_url": "https://arxiv.org/abs/2402.05917v2",
    "pdf_url": "https://arxiv.org/pdf/2402.05917v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2402.05917",
    "arxiv_authors": [
      "Idil Esen Zulfikar",
      "Sabarinath Mahadevan",
      "Paul Voigtlaender",
      "Bastian Leibe"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Point-VOS%3A+Pointing+Up+Video+Object+Segmentation+Idil+Esen+Zulfikar+Sabarinath+Mahadevan+Paul+Voigtlaender+Bastian+Leibe",
    "gs_search_success": true,
    "gs_authors": [
      "MqI866QAAAAJ",
      "taUv_MUAAAAJ",
      "89vcmSoAAAAJ"
    ],
    "citation_count": 3,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2405.01230",
    "title": "Evaluation of Video-Based rPPG in Challenging Environments: Artifact Mitigation and Network Resilience",
    "year": 2024,
    "published": "2024-05-02T12:21:51Z",
    "categories": [
      "cs.CV",
      "eess.SP"
    ],
    "abstract": "Video-based remote photoplethysmography (rPPG) has emerged as a promising technology for non-contact vital sign monitoring, especially under controlled conditions. However, the accurate measurement of vital signs in real-world scenarios faces several challenges, including artifacts induced by videocodecs, low-light noise, degradation, low dynamic range, occlusions, and hardware and network constraints. In this article, we systematically investigate comprehensive investigate these issues, measuri",
    "arxiv_url": "https://arxiv.org/abs/2405.01230v1",
    "pdf_url": "https://arxiv.org/pdf/2405.01230v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2405.01230",
    "arxiv_authors": [
      "Nhi Nguyen",
      "Le Nguyen",
      "Honghan Li",
      "Miguel Bordallo López",
      "Constantino Álvarez Casado"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Evaluation+of+Video-Based+rPPG+in+Challenging+Environments%3A+Artifact+Mitigation+and+Network+Resilience+Nhi+Nguyen+Le+Nguyen+Honghan+Li+Miguel+Bordallo+L%C3%B3pez+Constantino+%C3%81lvarez+Casado",
    "gs_search_success": true,
    "gs_authors": [
      "tow0oVoAAAAJ",
      "vLYIwqUAAAAJ",
      "7_t4zCIAAAAJ",
      "rSu541oAAAAJ",
      "j_lXNWMAAAAJ"
    ],
    "citation_count": 13,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2501.19047",
    "title": "Understanding Model Calibration -- A gentle introduction and visual exploration of calibration and the expected calibration error (ECE)",
    "year": 2025,
    "published": "2025-01-31T11:18:45Z",
    "categories": [
      "stat.ME",
      "cs.AI",
      "cs.CV",
      "cs.LG",
      "stat.ML"
    ],
    "abstract": "To be considered reliable, a model must be calibrated so that its confidence in each decision closely reflects its true outcome. In this blogpost we'll take a look at the most commonly used definition for calibration and then dive into a frequently used evaluation measure for model calibration. We'll then cover some of the drawbacks of this measure and how these surfaced the need for additional notions of calibration, which require their own new evaluation measures. This post is not intended to ",
    "arxiv_url": "https://arxiv.org/abs/2501.19047v5",
    "pdf_url": "https://arxiv.org/pdf/2501.19047v5",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2501.19047",
    "arxiv_authors": [
      "Maja Pavlovic"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Understanding+Model+Calibration+--+A+gentle+introduction+and+visual+exploration+of+calibration+and+the+expected+calibration+error+%28ECE%29+Maja+Pavlovic",
    "gs_search_success": true,
    "gs_authors": [
      "NEa-wIIAAAAJ"
    ],
    "citation_count": 8,
    "gs_author_count": 1
  },
  {
    "arxiv_id": "2406.02559",
    "title": "ShadowRefiner: Towards Mask-free Shadow Removal via Fast Fourier Transformer",
    "year": 2024,
    "published": "2024-04-18T03:53:33Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Shadow-affected images often exhibit pronounced spatial discrepancies in color and illumination, consequently degrading various vision applications including object detection and segmentation systems. To effectively eliminate shadows in real-world images while preserving intricate details and producing visually compelling outcomes, we introduce a mask-free Shadow Removal and Refinement network (ShadowRefiner) via Fast Fourier Transformer. Specifically, the Shadow Removal module in our method aim",
    "arxiv_url": "https://arxiv.org/abs/2406.02559v2",
    "pdf_url": "https://arxiv.org/pdf/2406.02559v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2406.02559",
    "arxiv_authors": [
      "Wei Dong",
      "Han Zhou",
      "Yuqiong Tian",
      "Jingke Sun",
      "Xiaohong Liu",
      "Guangtao Zhai",
      "Jun Chen"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=ShadowRefiner%3A+Towards+Mask-free+Shadow+Removal+via+Fast+Fourier+Transformer+Wei+Dong+Han+Zhou+Yuqiong+Tian+Jingke+Sun+Xiaohong+Liu",
    "gs_search_success": true,
    "gs_authors": [
      "E6zbSYgAAAAJ",
      "XI79Mw0AAAAJ",
      "dlcOwwkAAAAJ",
      "Tq2hoMQAAAAJ",
      "tkTl3BMAAAAJ"
    ],
    "citation_count": 26,
    "gs_author_count": 7
  },
  {
    "arxiv_id": "2302.12393",
    "title": "Blind Omnidirectional Image Quality Assessment: Integrating Local Statistics and Global Semantics",
    "year": 2023,
    "published": "2023-02-24T01:47:13Z",
    "categories": [
      "cs.MM",
      "cs.CV"
    ],
    "abstract": "Omnidirectional image quality assessment (OIQA) aims to predict the perceptual quality of omnidirectional images that cover the whole 180$\\times$360$^{\\circ}$ viewing range of the visual environment. Here we propose a blind/no-reference OIQA method named S$^2$ that bridges the gap between low-level statistics and high-level semantics of omnidirectional images. Specifically, statistic and semantic features are extracted in separate paths from multiple local viewports and the hallucinated global o",
    "arxiv_url": "https://arxiv.org/abs/2302.12393v1",
    "pdf_url": "https://arxiv.org/pdf/2302.12393v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2302.12393",
    "arxiv_authors": [
      "Wei Zhou",
      "Zhou Wang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Blind+Omnidirectional+Image+Quality+Assessment%3A+Integrating+Local+Statistics+and+Global+Semantics+Wei+Zhou+Zhou+Wang",
    "gs_search_success": true,
    "gs_authors": [
      "O0PjM4QAAAAJ",
      "D8SDfHQAAAAJ"
    ],
    "citation_count": 15,
    "gs_author_count": 2
  },
  {
    "arxiv_id": "2503.09361",
    "title": "Deep Learning for Climate Action: Computer Vision Analysis of Visual Narratives on X",
    "year": 2025,
    "published": "2025-03-12T13:03:49Z",
    "categories": [
      "cs.CV",
      "cs.SI"
    ],
    "abstract": "Climate change is one of the most pressing challenges of the 21st century, sparking widespread discourse across social media platforms. Activists, policymakers, and researchers seek to understand public sentiment and narratives while access to social media data has become increasingly restricted in the post-API era. In this study, we analyze a dataset of climate change-related tweets from X (formerly Twitter) shared in 2019, containing 730k tweets along with the shared images. Our approach integ",
    "arxiv_url": "https://arxiv.org/abs/2503.09361v1",
    "pdf_url": "https://arxiv.org/pdf/2503.09361v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2503.09361",
    "arxiv_authors": [
      "Katharina Prasse",
      "Marcel Kleinmann",
      "Inken Adam",
      "Kerstin Beckersjuergen",
      "Andreas Edte",
      "Jona Frroku",
      "Timotheus Gumpp",
      "Steffen Jung",
      "Isaac Bravo",
      "Stefanie Walter",
      "Margret Keuper"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Deep+Learning+for+Climate+Action%3A+Computer+Vision+Analysis+of+Visual+Narratives+on+X+Katharina+Prasse+Marcel+Kleinmann+Inken+Adam+Kerstin+Beckersjuergen+Andreas+Edte",
    "gs_search_success": true,
    "gs_authors": [
      "PaVhYr4AAAAJ",
      "x5ovaJcAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 9
  },
  {
    "arxiv_id": "2307.00040",
    "title": "DisCo: Disentangled Control for Realistic Human Dance Generation",
    "year": 2023,
    "published": "2023-06-30T17:37:48Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Generative AI has made significant strides in computer vision, particularly in text-driven image/video synthesis (T2I/T2V). Despite the notable advancements, it remains challenging in human-centric content synthesis such as realistic dance generation. Current methodologies, primarily tailored for human motion transfer, encounter difficulties when confronted with real-world dance scenarios (e.g., social media dance), which require to generalize across a wide spectrum of poses and intricate human ",
    "arxiv_url": "https://arxiv.org/abs/2307.00040v3",
    "pdf_url": "https://arxiv.org/pdf/2307.00040v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2307.00040",
    "arxiv_authors": [
      "Tan Wang",
      "Linjie Li",
      "Kevin Lin",
      "Yuanhao Zhai",
      "Chung-Ching Lin",
      "Zhengyuan Yang",
      "Hanwang Zhang",
      "Zicheng Liu",
      "Lijuan Wang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=DisCo%3A+Disentangled+Control+for+Realistic+Human+Dance+Generation+Tan+Wang+Linjie+Li+Kevin+Lin+Yuanhao+Zhai+Chung-Ching+Lin",
    "gs_search_success": true,
    "gs_authors": [
      "YG0DFyYAAAAJ",
      "LKSy1kwAAAAJ",
      "rP02ve8AAAAJ",
      "bkALdvsAAAAJ",
      "legkbM0AAAAJ",
      "WR875gYAAAAJ",
      "wFduC9EAAAAJ",
      "PKgt2LgAAAAJ",
      "cDcWXuIAAAAJ"
    ],
    "citation_count": 149,
    "gs_author_count": 9
  },
  {
    "arxiv_id": "2409.13251",
    "title": "T2M-X: Learning Expressive Text-to-Motion Generation from Partially Annotated Data",
    "year": 2024,
    "published": "2024-09-20T06:20:00Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "The generation of humanoid animation from text prompts can profoundly impact animation production and AR/VR experiences. However, existing methods only generate body motion data, excluding facial expressions and hand movements. This limitation, primarily due to a lack of a comprehensive whole-body motion dataset, inhibits their readiness for production use. Recent attempts to create such a dataset have resulted in either motion inconsistency among different body parts in the artificially augment",
    "arxiv_url": "https://arxiv.org/abs/2409.13251v1",
    "pdf_url": "https://arxiv.org/pdf/2409.13251v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2409.13251",
    "arxiv_authors": [
      "Mingdian Liu",
      "Yilin Liu",
      "Gurunandan Krishnan",
      "Karl S Bayer",
      "Bing Zhou"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=T2M-X%3A+Learning+Expressive+Text-to-Motion+Generation+from+Partially+Annotated+Data+Mingdian+Liu+Yilin+Liu+Gurunandan+Krishnan+Karl+S+Bayer+Bing+Zhou",
    "gs_search_success": false,
    "gs_authors": [],
    "citation_count": null,
    "error_type": "fetch_failed",
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2505.15545",
    "title": "Multi-View Projection for Unsupervised Domain Adaptation in 3D Semantic Segmentation",
    "year": 2025,
    "published": "2025-05-21T14:08:42Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "3D semantic segmentation is essential for autonomous driving and road infrastructure analysis, but state-of-the-art 3D models suffer from severe domain shift when applied across datasets. We propose a multi-view projection framework for unsupervised domain adaptation (UDA). Our method aligns LiDAR scans into coherent 3D scenes and renders them from multiple virtual camera poses to generate large-scale synthetic 2D datasets (PC2D) in various modalities. An ensemble of 2D segmentation models is tr",
    "arxiv_url": "https://arxiv.org/abs/2505.15545v2",
    "pdf_url": "https://arxiv.org/pdf/2505.15545v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2505.15545",
    "arxiv_authors": [
      "Andrew Caunes",
      "Thierry Chateau",
      "Vincent Fremont"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Multi-View+Projection+for+Unsupervised+Domain+Adaptation+in+3D+Semantic+Segmentation+Andrew+Caunes+Thierry+Chateau+Vincent+Fremont",
    "gs_search_success": true,
    "gs_authors": [
      "dq-S95wAAAAJ",
      "DW-OBZYAAAAJ",
      "0BzH_AEAAAAJ"
    ],
    "citation_count": 2,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2411.14723",
    "title": "Effective SAM Combination for Open-Vocabulary Semantic Segmentation",
    "year": 2024,
    "published": "2024-11-22T04:36:12Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Open-vocabulary semantic segmentation aims to assign pixel-level labels to images across an unlimited range of classes. Traditional methods address this by sequentially connecting a powerful mask proposal generator, such as the Segment Anything Model (SAM), with a pre-trained vision-language model like CLIP. But these two-stage approaches often suffer from high computational costs, memory inefficiencies. In this paper, we propose ESC-Net, a novel one-stage open-vocabulary segmentation model that",
    "arxiv_url": "https://arxiv.org/abs/2411.14723v2",
    "pdf_url": "https://arxiv.org/pdf/2411.14723v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2411.14723",
    "arxiv_authors": [
      "Minhyeok Lee",
      "Suhwan Cho",
      "Jungho Lee",
      "Sunghun Yang",
      "Heeseung Choi",
      "Ig-Jae Kim",
      "Sangyoun Lee"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Effective+SAM+Combination+for+Open-Vocabulary+Semantic+Segmentation+Minhyeok+Lee+Suhwan+Cho+Jungho+Lee+Sunghun+Yang+Heeseung+Choi",
    "gs_search_success": true,
    "gs_authors": [
      "GbgofCEAAAAJ",
      "NAj3cTcAAAAJ",
      "pTueCQ8AAAAJ",
      "WGchT7cAAAAJ",
      "H647yqgAAAAJ"
    ],
    "citation_count": 6,
    "gs_author_count": 7
  },
  {
    "arxiv_id": "2412.15341",
    "title": "Efficient Fine-Tuning and Concept Suppression for Pruned Diffusion Models",
    "year": 2024,
    "published": "2024-12-19T19:13:18Z",
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "abstract": "Recent advances in diffusion generative models have yielded remarkable progress. While the quality of generated content continues to improve, these models have grown considerably in size and complexity. This increasing computational burden poses significant challenges, particularly in resource-constrained deployment scenarios such as mobile devices. The combination of model pruning and knowledge distillation has emerged as a promising solution to reduce computational demands while preserving gen",
    "arxiv_url": "https://arxiv.org/abs/2412.15341v2",
    "pdf_url": "https://arxiv.org/pdf/2412.15341v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2412.15341",
    "arxiv_authors": [
      "Reza Shirkavand",
      "Peiran Yu",
      "Shangqian Gao",
      "Gowthami Somepalli",
      "Tom Goldstein",
      "Heng Huang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Efficient+Fine-Tuning+and+Concept+Suppression+for+Pruned+Diffusion+Models+Reza+Shirkavand+Peiran+Yu+Shangqian+Gao+Gowthami+Somepalli+Tom+Goldstein",
    "gs_search_success": true,
    "gs_authors": [
      "4OqLaDwAAAAJ",
      "uS4nCAMAAAAJ",
      "KmSuVtgAAAAJ",
      "9mNI83oAAAAJ",
      "T2ezBDsAAAAJ",
      "SXJ4R24AAAAJ"
    ],
    "citation_count": 5,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2306.16544",
    "title": "Multi-Scale Deformable Alignment and Content-Adaptive Inference for Flexible-Rate Bi-Directional Video Compression",
    "year": 2023,
    "published": "2023-06-28T20:32:16Z",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "abstract": "The lack of ability to adapt the motion compensation model to video content is an important limitation of current end-to-end learned video compression models. This paper advances the state-of-the-art by proposing an adaptive motion-compensation model for end-to-end rate-distortion optimized hierarchical bi-directional video compression. In particular, we propose two novelties: i) a multi-scale deformable alignment scheme at the feature level combined with multi-scale conditional coding, ii) moti",
    "arxiv_url": "https://arxiv.org/abs/2306.16544v1",
    "pdf_url": "https://arxiv.org/pdf/2306.16544v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2306.16544",
    "arxiv_authors": [
      "M. Akın Yılmaz",
      "O. Ugur Ulas",
      "A. Murat Tekalp"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Multi-Scale+Deformable+Alignment+and+Content-Adaptive+Inference+for+Flexible-Rate+Bi-Directional+Video+Compression+M.+Ak%C4%B1n+Y%C4%B1lmaz+O.+Ugur+Ulas+A.+Murat+Tekalp",
    "gs_search_success": true,
    "gs_authors": [
      "kHZOZzkAAAAJ",
      "GzwcDjUAAAAJ",
      "gRgNm7oAAAAJ"
    ],
    "citation_count": 13,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2505.15814",
    "title": "A Taxonomy of Structure from Motion Methods",
    "year": 2025,
    "published": "2025-05-21T17:59:44Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Structure from Motion (SfM) refers to the problem of recovering both structure (i.e., 3D coordinates of points in the scene) and motion (i.e., camera matrices) starting from point correspondences in multiple images. It has attracted significant attention over the years, counting practical reconstruction pipelines as well as theoretical results. This paper is conceived as a conceptual review of SfM methods, which are grouped into three main categories, according to which part of the problem - bet",
    "arxiv_url": "https://arxiv.org/abs/2505.15814v1",
    "pdf_url": "https://arxiv.org/pdf/2505.15814v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2505.15814",
    "arxiv_authors": [
      "Federica Arrigoni"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=A+Taxonomy+of+Structure+from+Motion+Methods+Federica+Arrigoni",
    "gs_search_success": true,
    "gs_authors": [
      "bzBtqfQAAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 1
  },
  {
    "arxiv_id": "2504.19115",
    "title": "Towards Latency-Aware 3D Streaming Perception for Autonomous Driving",
    "year": 2025,
    "published": "2025-04-27T05:49:52Z",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "abstract": "Although existing 3D perception algorithms have demonstrated significant improvements in performance, their deployment on edge devices continues to encounter critical challenges due to substantial runtime latency. We propose a new benchmark tailored for online evaluation by considering runtime latency. Based on the benchmark, we build a Latency-Aware 3D Streaming Perception (LASP) framework that addresses the latency issue through two primary components: 1) latency-aware history integration, whi",
    "arxiv_url": "https://arxiv.org/abs/2504.19115v1",
    "pdf_url": "https://arxiv.org/pdf/2504.19115v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2504.19115",
    "arxiv_authors": [
      "Jiaqi Peng",
      "Tai Wang",
      "Jiangmiao Pang",
      "Yuan Shen"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Towards+Latency-Aware+3D+Streaming+Perception+for+Autonomous+Driving+Jiaqi+Peng+Tai+Wang+Jiangmiao+Pang+Yuan+Shen",
    "gs_search_success": true,
    "gs_authors": [
      "5Mtc0ZoAAAAJ",
      "JmbbZWIAAAAJ",
      "ssSfKpAAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2311.04588",
    "title": "Army of Thieves: Enhancing Black-Box Model Extraction via Ensemble based sample selection",
    "year": 2023,
    "published": "2023-11-08T10:31:29Z",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR",
      "cs.CV"
    ],
    "abstract": "Machine Learning (ML) models become vulnerable to Model Stealing Attacks (MSA) when they are deployed as a service. In such attacks, the deployed model is queried repeatedly to build a labelled dataset. This dataset allows the attacker to train a thief model that mimics the original model. To maximize query efficiency, the attacker has to select the most informative subset of data points from the pool of available data. Existing attack strategies utilize approaches like Active Learning and Semi-",
    "arxiv_url": "https://arxiv.org/abs/2311.04588v1",
    "pdf_url": "https://arxiv.org/pdf/2311.04588v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2311.04588",
    "arxiv_authors": [
      "Akshit Jindal",
      "Vikram Goyal",
      "Saket Anand",
      "Chetan Arora"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Army+of+Thieves%3A+Enhancing+Black-Box+Model+Extraction+via+Ensemble+based+sample+selection+Akshit+Jindal+Vikram+Goyal+Saket+Anand+Chetan+Arora",
    "gs_search_success": true,
    "gs_authors": [
      "c-DznCMAAAAJ",
      "YmYvVEQAAAAJ",
      "YJaVmSwAAAAJ",
      "Q8cTLNMAAAAJ"
    ],
    "citation_count": 10,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2504.16276",
    "title": "An Automated Pipeline for Few-Shot Bird Call Classification: A Case Study with the Tooth-Billed Pigeon",
    "year": 2025,
    "published": "2025-04-22T21:21:41Z",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "cs.SD"
    ],
    "abstract": "This paper presents an automated one-shot bird call classification pipeline designed for rare species absent from large publicly available classifiers like BirdNET and Perch. While these models excel at detecting common birds with abundant training data, they lack options for species with only 1-3 known recordings-a critical limitation for conservationists monitoring the last remaining individuals of endangered birds. To address this, we leverage the embedding space of large bird classification ",
    "arxiv_url": "https://arxiv.org/abs/2504.16276v2",
    "pdf_url": "https://arxiv.org/pdf/2504.16276v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2504.16276",
    "arxiv_authors": [
      "Abhishek Jana",
      "Moeumu Uili",
      "James Atherton",
      "Mark O'Brien",
      "Joe Wood",
      "Leandra Brickson"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=An+Automated+Pipeline+for+Few-Shot+Bird+Call+Classification%3A+A+Case+Study+with+the+Tooth-Billed+Pigeon+Abhishek+Jana+Moeumu+Uili+James+Atherton+Mark+O%27Brien+Joe+Wood",
    "gs_search_success": true,
    "gs_authors": [
      "DCN3neAAAAAJ",
      "IxEIY0gAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2412.19062",
    "title": "DAPoinTr: Domain Adaptive Point Transformer for Point Cloud Completion",
    "year": 2024,
    "published": "2024-12-26T05:16:54Z",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "Point Transformers (PoinTr) have shown great potential in point cloud completion recently. Nevertheless, effective domain adaptation that improves transferability toward target domains remains unexplored. In this paper, we delve into this topic and empirically discover that direct feature alignment on point Transformer's CNN backbone only brings limited improvements since it cannot guarantee sequence-wise domain-invariant features in the Transformer. To this end, we propose a pioneering Domain A",
    "arxiv_url": "https://arxiv.org/abs/2412.19062v1",
    "pdf_url": "https://arxiv.org/pdf/2412.19062v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2412.19062",
    "arxiv_authors": [
      "Yinghui Li",
      "Qianyu Zhou",
      "Jingyu Gong",
      "Ye Zhu",
      "Richard Dazeley",
      "Xinkui Zhao",
      "Xuequan Lu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=DAPoinTr%3A+Domain+Adaptive+Point+Transformer+for+Point+Cloud+Completion+Yinghui+Li+Qianyu+Zhou+Jingyu+Gong+Ye+Zhu+Richard+Dazeley",
    "gs_search_success": true,
    "gs_authors": [
      "QxRHA48AAAAJ",
      "KHg04fkAAAAJ",
      "Y_kHeQwAAAAJ",
      "w4YcdsoAAAAJ",
      "cG_WXywAAAAJ",
      "Tp8Sx6AAAAAJ"
    ],
    "citation_count": 5,
    "gs_author_count": 7
  },
  {
    "arxiv_id": "2503.05051",
    "title": "Accelerated Patient-specific Non-Cartesian MRI Reconstruction using Implicit Neural Representations",
    "year": 2025,
    "published": "2025-03-07T00:05:43Z",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "abstract": "The scanning time for a fully sampled MRI can be undesirably lengthy. Compressed sensing has been developed to minimize image artifacts in accelerated scans, but the required iterative reconstruction is computationally complex and difficult to generalize on new cases. Image-domain-based deep learning methods (e.g., convolutional neural networks) emerged as a faster alternative but face challenges in modeling continuous k-space, a problem amplified with non-Cartesian sampling commonly used in acc",
    "arxiv_url": "https://arxiv.org/abs/2503.05051v1",
    "pdf_url": "https://arxiv.org/pdf/2503.05051v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2503.05051",
    "arxiv_authors": [
      "Di Xu",
      "Hengjie Liu",
      "Xin Miao",
      "Daniel O'Connor",
      "Jessica E. Scholey",
      "Wensha Yang",
      "Mary Feng",
      "Michael Ohliger",
      "Hui Lin",
      "Dan Ruan",
      "Yang Yang",
      "Ke Sheng"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Accelerated+Patient-specific+Non-Cartesian+MRI+Reconstruction+using+Implicit+Neural+Representations+Di+Xu+Hengjie+Liu+Xin+Miao+Daniel+O%27Connor+Jessica+E.+Scholey",
    "gs_search_success": true,
    "gs_authors": [
      "LbXcdmsAAAAJ",
      "2jrPdDgAAAAJ",
      "gwSqxwEAAAAJ",
      "gK_sUR8AAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 9
  },
  {
    "arxiv_id": "2305.15975",
    "title": "Triplet Knowledge Distillation",
    "year": 2023,
    "published": "2023-05-25T12:12:31Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "In Knowledge Distillation, the teacher is generally much larger than the student, making the solution of the teacher likely to be difficult for the student to learn. To ease the mimicking difficulty, we introduce a triplet knowledge distillation mechanism named TriKD. Besides teacher and student, TriKD employs a third role called anchor model. Before distillation begins, the pre-trained anchor model delimits a subspace within the full solution space of the target problem. Solutions within the su",
    "arxiv_url": "https://arxiv.org/abs/2305.15975v1",
    "pdf_url": "https://arxiv.org/pdf/2305.15975v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2305.15975",
    "arxiv_authors": [
      "Xijun Wang",
      "Dongyang Liu",
      "Meina Kan",
      "Chunrui Han",
      "Zhongqin Wu",
      "Shiguang Shan"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Triplet+Knowledge+Distillation+Xijun+Wang+Dongyang+Liu+Meina+Kan+Chunrui+Han+Zhongqin+Wu",
    "gs_search_success": true,
    "gs_authors": [
      "4AKCKKEAAAAJ",
      "KlqOhiUAAAAJ",
      "pHnJPmcAAAAJ",
      "VxQGEOcAAAAJ",
      "D6tWz44AAAAJ",
      "Vkzd7MIAAAAJ"
    ],
    "citation_count": 3,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2503.01085",
    "title": "Identity documents recognition and detection using semantic segmentation with convolutional neural network",
    "year": 2025,
    "published": "2025-03-03T01:13:28Z",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "Object recognition and detection are well-studied problems with a developed set of almost standard solutions. Identity documents recognition, classification, detection, and localization are the tasks required in a number of applications, particularly, in physical access control security systems at critical infrastructure premises. In this paper, we propose the new original architecture of a model based on an artificial convolutional neural network and semantic segmentation approach for the recog",
    "arxiv_url": "https://arxiv.org/abs/2503.01085v1",
    "pdf_url": "https://arxiv.org/pdf/2503.01085v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2503.01085",
    "arxiv_authors": [
      "Mykola Kozlenko",
      "Volodymyr Sendetskyi",
      "Oleksiy Simkiv",
      "Nazar Savchenko",
      "Andy Bosyi"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Identity+documents+recognition+and+detection+using+semantic+segmentation+with+convolutional+neural+network+Mykola+Kozlenko+Volodymyr+Sendetskyi+Oleksiy+Simkiv+Nazar+Savchenko+Andy+Bosyi",
    "gs_search_success": true,
    "gs_authors": [
      "KYT7J_8AAAAJ"
    ],
    "citation_count": 9,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2504.13406",
    "title": "LangCoop: Collaborative Driving with Language",
    "year": 2025,
    "published": "2025-04-18T02:03:14Z",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CL",
      "cs.CV"
    ],
    "abstract": "Multi-agent collaboration holds great promise for enhancing the safety, reliability, and mobility of autonomous driving systems by enabling information sharing among multiple connected agents. However, existing multi-agent communication approaches are hindered by limitations of existing communication media, including high bandwidth demands, agent heterogeneity, and information loss. To address these challenges, we introduce LangCoop, a new paradigm for collaborative autonomous driving that lever",
    "arxiv_url": "https://arxiv.org/abs/2504.13406v2",
    "pdf_url": "https://arxiv.org/pdf/2504.13406v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2504.13406",
    "arxiv_authors": [
      "Xiangbo Gao",
      "Yuheng Wu",
      "Rujia Wang",
      "Chenxi Liu",
      "Yang Zhou",
      "Zhengzhong Tu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=LangCoop%3A+Collaborative+Driving+with+Language+Xiangbo+Gao+Yuheng+Wu+Rujia+Wang+Chenxi+Liu+Yang+Zhou",
    "gs_search_success": true,
    "gs_authors": [
      "0xCRTw0AAAAJ",
      "9ajdZaEAAAAJ",
      "bSpZc84AAAAJ",
      "bAhIqYwAAAAJ",
      "UFvM01gAAAAJ"
    ],
    "citation_count": 15,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2404.01367",
    "title": "Bigger is not Always Better: Scaling Properties of Latent Diffusion Models",
    "year": 2024,
    "published": "2024-04-01T17:59:48Z",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "We study the scaling properties of latent diffusion models (LDMs) with an emphasis on their sampling efficiency. While improved network architecture and inference algorithms have shown to effectively boost sampling efficiency of diffusion models, the role of model size -- a critical determinant of sampling efficiency -- has not been thoroughly examined. Through empirical analysis of established text-to-image diffusion models, we conduct an in-depth investigation into how model size influences sa",
    "arxiv_url": "https://arxiv.org/abs/2404.01367v2",
    "pdf_url": "https://arxiv.org/pdf/2404.01367v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2404.01367",
    "arxiv_authors": [
      "Kangfu Mei",
      "Zhengzhong Tu",
      "Mauricio Delbracio",
      "Hossein Talebi",
      "Vishal M. Patel",
      "Peyman Milanfar"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Bigger+is+not+Always+Better%3A+Scaling+Properties+of+Latent+Diffusion+Models+Kangfu+Mei+Zhengzhong+Tu+Mauricio+Delbracio+Hossein+Talebi+Vishal+M.+Patel",
    "gs_search_success": true,
    "gs_authors": [
      "lDDm920AAAAJ",
      "9ajdZaEAAAAJ",
      "kqXlLzYAAAAJ",
      "AkEXTbIAAAAJ",
      "e_nu_TIAAAAJ",
      "iGzDl8IAAAAJ"
    ],
    "citation_count": 20,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2304.09498",
    "title": "Learning Robust Visual-Semantic Embedding for Generalizable Person Re-identification",
    "year": 2023,
    "published": "2023-04-19T08:37:25Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Generalizable person re-identification (Re-ID) is a very hot research topic in machine learning and computer vision, which plays a significant role in realistic scenarios due to its various applications in public security and video surveillance. However, previous methods mainly focus on the visual representation learning, while neglect to explore the potential of semantic features during training, which easily leads to poor generalization capability when adapted to the new domain. In this paper,",
    "arxiv_url": "https://arxiv.org/abs/2304.09498v1",
    "pdf_url": "https://arxiv.org/pdf/2304.09498v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2304.09498",
    "arxiv_authors": [
      "Suncheng Xiang",
      "Jingsheng Gao",
      "Mengyuan Guan",
      "Jiacheng Ruan",
      "Chengfeng Zhou",
      "Ting Liu",
      "Dahong Qian",
      "Yuzhuo Fu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Learning+Robust+Visual-Semantic+Embedding+for+Generalizable+Person+Re-identification+Suncheng+Xiang+Jingsheng+Gao+Mengyuan+Guan+Jiacheng+Ruan+Chengfeng+Zhou",
    "gs_search_success": true,
    "gs_authors": [
      "IyrSeG8AAAAJ",
      "O4o2aQcAAAAJ",
      "QDlaRJkAAAAJ",
      "oymfXUIAAAAJ",
      "_PdbsmAAAAAJ",
      "EQEPRKAAAAAJ"
    ],
    "citation_count": 17,
    "gs_author_count": 8
  },
  {
    "arxiv_id": "2402.12536",
    "title": "Designing High-Performing Networks for Multi-Scale Computer Vision",
    "year": 2024,
    "published": "2024-02-19T20:50:55Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Since the emergence of deep learning, the computer vision field has flourished with models improving at a rapid pace on more and more complex tasks. We distinguish three main ways to improve a computer vision model: (1) improving the data aspect by for example training on a large, more diverse dataset, (2) improving the training aspect by for example designing a better optimizer, and (3) improving the network architecture (or network for short). In this thesis, we chose to improve the latter, i.",
    "arxiv_url": "https://arxiv.org/abs/2402.12536v1",
    "pdf_url": "https://arxiv.org/pdf/2402.12536v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2402.12536",
    "arxiv_authors": [
      "Cédric Picron"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Designing+High-Performing+Networks+for+Multi-Scale+Computer+Vision+C%C3%A9dric+Picron",
    "gs_search_success": true,
    "gs_authors": [
      "EuFF9kUAAAAJ",
      "nzXOc4QAAAAJ",
      "AyXW9gYAAAAJ"
    ],
    "citation_count": 2,
    "gs_author_count": 1
  },
  {
    "arxiv_id": "2405.17698",
    "title": "BaboonLand Dataset: Tracking Primates in the Wild and Automating Behaviour Recognition from Drone Videos",
    "year": 2024,
    "published": "2024-05-27T23:09:37Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Using drones to track multiple individuals simultaneously in their natural environment is a powerful approach for better understanding group primate behavior. Previous studies have demonstrated that it is possible to automate the classification of primate behavior from video data, but these studies have been carried out in captivity or from ground-based cameras. To understand group behavior and the self-organization of a collective, the whole troop needs to be seen at a scale where behavior can ",
    "arxiv_url": "https://arxiv.org/abs/2405.17698v3",
    "pdf_url": "https://arxiv.org/pdf/2405.17698v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2405.17698",
    "arxiv_authors": [
      "Isla Duporge",
      "Maksim Kholiavchenko",
      "Roi Harel",
      "Scott Wolf",
      "Dan Rubenstein",
      "Meg Crofoot",
      "Tanya Berger-Wolf",
      "Stephen Lee",
      "Julie Barreau",
      "Jenna Kline",
      "Michelle Ramirez",
      "Charles Stewart"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=BaboonLand+Dataset%3A+Tracking+Primates+in+the+Wild+and+Automating+Behaviour+Recognition+from+Drone+Videos+Isla+Duporge+Maksim+Kholiavchenko+Roi+Harel+Scott+Wolf+Dan+Rubenstein",
    "gs_search_success": true,
    "gs_authors": [
      "FiaKXNMAAAAJ",
      "JeflQ3IAAAAJ",
      "Hq28JM0AAAAJ",
      "iO-ruAkAAAAJ",
      "oD4QN6wAAAAJ",
      "QcA4Z_QAAAAJ",
      "oJ_KilcAAAAJ"
    ],
    "citation_count": 31,
    "gs_author_count": 9
  },
  {
    "arxiv_id": "2405.14419",
    "title": "A motion-based compression algorithm for resource-constrained video camera traps",
    "year": 2024,
    "published": "2024-05-23T10:39:33Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "q-bio.QM"
    ],
    "abstract": "Field-captured video facilitates detailed studies of spatio-temporal aspects of animal locomotion, decision-making and environmental interactions including predator-prey relationships and habitat utilisation. But even though data capture is cheap with mass-produced hardware, storage, processing and transmission overheads provide a hurdle to acquisition of high resolution video from field-situated edge computing devices. Efficient compression algorithms are therefore essential if monitoring is to",
    "arxiv_url": "https://arxiv.org/abs/2405.14419v3",
    "pdf_url": "https://arxiv.org/pdf/2405.14419v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2405.14419",
    "arxiv_authors": [
      "Malika Nisal Ratnayake",
      "Lex Gallon",
      "Adel N. Toosi",
      "Alan Dorin"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=A+motion-based+compression+algorithm+for+resource-constrained+video+camera+traps+Malika+Nisal+Ratnayake+Lex+Gallon+Adel+N.+Toosi+Alan+Dorin",
    "gs_search_success": true,
    "gs_authors": [
      "akGqM2IAAAAJ",
      "0QydmJAAAAAJ",
      "qIh_I-gAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2404.10838",
    "title": "Dynamic Self-adaptive Multiscale Distillation from Pre-trained Multimodal Large Model for Efficient Cross-modal Representation Learning",
    "year": 2024,
    "published": "2024-04-16T18:22:49Z",
    "categories": [
      "cs.CV",
      "cs.CL",
      "cs.MM"
    ],
    "abstract": "In recent years, pre-trained multimodal large models have attracted widespread attention due to their outstanding performance in various multimodal applications. Nonetheless, the extensive computational resources and vast datasets required for their training present significant hurdles for deployment in environments with limited computational resources. To address this challenge, we propose a novel dynamic self-adaptive multiscale distillation from pre-trained multimodal large model for efficien",
    "arxiv_url": "https://arxiv.org/abs/2404.10838v1",
    "pdf_url": "https://arxiv.org/pdf/2404.10838v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2404.10838",
    "arxiv_authors": [
      "Zhengyang Liang",
      "Meiyu Liang",
      "Wei Huang",
      "Yawen Li",
      "Zhe Xue"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Dynamic+Self-adaptive+Multiscale+Distillation+from+Pre-trained+Multimodal+Large+Model+for+Efficient+Cross-modal+Representation+Learning+Zhengyang+Liang+Meiyu+Liang+Wei+Huang+Yawen+Li+Zhe+Xue",
    "gs_search_success": true,
    "gs_authors": [
      "rQpizr0AAAAJ",
      "9IC8FBQAAAAJ",
      "h6gVF8YAAAAJ",
      "RzNY-o4AAAAJ"
    ],
    "citation_count": 4,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2304.08709",
    "title": "You Only Need Two Detectors to Achieve Multi-Modal 3D Multi-Object Tracking",
    "year": 2023,
    "published": "2023-04-18T02:45:18Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "In the classical tracking-by-detection (TBD) paradigm, detection and tracking are separately and sequentially conducted, and data association must be properly performed to achieve satisfactory tracking performance. In this paper, a new end-to-end multi-object tracking framework is proposed, which integrates object detection and multi-object tracking into a single model. The proposed tracking framework eliminates the complex data association process in the classical TBD paradigm, and requires no ",
    "arxiv_url": "https://arxiv.org/abs/2304.08709v2",
    "pdf_url": "https://arxiv.org/pdf/2304.08709v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2304.08709",
    "arxiv_authors": [
      "Xiyang Wang",
      "Chunyun Fu",
      "Jiawei He",
      "Mingguang Huang",
      "Ting Meng",
      "Siyu Zhang",
      "Hangning Zhou",
      "Ziyao Xu",
      "Chi Zhang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=You+Only+Need+Two+Detectors+to+Achieve+Multi-Modal+3D+Multi-Object+Tracking+Xiyang+Wang+Chunyun+Fu+Jiawei+He+Mingguang+Huang+Ting+Meng",
    "gs_search_success": true,
    "gs_authors": [
      "EKtNV5sAAAAJ",
      "b5Le6AEAAAAJ"
    ],
    "citation_count": 33,
    "gs_author_count": 9
  },
  {
    "arxiv_id": "2308.09515",
    "title": "Learnt Contrastive Concept Embeddings for Sign Recognition",
    "year": 2023,
    "published": "2023-08-18T12:47:18Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "In natural language processing (NLP) of spoken languages, word embeddings have been shown to be a useful method to encode the meaning of words. Sign languages are visual languages, which require sign embeddings to capture the visual and linguistic semantics of sign. Unlike many common approaches to Sign Recognition, we focus on explicitly creating sign embeddings that bridge the gap between sign language and spoken language. We propose a learning framework to derive LCC (Learnt Contrastive Conce",
    "arxiv_url": "https://arxiv.org/abs/2308.09515v1",
    "pdf_url": "https://arxiv.org/pdf/2308.09515v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2308.09515",
    "arxiv_authors": [
      "Ryan Wong",
      "Necati Cihan Camgoz",
      "Richard Bowden"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Learnt+Contrastive+Concept+Embeddings+for+Sign+Recognition+Ryan+Wong+Necati+Cihan+Camgoz+Richard+Bowden",
    "gs_search_success": true,
    "gs_authors": [
      "mvvgDvcAAAAJ",
      "T3C1SW0AAAAJ",
      "Tk5Egv8AAAAJ"
    ],
    "citation_count": 10,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2408.16809",
    "title": "See or Guess: Counterfactually Regularized Image Captioning",
    "year": 2024,
    "published": "2024-08-29T17:59:57Z",
    "categories": [
      "cs.CV",
      "cs.CL",
      "cs.MM"
    ],
    "abstract": "Image captioning, which generates natural language descriptions of the visual information in an image, is a crucial task in vision-language research. Previous models have typically addressed this task by aligning the generative capabilities of machines with human intelligence through statistical fitting of existing datasets. While effective for normal images, they may struggle to accurately describe those where certain parts of the image are obscured or edited, unlike humans who excel in such ca",
    "arxiv_url": "https://arxiv.org/abs/2408.16809v1",
    "pdf_url": "https://arxiv.org/pdf/2408.16809v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2408.16809",
    "arxiv_authors": [
      "Qian Cao",
      "Xu Chen",
      "Ruihua Song",
      "Xiting Wang",
      "Xinting Huang",
      "Yuchen Ren"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=See+or+Guess%3A+Counterfactually+Regularized+Image+Captioning+Qian+Cao+Xu+Chen+Ruihua+Song+Xiting+Wang+Xinting+Huang",
    "gs_search_success": true,
    "gs_authors": [
      "urC8meQAAAAJ",
      "v5LctN8AAAAJ",
      "2OTi2RUAAAAJ",
      "loPoqy0AAAAJ",
      "QmyPDWQAAAAJ",
      "U4EnCRYAAAAJ"
    ],
    "citation_count": 6,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2406.13445",
    "title": "Lost in UNet: Improving Infrared Small Target Detection by Underappreciated Local Features",
    "year": 2024,
    "published": "2024-06-19T11:11:38Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Many targets are often very small in infrared images due to the long-distance imaging meachnism. UNet and its variants, as popular detection backbone networks, downsample the local features early and cause the irreversible loss of these local features, leading to both the missed and false detection of small targets in infrared images. We propose HintU, a novel network to recover the local features lost by various UNet-based methods for effective infrared small target detection. HintU has two key",
    "arxiv_url": "https://arxiv.org/abs/2406.13445v1",
    "pdf_url": "https://arxiv.org/pdf/2406.13445v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2406.13445",
    "arxiv_authors": [
      "Wuzhou Quan",
      "Wei Zhao",
      "Weiming Wang",
      "Haoran Xie",
      "Fu Lee Wang",
      "Mingqiang Wei"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Lost+in+UNet%3A+Improving+Infrared+Small+Target+Detection+by+Underappreciated+Local+Features+Wuzhou+Quan+Wei+Zhao+Weiming+Wang+Haoran+Xie+Fu+Lee+Wang",
    "gs_search_success": true,
    "gs_authors": [
      "bxz8Pn8AAAAJ",
      "O4lGUj8AAAAJ",
      "EN0PM30AAAAJ",
      "MEBjvKsAAAAJ",
      "OyzZmeAAAAAJ",
      "TdrJj8MAAAAJ"
    ],
    "citation_count": 12,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2409.13648",
    "title": "V^3: Viewing Volumetric Videos on Mobiles via Streamable 2D Dynamic Gaussians",
    "year": 2024,
    "published": "2024-09-20T16:54:27Z",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "abstract": "Experiencing high-fidelity volumetric video as seamlessly as 2D videos is a long-held dream. However, current dynamic 3DGS methods, despite their high rendering quality, face challenges in streaming on mobile devices due to computational and bandwidth constraints. In this paper, we introduce V^3 (Viewing Volumetric Videos), a novel approach that enables high-quality mobile rendering through the streaming of dynamic Gaussians. Our key innovation is to view dynamic 3DGS as 2D videos, facilitating ",
    "arxiv_url": "https://arxiv.org/abs/2409.13648v2",
    "pdf_url": "https://arxiv.org/pdf/2409.13648v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2409.13648",
    "arxiv_authors": [
      "Penghao Wang",
      "Zhirui Zhang",
      "Liao Wang",
      "Kaixin Yao",
      "Siyuan Xie",
      "Jingyi Yu",
      "Minye Wu",
      "Lan Xu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=V%5E3%3A+Viewing+Volumetric+Videos+on+Mobiles+via+Streamable+2D+Dynamic+Gaussians+Penghao+Wang+Zhirui+Zhang+Liao+Wang+Kaixin+Yao+Siyuan+Xie",
    "gs_search_success": true,
    "gs_authors": [
      "aPS5pJkAAAAJ",
      "UCO39HkAAAAJ",
      "3J9Xq68AAAAJ",
      "aUAnceQAAAAJ",
      "cTpteaQAAAAJ"
    ],
    "citation_count": 31,
    "gs_author_count": 8
  },
  {
    "arxiv_id": "2310.01881",
    "title": "Adaptive Multi-NeRF: Exploit Efficient Parallelism in Adaptive Multiple Scale Neural Radiance Field Rendering",
    "year": 2023,
    "published": "2023-10-03T08:34:49Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Recent advances in Neural Radiance Fields (NeRF) have demonstrated significant potential for representing 3D scene appearances as implicit neural networks, enabling the synthesis of high-fidelity novel views. However, the lengthy training and rendering process hinders the widespread adoption of this promising technique for real-time rendering applications. To address this issue, we present an effective adaptive multi-NeRF method designed to accelerate the neural rendering process for large scene",
    "arxiv_url": "https://arxiv.org/abs/2310.01881v1",
    "pdf_url": "https://arxiv.org/pdf/2310.01881v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2310.01881",
    "arxiv_authors": [
      "Tong Wang",
      "Shuichi Kurabayashi"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Adaptive+Multi-NeRF%3A+Exploit+Efficient+Parallelism+in+Adaptive+Multiple+Scale+Neural+Radiance+Field+Rendering+Tong+Wang+Shuichi+Kurabayashi",
    "gs_search_success": true,
    "gs_authors": [
      "thXGrVYAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 2
  },
  {
    "arxiv_id": "2407.13771",
    "title": "Training-Free Model Merging for Multi-target Domain Adaptation",
    "year": 2024,
    "published": "2024-07-18T17:59:57Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "In this paper, we study multi-target domain adaptation of scene understanding models. While previous methods achieved commendable results through inter-domain consistency losses, they often assumed unrealistic simultaneous access to images from all target domains, overlooking constraints such as data transfer bandwidth limitations and data privacy concerns. Given these challenges, we pose the question: How to merge models adapted independently on distinct domains while bypassing the need for dir",
    "arxiv_url": "https://arxiv.org/abs/2407.13771v1",
    "pdf_url": "https://arxiv.org/pdf/2407.13771v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2407.13771",
    "arxiv_authors": [
      "Wenyi Li",
      "Huan-ang Gao",
      "Mingju Gao",
      "Beiwen Tian",
      "Rong Zhi",
      "Hao Zhao"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Training-Free+Model+Merging+for+Multi-target+Domain+Adaptation+Wenyi+Li+Huan-ang+Gao+Mingju+Gao+Beiwen+Tian+Rong+Zhi",
    "gs_search_success": true,
    "gs_authors": [
      "ygQznUQAAAAJ",
      "WvbKfLgAAAAJ",
      "PMuXXWQAAAAJ",
      "y_jNFVgAAAAJ"
    ],
    "citation_count": 15,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2311.12070",
    "title": "FDDM: Unsupervised Medical Image Translation with a Frequency-Decoupled Diffusion Model",
    "year": 2023,
    "published": "2023-11-19T19:44:44Z",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "abstract": "Diffusion models have demonstrated significant potential in producing high-quality images in medical image translation to aid disease diagnosis, localization, and treatment. Nevertheless, current diffusion models have limited success in achieving faithful image translations that can accurately preserve the anatomical structures of medical images, especially for unpaired datasets. The preservation of structural and anatomical details is essential to reliable medical diagnosis and treatment planni",
    "arxiv_url": "https://arxiv.org/abs/2311.12070v3",
    "pdf_url": "https://arxiv.org/pdf/2311.12070v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2311.12070",
    "arxiv_authors": [
      "Yunxiang Li",
      "Hua-Chieh Shao",
      "Xiaoxue Qian",
      "You Zhang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=FDDM%3A+Unsupervised+Medical+Image+Translation+with+a+Frequency-Decoupled+Diffusion+Model+Yunxiang+Li+Hua-Chieh+Shao+Xiaoxue+Qian+You+Zhang",
    "gs_search_success": true,
    "gs_authors": [
      "evbcKz8AAAAJ",
      "VzdJe_4AAAAJ",
      "0vAGzmMAAAAJ"
    ],
    "citation_count": 8,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2412.11435",
    "title": "Learning Implicit Features with Flow Infused Attention for Realistic Virtual Try-On",
    "year": 2024,
    "published": "2024-12-16T04:23:33Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Image-based virtual try-on is challenging since the generated image should fit the garment to model images in various poses and keep the characteristics and details of the garment simultaneously. A popular research stream warps the garment image firstly to reduce the burden of the generation stage, which relies highly on the performance of the warping module. Other methods without explicit warping often lack sufficient guidance to fit the garment to the model images. In this paper, we propose FI",
    "arxiv_url": "https://arxiv.org/abs/2412.11435v1",
    "pdf_url": "https://arxiv.org/pdf/2412.11435v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2412.11435",
    "arxiv_authors": [
      "Delong Zhang",
      "Qiwei Huang",
      "Yuanliu Liu",
      "Yang Sun",
      "Wei-Shi Zheng",
      "Pengfei Xiong",
      "Wei Zhang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Learning+Implicit+Features+with+Flow+Infused+Attention+for+Realistic+Virtual+Try-On+Delong+Zhang+Qiwei+Huang+Yuanliu+Liu+Yang+Sun+Wei-Shi+Zheng",
    "gs_search_success": false,
    "gs_authors": [],
    "citation_count": null,
    "error_type": "fetch_failed",
    "gs_author_count": 7
  },
  {
    "arxiv_id": "2403.09363",
    "title": "Sentinel-Guided Zero-Shot Learning: A Collaborative Paradigm without Real Data Exposure",
    "year": 2024,
    "published": "2024-03-14T13:12:49Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "With increasing concerns over data privacy and model copyrights, especially in the context of collaborations between AI service providers and data owners, an innovative SG-ZSL paradigm is proposed in this work. SG-ZSL is designed to foster efficient collaboration without the need to exchange models or sensitive data. It consists of a teacher model, a student model and a generator that links both model entities. The teacher model serves as a sentinel on behalf of the data owner, replacing real da",
    "arxiv_url": "https://arxiv.org/abs/2403.09363v1",
    "pdf_url": "https://arxiv.org/pdf/2403.09363v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2403.09363",
    "arxiv_authors": [
      "Fan Wan",
      "Xingyu Miao",
      "Haoran Duan",
      "Jingjing Deng",
      "Rui Gao",
      "Yang Long"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Sentinel-Guided+Zero-Shot+Learning%3A+A+Collaborative+Paradigm+without+Real+Data+Exposure+Fan+Wan+Xingyu+Miao+Haoran+Duan+Jingjing+Deng+Rui+Gao",
    "gs_search_success": true,
    "gs_authors": [
      "Ft_70SQAAAAJ",
      "AVemHcIAAAAJ",
      "2fIQOBsAAAAJ",
      "QSY2OkIAAAAJ",
      "2NRO1eoAAAAJ",
      "IrkuknEAAAAJ"
    ],
    "citation_count": 7,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2403.12035",
    "title": "CoCoCo: Improving Text-Guided Video Inpainting for Better Consistency, Controllability and Compatibility",
    "year": 2024,
    "published": "2024-03-18T17:59:27Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Recent advancements in video generation have been remarkable, yet many existing methods struggle with issues of consistency and poor text-video alignment. Moreover, the field lacks effective techniques for text-guided video inpainting, a stark contrast to the well-explored domain of text-guided image inpainting. To this end, this paper proposes a novel text-guided video inpainting model that achieves better consistency, controllability and compatibility. Specifically, we introduce a simple but e",
    "arxiv_url": "https://arxiv.org/abs/2403.12035v1",
    "pdf_url": "https://arxiv.org/pdf/2403.12035v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2403.12035",
    "arxiv_authors": [
      "Bojia Zi",
      "Shihao Zhao",
      "Xianbiao Qi",
      "Jianan Wang",
      "Yukai Shi",
      "Qianyu Chen",
      "Bin Liang",
      "Kam-Fai Wong",
      "Lei Zhang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=CoCoCo%3A+Improving+Text-Guided+Video+Inpainting+for+Better+Consistency%2C+Controllability+and+Compatibility+Bojia+Zi+Shihao+Zhao+Xianbiao+Qi+Jianan+Wang+Yukai+Shi",
    "gs_search_success": true,
    "gs_authors": [
      "mt5mvZ8AAAAJ",
      "dNQiLDQAAAAJ",
      "Zb5wT08AAAAJ",
      "QrMKIkEAAAAJ",
      "Kh8FoLQAAAAJ",
      "oQXfkSQAAAAJ",
      "fyMni2cAAAAJ",
      "djpQeLEAAAAJ"
    ],
    "citation_count": 28,
    "gs_author_count": 9
  },
  {
    "arxiv_id": "2306.04021",
    "title": "Energy-Based Models for Cross-Modal Localization using Convolutional Transformers",
    "year": 2023,
    "published": "2023-06-06T21:27:08Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "cs.RO"
    ],
    "abstract": "We present a novel framework using Energy-Based Models (EBMs) for localizing a ground vehicle mounted with a range sensor against satellite imagery in the absence of GPS. Lidar sensors have become ubiquitous on autonomous vehicles for describing its surrounding environment. Map priors are typically built using the same sensor modality for localization purposes. However, these map building endeavors using range sensors are often expensive and time-consuming. Alternatively, we leverage the use of ",
    "arxiv_url": "https://arxiv.org/abs/2306.04021v1",
    "pdf_url": "https://arxiv.org/pdf/2306.04021v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2306.04021",
    "arxiv_authors": [
      "Alan Wu",
      "Michael S. Ryoo"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Energy-Based+Models+for+Cross-Modal+Localization+using+Convolutional+Transformers+Alan+Wu+Michael+S.+Ryoo",
    "gs_search_success": true,
    "gs_authors": [
      "vcw0TJIAAAAJ"
    ],
    "citation_count": 3,
    "gs_author_count": 2
  },
  {
    "arxiv_id": "2405.18021",
    "title": "MULi-Ev: Maintaining Unperturbed LiDAR-Event Calibration",
    "year": 2024,
    "published": "2024-05-28T10:09:49Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Despite the increasing interest in enhancing perception systems for autonomous vehicles, the online calibration between event cameras and LiDAR - two sensors pivotal in capturing comprehensive environmental information - remains unexplored. We introduce MULi-Ev, the first online, deep learning-based framework tailored for the extrinsic calibration of event cameras with LiDAR. This advancement is instrumental for the seamless integration of LiDAR and event cameras, enabling dynamic, real-time cal",
    "arxiv_url": "https://arxiv.org/abs/2405.18021v1",
    "pdf_url": "https://arxiv.org/pdf/2405.18021v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2405.18021",
    "arxiv_authors": [
      "Mathieu Cocheteux",
      "Julien Moreau",
      "Franck Davoine"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=MULi-Ev%3A+Maintaining+Unperturbed+LiDAR-Event+Calibration+Mathieu+Cocheteux+Julien+Moreau+Franck+Davoine",
    "gs_search_success": true,
    "gs_authors": [
      "_wJgcSUAAAAJ",
      "QWX91zcAAAAJ"
    ],
    "citation_count": 9,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2404.04421",
    "title": "PhysAvatar: Learning the Physics of Dressed 3D Avatars from Visual Observations",
    "year": 2024,
    "published": "2024-04-05T21:44:57Z",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "abstract": "Modeling and rendering photorealistic avatars is of crucial importance in many applications. Existing methods that build a 3D avatar from visual observations, however, struggle to reconstruct clothed humans. We introduce PhysAvatar, a novel framework that combines inverse rendering with inverse physics to automatically estimate the shape and appearance of a human from multi-view video data along with the physical parameters of the fabric of their clothes. For this purpose, we adopt a mesh-aligne",
    "arxiv_url": "https://arxiv.org/abs/2404.04421v2",
    "pdf_url": "https://arxiv.org/pdf/2404.04421v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2404.04421",
    "arxiv_authors": [
      "Yang Zheng",
      "Qingqing Zhao",
      "Guandao Yang",
      "Wang Yifan",
      "Donglai Xiang",
      "Florian Dubost",
      "Dmitry Lagun",
      "Thabo Beeler",
      "Federico Tombari",
      "Leonidas Guibas",
      "Gordon Wetzstein"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=PhysAvatar%3A+Learning+the+Physics+of+Dressed+3D+Avatars+from+Visual+Observations+Yang+Zheng+Qingqing+Zhao+Guandao+Yang+Wang+Yifan+Donglai+Xiang",
    "gs_search_success": true,
    "gs_authors": [
      "Q7Ouk0QAAAAJ",
      "sY8lt7AAAAAJ",
      "UVMgJvYAAAAJ",
      "_kElCmMAAAAJ",
      "TFsE4BIAAAAJ",
      "_yNBmx8AAAAJ",
      "tjT-DfsAAAAJ",
      "q5y44h8AAAAJ",
      "4zyT8SYAAAAJ"
    ],
    "citation_count": 45,
    "gs_author_count": 9
  },
  {
    "arxiv_id": "2311.00562",
    "title": "MNN: Mixed Nearest-Neighbors for Self-Supervised Learning",
    "year": 2023,
    "published": "2023-11-01T14:59:41Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "In contrastive self-supervised learning, positive samples are typically drawn from the same image but in different augmented views, resulting in a relatively limited source of positive samples. An effective way to alleviate this problem is to incorporate the relationship between samples, which involves including the top-K nearest neighbors of positive samples. However, the problem of false neighbors (i.e., neighbors that do not belong to the same category as the positive sample) is an objective ",
    "arxiv_url": "https://arxiv.org/abs/2311.00562v2",
    "pdf_url": "https://arxiv.org/pdf/2311.00562v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2311.00562",
    "arxiv_authors": [
      "Xianzhong Long",
      "Chen Peng",
      "Yun Li"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=MNN%3A+Mixed+Nearest-Neighbors+for+Self-Supervised+Learning+Xianzhong+Long+Chen+Peng+Yun+Li",
    "gs_search_success": true,
    "gs_authors": [
      "2BK_EXgAAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2409.13591",
    "title": "Portrait Video Editing Empowered by Multimodal Generative Priors",
    "year": 2024,
    "published": "2024-09-20T15:45:13Z",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "abstract": "We introduce PortraitGen, a powerful portrait video editing method that achieves consistent and expressive stylization with multimodal prompts. Traditional portrait video editing methods often struggle with 3D and temporal consistency, and typically lack in rendering quality and efficiency. To address these issues, we lift the portrait video frames to a unified dynamic 3D Gaussian field, which ensures structural and temporal coherence across frames. Furthermore, we design a novel Neural Gaussian",
    "arxiv_url": "https://arxiv.org/abs/2409.13591v1",
    "pdf_url": "https://arxiv.org/pdf/2409.13591v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2409.13591",
    "arxiv_authors": [
      "Xuan Gao",
      "Haiyao Xiao",
      "Chenglai Zhong",
      "Shimin Hu",
      "Yudong Guo",
      "Juyong Zhang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Portrait+Video+Editing+Empowered+by+Multimodal+Generative+Priors+Xuan+Gao+Haiyao+Xiao+Chenglai+Zhong+Shimin+Hu+Yudong+Guo",
    "gs_search_success": true,
    "gs_authors": [
      "1vLVI9IAAAAJ",
      "cxF_-i4AAAAJ",
      "_D9aUrgAAAAJ"
    ],
    "citation_count": 8,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2305.12250",
    "title": "DAC: Detector-Agnostic Spatial Covariances for Deep Local Features",
    "year": 2023,
    "published": "2023-05-20T17:43:09Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Current deep visual local feature detectors do not model the spatial uncertainty of detected features, producing suboptimal results in downstream applications. In this work, we propose two post-hoc covariance estimates that can be plugged into any pretrained deep feature detector: a simple, isotropic covariance estimate that uses the predicted score at a given pixel location, and a full covariance estimate via the local structure tensor of the learned score maps. Both methods are easy to impleme",
    "arxiv_url": "https://arxiv.org/abs/2305.12250v2",
    "pdf_url": "https://arxiv.org/pdf/2305.12250v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2305.12250",
    "arxiv_authors": [
      "Javier Tirado-Garín",
      "Frederik Warburg",
      "Javier Civera"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=DAC%3A+Detector-Agnostic+Spatial+Covariances+for+Deep+Local+Features+Javier+Tirado-Gar%C3%ADn+Frederik+Warburg+Javier+Civera",
    "gs_search_success": true,
    "gs_authors": [
      "j_sMzokAAAAJ",
      "0Ozzy4IAAAAJ",
      "Bh77DXYAAAAJ"
    ],
    "citation_count": 2,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2407.13677",
    "title": "PASTA: Controllable Part-Aware Shape Generation with Autoregressive Transformers",
    "year": 2024,
    "published": "2024-07-18T16:52:45Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR",
      "cs.LG"
    ],
    "abstract": "The increased demand for tools that automate the 3D content creation process led to tremendous progress in deep generative models that can generate diverse 3D objects of high fidelity. In this paper, we present PASTA, an autoregressive transformer architecture for generating high quality 3D shapes. PASTA comprises two main components: An autoregressive transformer that generates objects as a sequence of cuboidal primitives and a blending network, implemented with a transformer decoder that compo",
    "arxiv_url": "https://arxiv.org/abs/2407.13677v1",
    "pdf_url": "https://arxiv.org/pdf/2407.13677v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2407.13677",
    "arxiv_authors": [
      "Songlin Li",
      "Despoina Paschalidou",
      "Leonidas Guibas"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=PASTA%3A+Controllable+Part-Aware+Shape+Generation+with+Autoregressive+Transformers+Songlin+Li+Despoina+Paschalidou+Leonidas+Guibas",
    "gs_search_success": true,
    "gs_authors": [
      "5JlEyTAAAAAJ",
      "zxFlR6sAAAAJ",
      "WXw-gLgAAAAJ"
    ],
    "citation_count": 8,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2401.06442",
    "title": "RotationDrag: Point-based Image Editing with Rotated Diffusion Features",
    "year": 2024,
    "published": "2024-01-12T08:24:35Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "A precise and user-friendly manipulation of image content while preserving image fidelity has always been crucial to the field of image editing. Thanks to the power of generative models, recent point-based image editing methods allow users to interactively change the image content with high generalizability by clicking several control points. But the above mentioned editing process is usually based on the assumption that features stay constant in the motion supervision step from initial to targe",
    "arxiv_url": "https://arxiv.org/abs/2401.06442v1",
    "pdf_url": "https://arxiv.org/pdf/2401.06442v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2401.06442",
    "arxiv_authors": [
      "Minxing Luo",
      "Wentao Cheng",
      "Jian Yang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=RotationDrag%3A+Point-based+Image+Editing+with+Rotated+Diffusion+Features+Minxing+Luo+Wentao+Cheng+Jian+Yang",
    "gs_search_success": true,
    "gs_authors": [
      "cpIySUoAAAAJ",
      "6CIDtZQAAAAJ"
    ],
    "citation_count": 3,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2309.13516",
    "title": "InSpaceType: Reconsider Space Type in Indoor Monocular Depth Estimation",
    "year": 2023,
    "published": "2023-09-24T00:39:41Z",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "abstract": "Indoor monocular depth estimation has attracted increasing research interest. Most previous works have been focusing on methodology, primarily experimenting with NYU-Depth-V2 (NYUv2) Dataset, and only concentrated on the overall performance over the test set. However, little is known regarding robustness and generalization when it comes to applying monocular depth estimation methods to real-world scenarios where highly varying and diverse functional \\textit{space types} are present such as libra",
    "arxiv_url": "https://arxiv.org/abs/2309.13516v2",
    "pdf_url": "https://arxiv.org/pdf/2309.13516v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2309.13516",
    "arxiv_authors": [
      "Cho-Ying Wu",
      "Quankai Gao",
      "Chin-Cheng Hsu",
      "Te-Lin Wu",
      "Jing-Wen Chen",
      "Ulrich Neumann"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=InSpaceType%3A+Reconsider+Space+Type+in+Indoor+Monocular+Depth+Estimation+Cho-Ying+Wu+Quankai+Gao+Chin-Cheng+Hsu+Te-Lin+Wu+Jing-Wen+Chen",
    "gs_search_success": true,
    "gs_authors": [
      "0bXl62UAAAAJ",
      "TTDb6YMAAAAJ",
      "5i7k1RoAAAAJ",
      "UyWUO9IAAAAJ",
      "tIdThSIAAAAJ",
      "MHet2VoAAAAJ"
    ],
    "citation_count": 10,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2405.11129",
    "title": "MotionGS : Compact Gaussian Splatting SLAM by Motion Filter",
    "year": 2024,
    "published": "2024-05-18T00:47:29Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "With their high-fidelity scene representation capability, the attention of SLAM field is deeply attracted by the Neural Radiation Field (NeRF) and 3D Gaussian Splatting (3DGS). Recently, there has been a surge in NeRF-based SLAM, while 3DGS-based SLAM is sparse. A novel 3DGS-based SLAM approach with a fusion of deep visual feature, dual keyframe selection and 3DGS is presented in this paper. Compared with the existing methods, the proposed tracking is achieved by feature extraction and motion fi",
    "arxiv_url": "https://arxiv.org/abs/2405.11129v2",
    "pdf_url": "https://arxiv.org/pdf/2405.11129v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2405.11129",
    "arxiv_authors": [
      "Xinli Guo",
      "Weidong Zhang",
      "Ruonan Liu",
      "Peng Han",
      "Hongtian Chen"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=MotionGS+%3A+Compact+Gaussian+Splatting+SLAM+by+Motion+Filter+Xinli+Guo+Weidong+Zhang+Ruonan+Liu+Peng+Han+Hongtian+Chen",
    "gs_search_success": true,
    "gs_authors": [],
    "citation_count": 11,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2504.08823",
    "title": "FM-LoRA: Factorized Low-Rank Meta-Prompting for Continual Learning",
    "year": 2025,
    "published": "2025-04-09T19:36:18Z",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "abstract": "How to adapt a pre-trained model continuously for sequential tasks with different prediction class labels and domains and finally learn a generalizable model across diverse tasks is a long-lasting challenge. Continual learning (CL) has emerged as a promising approach to leverage pre-trained models (e.g., Transformers) for sequential tasks. While many existing CL methods incrementally store additional learned structures, such as Low-Rank Adaptation (LoRA) adapters or prompts and sometimes even pr",
    "arxiv_url": "https://arxiv.org/abs/2504.08823v1",
    "pdf_url": "https://arxiv.org/pdf/2504.08823v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2504.08823",
    "arxiv_authors": [
      "Xiaobing Yu",
      "Jin Yang",
      "Xiao Wu",
      "Peijie Qiu",
      "Xiaofeng Liu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=FM-LoRA%3A+Factorized+Low-Rank+Meta-Prompting+for+Continual+Learning+Xiaobing+Yu+Jin+Yang+Xiao+Wu+Peijie+Qiu+Xiaofeng+Liu",
    "gs_search_success": true,
    "gs_authors": [
      "VighnTUAAAAJ",
      "7HLmlHMAAAAJ",
      "53LJfHAAAAAJ",
      "ymNbC1YAAAAJ"
    ],
    "citation_count": 4,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2505.24866",
    "title": "TalkingHeadBench: A Multi-Modal Benchmark & Analysis of Talking-Head DeepFake Detection",
    "year": 2025,
    "published": "2025-05-30T17:59:08Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "The rapid advancement of talking-head deepfake generation fueled by advanced generative models has elevated the realism of synthetic videos to a level that poses substantial risks in domains such as media, politics, and finance. However, current benchmarks for deepfake talking-head detection fail to reflect this progress, relying on outdated generators and offering limited insight into model robustness and generalization. We introduce TalkingHeadBench, a comprehensive multi-model multi-generator",
    "arxiv_url": "https://arxiv.org/abs/2505.24866v1",
    "pdf_url": "https://arxiv.org/pdf/2505.24866v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2505.24866",
    "arxiv_authors": [
      "Xinqi Xiong",
      "Prakrut Patel",
      "Qingyuan Fan",
      "Amisha Wadhwa",
      "Sarathy Selvam",
      "Xiao Guo",
      "Luchao Qi",
      "Xiaoming Liu",
      "Roni Sengupta"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=TalkingHeadBench%3A+A+Multi-Modal+Benchmark+%26+Analysis+of+Talking-Head+DeepFake+Detection+Xinqi+Xiong+Prakrut+Patel+Qingyuan+Fan+Amisha+Wadhwa+Sarathy+Selvam",
    "gs_search_success": true,
    "gs_authors": [
      "Id8SJl8AAAAJ",
      "H93xhggAAAAJ",
      "j1k9a_QAAAAJ",
      "xrArE5YAAAAJ",
      "kcJpqKgAAAAJ"
    ],
    "citation_count": 3,
    "gs_author_count": 9
  },
  {
    "arxiv_id": "2403.19589",
    "title": "TOD3Cap: Towards 3D Dense Captioning in Outdoor Scenes",
    "year": 2024,
    "published": "2024-03-28T17:12:55Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "3D dense captioning stands as a cornerstone in achieving a comprehensive understanding of 3D scenes through natural language. It has recently witnessed remarkable achievements, particularly in indoor settings. However, the exploration of 3D dense captioning in outdoor scenes is hindered by two major challenges: 1) the domain gap between indoor and outdoor scenes, such as dynamics and sparse visual inputs, makes it difficult to directly adapt existing indoor methods; 2) the lack of data with comp",
    "arxiv_url": "https://arxiv.org/abs/2403.19589v2",
    "pdf_url": "https://arxiv.org/pdf/2403.19589v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2403.19589",
    "arxiv_authors": [
      "Bu Jin",
      "Yupeng Zheng",
      "Pengfei Li",
      "Weize Li",
      "Yuhang Zheng",
      "Sujie Hu",
      "Xinyu Liu",
      "Jinwei Zhu",
      "Zhijie Yan",
      "Haiyang Sun",
      "Kun Zhan",
      "Peng Jia",
      "Xiaoxiao Long",
      "Yilun Chen",
      "Hao Zhao"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=TOD3Cap%3A+Towards+3D+Dense+Captioning+in+Outdoor+Scenes+Bu+Jin+Yupeng+Zheng+Pengfei+Li+Weize+Li+Yuhang+Zheng",
    "gs_search_success": true,
    "gs_authors": [
      "CyPiUucAAAAJ",
      "4PXGeaYAAAAJ",
      "8YJvY9cAAAAJ",
      "Z_QY_VwAAAAJ",
      "anGhGdYAAAAJ",
      "kgRjFN8AAAAJ",
      "Wn2Aic0AAAAJ",
      "W3G5kZEAAAAJ",
      "SYbFNsIAAAAJ",
      "uUd5v2cAAAAJ",
      "1J061HIAAAAJ"
    ],
    "citation_count": 30,
    "gs_author_count": 9
  },
  {
    "arxiv_id": "2501.13418",
    "title": "Rethinking the Sample Relations for Few-Shot Classification",
    "year": 2025,
    "published": "2025-01-23T06:45:17Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Feature quality is paramount for classification performance, particularly in few-shot scenarios. Contrastive learning, a widely adopted technique for enhancing feature quality, leverages sample relations to extract intrinsic features that capture semantic information and has achieved remarkable success in Few-Shot Learning (FSL). Nevertheless, current few-shot contrastive learning approaches often overlook the semantic similarity discrepancies at different granularities when employing the same m",
    "arxiv_url": "https://arxiv.org/abs/2501.13418v1",
    "pdf_url": "https://arxiv.org/pdf/2501.13418v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2501.13418",
    "arxiv_authors": [
      "Guowei Yin",
      "Sheng Huang",
      "Luwen Huangfu",
      "Yi Zhang",
      "Xiaohong Zhang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Rethinking+the+Sample+Relations+for+Few-Shot+Classification+Guowei+Yin+Sheng+Huang+Luwen+Huangfu+Yi+Zhang+Xiaohong+Zhang",
    "gs_search_success": true,
    "gs_authors": [
      "BMo9cZ4AAAAJ",
      "JL-pDFYAAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2502.10794",
    "title": "Distraction is All You Need for Multimodal Large Language Model Jailbreaking",
    "year": 2025,
    "published": "2025-02-15T13:25:12Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Multimodal Large Language Models (MLLMs) bridge the gap between visual and textual data, enabling a range of advanced applications. However, complex internal interactions among visual elements and their alignment with text can introduce vulnerabilities, which may be exploited to bypass safety mechanisms. To address this, we analyze the relationship between image content and task and find that the complexity of subimages, rather than their content, is key. Building on this insight, we propose the",
    "arxiv_url": "https://arxiv.org/abs/2502.10794v2",
    "pdf_url": "https://arxiv.org/pdf/2502.10794v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2502.10794",
    "arxiv_authors": [
      "Zuopeng Yang",
      "Jiluan Fan",
      "Anli Yan",
      "Erdun Gao",
      "Xin Lin",
      "Tao Li",
      "Kanghua Mo",
      "Changyu Dong"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Distraction+is+All+You+Need+for+Multimodal+Large+Language+Model+Jailbreaking+Zuopeng+Yang+Jiluan+Fan+Anli+Yan+Erdun+Gao+Xin+Lin",
    "gs_search_success": true,
    "gs_authors": [
      "UKR9sNQAAAAJ",
      "qbuqD10AAAAJ",
      "SilAzvwAAAAJ",
      "usKTO3wAAAAJ",
      "Mj2CqyUAAAAJ",
      "pyHD7UQAAAAJ",
      "qtMWgYsAAAAJ"
    ],
    "citation_count": 15,
    "gs_author_count": 8
  },
  {
    "arxiv_id": "2411.15355",
    "title": "UniGaussian: Driving Scene Reconstruction from Multiple Camera Models via Unified Gaussian Representations",
    "year": 2024,
    "published": "2024-11-22T21:59:46Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Urban scene reconstruction is crucial for real-world autonomous driving simulators. Although existing methods have achieved photorealistic reconstruction, they mostly focus on pinhole cameras and neglect fisheye cameras. In fact, how to effectively simulate fisheye cameras in driving scene remains an unsolved problem. In this work, we propose UniGaussian, a novel approach that learns a unified 3D Gaussian representation from multiple camera models for urban scene reconstruction in autonomous dri",
    "arxiv_url": "https://arxiv.org/abs/2411.15355v2",
    "pdf_url": "https://arxiv.org/pdf/2411.15355v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2411.15355",
    "arxiv_authors": [
      "Yuan Ren",
      "Guile Wu",
      "Runhao Li",
      "Zheyuan Yang",
      "Yibo Liu",
      "Xingxin Chen",
      "Tongtong Cao",
      "Bingbing Liu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=UniGaussian%3A+Driving+Scene+Reconstruction+from+Multiple+Camera+Models+via+Unified+Gaussian+Representations+Yuan+Ren+Guile+Wu+Runhao+Li+Zheyuan+Yang+Yibo+Liu",
    "gs_search_success": true,
    "gs_authors": [
      "P4Rp5uAAAAAJ",
      "zqDgDQ4AAAAJ",
      "-rCulKwAAAAJ",
      "ZtaFLE0AAAAJ",
      "SkzzXSYAAAAJ",
      "NdOL5w0AAAAJ"
    ],
    "citation_count": 8,
    "gs_author_count": 8
  },
  {
    "arxiv_id": "2408.09181",
    "title": "PADetBench: Towards Benchmarking Physical Attacks against Object Detection",
    "year": 2024,
    "published": "2024-08-17T12:11:22Z",
    "categories": [
      "cs.CV",
      "cs.CR",
      "cs.LG"
    ],
    "abstract": "Physical attacks against object detection have gained increasing attention due to their significant practical implications. However, conducting physical experiments is extremely time-consuming and labor-intensive. Moreover, physical dynamics and cross-domain transformation are challenging to strictly regulate in the real world, leading to unaligned evaluation and comparison, severely hindering the development of physically robust models. To accommodate these challenges, we explore utilizing real",
    "arxiv_url": "https://arxiv.org/abs/2408.09181v3",
    "pdf_url": "https://arxiv.org/pdf/2408.09181v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2408.09181",
    "arxiv_authors": [
      "Jiawei Lian",
      "Jianhong Pan",
      "Lefan Wang",
      "Yi Wang",
      "Lap-Pui Chau",
      "Shaohui Mei"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=PADetBench%3A+Towards+Benchmarking+Physical+Attacks+against+Object+Detection+Jiawei+Lian+Jianhong+Pan+Lefan+Wang+Yi+Wang+Lap-Pui+Chau",
    "gs_search_success": true,
    "gs_authors": [
      "rAPPIPQAAAAJ",
      "MYREIH0AAAAJ",
      "84agyF4AAAAJ",
      "MAG909MAAAAJ",
      "K_51OTcAAAAJ"
    ],
    "citation_count": 3,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2502.09779",
    "title": "Automated Muscle and Fat Segmentation in Computed Tomography for Comprehensive Body Composition Analysis",
    "year": 2025,
    "published": "2025-02-13T21:27:10Z",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "abstract": "Body composition assessment using CT images can potentially be used for a number of clinical applications, including the prognostication of cardiovascular outcomes, evaluation of metabolic health, monitoring of disease progression, assessment of nutritional status, prediction of treatment response in oncology, and risk stratification for surgical and critical care outcomes. While multiple groups have developed in-house segmentation tools for this analysis, there are very limited publicly availab",
    "arxiv_url": "https://arxiv.org/abs/2502.09779v4",
    "pdf_url": "https://arxiv.org/pdf/2502.09779v4",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2502.09779",
    "arxiv_authors": [
      "Yaqian Chen",
      "Hanxue Gu",
      "Yuwen Chen",
      "Jichen Yang",
      "Haoyu Dong",
      "Joseph Y. Cao",
      "Adrian Camarena",
      "Christopher Mantyh",
      "Roy Colglazier",
      "Maciej A. Mazurowski"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Automated+Muscle+and+Fat+Segmentation+in+Computed+Tomography+for+Comprehensive+Body+Composition+Analysis+Yaqian+Chen+Hanxue+Gu+Yuwen+Chen+Jichen+Yang+Haoyu+Dong",
    "gs_search_success": true,
    "gs_authors": [
      "eZVEUCIAAAAJ",
      "jGv3bRUAAAAJ",
      "aGjCpQUAAAAJ",
      "61s49p0AAAAJ",
      "iegKFuQAAAAJ"
    ],
    "citation_count": 2,
    "gs_author_count": 9
  },
  {
    "arxiv_id": "2410.02705",
    "title": "ControlAR: Controllable Image Generation with Autoregressive Models",
    "year": 2024,
    "published": "2024-10-03T17:28:07Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Autoregressive (AR) models have reformulated image generation as next-token prediction, demonstrating remarkable potential and emerging as strong competitors to diffusion models. However, control-to-image generation, akin to ControlNet, remains largely unexplored within AR models. Although a natural approach, inspired by advancements in Large Language Models, is to tokenize control images into tokens and prefill them into the autoregressive model before decoding image tokens, it still falls shor",
    "arxiv_url": "https://arxiv.org/abs/2410.02705v3",
    "pdf_url": "https://arxiv.org/pdf/2410.02705v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2410.02705",
    "arxiv_authors": [
      "Zongming Li",
      "Tianheng Cheng",
      "Shoufa Chen",
      "Peize Sun",
      "Haocheng Shen",
      "Longjin Ran",
      "Xiaoxin Chen",
      "Wenyu Liu",
      "Xinggang Wang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=ControlAR%3A+Controllable+Image+Generation+with+Autoregressive+Models+Zongming+Li+Tianheng+Cheng+Shoufa+Chen+Peize+Sun+Haocheng+Shen",
    "gs_search_success": true,
    "gs_authors": [
      "PH8rJHYAAAAJ",
      "AfC_R58AAAAJ",
      "s697AYIAAAAJ",
      "SmHr4W4AAAAJ",
      "ogoCvHEAAAAJ",
      "Grkp5AQAAAAJ",
      "qNCTLV0AAAAJ"
    ],
    "citation_count": 41,
    "gs_author_count": 9
  },
  {
    "arxiv_id": "2312.13277",
    "title": "Deep Learning on Object-centric 3D Neural Fields",
    "year": 2023,
    "published": "2023-12-20T18:56:45Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "In recent years, Neural Fields (NFs) have emerged as an effective tool for encoding diverse continuous signals such as images, videos, audio, and 3D shapes. When applied to 3D data, NFs offer a solution to the fragmentation and limitations associated with prevalent discrete representations. However, given that NFs are essentially neural networks, it remains unclear whether and how they can be seamlessly integrated into deep learning pipelines for solving downstream tasks. This paper addresses th",
    "arxiv_url": "https://arxiv.org/abs/2312.13277v2",
    "pdf_url": "https://arxiv.org/pdf/2312.13277v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2312.13277",
    "arxiv_authors": [
      "Pierluigi Zama Ramirez",
      "Luca De Luigi",
      "Daniele Sirocchi",
      "Adriano Cardace",
      "Riccardo Spezialetti",
      "Francesco Ballerini",
      "Samuele Salti",
      "Luigi Di Stefano"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Deep+Learning+on+Object-centric+3D+Neural+Fields+Pierluigi+Zama+Ramirez+Luca+De+Luigi+Daniele+Sirocchi+Adriano+Cardace+Riccardo+Spezialetti",
    "gs_search_success": true,
    "gs_authors": [
      "PpHLOpQAAAAJ",
      "OAYAmKYAAAAJ"
    ],
    "citation_count": 9,
    "gs_author_count": 8
  },
  {
    "arxiv_id": "2506.08020",
    "title": "Bi-level Unbalanced Optimal Transport for Partial Domain Adaptation",
    "year": 2025,
    "published": "2025-05-19T13:40:40Z",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "abstract": "Partial domain adaptation (PDA) problem requires aligning cross-domain samples while distinguishing the outlier classes for accurate knowledge transfer. The widely used weighting framework tries to address the outlier classes by introducing the reweighed source domain with a similar label distribution to the target domain. However, the empirical modeling of weights can only characterize the sample-wise relations, which leads to insufficient exploration of cluster structures, and the weights coul",
    "arxiv_url": "https://arxiv.org/abs/2506.08020v1",
    "pdf_url": "https://arxiv.org/pdf/2506.08020v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2506.08020",
    "arxiv_authors": [
      "Zi-Ying Chen",
      "Chuan-Xian Ren",
      "Hong Yan"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Bi-level+Unbalanced+Optimal+Transport+for+Partial+Domain+Adaptation+Zi-Ying+Chen+Chuan-Xian+Ren+Hong+Yan",
    "gs_search_success": true,
    "gs_authors": [
      "nWsPNkQAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2410.15472",
    "title": "Multi-Layer Feature Fusion with Cross-Channel Attention-Based U-Net for Kidney Tumor Segmentation",
    "year": 2024,
    "published": "2024-10-20T19:02:41Z",
    "categories": [
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "Renal tumors, especially renal cell carcinoma (RCC), show significant heterogeneity, posing challenges for diagnosis using radiology images such as MRI, echocardiograms, and CT scans. U-Net based deep learning techniques are emerging as a promising approach for automated medical image segmentation for minimally invasive diagnosis of renal tumors. However, current techniques need further improvements in accuracy to become clinically useful to radiologists. In this study, we present an improved U-",
    "arxiv_url": "https://arxiv.org/abs/2410.15472v2",
    "pdf_url": "https://arxiv.org/pdf/2410.15472v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2410.15472",
    "arxiv_authors": [
      "Fnu Neha",
      "Arvind K. Bansal"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Multi-Layer+Feature+Fusion+with+Cross-Channel+Attention-Based+U-Net+for+Kidney+Tumor+Segmentation+Fnu+Neha+Arvind+K.+Bansal",
    "gs_search_success": true,
    "gs_authors": [
      "Fkzk4oAAAAAJ"
    ],
    "citation_count": 8,
    "gs_author_count": 2
  },
  {
    "arxiv_id": "2504.14708",
    "title": "Time Frequency Analysis of EMG Signal for Gesture Recognition using Fine grained Features",
    "year": 2025,
    "published": "2025-04-20T18:51:10Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "abstract": "Electromyography (EMG) based hand gesture recognition converts forearm muscle activity into control commands for prosthetics, rehabilitation, and human computer interaction. This paper proposes a novel approach to EMG-based hand gesture recognition that uses fine-grained classification and presents XMANet, which unifies low-level local and high level semantic cues through cross layer mutual attention among shallow to deep CNN experts. Using stacked spectrograms and scalograms derived from the Sh",
    "arxiv_url": "https://arxiv.org/abs/2504.14708v1",
    "pdf_url": "https://arxiv.org/pdf/2504.14708v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2504.14708",
    "arxiv_authors": [
      "Parshuram N. Aarotale",
      "Ajita Rattani"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Time+Frequency+Analysis+of+EMG+Signal+for+Gesture+Recognition+using+Fine+grained+Features+Parshuram+N.+Aarotale+Ajita+Rattani",
    "gs_search_success": true,
    "gs_authors": [
      "9esyU2EAAAAJ",
      "rr7lycwAAAAJ"
    ],
    "citation_count": 2,
    "gs_author_count": 2
  },
  {
    "arxiv_id": "2411.13291",
    "title": "DATAP-SfM: Dynamic-Aware Tracking Any Point for Robust Structure from Motion in the Wild",
    "year": 2024,
    "published": "2024-11-20T13:01:16Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "This paper proposes a concise, elegant, and robust pipeline to estimate smooth camera trajectories and obtain dense point clouds for casual videos in the wild. Traditional frameworks, such as ParticleSfM~\\cite{zhao2022particlesfm}, address this problem by sequentially computing the optical flow between adjacent frames to obtain point trajectories. They then remove dynamic trajectories through motion segmentation and perform global bundle adjustment. However, the process of estimating optical flo",
    "arxiv_url": "https://arxiv.org/abs/2411.13291v1",
    "pdf_url": "https://arxiv.org/pdf/2411.13291v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2411.13291",
    "arxiv_authors": [
      "Weicai Ye",
      "Xinyu Chen",
      "Ruohao Zhan",
      "Di Huang",
      "Xiaoshui Huang",
      "Haoyi Zhu",
      "Hujun Bao",
      "Wanli Ouyang",
      "Tong He",
      "Guofeng Zhang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=DATAP-SfM%3A+Dynamic-Aware+Tracking+Any+Point+for+Robust+Structure+from+Motion+in+the+Wild+Weicai+Ye+Xinyu+Chen+Ruohao+Zhan+Di+Huang+Xiaoshui+Huang",
    "gs_search_success": true,
    "gs_authors": [
      "CTqD_VwAAAAJ",
      "pD1NOyUAAAAJ",
      "pw_0Z_UAAAAJ",
      "F0xfpXAAAAAJ"
    ],
    "citation_count": 2,
    "gs_author_count": 9
  },
  {
    "arxiv_id": "2412.17098",
    "title": "DreamOmni: Unified Image Generation and Editing",
    "year": 2024,
    "published": "2024-12-22T17:17:28Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Currently, the success of large language models (LLMs) illustrates that a unified multitasking approach can significantly enhance model usability, streamline deployment, and foster synergistic benefits across different tasks. However, in computer vision, while text-to-image (T2I) models have significantly improved generation quality through scaling up, their framework design did not initially consider how to unify with downstream tasks, such as various types of editing. To address this, we intro",
    "arxiv_url": "https://arxiv.org/abs/2412.17098v2",
    "pdf_url": "https://arxiv.org/pdf/2412.17098v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2412.17098",
    "arxiv_authors": [
      "Bin Xia",
      "Yuechen Zhang",
      "Jingyao Li",
      "Chengyao Wang",
      "Yitong Wang",
      "Xinglong Wu",
      "Bei Yu",
      "Jiaya Jia"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=DreamOmni%3A+Unified+Image+Generation+and+Editing+Bin+Xia+Yuechen+Zhang+Jingyao+Li+Chengyao+Wang+Yitong+Wang",
    "gs_search_success": true,
    "gs_authors": [
      "rh2fID8AAAAJ",
      "XPAkzTEAAAAJ",
      "mqrKmvcAAAAJ",
      "NfFTKfYAAAAJ",
      "tGneTm4AAAAJ",
      "1pZcoqgAAAAJ",
      "8OijNgkAAAAJ",
      "LVsp9RQAAAAJ"
    ],
    "citation_count": 11,
    "gs_author_count": 8
  },
  {
    "arxiv_id": "2401.06506",
    "title": "Frequency Masking for Universal Deepfake Detection",
    "year": 2024,
    "published": "2024-01-12T11:02:12Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "We study universal deepfake detection. Our goal is to detect synthetic images from a range of generative AI approaches, particularly from emerging ones which are unseen during training of the deepfake detector. Universal deepfake detection requires outstanding generalization capability. Motivated by recently proposed masked image modeling which has demonstrated excellent generalization in self-supervised pre-training, we make the first attempt to explore masked image modeling for universal deepf",
    "arxiv_url": "https://arxiv.org/abs/2401.06506v3",
    "pdf_url": "https://arxiv.org/pdf/2401.06506v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2401.06506",
    "arxiv_authors": [
      "Chandler Timm Doloriel",
      "Ngai-Man Cheung"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Frequency+Masking+for+Universal+Deepfake+Detection+Chandler+Timm+Doloriel+Ngai-Man+Cheung",
    "gs_search_success": true,
    "gs_authors": [
      "vJQArFkAAAAJ",
      "9og9RgcAAAAJ"
    ],
    "citation_count": 46,
    "gs_author_count": 2
  },
  {
    "arxiv_id": "2411.04844",
    "title": "Discretized Gaussian Representation for Tomographic Reconstruction",
    "year": 2024,
    "published": "2024-11-07T16:32:29Z",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "abstract": "Computed Tomography (CT) enables detailed cross-sectional imaging but continues to face challenges in balancing reconstruction quality and computational efficiency. While deep learning-based methods have significantly improved image quality and noise reduction, they typically require large-scale training data and intensive computation. Recent advances in scene reconstruction, such as Neural Radiance Fields and 3D Gaussian Splatting, offer alternative perspectives but are not well-suited for dire",
    "arxiv_url": "https://arxiv.org/abs/2411.04844v4",
    "pdf_url": "https://arxiv.org/pdf/2411.04844v4",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2411.04844",
    "arxiv_authors": [
      "Shaokai Wu",
      "Yuxiang Lu",
      "Yapan Guo",
      "Wei Ji",
      "Suizhi Huang",
      "Fengyu Yang",
      "Shalayiding Sirejiding",
      "Qichen He",
      "Jing Tong",
      "Yanbiao Ji",
      "Yue Ding",
      "Hongtao Lu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Discretized+Gaussian+Representation+for+Tomographic+Reconstruction+Shaokai+Wu+Yuxiang+Lu+Yapan+Guo+Wei+Ji+Suizhi+Huang",
    "gs_search_success": true,
    "gs_authors": [
      "wwNa3wMAAAAJ",
      "mna2LlkAAAAJ",
      "uqBvbN8AAAAJ",
      "kisWeuMAAAAJ",
      "GtNuBJcAAAAJ",
      "F0oFN4gAAAAJ",
      "7m-TOp8AAAAJ",
      "Bg9FHewAAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 9
  },
  {
    "arxiv_id": "2502.20668",
    "title": "OpenEarthSensing: Large-Scale Fine-Grained Benchmark for Open-World Remote Sensing",
    "year": 2025,
    "published": "2025-02-28T02:49:52Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "eess.IV"
    ],
    "abstract": "The advancement of remote sensing, including satellite systems, facilitates the continuous acquisition of remote sensing imagery globally, introducing novel challenges for achieving open-world tasks. Deployed models need to continuously adjust to a constant influx of new data, which frequently exhibits diverse shifts from the data encountered during the training phase. To effectively handle the new data, models are required to detect semantic shifts, adapt to covariate shifts, and continuously u",
    "arxiv_url": "https://arxiv.org/abs/2502.20668v2",
    "pdf_url": "https://arxiv.org/pdf/2502.20668v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2502.20668",
    "arxiv_authors": [
      "Xiang Xiang",
      "Zhuo Xu",
      "Yao Deng",
      "Qinhao Zhou",
      "Yifan Liang",
      "Ke Chen",
      "Qingfang Zheng",
      "Yaowei Wang",
      "Xilin Chen",
      "Wen Gao"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=OpenEarthSensing%3A+Large-Scale+Fine-Grained+Benchmark+for+Open-World+Remote+Sensing+Xiang+Xiang+Zhuo+Xu+Yao+Deng+Qinhao+Zhou+Yifan+Liang",
    "gs_search_success": true,
    "gs_authors": [
      "-D5k5ioAAAAJ",
      "bfjhJbcAAAAJ",
      "Ue98P8IAAAAJ",
      "o_DllmIAAAAJ"
    ],
    "citation_count": 2,
    "gs_author_count": 9
  },
  {
    "arxiv_id": "2406.16886",
    "title": "Sensor Data Augmentation from Skeleton Pose Sequences for Improving Human Activity Recognition",
    "year": 2024,
    "published": "2024-04-25T10:13:18Z",
    "categories": [
      "eess.SP",
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "The proliferation of deep learning has significantly advanced various fields, yet Human Activity Recognition (HAR) has not fully capitalized on these developments, primarily due to the scarcity of labeled datasets. Despite the integration of advanced Inertial Measurement Units (IMUs) in ubiquitous wearable devices like smartwatches and fitness trackers, which offer self-labeled activity data from users, the volume of labeled data remains insufficient compared to domains where deep learning has a",
    "arxiv_url": "https://arxiv.org/abs/2406.16886v1",
    "pdf_url": "https://arxiv.org/pdf/2406.16886v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2406.16886",
    "arxiv_authors": [
      "Parham Zolfaghari",
      "Vitor Fortes Rey",
      "Lala Ray",
      "Hyun Kim",
      "Sungho Suh",
      "Paul Lukowicz"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Sensor+Data+Augmentation+from+Skeleton+Pose+Sequences+for+Improving+Human+Activity+Recognition+Parham+Zolfaghari+Vitor+Fortes+Rey+Lala+Ray+Hyun+Kim+Sungho+Suh",
    "gs_search_success": true,
    "gs_authors": [
      "mCuJP1UAAAAJ",
      "DMjDOYwAAAAJ",
      "rjJwgO0AAAAJ",
      "1FIyc9kAAAAJ",
      "r7QYkIgAAAAJ"
    ],
    "citation_count": 14,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2312.07899",
    "title": "Morphological Profiling for Drug Discovery in the Era of Deep Learning",
    "year": 2023,
    "published": "2023-12-13T05:08:32Z",
    "categories": [
      "q-bio.QM",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "Morphological profiling is a valuable tool in phenotypic drug discovery. The advent of high-throughput automated imaging has enabled the capturing of a wide range of morphological features of cells or organisms in response to perturbations at the single-cell resolution. Concurrently, significant advances in machine learning and deep learning, especially in computer vision, have led to substantial improvements in analyzing large-scale high-content images at high-throughput. These efforts have fac",
    "arxiv_url": "https://arxiv.org/abs/2312.07899v2",
    "pdf_url": "https://arxiv.org/pdf/2312.07899v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2312.07899",
    "arxiv_authors": [
      "Qiaosi Tang",
      "Ranjala Ratnayake",
      "Gustavo Seabra",
      "Zhe Jiang",
      "Ruogu Fang",
      "Lina Cui",
      "Yousong Ding",
      "Tamer Kahveci",
      "Jiang Bian",
      "Chenglong Li",
      "Hendrik Luesch",
      "Yanjun Li"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Morphological+Profiling+for+Drug+Discovery+in+the+Era+of+Deep+Learning+Qiaosi+Tang+Ranjala+Ratnayake+Gustavo+Seabra+Zhe+Jiang+Ruogu+Fang",
    "gs_search_success": true,
    "gs_authors": [
      "x_B0-1gAAAAJ",
      "R7xPuT8AAAAJ",
      "vP1Q2wkAAAAJ",
      "wol4m1IAAAAJ"
    ],
    "citation_count": 30,
    "gs_author_count": 9
  },
  {
    "arxiv_id": "2302.03027",
    "title": "Zero-shot Image-to-Image Translation",
    "year": 2023,
    "published": "2023-02-06T18:59:51Z",
    "categories": [
      "cs.CV",
      "cs.GR",
      "cs.LG"
    ],
    "abstract": "Large-scale text-to-image generative models have shown their remarkable ability to synthesize diverse and high-quality images. However, it is still challenging to directly apply these models for editing real images for two reasons. First, it is hard for users to come up with a perfect text prompt that accurately describes every visual detail in the input image. Second, while existing models can introduce desirable changes in certain regions, they often dramatically alter the input content and in",
    "arxiv_url": "https://arxiv.org/abs/2302.03027v1",
    "pdf_url": "https://arxiv.org/pdf/2302.03027v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2302.03027",
    "arxiv_authors": [
      "Gaurav Parmar",
      "Krishna Kumar Singh",
      "Richard Zhang",
      "Yijun Li",
      "Jingwan Lu",
      "Jun-Yan Zhu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Zero-shot+Image-to-Image+Translation+Gaurav+Parmar+Krishna+Kumar+Singh+Richard+Zhang+Yijun+Li+Jingwan+Lu",
    "gs_search_success": true,
    "gs_authors": [
      "nrsWSt4AAAAJ",
      "UdpacsMAAAAJ",
      "xDJzuz8AAAAJ",
      "LW8ze_UAAAAJ",
      "3TMipekAAAAJ"
    ],
    "citation_count": 638,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2405.05518",
    "title": "DTCLMapper: Dual Temporal Consistent Learning for Vectorized HD Map Construction",
    "year": 2024,
    "published": "2024-05-09T02:58:55Z",
    "categories": [
      "cs.CV",
      "cs.RO",
      "eess.IV"
    ],
    "abstract": "Temporal information plays a pivotal role in Bird's-Eye-View (BEV) driving scene understanding, which can alleviate the visual information sparsity. However, the indiscriminate temporal fusion method will cause the barrier of feature redundancy when constructing vectorized High-Definition (HD) maps. In this paper, we revisit the temporal fusion of vectorized HD maps, focusing on temporal instance consistency and temporal map consistency learning. To improve the representation of instances in sin",
    "arxiv_url": "https://arxiv.org/abs/2405.05518v2",
    "pdf_url": "https://arxiv.org/pdf/2405.05518v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2405.05518",
    "arxiv_authors": [
      "Siyu Li",
      "Jiacheng Lin",
      "Hao Shi",
      "Jiaming Zhang",
      "Song Wang",
      "You Yao",
      "Zhiyong Li",
      "Kailun Yang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=DTCLMapper%3A+Dual+Temporal+Consistent+Learning+for+Vectorized+HD+Map+Construction+Siyu+Li+Jiacheng+Lin+Hao+Shi+Jiaming+Zhang+Song+Wang",
    "gs_search_success": true,
    "gs_authors": [
      "rFiGjVEAAAAJ",
      "Jj0jbL8AAAAJ",
      "WFa7fUIAAAAJ",
      "pKFqWhgAAAAJ",
      "gbuEGBQAAAAJ",
      "0EI9msQAAAAJ"
    ],
    "citation_count": 19,
    "gs_author_count": 8
  },
  {
    "arxiv_id": "2411.03831",
    "title": "An Enhancement of Haar Cascade Algorithm Applied to Face Recognition for Gate Pass Security",
    "year": 2024,
    "published": "2024-11-06T11:03:34Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "This study is focused on enhancing the Haar Cascade Algorithm to decrease the false positive and false negative rate in face matching and face detection to increase the accuracy rate even under challenging conditions. The face recognition library was implemented with Haar Cascade Algorithm in which the 128-dimensional vectors representing the unique features of a face are encoded. A subprocess was applied where the grayscale image from Haar Cascade was converted to RGB to improve the face encodi",
    "arxiv_url": "https://arxiv.org/abs/2411.03831v1",
    "pdf_url": "https://arxiv.org/pdf/2411.03831v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2411.03831",
    "arxiv_authors": [
      "Clarence A. Antipona",
      "Romeo R. Magsino",
      "Raymund M. Dioses",
      "Khatalyn E. Mata"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=An+Enhancement+of+Haar+Cascade+Algorithm+Applied+to+Face+Recognition+for+Gate+Pass+Security+Clarence+A.+Antipona+Romeo+R.+Magsino+Raymund+M.+Dioses+Khatalyn+E.+Mata",
    "gs_search_success": true,
    "gs_authors": [
      "pCDOXmYAAAAJ",
      "Xf11dEcAAAAJ"
    ],
    "citation_count": 2,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2503.17914",
    "title": "Semi-supervised Semantic Segmentation with Multi-Constraint Consistency Learning",
    "year": 2025,
    "published": "2025-03-23T03:21:33Z",
    "categories": [
      "cs.MM",
      "cs.CV"
    ],
    "abstract": "Consistency regularization has prevailed in semi-supervised semantic segmentation and achieved promising performance. However, existing methods typically concentrate on enhancing the Image-augmentation based Prediction consistency and optimizing the segmentation network as a whole, resulting in insufficient utilization of potential supervisory information. In this paper, we propose a Multi-Constraint Consistency Learning (MCCL) approach to facilitate the staged enhancement of the encoder and dec",
    "arxiv_url": "https://arxiv.org/abs/2503.17914v1",
    "pdf_url": "https://arxiv.org/pdf/2503.17914v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2503.17914",
    "arxiv_authors": [
      "Jianjian Yin",
      "Tao Chen",
      "Gensheng Pei",
      "Yazhou Yao",
      "Liqiang Nie",
      "Xiansheng Hua"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Semi-supervised+Semantic+Segmentation+with+Multi-Constraint+Consistency+Learning+Jianjian+Yin+Tao+Chen+Gensheng+Pei+Yazhou+Yao+Liqiang+Nie",
    "gs_search_success": true,
    "gs_authors": [
      "6G-l4o0AAAAJ",
      "9RvG9F0AAAAJ",
      "yywVMhUAAAAJ",
      "_3Ucwv4AAAAJ"
    ],
    "citation_count": 5,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2306.17367",
    "title": "Spatially Varying Exposure with 2-by-2 Multiplexing: Optimality and Universality",
    "year": 2023,
    "published": "2023-06-30T02:08:25Z",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "abstract": "The advancement of new digital image sensors has enabled the design of exposure multiplexing schemes where a single image capture can have multiple exposures and conversion gains in an interlaced format, similar to that of a Bayer color filter array. In this paper, we ask the question of how to design such multiplexing schemes for adaptive high-dynamic range (HDR) imaging where the multiplexing scheme can be updated according to the scenes. We present two new findings.   (i) We address the probl",
    "arxiv_url": "https://arxiv.org/abs/2306.17367v1",
    "pdf_url": "https://arxiv.org/pdf/2306.17367v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2306.17367",
    "arxiv_authors": [
      "Xiangyu Qu",
      "Yiheng Chi",
      "Stanley H. Chan"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Spatially+Varying+Exposure+with+2-by-2+Multiplexing%3A+Optimality+and+Universality+Xiangyu+Qu+Yiheng+Chi+Stanley+H.+Chan",
    "gs_search_success": true,
    "gs_authors": [
      "WZYUtwQAAAAJ",
      "ZlNq4_4AAAAJ"
    ],
    "citation_count": 14,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2312.04152",
    "title": "EulerMormer: Robust Eulerian Motion Magnification via Dynamic Filtering within Transformer",
    "year": 2023,
    "published": "2023-12-07T09:10:16Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Video Motion Magnification (VMM) aims to break the resolution limit of human visual perception capability and reveal the imperceptible minor motion that contains valuable information in the macroscopic domain. However, challenges arise in this task due to photon noise inevitably introduced by photographic devices and spatial inconsistency in amplification, leading to flickering artifacts in static fields and motion blur and distortion in dynamic fields in the video. Existing methods focus on exp",
    "arxiv_url": "https://arxiv.org/abs/2312.04152v1",
    "pdf_url": "https://arxiv.org/pdf/2312.04152v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2312.04152",
    "arxiv_authors": [
      "Fei Wang",
      "Dan Guo",
      "Kun Li",
      "Meng Wang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=EulerMormer%3A+Robust+Eulerian+Motion+Magnification+via+Dynamic+Filtering+within+Transformer+Fei+Wang+Dan+Guo+Kun+Li+Meng+Wang",
    "gs_search_success": true,
    "gs_authors": [
      "rHagaaIAAAAJ",
      "DsEONuMAAAAJ"
    ],
    "citation_count": 45,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2405.05803",
    "title": "Boosting Multimodal Large Language Models with Visual Tokens Withdrawal for Rapid Inference",
    "year": 2024,
    "published": "2024-05-09T14:38:53Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Multimodal large language models (MLLMs) demand considerable computations for inference due to the extensive parameters and the additional input tokens needed for visual information representation. Herein, we introduce Visual Tokens Withdrawal (VTW), a plug-and-play module to boost MLLMs for rapid inference. Our approach is inspired by two intriguing phenomena we have observed: (1) the attention sink phenomenon that is prevalent in LLMs also persists in MLLMs, suggesting that initial tokens and ",
    "arxiv_url": "https://arxiv.org/abs/2405.05803v3",
    "pdf_url": "https://arxiv.org/pdf/2405.05803v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2405.05803",
    "arxiv_authors": [
      "Zhihang Lin",
      "Mingbao Lin",
      "Luxi Lin",
      "Rongrong Ji"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Boosting+Multimodal+Large+Language+Models+with+Visual+Tokens+Withdrawal+for+Rapid+Inference+Zhihang+Lin+Mingbao+Lin+Luxi+Lin+Rongrong+Ji",
    "gs_search_success": true,
    "gs_authors": [
      "UpqNGLYAAAAJ",
      "eaVMoiUAAAAJ",
      "lRSD7PQAAAAJ",
      "Dp3L1bsAAAAJ"
    ],
    "citation_count": 88,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2312.09935",
    "title": "LogoStyleFool: Vitiating Video Recognition Systems via Logo Style Transfer",
    "year": 2023,
    "published": "2023-12-15T16:44:38Z",
    "categories": [
      "cs.CV",
      "cs.CR"
    ],
    "abstract": "Video recognition systems are vulnerable to adversarial examples. Recent studies show that style transfer-based and patch-based unrestricted perturbations can effectively improve attack efficiency. These attacks, however, face two main challenges: 1) Adding large stylized perturbations to all pixels reduces the naturalness of the video and such perturbations can be easily detected. 2) Patch-based video attacks are not extensible to targeted attacks due to the limited search space of reinforcemen",
    "arxiv_url": "https://arxiv.org/abs/2312.09935v2",
    "pdf_url": "https://arxiv.org/pdf/2312.09935v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2312.09935",
    "arxiv_authors": [
      "Yuxin Cao",
      "Ziyu Zhao",
      "Xi Xiao",
      "Derui Wang",
      "Minhui Xue",
      "Jin Lu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=LogoStyleFool%3A+Vitiating+Video+Recognition+Systems+via+Logo+Style+Transfer+Yuxin+Cao+Ziyu+Zhao+Xi+Xiao+Derui+Wang+Minhui+Xue",
    "gs_search_success": true,
    "gs_authors": [
      "lNQ73QsAAAAJ",
      "uAbiaaUAAAAJ",
      "jdqkm1IAAAAJ",
      "GCYqueEAAAAJ"
    ],
    "citation_count": 5,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2501.18851",
    "title": "Project-and-Fuse: Improving RGB-D Semantic Segmentation via Graph Convolution Networks",
    "year": 2025,
    "published": "2025-01-31T02:24:13Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Most existing RGB-D semantic segmentation methods focus on the feature level fusion, including complex cross-modality and cross-scale fusion modules. However, these methods may cause misalignment problem in the feature fusion process and counter-intuitive patches in the segmentation results. Inspired by the popular pixel-node-pixel pipeline, we propose to 1) fuse features from two modalities in a late fusion style, during which the geometric feature injection is guided by texture feature prior; ",
    "arxiv_url": "https://arxiv.org/abs/2501.18851v3",
    "pdf_url": "https://arxiv.org/pdf/2501.18851v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2501.18851",
    "arxiv_authors": [
      "Xiaoyan Jiang",
      "Bohan Wang",
      "Xinlong Wan",
      "Shanshan Chen",
      "Hamido Fujita",
      "Hanan Abd. Al Juaid"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Project-and-Fuse%3A+Improving+RGB-D+Semantic+Segmentation+via+Graph+Convolution+Networks+Xiaoyan+Jiang+Bohan+Wang+Xinlong+Wan+Shanshan+Chen+Hamido+Fujita",
    "gs_search_success": true,
    "gs_authors": [
      "pKnD8WcAAAAJ",
      "R06HiLwAAAAJ",
      "MxzV1nQAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2312.02973",
    "title": "GauHuman: Articulated Gaussian Splatting from Monocular Human Videos",
    "year": 2023,
    "published": "2023-12-05T18:59:14Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "We present, GauHuman, a 3D human model with Gaussian Splatting for both fast training (1 ~ 2 minutes) and real-time rendering (up to 189 FPS), compared with existing NeRF-based implicit representation modelling frameworks demanding hours of training and seconds of rendering per frame. Specifically, GauHuman encodes Gaussian Splatting in the canonical space and transforms 3D Gaussians from canonical space to posed space with linear blend skinning (LBS), in which effective pose and LBS refinement ",
    "arxiv_url": "https://arxiv.org/abs/2312.02973v1",
    "pdf_url": "https://arxiv.org/pdf/2312.02973v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2312.02973",
    "arxiv_authors": [
      "Shoukang Hu",
      "Ziwei Liu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=GauHuman%3A+Articulated+Gaussian+Splatting+from+Monocular+Human+Videos+Shoukang+Hu+Ziwei+Liu",
    "gs_search_success": true,
    "gs_authors": [
      "lc45xlcAAAAJ",
      "9cUPotAAAAAJ",
      "lKB_cJAAAAAJ"
    ],
    "citation_count": 179,
    "gs_author_count": 2
  },
  {
    "arxiv_id": "2408.10488",
    "title": "Event Stream-based Sign Language Translation: A High-Definition Benchmark Dataset and A Novel Baseline",
    "year": 2024,
    "published": "2024-08-20T02:01:30Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.NE"
    ],
    "abstract": "Sign Language Translation (SLT) is a core task in the field of AI-assisted disability. Traditional SLT methods are typically based on visible light videos, which are easily affected by factors such as lighting variations, rapid hand movements, and privacy concerns. This paper proposes the use of bio-inspired event cameras to alleviate the aforementioned issues. Specifically, we introduce a new high-definition event-based sign language dataset, termed Event-CSL, which effectively addresses the da",
    "arxiv_url": "https://arxiv.org/abs/2408.10488v2",
    "pdf_url": "https://arxiv.org/pdf/2408.10488v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2408.10488",
    "arxiv_authors": [
      "Shiao Wang",
      "Xiao Wang",
      "Duoqing Yang",
      "Yao Rong",
      "Fuling Wang",
      "Jianing Li",
      "Lin Zhu",
      "Bo Jiang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Event+Stream-based+Sign+Language+Translation%3A+A+High-Definition+Benchmark+Dataset+and+A+Novel+Baseline+Shiao+Wang+Xiao+Wang+Duoqing+Yang+Yao+Rong+Fuling+Wang",
    "gs_search_success": true,
    "gs_authors": [
      "xrYnfwcAAAAJ",
      "32d6xfEAAAAJ",
      "5MO9ojoAAAAJ",
      "oq9awGMAAAAJ"
    ],
    "citation_count": 3,
    "gs_author_count": 8
  },
  {
    "arxiv_id": "2304.10410",
    "title": "Radar-Camera Fusion for Object Detection and Semantic Segmentation in Autonomous Driving: A Comprehensive Review",
    "year": 2023,
    "published": "2023-04-20T15:48:50Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.RO"
    ],
    "abstract": "Driven by deep learning techniques, perception technology in autonomous driving has developed rapidly in recent years, enabling vehicles to accurately detect and interpret surrounding environment for safe and efficient navigation. To achieve accurate and robust perception capabilities, autonomous vehicles are often equipped with multiple sensors, making sensor fusion a crucial part of the perception system. Among these fused sensors, radars and cameras enable a complementary and cost-effective p",
    "arxiv_url": "https://arxiv.org/abs/2304.10410v2",
    "pdf_url": "https://arxiv.org/pdf/2304.10410v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2304.10410",
    "arxiv_authors": [
      "Shanliang Yao",
      "Runwei Guan",
      "Xiaoyu Huang",
      "Zhuoxiao Li",
      "Xiangyu Sha",
      "Yong Yue",
      "Eng Gee Lim",
      "Hyungjoon Seo",
      "Ka Lok Man",
      "Xiaohui Zhu",
      "Yutao Yue"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Radar-Camera+Fusion+for+Object+Detection+and+Semantic+Segmentation+in+Autonomous+Driving%3A+A+Comprehensive+Review+Shanliang+Yao+Runwei+Guan+Xiaoyu+Huang+Zhuoxiao+Li+Xiangyu+Sha",
    "gs_search_success": true,
    "gs_authors": [
      "HXU7M0kAAAAJ",
      "SHLZ-cYAAAAJ",
      "YmT29FUAAAAJ",
      "zHw8eegAAAAJ",
      "9FOIHmYAAAAJ",
      "Pa_xqn8AAAAJ",
      "Ug_UiIoAAAAJ",
      "Fjo72tUAAAAJ"
    ],
    "citation_count": 217,
    "gs_author_count": 9
  },
  {
    "arxiv_id": "2504.10738",
    "title": "CleanMAP: Distilling Multimodal LLMs for Confidence-Driven Crowdsourced HD Map Updates",
    "year": 2025,
    "published": "2025-04-14T22:16:10Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "cs.RO"
    ],
    "abstract": "The rapid growth of intelligent connected vehicles (ICVs) and integrated vehicle-road-cloud systems has increased the demand for accurate, real-time HD map updates. However, ensuring map reliability remains challenging due to inconsistencies in crowdsourced data, which suffer from motion blur, lighting variations, adverse weather, and lane marking degradation. This paper introduces CleanMAP, a Multimodal Large Language Model (MLLM)-based distillation framework designed to filter and refine crowd",
    "arxiv_url": "https://arxiv.org/abs/2504.10738v1",
    "pdf_url": "https://arxiv.org/pdf/2504.10738v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2504.10738",
    "arxiv_authors": [
      "Ankit Kumar Shaw",
      "Kun Jiang",
      "Tuopu Wen",
      "Chandan Kumar Sah",
      "Yining Shi",
      "Mengmeng Yang",
      "Diange Yang",
      "Xiaoli Lian"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=CleanMAP%3A+Distilling+Multimodal+LLMs+for+Confidence-Driven+Crowdsourced+HD+Map+Updates+Ankit+Kumar+Shaw+Kun+Jiang+Tuopu+Wen+Chandan+Kumar+Sah+Yining+Shi",
    "gs_search_success": true,
    "gs_authors": [
      "3qK1cZMAAAAJ",
      "MbSyLggAAAAJ",
      "lbtJvIMAAAAJ",
      "-rhAnRQAAAAJ",
      "2zOwf-QAAAAJ",
      "PyH-zqygdCYC"
    ],
    "citation_count": 2,
    "gs_author_count": 8
  },
  {
    "arxiv_id": "2503.02511",
    "title": "TeTRA-VPR: A Ternary Transformer Approach for Compact Visual Place Recognition",
    "year": 2025,
    "published": "2025-03-04T11:20:10Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Visual Place Recognition (VPR) localizes a query image by matching it against a database of geo-tagged reference images, making it essential for navigation and mapping in robotics. Although Vision Transformer (ViT) solutions deliver high accuracy, their large models often exceed the memory and compute budgets of resource-constrained platforms such as drones and mobile robots. To address this issue, we propose TeTRA, a ternary transformer approach that progressively quantizes the ViT backbone to ",
    "arxiv_url": "https://arxiv.org/abs/2503.02511v1",
    "pdf_url": "https://arxiv.org/pdf/2503.02511v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2503.02511",
    "arxiv_authors": [
      "Oliver Grainge",
      "Michael Milford",
      "Indu Bodala",
      "Sarvapali D. Ramchurn",
      "Shoaib Ehsan"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=TeTRA-VPR%3A+A+Ternary+Transformer+Approach+for+Compact+Visual+Place+Recognition+Oliver+Grainge+Michael+Milford+Indu+Bodala+Sarvapali+D.+Ramchurn+Shoaib+Ehsan",
    "gs_search_success": true,
    "gs_authors": [
      "tB26zqYAAAAJ",
      "40KlWugAAAAJ",
      "TDSmCKgAAAAJ",
      "PVZnzdsAAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2501.01184",
    "title": "Vulnerability-Aware Spatio-Temporal Learning for Generalizable Deepfake Video Detection",
    "year": 2025,
    "published": "2025-01-02T10:21:34Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Detecting deepfake videos is highly challenging given the complexity of characterizing spatio-temporal artifacts. Most existing methods rely on binary classifiers trained using real and fake image sequences, therefore hindering their generalization capabilities to unseen generation methods. Moreover, with the constant progress in generative Artificial Intelligence (AI), deepfake artifacts are becoming imperceptible at both the spatial and the temporal levels, making them extremely difficult to c",
    "arxiv_url": "https://arxiv.org/abs/2501.01184v3",
    "pdf_url": "https://arxiv.org/pdf/2501.01184v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2501.01184",
    "arxiv_authors": [
      "Dat Nguyen",
      "Marcella Astrid",
      "Anis Kacem",
      "Enjie Ghorbel",
      "Djamila Aouada"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Vulnerability-Aware+Spatio-Temporal+Learning+for+Generalizable+Deepfake+Video+Detection+Dat+Nguyen+Marcella+Astrid+Anis+Kacem+Enjie+Ghorbel+Djamila+Aouada",
    "gs_search_success": true,
    "gs_authors": [
      "kXNJXMAAAAAJ",
      "K3EWusMAAAAJ",
      "WBmJVSkAAAAJ",
      "TXBGJhgAAAAJ",
      "gQjuVcEAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2409.09588",
    "title": "GLCONet: Learning Multi-source Perception Representation for Camouflaged Object Detection",
    "year": 2024,
    "published": "2024-09-15T02:26:17Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Recently, biological perception has been a powerful tool for handling the camouflaged object detection (COD) task. However, most existing methods are heavily dependent on the local spatial information of diverse scales from convolutional operations to optimize initial features. A commonly neglected point in these methods is the long-range dependencies between feature pixels from different scale spaces that can help the model build a global structure of the object, inducing a more precise image r",
    "arxiv_url": "https://arxiv.org/abs/2409.09588v1",
    "pdf_url": "https://arxiv.org/pdf/2409.09588v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2409.09588",
    "arxiv_authors": [
      "Yanguang Sun",
      "Hanyu Xuan",
      "Jian Yang",
      "Lei Luo"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=GLCONet%3A+Learning+Multi-source+Perception+Representation+for+Camouflaged+Object+Detection+Yanguang+Sun+Hanyu+Xuan+Jian+Yang+Lei+Luo",
    "gs_search_success": true,
    "gs_authors": [
      "KpSTy3sAAAAJ",
      "i3b5t0YAAAAJ",
      "6CIDtZQAAAAJ"
    ],
    "citation_count": 21,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2312.14201",
    "title": "Towards Better Visualizing the Decision Basis of Networks via Unfold and Conquer Attribution Guidance",
    "year": 2023,
    "published": "2023-12-21T03:43:19Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Revealing the transparency of Deep Neural Networks (DNNs) has been widely studied to describe the decision mechanisms of network inner structures. In this paper, we propose a novel post-hoc framework, Unfold and Conquer Attribution Guidance (UCAG), which enhances the explainability of the network decision by spatially scrutinizing the input features with respect to the model confidence. Addressing the phenomenon of missing detailed descriptions, UCAG sequentially complies with the confidence of ",
    "arxiv_url": "https://arxiv.org/abs/2312.14201v2",
    "pdf_url": "https://arxiv.org/pdf/2312.14201v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2312.14201",
    "arxiv_authors": [
      "Jung-Ho Hong",
      "Woo-Jeoung Nam",
      "Kyu-Sung Jeon",
      "Seong-Whan Lee"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Towards+Better+Visualizing+the+Decision+Basis+of+Networks+via+Unfold+and+Conquer+Attribution+Guidance+Jung-Ho+Hong+Woo-Jeoung+Nam+Kyu-Sung+Jeon+Seong-Whan+Lee",
    "gs_search_success": true,
    "gs_authors": [
      "9wo5l0QAAAAJ",
      "1xDz9MkAAAAJ"
    ],
    "citation_count": 4,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2402.17767",
    "title": "Opening Articulated Structures in the Real World",
    "year": 2024,
    "published": "2024-02-27T18:58:54Z",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "What does it take to build mobile manipulation systems that can competently operate on previously unseen objects in previously unseen environments? This work answers this question using opening of articulated structures as a mobile manipulation testbed. Specifically, our focus is on the end-to-end performance on this task without any privileged information, i.e. the robot starts at a location with the novel target articulated object in view, and has to approach the object and successfully open i",
    "arxiv_url": "https://arxiv.org/abs/2402.17767v3",
    "pdf_url": "https://arxiv.org/pdf/2402.17767v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2402.17767",
    "arxiv_authors": [
      "Arjun Gupta",
      "Michelle Zhang",
      "Rishik Sathua",
      "Saurabh Gupta"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Opening+Articulated+Structures+in+the+Real+World+Arjun+Gupta+Michelle+Zhang+Rishik+Sathua+Saurabh+Gupta",
    "gs_search_success": true,
    "gs_authors": [
      "1HO5UacAAAAJ",
      "0qUErHAAAAAJ"
    ],
    "citation_count": 2,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2306.00427",
    "title": "Out-of-distribution forgetting: vulnerability of continual learning to intra-class distribution shift",
    "year": 2023,
    "published": "2023-06-01T08:07:58Z",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "abstract": "Continual learning (CL) is an important technique to allow artificial neural networks to work in open environments. CL enables a system to learn new tasks without severe interference to its performance on old tasks, i.e., overcome the problems of catastrophic forgetting. In joint learning, it is well known that the out-of-distribution (OOD) problem caused by intentional attacks or environmental perturbations will severely impair the ability of networks to generalize. In this work, we reported a ",
    "arxiv_url": "https://arxiv.org/abs/2306.00427v2",
    "pdf_url": "https://arxiv.org/pdf/2306.00427v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2306.00427",
    "arxiv_authors": [
      "Liangxuan Guo",
      "Yang Chen",
      "Shan Yu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Out-of-distribution+forgetting%3A+vulnerability+of+continual+learning+to+intra-class+distribution+shift+Liangxuan+Guo+Yang+Chen+Shan+Yu",
    "gs_search_success": true,
    "gs_authors": [
      "YdaRHiIAAAAJ",
      "mgUhgvAAAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2408.02231",
    "title": "REVISION: Rendering Tools Enable Spatial Fidelity in Vision-Language Models",
    "year": 2024,
    "published": "2024-08-05T04:51:46Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Text-to-Image (T2I) and multimodal large language models (MLLMs) have been adopted in solutions for several computer vision and multimodal learning tasks. However, it has been found that such vision-language models lack the ability to correctly reason over spatial relationships. To tackle this shortcoming, we develop the REVISION framework which improves spatial fidelity in vision-language models. REVISION is a 3D rendering based pipeline that generates spatially accurate synthetic images, given",
    "arxiv_url": "https://arxiv.org/abs/2408.02231v1",
    "pdf_url": "https://arxiv.org/pdf/2408.02231v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2408.02231",
    "arxiv_authors": [
      "Agneet Chatterjee",
      "Yiran Luo",
      "Tejas Gokhale",
      "Yezhou Yang",
      "Chitta Baral"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=REVISION%3A+Rendering+Tools+Enable+Spatial+Fidelity+in+Vision-Language+Models+Agneet+Chatterjee+Yiran+Luo+Tejas+Gokhale+Yezhou+Yang+Chitta+Baral",
    "gs_search_success": true,
    "gs_authors": [
      "9Yd716IAAAAJ",
      "_ILTlEwAAAAJ",
      "n4IrxbUAAAAJ",
      "RGRaOegAAAAJ",
      "k2suuZgAAAAJ"
    ],
    "citation_count": 9,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2307.07847",
    "title": "Enabling Real-time Neural Recovery for Cloud Gaming on Mobile Devices",
    "year": 2023,
    "published": "2023-07-15T16:45:01Z",
    "categories": [
      "cs.NI",
      "cs.CV",
      "cs.LG",
      "cs.MM"
    ],
    "abstract": "Cloud gaming is a multi-billion dollar industry. A client in cloud gaming sends its movement to the game server on the Internet, which renders and transmits the resulting video back. In order to provide a good gaming experience, a latency below 80 ms is required. This means that video rendering, encoding, transmission, decoding, and display have to finish within that time frame, which is especially challenging to achieve due to server overload, network congestion, and losses. In this paper, we p",
    "arxiv_url": "https://arxiv.org/abs/2307.07847v4",
    "pdf_url": "https://arxiv.org/pdf/2307.07847v4",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2307.07847",
    "arxiv_authors": [
      "Zhaoyuan He",
      "Yifan Yang",
      "Shuozhe Li",
      "Diyuan Dai",
      "Lili Qiu",
      "Yuqing Yang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Enabling+Real-time+Neural+Recovery+for+Cloud+Gaming+on+Mobile+Devices+Zhaoyuan+He+Yifan+Yang+Shuozhe+Li+Diyuan+Dai+Lili+Qiu",
    "gs_search_success": true,
    "gs_authors": [
      "16posrQAAAAJ",
      "x7-Q64EAAAAJ",
      "EAth38AAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2312.00416",
    "title": "Towards Explaining Satellite Based Poverty Predictions with Convolutional Neural Networks",
    "year": 2023,
    "published": "2023-12-01T08:40:09Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Deep convolutional neural networks (CNNs) have been shown to predict poverty and development indicators from satellite images with surprising accuracy. This paper presents a first attempt at analyzing the CNNs responses in detail and explaining the basis for the predictions. The CNN model, while trained on relatively low resolution day- and night-time satellite images, is able to outperform human subjects who look at high-resolution images in ranking the Wealth Index categories. Multiple explain",
    "arxiv_url": "https://arxiv.org/abs/2312.00416v1",
    "pdf_url": "https://arxiv.org/pdf/2312.00416v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2312.00416",
    "arxiv_authors": [
      "Hamid Sarmadi",
      "Thorsteinn Rögnvaldsson",
      "Nils Roger Carlsson",
      "Mattias Ohlsson",
      "Ibrahim Wahab",
      "Ola Hall"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Towards+Explaining+Satellite+Based+Poverty+Predictions+with+Convolutional+Neural+Networks+Hamid+Sarmadi+Thorsteinn+R%C3%B6gnvaldsson+Nils+Roger+Carlsson+Mattias+Ohlsson+Ibrahim+Wahab",
    "gs_search_success": true,
    "gs_authors": [
      "hlMpAJMAAAAJ",
      "wJvtMD0AAAAJ",
      "MwFjjdsAAAAJ",
      "So5eNfkAAAAJ",
      "c8BJ06MAAAAJ"
    ],
    "citation_count": 9,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2407.03900",
    "title": "Oracle Bone Inscriptions Multi-modal Dataset",
    "year": 2024,
    "published": "2024-07-04T12:47:32Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Oracle bone inscriptions(OBI) is the earliest developed writing system in China, bearing invaluable written exemplifications of early Shang history and paleography. However, the task of deciphering OBI, in the current climate of the scholarship, can prove extremely challenging. Out of the 4,500 oracle bone characters excavated, only a third have been successfully identified. Therefore, leveraging the advantages of advanced AI technology to assist in the decipherment of OBI is a highly essential ",
    "arxiv_url": "https://arxiv.org/abs/2407.03900v1",
    "pdf_url": "https://arxiv.org/pdf/2407.03900v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2407.03900",
    "arxiv_authors": [
      "Bang Li",
      "Donghao Luo",
      "Yujie Liang",
      "Jing Yang",
      "Zengmao Ding",
      "Xu Peng",
      "Boyuan Jiang",
      "Shengwei Han",
      "Dan Sui",
      "Peichao Qin",
      "Pian Wu",
      "Chaoyang Wang",
      "Yun Qi",
      "Taisong Jin",
      "Chengjie Wang",
      "Xiaoming Huang",
      "Zhan Shu",
      "Rongrong Ji",
      "Yongge Liu",
      "Yunsheng Wu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Oracle+Bone+Inscriptions+Multi-modal+Dataset+Bang+Li+Donghao+Luo+Yujie+Liang+Jing+Yang+Zengmao+Ding",
    "gs_search_success": true,
    "gs_authors": [
      "2yj541QAAAAJ",
      "jENyc98AAAAJ",
      "u62rcKoAAAAJ",
      "1GMTwfEAAAAJ"
    ],
    "citation_count": 10,
    "gs_author_count": 9
  },
  {
    "arxiv_id": "2406.02202",
    "title": "No Captions, No Problem: Captionless 3D-CLIP Alignment with Hard Negatives via CLIP Knowledge and LLMs",
    "year": 2024,
    "published": "2024-06-04T10:57:59Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "In this study, we explore an alternative approach to enhance contrastive text-image-3D alignment in the absence of textual descriptions for 3D objects. We introduce two unsupervised methods, $I2I$ and $(I2L)^2$, which leverage CLIP knowledge about textual and 2D data to compute the neural perceived similarity between two 3D samples. We employ the proposed methods to mine 3D hard negatives, establishing a multimodal contrastive pipeline with hard negative weighting via a custom loss function. We ",
    "arxiv_url": "https://arxiv.org/abs/2406.02202v2",
    "pdf_url": "https://arxiv.org/pdf/2406.02202v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2406.02202",
    "arxiv_authors": [
      "Cristian Sbrolli",
      "Matteo Matteucci"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=No+Captions%2C+No+Problem%3A+Captionless+3D-CLIP+Alignment+with+Hard+Negatives+via+CLIP+Knowledge+and+LLMs+Cristian+Sbrolli+Matteo+Matteucci",
    "gs_search_success": true,
    "gs_authors": [
      "yhvx_3AAAAAJ",
      "PdbEg5YAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 2
  },
  {
    "arxiv_id": "2406.03835",
    "title": "Monocular Localization with Semantics Map for Autonomous Vehicles",
    "year": 2024,
    "published": "2024-06-06T08:12:38Z",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "abstract": "Accurate and robust localization remains a significant challenge for autonomous vehicles. The cost of sensors and limitations in local computational efficiency make it difficult to scale to large commercial applications. Traditional vision-based approaches focus on texture features that are susceptible to changes in lighting, season, perspective, and appearance. Additionally, the large storage size of maps with descriptors and complex optimization processes hinder system performance. To balance ",
    "arxiv_url": "https://arxiv.org/abs/2406.03835v1",
    "pdf_url": "https://arxiv.org/pdf/2406.03835v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2406.03835",
    "arxiv_authors": [
      "Jixiang Wan",
      "Xudong Zhang",
      "Shuzhou Dong",
      "Yuwei Zhang",
      "Yuchen Yang",
      "Ruoxi Wu",
      "Ye Jiang",
      "Jijunnan Li",
      "Jinquan Lin",
      "Ming Yang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Monocular+Localization+with+Semantics+Map+for+Autonomous+Vehicles+Jixiang+Wan+Xudong+Zhang+Shuzhou+Dong+Yuwei+Zhang+Yuchen+Yang",
    "gs_search_success": true,
    "gs_authors": [
      "DjYGIdgAAAAJ",
      "2jRt7vEAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 9
  },
  {
    "arxiv_id": "2403.16794",
    "title": "CurbNet: Curb Detection Framework Based on LiDAR Point Cloud Segmentation",
    "year": 2024,
    "published": "2024-03-25T14:13:09Z",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "abstract": "Curb detection is a crucial function in intelligent driving, essential for determining drivable areas on the road. However, the complexity of road environments makes curb detection challenging. This paper introduces CurbNet, a novel framework for curb detection utilizing point cloud segmentation. To address the lack of comprehensive curb datasets with 3D annotations, we have developed the 3D-Curb dataset based on SemanticKITTI, currently the largest and most diverse collection of curb point clou",
    "arxiv_url": "https://arxiv.org/abs/2403.16794v3",
    "pdf_url": "https://arxiv.org/pdf/2403.16794v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2403.16794",
    "arxiv_authors": [
      "Guoyang Zhao",
      "Fulong Ma",
      "Weiqing Qi",
      "Yuxuan Liu",
      "Ming Liu",
      "Jun Ma"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=CurbNet%3A+Curb+Detection+Framework+Based+on+LiDAR+Point+Cloud+Segmentation+Guoyang+Zhao+Fulong+Ma+Weiqing+Qi+Yuxuan+Liu+Ming+Liu",
    "gs_search_success": true,
    "gs_authors": [
      "8VepsVAAAAAJ",
      "CdV5LfQAAAAJ",
      "G5T6_SQAAAAJ",
      "uS8x8TkAAAAJ"
    ],
    "citation_count": 9,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2410.14690",
    "title": "Rethinking VLMs and LLMs for Image Classification",
    "year": 2024,
    "published": "2024-10-03T23:40:21Z",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "abstract": "Visual Language Models (VLMs) are now increasingly being merged with Large Language Models (LLMs) to enable new capabilities, particularly in terms of improved interactivity and open-ended responsiveness. While these are remarkable capabilities, the contribution of LLMs to enhancing the longstanding key problem of classifying an image among a set of choices remains unclear. Through extensive experiments involving seven models, ten visual understanding datasets, and multiple prompt variations per",
    "arxiv_url": "https://arxiv.org/abs/2410.14690v1",
    "pdf_url": "https://arxiv.org/pdf/2410.14690v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2410.14690",
    "arxiv_authors": [
      "Avi Cooper",
      "Keizo Kato",
      "Chia-Hsien Shih",
      "Hiroaki Yamane",
      "Kasper Vinken",
      "Kentaro Takemoto",
      "Taro Sunagawa",
      "Hao-Wei Yeh",
      "Jin Yamanaka",
      "Ian Mason",
      "Xavier Boix"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Rethinking+VLMs+and+LLMs+for+Image+Classification+Avi+Cooper+Keizo+Kato+Chia-Hsien+Shih+Hiroaki+Yamane+Kasper+Vinken",
    "gs_search_success": false,
    "gs_authors": [],
    "citation_count": null,
    "error_type": "fetch_failed",
    "gs_author_count": 9
  },
  {
    "arxiv_id": "2410.00152",
    "title": "Multimodal Alignment of Histopathological Images Using Cell Segmentation and Point Set Matching for Integrative Cancer Analysis",
    "year": 2024,
    "published": "2024-09-30T18:52:52Z",
    "categories": [
      "eess.IV",
      "cs.CV",
      "cs.LG",
      "q-bio.QM"
    ],
    "abstract": "Histopathological imaging is vital for cancer research and clinical practice, with multiplexed Immunofluorescence (MxIF) and Hematoxylin and Eosin (H&E) providing complementary insights. However, aligning different stains at the cell level remains a challenge due to modality differences. In this paper, we present a novel framework for multimodal image alignment using cell segmentation outcomes. By treating cells as point sets, we apply Coherent Point Drift (CPD) for initial alignment and refine ",
    "arxiv_url": "https://arxiv.org/abs/2410.00152v1",
    "pdf_url": "https://arxiv.org/pdf/2410.00152v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2410.00152",
    "arxiv_authors": [
      "Jun Jiang",
      "Raymond Moore",
      "Brenna Novotny",
      "Leo Liu",
      "Zachary Fogarty",
      "Ray Guo",
      "Markovic Svetomir",
      "Chen Wang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Multimodal+Alignment+of+Histopathological+Images+Using+Cell+Segmentation+and+Point+Set+Matching+for+Integrative+Cancer+Analysis+Jun+Jiang+Raymond+Moore+Brenna+Novotny+Leo+Liu+Zachary+Fogarty",
    "gs_search_success": true,
    "gs_authors": [
      "dxdAuCQAAAAJ",
      "HvEAziAAAAAJ",
      "4irpbaYAAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 8
  },
  {
    "arxiv_id": "2404.03042",
    "title": "AWOL: Analysis WithOut synthesis using Language",
    "year": 2024,
    "published": "2024-04-03T20:04:44Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Many classical parametric 3D shape models exist, but creating novel shapes with such models requires expert knowledge of their parameters. For example, imagine creating a specific type of tree using procedural graphics or a new kind of animal from a statistical shape model. Our key idea is to leverage language to control such existing models to produce novel shapes. This involves learning a mapping between the latent space of a vision-language model and the parameter space of the 3D model, which",
    "arxiv_url": "https://arxiv.org/abs/2404.03042v1",
    "pdf_url": "https://arxiv.org/pdf/2404.03042v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2404.03042",
    "arxiv_authors": [
      "Silvia Zuffi",
      "Michael J. Black"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=AWOL%3A+Analysis+WithOut+synthesis+using+Language+Silvia+Zuffi+Michael+J.+Black",
    "gs_search_success": true,
    "gs_authors": [
      "6NjbexEAAAAJ",
      "-d7Ib5UAAAAJ"
    ],
    "citation_count": 6,
    "gs_author_count": 2
  },
  {
    "arxiv_id": "2310.08367",
    "title": "MCU: An Evaluation Framework for Open-Ended Game Agents",
    "year": 2023,
    "published": "2023-10-12T14:38:25Z",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "Developing AI agents capable of interacting with open-world environments to solve diverse tasks is a compelling challenge. However, evaluating such open-ended agents remains difficult, with current benchmarks facing scalability limitations. To address this, we introduce Minecraft Universe (MCU), a comprehensive evaluation framework set within the open-world video game Minecraft. MCU incorporates three key components: (1) an expanding collection of 3,452 composable atomic tasks that encompasses 1",
    "arxiv_url": "https://arxiv.org/abs/2310.08367v4",
    "pdf_url": "https://arxiv.org/pdf/2310.08367v4",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2310.08367",
    "arxiv_authors": [
      "Xinyue Zheng",
      "Haowei Lin",
      "Kaichen He",
      "Zihao Wang",
      "Zilong Zheng",
      "Yitao Liang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=MCU%3A+An+Evaluation+Framework+for+Open-Ended+Game+Agents+Xinyue+Zheng+Haowei+Lin+Kaichen+He+Zihao+Wang+Zilong+Zheng",
    "gs_search_success": true,
    "gs_authors": [
      "9sDx70IAAAAJ",
      "gANaxT0AAAAJ",
      "LFdJXNcAAAAJ",
      "KVzR1XEAAAAJ",
      "I0D-EgQAAAAJ",
      "Ng-DmJgAAAAJ"
    ],
    "citation_count": 4,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2309.15313",
    "title": "M$^{3}$3D: Learning 3D priors using Multi-Modal Masked Autoencoders for 2D image and video understanding",
    "year": 2023,
    "published": "2023-09-26T23:52:09Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "We present a new pre-training strategy called M$^{3}$3D ($\\underline{M}$ulti-$\\underline{M}$odal $\\underline{M}$asked $\\underline{3D}$) built based on Multi-modal masked autoencoders that can leverage 3D priors and learned cross-modal representations in RGB-D data. We integrate two major self-supervised learning frameworks; Masked Image Modeling (MIM) and contrastive learning; aiming to effectively embed masked 3D priors and modality complementary features to enhance the correspondence between m",
    "arxiv_url": "https://arxiv.org/abs/2309.15313v1",
    "pdf_url": "https://arxiv.org/pdf/2309.15313v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2309.15313",
    "arxiv_authors": [
      "Muhammad Abdullah Jamal",
      "Omid Mohareri"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=M%24%5E%7B3%7D%243D%3A+Learning+3D+priors+using+Multi-Modal+Masked+Autoencoders+for+2D+image+and+video+understanding+Muhammad+Abdullah+Jamal+Omid+Mohareri",
    "gs_search_success": true,
    "gs_authors": [
      "gWNpXlMAAAAJ",
      "-t_yjBIAAAAJ"
    ],
    "citation_count": 3,
    "gs_author_count": 2
  },
  {
    "arxiv_id": "2304.09446",
    "title": "Density-Insensitive Unsupervised Domain Adaption on 3D Object Detection",
    "year": 2023,
    "published": "2023-04-19T06:33:07Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "3D object detection from point clouds is crucial in safety-critical autonomous driving. Although many works have made great efforts and achieved significant progress on this task, most of them suffer from expensive annotation cost and poor transferability to unknown data due to the domain gap. Recently, few works attempt to tackle the domain gap in objects, but still fail to adapt to the gap of varying beam-densities between two domains, which is critical to mitigate the characteristic differenc",
    "arxiv_url": "https://arxiv.org/abs/2304.09446v1",
    "pdf_url": "https://arxiv.org/pdf/2304.09446v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2304.09446",
    "arxiv_authors": [
      "Qianjiang Hu",
      "Daizong Liu",
      "Wei Hu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Density-Insensitive+Unsupervised+Domain+Adaption+on+3D+Object+Detection+Qianjiang+Hu+Daizong+Liu+Wei+Hu",
    "gs_search_success": true,
    "gs_authors": [
      "lUw7tVIAAAAJ",
      "5oFf8Q4AAAAJ",
      "qiM8pNUAAAAJ"
    ],
    "citation_count": 55,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2405.04309",
    "title": "Non-rigid Structure-from-Motion: Temporally-smooth Procrustean Alignment and Spatially-variant Deformation Modeling",
    "year": 2024,
    "published": "2024-05-07T13:33:50Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Even though Non-rigid Structure-from-Motion (NRSfM) has been extensively studied and great progress has been made, there are still key challenges that hinder their broad real-world applications: 1) the inherent motion/rotation ambiguity requires either explicit camera motion recovery with extra constraint or complex Procrustean Alignment; 2) existing low-rank modeling of the global shape can over-penalize drastic deformations in the 3D shape sequence. This paper proposes to resolve the above iss",
    "arxiv_url": "https://arxiv.org/abs/2405.04309v3",
    "pdf_url": "https://arxiv.org/pdf/2405.04309v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2405.04309",
    "arxiv_authors": [
      "Jiawei Shi",
      "Hui Deng",
      "Yuchao Dai"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Non-rigid+Structure-from-Motion%3A+Temporally-smooth+Procrustean+Alignment+and+Spatially-variant+Deformation+Modeling+Jiawei+Shi+Hui+Deng+Yuchao+Dai",
    "gs_search_success": true,
    "gs_authors": [
      "fddAbqsAAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2303.10766",
    "title": "Multi-modal reward for visual relationships-based image captioning",
    "year": 2023,
    "published": "2023-03-19T20:52:44Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Deep neural networks have achieved promising results in automatic image captioning due to their effective representation learning and context-based content generation capabilities. As a prominent type of deep features used in many of the recent image captioning methods, the well-known bottomup features provide a detailed representation of different objects of the image in comparison with the feature maps directly extracted from the raw image. However, the lack of high-level semantic information ",
    "arxiv_url": "https://arxiv.org/abs/2303.10766v2",
    "pdf_url": "https://arxiv.org/pdf/2303.10766v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2303.10766",
    "arxiv_authors": [
      "Ali Abedi",
      "Hossein Karshenas",
      "Peyman Adibi"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Multi-modal+reward+for+visual+relationships-based+image+captioning+Ali+Abedi+Hossein+Karshenas+Peyman+Adibi",
    "gs_search_success": true,
    "gs_authors": [
      "33adiB4AAAAJ",
      "u-FQZMkAAAAJ"
    ],
    "citation_count": 7,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2411.19514",
    "title": "Enhancing AI microscopy for foodborne bacterial classification via adversarial domain adaptation across optical and biological variability",
    "year": 2024,
    "published": "2024-11-29T07:12:36Z",
    "categories": [
      "eess.IV",
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "Rapid detection of foodborne bacteria is critical for food safety and quality, yet traditional culture-based methods require extended incubation and specialized sample preparation. This study addresses these challenges by i) enhancing the generalizability of AI-enabled microscopy for bacterial classification using adversarial domain adaptation and ii) comparing the performance of single-target and multi-domain adaptation. Three Gram-positive (Bacillus coagulans, Bacillus subtilis, Listeria innoc",
    "arxiv_url": "https://arxiv.org/abs/2411.19514v1",
    "pdf_url": "https://arxiv.org/pdf/2411.19514v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2411.19514",
    "arxiv_authors": [
      "Siddhartha Bhattacharya",
      "Aarham Wasit",
      "Mason Earles",
      "Nitin Nitin",
      "Luyao Ma",
      "Jiyoon Yi"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Enhancing+AI+microscopy+for+foodborne+bacterial+classification+via+adversarial+domain+adaptation+across+optical+and+biological+variability+Siddhartha+Bhattacharya+Aarham+Wasit+Mason+Earles+Nitin+Nitin+Luyao+Ma",
    "gs_search_success": true,
    "gs_authors": [
      "PjrSfXgAAAAJ",
      "r47jW4gAAAAJ",
      "p8sz18MAAAAJ",
      "p-n_bTEAAAAJ",
      "J_zV590AAAAJ"
    ],
    "citation_count": 2,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2503.22154",
    "title": "Dataset Distillation of 3D Point Clouds via Distribution Matching",
    "year": 2025,
    "published": "2025-03-28T05:15:22Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Large-scale datasets are usually required to train deep neural networks, but it increases the computational complexity hindering the practical applications. Recently, dataset distillation for images and texts has been attracting a lot of attention, that reduces the original dataset to a synthetic dataset to alleviate the computational burden of training while preserving essential task-relevant information. However, the dataset distillation for 3D point clouds remains largely unexplored, as the p",
    "arxiv_url": "https://arxiv.org/abs/2503.22154v2",
    "pdf_url": "https://arxiv.org/pdf/2503.22154v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2503.22154",
    "arxiv_authors": [
      "Jae-Young Yim",
      "Dongwook Kim",
      "Jae-Young Sim"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Dataset+Distillation+of+3D+Point+Clouds+via+Distribution+Matching+Jae-Young+Yim+Dongwook+Kim+Jae-Young+Sim",
    "gs_search_success": true,
    "gs_authors": [
      "YwvMQCQAAAAJ",
      "Mx6iAMoAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2502.17212",
    "title": "A Two-step Linear Mixing Model for Unmixing under Hyperspectral Variability",
    "year": 2025,
    "published": "2025-02-24T14:44:40Z",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "abstract": "Spectral unmixing is an important task in the research field of hyperspectral image processing. It can be thought of as a regression problem, where the observed variable (i.e., an image pixel) is to be found as a function of the response variables (i.e., the pure materials in a scene, called endmembers). The Linear Mixing Model (LMM) has received a great deal of attention, due to its simplicity and ease of use in, e.g., optimization problems. Its biggest flaw is that it assumes that any pure mat",
    "arxiv_url": "https://arxiv.org/abs/2502.17212v2",
    "pdf_url": "https://arxiv.org/pdf/2502.17212v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2502.17212",
    "arxiv_authors": [
      "Xander Haijen",
      "Bikram Koirala",
      "Xuanwen Tao",
      "Paul Scheunders"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=A+Two-step+Linear+Mixing+Model+for+Unmixing+under+Hyperspectral+Variability+Xander+Haijen+Bikram+Koirala+Xuanwen+Tao+Paul+Scheunders",
    "gs_search_success": true,
    "gs_authors": [
      "ULDIe24AAAAJ",
      "ReqZbgUAAAAJ",
      "9iW_yQ4AAAAJ"
    ],
    "citation_count": 2,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2401.01730",
    "title": "STAF: 3D Human Mesh Recovery from Video with Spatio-Temporal Alignment Fusion",
    "year": 2024,
    "published": "2024-01-03T13:07:14Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "The recovery of 3D human mesh from monocular images has significantly been developed in recent years. However, existing models usually ignore spatial and temporal information, which might lead to mesh and image misalignment and temporal discontinuity. For this reason, we propose a novel Spatio-Temporal Alignment Fusion (STAF) model. As a video-based model, it leverages coherence clues from human motion by an attention-based Temporal Coherence Fusion Module (TCFM). As for spatial mesh-alignment e",
    "arxiv_url": "https://arxiv.org/abs/2401.01730v1",
    "pdf_url": "https://arxiv.org/pdf/2401.01730v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2401.01730",
    "arxiv_authors": [
      "Wei Yao",
      "Hongwen Zhang",
      "Yunlian Sun",
      "Jinhui Tang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=STAF%3A+3D+Human+Mesh+Recovery+from+Video+with+Spatio-Temporal+Alignment+Fusion+Wei+Yao+Hongwen+Zhang+Yunlian+Sun+Jinhui+Tang",
    "gs_search_success": true,
    "gs_authors": [
      "GlKbx-4AAAAJ",
      "6z0m_ZMAAAAJ",
      "ByBLlEwAAAAJ",
      "ObAJh4IAAAAJ"
    ],
    "citation_count": 19,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2406.00633",
    "title": "Improving GFlowNets for Text-to-Image Diffusion Alignment",
    "year": 2024,
    "published": "2024-06-02T06:36:46Z",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "stat.ML"
    ],
    "abstract": "Diffusion models have become the de-facto approach for generating visual data, which are trained to match the distribution of the training dataset. In addition, we also want to control generation to fulfill desired properties such as alignment to a text description, which can be specified with a black-box reward function. Prior works fine-tune pretrained diffusion models to achieve this goal through reinforcement learning-based algorithms. Nonetheless, they suffer from issues including slow cred",
    "arxiv_url": "https://arxiv.org/abs/2406.00633v3",
    "pdf_url": "https://arxiv.org/pdf/2406.00633v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2406.00633",
    "arxiv_authors": [
      "Dinghuai Zhang",
      "Yizhe Zhang",
      "Jiatao Gu",
      "Ruixiang Zhang",
      "Josh Susskind",
      "Navdeep Jaitly",
      "Shuangfei Zhai"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Improving+GFlowNets+for+Text-to-Image+Diffusion+Alignment+Dinghuai+Zhang+Yizhe+Zhang+Jiatao+Gu+Ruixiang+Zhang+Josh+Susskind",
    "gs_search_success": true,
    "gs_authors": [
      "VQYdApgAAAAJ",
      "Sv2TGqsAAAAJ",
      "G6vdBYsAAAAJ",
      "kjMNMLkAAAAJ",
      "8W-MW9sAAAAJ",
      "cB1mFBsAAAAJ",
      "WDVMfggAAAAJ"
    ],
    "citation_count": 20,
    "gs_author_count": 7
  },
  {
    "arxiv_id": "2305.18614",
    "title": "Simulation-Aided Deep Learning for Laser Ultrasonic Visualization Testing",
    "year": 2023,
    "published": "2023-05-30T00:19:12Z",
    "categories": [
      "eess.IV",
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "In recent years, laser ultrasonic visualization testing (LUVT) has attracted much attention because of its ability to efficiently perform non-contact ultrasonic non-destructive testing.Despite many success reports of deep learning based image analysis for widespread areas, attempts to apply deep learning to defect detection in LUVT images face the difficulty of preparing a large dataset of LUVT images that is too expensive to scale. To compensate for the scarcity of such training data, we propos",
    "arxiv_url": "https://arxiv.org/abs/2305.18614v1",
    "pdf_url": "https://arxiv.org/pdf/2305.18614v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2305.18614",
    "arxiv_authors": [
      "Miya Nakajima",
      "Takahiro Saitoh",
      "Tsuyoshi Kato"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Simulation-Aided+Deep+Learning+for+Laser+Ultrasonic+Visualization+Testing+Miya+Nakajima+Takahiro+Saitoh+Tsuyoshi+Kato",
    "gs_search_success": true,
    "gs_authors": [],
    "citation_count": 2,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2408.04955",
    "title": "Model Debiasing by Learnable Data Augmentation",
    "year": 2024,
    "published": "2024-08-09T09:19:59Z",
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "abstract": "Deep Neural Networks are well known for efficiently fitting training data, yet experiencing poor generalization capabilities whenever some kind of bias dominates over the actual task labels, resulting in models learning \"shortcuts\". In essence, such models are often prone to learn spurious correlations between data and labels. In this work, we tackle the problem of learning from biased data in the very realistic unsupervised scenario, i.e., when the bias is unknown. This is a much harder task as",
    "arxiv_url": "https://arxiv.org/abs/2408.04955v1",
    "pdf_url": "https://arxiv.org/pdf/2408.04955v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2408.04955",
    "arxiv_authors": [
      "Pietro Morerio",
      "Ruggero Ragonesi",
      "Vittorio Murino"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Model+Debiasing+by+Learnable+Data+Augmentation+Pietro+Morerio+Ruggero+Ragonesi+Vittorio+Murino",
    "gs_search_success": true,
    "gs_authors": [
      "gFf7g6YAAAAJ",
      "yV3_PTkAAAAJ",
      "lPV9rbkAAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2410.01185",
    "title": "Formula-Driven Data Augmentation and Partial Retinal Layer Copying for Retinal Layer Segmentation",
    "year": 2024,
    "published": "2024-10-02T02:37:11Z",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "abstract": "Major retinal layer segmentation methods from OCT images assume that the retina is flattened in advance, and thus cannot always deal with retinas that have changes in retinal structure due to ophthalmopathy and/or curvature due to myopia. To eliminate the use of flattening in retinal layer segmentation for practicality of such methods, we propose novel data augmentation methods for OCT images. Formula-driven data augmentation (FDDA) emulates a variety of retinal structures by vertically shifting",
    "arxiv_url": "https://arxiv.org/abs/2410.01185v1",
    "pdf_url": "https://arxiv.org/pdf/2410.01185v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2410.01185",
    "arxiv_authors": [
      "Tsubasa Konno",
      "Takahiro Ninomiya",
      "Kanta Miura",
      "Koichi Ito",
      "Noriko Himori",
      "Parmanand Sharma",
      "Toru Nakazawa",
      "Takafumi Aoki"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Formula-Driven+Data+Augmentation+and+Partial+Retinal+Layer+Copying+for+Retinal+Layer+Segmentation+Tsubasa+Konno+Takahiro+Ninomiya+Kanta+Miura+Koichi+Ito+Noriko+Himori",
    "gs_search_success": true,
    "gs_authors": [
      "HWBwO0sAAAAJ",
      "7P-vK94AAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 8
  },
  {
    "arxiv_id": "2505.18021",
    "title": "Building Floor Number Estimation from Crowdsourced Street-Level Images: Munich Dataset and Baseline Method",
    "year": 2025,
    "published": "2025-05-23T15:27:46Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Accurate information on the number of building floors, or above-ground storeys, is essential for household estimation, utility provision, risk assessment, evacuation planning, and energy modeling. Yet large-scale floor-count data are rarely available in cadastral and 3D city databases. This study proposes an end-to-end deep learning framework that infers floor numbers directly from unrestricted, crowdsourced street-level imagery, avoiding hand-crafted features and generalizing across diverse fac",
    "arxiv_url": "https://arxiv.org/abs/2505.18021v1",
    "pdf_url": "https://arxiv.org/pdf/2505.18021v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2505.18021",
    "arxiv_authors": [
      "Yao Sun",
      "Sining Chen",
      "Yifan Tian",
      "Xiao Xiang Zhu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Building+Floor+Number+Estimation+from+Crowdsourced+Street-Level+Images%3A+Munich+Dataset+and+Baseline+Method+Yao+Sun+Sining+Chen+Yifan+Tian+Xiao+Xiang+Zhu",
    "gs_search_success": true,
    "gs_authors": [
      "VGHLoF0AAAAJ",
      "iAN0lDQAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2501.06903",
    "title": "Synthetic Prior for Few-Shot Drivable Head Avatar Inversion",
    "year": 2025,
    "published": "2025-01-12T19:01:05Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "We present SynShot, a novel method for the few-shot inversion of a drivable head avatar based on a synthetic prior. We tackle three major challenges. First, training a controllable 3D generative network requires a large number of diverse sequences, for which pairs of images and high-quality tracked meshes are not always available. Second, the use of real data is strictly regulated (e.g., under the General Data Protection Regulation, which mandates frequent deletion of models and data to accommod",
    "arxiv_url": "https://arxiv.org/abs/2501.06903v3",
    "pdf_url": "https://arxiv.org/pdf/2501.06903v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2501.06903",
    "arxiv_authors": [
      "Wojciech Zielonka",
      "Stephan J. Garbin",
      "Alexandros Lattas",
      "George Kopanas",
      "Paulo Gotardo",
      "Thabo Beeler",
      "Justus Thies",
      "Timo Bolkart"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Synthetic+Prior+for+Few-Shot+Drivable+Head+Avatar+Inversion+Wojciech+Zielonka+Stephan+J.+Garbin+Alexandros+Lattas+George+Kopanas+Paulo+Gotardo",
    "gs_search_success": true,
    "gs_authors": [
      "QLWLLHMAAAAJ",
      "4vpQvuwAAAAJ",
      "0wJRUlsAAAAJ",
      "aITcRswAAAAJ",
      "Rcv1-EIAAAAJ",
      "RbfmHUgAAAAJ",
      "fNteVjwAAAAJ",
      "q5y44h8AAAAJ"
    ],
    "citation_count": 8,
    "gs_author_count": 8
  },
  {
    "arxiv_id": "2307.05784",
    "title": "EgoAdapt: A multi-stream evaluation study of adaptation to real-world egocentric user video",
    "year": 2023,
    "published": "2023-07-11T20:23:23Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "In egocentric action recognition a single population model is typically trained and subsequently embodied on a head-mounted device, such as an augmented reality headset. While this model remains static for new users and environments, we introduce an adaptive paradigm of two phases, where after pretraining a population model, the model adapts on-device and online to the user's experience. This setting is highly challenging due to the change from population to user domain and the distribution shif",
    "arxiv_url": "https://arxiv.org/abs/2307.05784v1",
    "pdf_url": "https://arxiv.org/pdf/2307.05784v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2307.05784",
    "arxiv_authors": [
      "Matthias De Lange",
      "Hamid Eghbalzadeh",
      "Reuben Tan",
      "Michael Iuzzolino",
      "Franziska Meier",
      "Karl Ridgeway"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=EgoAdapt%3A+A+multi-stream+evaluation+study+of+adaptation+to+real-world+egocentric+user+video+Matthias+De+Lange+Hamid+Eghbalzadeh+Reuben+Tan+Michael+Iuzzolino+Franziska+Meier",
    "gs_search_success": true,
    "gs_authors": [
      "-yGxzA4AAAAJ",
      "7oxkHYYAAAAJ",
      "WiWKsngAAAAJ",
      "ALlIxw8AAAAJ",
      "cjmjU5AAAAAJ",
      "_Y9V66EAAAAJ"
    ],
    "citation_count": 2,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2302.05116",
    "title": "Example-Based Sampling with Diffusion Models",
    "year": 2023,
    "published": "2023-02-10T08:35:17Z",
    "categories": [
      "cs.GR",
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "Much effort has been put into developing samplers with specific properties, such as producing blue noise, low-discrepancy, lattice or Poisson disk samples. These samplers can be slow if they rely on optimization processes, may rely on a wide range of numerical methods, are not always differentiable. The success of recent diffusion models for image generation suggests that these models could be appropriate for learning how to generate point sets from examples. However, their convolutional nature ",
    "arxiv_url": "https://arxiv.org/abs/2302.05116v1",
    "pdf_url": "https://arxiv.org/pdf/2302.05116v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2302.05116",
    "arxiv_authors": [
      "Bastien Doignies",
      "Nicolas Bonneel",
      "David Coeurjolly",
      "Julie Digne",
      "Loïs Paulin",
      "Jean-Claude Iehl",
      "Victor Ostromoukhov"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Example-Based+Sampling+with+Diffusion+Models+Bastien+Doignies+Nicolas+Bonneel+David+Coeurjolly+Julie+Digne+Lo%C3%AFs+Paulin",
    "gs_search_success": true,
    "gs_authors": [
      "EOBpDNQAAAAJ",
      "-NXsLG4AAAAJ",
      "C9NNbukAAAAJ",
      "NGYWGIsAAAAJ",
      "-KUdbDoAAAAJ"
    ],
    "citation_count": 3,
    "gs_author_count": 7
  },
  {
    "arxiv_id": "2311.12621",
    "title": "Crowd management, crime detection, work monitoring using aiml",
    "year": 2023,
    "published": "2023-11-21T14:12:17Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "This research endeavors to harness the potential of existing Closed-Circuit Television (CCTV) networks for a comprehensive approach to crowd management, crime prevention, and workplace monitoring through the integration of Artificial Intelligence (AI) and Machine Learning (ML) technologies. The primary objective is to develop and implement advanced algorithms capable of real-time analysis of video feeds, enabling the identification and assessment of crowd dynamics, early detection of potential c",
    "arxiv_url": "https://arxiv.org/abs/2311.12621v1",
    "pdf_url": "https://arxiv.org/pdf/2311.12621v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2311.12621",
    "arxiv_authors": [
      "P. R. Adithya",
      "Dheepak. S",
      "B. Akash",
      "Harshini. V",
      "Sai Lakshana"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Crowd+management%2C+crime+detection%2C+work+monitoring+using+aiml+P.+R.+Adithya+Dheepak.+S+B.+Akash+Harshini.+V+Sai+Lakshana",
    "gs_search_success": true,
    "gs_authors": [],
    "citation_count": 3,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2404.02733",
    "title": "InstantStyle: Free Lunch towards Style-Preserving in Text-to-Image Generation",
    "year": 2024,
    "published": "2024-04-03T13:34:09Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Tuning-free diffusion-based models have demonstrated significant potential in the realm of image personalization and customization. However, despite this notable progress, current models continue to grapple with several complex challenges in producing style-consistent image generation. Firstly, the concept of style is inherently underdetermined, encompassing a multitude of elements such as color, material, atmosphere, design, and structure, among others. Secondly, inversion-based methods are pro",
    "arxiv_url": "https://arxiv.org/abs/2404.02733v2",
    "pdf_url": "https://arxiv.org/pdf/2404.02733v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2404.02733",
    "arxiv_authors": [
      "Haofan Wang",
      "Matteo Spinelli",
      "Qixun Wang",
      "Xu Bai",
      "Zekui Qin",
      "Anthony Chen"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=InstantStyle%3A+Free+Lunch+towards+Style-Preserving+in+Text-to-Image+Generation+Haofan+Wang+Matteo+Spinelli+Qixun+Wang+Xu+Bai+Zekui+Qin",
    "gs_search_success": true,
    "gs_authors": [
      "DT589gQAAAAJ",
      "EaMsuB0AAAAJ",
      "GaYsdd0AAAAJ"
    ],
    "citation_count": 185,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2404.14588",
    "title": "Brain-Inspired Continual Learning-Robust Feature Distillation and Re-Consolidation for Class Incremental Learning",
    "year": 2024,
    "published": "2024-04-22T21:30:11Z",
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "abstract": "Artificial intelligence (AI) and neuroscience share a rich history, with advancements in neuroscience shaping the development of AI systems capable of human-like knowledge retention. Leveraging insights from neuroscience and existing research in adversarial and continual learning, we introduce a novel framework comprising two core concepts: feature distillation and re-consolidation. Our framework, named Robust Rehearsal, addresses the challenge of catastrophic forgetting inherent in continual le",
    "arxiv_url": "https://arxiv.org/abs/2404.14588v1",
    "pdf_url": "https://arxiv.org/pdf/2404.14588v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2404.14588",
    "arxiv_authors": [
      "Hikmat Khan",
      "Nidhal Carla Bouaynaya",
      "Ghulam Rasool"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Brain-Inspired+Continual+Learning-Robust+Feature+Distillation+and+Re-Consolidation+for+Class+Incremental+Learning+Hikmat+Khan+Nidhal+Carla+Bouaynaya+Ghulam+Rasool",
    "gs_search_success": true,
    "gs_authors": [
      "XU17nOwAAAAJ",
      "Ejo4SlwAAAAJ",
      "Cja4jlYAAAAJ"
    ],
    "citation_count": 11,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2306.12983",
    "title": "Towards More Realistic Membership Inference Attacks on Large Diffusion Models",
    "year": 2023,
    "published": "2023-06-22T15:41:15Z",
    "categories": [
      "cs.LG",
      "cs.CR",
      "cs.CV"
    ],
    "abstract": "Generative diffusion models, including Stable Diffusion and Midjourney, can generate visually appealing, diverse, and high-resolution images for various applications. These models are trained on billions of internet-sourced images, raising significant concerns about the potential unauthorized use of copyright-protected images. In this paper, we examine whether it is possible to determine if a specific image was used in the training set, a problem known in the cybersecurity community and referred",
    "arxiv_url": "https://arxiv.org/abs/2306.12983v2",
    "pdf_url": "https://arxiv.org/pdf/2306.12983v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2306.12983",
    "arxiv_authors": [
      "Jan Dubiński",
      "Antoni Kowalczuk",
      "Stanisław Pawlak",
      "Przemysław Rokita",
      "Tomasz Trzciński",
      "Paweł Morawiecki"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Towards+More+Realistic+Membership+Inference+Attacks+on+Large+Diffusion+Models+Jan+Dubi%C5%84ski+Antoni+Kowalczuk+Stanis%C5%82aw+Pawlak+Przemys%C5%82aw+Rokita+Tomasz+Trzci%C5%84ski",
    "gs_search_success": true,
    "gs_authors": [
      "eLuRi8oAAAAJ",
      "zpcvrmIAAAAJ",
      "KQXAZdUAAAAJ",
      "bJMRBFoAAAAJ",
      "iG319iwAAAAJ",
      "qwqKQ34AAAAJ"
    ],
    "citation_count": 43,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2405.07869",
    "title": "Enhancing Clinically Significant Prostate Cancer Prediction in T2-weighted Images through Transfer Learning from Breast Cancer",
    "year": 2024,
    "published": "2024-05-13T15:57:27Z",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "abstract": "In 2020, prostate cancer saw a staggering 1.4 million new cases, resulting in over 375,000 deaths. The accurate identification of clinically significant prostate cancer is crucial for delivering effective treatment to patients. Consequently, there has been a surge in research exploring the application of deep neural networks to predict clinical significance based on magnetic resonance images. However, these networks demand extensive datasets to attain optimal performance. Recently, transfer lear",
    "arxiv_url": "https://arxiv.org/abs/2405.07869v1",
    "pdf_url": "https://arxiv.org/pdf/2405.07869v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2405.07869",
    "arxiv_authors": [
      "Chi-en Amy Tai",
      "Alexander Wong"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Enhancing+Clinically+Significant+Prostate+Cancer+Prediction+in+T2-weighted+Images+through+Transfer+Learning+from+Breast+Cancer+Chi-en+Amy+Tai+Alexander+Wong",
    "gs_search_success": true,
    "gs_authors": [
      "i7h8-dYAAAAJ",
      "suu78HIAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 2
  },
  {
    "arxiv_id": "2301.06116",
    "title": "Maximally Compact and Separated Features with Regular Polytope Networks",
    "year": 2023,
    "published": "2023-01-15T15:20:57Z",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "Convolutional Neural Networks (CNNs) trained with the Softmax loss are widely used classification models for several vision tasks. Typically, a learnable transformation (i.e. the classifier) is placed at the end of such models returning class scores that are further normalized into probabilities by Softmax. This learnable transformation has a fundamental role in determining the network internal feature representation.   In this work we show how to extract from CNNs features with the properties o",
    "arxiv_url": "https://arxiv.org/abs/2301.06116v1",
    "pdf_url": "https://arxiv.org/pdf/2301.06116v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2301.06116",
    "arxiv_authors": [
      "Federico Pernici",
      "Matteo Bruni",
      "Claudio Baecchi",
      "Alberto Del Bimbo"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Maximally+Compact+and+Separated+Features+with+Regular+Polytope+Networks+Federico+Pernici+Matteo+Bruni+Claudio+Baecchi+Alberto+Del+Bimbo",
    "gs_search_success": true,
    "gs_authors": [
      "BJXYyhAAAAAJ",
      "bf2ZrFcAAAAJ",
      "Q0cKT9cAAAAJ"
    ],
    "citation_count": 35,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2308.01006",
    "title": "FusionAD: Multi-modality Fusion for Prediction and Planning Tasks of Autonomous Driving",
    "year": 2023,
    "published": "2023-08-02T08:29:44Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.RO"
    ],
    "abstract": "Building a multi-modality multi-task neural network toward accurate and robust performance is a de-facto standard in perception task of autonomous driving. However, leveraging such data from multiple sensors to jointly optimize the prediction and planning tasks remains largely unexplored. In this paper, we present FusionAD, to the best of our knowledge, the first unified framework that fuse the information from two most critical sensors, camera and LiDAR, goes beyond perception task. Concretely,",
    "arxiv_url": "https://arxiv.org/abs/2308.01006v4",
    "pdf_url": "https://arxiv.org/pdf/2308.01006v4",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2308.01006",
    "arxiv_authors": [
      "Tengju Ye",
      "Wei Jing",
      "Chunyong Hu",
      "Shikun Huang",
      "Lingping Gao",
      "Fangzhen Li",
      "Jingke Wang",
      "Ke Guo",
      "Wencong Xiao",
      "Weibo Mao",
      "Hang Zheng",
      "Kun Li",
      "Junbo Chen",
      "Kaicheng Yu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=FusionAD%3A+Multi-modality+Fusion+for+Prediction+and+Planning+Tasks+of+Autonomous+Driving+Tengju+Ye+Wei+Jing+Chunyong+Hu+Shikun+Huang+Lingping+Gao",
    "gs_search_success": true,
    "gs_authors": [
      "_xQWbAEAAAAJ",
      "1oysro0AAAAJ",
      "CyS4PacAAAAJ",
      "12L_IZMAAAAJ",
      "y1_8HucAAAAJ",
      "DPk835cAAAAJ"
    ],
    "citation_count": 71,
    "gs_author_count": 9
  },
  {
    "arxiv_id": "2308.11994",
    "title": "Progressive Feature Mining and External Knowledge-Assisted Text-Pedestrian Image Retrieval",
    "year": 2023,
    "published": "2023-08-23T08:29:10Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Text-Pedestrian Image Retrieval aims to use the text describing pedestrian appearance to retrieve the corresponding pedestrian image. This task involves not only modality discrepancy, but also the challenge of the textual diversity of pedestrians with the same identity. At present, although existing research progress has been made in text-pedestrian image retrieval, these methods do not comprehensively consider the above-mentioned problems. Considering these, this paper proposes a progressive fe",
    "arxiv_url": "https://arxiv.org/abs/2308.11994v1",
    "pdf_url": "https://arxiv.org/pdf/2308.11994v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2308.11994",
    "arxiv_authors": [
      "Huafeng Li",
      "Shedan Yang",
      "Yafei Zhang",
      "Dapeng Tao",
      "Zhengtao Yu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Progressive+Feature+Mining+and+External+Knowledge-Assisted+Text-Pedestrian+Image+Retrieval+Huafeng+Li+Shedan+Yang+Yafei+Zhang+Dapeng+Tao+Zhengtao+Yu",
    "gs_search_success": true,
    "gs_authors": [
      "AQzS40gAAAAJ",
      "mKhfyyIAAAAJ",
      "njjX8D0AAAAJ"
    ],
    "citation_count": 9,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2407.20446",
    "title": "MEVDT: Multi-Modal Event-Based Vehicle Detection and Tracking Dataset",
    "year": 2024,
    "published": "2024-07-29T22:57:20Z",
    "categories": [
      "cs.CV",
      "cs.DB"
    ],
    "abstract": "In this data article, we introduce the Multi-Modal Event-based Vehicle Detection and Tracking (MEVDT) dataset. This dataset provides a synchronized stream of event data and grayscale images of traffic scenes, captured using the Dynamic and Active-Pixel Vision Sensor (DAVIS) 240c hybrid event-based camera. MEVDT comprises 63 multi-modal sequences with approximately 13k images, 5M events, 10k object labels, and 85 unique object tracking trajectories. Additionally, MEVDT includes manually annotated",
    "arxiv_url": "https://arxiv.org/abs/2407.20446v1",
    "pdf_url": "https://arxiv.org/pdf/2407.20446v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2407.20446",
    "arxiv_authors": [
      "Zaid A. El Shair",
      "Samir A. Rawashdeh"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=MEVDT%3A+Multi-Modal+Event-Based+Vehicle+Detection+and+Tracking+Dataset+Zaid+A.+El+Shair+Samir+A.+Rawashdeh",
    "gs_search_success": true,
    "gs_authors": [
      "xWph1poAAAAJ",
      "bNi3iJ4AAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 2
  },
  {
    "arxiv_id": "2405.05295",
    "title": "Relevant Irrelevance: Generating Alterfactual Explanations for Image Classifiers",
    "year": 2024,
    "published": "2024-05-08T11:03:22Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "abstract": "In this paper, we demonstrate the feasibility of alterfactual explanations for black box image classifiers. Traditional explanation mechanisms from the field of Counterfactual Thinking are a widely-used paradigm for Explainable Artificial Intelligence (XAI), as they follow a natural way of reasoning that humans are familiar with. However, most common approaches from this field are based on communicating information about features or characteristics that are especially important for an AI's decis",
    "arxiv_url": "https://arxiv.org/abs/2405.05295v1",
    "pdf_url": "https://arxiv.org/pdf/2405.05295v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2405.05295",
    "arxiv_authors": [
      "Silvan Mertes",
      "Tobias Huber",
      "Christina Karle",
      "Katharina Weitz",
      "Ruben Schlagowski",
      "Cristina Conati",
      "Elisabeth André"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Relevant+Irrelevance%3A+Generating+Alterfactual+Explanations+for+Image+Classifiers+Silvan+Mertes+Tobias+Huber+Christina+Karle+Katharina+Weitz+Ruben+Schlagowski",
    "gs_search_success": true,
    "gs_authors": [
      "Ytp9oFQAAAAJ",
      "5gnr0JUAAAAJ",
      "TRPACTcAAAAJ",
      "Xr_0Zt4AAAAJ",
      "XqRCYvsAAAAJ",
      "fyEqH-oAAAAJ"
    ],
    "citation_count": 4,
    "gs_author_count": 7
  },
  {
    "arxiv_id": "2503.01339",
    "title": "Wavelet-Enhanced Desnowing: A Novel Single Image Restoration Approach for Traffic Surveillance under Adverse Weather Conditions",
    "year": 2025,
    "published": "2025-03-03T09:23:46Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Image restoration under adverse weather conditions refers to the process of removing degradation caused by weather particles while improving visual quality. Most existing deweathering methods rely on increasing the network scale and data volume to achieve better performance which requires more expensive computing power. Also, many methods lack generalization for specific applications. In the traffic surveillance screener, the main challenges are snow removal and veil effect elimination. In this ",
    "arxiv_url": "https://arxiv.org/abs/2503.01339v1",
    "pdf_url": "https://arxiv.org/pdf/2503.01339v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2503.01339",
    "arxiv_authors": [
      "Zihan Shen",
      "Yu Xuan",
      "Qingyu Yang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Wavelet-Enhanced+Desnowing%3A+A+Novel+Single+Image+Restoration+Approach+for+Traffic+Surveillance+under+Adverse+Weather+Conditions+Zihan+Shen+Yu+Xuan+Qingyu+Yang",
    "gs_search_success": true,
    "gs_authors": [],
    "citation_count": null,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2406.08850",
    "title": "COVE: Unleashing the Diffusion Feature Correspondence for Consistent Video Editing",
    "year": 2024,
    "published": "2024-06-13T06:27:13Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Video editing is an emerging task, in which most current methods adopt the pre-trained text-to-image (T2I) diffusion model to edit the source video in a zero-shot manner. Despite extensive efforts, maintaining the temporal consistency of edited videos remains challenging due to the lack of temporal constraints in the regular T2I diffusion model. To address this issue, we propose COrrespondence-guided Video Editing (COVE), leveraging the inherent diffusion feature correspondence to achieve high-q",
    "arxiv_url": "https://arxiv.org/abs/2406.08850v2",
    "pdf_url": "https://arxiv.org/pdf/2406.08850v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2406.08850",
    "arxiv_authors": [
      "Jiangshan Wang",
      "Yue Ma",
      "Jiayi Guo",
      "Yicheng Xiao",
      "Gao Huang",
      "Xiu Li"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=COVE%3A+Unleashing+the+Diffusion+Feature+Correspondence+for+Consistent+Video+Editing+Jiangshan+Wang+Yue+Ma+Jiayi+Guo+Yicheng+Xiao+Gao+Huang",
    "gs_search_success": true,
    "gs_authors": [
      "2p6GCEEAAAAJ",
      "oakZP0cAAAAJ",
      "HoKoCv0AAAAJ",
      "-P9LwcgAAAAJ",
      "Xrh1OIUAAAAJ"
    ],
    "citation_count": 21,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2407.05671",
    "title": "MSTF: Multiscale Transformer for Incomplete Trajectory Prediction",
    "year": 2024,
    "published": "2024-07-08T07:10:17Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Motion forecasting plays a pivotal role in autonomous driving systems, enabling vehicles to execute collision warnings and rational local-path planning based on predictions of the surrounding vehicles. However, prevalent methods often assume complete observed trajectories, neglecting the potential impact of missing values induced by object occlusion, scope limitation, and sensor failures. Such oversights inevitably compromise the accuracy of trajectory predictions. To tackle this challenge, we p",
    "arxiv_url": "https://arxiv.org/abs/2407.05671v1",
    "pdf_url": "https://arxiv.org/pdf/2407.05671v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2407.05671",
    "arxiv_authors": [
      "Zhanwen Liu",
      "Chao Li",
      "Nan Yang",
      "Yang Wang",
      "Jiaqi Ma",
      "Guangliang Cheng",
      "Xiangmo Zhao"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=MSTF%3A+Multiscale+Transformer+for+Incomplete+Trajectory+Prediction+Zhanwen+Liu+Chao+Li+Nan+Yang+Yang+Wang+Jiaqi+Ma",
    "gs_search_success": true,
    "gs_authors": [
      "S3cQz1AAAAAJ",
      "FToOC-wAAAAJ",
      "7TFKCpQAAAAJ"
    ],
    "citation_count": 3,
    "gs_author_count": 7
  },
  {
    "arxiv_id": "2503.12131",
    "title": "DiffGAP: A Lightweight Diffusion Module in Contrastive Space for Bridging Cross-Model Gap",
    "year": 2025,
    "published": "2025-03-15T13:24:09Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "cs.SD",
      "eess.AS"
    ],
    "abstract": "Recent works in cross-modal understanding and generation, notably through models like CLAP (Contrastive Language-Audio Pretraining) and CAVP (Contrastive Audio-Visual Pretraining), have significantly enhanced the alignment of text, video, and audio embeddings via a single contrastive loss. However, these methods often overlook the bidirectional interactions and inherent noises present in each modality, which can crucially impact the quality and efficacy of cross-modal integration. To address thi",
    "arxiv_url": "https://arxiv.org/abs/2503.12131v1",
    "pdf_url": "https://arxiv.org/pdf/2503.12131v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2503.12131",
    "arxiv_authors": [
      "Shentong Mo",
      "Zehua Chen",
      "Fan Bao",
      "Jun Zhu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=DiffGAP%3A+A+Lightweight+Diffusion+Module+in+Contrastive+Space+for+Bridging+Cross-Model+Gap+Shentong+Mo+Zehua+Chen+Fan+Bao+Jun+Zhu",
    "gs_search_success": false,
    "gs_authors": [],
    "citation_count": null,
    "error_type": "fetch_failed",
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2306.11560",
    "title": "MILD: Modeling the Instance Learning Dynamics for Learning with Noisy Labels",
    "year": 2023,
    "published": "2023-06-20T14:26:53Z",
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "abstract": "Despite deep learning has achieved great success, it often relies on a large amount of training data with accurate labels, which are expensive and time-consuming to collect. A prominent direction to reduce the cost is to learn with noisy labels, which are ubiquitous in the real-world applications. A critical challenge for such a learning task is to reduce the effect of network memorization on the falsely-labeled data. In this work, we propose an iterative selection approach based on the Weibull ",
    "arxiv_url": "https://arxiv.org/abs/2306.11560v2",
    "pdf_url": "https://arxiv.org/pdf/2306.11560v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2306.11560",
    "arxiv_authors": [
      "Chuanyang Hu",
      "Shipeng Yan",
      "Zhitong Gao",
      "Xuming He"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=MILD%3A+Modeling+the+Instance+Learning+Dynamics+for+Learning+with+Noisy+Labels+Chuanyang+Hu+Shipeng+Yan+Zhitong+Gao+Xuming+He",
    "gs_search_success": true,
    "gs_authors": [
      "XAWSPo8AAAAJ",
      "0KyeZ2QAAAAJ",
      "oYILsyoAAAAJ"
    ],
    "citation_count": 9,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2311.07417",
    "title": "Mitigating Backdoors within Deep Neural Networks in Data-limited Configuration",
    "year": 2023,
    "published": "2023-11-13T15:54:27Z",
    "categories": [
      "cs.LG",
      "cs.CR",
      "cs.CV",
      "cs.NE"
    ],
    "abstract": "As the capacity of deep neural networks (DNNs) increases, their need for huge amounts of data significantly grows. A common practice is to outsource the training process or collect more data over the Internet, which introduces the risks of a backdoored DNN. A backdoored DNN shows normal behavior on clean data while behaving maliciously once a trigger is injected into a sample at the test time. In such cases, the defender faces multiple difficulties. First, the available clean dataset may not be ",
    "arxiv_url": "https://arxiv.org/abs/2311.07417v1",
    "pdf_url": "https://arxiv.org/pdf/2311.07417v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2311.07417",
    "arxiv_authors": [
      "Soroush Hashemifar",
      "Saeed Parsa",
      "Morteza Zakeri-Nasrabadi"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Mitigating+Backdoors+within+Deep+Neural+Networks+in+Data-limited+Configuration+Soroush+Hashemifar+Saeed+Parsa+Morteza+Zakeri-Nasrabadi",
    "gs_search_success": true,
    "gs_authors": [
      "km5DzwwAAAAJ",
      "uONxvOwAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2306.04709",
    "title": "Improved statistical benchmarking of digital pathology models using pairwise frames evaluation",
    "year": 2023,
    "published": "2023-06-07T18:21:22Z",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "Nested pairwise frames is a method for relative benchmarking of cell or tissue digital pathology models against manual pathologist annotations on a set of sampled patches. At a high level, the method compares agreement between a candidate model and pathologist annotations with agreement among pathologists' annotations. This evaluation framework addresses fundamental issues of data size and annotator variability in using manual pathologist annotations as a source of ground truth for model validat",
    "arxiv_url": "https://arxiv.org/abs/2306.04709v1",
    "pdf_url": "https://arxiv.org/pdf/2306.04709v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2306.04709",
    "arxiv_authors": [
      "Ylaine Gerardin",
      "John Shamshoian",
      "Judy Shen",
      "Nhat Le",
      "Jamie Prezioso",
      "John Abel",
      "Isaac Finberg",
      "Daniel Borders",
      "Raymond Biju",
      "Michael Nercessian",
      "Vaed Prasad",
      "Joseph Lee",
      "Spencer Wyman",
      "Sid Gupta",
      "Abigail Emerson",
      "Bahar Rahsepar",
      "Darpan Sanghavi",
      "Ryan Leung",
      "Limin Yu",
      "Archit Khosla",
      "Amaro Taylor-Weiner"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Improved+statistical+benchmarking+of+digital+pathology+models+using+pairwise+frames+evaluation+Ylaine+Gerardin+John+Shamshoian+Judy+Shen+Nhat+Le+Jamie+Prezioso",
    "gs_search_success": true,
    "gs_authors": [
      "1AYZ0IAAAAAJ",
      "JbAFruYAAAAJ",
      "KuSKY4AAAAAJ",
      "uP4v3P8AAAAJ",
      "cRWi0w4AAAAJ",
      "9254CckAAAAJ"
    ],
    "citation_count": 7,
    "gs_author_count": 9
  },
  {
    "arxiv_id": "2503.10256",
    "title": "ROODI: Reconstructing Occluded Objects with Denoising Inpainters",
    "year": 2025,
    "published": "2025-03-13T11:16:21Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "While the quality of novel-view images has improved dramatically with 3D Gaussian Splatting, extracting specific objects from scenes remains challenging. Isolating individual 3D Gaussian primitives for each object and handling occlusions in scenes remains far from being solved. We propose a novel object extraction method based on two key principles: (1) object-centric reconstruction through removal of irrelevant primitives; and (2) leveraging generative inpainting to compensate for missing obser",
    "arxiv_url": "https://arxiv.org/abs/2503.10256v2",
    "pdf_url": "https://arxiv.org/pdf/2503.10256v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2503.10256",
    "arxiv_authors": [
      "Yeonjin Chang",
      "Erqun Dong",
      "Seunghyeon Seo",
      "Nojun Kwak",
      "Kwang Moo Yi"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=ROODI%3A+Reconstructing+Occluded+Objects+with+Denoising+Inpainters+Yeonjin+Chang+Erqun+Dong+Seunghyeon+Seo+Nojun+Kwak+Kwang+Moo+Yi",
    "gs_search_success": true,
    "gs_authors": [
      "pr6rIJEAAAAJ",
      "SzxgfTIAAAAJ",
      "h_8-1M0AAAAJ",
      "VA88BFMAAAAJ",
      "LL9u-5IAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2412.08670",
    "title": "A feature refinement module for light-weight semantic segmentation network",
    "year": 2024,
    "published": "2024-12-11T03:31:20Z",
    "categories": [
      "cs.CV",
      "cs.LG",
      "eess.IV"
    ],
    "abstract": "Low computational complexity and high segmentation accuracy are both essential to the real-world semantic segmentation tasks. However, to speed up the model inference, most existing approaches tend to design light-weight networks with a very limited number of parameters, leading to a considerable degradation in accuracy due to the decrease of the representation ability of the networks. To solve the problem, this paper proposes a novel semantic segmentation method to improve the capacity of obtai",
    "arxiv_url": "https://arxiv.org/abs/2412.08670v1",
    "pdf_url": "https://arxiv.org/pdf/2412.08670v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2412.08670",
    "arxiv_authors": [
      "Zhiyan Wang",
      "Xin Guo",
      "Song Wang",
      "Peixiao Zheng",
      "Lin Qi"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=A+feature+refinement+module+for+light-weight+semantic+segmentation+network+Zhiyan+Wang+Xin+Guo+Song+Wang+Peixiao+Zheng+Lin+Qi",
    "gs_search_success": true,
    "gs_authors": [
      "lfxbInoAAAAJ"
    ],
    "citation_count": 6,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2411.04711",
    "title": "Progressive Multi-Level Alignments for Semi-Supervised Domain Adaptation SAR Target Recognition Using Simulated Data",
    "year": 2024,
    "published": "2024-11-07T13:53:13Z",
    "categories": [
      "cs.CV",
      "eess.IV"
    ],
    "abstract": "Recently, an intriguing research trend for automatic target recognition (ATR) from synthetic aperture radar (SAR) imagery has arisen: using simulated data to train ATR models is a feasible solution to the issue of inadequate measured data. To close the domain gap that exists between the real and simulated data, the unsupervised domain adaptation (UDA) techniques are frequently exploited to construct ATR models. However, for UDA, the target domain lacks labeled data to direct the model training, ",
    "arxiv_url": "https://arxiv.org/abs/2411.04711v1",
    "pdf_url": "https://arxiv.org/pdf/2411.04711v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2411.04711",
    "arxiv_authors": [
      "Xinzheng Zhang",
      "Hui Zhu",
      "Hongqian Zhuang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Progressive+Multi-Level+Alignments+for+Semi-Supervised+Domain+Adaptation+SAR+Target+Recognition+Using+Simulated+Data+Xinzheng+Zhang+Hui+Zhu+Hongqian+Zhuang",
    "gs_search_success": true,
    "gs_authors": [],
    "citation_count": 1,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2411.00705",
    "title": "ReMatching Dynamic Reconstruction Flow",
    "year": 2024,
    "published": "2024-11-01T16:09:33Z",
    "categories": [
      "cs.CV",
      "cs.LG",
      "eess.IV"
    ],
    "abstract": "Reconstructing a dynamic scene from image inputs is a fundamental computer vision task with many downstream applications. Despite recent advancements, existing approaches still struggle to achieve high-quality reconstructions from unseen viewpoints and timestamps. This work introduces the ReMatching framework, designed to improve reconstruction quality by incorporating deformation priors into dynamic reconstruction models. Our approach advocates for velocity-field based priors, for which we sugg",
    "arxiv_url": "https://arxiv.org/abs/2411.00705v2",
    "pdf_url": "https://arxiv.org/pdf/2411.00705v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2411.00705",
    "arxiv_authors": [
      "Sara Oblak",
      "Despoina Paschalidou",
      "Sanja Fidler",
      "Matan Atzmon"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=ReMatching+Dynamic+Reconstruction+Flow+Sara+Oblak+Despoina+Paschalidou+Sanja+Fidler+Matan+Atzmon",
    "gs_search_success": true,
    "gs_authors": [
      "zxFlR6sAAAAJ",
      "BXNft08AAAAJ",
      "kHEdjAMAAAAJ",
      "CUlqK5EAAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2407.07479",
    "title": "How to Make Cross Encoder a Good Teacher for Efficient Image-Text Retrieval?",
    "year": 2024,
    "published": "2024-07-10T09:10:01Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Dominant dual-encoder models enable efficient image-text retrieval but suffer from limited accuracy while the cross-encoder models offer higher accuracy at the expense of efficiency. Distilling cross-modality matching knowledge from cross-encoder to dual-encoder provides a natural approach to harness their strengths. Thus we investigate the following valuable question: how to make cross-encoder a good teacher for dual-encoder? Our findings are threefold:(1) Cross-modal similarity score distribut",
    "arxiv_url": "https://arxiv.org/abs/2407.07479v1",
    "pdf_url": "https://arxiv.org/pdf/2407.07479v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2407.07479",
    "arxiv_authors": [
      "Yuxin Chen",
      "Zongyang Ma",
      "Ziqi Zhang",
      "Zhongang Qi",
      "Chunfeng Yuan",
      "Bing Li",
      "Junfu Pu",
      "Ying Shan",
      "Xiaojuan Qi",
      "Weiming Hu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=How+to+Make+Cross+Encoder+a+Good+Teacher+for+Efficient+Image-Text+Retrieval%3F+Yuxin+Chen+Zongyang+Ma+Ziqi+Zhang+Zhongang+Qi+Chunfeng+Yuan",
    "gs_search_success": true,
    "gs_authors": [
      "bGn0uacAAAAJ",
      "Wl4tl4QAAAAJ",
      "qtdueToAAAAJ",
      "_qPX-hcAAAAJ",
      "zJvrrusAAAAJ",
      "dEm4OKAAAAAJ",
      "igML-F8AAAAJ",
      "tddnkNYAAAAJ",
      "G31SX1IAAAAJ",
      "4oXBp9UAAAAJ"
    ],
    "citation_count": 7,
    "gs_author_count": 9
  },
  {
    "arxiv_id": "2403.04151",
    "title": "Dual-path Frequency Discriminators for Few-shot Anomaly Detection",
    "year": 2024,
    "published": "2024-03-07T02:17:59Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Few-shot anomaly detection (FSAD) plays a crucial role in industrial manufacturing. However, existing FSAD methods encounter difficulties leveraging a limited number of normal samples, frequently failing to detect and locate inconspicuous anomalies in the spatial domain. We have further discovered that these subtle anomalies would be more noticeable in the frequency domain. In this paper, we propose a Dual-Path Frequency Discriminators (DFD) network from a frequency perspective to tackle these i",
    "arxiv_url": "https://arxiv.org/abs/2403.04151v4",
    "pdf_url": "https://arxiv.org/pdf/2403.04151v4",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2403.04151",
    "arxiv_authors": [
      "Yuhu Bai",
      "Jiangning Zhang",
      "Zhaofeng Chen",
      "Yuhang Dong",
      "Yunkang Cao",
      "Guanzhong Tian"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Dual-path+Frequency+Discriminators+for+Few-shot+Anomaly+Detection+Yuhu+Bai+Jiangning+Zhang+Zhaofeng+Chen+Yuhang+Dong+Yunkang+Cao",
    "gs_search_success": true,
    "gs_authors": [
      "0q-7PI4AAAAJ",
      "dttX3KsAAAAJ",
      "ucCvgooAAAAJ",
      "aLJ8_G4AAAAJ",
      "2hA4X9wAAAAJ"
    ],
    "citation_count": 13,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2402.06984",
    "title": "Speech motion anomaly detection via cross-modal translation of 4D motion fields from tagged MRI",
    "year": 2024,
    "published": "2024-02-10T16:16:24Z",
    "categories": [
      "cs.SD",
      "cs.CV",
      "cs.MM",
      "eess.AS",
      "eess.IV"
    ],
    "abstract": "Understanding the relationship between tongue motion patterns during speech and their resulting speech acoustic outcomes -- i.e., articulatory-acoustic relation -- is of great importance in assessing speech quality and developing innovative treatment and rehabilitative strategies. This is especially important when evaluating and detecting abnormal articulatory features in patients with speech-related disorders. In this work, we aim to develop a framework for detecting speech motion anomalies in ",
    "arxiv_url": "https://arxiv.org/abs/2402.06984v1",
    "pdf_url": "https://arxiv.org/pdf/2402.06984v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2402.06984",
    "arxiv_authors": [
      "Xiaofeng Liu",
      "Fangxu Xing",
      "Jiachen Zhuo",
      "Maureen Stone",
      "Jerry L. Prince",
      "Georges El Fakhri",
      "Jonghye Woo"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Speech+motion+anomaly+detection+via+cross-modal+translation+of+4D+motion+fields+from+tagged+MRI+Xiaofeng+Liu+Fangxu+Xing+Jiachen+Zhuo+Maureen+Stone+Jerry+L.+Prince",
    "gs_search_success": true,
    "gs_authors": [
      "BQcJMmcAAAAJ",
      "02Ate9UAAAAJ",
      "VighnTUAAAAJ",
      "lgLXQYkAAAAJ",
      "vXq6jPgAAAAJ",
      "QIablCYAAAAJ",
      "yy5SqvgAAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 7
  },
  {
    "arxiv_id": "2406.14941",
    "title": "Brightearth roads: Towards fully automatic road network extraction from satellite imagery",
    "year": 2024,
    "published": "2024-06-21T07:55:15Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "The modern road network topology comprises intricately designed structures that introduce complexity when automatically reconstructing road networks. While open resources like OpenStreetMap (OSM) offer road networks with well-defined topology, they may not always be up to date worldwide. In this paper, we propose a fully automated pipeline for extracting road networks from very-high-resolution (VHR) satellite imagery. Our approach directly generates road line-strings that are seamlessly connecte",
    "arxiv_url": "https://arxiv.org/abs/2406.14941v1",
    "pdf_url": "https://arxiv.org/pdf/2406.14941v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2406.14941",
    "arxiv_authors": [
      "Liuyun Duan",
      "Willard Mapurisa",
      "Maxime Leras",
      "Leigh Lotter",
      "Yuliya Tarabalka"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Brightearth+roads%3A+Towards+fully+automatic+road+network+extraction+from+satellite+imagery+Liuyun+Duan+Willard+Mapurisa+Maxime+Leras+Leigh+Lotter+Yuliya+Tarabalka",
    "gs_search_success": true,
    "gs_authors": [
      "I3nHKIEAAAAJ",
      "N5raVlIAAAAJ",
      "Z8g6GYwAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2309.17448",
    "title": "SMPLer-X: Scaling Up Expressive Human Pose and Shape Estimation",
    "year": 2023,
    "published": "2023-09-29T17:58:06Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Expressive human pose and shape estimation (EHPS) unifies body, hands, and face motion capture with numerous applications. Despite encouraging progress, current state-of-the-art methods still depend largely on a confined set of training datasets. In this work, we investigate scaling up EHPS towards the first generalist foundation model (dubbed SMPLer-X), with up to ViT-Huge as the backbone and training with up to 4.5M instances from diverse data sources. With big data and the large model, SMPLer",
    "arxiv_url": "https://arxiv.org/abs/2309.17448v3",
    "pdf_url": "https://arxiv.org/pdf/2309.17448v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2309.17448",
    "arxiv_authors": [
      "Zhongang Cai",
      "Wanqi Yin",
      "Ailing Zeng",
      "Chen Wei",
      "Qingping Sun",
      "Yanjun Wang",
      "Hui En Pang",
      "Haiyi Mei",
      "Mingyuan Zhang",
      "Lei Zhang",
      "Chen Change Loy",
      "Lei Yang",
      "Ziwei Liu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=SMPLer-X%3A+Scaling+Up+Expressive+Human+Pose+and+Shape+Estimation+Zhongang+Cai+Wanqi+Yin+Ailing+Zeng+Chen+Wei+Qingping+Sun",
    "gs_search_success": true,
    "gs_authors": [
      "szblOQ4AAAAJ",
      "TOZ9wR4AAAAJ",
      "fIlGZToAAAAJ",
      "Tn7fzS8AAAAJ",
      "zlIJwBEAAAAJ",
      "559LF80AAAAJ",
      "X-WP6DYAAAAJ",
      "WrDKqIAAAAAJ",
      "2QLD4fAAAAAJ"
    ],
    "citation_count": 158,
    "gs_author_count": 9
  },
  {
    "arxiv_id": "2311.08284",
    "title": "Level Set KSVD",
    "year": 2023,
    "published": "2023-11-14T16:27:33Z",
    "categories": [
      "cs.CV",
      "eess.IV"
    ],
    "abstract": "We present a new algorithm for image segmentation - Level-set KSVD. Level-set KSVD merges the methods of sparse dictionary learning for feature extraction and variational level-set method for image segmentation. Specifically, we use a generalization of the Chan-Vese functional with features learned by KSVD. The motivation for this model is agriculture based. Aerial images are taken in order to detect the spread of fungi in various crops. Our model is tested on such images of cotton fields. The r",
    "arxiv_url": "https://arxiv.org/abs/2311.08284v1",
    "pdf_url": "https://arxiv.org/pdf/2311.08284v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2311.08284",
    "arxiv_authors": [
      "Omer Sapir",
      "Iftach Klapp",
      "Nir Sochen"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Level+Set+KSVD+Omer+Sapir+Iftach+Klapp+Nir+Sochen",
    "gs_search_success": true,
    "gs_authors": [
      "5xVW95sAAAAJ",
      "5ADhH10AAAAJ",
      "T9UTjEMAAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2404.17867",
    "title": "Are Watermarks Bugs for Deepfake Detectors? Rethinking Proactive Forensics",
    "year": 2024,
    "published": "2024-04-27T11:20:49Z",
    "categories": [
      "cs.CV",
      "eess.IV"
    ],
    "abstract": "AI-generated content has accelerated the topic of media synthesis, particularly Deepfake, which can manipulate our portraits for positive or malicious purposes. Before releasing these threatening face images, one promising forensics solution is the injection of robust watermarks to track their own provenance. However, we argue that current watermarking models, originally devised for genuine images, may harm the deployed Deepfake detectors when directly applied to forged images, since the waterma",
    "arxiv_url": "https://arxiv.org/abs/2404.17867v1",
    "pdf_url": "https://arxiv.org/pdf/2404.17867v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2404.17867",
    "arxiv_authors": [
      "Xiaoshuai Wu",
      "Xin Liao",
      "Bo Ou",
      "Yuling Liu",
      "Zheng Qin"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Are+Watermarks+Bugs+for+Deepfake+Detectors%3F+Rethinking+Proactive+Forensics+Xiaoshuai+Wu+Xin+Liao+Bo+Ou+Yuling+Liu+Zheng+Qin",
    "gs_search_success": false,
    "gs_authors": [],
    "citation_count": null,
    "error_type": "fetch_failed",
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2310.07814",
    "title": "Explorable Mesh Deformation Subspaces from Unstructured Generative Models",
    "year": 2023,
    "published": "2023-10-11T18:53:57Z",
    "categories": [
      "cs.GR",
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "Exploring variations of 3D shapes is a time-consuming process in traditional 3D modeling tools. Deep generative models of 3D shapes often feature continuous latent spaces that can, in principle, be used to explore potential variations starting from a set of input shapes. In practice, doing so can be problematic: latent spaces are high dimensional and hard to visualize, contain shapes that are not relevant to the input shapes, and linear paths through them often lead to sub-optimal shape transiti",
    "arxiv_url": "https://arxiv.org/abs/2310.07814v1",
    "pdf_url": "https://arxiv.org/pdf/2310.07814v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2310.07814",
    "arxiv_authors": [
      "Arman Maesumi",
      "Paul Guerrero",
      "Vladimir G. Kim",
      "Matthew Fisher",
      "Siddhartha Chaudhuri",
      "Noam Aigerman",
      "Daniel Ritchie"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Explorable+Mesh+Deformation+Subspaces+from+Unstructured+Generative+Models+Arman+Maesumi+Paul+Guerrero+Vladimir+G.+Kim+Matthew+Fisher+Siddhartha+Chaudhuri",
    "gs_search_success": true,
    "gs_authors": [
      "5S1kGcAAAAAJ",
      "0RiypNsAAAAJ",
      "QMc-grEAAAAJ",
      "hNjubvkAAAAJ",
      "PUSWc4EAAAAJ",
      "SOJBF1wAAAAJ",
      "eO2xkdMAAAAJ"
    ],
    "citation_count": 9,
    "gs_author_count": 7
  },
  {
    "arxiv_id": "2401.17200",
    "title": "NormEnsembleXAI: Unveiling the Strengths and Weaknesses of XAI Ensemble Techniques",
    "year": 2024,
    "published": "2024-01-30T17:33:35Z",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "abstract": "This paper presents a comprehensive comparative analysis of explainable artificial intelligence (XAI) ensembling methods. Our research brings three significant contributions. Firstly, we introduce a novel ensembling method, NormEnsembleXAI, that leverages minimum, maximum, and average functions in conjunction with normalization techniques to enhance interpretability. Secondly, we offer insights into the strengths and weaknesses of XAI ensemble methods. Lastly, we provide a library, facilitating ",
    "arxiv_url": "https://arxiv.org/abs/2401.17200v1",
    "pdf_url": "https://arxiv.org/pdf/2401.17200v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2401.17200",
    "arxiv_authors": [
      "Weronika Hryniewska-Guzik",
      "Bartosz Sawicki",
      "Przemysław Biecek"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=NormEnsembleXAI%3A+Unveiling+the+Strengths+and+Weaknesses+of+XAI+Ensemble+Techniques+Weronika+Hryniewska-Guzik+Bartosz+Sawicki+Przemys%C5%82aw+Biecek",
    "gs_search_success": true,
    "gs_authors": [
      "aJeg3IQAAAAJ",
      "Af0O75cAAAAJ"
    ],
    "citation_count": 2,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2311.17952",
    "title": "Synchronizing Vision and Language: Bidirectional Token-Masking AutoEncoder for Referring Image Segmentation",
    "year": 2023,
    "published": "2023-11-29T07:33:38Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Referring Image Segmentation (RIS) aims to segment target objects expressed in natural language within a scene at the pixel level. Various recent RIS models have achieved state-of-the-art performance by generating contextual tokens to model multimodal features from pretrained encoders and effectively fusing them using transformer-based cross-modal attention. While these methods match language features with image features to effectively identify likely target objects, they often struggle to corre",
    "arxiv_url": "https://arxiv.org/abs/2311.17952v1",
    "pdf_url": "https://arxiv.org/pdf/2311.17952v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2311.17952",
    "arxiv_authors": [
      "Minhyeok Lee",
      "Dogyoon Lee",
      "Jungho Lee",
      "Suhwan Cho",
      "Heeseung Choi",
      "Ig-Jae Kim",
      "Sangyoun Lee"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Synchronizing+Vision+and+Language%3A+Bidirectional+Token-Masking+AutoEncoder+for+Referring+Image+Segmentation+Minhyeok+Lee+Dogyoon+Lee+Jungho+Lee+Suhwan+Cho+Heeseung+Choi",
    "gs_search_success": true,
    "gs_authors": [
      "pTueCQ8AAAAJ",
      "GbgofCEAAAAJ",
      "ww2vWOcAAAAJ",
      "WGchT7cAAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 7
  },
  {
    "arxiv_id": "2401.17038",
    "title": "Towards Assessing the Synthetic-to-Measured Adversarial Vulnerability of SAR ATR",
    "year": 2024,
    "published": "2024-01-30T14:16:24Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Recently, there has been increasing concern about the vulnerability of deep neural network (DNN)-based synthetic aperture radar (SAR) automatic target recognition (ATR) to adversarial attacks, where a DNN could be easily deceived by clean input with imperceptible but aggressive perturbations. This paper studies the synthetic-to-measured (S2M) transfer setting, where an attacker generates adversarial perturbation based solely on synthetic data and transfers it against victim models trained with m",
    "arxiv_url": "https://arxiv.org/abs/2401.17038v1",
    "pdf_url": "https://arxiv.org/pdf/2401.17038v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2401.17038",
    "arxiv_authors": [
      "Bowen Peng",
      "Bo Peng",
      "Jingyuan Xia",
      "Tianpeng Liu",
      "Yongxiang Liu",
      "Li Liu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Towards+Assessing+the+Synthetic-to-Measured+Adversarial+Vulnerability+of+SAR+ATR+Bowen+Peng+Bo+Peng+Jingyuan+Xia+Tianpeng+Liu+Yongxiang+Liu",
    "gs_search_success": true,
    "gs_authors": [
      "a9tTHSEAAAAJ",
      "9cMQrVsAAAAJ",
      "I8QD_w0AAAAJ",
      "VKJr2mgAAAAJ",
      "Pdt-EUAAAAAJ"
    ],
    "citation_count": 16,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2403.15651",
    "title": "GaNI: Global and Near Field Illumination Aware Neural Inverse Rendering",
    "year": 2024,
    "published": "2024-03-22T23:47:19Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "In this paper, we present GaNI, a Global and Near-field Illumination-aware neural inverse rendering technique that can reconstruct geometry, albedo, and roughness parameters from images of a scene captured with co-located light and camera. Existing inverse rendering techniques with co-located light-camera focus on single objects only, without modeling global illumination and near-field lighting more prominent in scenes with multiple objects. We introduce a system that solves this problem in two ",
    "arxiv_url": "https://arxiv.org/abs/2403.15651v3",
    "pdf_url": "https://arxiv.org/pdf/2403.15651v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2403.15651",
    "arxiv_authors": [
      "Jiaye Wu",
      "Saeed Hadadan",
      "Geng Lin",
      "Matthias Zwicker",
      "David Jacobs",
      "Roni Sengupta"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=GaNI%3A+Global+and+Near+Field+Illumination+Aware+Neural+Inverse+Rendering+Jiaye+Wu+Saeed+Hadadan+Geng+Lin+Matthias+Zwicker+David+Jacobs",
    "gs_search_success": true,
    "gs_authors": [
      "KW0FmzgAAAAJ",
      "2Vh_sboAAAAJ",
      "WH2KmRgAAAAJ",
      "Id8SJl8AAAAJ",
      "g2CuYi4AAAAJ",
      "y8aHV2sAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2503.16257",
    "title": "Plug-and-Play 1.x-Bit KV Cache Quantization for Video Large Language Models",
    "year": 2025,
    "published": "2025-03-20T15:52:43Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Video large language models (VideoLLMs) have demonstrated the capability to process longer video inputs and enable complex reasoning and analysis. However, due to the thousands of visual tokens from the video frames, the key-value (KV) cache can significantly increase memory requirements, becoming a bottleneck for inference speed and memory usage. KV cache quantization is a widely used approach to address this problem. In this paper, we find that 2-bit KV quantization of VideoLLMs can hardly hur",
    "arxiv_url": "https://arxiv.org/abs/2503.16257v2",
    "pdf_url": "https://arxiv.org/pdf/2503.16257v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2503.16257",
    "arxiv_authors": [
      "Keda Tao",
      "Haoxuan You",
      "Yang Sui",
      "Can Qin",
      "Huan Wang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Plug-and-Play+1.x-Bit+KV+Cache+Quantization+for+Video+Large+Language+Models+Keda+Tao+Haoxuan+You+Yang+Sui+Can+Qin+Huan+Wang",
    "gs_search_success": true,
    "gs_authors": [
      "0-On0y4AAAAJ",
      "BhysChMAAAAJ",
      "QCik-YcAAAAJ",
      "Q2W1p6sAAAAJ",
      "ek8xaLUAAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2312.15571",
    "title": "A Survey on Open-Set Image Recognition",
    "year": 2023,
    "published": "2023-12-25T00:30:23Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Open-set image recognition (OSR) aims to both classify known-class samples and identify unknown-class samples in the testing set, which supports robust classifiers in many realistic applications, such as autonomous driving, medical diagnosis, security monitoring, etc. In recent years, open-set recognition methods have achieved more and more attention, since it is usually difficult to obtain holistic information about the open world for model training. In this paper, we aim to summarize the up-to",
    "arxiv_url": "https://arxiv.org/abs/2312.15571v1",
    "pdf_url": "https://arxiv.org/pdf/2312.15571v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2312.15571",
    "arxiv_authors": [
      "Jiayin Sun",
      "Qiulei Dong"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=A+Survey+on+Open-Set+Image+Recognition+Jiayin+Sun+Qiulei+Dong",
    "gs_search_success": true,
    "gs_authors": [
      "NlJP-rMAAAAJ"
    ],
    "citation_count": 11,
    "gs_author_count": 2
  },
  {
    "arxiv_id": "2409.18686",
    "title": "A Novel Unified Architecture for Low-Shot Counting by Detection and Segmentation",
    "year": 2024,
    "published": "2024-09-27T12:20:29Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Low-shot object counters estimate the number of objects in an image using few or no annotated exemplars. Objects are localized by matching them to prototypes, which are constructed by unsupervised image-wide object appearance aggregation. Due to potentially diverse object appearances, the existing approaches often lead to overgeneralization and false positive detections. Furthermore, the best-performing methods train object localization by a surrogate loss, that predicts a unit Gaussian at each ",
    "arxiv_url": "https://arxiv.org/abs/2409.18686v2",
    "pdf_url": "https://arxiv.org/pdf/2409.18686v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2409.18686",
    "arxiv_authors": [
      "Jer Pelhan",
      "Alan Lukežič",
      "Vitjan Zavrtanik",
      "Matej Kristan"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=A+Novel+Unified+Architecture+for+Low-Shot+Counting+by+Detection+and+Segmentation+Jer+Pelhan+Alan+Luke%C5%BEi%C4%8D+Vitjan+Zavrtanik+Matej+Kristan",
    "gs_search_success": true,
    "gs_authors": [
      "pDLR7N8AAAAJ",
      "z_8FrEYAAAAJ",
      "GO-UpVgAAAAJ",
      "1_5im9oAAAAJ"
    ],
    "citation_count": 20,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2311.11171",
    "title": "LOSTU: Fast, Scalable, and Uncertainty-Aware Triangulation",
    "year": 2023,
    "published": "2023-11-18T21:27:04Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "This work proposes a non-iterative, scalable, and statistically optimal way to triangulate called \\texttt{LOSTU}. Unlike triangulation algorithms that minimize the reprojection ($L_2$) error, LOSTU will still provide the maximum likelihood estimate when there are errors in camera pose or parameters. This generic framework is used to contextualize other triangulation methods like the direct linear transform (DLT) or the midpoint. Synthetic experiments show that LOSTU can be substantially faster t",
    "arxiv_url": "https://arxiv.org/abs/2311.11171v2",
    "pdf_url": "https://arxiv.org/pdf/2311.11171v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2311.11171",
    "arxiv_authors": [
      "Sébastien Henry",
      "John A. Christian"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=LOSTU%3A+Fast%2C+Scalable%2C+and+Uncertainty-Aware+Triangulation+S%C3%A9bastien+Henry+John+A.+Christian",
    "gs_search_success": true,
    "gs_authors": [
      "Ae5ThqMAAAAJ",
      "fouJH0cAAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 2
  },
  {
    "arxiv_id": "2503.11439",
    "title": "COIN: Confidence Score-Guided Distillation for Annotation-Free Cell Segmentation",
    "year": 2025,
    "published": "2025-03-14T14:27:24Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Cell instance segmentation (CIS) is crucial for identifying individual cell morphologies in histopathological images, providing valuable insights for biological and medical research. While unsupervised CIS (UCIS) models aim to reduce the heavy reliance on labor-intensive image annotations, they fail to accurately capture cell boundaries, causing missed detections and poor performance. Recognizing the absence of error-free instances as a key limitation, we present COIN (COnfidence score-guided IN",
    "arxiv_url": "https://arxiv.org/abs/2503.11439v4",
    "pdf_url": "https://arxiv.org/pdf/2503.11439v4",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2503.11439",
    "arxiv_authors": [
      "Sanghyun Jo",
      "Seo Jin Lee",
      "Seungwoo Lee",
      "Seohyung Hong",
      "Hyungseok Seo",
      "Kyungsu Kim"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=COIN%3A+Confidence+Score-Guided+Distillation+for+Annotation-Free+Cell+Segmentation+Sanghyun+Jo+Seo+Jin+Lee+Seungwoo+Lee+Seohyung+Hong+Hyungseok+Seo",
    "gs_search_success": true,
    "gs_authors": [
      "xgP6q2YAAAAJ",
      "RbJDbtgAAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2409.11104",
    "title": "Depth-based Privileged Information for Boosting 3D Human Pose Estimation on RGB",
    "year": 2024,
    "published": "2024-09-17T11:59:34Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Despite the recent advances in computer vision research, estimating the 3D human pose from single RGB images remains a challenging task, as multiple 3D poses can correspond to the same 2D projection on the image. In this context, depth data could help to disambiguate the 2D information by providing additional constraints about the distance between objects in the scene and the camera. Unfortunately, the acquisition of accurate depth data is limited to indoor spaces and usually is tied to specific",
    "arxiv_url": "https://arxiv.org/abs/2409.11104v1",
    "pdf_url": "https://arxiv.org/pdf/2409.11104v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2409.11104",
    "arxiv_authors": [
      "Alessandro Simoni",
      "Francesco Marchetti",
      "Guido Borghi",
      "Federico Becattini",
      "Davide Davoli",
      "Lorenzo Garattoni",
      "Gianpiero Francesca",
      "Lorenzo Seidenari",
      "Roberto Vezzani"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Depth-based+Privileged+Information+for+Boosting+3D+Human+Pose+Estimation+on+RGB+Alessandro+Simoni+Francesco+Marchetti+Guido+Borghi+Federico+Becattini+Davide+Davoli",
    "gs_search_success": true,
    "gs_authors": [
      "PxeB2pEAAAAJ",
      "G6Xv6L8AAAAJ",
      "t0sXvkAAAAAJ",
      "mBMXVPAAAAAJ",
      "xpLnx7gAAAAJ",
      "81ovlBIAAAAJ",
      "giSjEBMAAAAJ",
      "8ZQZ_QUAAAAJ"
    ],
    "citation_count": 2,
    "gs_author_count": 9
  },
  {
    "arxiv_id": "2311.18695",
    "title": "Seg2Reg: Differentiable 2D Segmentation to 1D Regression Rendering for 360 Room Layout Reconstruction",
    "year": 2023,
    "published": "2023-11-30T16:42:24Z",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "State-of-the-art single-view 360-degree room layout reconstruction methods formulate the problem as a high-level 1D (per-column) regression task. On the other hand, traditional low-level 2D layout segmentation is simpler to learn and can represent occluded regions, but it requires complex post-processing for the targeting layout polygon and sacrifices accuracy. We present Seg2Reg to render 1D layout depth regression from the 2D segmentation map in a differentiable and occlusion-aware way, marryi",
    "arxiv_url": "https://arxiv.org/abs/2311.18695v1",
    "pdf_url": "https://arxiv.org/pdf/2311.18695v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2311.18695",
    "arxiv_authors": [
      "Cheng Sun",
      "Wei-En Tai",
      "Yu-Lin Shih",
      "Kuan-Wei Chen",
      "Yong-Jing Syu",
      "Kent Selwyn The",
      "Yu-Chiang Frank Wang",
      "Hwann-Tzong Chen"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Seg2Reg%3A+Differentiable+2D+Segmentation+to+1D+Regression+Rendering+for+360+Room+Layout+Reconstruction+Cheng+Sun+Wei-En+Tai+Yu-Lin+Shih+Kuan-Wei+Chen+Yong-Jing+Syu",
    "gs_search_success": true,
    "gs_authors": [
      "mwFy9kkAAAAJ",
      "JP0fuKAAAAAJ"
    ],
    "citation_count": 6,
    "gs_author_count": 8
  },
  {
    "arxiv_id": "2505.17921",
    "title": "Evaluation of Few-Shot Learning Methods for Kidney Stone Type Recognition in Ureteroscopy",
    "year": 2025,
    "published": "2025-05-23T13:59:02Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "abstract": "Determining the type of kidney stones is crucial for prescribing appropriate treatments to prevent recurrence. Currently, various approaches exist to identify the type of kidney stones. However, obtaining results through the reference ex vivo identification procedure can take several weeks, while in vivo visual recognition requires highly trained specialists. For this reason, deep learning models have been developed to provide urologists with an automated classification of kidney stones during u",
    "arxiv_url": "https://arxiv.org/abs/2505.17921v1",
    "pdf_url": "https://arxiv.org/pdf/2505.17921v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2505.17921",
    "arxiv_authors": [
      "Carlos Salazar-Ruiz",
      "Francisco Lopez-Tiro",
      "Ivan Reyes-Amezcua",
      "Clement Larose",
      "Gilberto Ochoa-Ruiz",
      "Christian Daul"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Evaluation+of+Few-Shot+Learning+Methods+for+Kidney+Stone+Type+Recognition+in+Ureteroscopy+Carlos+Salazar-Ruiz+Francisco+Lopez-Tiro+Ivan+Reyes-Amezcua+Clement+Larose+Gilberto+Ochoa-Ruiz",
    "gs_search_success": true,
    "gs_authors": [
      "AlKdFRsAAAAJ",
      "DDtiliwAAAAJ",
      "wTvmLOcAAAAJ",
      "XPH6u74AAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2407.12792",
    "title": "Visually Robust Adversarial Imitation Learning from Videos with Contrastive Learning",
    "year": 2024,
    "published": "2024-06-18T20:56:18Z",
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "abstract": "We propose C-LAIfO, a computationally efficient algorithm designed for imitation learning from videos in the presence of visual mismatch between agent and expert domains. We analyze the problem of imitation from expert videos with visual discrepancies, and introduce a solution for robust latent space estimation using contrastive learning and data augmentation. Provided a visually robust latent space, our algorithm performs imitation entirely within this space using off-policy adversarial imitati",
    "arxiv_url": "https://arxiv.org/abs/2407.12792v2",
    "pdf_url": "https://arxiv.org/pdf/2407.12792v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2407.12792",
    "arxiv_authors": [
      "Vittorio Giammarino",
      "James Queeney",
      "Ioannis Ch. Paschalidis"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Visually+Robust+Adversarial+Imitation+Learning+from+Videos+with+Contrastive+Learning+Vittorio+Giammarino+James+Queeney+Ioannis+Ch.+Paschalidis",
    "gs_search_success": true,
    "gs_authors": [
      "ybOJ8CwAAAAJ",
      "ezcss6YAAAAJ",
      "Es_hZ0QAAAAJ"
    ],
    "citation_count": 5,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2302.10717",
    "title": "Deep Reinforcement Learning for Robotic Pushing and Picking in Cluttered Environment",
    "year": 2023,
    "published": "2023-02-21T15:10:35Z",
    "categories": [
      "cs.RO",
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "In this paper, a novel robotic grasping system is established to automatically pick up objects in cluttered scenes. A composite robotic hand composed of a suction cup and a gripper is designed for grasping the object stably. The suction cup is used for lifting the object from the clutter first and the gripper for grasping the object accordingly. We utilize the affordance map to provide pixel-wise lifting point candidates for the suction cup. To obtain a good affordance map, the active exploratio",
    "arxiv_url": "https://arxiv.org/abs/2302.10717v1",
    "pdf_url": "https://arxiv.org/pdf/2302.10717v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2302.10717",
    "arxiv_authors": [
      "Yuhong Deng",
      "Xiaofeng Guo",
      "Yixuan Wei",
      "Kai Lu",
      "Bin Fang",
      "Di Guo",
      "Huaping Liu",
      "Fuchun Sun"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Deep+Reinforcement+Learning+for+Robotic+Pushing+and+Picking+in+Cluttered+Environment+Yuhong+Deng+Xiaofeng+Guo+Yixuan+Wei+Kai+Lu+Bin+Fang",
    "gs_search_success": true,
    "gs_authors": [
      "Gb2L268AAAAJ",
      "5G47IcIAAAAJ",
      "OCauNHUAAAAJ",
      "KdN7CjEAAAAJ",
      "xwudKb4AAAAJ",
      "XGBg7wEAAAAJ",
      "HXnkIkwAAAAJ"
    ],
    "citation_count": 114,
    "gs_author_count": 8
  },
  {
    "arxiv_id": "2406.12587",
    "title": "Restorer: Removing Multi-Degradation with All-Axis Attention and Prompt Guidance",
    "year": 2024,
    "published": "2024-06-18T13:18:32Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "There are many excellent solutions in image restoration.However, most methods require on training separate models to restore images with different types of degradation.Although existing all-in-one models effectively address multiple types of degradation simultaneously, their performance in real-world scenarios is still constrained by the task confusion problem.In this work, we attempt to address this issue by introducing \\textbf{Restorer}, a novel Transformer-based all-in-one image restoration m",
    "arxiv_url": "https://arxiv.org/abs/2406.12587v2",
    "pdf_url": "https://arxiv.org/pdf/2406.12587v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2406.12587",
    "arxiv_authors": [
      "Jiawei Mao",
      "Juncheng Wu",
      "Yuyin Zhou",
      "Xuesong Yin",
      "Yuanqi Chang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Restorer%3A+Removing+Multi-Degradation+with+All-Axis+Attention+and+Prompt+Guidance+Jiawei+Mao+Juncheng+Wu+Yuyin+Zhou+Xuesong+Yin+Yuanqi+Chang",
    "gs_search_success": true,
    "gs_authors": [],
    "citation_count": null,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2405.02068",
    "title": "Advancing Pre-trained Teacher: Towards Robust Feature Discrepancy for Anomaly Detection",
    "year": 2024,
    "published": "2024-05-03T13:00:22Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "With the wide application of knowledge distillation between an ImageNet pre-trained teacher model and a learnable student model, industrial anomaly detection has witnessed a significant achievement in the past few years. The success of knowledge distillation mainly relies on how to keep the feature discrepancy between the teacher and student model, in which it assumes that: (1) the teacher model can jointly represent two different distributions for the normal and abnormal patterns, while (2) the",
    "arxiv_url": "https://arxiv.org/abs/2405.02068v1",
    "pdf_url": "https://arxiv.org/pdf/2405.02068v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2405.02068",
    "arxiv_authors": [
      "Canhui Tang",
      "Sanping Zhou",
      "Yizhe Li",
      "Yonghao Dong",
      "Le Wang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Advancing+Pre-trained+Teacher%3A+Towards+Robust+Feature+Discrepancy+for+Anomaly+Detection+Canhui+Tang+Sanping+Zhou+Yizhe+Li+Yonghao+Dong+Le+Wang",
    "gs_search_success": true,
    "gs_authors": [
      "TKqkrnUAAAAJ",
      "2Drvv44AAAAJ"
    ],
    "citation_count": 3,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2502.06094",
    "title": "Fair-MoE: Fairness-Oriented Mixture of Experts in Vision-Language Models",
    "year": 2025,
    "published": "2025-02-10T01:45:26Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Fairness is a fundamental principle in medical ethics. Vision Language Models (VLMs) have shown significant potential in the medical field due to their ability to leverage both visual and linguistic contexts, reducing the need for large datasets and enabling the performance of complex tasks. However, the exploration of fairness within VLM applications remains limited. Applying VLMs without a comprehensive analysis of fairness could lead to concerns about equal treatment opportunities and diminis",
    "arxiv_url": "https://arxiv.org/abs/2502.06094v1",
    "pdf_url": "https://arxiv.org/pdf/2502.06094v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2502.06094",
    "arxiv_authors": [
      "Peiran Wang",
      "Linjie Tong",
      "Jiaxiang Liu",
      "Zuozhu Liu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Fair-MoE%3A+Fairness-Oriented+Mixture+of+Experts+in+Vision-Language+Models+Peiran+Wang+Linjie+Tong+Jiaxiang+Liu+Zuozhu+Liu",
    "gs_search_success": true,
    "gs_authors": [
      "z6b1BO0AAAAJ",
      "LLVGPzcAAAAJ",
      "yKHgFP0AAAAJ",
      "h602wLIAAAAJ"
    ],
    "citation_count": 6,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2406.06496",
    "title": "Direct Preference Optimization for Suppressing Hallucinated Prior Exams in Radiology Report Generation",
    "year": 2024,
    "published": "2024-06-10T17:31:36Z",
    "categories": [
      "cs.LG",
      "cs.CL",
      "cs.CV"
    ],
    "abstract": "Recent advances in generative vision-language models (VLMs) have exciting potential implications for AI in radiology, yet VLMs are also known to produce hallucinations, nonsensical text, and other unwanted behaviors that can waste clinicians' time and cause patient harm. Drawing on recent work on direct preference optimization (DPO), we propose a simple method for modifying the behavior of pretrained VLMs performing radiology report generation by suppressing unwanted types of generations. We app",
    "arxiv_url": "https://arxiv.org/abs/2406.06496v2",
    "pdf_url": "https://arxiv.org/pdf/2406.06496v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2406.06496",
    "arxiv_authors": [
      "Oishi Banerjee",
      "Hong-Yu Zhou",
      "Subathra Adithan",
      "Stephen Kwak",
      "Kay Wu",
      "Pranav Rajpurkar"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Direct+Preference+Optimization+for+Suppressing+Hallucinated+Prior+Exams+in+Radiology+Report+Generation+Oishi+Banerjee+Hong-Yu+Zhou+Subathra+Adithan+Stephen+Kwak+Kay+Wu",
    "gs_search_success": true,
    "gs_authors": [
      "3rFppXwAAAAJ",
      "QcOG6sgAAAAJ",
      "ivEDxyYAAAAJ",
      "aJnvh8gAAAAJ",
      "7PLOdWEAAAAJ"
    ],
    "citation_count": 12,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2311.06169",
    "title": "Deep Fast Vision: A Python Library for Accelerated Deep Transfer Learning Vision Prototyping",
    "year": 2023,
    "published": "2023-11-10T16:36:49Z",
    "categories": [
      "cs.CV",
      "cs.LG",
      "eess.IV"
    ],
    "abstract": "Deep learning-based vision is characterized by intricate frameworks that often necessitate a profound understanding, presenting a barrier to newcomers and limiting broad adoption. With many researchers grappling with the constraints of smaller datasets, there's a pronounced reliance on pre-trained neural networks, especially for tasks such as image classification. This reliance is further intensified in niche imaging areas where obtaining vast datasets is challenging. Despite the widespread use ",
    "arxiv_url": "https://arxiv.org/abs/2311.06169v1",
    "pdf_url": "https://arxiv.org/pdf/2311.06169v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2311.06169",
    "arxiv_authors": [
      "Fabi Prezja"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Deep+Fast+Vision%3A+A+Python+Library+for+Accelerated+Deep+Transfer+Learning+Vision+Prototyping+Fabi+Prezja",
    "gs_search_success": true,
    "gs_authors": [
      "imbc-1kAAAAJ"
    ],
    "citation_count": 3,
    "gs_author_count": 1
  },
  {
    "arxiv_id": "2312.11470",
    "title": "An Improved Anomaly Detection Model for Automated Inspection of Power Line Insulators",
    "year": 2023,
    "published": "2023-11-14T11:36:20Z",
    "categories": [
      "cs.CV",
      "eess.IV"
    ],
    "abstract": "Inspection of insulators is important to ensure reliable operation of the power system. Deep learning is being increasingly exploited to automate the inspection process by leveraging object detection models to analyse aerial images captured by drones. A purely object detection-based approach, however, suffers from class imbalance-induced poor performance, which can be accentuated for infrequent and hard-to-detect incipient faults. This article proposes the use of anomaly detection along with obj",
    "arxiv_url": "https://arxiv.org/abs/2312.11470v2",
    "pdf_url": "https://arxiv.org/pdf/2312.11470v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2312.11470",
    "arxiv_authors": [
      "Laya Das",
      "Blazhe Gjorgiev",
      "Giovanni Sansavini"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=An+Improved+Anomaly+Detection+Model+for+Automated+Inspection+of+Power+Line+Insulators+Laya+Das+Blazhe+Gjorgiev+Giovanni+Sansavini",
    "gs_search_success": true,
    "gs_authors": [
      "3xlFqmAAAAAJ",
      "3pDpQIAAAAAJ"
    ],
    "citation_count": 3,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2401.12978",
    "title": "Beyond the Contact: Discovering Comprehensive Affordance for 3D Objects from Pre-trained 2D Diffusion Models",
    "year": 2024,
    "published": "2024-01-23T18:59:59Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Understanding the inherent human knowledge in interacting with a given environment (e.g., affordance) is essential for improving AI to better assist humans. While existing approaches primarily focus on human-object contacts during interactions, such affordance representation cannot fully address other important aspects of human-object interactions (HOIs), i.e., patterns of relative positions and orientations. In this paper, we introduce a novel affordance representation, named Comprehensive Affo",
    "arxiv_url": "https://arxiv.org/abs/2401.12978v3",
    "pdf_url": "https://arxiv.org/pdf/2401.12978v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2401.12978",
    "arxiv_authors": [
      "Hyeonwoo Kim",
      "Sookwan Han",
      "Patrick Kwon",
      "Hanbyul Joo"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Beyond+the+Contact%3A+Discovering+Comprehensive+Affordance+for+3D+Objects+from+Pre-trained+2D+Diffusion+Models+Hyeonwoo+Kim+Sookwan+Han+Patrick+Kwon+Hanbyul+Joo",
    "gs_search_success": false,
    "gs_authors": [],
    "citation_count": null,
    "error_type": "fetch_failed",
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2312.15610",
    "title": "Towards Learning Geometric Eigen-Lengths Crucial for Fitting Tasks",
    "year": 2023,
    "published": "2023-12-25T04:41:52Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Some extremely low-dimensional yet crucial geometric eigen-lengths often determine the success of some geometric tasks. For example, the height of an object is important to measure to check if it can fit between the shelves of a cabinet, while the width of a couch is crucial when trying to move it through a doorway. Humans have materialized such crucial geometric eigen-lengths in common sense since they are very useful in serving as succinct yet effective, highly interpretable, and universal obj",
    "arxiv_url": "https://arxiv.org/abs/2312.15610v1",
    "pdf_url": "https://arxiv.org/pdf/2312.15610v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2312.15610",
    "arxiv_authors": [
      "Yijia Weng",
      "Kaichun Mo",
      "Ruoxi Shi",
      "Yanchao Yang",
      "Leonidas J. Guibas"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Towards+Learning+Geometric+Eigen-Lengths+Crucial+for+Fitting+Tasks+Yijia+Weng+Kaichun+Mo+Ruoxi+Shi+Yanchao+Yang+Leonidas+J.+Guibas",
    "gs_search_success": true,
    "gs_authors": [
      "yeuv8L4AAAAJ",
      "pL7JsOsAAAAJ",
      "r2tKnV4AAAAJ",
      "5JlEyTAAAAAJ",
      "Z7zLvdkAAAAJ"
    ],
    "citation_count": 6,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2312.07922",
    "title": "Memory-Efficient Reversible Spiking Neural Networks",
    "year": 2023,
    "published": "2023-12-13T06:39:49Z",
    "categories": [
      "cs.CV",
      "cs.NE"
    ],
    "abstract": "Spiking neural networks (SNNs) are potential competitors to artificial neural networks (ANNs) due to their high energy-efficiency on neuromorphic hardware. However, SNNs are unfolded over simulation time steps during the training process. Thus, SNNs require much more memory than ANNs, which impedes the training of deeper SNN models. In this paper, we propose the reversible spiking neural network to reduce the memory cost of intermediate activations and membrane potentials during training. Firstl",
    "arxiv_url": "https://arxiv.org/abs/2312.07922v1",
    "pdf_url": "https://arxiv.org/pdf/2312.07922v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2312.07922",
    "arxiv_authors": [
      "Hong Zhang",
      "Yu Zhang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Memory-Efficient+Reversible+Spiking+Neural+Networks+Hong+Zhang+Yu+Zhang",
    "gs_search_success": true,
    "gs_authors": [
      "tpOOC4zBfZ8C"
    ],
    "citation_count": 25,
    "gs_author_count": 2
  },
  {
    "arxiv_id": "2501.18313",
    "title": "Simulation of microstructures and machine learning",
    "year": 2025,
    "published": "2025-01-30T12:43:17Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Machine learning offers attractive solutions to challenging image processing tasks. Tedious development and parametrization of algorithmic solutions can be replaced by training a convolutional neural network or a random forest with a high potential to generalize. However, machine learning methods rely on huge amounts of representative image data along with a ground truth, usually obtained by manual annotation. Thus, limited availability of training data is a critical bottleneck. We discuss two u",
    "arxiv_url": "https://arxiv.org/abs/2501.18313v1",
    "pdf_url": "https://arxiv.org/pdf/2501.18313v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2501.18313",
    "arxiv_authors": [
      "Katja Schladitz",
      "Claudia Redenbach",
      "Tin Barisin",
      "Christian Jung",
      "Natascha Jeziorski",
      "Lovro Bosnar",
      "Juraj Fulir",
      "Petra Gospodnetić"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Simulation+of+microstructures+and+machine+learning+Katja+Schladitz+Claudia+Redenbach+Tin+Barisin+Christian+Jung+Natascha+Jeziorski",
    "gs_search_success": true,
    "gs_authors": [
      "s-iomFQAAAAJ",
      "TlMC7QkAAAAJ",
      "eY2ZV4UAAAAJ",
      "gI8NEa0AAAAJ",
      "yPEWIIAAAAAJ",
      "wCk7C3oAAAAJ"
    ],
    "citation_count": 6,
    "gs_author_count": 8
  },
  {
    "arxiv_id": "2312.06109",
    "title": "Vary: Scaling up the Vision Vocabulary for Large Vision-Language Models",
    "year": 2023,
    "published": "2023-12-11T04:26:17Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Modern Large Vision-Language Models (LVLMs) enjoy the same vision vocabulary -- CLIP, which can cover most common vision tasks. However, for some special vision task that needs dense and fine-grained vision perception, e.g., document-level OCR or chart understanding, especially in non-English scenarios, the CLIP-style vocabulary may encounter low efficiency in tokenizing the vision knowledge and even suffer out-of-vocabulary problem. Accordingly, we propose Vary, an efficient and effective metho",
    "arxiv_url": "https://arxiv.org/abs/2312.06109v1",
    "pdf_url": "https://arxiv.org/pdf/2312.06109v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2312.06109",
    "arxiv_authors": [
      "Haoran Wei",
      "Lingyu Kong",
      "Jinyue Chen",
      "Liang Zhao",
      "Zheng Ge",
      "Jinrong Yang",
      "Jianjian Sun",
      "Chunrui Han",
      "Xiangyu Zhang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Vary%3A+Scaling+up+the+Vision+Vocabulary+for+Large+Vision-Language+Models+Haoran+Wei+Lingyu+Kong+Jinyue+Chen+Liang+Zhao+Zheng+Ge",
    "gs_search_success": true,
    "gs_authors": [
      "8Of_NYQAAAAJ",
      "D6tWz44AAAAJ",
      "yuB-cfoAAAAJ",
      "hJ-VrrIAAAAJ",
      "uJJ5zskAAAAJ",
      "J4naK0MAAAAJ",
      "MVZrGkYAAAAJ"
    ],
    "citation_count": 159,
    "gs_author_count": 9
  },
  {
    "arxiv_id": "2407.20582",
    "title": "Image-based Detection of Segment Misalignment in Multi-mirror Satellites using Transfer Learning",
    "year": 2024,
    "published": "2024-07-30T06:29:40Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "eess.IV"
    ],
    "abstract": "In this paper, we introduce a system based on transfer learning for detecting segment misalignment in multimirror satellites, such as future CubeSat designs and the James Webb Space Telescope (JWST), using image-based methods. When a mirror segment becomes misaligned due to various environmental factors, such as space debris, the images can become distorted with a shifted copy of itself called a \"ghost image\". To detect whether segments are misaligned, we use pre-trained, large-scale image model",
    "arxiv_url": "https://arxiv.org/abs/2407.20582v1",
    "pdf_url": "https://arxiv.org/pdf/2407.20582v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2407.20582",
    "arxiv_authors": [
      "C. Tanner Fredieu",
      "Jonathan Tesch",
      "Andrew Kee",
      "David Redding"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Image-based+Detection+of+Segment+Misalignment+in+Multi-mirror+Satellites+using+Transfer+Learning+C.+Tanner+Fredieu+Jonathan+Tesch+Andrew+Kee+David+Redding",
    "gs_search_success": false,
    "gs_authors": [],
    "citation_count": null,
    "error_type": "fetch_failed",
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2409.12597",
    "title": "LARE: Latent Augmentation using Regional Embedding with Vision-Language Model",
    "year": 2024,
    "published": "2024-09-19T09:21:42Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "In recent years, considerable research has been conducted on vision-language models that handle both image and text data; these models are being applied to diverse downstream tasks, such as \"image-related chat,\" \"image recognition by instruction,\" and \"answering visual questions.\" Vision-language models (VLMs), such as Contrastive Language-Image Pre-training (CLIP), are also high-performance image classifiers that are being developed into domain adaptation methods that can utilize language infor",
    "arxiv_url": "https://arxiv.org/abs/2409.12597v1",
    "pdf_url": "https://arxiv.org/pdf/2409.12597v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2409.12597",
    "arxiv_authors": [
      "Kosuke Sakurai",
      "Tatsuya Ishii",
      "Ryotaro Shimizu",
      "Linxin Song",
      "Masayuki Goto"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=LARE%3A+Latent+Augmentation+using+Regional+Embedding+with+Vision-Language+Model+Kosuke+Sakurai+Tatsuya+Ishii+Ryotaro+Shimizu+Linxin+Song+Masayuki+Goto",
    "gs_search_success": false,
    "gs_authors": [],
    "citation_count": null,
    "error_type": "fetch_failed",
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2410.01395",
    "title": "Toward Zero-Shot Learning for Visual Dehazing of Urological Surgical Robots",
    "year": 2024,
    "published": "2024-10-02T10:16:42Z",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "abstract": "Robot-assisted surgery has profoundly influenced current forms of minimally invasive surgery. However, in transurethral suburethral urological surgical robots, they need to work in a liquid environment. This causes vaporization of the liquid when shearing and heating is performed, resulting in bubble atomization that affects the visual perception of the robot. This can lead to the need for uninterrupted pauses in the surgical procedure, which makes the surgery take longer. To address the atomiza",
    "arxiv_url": "https://arxiv.org/abs/2410.01395v1",
    "pdf_url": "https://arxiv.org/pdf/2410.01395v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2410.01395",
    "arxiv_authors": [
      "Renkai Wu",
      "Xianjin Wang",
      "Pengchen Liang",
      "Zhenyu Zhang",
      "Qing Chang",
      "Hao Tang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Toward+Zero-Shot+Learning+for+Visual+Dehazing+of+Urological+Surgical+Robots+Renkai+Wu+Xianjin+Wang+Pengchen+Liang+Zhenyu+Zhang+Qing+Chang",
    "gs_search_success": true,
    "gs_authors": [
      "4daxK2AAAAAJ",
      "9zJkeEMAAAAJ",
      "sEVeslcAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2410.21108",
    "title": "LiGAR: LiDAR-Guided Hierarchical Transformer for Multi-Modal Group Activity Recognition",
    "year": 2024,
    "published": "2024-10-28T15:11:49Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Group Activity Recognition (GAR) remains challenging in computer vision due to the complex nature of multi-agent interactions. This paper introduces LiGAR, a LIDAR-Guided Hierarchical Transformer for Multi-Modal Group Activity Recognition. LiGAR leverages LiDAR data as a structural backbone to guide the processing of visual and textual information, enabling robust handling of occlusions and complex spatial arrangements. Our framework incorporates a Multi-Scale LIDAR Transformer, Cross-Modal Guid",
    "arxiv_url": "https://arxiv.org/abs/2410.21108v2",
    "pdf_url": "https://arxiv.org/pdf/2410.21108v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2410.21108",
    "arxiv_authors": [
      "Naga Venkata Sai Raviteja Chappa",
      "Khoa Luu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=LiGAR%3A+LiDAR-Guided+Hierarchical+Transformer+for+Multi-Modal+Group+Activity+Recognition+Naga+Venkata+Sai+Raviteja+Chappa+Khoa+Luu",
    "gs_search_success": true,
    "gs_authors": [
      "JPAl8-gAAAAJ",
      "v3HJa0EAAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 2
  },
  {
    "arxiv_id": "2403.11184",
    "title": "DuPL: Dual Student with Trustworthy Progressive Learning for Robust Weakly Supervised Semantic Segmentation",
    "year": 2024,
    "published": "2024-03-17T12:14:34Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Recently, One-stage Weakly Supervised Semantic Segmentation (WSSS) with image-level labels has gained increasing interest due to simplification over its cumbersome multi-stage counterpart. Limited by the inherent ambiguity of Class Activation Map (CAM), we observe that one-stage pipelines often encounter confirmation bias caused by incorrect CAM pseudo-labels, impairing their final segmentation performance. Although recent works discard many unreliable pseudo-labels to implicitly alleviate this ",
    "arxiv_url": "https://arxiv.org/abs/2403.11184v1",
    "pdf_url": "https://arxiv.org/pdf/2403.11184v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2403.11184",
    "arxiv_authors": [
      "Yuanchen Wu",
      "Xichen Ye",
      "Kequan Yang",
      "Jide Li",
      "Xiaoqiang Li"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=DuPL%3A+Dual+Student+with+Trustworthy+Progressive+Learning+for+Robust+Weakly+Supervised+Semantic+Segmentation+Yuanchen+Wu+Xichen+Ye+Kequan+Yang+Jide+Li+Xiaoqiang+Li",
    "gs_search_success": true,
    "gs_authors": [
      "nO67gBoAAAAJ",
      "9FjlR8cAAAAJ",
      "JGm4z4YAAAAJ",
      "EkqyGmoAAAAJ"
    ],
    "citation_count": 323,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2312.06991",
    "title": "Attacking the Loop: Adversarial Attacks on Graph-based Loop Closure Detection",
    "year": 2023,
    "published": "2023-12-12T05:23:15Z",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "abstract": "With the advancement in robotics, it is becoming increasingly common for large factories and warehouses to incorporate visual SLAM (vSLAM) enabled automated robots that operate closely next to humans. This makes any adversarial attacks on vSLAM components potentially detrimental to humans working alongside them. Loop Closure Detection (LCD) is a crucial component in vSLAM that minimizes the accumulation of drift in mapping, since even a small drift can accumulate into a significant drift over ti",
    "arxiv_url": "https://arxiv.org/abs/2312.06991v1",
    "pdf_url": "https://arxiv.org/pdf/2312.06991v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2312.06991",
    "arxiv_authors": [
      "Jonathan J. Y. Kim",
      "Martin Urschler",
      "Patricia J. Riddle",
      "Jorg S. Wicker"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Attacking+the+Loop%3A+Adversarial+Attacks+on+Graph-based+Loop+Closure+Detection+Jonathan+J.+Y.+Kim+Martin+Urschler+Patricia+J.+Riddle+Jorg+S.+Wicker",
    "gs_search_success": true,
    "gs_authors": [
      "HKLMeBIAAAAJ",
      "_Q11KBoAAAAJ",
      "dgC2NwsAAAAJ",
      "rih68CwAAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2307.00477",
    "title": "Query-Efficient Decision-based Black-Box Patch Attack",
    "year": 2023,
    "published": "2023-07-02T05:15:43Z",
    "categories": [
      "cs.CV",
      "cs.CR"
    ],
    "abstract": "Deep neural networks (DNNs) have been showed to be highly vulnerable to imperceptible adversarial perturbations. As a complementary type of adversary, patch attacks that introduce perceptible perturbations to the images have attracted the interest of researchers. Existing patch attacks rely on the architecture of the model or the probabilities of predictions and perform poorly in the decision-based setting, which can still construct a perturbation with the minimal information exposed -- the top-",
    "arxiv_url": "https://arxiv.org/abs/2307.00477v1",
    "pdf_url": "https://arxiv.org/pdf/2307.00477v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2307.00477",
    "arxiv_authors": [
      "Zhaoyu Chen",
      "Bo Li",
      "Shuang Wu",
      "Shouhong Ding",
      "Wenqiang Zhang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Query-Efficient+Decision-based+Black-Box+Patch+Attack+Zhaoyu+Chen+Bo+Li+Shuang+Wu+Shouhong+Ding+Wenqiang+Zhang",
    "gs_search_success": true,
    "gs_authors": [
      "NVzQ87sAAAAJ",
      "OGf40fkAAAAJ",
      "Na9u1wMAAAAJ",
      "1FbMihMAAAAJ"
    ],
    "citation_count": 50,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2403.15675",
    "title": "An active learning model to classify animal species in Hong Kong",
    "year": 2024,
    "published": "2024-03-23T01:42:19Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Camera traps are used by ecologists globally as an efficient and non-invasive method to monitor animals. While it is time-consuming to manually label the collected images, recent advances in deep learning and computer vision has made it possible to automating this process [1]. A major obstacle to this is the generalisability of these models when applying these images to independently collected data from other parts of the world [2]. Here, we use a deep active learning workflow [3], and train a m",
    "arxiv_url": "https://arxiv.org/abs/2403.15675v1",
    "pdf_url": "https://arxiv.org/pdf/2403.15675v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2403.15675",
    "arxiv_authors": [
      "Gareth Lamb",
      "Ching Hei Lo",
      "Jin Wu",
      "Calvin K. F. Lee"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=An+active+learning+model+to+classify+animal+species+in+Hong+Kong+Gareth+Lamb+Ching+Hei+Lo+Jin+Wu+Calvin+K.+F.+Lee",
    "gs_search_success": true,
    "gs_authors": [
      "UEB8pkIAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2307.16361",
    "title": "Benchmarking and Analyzing Robust Point Cloud Recognition: Bag of Tricks for Defending Adversarial Examples",
    "year": 2023,
    "published": "2023-07-31T01:34:24Z",
    "categories": [
      "cs.CV",
      "cs.CR",
      "cs.LG"
    ],
    "abstract": "Deep Neural Networks (DNNs) for 3D point cloud recognition are vulnerable to adversarial examples, threatening their practical deployment. Despite the many research endeavors have been made to tackle this issue in recent years, the diversity of adversarial examples on 3D point clouds makes them more challenging to defend against than those on 2D images. For examples, attackers can generate adversarial examples by adding, shifting, or removing points. Consequently, existing defense strategies are",
    "arxiv_url": "https://arxiv.org/abs/2307.16361v2",
    "pdf_url": "https://arxiv.org/pdf/2307.16361v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2307.16361",
    "arxiv_authors": [
      "Qiufan Ji",
      "Lin Wang",
      "Cong Shi",
      "Shengshan Hu",
      "Yingying Chen",
      "Lichao Sun"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Benchmarking+and+Analyzing+Robust+Point+Cloud+Recognition%3A+Bag+of+Tricks+for+Defending+Adversarial+Examples+Qiufan+Ji+Lin+Wang+Cong+Shi+Shengshan+Hu+Yingying+Chen",
    "gs_search_success": true,
    "gs_authors": [
      "lkAFwJgAAAAJ",
      "EVKeaCEAAAAJ",
      "5MvTNBEAAAAJ",
      "WhGUE7AAAAAJ"
    ],
    "citation_count": 9,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2309.13570",
    "title": "Robust 6DoF Pose Estimation Against Depth Noise and a Comprehensive Evaluation on a Mobile Dataset",
    "year": 2023,
    "published": "2023-09-24T07:06:45Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Robust 6DoF pose estimation with mobile devices is the foundation for applications in robotics, augmented reality, and digital twin localization. In this paper, we extensively investigate the robustness of existing RGBD-based 6DoF pose estimation methods against varying levels of depth sensor noise. We highlight that existing 6DoF pose estimation methods suffer significant performance discrepancies due to depth measurement inaccuracies. In response to the robustness issue, we present a simple an",
    "arxiv_url": "https://arxiv.org/abs/2309.13570v5",
    "pdf_url": "https://arxiv.org/pdf/2309.13570v5",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2309.13570",
    "arxiv_authors": [
      "Zixun Huang",
      "Keling Yao",
      "Seth Z. Zhao",
      "Chuanyu Pan",
      "Allen Y. Yang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Robust+6DoF+Pose+Estimation+Against+Depth+Noise+and+a+Comprehensive+Evaluation+on+a+Mobile+Dataset+Zixun+Huang+Keling+Yao+Seth+Z.+Zhao+Chuanyu+Pan+Allen+Y.+Yang",
    "gs_search_success": true,
    "gs_authors": [
      "wNKoPGAAAAAJ",
      "OfsIi_4AAAAJ",
      "0OsAngEAAAAJ",
      "d2kAaQgAAAAJ"
    ],
    "citation_count": 2,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2501.02669",
    "title": "Generalizing from SIMPLE to HARD Visual Reasoning: Can We Mitigate Modality Imbalance in VLMs?",
    "year": 2025,
    "published": "2025-01-05T21:36:38Z",
    "categories": [
      "cs.CV",
      "cs.CL",
      "cs.LG"
    ],
    "abstract": "Vision Language Models (VLMs) are impressive at visual question answering and image captioning. But they underperform on multi-step visual reasoning -- even compared to LLMs on the same tasks presented in text form -- giving rise to perceptions of modality imbalance or brittleness. Towards a systematic study of such issues, we introduce a synthetic framework for assessing the ability of VLMs to perform algorithmic visual reasoning, comprising three tasks: Table Readout, Grid Navigation, and Visu",
    "arxiv_url": "https://arxiv.org/abs/2501.02669v2",
    "pdf_url": "https://arxiv.org/pdf/2501.02669v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2501.02669",
    "arxiv_authors": [
      "Simon Park",
      "Abhishek Panigrahi",
      "Yun Cheng",
      "Dingli Yu",
      "Anirudh Goyal",
      "Sanjeev Arora"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Generalizing+from+SIMPLE+to+HARD+Visual+Reasoning%3A+Can+We+Mitigate+Modality+Imbalance+in+VLMs%3F+Simon+Park+Abhishek+Panigrahi+Yun+Cheng+Dingli+Yu+Anirudh+Goyal",
    "gs_search_success": true,
    "gs_authors": [
      "kyIuB_MAAAAJ",
      "I4p2ikMAAAAJ",
      "KJLJstYAAAAJ",
      "RUP4S68AAAAJ",
      "oMhp8p8AAAAJ",
      "krrh6OUAAAAJ"
    ],
    "citation_count": 7,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2305.13659",
    "title": "Flare-Aware Cross-modal Enhancement Network for Multi-spectral Vehicle Re-identification",
    "year": 2023,
    "published": "2023-05-23T04:04:24Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Multi-spectral vehicle re-identification aims to address the challenge of identifying vehicles in complex lighting conditions by incorporating complementary visible and infrared information. However, in harsh environments, the discriminative cues in RGB and NIR modalities are often lost due to strong flares from vehicle lamps or sunlight, and existing multi-modal fusion methods are limited in their ability to recover these important cues. To address this problem, we propose a Flare-Aware Cross-m",
    "arxiv_url": "https://arxiv.org/abs/2305.13659v2",
    "pdf_url": "https://arxiv.org/pdf/2305.13659v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2305.13659",
    "arxiv_authors": [
      "Aihua Zheng",
      "Zhiqi Ma",
      "Zi Wang",
      "Chenglong Li"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Flare-Aware+Cross-modal+Enhancement+Network+for+Multi-spectral+Vehicle+Re-identification+Aihua+Zheng+Zhiqi+Ma+Zi+Wang+Chenglong+Li",
    "gs_search_success": true,
    "gs_authors": [
      "F-ndT3UAAAAJ",
      "cpPZNU0AAAAJ"
    ],
    "citation_count": 8,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2406.12142",
    "title": "Slicing Through Bias: Explaining Performance Gaps in Medical Image Analysis using Slice Discovery Methods",
    "year": 2024,
    "published": "2024-06-17T23:08:46Z",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "cs.CY"
    ],
    "abstract": "Machine learning models have achieved high overall accuracy in medical image analysis. However, performance disparities on specific patient groups pose challenges to their clinical utility, safety, and fairness. This can affect known patient groups - such as those based on sex, age, or disease subtype - as well as previously unknown and unlabeled groups. Furthermore, the root cause of such observed performance disparities is often challenging to uncover, hindering mitigation efforts. In this pap",
    "arxiv_url": "https://arxiv.org/abs/2406.12142v2",
    "pdf_url": "https://arxiv.org/pdf/2406.12142v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2406.12142",
    "arxiv_authors": [
      "Vincent Olesen",
      "Nina Weng",
      "Aasa Feragen",
      "Eike Petersen"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Slicing+Through+Bias%3A+Explaining+Performance+Gaps+in+Medical+Image+Analysis+using+Slice+Discovery+Methods+Vincent+Olesen+Nina+Weng+Aasa+Feragen+Eike+Petersen",
    "gs_search_success": true,
    "gs_authors": [
      "Os3OANQAAAAJ",
      "MNDVpoUAAAAJ",
      "juRQtRgAAAAJ"
    ],
    "citation_count": 10,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2303.18219",
    "title": "SemHint-MD: Learning from Noisy Semantic Labels for Self-Supervised Monocular Depth Estimation",
    "year": 2023,
    "published": "2023-03-31T17:20:27Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Without ground truth supervision, self-supervised depth estimation can be trapped in a local minimum due to the gradient-locality issue of the photometric loss. In this paper, we present a framework to enhance depth by leveraging semantic segmentation to guide the network to jump out of the local minimum. Prior works have proposed to share encoders between these two tasks or explicitly align them based on priors like the consistency between edges in the depth and segmentation maps. Yet, these me",
    "arxiv_url": "https://arxiv.org/abs/2303.18219v1",
    "pdf_url": "https://arxiv.org/pdf/2303.18219v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2303.18219",
    "arxiv_authors": [
      "Shan Lin",
      "Yuheng Zhi",
      "Michael C. Yip"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=SemHint-MD%3A+Learning+from+Noisy+Semantic+Labels+for+Self-Supervised+Monocular+Depth+Estimation+Shan+Lin+Yuheng+Zhi+Michael+C.+Yip",
    "gs_search_success": true,
    "gs_authors": [
      "gSYxbCYAAAAJ",
      "HWIoDGYAAAAJ",
      "lbdl88wAAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2309.05818",
    "title": "Rice Plant Disease Detection and Diagnosis using Deep Convolutional Neural Networks and Multispectral Imaging",
    "year": 2023,
    "published": "2023-09-11T20:51:21Z",
    "categories": [
      "cs.CV",
      "eess.IV"
    ],
    "abstract": "Rice is considered a strategic crop in Egypt as it is regularly consumed in the Egyptian people's diet. Even though Egypt is the highest rice producer in Africa with a share of 6 million tons per year, it still imports rice to satisfy its local needs due to production loss, especially due to rice disease. Rice blast disease is responsible for 30% loss in rice production worldwide. Therefore, it is crucial to target limiting yield damage by detecting rice crops diseases in its early stages. This ",
    "arxiv_url": "https://arxiv.org/abs/2309.05818v1",
    "pdf_url": "https://arxiv.org/pdf/2309.05818v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2309.05818",
    "arxiv_authors": [
      "Yara Ali Alnaggar",
      "Ahmad Sebaq",
      "Karim Amer",
      "ElSayed Naeem",
      "Mohamed Elhelw"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Rice+Plant+Disease+Detection+and+Diagnosis+using+Deep+Convolutional+Neural+Networks+and+Multispectral+Imaging+Yara+Ali+Alnaggar+Ahmad+Sebaq+Karim+Amer+ElSayed+Naeem+Mohamed+Elhelw",
    "gs_search_success": true,
    "gs_authors": [
      "-72VxMEAAAAJ",
      "Ixo5qL0AAAAJ"
    ],
    "citation_count": 9,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2304.11521",
    "title": "An Order-Complexity Model for Aesthetic Quality Assessment of Homophony Music Performance",
    "year": 2023,
    "published": "2023-04-23T03:02:24Z",
    "categories": [
      "cs.SD",
      "cs.CV",
      "cs.MM",
      "eess.AS"
    ],
    "abstract": "Although computational aesthetics evaluation has made certain achievements in many fields, its research of music performance remains to be explored. At present, subjective evaluation is still a ultimate method of music aesthetics research, but it will consume a lot of human and material resources. In addition, the music performance generated by AI is still mechanical, monotonous and lacking in beauty. In order to guide the generation task of AI music performance, and to improve the performance e",
    "arxiv_url": "https://arxiv.org/abs/2304.11521v1",
    "pdf_url": "https://arxiv.org/pdf/2304.11521v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2304.11521",
    "arxiv_authors": [
      "Xin Jin",
      "Wu Zhou",
      "Jinyu Wang",
      "Duo Xu",
      "Yiqing Rong",
      "Jialin Sun"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=An+Order-Complexity+Model+for+Aesthetic+Quality+Assessment+of+Homophony+Music+Performance+Xin+Jin+Wu+Zhou+Jinyu+Wang+Duo+Xu+Yiqing+Rong",
    "gs_search_success": true,
    "gs_authors": [
      "UGPgvcUAAAAJ"
    ],
    "citation_count": 3,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2505.12606",
    "title": "Diff-MM: Exploring Pre-trained Text-to-Image Generation Model for Unified Multi-modal Object Tracking",
    "year": 2025,
    "published": "2025-05-19T01:42:13Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Multi-modal object tracking integrates auxiliary modalities such as depth, thermal infrared, event flow, and language to provide additional information beyond RGB images, showing great potential in improving tracking stabilization in complex scenarios. Existing methods typically start from an RGB-based tracker and learn to understand auxiliary modalities only from training data. Constrained by the limited multi-modal training data, the performance of these methods is unsatisfactory. To alleviate",
    "arxiv_url": "https://arxiv.org/abs/2505.12606v1",
    "pdf_url": "https://arxiv.org/pdf/2505.12606v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2505.12606",
    "arxiv_authors": [
      "Shiyu Xuan",
      "Zechao Li",
      "Jinhui Tang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Diff-MM%3A+Exploring+Pre-trained+Text-to-Image+Generation+Model+for+Unified+Multi-modal+Object+Tracking+Shiyu+Xuan+Zechao+Li+Jinhui+Tang",
    "gs_search_success": true,
    "gs_authors": [
      "UyZgrZAAAAAJ",
      "ByBLlEwAAAAJ",
      "L6J2V3sAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2412.06284",
    "title": "Your Data Is Not Perfect: Towards Cross-Domain Out-of-Distribution Detection in Class-Imbalanced Data",
    "year": 2024,
    "published": "2024-12-09T08:03:28Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Previous OOD detection systems only focus on the semantic gap between ID and OOD samples. Besides the semantic gap, we are faced with two additional gaps: the domain gap between source and target domains, and the class-imbalance gap between different classes. In fact, similar objects from different domains should belong to the same class. In this paper, we introduce a realistic yet challenging setting: class-imbalanced cross-domain OOD detection (CCOD), which contains a well-labeled (but usually",
    "arxiv_url": "https://arxiv.org/abs/2412.06284v2",
    "pdf_url": "https://arxiv.org/pdf/2412.06284v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2412.06284",
    "arxiv_authors": [
      "Xiang Fang",
      "Arvind Easwaran",
      "Blaise Genest",
      "Ponnuthurai Nagaratnam Suganthan"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Your+Data+Is+Not+Perfect%3A+Towards+Cross-Domain+Out-of-Distribution+Detection+in+Class-Imbalanced+Data+Xiang+Fang+Arvind+Easwaran+Blaise+Genest+Ponnuthurai+Nagaratnam+Suganthan",
    "gs_search_success": true,
    "gs_authors": [
      "yZNzBU0AAAAJ",
      "B_ouhTgAAAAJ"
    ],
    "citation_count": 43,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2304.01963",
    "title": "Model-corrected learned primal-dual models for fast limited-view photoacoustic tomography",
    "year": 2023,
    "published": "2023-04-04T17:13:22Z",
    "categories": [
      "eess.IV",
      "cs.CV",
      "cs.LG",
      "eess.SP",
      "math.OC",
      "physics.med-ph"
    ],
    "abstract": "Learned iterative reconstructions hold great promise to accelerate tomographic imaging with empirical robustness to model perturbations. Nevertheless, an adoption for photoacoustic tomography is hindered by the need to repeatedly evaluate the computational expensive forward model. Computational feasibility can be obtained by the use of fast approximate models, but a need to compensate model errors arises. In this work we advance the methodological and theoretical basis for model corrections in l",
    "arxiv_url": "https://arxiv.org/abs/2304.01963v1",
    "pdf_url": "https://arxiv.org/pdf/2304.01963v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2304.01963",
    "arxiv_authors": [
      "Andreas Hauptmann",
      "Jenni Poimala"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Model-corrected+learned+primal-dual+models+for+fast+limited-view+photoacoustic+tomography+Andreas+Hauptmann+Jenni+Poimala",
    "gs_search_success": true,
    "gs_authors": [
      "y8nRmTYAAAAJ"
    ],
    "citation_count": 11,
    "gs_author_count": 2
  },
  {
    "arxiv_id": "2402.01393",
    "title": "ALERT-Transformer: Bridging Asynchronous and Synchronous Machine Learning for Real-Time Event-based Spatio-Temporal Data",
    "year": 2024,
    "published": "2024-02-02T13:17:19Z",
    "categories": [
      "cs.CV",
      "cs.LG",
      "cs.NE"
    ],
    "abstract": "We seek to enable classic processing of continuous ultra-sparse spatiotemporal data generated by event-based sensors with dense machine learning models. We propose a novel hybrid pipeline composed of asynchronous sensing and synchronous processing that combines several ideas: (1) an embedding based on PointNet models -- the ALERT module -- that can continuously integrate new and dismiss old events thanks to a leakage mechanism, (2) a flexible readout of the embedded data that allows to feed any ",
    "arxiv_url": "https://arxiv.org/abs/2402.01393v3",
    "pdf_url": "https://arxiv.org/pdf/2402.01393v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2402.01393",
    "arxiv_authors": [
      "Carmen Martin-Turrero",
      "Maxence Bouvier",
      "Manuel Breitenstein",
      "Pietro Zanuttigh",
      "Vincent Parret"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=ALERT-Transformer%3A+Bridging+Asynchronous+and+Synchronous+Machine+Learning+for+Real-Time+Event-based+Spatio-Temporal+Data+Carmen+Martin-Turrero+Maxence+Bouvier+Manuel+Breitenstein+Pietro+Zanuttigh+Vincent+Parret",
    "gs_search_success": true,
    "gs_authors": [
      "AeceaFAAAAAJ",
      "xk2N2wkAAAAJ"
    ],
    "citation_count": 9,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2312.08591",
    "title": "Joint2Human: High-quality 3D Human Generation via Compact Spherical Embedding of 3D Joints",
    "year": 2023,
    "published": "2023-12-14T01:24:22Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "3D human generation is increasingly significant in various applications. However, the direct use of 2D generative methods in 3D generation often results in losing local details, while methods that reconstruct geometry from generated images struggle with global view consistency. In this work, we introduce Joint2Human, a novel method that leverages 2D diffusion models to generate detailed 3D human geometry directly, ensuring both global structure and local details. To achieve this, we employ the F",
    "arxiv_url": "https://arxiv.org/abs/2312.08591v2",
    "pdf_url": "https://arxiv.org/pdf/2312.08591v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2312.08591",
    "arxiv_authors": [
      "Muxin Zhang",
      "Qiao Feng",
      "Zhuo Su",
      "Chao Wen",
      "Zhou Xue",
      "Kun Li"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Joint2Human%3A+High-quality+3D+Human+Generation+via+Compact+Spherical+Embedding+of+3D+Joints+Muxin+Zhang+Qiao+Feng+Zhuo+Su+Chao+Wen+Zhou+Xue",
    "gs_search_success": true,
    "gs_authors": [
      "ECKq3aUAAAAJ",
      "iaqDkqMAAAAJ",
      "Xt3LJUAAAAAJ",
      "dnv5SlkAAAAJ",
      "v8TFZI4AAAAJ"
    ],
    "citation_count": 8,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2409.19960",
    "title": "TROPE: TRaining-Free Object-Part Enhancement for Seamlessly Improving Fine-Grained Zero-Shot Image Captioning",
    "year": 2024,
    "published": "2024-09-30T05:24:01Z",
    "categories": [
      "cs.CV",
      "cs.CL"
    ],
    "abstract": "Zero-shot inference, where pre-trained models perform tasks without specific training data, is an exciting emergent ability of large models like CLIP. Although there has been considerable exploration into enhancing zero-shot abilities in image captioning (IC) for popular datasets such as MSCOCO and Flickr8k, these approaches fall short with fine-grained datasets like CUB, FLO, UCM-Captions, and Sydney-Captions. These datasets require captions to discern between visually and semantically similar ",
    "arxiv_url": "https://arxiv.org/abs/2409.19960v2",
    "pdf_url": "https://arxiv.org/pdf/2409.19960v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2409.19960",
    "arxiv_authors": [
      "Joshua Feinglass",
      "Yezhou Yang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=TROPE%3A+TRaining-Free+Object-Part+Enhancement+for+Seamlessly+Improving+Fine-Grained+Zero-Shot+Image+Captioning+Joshua+Feinglass+Yezhou+Yang",
    "gs_search_success": true,
    "gs_authors": [
      "k2suuZgAAAAJ",
      "V2h3z7oAAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 2
  },
  {
    "arxiv_id": "2406.04338",
    "title": "Physics3D: Learning Physical Properties of 3D Gaussians via Video Diffusion",
    "year": 2024,
    "published": "2024-06-06T17:59:47Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR"
    ],
    "abstract": "In recent years, there has been rapid development in 3D generation models, opening up new possibilities for applications such as simulating the dynamic movements of 3D objects and customizing their behaviors. However, current 3D generative models tend to focus only on surface features such as color and shape, neglecting the inherent physical properties that govern the behavior of objects in the real world. To accurately simulate physics-aligned dynamics, it is essential to predict the physical p",
    "arxiv_url": "https://arxiv.org/abs/2406.04338v3",
    "pdf_url": "https://arxiv.org/pdf/2406.04338v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2406.04338",
    "arxiv_authors": [
      "Fangfu Liu",
      "Hanyang Wang",
      "Shunyu Yao",
      "Shengjun Zhang",
      "Jie Zhou",
      "Yueqi Duan"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Physics3D%3A+Learning+Physical+Properties+of+3D+Gaussians+via+Video+Diffusion+Fangfu+Liu+Hanyang+Wang+Shunyu+Yao+Shengjun+Zhang+Jie+Zhou",
    "gs_search_success": true,
    "gs_authors": [
      "b-4FUVsAAAAJ",
      "ex8jWtUAAAAJ",
      "qDseo3cAAAAJ",
      "wsmc4XcAAAAJ",
      "i4kyLbwAAAAJ"
    ],
    "citation_count": 48,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2407.18534",
    "title": "Boosting Cross-Domain Point Classification via Distilling Relational Priors from 2D Transformers",
    "year": 2024,
    "published": "2024-07-26T06:29:09Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Semantic pattern of an object point cloud is determined by its topological configuration of local geometries. Learning discriminative representations can be challenging due to large shape variations of point sets in local regions and incomplete surface in a global perspective, which can be made even more severe in the context of unsupervised domain adaptation (UDA). In specific, traditional 3D networks mainly focus on local geometric details and ignore the topological structure between local geo",
    "arxiv_url": "https://arxiv.org/abs/2407.18534v2",
    "pdf_url": "https://arxiv.org/pdf/2407.18534v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2407.18534",
    "arxiv_authors": [
      "Longkun Zou",
      "Wanru Zhu",
      "Ke Chen",
      "Lihua Guo",
      "Kailing Guo",
      "Kui Jia",
      "Yaowei Wang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Boosting+Cross-Domain+Point+Classification+via+Distilling+Relational+Priors+from+2D+Transformers+Longkun+Zou+Wanru+Zhu+Ke+Chen+Lihua+Guo+Kailing+Guo",
    "gs_search_success": true,
    "gs_authors": [
      "pbNCoTwAAAAJ",
      "Mf9VHRcAAAAJ",
      "o_DllmIAAAAJ",
      "6y_KKxoAAAAJ",
      "NIpixBcAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 7
  },
  {
    "arxiv_id": "2304.14371",
    "title": "Neural Field Conditioning Strategies for 2D Semantic Segmentation",
    "year": 2023,
    "published": "2023-04-12T15:04:37Z",
    "categories": [
      "cs.CV",
      "cs.LG",
      "eess.IV"
    ],
    "abstract": "Neural fields are neural networks which map coordinates to a desired signal. When a neural field should jointly model multiple signals, and not memorize only one, it needs to be conditioned on a latent code which describes the signal at hand. Despite being an important aspect, there has been little research on conditioning strategies for neural fields. In this work, we explore the use of neural fields as decoders for 2D semantic segmentation. For this task, we compare three conditioning methods,",
    "arxiv_url": "https://arxiv.org/abs/2304.14371v1",
    "pdf_url": "https://arxiv.org/pdf/2304.14371v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2304.14371",
    "arxiv_authors": [
      "Martin Gromniak",
      "Sven Magg",
      "Stefan Wermter"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Neural+Field+Conditioning+Strategies+for+2D+Semantic+Segmentation+Martin+Gromniak+Sven+Magg+Stefan+Wermter",
    "gs_search_success": false,
    "gs_authors": [],
    "citation_count": null,
    "error_type": "fetch_failed",
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2501.15573",
    "title": "Approximate Message Passing for Bayesian Neural Networks",
    "year": 2025,
    "published": "2025-01-26T15:58:42Z",
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "abstract": "Bayesian neural networks (BNNs) offer the potential for reliable uncertainty quantification and interpretability, which are critical for trustworthy AI in high-stakes domains. However, existing methods often struggle with issues such as overconfidence, hyperparameter sensitivity, and posterior collapse, leaving room for alternative approaches. In this work, we advance message passing (MP) for BNNs and present a novel framework that models the predictive posterior as a factor graph. To the best o",
    "arxiv_url": "https://arxiv.org/abs/2501.15573v1",
    "pdf_url": "https://arxiv.org/pdf/2501.15573v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2501.15573",
    "arxiv_authors": [
      "Romeo Sommerfeld",
      "Christian Helms",
      "Ralf Herbrich"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Approximate+Message+Passing+for+Bayesian+Neural+Networks+Romeo+Sommerfeld+Christian+Helms+Ralf+Herbrich",
    "gs_search_success": true,
    "gs_authors": [
      "RuvHkikAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2502.19955",
    "title": "RUBIK: A Structured Benchmark for Image Matching across Geometric Challenges",
    "year": 2025,
    "published": "2025-02-27T10:32:41Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Camera pose estimation is crucial for many computer vision applications, yet existing benchmarks offer limited insight into method limitations across different geometric challenges. We introduce RUBIK, a novel benchmark that systematically evaluates image matching methods across well-defined geometric difficulty levels. Using three complementary criteria - overlap, scale ratio, and viewpoint angle - we organize 16.5K image pairs from nuScenes into 33 difficulty levels. Our comprehensive evaluati",
    "arxiv_url": "https://arxiv.org/abs/2502.19955v1",
    "pdf_url": "https://arxiv.org/pdf/2502.19955v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2502.19955",
    "arxiv_authors": [
      "Thibaut Loiseau",
      "Guillaume Bourmaud"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=RUBIK%3A+A+Structured+Benchmark+for+Image+Matching+across+Geometric+Challenges+Thibaut+Loiseau+Guillaume+Bourmaud",
    "gs_search_success": true,
    "gs_authors": [
      "qDSlhTUAAAAJ",
      "d4v2IYMAAAAJ"
    ],
    "citation_count": 3,
    "gs_author_count": 2
  },
  {
    "arxiv_id": "2505.10565",
    "title": "Depth Anything with Any Prior",
    "year": 2025,
    "published": "2025-05-15T17:59:50Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "This work presents Prior Depth Anything, a framework that combines incomplete but precise metric information in depth measurement with relative but complete geometric structures in depth prediction, generating accurate, dense, and detailed metric depth maps for any scene. To this end, we design a coarse-to-fine pipeline to progressively integrate the two complementary depth sources. First, we introduce pixel-level metric alignment and distance-aware weighting to pre-fill diverse metric priors by",
    "arxiv_url": "https://arxiv.org/abs/2505.10565v1",
    "pdf_url": "https://arxiv.org/pdf/2505.10565v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2505.10565",
    "arxiv_authors": [
      "Zehan Wang",
      "Siyu Chen",
      "Lihe Yang",
      "Jialei Wang",
      "Ziang Zhang",
      "Hengshuang Zhao",
      "Zhou Zhao"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Depth+Anything+with+Any+Prior+Zehan+Wang+Siyu+Chen+Lihe+Yang+Jialei+Wang+Ziang+Zhang",
    "gs_search_success": true,
    "gs_authors": [
      "QX7xv3UAAAAJ",
      "4uE10I0AAAAJ",
      "euXK0lkAAAAJ",
      "OIuFz1gAAAAJ"
    ],
    "citation_count": 7,
    "gs_author_count": 7
  },
  {
    "arxiv_id": "2311.11580",
    "title": "SeaDSC: A video-based unsupervised method for dynamic scene change detection in unmanned surface vehicles",
    "year": 2023,
    "published": "2023-11-20T07:34:01Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Recently, there has been an upsurge in the research on maritime vision, where a lot of works are influenced by the application of computer vision for Unmanned Surface Vehicles (USVs). Various sensor modalities such as camera, radar, and lidar have been used to perform tasks such as object detection, segmentation, object tracking, and motion planning. A large subset of this research is focused on the video analysis, since most of the current vessel fleets contain the camera's onboard for various ",
    "arxiv_url": "https://arxiv.org/abs/2311.11580v1",
    "pdf_url": "https://arxiv.org/pdf/2311.11580v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2311.11580",
    "arxiv_authors": [
      "Linh Trinh",
      "Ali Anwar",
      "Siegfried Mercelis"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=SeaDSC%3A+A+video-based+unsupervised+method+for+dynamic+scene+change+detection+in+unmanned+surface+vehicles+Linh+Trinh+Ali+Anwar+Siegfried+Mercelis",
    "gs_search_success": true,
    "gs_authors": [
      "7dUIGzkAAAAJ",
      "8bC9-DYAAAAJ",
      "FtcBjx8AAAAJ"
    ],
    "citation_count": 6,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2311.15993",
    "title": "Unified Batch Normalization: Identifying and Alleviating the Feature Condensation in Batch Normalization and a Unified Framework",
    "year": 2023,
    "published": "2023-11-27T16:41:31Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Batch Normalization (BN) has become an essential technique in contemporary neural network design, enhancing training stability. Specifically, BN employs centering and scaling operations to standardize features along the batch dimension and uses an affine transformation to recover features. Although standard BN has shown its capability to improve deep neural network training and convergence, it still exhibits inherent limitations in certain cases. Current enhancements to BN typically address only",
    "arxiv_url": "https://arxiv.org/abs/2311.15993v2",
    "pdf_url": "https://arxiv.org/pdf/2311.15993v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2311.15993",
    "arxiv_authors": [
      "Shaobo Wang",
      "Xiangdong Zhang",
      "Dongrui Liu",
      "Junchi Yan"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Unified+Batch+Normalization%3A+Identifying+and+Alleviating+the+Feature+Condensation+in+Batch+Normalization+and+a+Unified+Framework+Shaobo+Wang+Xiangdong+Zhang+Dongrui+Liu+Junchi+Yan",
    "gs_search_success": true,
    "gs_authors": [
      "i0paeq4AAAAJ",
      "5S-TKKoAAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2303.06904",
    "title": "Contextually-rich human affect perception using multimodal scene information",
    "year": 2023,
    "published": "2023-03-13T07:46:41Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "abstract": "The process of human affect understanding involves the ability to infer person specific emotional states from various sources including images, speech, and language. Affect perception from images has predominantly focused on expressions extracted from salient face crops. However, emotions perceived by humans rely on multiple contextual cues including social settings, foreground interactions, and ambient visual scenes. In this work, we leverage pretrained vision-language (VLN) models to extract d",
    "arxiv_url": "https://arxiv.org/abs/2303.06904v1",
    "pdf_url": "https://arxiv.org/pdf/2303.06904v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2303.06904",
    "arxiv_authors": [
      "Digbalay Bose",
      "Rajat Hebbar",
      "Krishna Somandepalli",
      "Shrikanth Narayanan"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Contextually-rich+human+affect+perception+using+multimodal+scene+information+Digbalay+Bose+Rajat+Hebbar+Krishna+Somandepalli+Shrikanth+Narayanan",
    "gs_search_success": true,
    "gs_authors": [
      "Jf6gjLUAAAAJ",
      "8EDHmYkAAAAJ",
      "RwFxqloAAAAJ",
      "ih4OLKoAAAAJ"
    ],
    "citation_count": 7,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2411.07643",
    "title": "xCG: Explainable Cell Graphs for Survival Prediction in Non-Small Cell Lung Cancer",
    "year": 2024,
    "published": "2024-11-12T08:53:49Z",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "Understanding how deep learning models predict oncology patient risk can provide critical insights into disease progression, support clinical decision-making, and pave the way for trustworthy and data-driven precision medicine. Building on recent advances in the spatial modeling of the tumor microenvironment using graph neural networks, we present an explainable cell graph (xCG) approach for survival prediction. We validate our model on a public cohort of imaging mass cytometry (IMC) data for 41",
    "arxiv_url": "https://arxiv.org/abs/2411.07643v1",
    "pdf_url": "https://arxiv.org/pdf/2411.07643v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2411.07643",
    "arxiv_authors": [
      "Marvin Sextro",
      "Gabriel Dernbach",
      "Kai Standvoss",
      "Simon Schallenberg",
      "Frederick Klauschen",
      "Klaus-Robert Müller",
      "Maximilian Alber",
      "Lukas Ruff"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=xCG%3A+Explainable+Cell+Graphs+for+Survival+Prediction+in+Non-Small+Cell+Lung+Cancer+Marvin+Sextro+Gabriel+Dernbach+Kai+Standvoss+Simon+Schallenberg+Frederick+Klauschen",
    "gs_search_success": true,
    "gs_authors": [
      "pnOLBcoAAAAJ",
      "S2JS0ZIAAAAJ",
      "yqaK-D8AAAAJ",
      "FqlYo_IAAAAJ",
      "wcI6UN4AAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 8
  },
  {
    "arxiv_id": "2502.10614",
    "title": "Optimizing CNN Architectures for Advanced Thoracic Disease Classification",
    "year": 2025,
    "published": "2025-02-15T00:27:37Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "abstract": "Machine learning, particularly convolutional neural networks (CNNs), has shown promise in medical image analysis, especially for thoracic disease detection using chest X-ray images. In this study, we evaluate various CNN architectures, including binary classification, multi-label classification, and ResNet50 models, to address challenges like dataset imbalance, variations in image quality, and hidden biases. We introduce advanced preprocessing techniques such as principal component analysis (PCA",
    "arxiv_url": "https://arxiv.org/abs/2502.10614v1",
    "pdf_url": "https://arxiv.org/pdf/2502.10614v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2502.10614",
    "arxiv_authors": [
      "Tejas Mirthipati"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Optimizing+CNN+Architectures+for+Advanced+Thoracic+Disease+Classification+Tejas+Mirthipati",
    "gs_search_success": true,
    "gs_authors": [],
    "citation_count": null,
    "gs_author_count": 1
  },
  {
    "arxiv_id": "2402.15587",
    "title": "A Study of Shape Modeling Against Noise",
    "year": 2024,
    "published": "2024-02-23T20:01:25Z",
    "categories": [
      "cs.CV",
      "stat.ML"
    ],
    "abstract": "Shape modeling is a challenging task with many potential applications in computer vision and medical imaging. There are many shape modeling methods in the literature, each with its advantages and applications. However, many shape modeling methods have difficulties handling shapes that have missing pieces or outliers. In this regard, this paper introduces shape denoising, a fundamental problem in shape modeling that lies at the core of many computer vision and medical imaging applications and has",
    "arxiv_url": "https://arxiv.org/abs/2402.15587v1",
    "pdf_url": "https://arxiv.org/pdf/2402.15587v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2402.15587",
    "arxiv_authors": [
      "Cheng Long",
      "Adrian Barbu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=A+Study+of+Shape+Modeling+Against+Noise+Cheng+Long+Adrian+Barbu",
    "gs_search_success": true,
    "gs_authors": [
      "uFwXovsAAAAJ",
      "VachCHgAAAAJ"
    ],
    "citation_count": 3,
    "gs_author_count": 2
  },
  {
    "arxiv_id": "2305.00678",
    "title": "Rethinking Boundary Detection in Deep Learning Models for Medical Image Segmentation",
    "year": 2023,
    "published": "2023-05-01T06:13:08Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Medical image segmentation is a fundamental task in the community of medical image analysis. In this paper, a novel network architecture, referred to as Convolution, Transformer, and Operator (CTO), is proposed. CTO employs a combination of Convolutional Neural Networks (CNNs), Vision Transformer (ViT), and an explicit boundary detection operator to achieve high recognition accuracy while maintaining an optimal balance between accuracy and efficiency. The proposed CTO follows the standard encode",
    "arxiv_url": "https://arxiv.org/abs/2305.00678v1",
    "pdf_url": "https://arxiv.org/pdf/2305.00678v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2305.00678",
    "arxiv_authors": [
      "Yi Lin",
      "Dong Zhang",
      "Xiao Fang",
      "Yufan Chen",
      "Kwang-Ting Cheng",
      "Hao Chen"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Rethinking+Boundary+Detection+in+Deep+Learning+Models+for+Medical+Image+Segmentation+Yi+Lin+Dong+Zhang+Xiao+Fang+Yufan+Chen+Kwang-Ting+Cheng",
    "gs_search_success": true,
    "gs_authors": [
      "ta0bSFwAAAAJ",
      "zxVy7sIAAAAJ",
      "JaSNrNEAAAAJ",
      "Z_t5DjwAAAAJ",
      "-SgpaF8AAAAJ",
      "wkc1Eh8AAAAJ"
    ],
    "citation_count": 76,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2505.17132",
    "title": "Robustifying Vision-Language Models via Dynamic Token Reweighting",
    "year": 2025,
    "published": "2025-05-22T03:00:39Z",
    "categories": [
      "cs.CV",
      "cs.CL"
    ],
    "abstract": "Large vision-language models (VLMs) are highly vulnerable to jailbreak attacks that exploit visual-textual interactions to bypass safety guardrails. In this paper, we present DTR, a novel inference-time defense that mitigates multimodal jailbreak attacks through optimizing the model's key-value (KV) caches. Rather than relying on curated safety-specific data or costly image-to-text conversion, we introduce a new formulation of the safety-relevant distributional shift induced by the visual modali",
    "arxiv_url": "https://arxiv.org/abs/2505.17132v2",
    "pdf_url": "https://arxiv.org/pdf/2505.17132v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2505.17132",
    "arxiv_authors": [
      "Tanqiu Jiang",
      "Jiacheng Liang",
      "Rongyi Zhu",
      "Jiawei Zhou",
      "Fenglong Ma",
      "Ting Wang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Robustifying+Vision-Language+Models+via+Dynamic+Token+Reweighting+Tanqiu+Jiang+Jiacheng+Liang+Rongyi+Zhu+Jiawei+Zhou+Fenglong+Ma",
    "gs_search_success": true,
    "gs_authors": [
      "UQBgOP4AAAAJ",
      "Qsp7ts0AAAAJ",
      "fgwOGB4AAAAJ",
      "DLJIxNMAAAAJ",
      "cwcBTegAAAAJ"
    ],
    "citation_count": 2,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2504.13593",
    "title": "KAN or MLP? Point Cloud Shows the Way Forward",
    "year": 2025,
    "published": "2025-04-18T09:52:22Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Multi-Layer Perceptrons (MLPs) have become one of the fundamental architectural component in point cloud analysis due to its effective feature learning mechanism. However, when processing complex geometric structures in point clouds, MLPs' fixed activation functions struggle to efficiently capture local geometric features, while suffering from poor parameter efficiency and high model redundancy. In this paper, we propose PointKAN, which applies Kolmogorov-Arnold Networks (KANs) to point cloud an",
    "arxiv_url": "https://arxiv.org/abs/2504.13593v4",
    "pdf_url": "https://arxiv.org/pdf/2504.13593v4",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2504.13593",
    "arxiv_authors": [
      "Yan Shi",
      "Qingdong He",
      "Yijun Liu",
      "Xiaoyu Liu",
      "Jingyong Su"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=KAN+or+MLP%3F+Point+Cloud+Shows+the+Way+Forward+Yan+Shi+Qingdong+He+Yijun+Liu+Xiaoyu+Liu+Jingyong+Su",
    "gs_search_success": true,
    "gs_authors": [
      "T8YbHBwAAAAJ",
      "gUJWww0AAAAJ"
    ],
    "citation_count": 3,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2404.00043",
    "title": "Improve accessibility for Low Vision and Blind people using Machine Learning and Computer Vision",
    "year": 2024,
    "published": "2024-03-24T21:19:17Z",
    "categories": [
      "cs.HC",
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "With the ever-growing expansion of mobile technology worldwide, there is an increasing need for accommodation for those who are disabled. This project explores how machine learning and computer vision could be utilized to improve accessibility for people with visual impairments. There have been many attempts to develop various software that would improve accessibility in the day-to-day lives of blind people. However, applications on the market have low accuracy and only provide audio feedback. T",
    "arxiv_url": "https://arxiv.org/abs/2404.00043v1",
    "pdf_url": "https://arxiv.org/pdf/2404.00043v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2404.00043",
    "arxiv_authors": [
      "Jasur Shukurov"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Improve+accessibility+for+Low+Vision+and+Blind+people+using+Machine+Learning+and+Computer+Vision+Jasur+Shukurov",
    "gs_search_success": true,
    "gs_authors": [],
    "citation_count": 4,
    "gs_author_count": 1
  },
  {
    "arxiv_id": "2311.04698",
    "title": "Examining Common Paradigms in Multi-Task Learning",
    "year": 2023,
    "published": "2023-11-08T14:10:19Z",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "abstract": "While multi-task learning (MTL) has gained significant attention in recent years, its underlying mechanisms remain poorly understood. Recent methods did not yield consistent performance improvements over single task learning (STL) baselines, underscoring the importance of gaining more profound insights about challenges specific to MTL. In our study, we investigate paradigms in MTL in the context of STL: First, the impact of the choice of optimizer has only been mildly investigated in MTL. We sho",
    "arxiv_url": "https://arxiv.org/abs/2311.04698v5",
    "pdf_url": "https://arxiv.org/pdf/2311.04698v5",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2311.04698",
    "arxiv_authors": [
      "Cathrin Elich",
      "Lukas Kirchdorfer",
      "Jan M. Köhler",
      "Lukas Schott"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Examining+Common+Paradigms+in+Multi-Task+Learning+Cathrin+Elich+Lukas+Kirchdorfer+Jan+M.+K%C3%B6hler+Lukas+Schott",
    "gs_search_success": true,
    "gs_authors": [
      "56QNhl4AAAAJ",
      "I4mXKS4AAAAJ",
      "4NuUVSkAAAAJ",
      "pJLq66sAAAAJ"
    ],
    "citation_count": 3,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2301.04904",
    "title": "Lesion-aware Dynamic Kernel for Polyp Segmentation",
    "year": 2023,
    "published": "2023-01-12T09:53:57Z",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "abstract": "Automatic and accurate polyp segmentation plays an essential role in early colorectal cancer diagnosis. However, it has always been a challenging task due to 1) the diverse shape, size, brightness and other appearance characteristics of polyps, 2) the tiny contrast between concealed polyps and their surrounding regions. To address these problems, we propose a lesion-aware dynamic network (LDNet) for polyp segmentation, which is a traditional u-shape encoder-decoder structure incorporated with a ",
    "arxiv_url": "https://arxiv.org/abs/2301.04904v1",
    "pdf_url": "https://arxiv.org/pdf/2301.04904v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2301.04904",
    "arxiv_authors": [
      "Ruifei Zhang",
      "Peiwen Lai",
      "Xiang Wan",
      "De-Jun Fan",
      "Feng Gao",
      "Xiao-Jian Wu",
      "Guanbin Li"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Lesion-aware+Dynamic+Kernel+for+Polyp+Segmentation+Ruifei+Zhang+Peiwen+Lai+Xiang+Wan+De-Jun+Fan+Feng+Gao",
    "gs_search_success": true,
    "gs_authors": [
      "e3_kWigAAAAJ",
      "2A2Bx2UAAAAJ",
      "deiLMp8AAAAJ",
      "W4zOhmEAAAAJ"
    ],
    "citation_count": 154,
    "gs_author_count": 7
  },
  {
    "arxiv_id": "2505.16915",
    "title": "DetailMaster: Can Your Text-to-Image Model Handle Long Prompts?",
    "year": 2025,
    "published": "2025-05-22T17:11:27Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "While recent text-to-image (T2I) models show impressive capabilities in synthesizing images from brief descriptions, their performance significantly degrades when confronted with long, detail-intensive prompts required in professional applications. We present DetailMaster, the first comprehensive benchmark specifically designed to evaluate T2I models' systematic abilities to handle extended textual inputs that contain complex compositional requirements. Our benchmark introduces four critical eva",
    "arxiv_url": "https://arxiv.org/abs/2505.16915v2",
    "pdf_url": "https://arxiv.org/pdf/2505.16915v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2505.16915",
    "arxiv_authors": [
      "Qirui Jiao",
      "Daoyuan Chen",
      "Yilun Huang",
      "Xika Lin",
      "Ying Shen",
      "Yaliang Li"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=DetailMaster%3A+Can+Your+Text-to-Image+Model+Handle+Long+Prompts%3F+Qirui+Jiao+Daoyuan+Chen+Yilun+Huang+Xika+Lin+Ying+Shen",
    "gs_search_success": true,
    "gs_authors": [
      "1GdfinUAAAAJ",
      "4nOOybQAAAAJ",
      "bAWeGQUAAAAJ",
      "CCPBcdYAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2504.05741",
    "title": "DDT: Decoupled Diffusion Transformer",
    "year": 2025,
    "published": "2025-04-08T07:17:45Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Diffusion transformers have demonstrated remarkable generation quality, albeit requiring longer training iterations and numerous inference steps. In each denoising step, diffusion transformers encode the noisy inputs to extract the lower-frequency semantic component and then decode the higher frequency with identical modules. This scheme creates an inherent optimization dilemma: encoding low-frequency semantics necessitates reducing high-frequency components, creating tension between semantic en",
    "arxiv_url": "https://arxiv.org/abs/2504.05741v2",
    "pdf_url": "https://arxiv.org/pdf/2504.05741v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2504.05741",
    "arxiv_authors": [
      "Shuai Wang",
      "Zhi Tian",
      "Weilin Huang",
      "Limin Wang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=DDT%3A+Decoupled+Diffusion+Transformer+Shuai+Wang+Zhi+Tian+Weilin+Huang+Limin+Wang",
    "gs_search_success": true,
    "gs_authors": [
      "xSF3BBoAAAAJ",
      "HEuN8PcAAAAJ",
      "78vU1IUAAAAJ",
      "us4prRUAAAAJ"
    ],
    "citation_count": 12,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2310.16684",
    "title": "Local Statistics for Generative Image Detection",
    "year": 2023,
    "published": "2023-10-25T14:47:32Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Diffusion models (DMs) are generative models that learn to synthesize images from Gaussian noise. DMs can be trained to do a variety of tasks such as image generation and image super-resolution. Researchers have made significant improvements in the capability of synthesizing photorealistic images in the past few years. These successes also hasten the need to address the potential misuse of synthesized images. In this paper, we highlighted the effectiveness of Bayer pattern and local statistics i",
    "arxiv_url": "https://arxiv.org/abs/2310.16684v2",
    "pdf_url": "https://arxiv.org/pdf/2310.16684v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2310.16684",
    "arxiv_authors": [
      "Yung Jer Wong",
      "Teck Khim Ng"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Local+Statistics+for+Generative+Image+Detection+Yung+Jer+Wong+Teck+Khim+Ng",
    "gs_search_success": true,
    "gs_authors": [
      "koHhid0AAAAJ"
    ],
    "citation_count": 2,
    "gs_author_count": 2
  },
  {
    "arxiv_id": "2407.11316",
    "title": "BUSClean: Open-source software for breast ultrasound image pre-processing and knowledge extraction for medical AI",
    "year": 2024,
    "published": "2024-07-16T02:02:51Z",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "abstract": "Development of artificial intelligence (AI) for medical imaging demands curation and cleaning of large-scale clinical datasets comprising hundreds of thousands of images. Some modalities, such as mammography, contain highly standardized imaging. In contrast, breast ultrasound imaging (BUS) can contain many irregularities not indicated by scan metadata, such as enhanced scan modes, sonographer annotations, or additional views. We present an open-source software solution for automatically processi",
    "arxiv_url": "https://arxiv.org/abs/2407.11316v3",
    "pdf_url": "https://arxiv.org/pdf/2407.11316v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2407.11316",
    "arxiv_authors": [
      "Arianna Bunnell",
      "Kailee Hung",
      "John A. Shepherd",
      "Peter Sadowski"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=BUSClean%3A+Open-source+software+for+breast+ultrasound+image+pre-processing+and+knowledge+extraction+for+medical+AI+Arianna+Bunnell+Kailee+Hung+John+A.+Shepherd+Peter+Sadowski",
    "gs_search_success": true,
    "gs_authors": [
      "1r51d_AAAAAJ",
      "5-s3bS8AAAAJ",
      "Hm4FEQYAAAAJ"
    ],
    "citation_count": 2,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2312.07549",
    "title": "Make-A-Storyboard: A General Framework for Storyboard with Disentangled and Merged Control",
    "year": 2023,
    "published": "2023-12-06T12:16:23Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Story Visualization aims to generate images aligned with story prompts, reflecting the coherence of storybooks through visual consistency among characters and scenes.Whereas current approaches exclusively concentrate on characters and neglect the visual consistency among contextually correlated scenes, resulting in independent character images without inter-image coherence.To tackle this issue, we propose a new presentation form for Story Visualization called Storyboard, inspired by film-making,",
    "arxiv_url": "https://arxiv.org/abs/2312.07549v1",
    "pdf_url": "https://arxiv.org/pdf/2312.07549v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2312.07549",
    "arxiv_authors": [
      "Sitong Su",
      "Litao Guo",
      "Lianli Gao",
      "Heng Tao Shen",
      "Jingkuan Song"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Make-A-Storyboard%3A+A+General+Framework+for+Storyboard+with+Disentangled+and+Merged+Control+Sitong+Su+Litao+Guo+Lianli+Gao+Heng+Tao+Shen+Jingkuan+Song",
    "gs_search_success": false,
    "gs_authors": [],
    "citation_count": null,
    "error_type": "fetch_failed",
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2304.05758",
    "title": "Best Practices for 2-Body Pose Forecasting",
    "year": 2023,
    "published": "2023-04-12T10:46:23Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "The task of collaborative human pose forecasting stands for predicting the future poses of multiple interacting people, given those in previous frames. Predicting two people in interaction, instead of each separately, promises better performance, due to their body-body motion correlations. But the task has remained so far primarily unexplored.   In this paper, we review the progress in human pose forecasting and provide an in-depth assessment of the single-person practices that perform best for ",
    "arxiv_url": "https://arxiv.org/abs/2304.05758v1",
    "pdf_url": "https://arxiv.org/pdf/2304.05758v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2304.05758",
    "arxiv_authors": [
      "Muhammad Rameez Ur Rahman",
      "Luca Scofano",
      "Edoardo De Matteis",
      "Alessandro Flaborea",
      "Alessio Sampieri",
      "Fabio Galasso"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Best+Practices+for+2-Body+Pose+Forecasting+Muhammad+Rameez+Ur+Rahman+Luca+Scofano+Edoardo+De+Matteis+Alessandro+Flaborea+Alessio+Sampieri",
    "gs_search_success": true,
    "gs_authors": [
      "HHDHIVoAAAAJ",
      "lzFb_ZUAAAAJ",
      "2gSuGBEAAAAJ",
      "LWFuG5sAAAAJ",
      "Tjt7BFEAAAAJ",
      "2nLw-cgAAAAJ"
    ],
    "citation_count": 9,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2311.15658",
    "title": "Regularization by Texts for Latent Diffusion Inverse Solvers",
    "year": 2023,
    "published": "2023-11-27T09:40:14Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "abstract": "The recent development of diffusion models has led to significant progress in solving inverse problems by leveraging these models as powerful generative priors. However, challenges persist due to the ill-posed nature of such problems, often arising from ambiguities in measurements or intrinsic system symmetries. To address this, here we introduce a novel latent diffusion inverse solver, regularization by text (TReg), inspired by the human ability to resolve visual ambiguities through perceptual ",
    "arxiv_url": "https://arxiv.org/abs/2311.15658v3",
    "pdf_url": "https://arxiv.org/pdf/2311.15658v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2311.15658",
    "arxiv_authors": [
      "Jeongsol Kim",
      "Geon Yeong Park",
      "Hyungjin Chung",
      "Jong Chul Ye"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Regularization+by+Texts+for+Latent+Diffusion+Inverse+Solvers+Jeongsol+Kim+Geon+Yeong+Park+Hyungjin+Chung+Jong+Chul+Ye",
    "gs_search_success": true,
    "gs_authors": [
      "KdchEyoAAAAJ",
      "HNMjoNEAAAAJ",
      "ZaVNwcQAAAAJ",
      "HGF4a14AAAAJ"
    ],
    "citation_count": 22,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2505.02176",
    "title": "Saliency-Guided Training for Fingerprint Presentation Attack Detection",
    "year": 2025,
    "published": "2025-05-04T16:35:13Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Saliency-guided training, which directs model learning to important regions of images, has demonstrated generalization improvements across various biometric presentation attack detection (PAD) tasks. This paper presents its first application to fingerprint PAD. We conducted a 50-participant study to create a dataset of 800 human-annotated fingerprint perceptually-important maps, explored alongside algorithmically-generated \"pseudosaliency,\" including minutiae-based, image quality-based, and auto",
    "arxiv_url": "https://arxiv.org/abs/2505.02176v2",
    "pdf_url": "https://arxiv.org/pdf/2505.02176v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2505.02176",
    "arxiv_authors": [
      "Samuel Webster",
      "Adam Czajka"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Saliency-Guided+Training+for+Fingerprint+Presentation+Attack+Detection+Samuel+Webster+Adam+Czajka",
    "gs_search_success": true,
    "gs_authors": [
      "TJohAfIAAAAJ",
      "_OObpEkAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 2
  },
  {
    "arxiv_id": "2402.12525",
    "title": "LangXAI: Integrating Large Vision Models for Generating Textual Explanations to Enhance Explainability in Visual Perception Tasks",
    "year": 2024,
    "published": "2024-02-19T20:36:32Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "LangXAI is a framework that integrates Explainable Artificial Intelligence (XAI) with advanced vision models to generate textual explanations for visual recognition tasks. Despite XAI advancements, an understanding gap persists for end-users with limited domain knowledge in artificial intelligence and computer vision. LangXAI addresses this by furnishing text-based explanations for classification, object detection, and semantic segmentation model outputs to end-users. Preliminary results demonst",
    "arxiv_url": "https://arxiv.org/abs/2402.12525v1",
    "pdf_url": "https://arxiv.org/pdf/2402.12525v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2402.12525",
    "arxiv_authors": [
      "Truong Thanh Hung Nguyen",
      "Tobias Clement",
      "Phuc Truong Loc Nguyen",
      "Nils Kemmerzell",
      "Van Binh Truong",
      "Vo Thanh Khang Nguyen",
      "Mohamed Abdelaal",
      "Hung Cao"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=LangXAI%3A+Integrating+Large+Vision+Models+for+Generating+Textual+Explanations+to+Enhance+Explainability+in+Visual+Perception+Tasks+Truong+Thanh+Hung+Nguyen+Tobias+Clement+Phuc+Truong+Loc+Nguyen+Nils+Kemmerzell+Van+Binh+Truong",
    "gs_search_success": true,
    "gs_authors": [
      "P6lKhmIAAAAJ",
      "ddyCwqIAAAAJ",
      "TTFiKAgAAAAJ",
      "kh9KljsAAAAJ",
      "cmxDCFAAAAAJ",
      "U_v7A40AAAAJ",
      "uQAegakAAAAJ"
    ],
    "citation_count": 19,
    "gs_author_count": 8
  },
  {
    "arxiv_id": "2407.02625",
    "title": "Lung-CADex: Fully automatic Zero-Shot Detection and Classification of Lung Nodules in Thoracic CT Images",
    "year": 2024,
    "published": "2024-07-02T19:30:25Z",
    "categories": [
      "eess.IV",
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "Lung cancer has been one of the major threats to human life for decades. Computer-aided diagnosis can help with early lung nodul detection and facilitate subsequent nodule characterization. Large Visual Language models (VLMs) have been found effective for multiple downstream medical tasks that rely on both imaging and text data. However, lesion level detection and subsequent diagnosis using VLMs have not been explored yet. We propose CADe, for segmenting lung nodules in a zero-shot manner using ",
    "arxiv_url": "https://arxiv.org/abs/2407.02625v1",
    "pdf_url": "https://arxiv.org/pdf/2407.02625v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2407.02625",
    "arxiv_authors": [
      "Furqan Shaukat",
      "Syed Muhammad Anwar",
      "Abhijeet Parida",
      "Van Khanh Lam",
      "Marius George Linguraru",
      "Mubarak Shah"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Lung-CADex%3A+Fully+automatic+Zero-Shot+Detection+and+Classification+of+Lung+Nodules+in+Thoracic+CT+Images+Furqan+Shaukat+Syed+Muhammad+Anwar+Abhijeet+Parida+Van+Khanh+Lam+Marius+George+Linguraru",
    "gs_search_success": true,
    "gs_authors": [
      "5cqvsA4AAAAJ",
      "AqEhhQoAAAAJ",
      "TAues7sAAAAJ",
      "p8gsO3gAAAAJ",
      "sWYYUzkAAAAJ"
    ],
    "citation_count": 8,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2409.14538",
    "title": "Towards Model-Agnostic Dataset Condensation by Heterogeneous Models",
    "year": 2024,
    "published": "2024-09-22T17:13:07Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Abstract. The advancement of deep learning has coincided with the proliferation of both models and available data. The surge in dataset sizes and the subsequent surge in computational requirements have led to the development of the Dataset Condensation (DC). While prior studies have delved into generating synthetic images through methods like distribution alignment and training trajectory tracking for more efficient model training, a significant challenge arises when employing these condensed im",
    "arxiv_url": "https://arxiv.org/abs/2409.14538v1",
    "pdf_url": "https://arxiv.org/pdf/2409.14538v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2409.14538",
    "arxiv_authors": [
      "Jun-Yeong Moon",
      "Jung Uk Kim",
      "Gyeong-Moon Park"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Towards+Model-Agnostic+Dataset+Condensation+by+Heterogeneous+Models+Jun-Yeong+Moon+Jung+Uk+Kim+Gyeong-Moon+Park",
    "gs_search_success": true,
    "gs_authors": [
      "JMZ80R8AAAAJ",
      "Sz6rfOMAAAAJ",
      "WAK5QsgAAAAJ"
    ],
    "citation_count": 2,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2303.00818",
    "title": "Improving Model's Focus Improves Performance of Deep Learning-Based Synthetic Face Detectors",
    "year": 2023,
    "published": "2023-03-01T20:39:46Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Deep learning-based models generalize better to unknown data samples after being guided \"where to look\" by incorporating human perception into training strategies. We made an observation that the entropy of the model's salience trained in that way is lower when compared to salience entropy computed for models training without human perceptual intelligence. Thus the question: does further increase of model's focus, by lowering the entropy of model's class activation map, help in further increasin",
    "arxiv_url": "https://arxiv.org/abs/2303.00818v1",
    "pdf_url": "https://arxiv.org/pdf/2303.00818v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2303.00818",
    "arxiv_authors": [
      "Jacob Piland",
      "Adam Czajka",
      "Christopher Sweet"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Improving+Model%27s+Focus+Improves+Performance+of+Deep+Learning-Based+Synthetic+Face+Detectors+Jacob+Piland+Adam+Czajka+Christopher+Sweet",
    "gs_search_success": true,
    "gs_authors": [
      "0NhA_wEAAAAJ",
      "_OObpEkAAAAJ",
      "SpHCsEwAAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2503.00881",
    "title": "Evolving High-Quality Rendering and Reconstruction in a Unified Framework with Contribution-Adaptive Regularization",
    "year": 2025,
    "published": "2025-03-02T12:51:38Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Representing 3D scenes from multiview images is a core challenge in computer vision and graphics, which requires both precise rendering and accurate reconstruction. Recently, 3D Gaussian Splatting (3DGS) has garnered significant attention for its high-quality rendering and fast inference speed. Yet, due to the unstructured and irregular nature of Gaussian point clouds, ensuring accurate geometry reconstruction remains difficult. Existing methods primarily focus on geometry regularization, with c",
    "arxiv_url": "https://arxiv.org/abs/2503.00881v1",
    "pdf_url": "https://arxiv.org/pdf/2503.00881v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2503.00881",
    "arxiv_authors": [
      "You Shen",
      "Zhipeng Zhang",
      "Xinyang Li",
      "Yansong Qu",
      "Yu Lin",
      "Shengchuan Zhang",
      "Liujuan Cao"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Evolving+High-Quality+Rendering+and+Reconstruction+in+a+Unified+Framework+with+Contribution-Adaptive+Regularization+You+Shen+Zhipeng+Zhang+Xinyang+Li+Yansong+Qu+Yu+Lin",
    "gs_search_success": true,
    "gs_authors": [
      "GToqXScAAAAJ",
      "7Ws0QHYAAAAJ",
      "M9rwkHwAAAAJ",
      "zBLDzs4AAAAJ",
      "O3xSoK8AAAAJ"
    ],
    "citation_count": 7,
    "gs_author_count": 7
  },
  {
    "arxiv_id": "2505.19873",
    "title": "Deep Spectral Prior",
    "year": 2025,
    "published": "2025-05-26T12:00:37Z",
    "categories": [
      "cs.CV",
      "math.NA"
    ],
    "abstract": "We introduce the Deep Spectral Prior (DSP), a new framework for unsupervised image reconstruction that operates entirely in the complex frequency domain. Unlike the Deep Image Prior (DIP), which optimises pixel-level errors and is highly sensitive to overfitting, DSP performs joint learning of amplitude and phase to capture the full spectral structure of images. We derive a rigorous theoretical characterisation of DSP's optimisation dynamics, proving that it follows frequency-dependent descent t",
    "arxiv_url": "https://arxiv.org/abs/2505.19873v2",
    "pdf_url": "https://arxiv.org/pdf/2505.19873v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2505.19873",
    "arxiv_authors": [
      "Yanqi Cheng",
      "Xuxiang Zhao",
      "Tieyong Zeng",
      "Pietro Lio",
      "Carola-Bibiane Schönlieb",
      "Angelica I Aviles-Rivero"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Deep+Spectral+Prior+Yanqi+Cheng+Xuxiang+Zhao+Tieyong+Zeng+Pietro+Lio+Carola-Bibiane+Sch%C3%B6nlieb",
    "gs_search_success": true,
    "gs_authors": [
      "2yyTgRwAAAAJ",
      "q5AA4lEAAAAJ",
      "nPeOXjwAAAAJ",
      "4YhNJBEAAAAJ",
      "jdFk7ukAAAAJ",
      "b8nRcGEAAAAJ"
    ],
    "citation_count": 2,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2303.10383",
    "title": "Adaptive Multi-source Predictor for Zero-shot Video Object Segmentation",
    "year": 2023,
    "published": "2023-03-18T10:19:29Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Static and moving objects often occur in real-life videos. Most video object segmentation methods only focus on extracting and exploiting motion cues to perceive moving objects. Once faced with the frames of static objects, the moving object predictors may predict failed results caused by uncertain motion information, such as low-quality optical flow maps. Besides, different sources such as RGB, depth, optical flow and static saliency can provide useful information about the objects. However, ex",
    "arxiv_url": "https://arxiv.org/abs/2303.10383v2",
    "pdf_url": "https://arxiv.org/pdf/2303.10383v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2303.10383",
    "arxiv_authors": [
      "Xiaoqi Zhao",
      "Shijie Chang",
      "Youwei Pang",
      "Jiaxing Yang",
      "Lihe Zhang",
      "Huchuan Lu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Adaptive+Multi-source+Predictor+for+Zero-shot+Video+Object+Segmentation+Xiaoqi+Zhao+Shijie+Chang+Youwei+Pang+Jiaxing+Yang+Lihe+Zhang",
    "gs_search_success": true,
    "gs_authors": [
      "0EKcLI4AAAAJ",
      "D3nE0agAAAAJ",
      "jdo9_goAAAAJ",
      "yn5W75MAAAAJ",
      "XGPdQbIAAAAJ"
    ],
    "citation_count": 17,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2303.15840",
    "title": "Sparse Depth-Guided Attention for Accurate Depth Completion: A Stereo-Assisted Monitored Distillation Approach",
    "year": 2023,
    "published": "2023-03-28T09:23:19Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "This paper proposes a novel method for depth completion, which leverages multi-view improved monitored distillation to generate more precise depth maps. Our approach builds upon the state-of-the-art ensemble distillation method, in which we introduce a stereo-based model as a teacher model to improve the accuracy of the student model for depth completion. By minimizing the reconstruction error of a target image during ensemble distillation, we can avoid learning inherent error modes of completio",
    "arxiv_url": "https://arxiv.org/abs/2303.15840v3",
    "pdf_url": "https://arxiv.org/pdf/2303.15840v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2303.15840",
    "arxiv_authors": [
      "Jia-Wei Guo",
      "Hung-Chyun Chou",
      "Sen-Hua Zhu",
      "Chang-Zheng Zhang",
      "Ming Ouyang",
      "Ning Ding"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Sparse+Depth-Guided+Attention+for+Accurate+Depth+Completion%3A+A+Stereo-Assisted+Monitored+Distillation+Approach+Jia-Wei+Guo+Hung-Chyun+Chou+Sen-Hua+Zhu+Chang-Zheng+Zhang+Ming+Ouyang",
    "gs_search_success": true,
    "gs_authors": [],
    "citation_count": null,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2410.15360",
    "title": "Improving 3D Medical Image Segmentation at Boundary Regions using Local Self-attention and Global Volume Mixing",
    "year": 2024,
    "published": "2024-10-20T11:08:38Z",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "abstract": "Volumetric medical image segmentation is a fundamental problem in medical image analysis where the objective is to accurately classify a given 3D volumetric medical image with voxel-level precision. In this work, we propose a novel hierarchical encoder-decoder-based framework that strives to explicitly capture the local and global dependencies for volumetric 3D medical image segmentation. The proposed framework exploits local volume-based self-attention to encode the local dependencies at high r",
    "arxiv_url": "https://arxiv.org/abs/2410.15360v1",
    "pdf_url": "https://arxiv.org/pdf/2410.15360v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2410.15360",
    "arxiv_authors": [
      "Daniya Najiha Abdul Kareem",
      "Mustansar Fiaz",
      "Noa Novershtern",
      "Jacob Hanna",
      "Hisham Cholakkal"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Improving+3D+Medical+Image+Segmentation+at+Boundary+Regions+using+Local+Self-attention+and+Global+Volume+Mixing+Daniya+Najiha+Abdul+Kareem+Mustansar+Fiaz+Noa+Novershtern+Jacob+Hanna+Hisham+Cholakkal",
    "gs_search_success": true,
    "gs_authors": [
      "MiDbnSAAAAAJ",
      "Q-jUVCUAAAAJ",
      "GZHLEFUAAAAJ",
      "Yi5qJmkAAAAJ",
      "bZ3YBRcAAAAJ"
    ],
    "citation_count": 8,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2406.03117",
    "title": "VQUNet: Vector Quantization U-Net for Defending Adversarial Atacks by Regularizing Unwanted Noise",
    "year": 2024,
    "published": "2024-06-05T10:10:03Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Deep Neural Networks (DNN) have become a promising paradigm when developing Artificial Intelligence (AI) and Machine Learning (ML) applications. However, DNN applications are vulnerable to fake data that are crafted with adversarial attack algorithms. Under adversarial attacks, the prediction accuracy of DNN applications suffers, making them unreliable. In order to defend against adversarial attacks, we introduce a novel noise-reduction procedure, Vector Quantization U-Net (VQUNet), to reduce ad",
    "arxiv_url": "https://arxiv.org/abs/2406.03117v1",
    "pdf_url": "https://arxiv.org/pdf/2406.03117v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2406.03117",
    "arxiv_authors": [
      "Zhixun He",
      "Mukesh Singhal"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=VQUNet%3A+Vector+Quantization+U-Net+for+Defending+Adversarial+Atacks+by+Regularizing+Unwanted+Noise+Zhixun+He+Mukesh+Singhal",
    "gs_search_success": false,
    "gs_authors": [],
    "citation_count": null,
    "error_type": "fetch_failed",
    "gs_author_count": 2
  },
  {
    "arxiv_id": "2504.05774",
    "title": "TMT: Cross-domain Semantic Segmentation with Region-adaptive Transferability Estimation",
    "year": 2025,
    "published": "2025-04-08T07:53:51Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Recent advances in Vision Transformers (ViTs) have significantly advanced semantic segmentation performance. However, their adaptation to new target domains remains challenged by distribution shifts, which often disrupt global attention mechanisms. While existing global and patch-level adaptation methods offer some improvements, they overlook the spatially varying transferability inherent in different image regions. To address this, we propose the Transferable Mask Transformer (TMT), a region-ad",
    "arxiv_url": "https://arxiv.org/abs/2504.05774v3",
    "pdf_url": "https://arxiv.org/pdf/2504.05774v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2504.05774",
    "arxiv_authors": [
      "Enming Zhang",
      "Zhengyu Li",
      "Yanru Wu",
      "Jingge Wang",
      "Yang Tan",
      "Guan Wang",
      "Yang Li",
      "Xiaoping Zhang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=TMT%3A+Cross-domain+Semantic+Segmentation+with+Region-adaptive+Transferability+Estimation+Enming+Zhang+Zhengyu+Li+Yanru+Wu+Jingge+Wang+Yang+Tan",
    "gs_search_success": true,
    "gs_authors": [
      "x26LSO4AAAAJ",
      "qYp2EbkAAAAJ",
      "Ls_UhjcAAAAJ",
      "BI_NXbUAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 8
  },
  {
    "arxiv_id": "2406.14994",
    "title": "Benchmarking Retinal Blood Vessel Segmentation Models for Cross-Dataset and Cross-Disease Generalization",
    "year": 2024,
    "published": "2024-06-21T09:12:34Z",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "abstract": "Retinal blood vessel segmentation can extract clinically relevant information from fundus images. As manual tracing is cumbersome, algorithms based on Convolution Neural Networks have been developed. Such studies have used small publicly available datasets for training and measuring performance, running the risk of overfitting. Here, we provide a rigorous benchmark for various architectural and training choices commonly used in the literature on the largest dataset published to date. We train an",
    "arxiv_url": "https://arxiv.org/abs/2406.14994v1",
    "pdf_url": "https://arxiv.org/pdf/2406.14994v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2406.14994",
    "arxiv_authors": [
      "Jeremiah Fadugba",
      "Patrick Köhler",
      "Lisa Koch",
      "Petru Manescu",
      "Philipp Berens"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Benchmarking+Retinal+Blood+Vessel+Segmentation+Models+for+Cross-Dataset+and+Cross-Disease+Generalization+Jeremiah+Fadugba+Patrick+K%C3%B6hler+Lisa+Koch+Petru+Manescu+Philipp+Berens",
    "gs_search_success": true,
    "gs_authors": [
      "KwhiEjIAAAAJ",
      "R0iwuiIAAAAJ",
      "lPQLk3QAAAAJ",
      "FvXNY84AAAAJ"
    ],
    "citation_count": 3,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2312.09181",
    "title": "Improving Efficiency of Diffusion Models via Multi-Stage Framework and Tailored Multi-Decoder Architectures",
    "year": 2023,
    "published": "2023-12-14T17:48:09Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Diffusion models, emerging as powerful deep generative tools, excel in various applications. They operate through a two-steps process: introducing noise into training samples and then employing a model to convert random noise into new samples (e.g., images). However, their remarkable generative performance is hindered by slow training and sampling. This is due to the necessity of tracking extensive forward and reverse diffusion trajectories, and employing a large model with numerous parameters a",
    "arxiv_url": "https://arxiv.org/abs/2312.09181v3",
    "pdf_url": "https://arxiv.org/pdf/2312.09181v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2312.09181",
    "arxiv_authors": [
      "Huijie Zhang",
      "Yifu Lu",
      "Ismail Alkhouri",
      "Saiprasad Ravishankar",
      "Dogyoon Song",
      "Qing Qu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Improving+Efficiency+of+Diffusion+Models+via+Multi-Stage+Framework+and+Tailored+Multi-Decoder+Architectures+Huijie+Zhang+Yifu+Lu+Ismail+Alkhouri+Saiprasad+Ravishankar+Dogyoon+Song",
    "gs_search_success": true,
    "gs_authors": [
      "xC_PIeAAAAAJ",
      "ybsmKpsAAAAJ",
      "JfblW3MAAAAJ",
      "Dx9iby0AAAAJ",
      "CT84_rEAAAAJ",
      "W9Mplc4AAAAJ"
    ],
    "citation_count": 12,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2309.00147",
    "title": "Optimized Deep Feature Selection for Pneumonia Detection: A Novel RegNet and XOR-Based PSO Approach",
    "year": 2023,
    "published": "2023-08-31T21:42:54Z",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "abstract": "Pneumonia remains a significant cause of child mortality, particularly in developing countries where resources and expertise are limited. The automated detection of Pneumonia can greatly assist in addressing this challenge. In this research, an XOR based Particle Swarm Optimization (PSO) is proposed to select deep features from the second last layer of a RegNet model, aiming to improve the accuracy of the CNN model on Pneumonia detection. The proposed XOR PSO algorithm offers simplicity by incor",
    "arxiv_url": "https://arxiv.org/abs/2309.00147v1",
    "pdf_url": "https://arxiv.org/pdf/2309.00147v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2309.00147",
    "arxiv_authors": [
      "Fatemehsadat Ghanadi Ladani",
      "Samaneh Hosseini Semnani"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Optimized+Deep+Feature+Selection+for+Pneumonia+Detection%3A+A+Novel+RegNet+and+XOR-Based+PSO+Approach+Fatemehsadat+Ghanadi+Ladani+Samaneh+Hosseini+Semnani",
    "gs_search_success": true,
    "gs_authors": [
      "MYXUk2oAAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 2
  },
  {
    "arxiv_id": "2403.01977",
    "title": "TTA-Nav: Test-time Adaptive Reconstruction for Point-Goal Navigation under Visual Corruptions",
    "year": 2024,
    "published": "2024-03-04T12:20:29Z",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV"
    ],
    "abstract": "Robot navigation under visual corruption presents a formidable challenge. To address this, we propose a Test-time Adaptation (TTA) method, named as TTA-Nav, for point-goal navigation under visual corruptions. Our \"plug-and-play\" method incorporates a top-down decoder to a pre-trained navigation model. Firstly, the pre-trained navigation model gets a corrupted image and extracts features. Secondly, the top-down decoder produces the reconstruction given the high-level features extracted by the pre",
    "arxiv_url": "https://arxiv.org/abs/2403.01977v2",
    "pdf_url": "https://arxiv.org/pdf/2403.01977v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2403.01977",
    "arxiv_authors": [
      "Maytus Piriyajitakonkij",
      "Mingfei Sun",
      "Mengmi Zhang",
      "Wei Pan"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=TTA-Nav%3A+Test-time+Adaptive+Reconstruction+for+Point-Goal+Navigation+under+Visual+Corruptions+Maytus+Piriyajitakonkij+Mingfei+Sun+Mengmi+Zhang+Wei+Pan",
    "gs_search_success": true,
    "gs_authors": [
      "GqryWPsAAAAJ",
      "G2sVOhcAAAAJ",
      "zGfVbVkAAAAJ",
      "2Uzgp5kAAAAJ"
    ],
    "citation_count": 3,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2503.18709",
    "title": "Revisiting Automatic Data Curation for Vision Foundation Models in Digital Pathology",
    "year": 2025,
    "published": "2025-03-24T14:23:48Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Vision foundation models (FMs) are accelerating the development of digital pathology algorithms and transforming biomedical research. These models learn, in a self-supervised manner, to represent histological features in highly heterogeneous tiles extracted from whole-slide images (WSIs) of real-world patient samples. The performance of these FMs is significantly influenced by the size, diversity, and balance of the pre-training data. However, data selection has been primarily guided by expert k",
    "arxiv_url": "https://arxiv.org/abs/2503.18709v2",
    "pdf_url": "https://arxiv.org/pdf/2503.18709v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2503.18709",
    "arxiv_authors": [
      "Boqi Chen",
      "Cédric Vincent-Cuaz",
      "Lydia A. Schoenpflug",
      "Manuel Madeira",
      "Lisa Fournier",
      "Vaishnavi Subramanian",
      "Sonali Andani",
      "Samuel Ruiperez-Campillo",
      "Julia E. Vogt",
      "Raphaëlle Luisier",
      "Dorina Thanou",
      "Viktor H. Koelzer",
      "Pascal Frossard",
      "Gabriele Campanella",
      "Gunnar Rätsch"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Revisiting+Automatic+Data+Curation+for+Vision+Foundation+Models+in+Digital+Pathology+Boqi+Chen+C%C3%A9dric+Vincent-Cuaz+Lydia+A.+Schoenpflug+Manuel+Madeira+Lisa+Fournier",
    "gs_search_success": true,
    "gs_authors": [
      "3ymrjx8AAAAJ",
      "p0spNmMAAAAJ",
      "TOAT8jEAAAAJ",
      "hc5qHEkAAAAJ",
      "Jj0KzdEAAAAJ",
      "OhijpAwAAAAJ",
      "eAR23MAAAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 9
  },
  {
    "arxiv_id": "2401.10247",
    "title": "Resolution Chromatography of Diffusion Models",
    "year": 2023,
    "published": "2023-12-07T04:20:16Z",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "Diffusion models generate high-resolution images through iterative stochastic processes. In particular, the denoising method is one of the most popular approaches that predicts the noise in samples and denoises it at each time step. It has been commonly observed that the resolution of generated samples changes over time, starting off blurry and coarse, and becoming sharper and finer. In this paper, we introduce \"resolution chromatography\" that indicates the signal generation rate of each resolut",
    "arxiv_url": "https://arxiv.org/abs/2401.10247v1",
    "pdf_url": "https://arxiv.org/pdf/2401.10247v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2401.10247",
    "arxiv_authors": [
      "Juno Hwang",
      "Yong-Hyun Park",
      "Junghyo Jo"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Resolution+Chromatography+of+Diffusion+Models+Juno+Hwang+Yong-Hyun+Park+Junghyo+Jo",
    "gs_search_success": true,
    "gs_authors": [
      "6nMoHb0AAAAJ",
      "h1QXLx0AAAAJ",
      "H8N2tHgAAAAJ"
    ],
    "citation_count": 3,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2504.13220",
    "title": "SSTAF: Spatial-Spectral-Temporal Attention Fusion Transformer for Motor Imagery Classification",
    "year": 2025,
    "published": "2025-04-17T07:45:14Z",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "Brain-computer interfaces (BCI) in electroencephalography (EEG)-based motor imagery classification offer promising solutions in neurorehabilitation and assistive technologies by enabling communication between the brain and external devices. However, the non-stationary nature of EEG signals and significant inter-subject variability cause substantial challenges for developing robust cross-subject classification models. This paper introduces a novel Spatial-Spectral-Temporal Attention Fusion (SSTAF",
    "arxiv_url": "https://arxiv.org/abs/2504.13220v1",
    "pdf_url": "https://arxiv.org/pdf/2504.13220v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2504.13220",
    "arxiv_authors": [
      "Ummay Maria Muna",
      "Md. Mehedi Hasan Shawon",
      "Md Jobayer",
      "Sumaiya Akter",
      "Saifur Rahman Sabuj"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=SSTAF%3A+Spatial-Spectral-Temporal+Attention+Fusion+Transformer+for+Motor+Imagery+Classification+Ummay+Maria+Muna+Md.+Mehedi+Hasan+Shawon+Md+Jobayer+Sumaiya+Akter+Saifur+Rahman+Sabuj",
    "gs_search_success": true,
    "gs_authors": [
      "o7JGMlQAAAAJ",
      "a8DjRE0AAAAJ",
      "4XCuwtMAAAAJ",
      "hSocsGoAAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2403.05050",
    "title": "DyRoNet: Dynamic Routing and Low-Rank Adapters for Autonomous Driving Streaming Perception",
    "year": 2024,
    "published": "2024-03-08T04:53:53Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.MM"
    ],
    "abstract": "The advancement of autonomous driving systems hinges on the ability to achieve low-latency and high-accuracy perception. To address this critical need, this paper introduces Dynamic Routing Network (DyRoNet), a low-rank enhanced dynamic routing framework designed for streaming perception in autonomous driving systems. DyRoNet integrates a suite of pre-trained branch networks, each meticulously fine-tuned to function under distinct environmental conditions. At its core, the framework offers a spe",
    "arxiv_url": "https://arxiv.org/abs/2403.05050v4",
    "pdf_url": "https://arxiv.org/pdf/2403.05050v4",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2403.05050",
    "arxiv_authors": [
      "Xiang Huang",
      "Zhi-Qi Cheng",
      "Jun-Yan He",
      "Chenyang Li",
      "Wangmeng Xiang",
      "Baigui Sun"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=DyRoNet%3A+Dynamic+Routing+and+Low-Rank+Adapters+for+Autonomous+Driving+Streaming+Perception+Xiang+Huang+Zhi-Qi+Cheng+Jun-Yan+He+Chenyang+Li+Wangmeng+Xiang",
    "gs_search_success": true,
    "gs_authors": [
      "LFNwNF4AAAAJ",
      "uB2He2UAAAAJ",
      "bjNZqGAAAAAJ",
      "ZNhTHywAAAAJ"
    ],
    "citation_count": 2,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2306.06805",
    "title": "Unlocking Feature Visualization for Deeper Networks with MAgnitude Constrained Optimization",
    "year": 2023,
    "published": "2023-06-11T23:33:59Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Feature visualization has gained substantial popularity, particularly after the influential work by Olah et al. in 2017, which established it as a crucial tool for explainability. However, its widespread adoption has been limited due to a reliance on tricks to generate interpretable images, and corresponding challenges in scaling it to deeper neural networks. Here, we describe MACO, a simple approach to address these shortcomings. The main idea is to generate images by optimizing the phase spect",
    "arxiv_url": "https://arxiv.org/abs/2306.06805v3",
    "pdf_url": "https://arxiv.org/pdf/2306.06805v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2306.06805",
    "arxiv_authors": [
      "Thomas Fel",
      "Thibaut Boissin",
      "Victor Boutin",
      "Agustin Picard",
      "Paul Novello",
      "Julien Colin",
      "Drew Linsley",
      "Tom Rousseau",
      "Rémi Cadène",
      "Lore Goetschalckx",
      "Laurent Gardes",
      "Thomas Serre"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Unlocking+Feature+Visualization+for+Deeper+Networks+with+MAgnitude+Constrained+Optimization+Thomas+Fel+Thibaut+Boissin+Victor+Boutin+Agustin+Picard+Paul+Novello",
    "gs_search_success": true,
    "gs_authors": [
      "uaJK95oAAAAJ",
      "ABDSUgEAAAAJ",
      "_Rsjt9kAAAAJ",
      "Z-YF5FsAAAAJ",
      "zC-MstIAAAAJ",
      "WM6NdwMAAAAJ",
      "cXZlAuQAAAAJ"
    ],
    "citation_count": 30,
    "gs_author_count": 9
  },
  {
    "arxiv_id": "2405.02005",
    "title": "HoloGS: Instant Depth-based 3D Gaussian Splatting with Microsoft HoloLens 2",
    "year": 2024,
    "published": "2024-05-03T11:08:04Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "In the fields of photogrammetry, computer vision and computer graphics, the task of neural 3D scene reconstruction has led to the exploration of various techniques. Among these, 3D Gaussian Splatting stands out for its explicit representation of scenes using 3D Gaussians, making it appealing for tasks like 3D point cloud extraction and surface reconstruction. Motivated by its potential, we address the domain of 3D scene reconstruction, aiming to leverage the capabilities of the Microsoft HoloLen",
    "arxiv_url": "https://arxiv.org/abs/2405.02005v1",
    "pdf_url": "https://arxiv.org/pdf/2405.02005v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2405.02005",
    "arxiv_authors": [
      "Miriam Jäger",
      "Theodor Kapler",
      "Michael Feßenbecker",
      "Felix Birkelbach",
      "Markus Hillemann",
      "Boris Jutzi"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=HoloGS%3A+Instant+Depth-based+3D+Gaussian+Splatting+with+Microsoft+HoloLens+2+Miriam+J%C3%A4ger+Theodor+Kapler+Michael+Fe%C3%9Fenbecker+Felix+Birkelbach+Markus+Hillemann",
    "gs_search_success": true,
    "gs_authors": [
      "ZpB02CwAAAAJ",
      "_qfDWfUAAAAJ"
    ],
    "citation_count": 3,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2404.08540",
    "title": "On the Robustness of Language Guidance for Low-Level Vision Tasks: Findings from Depth Estimation",
    "year": 2024,
    "published": "2024-04-12T15:35:20Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Recent advances in monocular depth estimation have been made by incorporating natural language as additional guidance. Although yielding impressive results, the impact of the language prior, particularly in terms of generalization and robustness, remains unexplored. In this paper, we address this gap by quantifying the impact of this prior and introduce methods to benchmark its effectiveness across various settings. We generate \"low-level\" sentences that convey object-centric, three-dimensional ",
    "arxiv_url": "https://arxiv.org/abs/2404.08540v1",
    "pdf_url": "https://arxiv.org/pdf/2404.08540v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2404.08540",
    "arxiv_authors": [
      "Agneet Chatterjee",
      "Tejas Gokhale",
      "Chitta Baral",
      "Yezhou Yang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=On+the+Robustness+of+Language+Guidance+for+Low-Level+Vision+Tasks%3A+Findings+from+Depth+Estimation+Agneet+Chatterjee+Tejas+Gokhale+Chitta+Baral+Yezhou+Yang",
    "gs_search_success": true,
    "gs_authors": [
      "9Yd716IAAAAJ",
      "_ILTlEwAAAAJ",
      "RGRaOegAAAAJ",
      "k2suuZgAAAAJ"
    ],
    "citation_count": 7,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2309.10586",
    "title": "Adversarial Attacks Against Uncertainty Quantification",
    "year": 2023,
    "published": "2023-09-19T12:54:09Z",
    "categories": [
      "cs.CV",
      "cs.CR",
      "cs.LG"
    ],
    "abstract": "Machine-learning models can be fooled by adversarial examples, i.e., carefully-crafted input perturbations that force models to output wrong predictions. While uncertainty quantification has been recently proposed to detect adversarial inputs, under the assumption that such attacks exhibit a higher prediction uncertainty than pristine data, it has been shown that adaptive attacks specifically aimed at reducing also the uncertainty estimate can easily bypass this defense mechanism. In this work, ",
    "arxiv_url": "https://arxiv.org/abs/2309.10586v1",
    "pdf_url": "https://arxiv.org/pdf/2309.10586v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2309.10586",
    "arxiv_authors": [
      "Emanuele Ledda",
      "Daniele Angioni",
      "Giorgio Piras",
      "Giorgio Fumera",
      "Battista Biggio",
      "Fabio Roli"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Adversarial+Attacks+Against+Uncertainty+Quantification+Emanuele+Ledda+Daniele+Angioni+Giorgio+Piras+Giorgio+Fumera+Battista+Biggio",
    "gs_search_success": false,
    "gs_authors": [],
    "citation_count": null,
    "error_type": "fetch_failed",
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2412.01812",
    "title": "V2XPnP: Vehicle-to-Everything Spatio-Temporal Fusion for Multi-Agent Perception and Prediction",
    "year": 2024,
    "published": "2024-12-02T18:55:34Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Vehicle-to-everything (V2X) technologies offer a promising paradigm to mitigate the limitations of constrained observability in single-vehicle systems. Prior work primarily focuses on single-frame cooperative perception, which fuses agents' information across different spatial locations but ignores temporal cues and temporal tasks (e.g., temporal perception and prediction). In this paper, we focus on the spatio-temporal fusion in V2X scenarios and design one-step and multi-step communication str",
    "arxiv_url": "https://arxiv.org/abs/2412.01812v3",
    "pdf_url": "https://arxiv.org/pdf/2412.01812v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2412.01812",
    "arxiv_authors": [
      "Zewei Zhou",
      "Hao Xiang",
      "Zhaoliang Zheng",
      "Seth Z. Zhao",
      "Mingyue Lei",
      "Yun Zhang",
      "Tianhui Cai",
      "Xinyi Liu",
      "Johnson Liu",
      "Maheswari Bajji",
      "Xin Xia",
      "Zhiyu Huang",
      "Bolei Zhou",
      "Jiaqi Ma"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=V2XPnP%3A+Vehicle-to-Everything+Spatio-Temporal+Fusion+for+Multi-Agent+Perception+and+Prediction+Zewei+Zhou+Hao+Xiang+Zhaoliang+Zheng+Seth+Z.+Zhao+Mingyue+Lei",
    "gs_search_success": true,
    "gs_authors": [
      "OfsIi_4AAAAJ",
      "6YqkXM0AAAAJ",
      "H7M2Pt0AAAAJ",
      "XXevN4cAAAAJ",
      "04j4RzkAAAAJ",
      "vCYqMTIAAAAJ",
      "SyR4O7YAAAAJ",
      "TzhyHbYAAAAJ"
    ],
    "citation_count": 22,
    "gs_author_count": 9
  },
  {
    "arxiv_id": "2303.03565",
    "title": "CLIP-Layout: Style-Consistent Indoor Scene Synthesis with Semantic Furniture Embedding",
    "year": 2023,
    "published": "2023-03-07T00:26:02Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Indoor scene synthesis involves automatically picking and placing furniture appropriately on a floor plan, so that the scene looks realistic and is functionally plausible. Such scenes can serve as homes for immersive 3D experiences, or be used to train embodied agents. Existing methods for this task rely on labeled categories of furniture, e.g. bed, chair or table, to generate contextually relevant combinations of furniture. Whether heuristic or learned, these methods ignore instance-level visua",
    "arxiv_url": "https://arxiv.org/abs/2303.03565v2",
    "pdf_url": "https://arxiv.org/pdf/2303.03565v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2303.03565",
    "arxiv_authors": [
      "Jingyu Liu",
      "Wenhan Xiong",
      "Ian Jones",
      "Yixin Nie",
      "Anchit Gupta",
      "Barlas Oğuz"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=CLIP-Layout%3A+Style-Consistent+Indoor+Scene+Synthesis+with+Semantic+Furniture+Embedding+Jingyu+Liu+Wenhan+Xiong+Ian+Jones+Yixin+Nie+Anchit+Gupta",
    "gs_search_success": true,
    "gs_authors": [
      "J9_LwQUAAAAJ",
      "g5QpITUAAAAJ",
      "jidrykQAAAAJ",
      "L5y3GwgAAAAJ",
      "nJrUcXAAAAAJ",
      "iPmTQZMAAAAJ"
    ],
    "citation_count": 14,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2412.09513",
    "title": "Agent-based Video Trimming",
    "year": 2024,
    "published": "2024-12-12T17:59:28Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "As information becomes more accessible, user-generated videos are increasing in length, placing a burden on viewers to sift through vast content for valuable insights. This trend underscores the need for an algorithm to extract key video information efficiently. Despite significant advancements in highlight detection, moment retrieval, and video summarization, current approaches primarily focus on selecting specific time intervals, often overlooking the relevance between segments and the potenti",
    "arxiv_url": "https://arxiv.org/abs/2412.09513v1",
    "pdf_url": "https://arxiv.org/pdf/2412.09513v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2412.09513",
    "arxiv_authors": [
      "Lingfeng Yang",
      "Zhenyuan Chen",
      "Xiang Li",
      "Peiyang Jia",
      "Liangqu Long",
      "Jian Yang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Agent-based+Video+Trimming+Lingfeng+Yang+Zhenyuan+Chen+Xiang+Li+Peiyang+Jia+Liangqu+Long",
    "gs_search_success": true,
    "gs_authors": [
      "RLhH0jwAAAAJ",
      "i0_doPcAAAAJ",
      "6CIDtZQAAAAJ",
      "oamjJdYAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2410.08680",
    "title": "Gait Sequence Upsampling using Diffusion Models for Single LiDAR Sensors",
    "year": 2024,
    "published": "2024-10-11T10:11:21Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Recently, 3D LiDAR has emerged as a promising technique in the field of gait-based person identification, serving as an alternative to traditional RGB cameras, due to its robustness under varying lighting conditions and its ability to capture 3D geometric information. However, long capture distances or the use of low-cost LiDAR sensors often result in sparse human point clouds, leading to a decline in identification performance. To address these challenges, we propose a sparse-to-dense upsamplin",
    "arxiv_url": "https://arxiv.org/abs/2410.08680v2",
    "pdf_url": "https://arxiv.org/pdf/2410.08680v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2410.08680",
    "arxiv_authors": [
      "Jeongho Ahn",
      "Kazuto Nakashima",
      "Koki Yoshino",
      "Yumi Iwashita",
      "Ryo Kurazume"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Gait+Sequence+Upsampling+using+Diffusion+Models+for+Single+LiDAR+Sensors+Jeongho+Ahn+Kazuto+Nakashima+Koki+Yoshino+Yumi+Iwashita+Ryo+Kurazume",
    "gs_search_success": true,
    "gs_authors": [
      "VnDvavYAAAAJ",
      "noQtIJsAAAAJ",
      "PGhVyVcAAAAJ",
      "YYPwRboAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2505.17779",
    "title": "U2-BENCH: Benchmarking Large Vision-Language Models on Ultrasound Understanding",
    "year": 2025,
    "published": "2025-05-23T11:48:48Z",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "Ultrasound is a widely-used imaging modality critical to global healthcare, yet its interpretation remains challenging due to its varying image quality on operators, noises, and anatomical structures. Although large vision-language models (LVLMs) have demonstrated impressive multimodal capabilities across natural and medical domains, their performance on ultrasound remains largely unexplored. We introduce U2-BENCH, the first comprehensive benchmark to evaluate LVLMs on ultrasound understanding a",
    "arxiv_url": "https://arxiv.org/abs/2505.17779v2",
    "pdf_url": "https://arxiv.org/pdf/2505.17779v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2505.17779",
    "arxiv_authors": [
      "Anjie Le",
      "Henan Liu",
      "Yue Wang",
      "Zhenyu Liu",
      "Rongkun Zhu",
      "Taohan Weng",
      "Jinze Yu",
      "Boyang Wang",
      "Yalun Wu",
      "Kaiwen Yan",
      "Quanlin Sun",
      "Meirui Jiang",
      "Jialun Pei",
      "Siya Liu",
      "Haoyun Zheng",
      "Zhoujun Li",
      "Alison Noble",
      "Jacques Souquet",
      "Xiaoqing Guo",
      "Manxi Lin",
      "Hongcheng Guo"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=U2-BENCH%3A+Benchmarking+Large+Vision-Language+Models+on+Ultrasound+Understanding+Anjie+Le+Henan+Liu+Yue+Wang+Zhenyu+Liu+Rongkun+Zhu",
    "gs_search_success": true,
    "gs_authors": [
      "zT0m_RQAAAAJ",
      "TuyGlzwAAAAJ",
      "D-IHI1kAAAAJ",
      "XvQ4B1sAAAAJ"
    ],
    "citation_count": 5,
    "gs_author_count": 9
  },
  {
    "arxiv_id": "2311.15113",
    "title": "NCL-SM: A Fully Annotated Dataset of Images from Human Skeletal Muscle Biopsies",
    "year": 2023,
    "published": "2023-11-25T20:29:35Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "q-bio.TO"
    ],
    "abstract": "Single cell analysis of human skeletal muscle (SM) tissue cross-sections is a fundamental tool for understanding many neuromuscular disorders. For this analysis to be reliable and reproducible, identification of individual fibres within microscopy images (segmentation) of SM tissue should be automatic and precise. Biomedical scientists in this field currently rely on custom tools and general machine learning (ML) models, both followed by labour intensive and subjective manual interventions to fi",
    "arxiv_url": "https://arxiv.org/abs/2311.15113v1",
    "pdf_url": "https://arxiv.org/pdf/2311.15113v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2311.15113",
    "arxiv_authors": [
      "Atif Khan",
      "Conor Lawless",
      "Amy Vincent",
      "Charlotte Warren",
      "Valeria Di Leo",
      "Tiago Gomes",
      "A. Stephen McGough"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=NCL-SM%3A+A+Fully+Annotated+Dataset+of+Images+from+Human+Skeletal+Muscle+Biopsies+Atif+Khan+Conor+Lawless+Amy+Vincent+Charlotte+Warren+Valeria+Di+Leo",
    "gs_search_success": true,
    "gs_authors": [
      "qAxPi50AAAAJ",
      "dI0q18MAAAAJ",
      "4vn38zYAAAAJ",
      "_Ma0wHoAAAAJ",
      "ZgtwW-0AAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 7
  },
  {
    "arxiv_id": "2405.18119",
    "title": "Low-Resource Crop Classification from Multi-Spectral Time Series Using Lossless Compressors",
    "year": 2024,
    "published": "2024-05-28T12:28:12Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "abstract": "Deep learning has significantly improved the accuracy of crop classification using multispectral temporal data. However, these models have complex structures with numerous parameters, requiring large amounts of data and costly training. In low-resource situations with fewer labeled samples, deep learning models perform poorly due to insufficient data. Conversely, compressors are data-type agnostic, and non-parametric methods do not bring underlying assumptions. Inspired by this insight, we propo",
    "arxiv_url": "https://arxiv.org/abs/2405.18119v2",
    "pdf_url": "https://arxiv.org/pdf/2405.18119v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2405.18119",
    "arxiv_authors": [
      "Wei Cheng",
      "Hongrui Ye",
      "Xiao Wen",
      "Jiachen Zhang",
      "Jiping Xu",
      "Feifan Zhang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Low-Resource+Crop+Classification+from+Multi-Spectral+Time+Series+Using+Lossless+Compressors+Wei+Cheng+Hongrui+Ye+Xiao+Wen+Jiachen+Zhang+Jiping+Xu",
    "gs_search_success": true,
    "gs_authors": [],
    "citation_count": null,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2410.09962",
    "title": "LongHalQA: Long-Context Hallucination Evaluation for MultiModal Large Language Models",
    "year": 2024,
    "published": "2024-10-13T18:59:58Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Hallucination, a phenomenon where multimodal large language models~(MLLMs) tend to generate textual responses that are plausible but unaligned with the image, has become one major hurdle in various MLLM-related applications. Several benchmarks have been created to gauge the hallucination levels of MLLMs, by either raising discriminative questions about the existence of objects or introducing LLM evaluators to score the generated text from MLLMs. However, the discriminative data largely involve s",
    "arxiv_url": "https://arxiv.org/abs/2410.09962v2",
    "pdf_url": "https://arxiv.org/pdf/2410.09962v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2410.09962",
    "arxiv_authors": [
      "Han Qiu",
      "Jiaxing Huang",
      "Peng Gao",
      "Qin Qi",
      "Xiaoqin Zhang",
      "Ling Shao",
      "Shijian Lu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=LongHalQA%3A+Long-Context+Hallucination+Evaluation+for+MultiModal+Large+Language+Models+Han+Qiu+Jiaxing+Huang+Peng+Gao+Qin+Qi+Xiaoqin+Zhang",
    "gs_search_success": true,
    "gs_authors": [
      "czirNcwAAAAJ",
      "z84rLjoAAAAJ",
      "s2QuLQUAAAAJ",
      "uYmK-A0AAAAJ",
      "YThp3g8AAAAJ"
    ],
    "citation_count": 15,
    "gs_author_count": 7
  },
  {
    "arxiv_id": "2308.03766",
    "title": "AMaizeD: An End to End Pipeline for Automatic Maize Disease Detection",
    "year": 2023,
    "published": "2023-07-23T19:58:40Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "eess.IV"
    ],
    "abstract": "This research paper presents AMaizeD: An End to End Pipeline for Automatic Maize Disease Detection, an automated framework for early detection of diseases in maize crops using multispectral imagery obtained from drones. A custom hand-collected dataset focusing specifically on maize crops was meticulously gathered by expert researchers and agronomists. The dataset encompasses a diverse range of maize varieties, cultivation practices, and environmental conditions, capturing various stages of maize",
    "arxiv_url": "https://arxiv.org/abs/2308.03766v1",
    "pdf_url": "https://arxiv.org/pdf/2308.03766v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2308.03766",
    "arxiv_authors": [
      "Anish Mall",
      "Sanchit Kabra",
      "Ankur Lhila",
      "Pawan Ajmera"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=AMaizeD%3A+An+End+to+End+Pipeline+for+Automatic+Maize+Disease+Detection+Anish+Mall+Sanchit+Kabra+Ankur+Lhila+Pawan+Ajmera",
    "gs_search_success": true,
    "gs_authors": [
      "NTBVWSUAAAAJ",
      "n4gVd4IAAAAJ",
      "ZIP4HNMAAAAJ"
    ],
    "citation_count": 4,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2310.01934",
    "title": "Robust deformable image registration using cycle-consistent implicit representations",
    "year": 2023,
    "published": "2023-10-03T10:17:49Z",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "abstract": "Recent works in medical image registration have proposed the use of Implicit Neural Representations, demonstrating performance that rivals state-of-the-art learning-based methods. However, these implicit representations need to be optimized for each new image pair, which is a stochastic process that may fail to converge to a global minimum. To improve robustness, we propose a deformable registration method using pairs of cycle-consistent Implicit Neural Representations: each implicit representat",
    "arxiv_url": "https://arxiv.org/abs/2310.01934v1",
    "pdf_url": "https://arxiv.org/pdf/2310.01934v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2310.01934",
    "arxiv_authors": [
      "Louis D. van Harten",
      "Jaap Stoker",
      "Ivana Išgum"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Robust+deformable+image+registration+using+cycle-consistent+implicit+representations+Louis+D.+van+Harten+Jaap+Stoker+Ivana+I%C5%A1gum",
    "gs_search_success": true,
    "gs_authors": [
      "TuZIrdEAAAAJ",
      "xelGbyIAAAAJ"
    ],
    "citation_count": 28,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2312.02208",
    "title": "A Data-efficient Framework for Robotics Large-scale LiDAR Scene Parsing",
    "year": 2023,
    "published": "2023-12-03T02:38:51Z",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "abstract": "Existing state-of-the-art 3D point clouds understanding methods only perform well in a fully supervised manner. To the best of our knowledge, there exists no unified framework which simultaneously solves the downstream high-level understanding tasks, especially when labels are extremely limited. This work presents a general and simple framework to tackle point clouds understanding when labels are limited. We propose a novel unsupervised region expansion based clustering method for generating clu",
    "arxiv_url": "https://arxiv.org/abs/2312.02208v1",
    "pdf_url": "https://arxiv.org/pdf/2312.02208v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2312.02208",
    "arxiv_authors": [
      "Kangcheng Liu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=A+Data-efficient+Framework+for+Robotics+Large-scale+LiDAR+Scene+Parsing+Kangcheng+Liu",
    "gs_search_success": true,
    "gs_authors": [],
    "citation_count": 1,
    "gs_author_count": 1
  },
  {
    "arxiv_id": "2312.08773",
    "title": "Offshore Wind Plant Instance Segmentation Using Sentinel-1 Time Series, GIS, and Semantic Segmentation Models",
    "year": 2023,
    "published": "2023-12-14T09:49:15Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "eess.IV"
    ],
    "abstract": "Offshore wind farms represent a renewable energy source with a significant global growth trend, and their monitoring is strategic for territorial and environmental planning. This study's primary objective is to detect offshore wind plants at an instance level using semantic segmentation models and Sentinel-1 time series. The secondary objectives are: (a) to develop a database consisting of labeled data and S-1 time series; (b) to compare the performance of five deep semantic segmentation archite",
    "arxiv_url": "https://arxiv.org/abs/2312.08773v1",
    "pdf_url": "https://arxiv.org/pdf/2312.08773v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2312.08773",
    "arxiv_authors": [
      "Osmar Luiz Ferreira de Carvalho",
      "Osmar Abilio de Carvalho Junior",
      "Anesmar Olino de Albuquerque",
      "Daniel Guerreiro e Silva"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Offshore+Wind+Plant+Instance+Segmentation+Using+Sentinel-1+Time+Series%2C+GIS%2C+and+Semantic+Segmentation+Models+Osmar+Luiz+Ferreira+de+Carvalho+Osmar+Abilio+de+Carvalho+Junior+Anesmar+Olino+de+Albuquerque+Daniel+Guerreiro+e+Silva",
    "gs_search_success": true,
    "gs_authors": [
      "_LtW_kcAAAAJ",
      "fkXa6jIAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2411.01494",
    "title": "Finding NeMo: Negative-mined Mosaic Augmentation for Referring Image Segmentation",
    "year": 2024,
    "published": "2024-11-03T09:21:17Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Referring Image Segmentation is a comprehensive task to segment an object referred by a textual query from an image. In nature, the level of difficulty in this task is affected by the existence of similar objects and the complexity of the referring expression. Recent RIS models still show a significant performance gap between easy and hard scenarios. We pose that the bottleneck exists in the data, and propose a simple but powerful data augmentation method, Negative-mined Mosaic Augmentation (NeM",
    "arxiv_url": "https://arxiv.org/abs/2411.01494v1",
    "pdf_url": "https://arxiv.org/pdf/2411.01494v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2411.01494",
    "arxiv_authors": [
      "Seongsu Ha",
      "Chaeyun Kim",
      "Donghwa Kim",
      "Junho Lee",
      "Sangho Lee",
      "Joonseok Lee"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Finding+NeMo%3A+Negative-mined+Mosaic+Augmentation+for+Referring+Image+Segmentation+Seongsu+Ha+Chaeyun+Kim+Donghwa+Kim+Junho+Lee+Sangho+Lee",
    "gs_search_success": true,
    "gs_authors": [
      "lfkRbIsAAAAJ",
      "dmvMjvcAAAAJ",
      "Lq8MN6wAAAAJ",
      "buQ-WI0AAAAJ",
      "s_orZYMAAAAJ"
    ],
    "citation_count": 5,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2307.11702",
    "title": "SACReg: Scene-Agnostic Coordinate Regression for Visual Localization",
    "year": 2023,
    "published": "2023-07-21T16:56:36Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Scene coordinates regression (SCR), i.e., predicting 3D coordinates for every pixel of a given image, has recently shown promising potential. However, existing methods remain limited to small scenes memorized during training, and thus hardly scale to realistic datasets and scenarios. In this paper, we propose a generalized SCR model trained once to be deployed in new test scenes, regardless of their scale, without any finetuning. Instead of encoding the scene coordinates into the network weights",
    "arxiv_url": "https://arxiv.org/abs/2307.11702v3",
    "pdf_url": "https://arxiv.org/pdf/2307.11702v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2307.11702",
    "arxiv_authors": [
      "Jerome Revaud",
      "Yohann Cabon",
      "Romain Brégier",
      "JongMin Lee",
      "Philippe Weinzaepfel"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=SACReg%3A+Scene-Agnostic+Coordinate+Regression+for+Visual+Localization+Jerome+Revaud+Yohann+Cabon+Romain+Br%C3%A9gier+JongMin+Lee+Philippe+Weinzaepfel",
    "gs_search_success": true,
    "gs_authors": [
      "DvrEzyEAAAAJ",
      "asmBzogAAAAJ",
      "toKz9AQAAAAJ",
      "LSxIJ5cAAAAJ"
    ],
    "citation_count": 16,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2407.18673",
    "title": "A Survey on Cell Nuclei Instance Segmentation and Classification: Leveraging Context and Attention",
    "year": 2024,
    "published": "2024-07-26T11:30:22Z",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "Manually annotating nuclei from the gigapixel Hematoxylin and Eosin (H&E)-stained Whole Slide Images (WSIs) is a laborious and costly task, meaning automated algorithms for cell nuclei instance segmentation and classification could alleviate the workload of pathologists and clinical researchers and at the same time facilitate the automatic extraction of clinically interpretable features. But due to high intra- and inter-class variability of nuclei morphological and chromatic features, as well as",
    "arxiv_url": "https://arxiv.org/abs/2407.18673v1",
    "pdf_url": "https://arxiv.org/pdf/2407.18673v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2407.18673",
    "arxiv_authors": [
      "João D. Nunes",
      "Diana Montezuma",
      "Domingos Oliveira",
      "Tania Pereira",
      "Jaime S. Cardoso"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=A+Survey+on+Cell+Nuclei+Instance+Segmentation+and+Classification%3A+Leveraging+Context+and+Attention+Jo%C3%A3o+D.+Nunes+Diana+Montezuma+Domingos+Oliveira+Tania+Pereira+Jaime+S.+Cardoso",
    "gs_search_success": true,
    "gs_authors": [
      "yrsN28UAAAAJ",
      "eYZhwjsAAAAJ",
      "4JEUjEgAAAAJ",
      "qXSjBKYAAAAJ",
      "fadRi1YAAAAJ"
    ],
    "citation_count": 16,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2404.13873",
    "title": "Texture, Shape, Order, and Relation Matter: A New Transformer Design for Sequential DeepFake Detection",
    "year": 2024,
    "published": "2024-04-22T04:47:52Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Sequential DeepFake detection is an emerging task that predicts the manipulation sequence in order. Existing methods typically formulate it as an image-to-sequence problem, employing conventional Transformer architectures. However, these methods lack dedicated design and consequently result in limited performance. As such, this paper describes a new Transformer design, called {TSOM}, by exploring three perspectives: Texture, Shape, and Order of Manipulations. Our method features four major impro",
    "arxiv_url": "https://arxiv.org/abs/2404.13873v5",
    "pdf_url": "https://arxiv.org/pdf/2404.13873v5",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2404.13873",
    "arxiv_authors": [
      "Yunfei Li",
      "Yuezun Li",
      "Baoyuan Wu",
      "Junyu Dong",
      "Guopu Zhu",
      "Siwei Lyu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Texture%2C+Shape%2C+Order%2C+and+Relation+Matter%3A+A+New+Transformer+Design+for+Sequential+DeepFake+Detection+Yunfei+Li+Yuezun+Li+Baoyuan+Wu+Junyu+Dong+Guopu+Zhu",
    "gs_search_success": true,
    "gs_authors": [
      "tkz-3AgAAAAJ",
      "JNTG1KoAAAAJ",
      "Sq2q-E8AAAAJ",
      "iPYdUpAAAAAJ"
    ],
    "citation_count": 2,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2305.02325",
    "title": "Sex Detection in the Early Stage of Fertilized Chicken Eggs via Image Recognition",
    "year": 2023,
    "published": "2023-05-03T12:46:36Z",
    "categories": [
      "q-bio.QM",
      "cs.CV",
      "cs.LG",
      "eess.IV"
    ],
    "abstract": "Culling newly hatched male chicks in industrial hatcheries poses a serious ethical problem. Both laying and broiler breeders need males, but it is a problem because they are produced more than needed. Being able to determine the sex of chicks in the egg at the beginning or early stage of incubation can eliminate ethical problems as well as many additional costs. When we look at the literature, the methods used are very costly, low in applicability, invasive, inadequate in accuracy, or too late t",
    "arxiv_url": "https://arxiv.org/abs/2305.02325v1",
    "pdf_url": "https://arxiv.org/pdf/2305.02325v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2305.02325",
    "arxiv_authors": [
      "Ufuk Asil",
      "Efendi Nasibov"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Sex+Detection+in+the+Early+Stage+of+Fertilized+Chicken+Eggs+via+Image+Recognition+Ufuk+Asil+Efendi+Nasibov",
    "gs_search_success": true,
    "gs_authors": [
      "21gFPF0AAAAJ",
      "TWYpF_MAAAAJ"
    ],
    "citation_count": 3,
    "gs_author_count": 2
  },
  {
    "arxiv_id": "2502.05505",
    "title": "Differentially Private Synthetic Data via APIs 3: Using Simulators Instead of Foundation Model",
    "year": 2025,
    "published": "2025-02-08T09:50:30Z",
    "categories": [
      "cs.LG",
      "cs.CR",
      "cs.CV",
      "stat.ML"
    ],
    "abstract": "Differentially private (DP) synthetic data, which closely resembles the original private data while maintaining strong privacy guarantees, has become a key tool for unlocking the value of private data without compromising privacy. Recently, Private Evolution (PE) has emerged as a promising method for generating DP synthetic data. Unlike other training-based approaches, PE only requires access to inference APIs from foundation models, enabling it to harness the power of state-of-the-art (SoTA) mo",
    "arxiv_url": "https://arxiv.org/abs/2502.05505v3",
    "pdf_url": "https://arxiv.org/pdf/2502.05505v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2502.05505",
    "arxiv_authors": [
      "Zinan Lin",
      "Tadas Baltrusaitis",
      "Wenyu Wang",
      "Sergey Yekhanin"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Differentially+Private+Synthetic+Data+via+APIs+3%3A+Using+Simulators+Instead+of+Foundation+Model+Zinan+Lin+Tadas+Baltrusaitis+Wenyu+Wang+Sergey+Yekhanin",
    "gs_search_success": true,
    "gs_authors": [
      "xPqd-FMAAAAJ",
      "4WEQ8h0AAAAJ",
      "67nE-wQ_g_cC",
      "TUdv9A0AAAAJ"
    ],
    "citation_count": 8,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2405.20674",
    "title": "4Diffusion: Multi-view Video Diffusion Model for 4D Generation",
    "year": 2024,
    "published": "2024-05-31T08:18:39Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Current 4D generation methods have achieved noteworthy efficacy with the aid of advanced diffusion generative models. However, these methods lack multi-view spatial-temporal modeling and encounter challenges in integrating diverse prior knowledge from multiple diffusion models, resulting in inconsistent temporal appearance and flickers. In this paper, we propose a novel 4D generation pipeline, namely 4Diffusion, aimed at generating spatial-temporally consistent 4D content from a monocular video.",
    "arxiv_url": "https://arxiv.org/abs/2405.20674v2",
    "pdf_url": "https://arxiv.org/pdf/2405.20674v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2405.20674",
    "arxiv_authors": [
      "Haiyu Zhang",
      "Xinyuan Chen",
      "Yaohui Wang",
      "Xihui Liu",
      "Yunhong Wang",
      "Yu Qiao"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=4Diffusion%3A+Multi-view+Video+Diffusion+Model+for+4D+Generation+Haiyu+Zhang+Xinyuan+Chen+Yaohui+Wang+Xihui+Liu+Yunhong+Wang",
    "gs_search_success": true,
    "gs_authors": [
      "4YL23GMAAAAJ",
      "R7LyAb4AAAAJ",
      "3fWSC8YAAAAJ",
      "gFtI-8QAAAAJ",
      "9B1t_zIAAAAJ"
    ],
    "citation_count": 69,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2302.03299",
    "title": "3D Vessel Segmentation with Limited Guidance of 2D Structure-agnostic Vessel Annotations",
    "year": 2023,
    "published": "2023-02-07T07:26:00Z",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "abstract": "Delineating 3D blood vessels is essential for clinical diagnosis and treatment, however, is challenging due to complex structure variations and varied imaging conditions. Supervised deep learning has demonstrated its superior capacity in automatic 3D vessel segmentation. However, the reliance on expensive 3D manual annotations and limited capacity for annotation reuse hinder the clinical applications of supervised models. To avoid the repetitive and laborious annotating and make full use of exis",
    "arxiv_url": "https://arxiv.org/abs/2302.03299v1",
    "pdf_url": "https://arxiv.org/pdf/2302.03299v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2302.03299",
    "arxiv_authors": [
      "Huai Chen",
      "Xiuying Wang",
      "Lisheng Wang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=3D+Vessel+Segmentation+with+Limited+Guidance+of+2D+Structure-agnostic+Vessel+Annotations+Huai+Chen+Xiuying+Wang+Lisheng+Wang",
    "gs_search_success": true,
    "gs_authors": [
      "ZJaLNikAAAAJ",
      "K8lVq1YAAAAJ"
    ],
    "citation_count": 10,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2308.03060",
    "title": "TOPIQ: A Top-down Approach from Semantics to Distortions for Image Quality Assessment",
    "year": 2023,
    "published": "2023-08-06T09:08:37Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Image Quality Assessment (IQA) is a fundamental task in computer vision that has witnessed remarkable progress with deep neural networks. Inspired by the characteristics of the human visual system, existing methods typically use a combination of global and local representations (\\ie, multi-scale features) to achieve superior performance. However, most of them adopt simple linear fusion of multi-scale features, and neglect their possibly complex relationship and interaction. In contrast, humans t",
    "arxiv_url": "https://arxiv.org/abs/2308.03060v1",
    "pdf_url": "https://arxiv.org/pdf/2308.03060v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2308.03060",
    "arxiv_authors": [
      "Chaofeng Chen",
      "Jiadi Mo",
      "Jingwen Hou",
      "Haoning Wu",
      "Liang Liao",
      "Wenxiu Sun",
      "Qiong Yan",
      "Weisi Lin"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=TOPIQ%3A+A+Top-down+Approach+from+Semantics+to+Distortions+for+Image+Quality+Assessment+Chaofeng+Chen+Jiadi+Mo+Jingwen+Hou+Haoning+Wu+Liang+Liao",
    "gs_search_success": true,
    "gs_authors": [
      "wth-VbMAAAAJ",
      "D_S41X4AAAAJ",
      "X9lE6O4AAAAJ",
      "uT9CtPYAAAAJ",
      "kqTUHSIAAAAJ",
      "NlNOyiQAAAAJ",
      "lxiqnI0AAAAJ"
    ],
    "citation_count": 267,
    "gs_author_count": 8
  },
  {
    "arxiv_id": "2304.07072",
    "title": "CornerFormer: Boosting Corner Representation for Fine-Grained Structured Reconstruction",
    "year": 2023,
    "published": "2023-04-14T11:51:26Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Structured reconstruction is a non-trivial dense prediction problem, which extracts structural information (\\eg, building corners and edges) from a raster image, then reconstructs it to a 2D planar graph accordingly. Compared with common segmentation or detection problems, it significantly relays on the capability that leveraging holistic geometric information for structural reasoning. Current transformer-based approaches tackle this challenging problem in a two-stage manner, which detect corner",
    "arxiv_url": "https://arxiv.org/abs/2304.07072v4",
    "pdf_url": "https://arxiv.org/pdf/2304.07072v4",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2304.07072",
    "arxiv_authors": [
      "Hongbo Tian",
      "Yulong Li",
      "Linzhi Huang",
      "Xu Ling",
      "Yue Yang",
      "Jiani Hu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=CornerFormer%3A+Boosting+Corner+Representation+for+Fine-Grained+Structured+Reconstruction+Hongbo+Tian+Yulong+Li+Linzhi+Huang+Xu+Ling+Yue+Yang",
    "gs_search_success": true,
    "gs_authors": [
      "oC75phkAAAAJ",
      "Ld2hgWQAAAAJ",
      "hYQb9goAAAAJ",
      "x8CoMv4AAAAJ"
    ],
    "citation_count": 2,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2411.04826",
    "title": "D$^3$epth: Self-Supervised Depth Estimation with Dynamic Mask in Dynamic Scenes",
    "year": 2024,
    "published": "2024-11-07T16:07:00Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "abstract": "Depth estimation is a crucial technology in robotics. Recently, self-supervised depth estimation methods have demonstrated great potential as they can efficiently leverage large amounts of unlabelled real-world data. However, most existing methods are designed under the assumption of static scenes, which hinders their adaptability in dynamic environments. To address this issue, we present D$^3$epth, a novel method for self-supervised depth estimation in dynamic scenes. It tackles the challenge o",
    "arxiv_url": "https://arxiv.org/abs/2411.04826v1",
    "pdf_url": "https://arxiv.org/pdf/2411.04826v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2411.04826",
    "arxiv_authors": [
      "Siyu Chen",
      "Hong Liu",
      "Wenhao Li",
      "Ying Zhu",
      "Guoquan Wang",
      "Jianbing Wu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=D%24%5E3%24epth%3A+Self-Supervised+Depth+Estimation+with+Dynamic+Mask+in+Dynamic+Scenes+Siyu+Chen+Hong+Liu+Wenhao+Li+Ying+Zhu+Guoquan+Wang",
    "gs_search_success": true,
    "gs_authors": [
      "WLMUAjsAAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2411.09604",
    "title": "Local-Global Attention: An Adaptive Mechanism for Multi-Scale Feature Integration",
    "year": 2024,
    "published": "2024-11-14T17:22:16Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "In recent years, attention mechanisms have significantly enhanced the performance of object detection by focusing on key feature information. However, prevalent methods still encounter difficulties in effectively balancing local and global features. This imbalance hampers their ability to capture both fine-grained details and broader contextual information-two critical elements for achieving accurate object detection.To address these challenges, we propose a novel attention mechanism, termed Loc",
    "arxiv_url": "https://arxiv.org/abs/2411.09604v1",
    "pdf_url": "https://arxiv.org/pdf/2411.09604v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2411.09604",
    "arxiv_authors": [
      "Yifan Shao"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Local-Global+Attention%3A+An+Adaptive+Mechanism+for+Multi-Scale+Feature+Integration+Yifan+Shao",
    "gs_search_success": true,
    "gs_authors": [],
    "citation_count": 10,
    "gs_author_count": 1
  },
  {
    "arxiv_id": "2308.15236",
    "title": "Rotation Augmented Distillation for Exemplar-Free Class Incremental Learning with Detailed Analysis",
    "year": 2023,
    "published": "2023-08-29T11:51:27Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Class incremental learning (CIL) aims to recognize both the old and new classes along the increment tasks. Deep neural networks in CIL suffer from catastrophic forgetting and some approaches rely on saving exemplars from previous tasks, known as the exemplar-based setting, to alleviate this problem. On the contrary, this paper focuses on the Exemplar-Free setting with no old class sample preserved. Balancing the plasticity and stability in deep feature learning with only supervision from new cla",
    "arxiv_url": "https://arxiv.org/abs/2308.15236v2",
    "pdf_url": "https://arxiv.org/pdf/2308.15236v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2308.15236",
    "arxiv_authors": [
      "Xiuwei Chen",
      "Xiaobin Chang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Rotation+Augmented+Distillation+for+Exemplar-Free+Class+Incremental+Learning+with+Detailed+Analysis+Xiuwei+Chen+Xiaobin+Chang",
    "gs_search_success": true,
    "gs_authors": [
      "nDS74T4AAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 2
  },
  {
    "arxiv_id": "2310.14226",
    "title": "Multi-stream Cell Segmentation with Low-level Cues for Multi-modality Images",
    "year": 2023,
    "published": "2023-10-22T08:11:08Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Cell segmentation for multi-modal microscopy images remains a challenge due to the complex textures, patterns, and cell shapes in these images. To tackle the problem, we first develop an automatic cell classification pipeline to label the microscopy images based on their low-level image characteristics, and then train a classification model based on the category labels. Afterward, we train a separate segmentation model for each category using the images in the corresponding category. Besides, we",
    "arxiv_url": "https://arxiv.org/abs/2310.14226v1",
    "pdf_url": "https://arxiv.org/pdf/2310.14226v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2310.14226",
    "arxiv_authors": [
      "Wei Lou",
      "Xinyi Yu",
      "Chenyu Liu",
      "Xiang Wan",
      "Guanbin Li",
      "Siqi Liu",
      "Haofeng Li"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Multi-stream+Cell+Segmentation+with+Low-level+Cues+for+Multi-modality+Images+Wei+Lou+Xinyi+Yu+Chenyu+Liu+Xiang+Wan+Guanbin+Li",
    "gs_search_success": true,
    "gs_authors": [
      "hNHI_FwAAAAJ",
      "xdAReagAAAAJ",
      "e3_kWigAAAAJ",
      "k3g4oRgAAAAJ",
      "2A2Bx2UAAAAJ",
      "ugBEYC4AAAAJ",
      "jH-qGUcAAAAJ"
    ],
    "citation_count": 15,
    "gs_author_count": 7
  },
  {
    "arxiv_id": "2310.02255",
    "title": "MathVista: Evaluating Mathematical Reasoning of Foundation Models in Visual Contexts",
    "year": 2023,
    "published": "2023-10-03T17:57:24Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "abstract": "Large Language Models (LLMs) and Large Multimodal Models (LMMs) exhibit impressive problem-solving skills in many tasks and domains, but their ability in mathematical reasoning in visual contexts has not been systematically studied. To bridge this gap, we present MathVista, a benchmark designed to combine challenges from diverse mathematical and visual tasks. It consists of 6,141 examples, derived from 28 existing multimodal datasets involving mathematics and 3 newly created datasets (i.e., IQTe",
    "arxiv_url": "https://arxiv.org/abs/2310.02255v3",
    "pdf_url": "https://arxiv.org/pdf/2310.02255v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2310.02255",
    "arxiv_authors": [
      "Pan Lu",
      "Hritik Bansal",
      "Tony Xia",
      "Jiacheng Liu",
      "Chunyuan Li",
      "Hannaneh Hajishirzi",
      "Hao Cheng",
      "Kai-Wei Chang",
      "Michel Galley",
      "Jianfeng Gao"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=MathVista%3A+Evaluating+Mathematical+Reasoning+of+Foundation+Models+in+Visual+Contexts+Pan+Lu+Hritik+Bansal+Tony+Xia+Jiacheng+Liu+Chunyuan+Li",
    "gs_search_success": true,
    "gs_authors": [
      "Zd7WmXUAAAAJ",
      "LOV6_WIAAAAJ",
      "fqDBtzYAAAAJ",
      "gAKTYtoAAAAJ",
      "IyucsdQAAAAJ",
      "d9s3sbQAAAAJ",
      "rs1M7CAAAAAJ",
      "CQ1cqKkAAAAJ",
      "GJfoBZAAAAAJ",
      "ebx3ZKIAAAAJ"
    ],
    "citation_count": 1116,
    "gs_author_count": 9
  },
  {
    "arxiv_id": "2503.11609",
    "title": "Rethinking Few-Shot Adaptation of Vision-Language Models in Two Stages",
    "year": 2025,
    "published": "2025-03-14T17:24:01Z",
    "categories": [
      "cs.CV",
      "cs.LG",
      "cs.MM"
    ],
    "abstract": "An old-school recipe for training a classifier is to (i) learn a good feature extractor and (ii) optimize a linear layer atop. When only a handful of samples are available per category, as in Few-Shot Adaptation (FSA), data are insufficient to fit a large number of parameters, rendering the above impractical. This is especially true with large pre-trained Vision-Language Models (VLMs), which motivated successful research at the intersection of Parameter-Efficient Fine-tuning (PEFT) and FSA. In t",
    "arxiv_url": "https://arxiv.org/abs/2503.11609v1",
    "pdf_url": "https://arxiv.org/pdf/2503.11609v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2503.11609",
    "arxiv_authors": [
      "Matteo Farina",
      "Massimiliano Mancini",
      "Giovanni Iacca",
      "Elisa Ricci"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Rethinking+Few-Shot+Adaptation+of+Vision-Language+Models+in+Two+Stages+Matteo+Farina+Massimiliano+Mancini+Giovanni+Iacca+Elisa+Ricci",
    "gs_search_success": false,
    "gs_authors": [],
    "citation_count": null,
    "error_type": "fetch_failed",
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2411.12817",
    "title": "What Makes a Good Dataset for Knowledge Distillation?",
    "year": 2024,
    "published": "2024-11-19T19:10:12Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Knowledge distillation (KD) has been a popular and effective method for model compression. One important assumption of KD is that the teacher's original dataset will also be available when training the student. However, in situations such as continual learning and distilling large models trained on company-withheld datasets, having access to the original data may not always be possible. This leads practitioners towards utilizing other sources of supplemental data, which could yield mixed results",
    "arxiv_url": "https://arxiv.org/abs/2411.12817v2",
    "pdf_url": "https://arxiv.org/pdf/2411.12817v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2411.12817",
    "arxiv_authors": [
      "Logan Frank",
      "Jim Davis"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=What+Makes+a+Good+Dataset+for+Knowledge+Distillation%3F+Logan+Frank+Jim+Davis",
    "gs_search_success": true,
    "gs_authors": [
      "Jd6nw80AAAAJ",
      "--1OGZUAAAAJ"
    ],
    "citation_count": 4,
    "gs_author_count": 2
  },
  {
    "arxiv_id": "2505.17982",
    "title": "Few-Shot Learning from Gigapixel Images via Hierarchical Vision-Language Alignment and Modeling",
    "year": 2025,
    "published": "2025-05-23T14:48:32Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Vision-language models (VLMs) have recently been integrated into multiple instance learning (MIL) frameworks to address the challenge of few-shot, weakly supervised classification of whole slide images (WSIs). A key trend involves leveraging multi-scale information to better represent hierarchical tissue structures. However, existing methods often face two key limitations: (1) insufficient modeling of interactions within the same modalities across scales (e.g., 5x and 20x) and (2) inadequate ali",
    "arxiv_url": "https://arxiv.org/abs/2505.17982v4",
    "pdf_url": "https://arxiv.org/pdf/2505.17982v4",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2505.17982",
    "arxiv_authors": [
      "Bryan Wong",
      "Jong Woo Kim",
      "Huazhu Fu",
      "Mun Yong Yi"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Few-Shot+Learning+from+Gigapixel+Images+via+Hierarchical+Vision-Language+Alignment+and+Modeling+Bryan+Wong+Jong+Woo+Kim+Huazhu+Fu+Mun+Yong+Yi",
    "gs_search_success": true,
    "gs_authors": [
      "4RmNmQcAAAAJ",
      "jCvUBYMAAAAJ",
      "C8QGzCoAAAAJ",
      "liLmE2oAAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2304.10250",
    "title": "Revisiting Implicit Neural Representations in Low-Level Vision",
    "year": 2023,
    "published": "2023-04-20T12:19:27Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Implicit Neural Representation (INR) has been emerging in computer vision in recent years. It has been shown to be effective in parameterising continuous signals such as dense 3D models from discrete image data, e.g. the neural radius field (NeRF). However, INR is under-explored in 2D image processing tasks. Considering the basic definition and the structure of INR, we are interested in its effectiveness in low-level vision problems such as image restoration. In this work, we revisit INR and inv",
    "arxiv_url": "https://arxiv.org/abs/2304.10250v1",
    "pdf_url": "https://arxiv.org/pdf/2304.10250v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2304.10250",
    "arxiv_authors": [
      "Wentian Xu",
      "Jianbo Jiao"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Revisiting+Implicit+Neural+Representations+in+Low-Level+Vision+Wentian+Xu+Jianbo+Jiao",
    "gs_search_success": true,
    "gs_authors": [
      "JpeypkIAAAAJ",
      "HkEiMMwAAAAJ"
    ],
    "citation_count": 12,
    "gs_author_count": 2
  },
  {
    "arxiv_id": "2405.18684",
    "title": "Learning Diffeomorphism for Image Registration with Time-Continuous Networks using Semigroup Regularization",
    "year": 2024,
    "published": "2024-05-29T01:25:43Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Diffeomorphic image registration (DIR) is a fundamental task in 3D medical image analysis that seeks topology-preserving deformations between image pairs. To ensure diffeomorphism, a common approach is to model the deformation field as the flow map solution of a differential equation, which is solved using efficient schemes such as scaling and squaring along with multiple smoothness regularization terms. In this paper, we propose a novel learning-based approach for diffeomorphic 3D image registr",
    "arxiv_url": "https://arxiv.org/abs/2405.18684v3",
    "pdf_url": "https://arxiv.org/pdf/2405.18684v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2405.18684",
    "arxiv_authors": [
      "Mohammadjavad Matinkia",
      "Nilanjan Ray"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Learning+Diffeomorphism+for+Image+Registration+with+Time-Continuous+Networks+using+Semigroup+Regularization+Mohammadjavad+Matinkia+Nilanjan+Ray",
    "gs_search_success": true,
    "gs_authors": [
      "elk5VlMAAAAJ",
      "E3wuLqAAAAAJ"
    ],
    "citation_count": 2,
    "gs_author_count": 2
  },
  {
    "arxiv_id": "2301.12058",
    "title": "Aerial Image Object Detection With Vision Transformer Detector (ViTDet)",
    "year": 2023,
    "published": "2023-01-28T02:25:30Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "The past few years have seen an increased interest in aerial image object detection due to its critical value to large-scale geo-scientific research like environmental studies, urban planning, and intelligence monitoring. However, the task is very challenging due to the birds-eye view perspective, complex backgrounds, large and various image sizes, different appearances of objects, and the scarcity of well-annotated datasets. Recent advances in computer vision have shown promise tackling the cha",
    "arxiv_url": "https://arxiv.org/abs/2301.12058v2",
    "pdf_url": "https://arxiv.org/pdf/2301.12058v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2301.12058",
    "arxiv_authors": [
      "Liya Wang",
      "Alex Tien"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Aerial+Image+Object+Detection+With+Vision+Transformer+Detector+%28ViTDet%29+Liya+Wang+Alex+Tien",
    "gs_search_success": true,
    "gs_authors": [],
    "citation_count": 32,
    "gs_author_count": 2
  },
  {
    "arxiv_id": "2405.17261",
    "title": "Does Diffusion Beat GAN in Image Super Resolution?",
    "year": 2024,
    "published": "2024-05-27T15:19:59Z",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "abstract": "There is a prevalent opinion that diffusion-based models outperform GAN-based counterparts in the Image Super Resolution (ISR) problem. However, in most studies, diffusion-based ISR models employ larger networks and are trained longer than the GAN baselines. This raises the question of whether the high performance stems from the superiority of the diffusion paradigm or if it is a consequence of the increased scale and the greater computational resources of the contemporary studies. In our work, ",
    "arxiv_url": "https://arxiv.org/abs/2405.17261v2",
    "pdf_url": "https://arxiv.org/pdf/2405.17261v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2405.17261",
    "arxiv_authors": [
      "Denis Kuznedelev",
      "Valerii Startsev",
      "Daniil Shlenskii",
      "Sergey Kastryulin"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Does+Diffusion+Beat+GAN+in+Image+Super+Resolution%3F+Denis+Kuznedelev+Valerii+Startsev+Daniil+Shlenskii+Sergey+Kastryulin",
    "gs_search_success": true,
    "gs_authors": [
      "L78B2lcAAAAJ",
      "sE6tHDEAAAAJ",
      "765_fJYAAAAJ"
    ],
    "citation_count": 13,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2304.02211",
    "title": "METransformer: Radiology Report Generation by Transformer with Multiple Learnable Expert Tokens",
    "year": 2023,
    "published": "2023-04-05T03:54:28Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "In clinical scenarios, multi-specialist consultation could significantly benefit the diagnosis, especially for intricate cases. This inspires us to explore a \"multi-expert joint diagnosis\" mechanism to upgrade the existing \"single expert\" framework commonly seen in the current literature. To this end, we propose METransformer, a method to realize this idea with a transformer-based backbone. The key design of our method is the introduction of multiple learnable \"expert\" tokens into both the trans",
    "arxiv_url": "https://arxiv.org/abs/2304.02211v1",
    "pdf_url": "https://arxiv.org/pdf/2304.02211v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2304.02211",
    "arxiv_authors": [
      "Zhanyu Wang",
      "Lingqiao Liu",
      "Lei Wang",
      "Luping Zhou"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=METransformer%3A+Radiology+Report+Generation+by+Transformer+with+Multiple+Learnable+Expert+Tokens+Zhanyu+Wang+Lingqiao+Liu+Lei+Wang+Luping+Zhou",
    "gs_search_success": true,
    "gs_authors": [
      "SgofT2MAAAAJ",
      "5ClujcoAAAAJ",
      "Y2xu62UAAAAJ",
      "maeFb38AAAAJ"
    ],
    "citation_count": 198,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2506.03194",
    "title": "HueManity: Probing Fine-Grained Visual Perception in MLLMs",
    "year": 2025,
    "published": "2025-05-31T22:59:48Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "abstract": "Multimodal Large Language Models (MLLMs) excel at high-level visual reasoning, but their performance on nuanced perceptual tasks remains surprisingly limited. We present HueManity, a benchmark designed to assess visual perception in MLLMs. The dataset comprises 83,850 images featuring two-character alphanumeric strings embedded in Ishihara test style dot patterns, challenging models on precise pattern recognition. Our evaluation of nine state-of-the-art MLLMs on HueManity demonstrates a signific",
    "arxiv_url": "https://arxiv.org/abs/2506.03194v4",
    "pdf_url": "https://arxiv.org/pdf/2506.03194v4",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2506.03194",
    "arxiv_authors": [
      "Rynaa Grover",
      "Jayant Sravan Tamarapalli",
      "Sahiti Yerramilli",
      "Nilay Pande"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=HueManity%3A+Probing+Fine-Grained+Visual+Perception+in+MLLMs+Rynaa+Grover+Jayant+Sravan+Tamarapalli+Sahiti+Yerramilli+Nilay+Pande",
    "gs_search_success": true,
    "gs_authors": [
      "T0d7vyQAAAAJ",
      "LSgOh68AAAAJ",
      "pkcL6E8AAAAJ",
      "0zPayWAAAAAJ"
    ],
    "citation_count": 6,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2401.05772",
    "title": "Knowledge Translation: A New Pathway for Model Compression",
    "year": 2024,
    "published": "2024-01-11T09:25:42Z",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "abstract": "Deep learning has witnessed significant advancements in recent years at the cost of increasing training, inference, and model storage overhead. While existing model compression methods strive to reduce the number of model parameters while maintaining high accuracy, they inevitably necessitate the re-training of the compressed model or impose architectural constraints. To overcome these limitations, this paper presents a novel framework, termed \\textbf{K}nowledge \\textbf{T}ranslation (KT), wherei",
    "arxiv_url": "https://arxiv.org/abs/2401.05772v1",
    "pdf_url": "https://arxiv.org/pdf/2401.05772v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2401.05772",
    "arxiv_authors": [
      "Wujie Sun",
      "Defang Chen",
      "Jiawei Chen",
      "Yan Feng",
      "Chun Chen",
      "Can Wang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Knowledge+Translation%3A+A+New+Pathway+for+Model+Compression+Wujie+Sun+Defang+Chen+Jiawei+Chen+Yan+Feng+Chun+Chen",
    "gs_search_success": true,
    "gs_authors": [
      "Ic-uOv4AAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2309.01487",
    "title": "GenSelfDiff-HIS: Generative Self-Supervision Using Diffusion for Histopathological Image Segmentation",
    "year": 2023,
    "published": "2023-09-04T09:49:24Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Histopathological image segmentation is a laborious and time-intensive task, often requiring analysis from experienced pathologists for accurate examinations. To reduce this burden, supervised machine-learning approaches have been adopted using large-scale annotated datasets for histopathological image analysis. However, in several scenarios, the availability of large-scale annotated data is a bottleneck while training such models. Self-supervised learning (SSL) is an alternative paradigm that p",
    "arxiv_url": "https://arxiv.org/abs/2309.01487v2",
    "pdf_url": "https://arxiv.org/pdf/2309.01487v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2309.01487",
    "arxiv_authors": [
      "Vishnuvardhan Purma",
      "Suhas Srinath",
      "Seshan Srirangarajan",
      "Aanchal Kakkar",
      "Prathosh A. P"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=GenSelfDiff-HIS%3A+Generative+Self-Supervision+Using+Diffusion+for+Histopathological+Image+Segmentation+Vishnuvardhan+Purma+Suhas+Srinath+Seshan+Srirangarajan+Aanchal+Kakkar+Prathosh+A.+P",
    "gs_search_success": true,
    "gs_authors": [
      "wMnD6HkAAAAJ",
      "OEwV4bsAAAAJ",
      "ijFjQ7oAAAAJ",
      "KFNUgn8AAAAJ"
    ],
    "citation_count": 12,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2502.16799",
    "title": "Hierarchical Semantic Compression for Consistent Image Semantic Restoration",
    "year": 2025,
    "published": "2025-02-24T03:20:44Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "The emerging semantic compression has been receiving increasing research efforts most recently, capable of achieving high fidelity restoration during compression, even at extremely low bitrates. However, existing semantic compression methods typically combine standard pipelines with either pre-defined or high-dimensional semantics, thus suffering from deficiency in compression. To address this issue, we propose a novel hierarchical semantic compression (HSC) framework that purely operates within",
    "arxiv_url": "https://arxiv.org/abs/2502.16799v1",
    "pdf_url": "https://arxiv.org/pdf/2502.16799v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2502.16799",
    "arxiv_authors": [
      "Shengxi Li",
      "Zifu Zhang",
      "Mai Xu",
      "Lai Jiang",
      "Yufan Liu",
      "Ce Zhu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Hierarchical+Semantic+Compression+for+Consistent+Image+Semantic+Restoration+Shengxi+Li+Zifu+Zhang+Mai+Xu+Lai+Jiang+Yufan+Liu",
    "gs_search_success": true,
    "gs_authors": [
      "CHymRYQAAAAJ",
      "vUbf2i8AAAAJ",
      "GNxRL-gAAAAJ",
      "l5Zz_FcAAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2403.11504",
    "title": "MLVICX: Multi-Level Variance-Covariance Exploration for Chest X-ray Self-Supervised Representation Learning",
    "year": 2024,
    "published": "2024-03-18T06:19:37Z",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "Self-supervised learning (SSL) is potentially useful in reducing the need for manual annotation and making deep learning models accessible for medical image analysis tasks. By leveraging the representations learned from unlabeled data, self-supervised models perform well on tasks that require little to no fine-tuning. However, for medical images, like chest X-rays, which are characterized by complex anatomical structures and diverse clinical conditions, there arises a need for representation lea",
    "arxiv_url": "https://arxiv.org/abs/2403.11504v1",
    "pdf_url": "https://arxiv.org/pdf/2403.11504v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2403.11504",
    "arxiv_authors": [
      "Azad Singh",
      "Vandan Gorade",
      "Deepak Mishra"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=MLVICX%3A+Multi-Level+Variance-Covariance+Exploration+for+Chest+X-ray+Self-Supervised+Representation+Learning+Azad+Singh+Vandan+Gorade+Deepak+Mishra",
    "gs_search_success": true,
    "gs_authors": [
      "rbFzA8gAAAAJ",
      "D0HlgOcAAAAJ",
      "OsJYrdEAAAAJ"
    ],
    "citation_count": 6,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2406.05927",
    "title": "MeanSparse: Post-Training Robustness Enhancement Through Mean-Centered Feature Sparsification",
    "year": 2024,
    "published": "2024-06-09T22:14:55Z",
    "categories": [
      "cs.CV",
      "cs.CR",
      "cs.LG"
    ],
    "abstract": "We present a simple yet effective method to improve the robustness of both Convolutional and attention-based Neural Networks against adversarial examples by post-processing an adversarially trained model. Our technique, MeanSparse, cascades the activation functions of a trained model with novel operators that sparsify mean-centered feature vectors. This is equivalent to reducing feature variations around the mean, and we show that such reduced variations merely affect the model's utility, yet th",
    "arxiv_url": "https://arxiv.org/abs/2406.05927v3",
    "pdf_url": "https://arxiv.org/pdf/2406.05927v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2406.05927",
    "arxiv_authors": [
      "Sajjad Amini",
      "Mohammadreza Teymoorianfard",
      "Shiqing Ma",
      "Amir Houmansadr"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=MeanSparse%3A+Post-Training+Robustness+Enhancement+Through+Mean-Centered+Feature+Sparsification+Sajjad+Amini+Mohammadreza+Teymoorianfard+Shiqing+Ma+Amir+Houmansadr",
    "gs_search_success": true,
    "gs_authors": [
      "X_mDnjkAAAAJ",
      "bRB2UaYAAAAJ",
      "24GngZYAAAAJ",
      "cTTFHNwAAAAJ"
    ],
    "citation_count": 25,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2405.05811",
    "title": "Parallel Cross Strip Attention Network for Single Image Dehazing",
    "year": 2024,
    "published": "2024-05-09T14:50:07Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "The objective of single image dehazing is to restore hazy images and produce clear, high-quality visuals. Traditional convolutional models struggle with long-range dependencies due to their limited receptive field size. While Transformers excel at capturing such dependencies, their quadratic computational complexity in relation to feature map resolution makes them less suitable for pixel-to-pixel dense prediction tasks. Moreover, fixed kernels or tokens in most models do not adapt well to varyin",
    "arxiv_url": "https://arxiv.org/abs/2405.05811v1",
    "pdf_url": "https://arxiv.org/pdf/2405.05811v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2405.05811",
    "arxiv_authors": [
      "Lihan Tong",
      "Yun Liu",
      "Tian Ye",
      "Weijia Li",
      "Liyuan Chen",
      "Erkang Chen"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Parallel+Cross+Strip+Attention+Network+for+Single+Image+Dehazing+Lihan+Tong+Yun+Liu+Tian+Ye+Weijia+Li+Liyuan+Chen",
    "gs_search_success": true,
    "gs_authors": [
      "hWo1RTsAAAAJ",
      "9fjHp-EAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2403.18258",
    "title": "Enhancing Generative Class Incremental Learning Performance with Model Forgetting Approach",
    "year": 2024,
    "published": "2024-03-27T05:10:38Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "This study presents a novel approach to Generative Class Incremental Learning (GCIL) by introducing the forgetting mechanism, aimed at dynamically managing class information for better adaptation to streaming data. GCIL is one of the hot topics in the field of computer vision, and this is considered one of the crucial tasks in society, specifically the continual learning of generative models. The ability to forget is a crucial brain function that facilitates continual learning by selectively dis",
    "arxiv_url": "https://arxiv.org/abs/2403.18258v1",
    "pdf_url": "https://arxiv.org/pdf/2403.18258v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2403.18258",
    "arxiv_authors": [
      "Taro Togo",
      "Ren Togo",
      "Keisuke Maeda",
      "Takahiro Ogawa",
      "Miki Haseyama"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Enhancing+Generative+Class+Incremental+Learning+Performance+with+Model+Forgetting+Approach+Taro+Togo+Ren+Togo+Keisuke+Maeda+Takahiro+Ogawa+Miki+Haseyama",
    "gs_search_success": true,
    "gs_authors": [
      "HNaVCtUAAAAJ",
      "vPixFIsAAAAJ",
      "ubMXV98AAAAJ",
      "5sZKrK0AAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2504.02555",
    "title": "Noise Calibration and Spatial-Frequency Interactive Network for STEM Image Enhancement",
    "year": 2025,
    "published": "2025-04-03T13:11:57Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Scanning Transmission Electron Microscopy (STEM) enables the observation of atomic arrangements at sub-angstrom resolution, allowing for atomically resolved analysis of the physical and chemical properties of materials. However, due to the effects of noise, electron beam damage, sample thickness, etc, obtaining satisfactory atomic-level images is often challenging. Enhancing STEM images can reveal clearer structural details of materials. Nonetheless, existing STEM image enhancement methods usual",
    "arxiv_url": "https://arxiv.org/abs/2504.02555v1",
    "pdf_url": "https://arxiv.org/pdf/2504.02555v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2504.02555",
    "arxiv_authors": [
      "Hesong Li",
      "Ziqi Wu",
      "Ruiwen Shao",
      "Tao Zhang",
      "Ying Fu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Noise+Calibration+and+Spatial-Frequency+Interactive+Network+for+STEM+Image+Enhancement+Hesong+Li+Ziqi+Wu+Ruiwen+Shao+Tao+Zhang+Ying+Fu",
    "gs_search_success": true,
    "gs_authors": [
      "BfbSy4oAAAAJ",
      "PE4xMlkAAAAJ"
    ],
    "citation_count": 11,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2501.12390",
    "title": "GPS as a Control Signal for Image Generation",
    "year": 2025,
    "published": "2025-01-21T18:59:46Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "We show that the GPS tags contained in photo metadata provide a useful control signal for image generation. We train GPS-to-image models and use them for tasks that require a fine-grained understanding of how images vary within a city. In particular, we train a diffusion model to generate images conditioned on both GPS and text. The learned model generates images that capture the distinctive appearance of different neighborhoods, parks, and landmarks. We also extract 3D models from 2D GPS-to-ima",
    "arxiv_url": "https://arxiv.org/abs/2501.12390v2",
    "pdf_url": "https://arxiv.org/pdf/2501.12390v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2501.12390",
    "arxiv_authors": [
      "Chao Feng",
      "Ziyang Chen",
      "Aleksander Holynski",
      "Alexei A. Efros",
      "Andrew Owens"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=GPS+as+a+Control+Signal+for+Image+Generation+Chao+Feng+Ziyang+Chen+Aleksander+Holynski+Alexei+A.+Efros+Andrew+Owens",
    "gs_search_success": true,
    "gs_authors": [
      "9hX-JksAAAAJ",
      "ZI0tPeMAAAAJ",
      "d97bGd8AAAAJ",
      "PbsR83sAAAAJ",
      "ypBMJMgAAAAJ"
    ],
    "citation_count": 2,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2309.09513",
    "title": "Learning Parallax for Stereo Event-based Motion Deblurring",
    "year": 2023,
    "published": "2023-09-18T06:51:41Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Due to the extremely low latency, events have been recently exploited to supplement lost information for motion deblurring. Existing approaches largely rely on the perfect pixel-wise alignment between intensity images and events, which is not always fulfilled in the real world. To tackle this problem, we propose a novel coarse-to-fine framework, named NETwork of Event-based motion Deblurring with STereo event and intensity cameras (St-EDNet), to recover high-quality images directly from the misa",
    "arxiv_url": "https://arxiv.org/abs/2309.09513v1",
    "pdf_url": "https://arxiv.org/pdf/2309.09513v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2309.09513",
    "arxiv_authors": [
      "Mingyuan Lin",
      "Chi Zhang",
      "Chu He",
      "Lei Yu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Learning+Parallax+for+Stereo+Event-based+Motion+Deblurring+Mingyuan+Lin+Chi+Zhang+Chu+He+Lei+Yu",
    "gs_search_success": true,
    "gs_authors": [
      "jTguKTEAAAAJ",
      "pfhoSpUAAAAJ",
      "mvo0cL8AAAAJ",
      "Klc_GHUAAAAJ"
    ],
    "citation_count": 4,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2505.12998",
    "title": "A Skull-Adaptive Framework for AI-Based 3D Transcranial Focused Ultrasound Simulation",
    "year": 2025,
    "published": "2025-05-19T11:37:51Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Transcranial focused ultrasound (tFUS) is an emerging modality for non-invasive brain stimulation and therapeutic intervention, offering millimeter-scale spatial precision and the ability to target deep brain structures. However, the heterogeneous and anisotropic nature of the human skull introduces significant distortions to the propagating ultrasound wavefront, which require time-consuming patient-specific planning and corrections using numerical solvers for accurate targeting. To enable data-",
    "arxiv_url": "https://arxiv.org/abs/2505.12998v1",
    "pdf_url": "https://arxiv.org/pdf/2505.12998v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2505.12998",
    "arxiv_authors": [
      "Vinkle Srivastav",
      "Juliette Puel",
      "Jonathan Vappou",
      "Elijah Van Houten",
      "Paolo Cabras",
      "Nicolas Padoy"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=A+Skull-Adaptive+Framework+for+AI-Based+3D+Transcranial+Focused+Ultrasound+Simulation+Vinkle+Srivastav+Juliette+Puel+Jonathan+Vappou+Elijah+Van+Houten+Paolo+Cabras",
    "gs_search_success": true,
    "gs_authors": [
      "FDWwdoQAAAAJ",
      "aj84iHAAAAAJ",
      "Wo1vWYoAAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2407.18614",
    "title": "LookupForensics: A Large-Scale Multi-Task Dataset for Multi-Phase Image-Based Fact Verification",
    "year": 2024,
    "published": "2024-07-26T09:15:29Z",
    "categories": [
      "cs.CV",
      "cs.MM"
    ],
    "abstract": "Amid the proliferation of forged images, notably the tsunami of deepfake content, extensive research has been conducted on using artificial intelligence (AI) to identify forged content in the face of continuing advancements in counterfeiting technologies. We have investigated the use of AI to provide the original authentic image after deepfake detection, which we believe is a reliable and persuasive solution. We call this \"image-based automated fact verification,\" a name that originated from a t",
    "arxiv_url": "https://arxiv.org/abs/2407.18614v1",
    "pdf_url": "https://arxiv.org/pdf/2407.18614v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2407.18614",
    "arxiv_authors": [
      "Shuhan Cui",
      "Huy H. Nguyen",
      "Trung-Nghia Le",
      "Chun-Shien Lu",
      "Isao Echizen"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=LookupForensics%3A+A+Large-Scale+Multi-Task+Dataset+for+Multi-Phase+Image-Based+Fact+Verification+Shuhan+Cui+Huy+H.+Nguyen+Trung-Nghia+Le+Chun-Shien+Lu+Isao+Echizen",
    "gs_search_success": false,
    "gs_authors": [],
    "citation_count": null,
    "error_type": "fetch_failed",
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2310.01912",
    "title": "Improved Automatic Diabetic Retinopathy Severity Classification Using Deep Multimodal Fusion of UWF-CFP and OCTA Images",
    "year": 2023,
    "published": "2023-10-03T09:35:38Z",
    "categories": [
      "eess.IV",
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "Diabetic Retinopathy (DR), a prevalent and severe complication of diabetes, affects millions of individuals globally, underscoring the need for accurate and timely diagnosis. Recent advancements in imaging technologies, such as Ultra-WideField Color Fundus Photography (UWF-CFP) imaging and Optical Coherence Tomography Angiography (OCTA), provide opportunities for the early detection of DR but also pose significant challenges given the disparate nature of the data they produce. This study introdu",
    "arxiv_url": "https://arxiv.org/abs/2310.01912v1",
    "pdf_url": "https://arxiv.org/pdf/2310.01912v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2310.01912",
    "arxiv_authors": [
      "Mostafa El Habib Daho",
      "Yihao Li",
      "Rachid Zeghlache",
      "Yapo Cedric Atse",
      "Hugo Le Boité",
      "Sophie Bonnin",
      "Deborah Cosette",
      "Pierre Deman",
      "Laurent Borderie",
      "Capucine Lepicard",
      "Ramin Tadayoni",
      "Béatrice Cochener",
      "Pierre-Henri Conze",
      "Mathieu Lamard",
      "Gwenolé Quellec"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Improved+Automatic+Diabetic+Retinopathy+Severity+Classification+Using+Deep+Multimodal+Fusion+of+UWF-CFP+and+OCTA+Images+Mostafa+El+Habib+Daho+Yihao+Li+Rachid+Zeghlache+Yapo+Cedric+Atse+Hugo+Le+Boit%C3%A9",
    "gs_search_success": true,
    "gs_authors": [
      "iWgYuY0AAAAJ",
      "j8HE3iEAAAAJ"
    ],
    "citation_count": 22,
    "gs_author_count": 9
  },
  {
    "arxiv_id": "2410.03592",
    "title": "Variational Bayes Gaussian Splatting",
    "year": 2024,
    "published": "2024-10-04T16:52:03Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Recently, 3D Gaussian Splatting has emerged as a promising approach for modeling 3D scenes using mixtures of Gaussians. The predominant optimization method for these models relies on backpropagating gradients through a differentiable rendering pipeline, which struggles with catastrophic forgetting when dealing with continuous streams of data. To address this limitation, we propose Variational Bayes Gaussian Splatting (VBGS), a novel approach that frames training a Gaussian splat as variational i",
    "arxiv_url": "https://arxiv.org/abs/2410.03592v2",
    "pdf_url": "https://arxiv.org/pdf/2410.03592v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2410.03592",
    "arxiv_authors": [
      "Toon Van de Maele",
      "Ozan Catal",
      "Alexander Tschantz",
      "Christopher L. Buckley",
      "Tim Verbelen"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Variational+Bayes+Gaussian+Splatting+Toon+Van+de+Maele+Ozan+Catal+Alexander+Tschantz+Christopher+L.+Buckley+Tim+Verbelen",
    "gs_search_success": true,
    "gs_authors": [
      "5NbVgO0AAAAJ",
      "G86XVPEAAAAJ",
      "prA0Oa4AAAAJ",
      "Q1gb8ScAAAAJ",
      "nWuZ0XcAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2309.10783",
    "title": "Language as the Medium: Multimodal Video Classification through text only",
    "year": 2023,
    "published": "2023-09-19T17:32:21Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "abstract": "Despite an exciting new wave of multimodal machine learning models, current approaches still struggle to interpret the complex contextual relationships between the different modalities present in videos. Going beyond existing methods that emphasize simple activities or objects, we propose a new model-agnostic approach for generating detailed textual descriptions that captures multimodal video information. Our method leverages the extensive knowledge learnt by large language models, such as GPT-3",
    "arxiv_url": "https://arxiv.org/abs/2309.10783v1",
    "pdf_url": "https://arxiv.org/pdf/2309.10783v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2309.10783",
    "arxiv_authors": [
      "Laura Hanu",
      "Anita L. Verő",
      "James Thewlis"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Language+as+the+Medium%3A+Multimodal+Video+Classification+through+text+only+Laura+Hanu+Anita+L.+Ver%C5%91+James+Thewlis",
    "gs_search_success": true,
    "gs_authors": [
      "yIHiSswAAAAJ",
      "UQHlWF4AAAAJ",
      "07hm3DYAAAAJ"
    ],
    "citation_count": 5,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2407.17967",
    "title": "Lightweight Language-driven Grasp Detection using Conditional Consistency Model",
    "year": 2024,
    "published": "2024-07-25T11:39:20Z",
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "abstract": "Language-driven grasp detection is a fundamental yet challenging task in robotics with various industrial applications. In this work, we present a new approach for language-driven grasp detection that leverages the concept of lightweight diffusion models to achieve fast inference time. By integrating diffusion processes with grasping prompts in natural language, our method can effectively encode visual and textual information, enabling more accurate and versatile grasp positioning that aligns we",
    "arxiv_url": "https://arxiv.org/abs/2407.17967v1",
    "pdf_url": "https://arxiv.org/pdf/2407.17967v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2407.17967",
    "arxiv_authors": [
      "Nghia Nguyen",
      "Minh Nhat Vu",
      "Baoru Huang",
      "An Vuong",
      "Ngan Le",
      "Thieu Vo",
      "Anh Nguyen"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Lightweight+Language-driven+Grasp+Detection+using+Conditional+Consistency+Model+Nghia+Nguyen+Minh+Nhat+Vu+Baoru+Huang+An+Vuong+Ngan+Le",
    "gs_search_success": false,
    "gs_authors": [],
    "citation_count": null,
    "error_type": "fetch_failed",
    "gs_author_count": 7
  },
  {
    "arxiv_id": "2410.13882",
    "title": "Articulate-Anything: Automatic Modeling of Articulated Objects via a Vision-Language Foundation Model",
    "year": 2024,
    "published": "2024-10-03T19:42:16Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Interactive 3D simulated objects are crucial in AR/VR, animations, and robotics, driving immersive experiences and advanced automation. However, creating these articulated objects requires extensive human effort and expertise, limiting their broader applications. To overcome this challenge, we present Articulate-Anything, a system that automates the articulation of diverse, complex objects from many input modalities, including text, images, and videos. Articulate-Anything leverages vision-langua",
    "arxiv_url": "https://arxiv.org/abs/2410.13882v4",
    "pdf_url": "https://arxiv.org/pdf/2410.13882v4",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2410.13882",
    "arxiv_authors": [
      "Long Le",
      "Jason Xie",
      "William Liang",
      "Hung-Ju Wang",
      "Yue Yang",
      "Yecheng Jason Ma",
      "Kyle Vedder",
      "Arjun Krishna",
      "Dinesh Jayaraman",
      "Eric Eaton"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Articulate-Anything%3A+Automatic+Modeling+of+Articulated+Objects+via+a+Vision-Language+Foundation+Model+Long+Le+Jason+Xie+William+Liang+Hung-Ju+Wang+Yue+Yang",
    "gs_search_success": true,
    "gs_authors": [
      "wVYq9nYAAAAJ",
      "I6x_9IoAAAAJ",
      "K-2OXMIAAAAJ",
      "dNOBQ1sAAAAJ",
      "QIZWnnQAAAAJ",
      "Ml6RzmEAAAAJ",
      "Zp3RcC4AAAAJ",
      "XsYoY2QAAAAJ",
      "bQTXRrAAAAAJ",
      "QxLpghAAAAAJ"
    ],
    "citation_count": 38,
    "gs_author_count": 9
  },
  {
    "arxiv_id": "2308.14334",
    "title": "MetaWeather: Few-Shot Weather-Degraded Image Restoration",
    "year": 2023,
    "published": "2023-08-28T06:25:40Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Real-world weather conditions are intricate and often occur concurrently. However, most existing restoration approaches are limited in their applicability to specific weather conditions in training data and struggle to generalize to unseen weather types, including real-world weather conditions. To address this issue, we introduce MetaWeather, a universal approach that can handle diverse and novel weather conditions with a single unified model. Extending a powerful meta-learning framework, MetaWe",
    "arxiv_url": "https://arxiv.org/abs/2308.14334v4",
    "pdf_url": "https://arxiv.org/pdf/2308.14334v4",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2308.14334",
    "arxiv_authors": [
      "Youngrae Kim",
      "Younggeol Cho",
      "Thanh-Tung Nguyen",
      "Seunghoon Hong",
      "Dongman Lee"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=MetaWeather%3A+Few-Shot+Weather-Degraded+Image+Restoration+Youngrae+Kim+Younggeol+Cho+Thanh-Tung+Nguyen+Seunghoon+Hong+Dongman+Lee",
    "gs_search_success": true,
    "gs_authors": [
      "YExTU_cAAAAJ",
      "hvr3ALkAAAAJ",
      "cKzTWrIAAAAJ",
      "NkKC6zYAAAAJ"
    ],
    "citation_count": 3,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2402.02474",
    "title": "Deep Spectral Improvement for Unsupervised Image Instance Segmentation",
    "year": 2024,
    "published": "2024-02-04T13:09:13Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Deep spectral methods reframe the image decomposition process as a graph partitioning task by extracting features using self-supervised learning and utilizing the Laplacian of the affinity matrix to obtain eigensegments. However, instance segmentation has received less attention compared to other tasks within the context of deep spectral methods. This paper addresses the fact that not all channels of the feature map extracted from a self-supervised backbone contain sufficient information for ins",
    "arxiv_url": "https://arxiv.org/abs/2402.02474v3",
    "pdf_url": "https://arxiv.org/pdf/2402.02474v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2402.02474",
    "arxiv_authors": [
      "Farnoosh Arefi",
      "Amir M. Mansourian",
      "Shohreh Kasaei"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Deep+Spectral+Improvement+for+Unsupervised+Image+Instance+Segmentation+Farnoosh+Arefi+Amir+M.+Mansourian+Shohreh+Kasaei",
    "gs_search_success": true,
    "gs_authors": [
      "WAsj8gwAAAAJ",
      "_6s7U6IAAAAJ",
      "mvx4PvgAAAAJ"
    ],
    "citation_count": 3,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2312.13729",
    "title": "Gaussian Splatting with NeRF-based Color and Opacity",
    "year": 2023,
    "published": "2023-12-21T10:52:59Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Neural Radiance Fields (NeRFs) have demonstrated the remarkable potential of neural networks to capture the intricacies of 3D objects. By encoding the shape and color information within neural network weights, NeRFs excel at producing strikingly sharp novel views of 3D objects. Recently, numerous generalizations of NeRFs utilizing generative models have emerged, expanding its versatility. In contrast, Gaussian Splatting (GS) offers a similar render quality with faster training and inference as i",
    "arxiv_url": "https://arxiv.org/abs/2312.13729v5",
    "pdf_url": "https://arxiv.org/pdf/2312.13729v5",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2312.13729",
    "arxiv_authors": [
      "Dawid Malarz",
      "Weronika Smolak",
      "Jacek Tabor",
      "Sławomir Tadeja",
      "Przemysław Spurek"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Gaussian+Splatting+with+NeRF-based+Color+and+Opacity+Dawid+Malarz+Weronika+Smolak+Jacek+Tabor+S%C5%82awomir+Tadeja+Przemys%C5%82aw+Spurek",
    "gs_search_success": true,
    "gs_authors": [
      "3glCXHwAAAAJ",
      "zSKYziUAAAAJ",
      "p9hxe6wAAAAJ"
    ],
    "citation_count": 25,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2311.03938",
    "title": "Analysis of NaN Divergence in Training Monocular Depth Estimation Model",
    "year": 2023,
    "published": "2023-11-07T12:19:30Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "The latest advances in deep learning have facilitated the development of highly accurate monocular depth estimation models. However, when training a monocular depth estimation network, practitioners and researchers have observed not a number (NaN) loss, which disrupts gradient descent optimization. Although several practitioners have reported the stochastic and mysterious occurrence of NaN loss that bothers training, its root cause is not discussed in the literature. This study conducted an in-d",
    "arxiv_url": "https://arxiv.org/abs/2311.03938v1",
    "pdf_url": "https://arxiv.org/pdf/2311.03938v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2311.03938",
    "arxiv_authors": [
      "Bum Jun Kim",
      "Hyeonah Jang",
      "Sang Woo Kim"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Analysis+of+NaN+Divergence+in+Training+Monocular+Depth+Estimation+Model+Bum+Jun+Kim+Hyeonah+Jang+Sang+Woo+Kim",
    "gs_search_success": true,
    "gs_authors": [
      "TuoyWi8AAAAJ",
      "BH-AT54AAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2411.16788",
    "title": "TIDE: Training Locally Interpretable Domain Generalization Models Enables Test-time Correction",
    "year": 2024,
    "published": "2024-11-25T08:46:37Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "We consider the problem of single-source domain generalization. Existing methods typically rely on extensive augmentations to synthetically cover diverse domains during training. However, they struggle with semantic shifts (e.g., background and viewpoint changes), as they often learn global features instead of local concepts that tend to be domain invariant. To address this gap, we propose an approach that compels models to leverage such local concepts during prediction. Given no suitable datase",
    "arxiv_url": "https://arxiv.org/abs/2411.16788v2",
    "pdf_url": "https://arxiv.org/pdf/2411.16788v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2411.16788",
    "arxiv_authors": [
      "Aishwarya Agarwal",
      "Srikrishna Karanam",
      "Vineet Gandhi"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=TIDE%3A+Training+Locally+Interpretable+Domain+Generalization+Models+Enables+Test-time+Correction+Aishwarya+Agarwal+Srikrishna+Karanam+Vineet+Gandhi",
    "gs_search_success": true,
    "gs_authors": [
      "G2-skyUAAAAJ",
      "PVlBz8oAAAAJ",
      "PnIR8V4AAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2408.15032",
    "title": "Mamba2MIL: State Space Duality Based Multiple Instance Learning for Computational Pathology",
    "year": 2024,
    "published": "2024-08-27T13:01:19Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Computational pathology (CPath) has significantly advanced the clinical practice of pathology. Despite the progress made, Multiple Instance Learning (MIL), a promising paradigm within CPath, continues to face challenges, particularly related to incomplete information utilization. Existing frameworks, such as those based on Convolutional Neural Networks (CNNs), attention, and selective scan space state sequential model (SSM), lack sufficient flexibility and scalability in fusing diverse features,",
    "arxiv_url": "https://arxiv.org/abs/2408.15032v1",
    "pdf_url": "https://arxiv.org/pdf/2408.15032v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2408.15032",
    "arxiv_authors": [
      "Yuqi Zhang",
      "Xiaoqian Zhang",
      "Jiakai Wang",
      "Yuancheng Yang",
      "Taiying Peng",
      "Chao Tong"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Mamba2MIL%3A+State+Space+Duality+Based+Multiple+Instance+Learning+for+Computational+Pathology+Yuqi+Zhang+Xiaoqian+Zhang+Jiakai+Wang+Yuancheng+Yang+Taiying+Peng",
    "gs_search_success": true,
    "gs_authors": [
      "czcjW_QAAAAJ",
      "RoFr1qcAAAAJ",
      "F7PPve4AAAAJ",
      "maQlQdIAAAAJ"
    ],
    "citation_count": 6,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2505.21135",
    "title": "Learning Single Index Models with Diffusion Priors",
    "year": 2025,
    "published": "2025-05-27T12:50:04Z",
    "categories": [
      "cs.LG",
      "cs.CV",
      "stat.ML"
    ],
    "abstract": "Diffusion models (DMs) have demonstrated remarkable ability to generate diverse and high-quality images by efficiently modeling complex data distributions. They have also been explored as powerful generative priors for signal recovery, resulting in a substantial improvement in the quality of reconstructed signals. However, existing research on signal recovery with diffusion models either focuses on specific reconstruction problems or is unable to handle nonlinear measurement models with disconti",
    "arxiv_url": "https://arxiv.org/abs/2505.21135v1",
    "pdf_url": "https://arxiv.org/pdf/2505.21135v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2505.21135",
    "arxiv_authors": [
      "Anqi Tang",
      "Youming Chen",
      "Shuchen Xue",
      "Zhaoqiang Liu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Learning+Single+Index+Models+with+Diffusion+Priors+Anqi+Tang+Youming+Chen+Shuchen+Xue+Zhaoqiang+Liu",
    "gs_search_success": true,
    "gs_authors": [
      "EmGrPbIAAAAJ",
      "SrFAcEkAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2405.00900",
    "title": "LidaRF: Delving into Lidar for Neural Radiance Field on Street Scenes",
    "year": 2024,
    "published": "2024-05-01T23:07:12Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Photorealistic simulation plays a crucial role in applications such as autonomous driving, where advances in neural radiance fields (NeRFs) may allow better scalability through the automatic creation of digital 3D assets. However, reconstruction quality suffers on street scenes due to largely collinear camera motions and sparser samplings at higher speeds. On the other hand, the application often demands rendering from camera views that deviate from the inputs to accurately simulate behaviors li",
    "arxiv_url": "https://arxiv.org/abs/2405.00900v2",
    "pdf_url": "https://arxiv.org/pdf/2405.00900v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2405.00900",
    "arxiv_authors": [
      "Shanlin Sun",
      "Bingbing Zhuang",
      "Ziyu Jiang",
      "Buyu Liu",
      "Xiaohui Xie",
      "Manmohan Chandraker"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=LidaRF%3A+Delving+into+Lidar+for+Neural+Radiance+Field+on+Street+Scenes+Shanlin+Sun+Bingbing+Zhuang+Ziyu+Jiang+Buyu+Liu+Xiaohui+Xie",
    "gs_search_success": true,
    "gs_authors": [
      "t5KUxs4AAAAJ",
      "67HpPiEAAAAJ",
      "Ja-8AFsAAAAJ",
      "c6wKvwgAAAAJ",
      "9M6pGaYAAAAJ",
      "1CR0meYAAAAJ"
    ],
    "citation_count": 17,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2405.16082",
    "title": "Uncertainty Measurement of Deep Learning System based on the Convex Hull of Training Sets",
    "year": 2024,
    "published": "2024-05-25T06:25:24Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Deep Learning (DL) has made remarkable achievements in computer vision and adopted in safety critical domains such as medical imaging or autonomous drive. Thus, it is necessary to understand the uncertainty of the model to effectively reduce accidents and losses due to misjudgment of the Deep Neural Networks (DNN). This can start by efficiently selecting data that could potentially malfunction to the model. Traditionally, data collection and labeling have been done manually, but recently test da",
    "arxiv_url": "https://arxiv.org/abs/2405.16082v1",
    "pdf_url": "https://arxiv.org/pdf/2405.16082v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2405.16082",
    "arxiv_authors": [
      "Hyekyoung Hwang",
      "Jitae Shin"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Uncertainty+Measurement+of+Deep+Learning+System+based+on+the+Convex+Hull+of+Training+Sets+Hyekyoung+Hwang+Jitae+Shin",
    "gs_search_success": true,
    "gs_authors": [
      "lxtFyQ0AAAAJ"
    ],
    "citation_count": 3,
    "gs_author_count": 2
  },
  {
    "arxiv_id": "2412.13190",
    "title": "MotionBridge: Dynamic Video Inbetweening with Flexible Controls",
    "year": 2024,
    "published": "2024-12-17T18:59:33Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "By generating plausible and smooth transitions between two image frames, video inbetweening is an essential tool for video editing and long video synthesis. Traditional works lack the capability to generate complex large motions. While recent video generation techniques are powerful in creating high-quality results, they often lack fine control over the details of intermediate frames, which can lead to results that do not align with the creative mind. We introduce MotionBridge, a unified video i",
    "arxiv_url": "https://arxiv.org/abs/2412.13190v3",
    "pdf_url": "https://arxiv.org/pdf/2412.13190v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2412.13190",
    "arxiv_authors": [
      "Maham Tanveer",
      "Yang Zhou",
      "Simon Niklaus",
      "Ali Mahdavi Amiri",
      "Hao Zhang",
      "Krishna Kumar Singh",
      "Nanxuan Zhao"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=MotionBridge%3A+Dynamic+Video+Inbetweening+with+Flexible+Controls+Maham+Tanveer+Yang+Zhou+Simon+Niklaus+Ali+Mahdavi+Amiri+Hao+Zhang",
    "gs_search_success": true,
    "gs_authors": [
      "3TMipekAAAAJ",
      "M9eTADwAAAAJ",
      "UuwugFEAAAAJ",
      "iXFFJRUAAAAJ"
    ],
    "citation_count": 12,
    "gs_author_count": 7
  },
  {
    "arxiv_id": "2311.14633",
    "title": "One Strike, You're Out: Detecting Markush Structures in Low Signal-to-Noise Ratio Images",
    "year": 2023,
    "published": "2023-11-24T18:02:14Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Modern research increasingly relies on automated methods to assist researchers. An example of this is Optical Chemical Structure Recognition (OCSR), which aids chemists in retrieving information about chemicals from large amounts of documents. Markush structures are chemical structures that cannot be parsed correctly by OCSR and cause errors. The focus of this research was to propose and test a novel method for classifying Markush structures. Within this method, a comparison was made between fix",
    "arxiv_url": "https://arxiv.org/abs/2311.14633v1",
    "pdf_url": "https://arxiv.org/pdf/2311.14633v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2311.14633",
    "arxiv_authors": [
      "Thomas Jurriaans",
      "Kinga Szarkowska",
      "Eric Nalisnick",
      "Markus Schwoerer",
      "Camilo Thorne",
      "Saber Akhondi"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=One+Strike%2C+You%27re+Out%3A+Detecting+Markush+Structures+in+Low+Signal-to-Noise+Ratio+Images+Thomas+Jurriaans+Kinga+Szarkowska+Eric+Nalisnick+Markus+Schwoerer+Camilo+Thorne",
    "gs_search_success": true,
    "gs_authors": [
      "MrWuX5EAAAAJ",
      "cb1ZN7AAAAAJ",
      "kFiIBSEAAAAJ"
    ],
    "citation_count": 2,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2505.09073",
    "title": "2D-3D Attention and Entropy for Pose Robust 2D Facial Recognition",
    "year": 2025,
    "published": "2025-05-14T02:17:53Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Despite recent advances in facial recognition, there remains a fundamental issue concerning degradations in performance due to substantial perspective (pose) differences between enrollment and query (probe) imagery. Therefore, we propose a novel domain adaptive framework to facilitate improved performances across large discrepancies in pose by enabling image-based (2D) representations to infer properties of inherently pose invariant point cloud (3D) representations. Specifically, our proposed fr",
    "arxiv_url": "https://arxiv.org/abs/2505.09073v1",
    "pdf_url": "https://arxiv.org/pdf/2505.09073v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2505.09073",
    "arxiv_authors": [
      "J. Brennan Peace",
      "Shuowen Hu",
      "Benjamin S. Riggan"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=2D-3D+Attention+and+Entropy+for+Pose+Robust+2D+Facial+Recognition+J.+Brennan+Peace+Shuowen+Hu+Benjamin+S.+Riggan",
    "gs_search_success": true,
    "gs_authors": [
      "kpd1UoMAAAAJ",
      "U0KWOa8AAAAJ",
      "sEUpNUIAAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2505.22031",
    "title": "Guess the Age of Photos: An Interactive Web Platform for Historical Image Age Estimation",
    "year": 2025,
    "published": "2025-05-28T06:52:49Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "This paper introduces Guess the Age of Photos, a web platform engaging users in estimating the years of historical photographs through two gamified modes: Guess the Year (predicting a single image's year) and Timeline Challenge (comparing two images to identify the older). Built with Python, Flask, Bootstrap, and PostgreSQL, it uses a 10,150-image subset of the Date Estimation in the Wild dataset (1930-1999). Features like dynamic scoring and leaderboards boost engagement. Evaluated with 113 use",
    "arxiv_url": "https://arxiv.org/abs/2505.22031v1",
    "pdf_url": "https://arxiv.org/pdf/2505.22031v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2505.22031",
    "arxiv_authors": [
      "Hasan Yucedag",
      "Adam Jatowt"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Guess+the+Age+of+Photos%3A+An+Interactive+Web+Platform+for+Historical+Image+Age+Estimation+Hasan+Yucedag+Adam+Jatowt",
    "gs_search_success": true,
    "gs_authors": [
      "l2vn9GoAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 2
  },
  {
    "arxiv_id": "2505.12890",
    "title": "Specialized Foundation Models for Intelligent Operating Rooms",
    "year": 2025,
    "published": "2025-05-19T09:20:29Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Surgical procedures unfold in complex environments demanding coordination between surgical teams, tools, imaging and increasingly, intelligent robotic systems. Ensuring safety and efficiency in ORs of the future requires intelligent systems, like surgical robots, smart instruments and digital copilots, capable of understanding complex activities and hazards of surgeries. Yet, existing computational approaches, lack the breadth, and generalization needed for comprehensive OR understanding. We int",
    "arxiv_url": "https://arxiv.org/abs/2505.12890v2",
    "pdf_url": "https://arxiv.org/pdf/2505.12890v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2505.12890",
    "arxiv_authors": [
      "Ege Özsoy",
      "Chantal Pellegrini",
      "David Bani-Harouni",
      "Kun Yuan",
      "Matthias Keicher",
      "Nassir Navab"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Specialized+Foundation+Models+for+Intelligent+Operating+Rooms+Ege+%C3%96zsoy+Chantal+Pellegrini+David+Bani-Harouni+Kun+Yuan+Matthias+Keicher",
    "gs_search_success": true,
    "gs_authors": [
      "rJKqlzkAAAAJ",
      "dmkOvGcAAAAJ",
      "XrQVGwYAAAAJ",
      "D1EEcKAAAAAJ",
      "-tv59HYAAAAJ",
      "JhY7GewAAAAJ",
      "bVHYVXQAAAAJ",
      "JJPjKLIAAAAJ",
      "uLN9v6QAAAAJ",
      "k6JNN3cAAAAJ",
      "9nmW8qgAAAAJ",
      "3K-dyUkAAAAJ"
    ],
    "citation_count": 14,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2407.15447",
    "title": "SIGMA: Sinkhorn-Guided Masked Video Modeling",
    "year": 2024,
    "published": "2024-07-22T08:04:09Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Video-based pretraining offers immense potential for learning strong visual representations on an unprecedented scale. Recently, masked video modeling methods have shown promising scalability, yet fall short in capturing higher-level semantics due to reconstructing predefined low-level targets such as pixels. To tackle this, we present Sinkhorn-guided Masked Video Modelling (SIGMA), a novel video pretraining method that jointly learns the video model in addition to a target feature space using a",
    "arxiv_url": "https://arxiv.org/abs/2407.15447v1",
    "pdf_url": "https://arxiv.org/pdf/2407.15447v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2407.15447",
    "arxiv_authors": [
      "Mohammadreza Salehi",
      "Michael Dorkenwald",
      "Fida Mohammad Thoker",
      "Efstratios Gavves",
      "Cees G. M. Snoek",
      "Yuki M. Asano"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=SIGMA%3A+Sinkhorn-Guided+Masked+Video+Modeling+Mohammadreza+Salehi+Michael+Dorkenwald+Fida+Mohammad+Thoker+Efstratios+Gavves+Cees+G.+M.+Snoek",
    "gs_search_success": true,
    "gs_authors": [
      "KY5nvLUAAAAJ",
      "0uKdbscAAAAJ",
      "CdpLhlgAAAAJ",
      "vqN8M3MAAAAJ",
      "QqfCvsgAAAAJ",
      "kpT3gcsAAAAJ"
    ],
    "citation_count": 12,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2304.09728",
    "title": "Any-to-Any Style Transfer: Making Picasso and Da Vinci Collaborate",
    "year": 2023,
    "published": "2023-04-19T15:15:36Z",
    "categories": [
      "cs.CV",
      "eess.IV"
    ],
    "abstract": "Style transfer aims to render the style of a given image for style reference to another given image for content reference, and has been widely adopted in artistic generation and image editing. Existing approaches either apply the holistic style of the style image in a global manner, or migrate local colors and textures of the style image to the content counterparts in a pre-defined way. In either case, only one result can be generated for a specific pair of content and style images, which theref",
    "arxiv_url": "https://arxiv.org/abs/2304.09728v2",
    "pdf_url": "https://arxiv.org/pdf/2304.09728v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2304.09728",
    "arxiv_authors": [
      "Songhua Liu",
      "Jingwen Ye",
      "Xinchao Wang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Any-to-Any+Style+Transfer%3A+Making+Picasso+and+Da+Vinci+Collaborate+Songhua+Liu+Jingwen+Ye+Xinchao+Wang",
    "gs_search_success": true,
    "gs_authors": [
      "8GQnNP0AAAAJ",
      "w69Buq0AAAAJ",
      "AnYh2rAAAAAJ"
    ],
    "citation_count": 18,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2407.10052",
    "title": "Augmented Neural Fine-Tuning for Efficient Backdoor Purification",
    "year": 2024,
    "published": "2024-07-14T02:36:54Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Recent studies have revealed the vulnerability of deep neural networks (DNNs) to various backdoor attacks, where the behavior of DNNs can be compromised by utilizing certain types of triggers or poisoning mechanisms. State-of-the-art (SOTA) defenses employ too-sophisticated mechanisms that require either a computationally expensive adversarial search module for reverse-engineering the trigger distribution or an over-sensitive hyper-parameter selection module. Moreover, they offer sub-par perform",
    "arxiv_url": "https://arxiv.org/abs/2407.10052v2",
    "pdf_url": "https://arxiv.org/pdf/2407.10052v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2407.10052",
    "arxiv_authors": [
      "Nazmul Karim",
      "Abdullah Al Arafat",
      "Umar Khalid",
      "Zhishan Guo",
      "Nazanin Rahnavard"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Augmented+Neural+Fine-Tuning+for+Efficient+Backdoor+Purification+Nazmul+Karim+Abdullah+Al+Arafat+Umar+Khalid+Zhishan+Guo+Nazanin+Rahnavard",
    "gs_search_success": true,
    "gs_authors": [
      "xURjwjsAAAAJ",
      "hGJD_XUAAAAJ",
      "e3_l6fwAAAAJ",
      "TM7bqtYAAAAJ"
    ],
    "citation_count": 10,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2312.13309",
    "title": "Generate E-commerce Product Background by Integrating Category Commonality and Personalized Style",
    "year": 2023,
    "published": "2023-12-20T04:35:00Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "The state-of-the-art methods for e-commerce product background generation suffer from the inefficiency of designing product-wise prompts when scaling up the production, as well as the ineffectiveness of describing fine-grained styles when customizing personalized backgrounds for some specific brands. To address these obstacles, we integrate the category commonality and personalized style into diffusion models. Concretely, we propose a Category-Wise Generator to enable large-scale background gene",
    "arxiv_url": "https://arxiv.org/abs/2312.13309v2",
    "pdf_url": "https://arxiv.org/pdf/2312.13309v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2312.13309",
    "arxiv_authors": [
      "Haohan Wang",
      "Wei Feng",
      "Yaoyu Li",
      "Zheng Zhang",
      "Jingjing Lv",
      "Junjie Shen",
      "Zhangang Lin",
      "Jingping Shao"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Generate+E-commerce+Product+Background+by+Integrating+Category+Commonality+and+Personalized+Style+Haohan+Wang+Wei+Feng+Yaoyu+Li+Zheng+Zhang+Jingjing+Lv",
    "gs_search_success": true,
    "gs_authors": [
      "FvIAct4AAAAJ",
      "EYHZKMIAAAAJ"
    ],
    "citation_count": 15,
    "gs_author_count": 8
  },
  {
    "arxiv_id": "2401.08932",
    "title": "Learning to detect cloud and snow in remote sensing images from noisy labels",
    "year": 2024,
    "published": "2024-01-17T03:02:31Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Detecting clouds and snow in remote sensing images is an essential preprocessing task for remote sensing imagery. Previous works draw inspiration from semantic segmentation models in computer vision, with most research focusing on improving model architectures to enhance detection performance. However, unlike natural images, the complexity of scenes and the diversity of cloud types in remote sensing images result in many inaccurate labels in cloud and snow detection datasets, introducing unneces",
    "arxiv_url": "https://arxiv.org/abs/2401.08932v1",
    "pdf_url": "https://arxiv.org/pdf/2401.08932v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2401.08932",
    "arxiv_authors": [
      "Zili Liu",
      "Hao Chen",
      "Wenyuan Li",
      "Keyan Chen",
      "Zipeng Qi",
      "Chenyang Liu",
      "Zhengxia Zou",
      "Zhenwei Shi"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Learning+to+detect+cloud+and+snow+in+remote+sensing+images+from+noisy+labels+Zili+Liu+Hao+Chen+Wenyuan+Li+Keyan+Chen+Zipeng+Qi",
    "gs_search_success": true,
    "gs_authors": [
      "BEDNoZIAAAAJ",
      "5RF4ia8AAAAJ",
      "jBnA45cAAAAJ",
      "DzwoyZsAAAAJ",
      "cHwscjMAAAAJ",
      "KMEs58kAAAAJ",
      "6d0Pic4AAAAJ",
      "kNhFWQIAAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 8
  },
  {
    "arxiv_id": "2505.20935",
    "title": "ISAC: Training-Free Instance-to-Semantic Attention Control for Improving Multi-Instance Generation",
    "year": 2025,
    "published": "2025-05-27T09:23:10Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Text-to-image diffusion models have recently become highly capable, yet their behavior in multi-object scenes remains unreliable: models often produce an incorrect number of instances and exhibit semantics leaking across objects. We trace these failures to vague instance boundaries; self-attention already reveals instance layouts early in the denoising process, but existing approaches act only on semantic signals. We introduce $\\textbf{ISAC}$ ($\\textbf{I}$nstance-to-$\\textbf{S}$emantic $\\textbf{",
    "arxiv_url": "https://arxiv.org/abs/2505.20935v2",
    "pdf_url": "https://arxiv.org/pdf/2505.20935v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2505.20935",
    "arxiv_authors": [
      "Sanghyun Jo",
      "Wooyeol Lee",
      "Ziseok Lee",
      "Kyungsu Kim"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=ISAC%3A+Training-Free+Instance-to-Semantic+Attention+Control+for+Improving+Multi-Instance+Generation+Sanghyun+Jo+Wooyeol+Lee+Ziseok+Lee+Kyungsu+Kim",
    "gs_search_success": true,
    "gs_authors": [
      "xgP6q2YAAAAJ",
      "RbJDbtgAAAAJ",
      "y2rIfCkAAAAJ",
      "C3x_tewAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2401.03993",
    "title": "Behavioural Cloning in VizDoom",
    "year": 2024,
    "published": "2024-01-08T16:15:43Z",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "abstract": "This paper describes methods for training autonomous agents to play the game \"Doom 2\" through Imitation Learning (IL) using only pixel data as input. We also explore how Reinforcement Learning (RL) compares to IL for humanness by comparing camera movement and trajectory data. Through behavioural cloning, we examine the ability of individual models to learn varying behavioural traits. We attempt to mimic the behaviour of real players with different play styles, and find we can train agents that b",
    "arxiv_url": "https://arxiv.org/abs/2401.03993v1",
    "pdf_url": "https://arxiv.org/pdf/2401.03993v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2401.03993",
    "arxiv_authors": [
      "Ryan Spick",
      "Timothy Bradley",
      "Ayush Raina",
      "Pierluigi Vito Amadori",
      "Guy Moss"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Behavioural+Cloning+in+VizDoom+Ryan+Spick+Timothy+Bradley+Ayush+Raina+Pierluigi+Vito+Amadori+Guy+Moss",
    "gs_search_success": true,
    "gs_authors": [
      "MLTYHPMAAAAJ",
      "RCLvXx4AAAAJ",
      "apwjxDYAAAAJ",
      "0e35WgkAAAAJ"
    ],
    "citation_count": 2,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2504.14785",
    "title": "DC4CR: When Cloud Removal Meets Diffusion Control in Remote Sensing",
    "year": 2025,
    "published": "2025-04-21T00:56:57Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Cloud occlusion significantly hinders remote sensing applications by obstructing surface information and complicating analysis. To address this, we propose DC4CR (Diffusion Control for Cloud Removal), a novel multimodal diffusion-based framework for cloud removal in remote sensing imagery. Our method introduces prompt-driven control, allowing selective removal of thin and thick clouds without relying on pre-generated cloud masks, thereby enhancing preprocessing efficiency and model adaptability.",
    "arxiv_url": "https://arxiv.org/abs/2504.14785v2",
    "pdf_url": "https://arxiv.org/pdf/2504.14785v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2504.14785",
    "arxiv_authors": [
      "Zhenyu Yu",
      "Mohd Yamani Idna Idris",
      "Pei Wang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=DC4CR%3A+When+Cloud+Removal+Meets+Diffusion+Control+in+Remote+Sensing+Zhenyu+Yu+Mohd+Yamani+Idna+Idris+Pei+Wang",
    "gs_search_success": true,
    "gs_authors": [
      "JzyjGGUAAAAJ"
    ],
    "citation_count": 9,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2403.12046",
    "title": "GPT-4V(ision) Unsuitable for Clinical Care and Education: A Clinician-Evaluated Assessment",
    "year": 2023,
    "published": "2023-11-14T17:06:09Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "OpenAI's large multimodal model, GPT-4V(ision), was recently developed for general image interpretation. However, less is known about its capabilities with medical image interpretation and diagnosis. Board-certified physicians and senior residents assessed GPT-4V's proficiency across a range of medical conditions using imaging modalities such as CT scans, MRIs, ECGs, and clinical photographs. Although GPT-4V is able to identify and explain medical images, its diagnostic accuracy and clinical dec",
    "arxiv_url": "https://arxiv.org/abs/2403.12046v1",
    "pdf_url": "https://arxiv.org/pdf/2403.12046v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2403.12046",
    "arxiv_authors": [
      "Senthujan Senkaiahliyan",
      "Augustin Toma",
      "Jun Ma",
      "An-Wen Chan",
      "Andrew Ha",
      "Kevin R. An",
      "Hrishikesh Suresh",
      "Barry Rubin",
      "Bo Wang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=GPT-4V%28ision%29+Unsuitable+for+Clinical+Care+and+Education%3A+A+Clinician-Evaluated+Assessment+Senthujan+Senkaiahliyan+Augustin+Toma+Jun+Ma+An-Wen+Chan+Andrew+Ha",
    "gs_search_success": true,
    "gs_authors": [
      "ATJ8L98AAAAJ",
      "37FDILIAAAAJ",
      "e6axJOIAAAAJ",
      "tCEKn3YAAAAJ",
      "bW1UV4IAAAAJ"
    ],
    "citation_count": 10,
    "gs_author_count": 9
  },
  {
    "arxiv_id": "2505.04369",
    "title": "WDMamba: When Wavelet Degradation Prior Meets Vision Mamba for Image Dehazing",
    "year": 2025,
    "published": "2025-05-07T12:37:01Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "In this paper, we reveal a novel haze-specific wavelet degradation prior observed through wavelet transform analysis, which shows that haze-related information predominantly resides in low-frequency components. Exploiting this insight, we propose a novel dehazing framework, WDMamba, which decomposes the image dehazing task into two sequential stages: low-frequency restoration followed by detail enhancement. This coarse-to-fine strategy enables WDMamba to effectively capture features specific to ",
    "arxiv_url": "https://arxiv.org/abs/2505.04369v1",
    "pdf_url": "https://arxiv.org/pdf/2505.04369v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2505.04369",
    "arxiv_authors": [
      "Jie Sun",
      "Heng Liu",
      "Yongzhen Wang",
      "Xiao-Ping Zhang",
      "Mingqiang Wei"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=WDMamba%3A+When+Wavelet+Degradation+Prior+Meets+Vision+Mamba+for+Image+Dehazing+Jie+Sun+Heng+Liu+Yongzhen+Wang+Xiao-Ping+Zhang+Mingqiang+Wei",
    "gs_search_success": true,
    "gs_authors": [
      "TdrJj8MAAAAJ",
      "fLeZQYkAAAAJ",
      "1fzb_z8AAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2305.08779",
    "title": "TAA-GCN: A Temporally Aware Adaptive Graph Convolutional Network for Age Estimation",
    "year": 2023,
    "published": "2023-05-15T16:38:55Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "This paper proposes a novel age estimation algorithm, the Temporally-Aware Adaptive Graph Convolutional Network (TAA-GCN). Using a new representation based on graphs, the TAA-GCN utilizes skeletal, posture, clothing, and facial information to enrich the feature set associated with various ages. Such a novel graph representation has several advantages: First, reduced sensitivity to facial expression and other appearance variances; Second, robustness to partial occlusion and non-frontal-planar vie",
    "arxiv_url": "https://arxiv.org/abs/2305.08779v1",
    "pdf_url": "https://arxiv.org/pdf/2305.08779v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2305.08779",
    "arxiv_authors": [
      "Matthew Korban",
      "Peter Young",
      "Scott T. Acton"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=TAA-GCN%3A+A+Temporally+Aware+Adaptive+Graph+Convolutional+Network+for+Age+Estimation+Matthew+Korban+Peter+Young+Scott+T.+Acton",
    "gs_search_success": true,
    "gs_authors": [
      "MKfRy54AAAAJ",
      "FoD2KsEAAAAJ",
      "1HVnbCYAAAAJ"
    ],
    "citation_count": 25,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2502.10574",
    "title": "Classifier-free Guidance with Adaptive Scaling",
    "year": 2025,
    "published": "2025-02-14T22:04:53Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Classifier-free guidance (CFG) is an essential mechanism in contemporary text-driven diffusion models. In practice, in controlling the impact of guidance we can see the trade-off between the quality of the generated images and correspondence to the prompt. When we use strong guidance, generated images fit the conditioned text perfectly but at the cost of their quality. Dually, we can use small guidance to generate high-quality results, but the generated images do not suit our prompt. In this pap",
    "arxiv_url": "https://arxiv.org/abs/2502.10574v1",
    "pdf_url": "https://arxiv.org/pdf/2502.10574v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2502.10574",
    "arxiv_authors": [
      "Dawid Malarz",
      "Artur Kasymov",
      "Maciej Zięba",
      "Jacek Tabor",
      "Przemysław Spurek"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Classifier-free+Guidance+with+Adaptive+Scaling+Dawid+Malarz+Artur+Kasymov+Maciej+Zi%C4%99ba+Jacek+Tabor+Przemys%C5%82aw+Spurek",
    "gs_search_success": true,
    "gs_authors": [
      "0kp0MbgAAAAJ",
      "XmOBJZYAAAAJ",
      "zSKYziUAAAAJ",
      "p9hxe6wAAAAJ",
      "WRPDt0gAAAAJ"
    ],
    "citation_count": 8,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2402.02503",
    "title": "GeReA: Question-Aware Prompt Captions for Knowledge-based Visual Question Answering",
    "year": 2024,
    "published": "2024-02-04T14:28:23Z",
    "categories": [
      "cs.CV",
      "cs.CL"
    ],
    "abstract": "Knowledge-based visual question answering (VQA) requires world knowledge beyond the image for accurate answer. Recently, instead of extra knowledge bases, a large language model (LLM) like GPT-3 is activated as an implicit knowledge engine to jointly acquire and reason the necessary knowledge for answering by converting images into textual information (e.g., captions and answer candidates). However, such conversion may introduce irrelevant information, which causes the LLM to misinterpret images",
    "arxiv_url": "https://arxiv.org/abs/2402.02503v1",
    "pdf_url": "https://arxiv.org/pdf/2402.02503v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2402.02503",
    "arxiv_authors": [
      "Ziyu Ma",
      "Shutao Li",
      "Bin Sun",
      "Jianfei Cai",
      "Zuxiang Long",
      "Fuyan Ma"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=GeReA%3A+Question-Aware+Prompt+Captions+for+Knowledge-based+Visual+Question+Answering+Ziyu+Ma+Shutao+Li+Bin+Sun+Jianfei+Cai+Zuxiang+Long",
    "gs_search_success": true,
    "gs_authors": [
      "lmVsnBsAAAAJ",
      "N6czCoUAAAAJ",
      "KT4fQP0AAAAJ"
    ],
    "citation_count": 4,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2303.01669",
    "title": "Learning Common Rationale to Improve Self-Supervised Representation for Fine-Grained Visual Recognition Problems",
    "year": 2023,
    "published": "2023-03-03T02:07:40Z",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "Self-supervised learning (SSL) strategies have demonstrated remarkable performance in various recognition tasks. However, both our preliminary investigation and recent studies suggest that they may be less effective in learning representations for fine-grained visual recognition (FGVR) since many features helpful for optimizing SSL objectives are not suitable for characterizing the subtle differences in FGVR. To overcome this issue, we propose learning an additional screening mechanism to identi",
    "arxiv_url": "https://arxiv.org/abs/2303.01669v2",
    "pdf_url": "https://arxiv.org/pdf/2303.01669v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2303.01669",
    "arxiv_authors": [
      "Yangyang Shu",
      "Anton van den Hengel",
      "Lingqiao Liu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Learning+Common+Rationale+to+Improve+Self-Supervised+Representation+for+Fine-Grained+Visual+Recognition+Problems+Yangyang+Shu+Anton+van+den+Hengel+Lingqiao+Liu",
    "gs_search_success": true,
    "gs_authors": [
      "nMGZ2ZQAAAAJ",
      "Y2xu62UAAAAJ",
      "TpdRFZIAAAAJ"
    ],
    "citation_count": 31,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2407.04603",
    "title": "AWT: Transferring Vision-Language Models via Augmentation, Weighting, and Transportation",
    "year": 2024,
    "published": "2024-07-05T15:52:23Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Pre-trained vision-language models (VLMs) have shown impressive results in various visual classification tasks. However, we often fail to fully unleash their potential when adapting them for new concept understanding due to limited information on new classes. To address this limitation, we introduce a novel adaptation framework, AWT (Augment, Weight, then Transport). AWT comprises three key components: augmenting inputs with diverse visual perspectives and enriched class descriptions through ima",
    "arxiv_url": "https://arxiv.org/abs/2407.04603v2",
    "pdf_url": "https://arxiv.org/pdf/2407.04603v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2407.04603",
    "arxiv_authors": [
      "Yuhan Zhu",
      "Yuyang Ji",
      "Zhiyu Zhao",
      "Gangshan Wu",
      "Limin Wang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=AWT%3A+Transferring+Vision-Language+Models+via+Augmentation%2C+Weighting%2C+and+Transportation+Yuhan+Zhu+Yuyang+Ji+Zhiyu+Zhao+Gangshan+Wu+Limin+Wang",
    "gs_search_success": true,
    "gs_authors": [
      "ydgR3LgAAAAJ",
      "HEuN8PcAAAAJ",
      "2Ef8Y0IAAAAJ"
    ],
    "citation_count": 20,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2306.15410",
    "title": "AutoGraph: Predicting Lane Graphs from Traffic Observations",
    "year": 2023,
    "published": "2023-06-27T12:11:22Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Lane graph estimation is a long-standing problem in the context of autonomous driving. Previous works aimed at solving this problem by relying on large-scale, hand-annotated lane graphs, introducing a data bottleneck for training models to solve this task. To overcome this limitation, we propose to use the motion patterns of traffic participants as lane graph annotations. In our AutoGraph approach, we employ a pre-trained object tracker to collect the tracklets of traffic participants such as ve",
    "arxiv_url": "https://arxiv.org/abs/2306.15410v3",
    "pdf_url": "https://arxiv.org/pdf/2306.15410v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2306.15410",
    "arxiv_authors": [
      "Jannik Zürn",
      "Ingmar Posner",
      "Wolfram Burgard"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=AutoGraph%3A+Predicting+Lane+Graphs+from+Traffic+Observations+Jannik+Z%C3%BCrn+Ingmar+Posner+Wolfram+Burgard",
    "gs_search_success": true,
    "gs_authors": [
      "zj6FavAAAAAJ",
      "gB9JqUcAAAAJ",
      "dPk-iwsAAAAJ"
    ],
    "citation_count": 6,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2302.13392",
    "title": "NSANet: Noise Seeking Attention Network",
    "year": 2023,
    "published": "2023-02-26T19:22:36Z",
    "categories": [
      "cs.CV",
      "eess.IV"
    ],
    "abstract": "LiDAR (Light Detection and Ranging) technology has remained popular in capturing natural and built environments for numerous applications. The recent technological advancements in electro-optical engineering have aided in obtaining laser returns at a higher pulse repetition frequency (PRF), which considerably increased the density of the 3D point cloud. Conventional techniques with lower PRF had a single pulse-in-air (SPIA) zone, large enough to avoid a mismatch among pulse pairs at the receiver",
    "arxiv_url": "https://arxiv.org/abs/2302.13392v1",
    "pdf_url": "https://arxiv.org/pdf/2302.13392v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2302.13392",
    "arxiv_authors": [
      "Maryam Jameela",
      "Gunho Sohn"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=NSANet%3A+Noise+Seeking+Attention+Network+Maryam+Jameela+Gunho+Sohn",
    "gs_search_success": true,
    "gs_authors": [
      "lTM2SOAAAAAJ",
      "5TNrxxcAAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 2
  },
  {
    "arxiv_id": "2408.00331",
    "title": "DECIDER: Leveraging Foundation Model Priors for Improved Model Failure Detection and Explanation",
    "year": 2024,
    "published": "2024-08-01T07:08:11Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Reliably detecting when a deployed machine learning model is likely to fail on a given input is crucial for ensuring safe operation. In this work, we propose DECIDER (Debiasing Classifiers to Identify Errors Reliably), a novel approach that leverages priors from large language models (LLMs) and vision-language models (VLMs) to detect failures in image classification models. DECIDER utilizes LLMs to specify task-relevant core attributes and constructs a ``debiased'' version of the classifier by a",
    "arxiv_url": "https://arxiv.org/abs/2408.00331v1",
    "pdf_url": "https://arxiv.org/pdf/2408.00331v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2408.00331",
    "arxiv_authors": [
      "Rakshith Subramanyam",
      "Kowshik Thopalli",
      "Vivek Narayanaswamy",
      "Jayaraman J. Thiagarajan"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=DECIDER%3A+Leveraging+Foundation+Model+Priors+for+Improved+Model+Failure+Detection+and+Explanation+Rakshith+Subramanyam+Kowshik+Thopalli+Vivek+Narayanaswamy+Jayaraman+J.+Thiagarajan",
    "gs_search_success": true,
    "gs_authors": [
      "X0cQnxcAAAAJ",
      "cMz65_oAAAAJ",
      "7h2Ui6YAAAAJ",
      "bZhPl80AAAAJ"
    ],
    "citation_count": 5,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2303.13040",
    "title": "Open-Vocabulary Object Detection using Pseudo Caption Labels",
    "year": 2023,
    "published": "2023-03-23T05:10:22Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Recent open-vocabulary detection methods aim to detect novel objects by distilling knowledge from vision-language models (VLMs) trained on a vast amount of image-text pairs. To improve the effectiveness of these methods, researchers have utilized datasets with a large vocabulary that contains a large number of object classes, under the assumption that such data will enable models to extract comprehensive knowledge on the relationships between various objects and better generalize to unseen objec",
    "arxiv_url": "https://arxiv.org/abs/2303.13040v1",
    "pdf_url": "https://arxiv.org/pdf/2303.13040v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2303.13040",
    "arxiv_authors": [
      "Han-Cheol Cho",
      "Won Young Jhoo",
      "Wooyoung Kang",
      "Byungseok Roh"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Open-Vocabulary+Object+Detection+using+Pseudo+Caption+Labels+Han-Cheol+Cho+Won+Young+Jhoo+Wooyoung+Kang+Byungseok+Roh",
    "gs_search_success": true,
    "gs_authors": [
      "H4VWYHwAAAAJ",
      "j9-V7UgAAAAJ",
      "JWZfomgAAAAJ",
      "88FsBR4AAAAJ"
    ],
    "citation_count": 20,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2410.16163",
    "title": "Griffon-G: Bridging Vision-Language and Vision-Centric Tasks via Large Multimodal Models",
    "year": 2024,
    "published": "2024-10-21T16:30:29Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Large Multimodal Models (LMMs) have achieved significant breakthroughs in various vision-language and vision-centric tasks based on auto-regressive modeling. However, these models typically focus on either vision-centric tasks, such as visual grounding and region description, or vision-language tasks, like image caption and multi-scenario VQAs. None of the LMMs have yet comprehensively unified both types of tasks within a single model, as seen in Large Language Models in the natural language pro",
    "arxiv_url": "https://arxiv.org/abs/2410.16163v1",
    "pdf_url": "https://arxiv.org/pdf/2410.16163v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2410.16163",
    "arxiv_authors": [
      "Yufei Zhan",
      "Hongyin Zhao",
      "Yousong Zhu",
      "Fan Yang",
      "Ming Tang",
      "Jinqiao Wang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Griffon-G%3A+Bridging+Vision-Language+and+Vision-Centric+Tasks+via+Large+Multimodal+Models+Yufei+Zhan+Hongyin+Zhao+Yousong+Zhu+Fan+Yang+Ming+Tang",
    "gs_search_success": true,
    "gs_authors": [
      "fShYLyUAAAAJ",
      "oSjIjeUAAAAJ",
      "RvHqTGEAAAAJ",
      "7_BkyxEAAAAJ"
    ],
    "citation_count": 5,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2503.01292",
    "title": "PA-CLIP: Enhancing Zero-Shot Anomaly Detection through Pseudo-Anomaly Awareness",
    "year": 2025,
    "published": "2025-03-03T08:29:27Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "In industrial anomaly detection (IAD), accurately identifying defects amidst diverse anomalies and under varying imaging conditions remains a significant challenge. Traditional approaches often struggle with high false-positive rates, frequently misclassifying normal shadows and surface deformations as defects, an issue that becomes particularly pronounced in products with complex and intricate surface features. To address these challenges, we introduce PA-CLIP, a zero-shot anomaly detection met",
    "arxiv_url": "https://arxiv.org/abs/2503.01292v1",
    "pdf_url": "https://arxiv.org/pdf/2503.01292v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2503.01292",
    "arxiv_authors": [
      "Yurui Pan",
      "Lidong Wang",
      "Yuchao Chen",
      "Wenbing Zhu",
      "Bo Peng",
      "Mingmin Chi"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=PA-CLIP%3A+Enhancing+Zero-Shot+Anomaly+Detection+through+Pseudo-Anomaly+Awareness+Yurui+Pan+Lidong+Wang+Yuchao+Chen+Wenbing+Zhu+Bo+Peng",
    "gs_search_success": true,
    "gs_authors": [
      "Y8b1W00AAAAJ",
      "58aMZ6oAAAAJ",
      "et1qE1EAAAAJ",
      "Ka7EI2sAAAAJ"
    ],
    "citation_count": 3,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2412.07769",
    "title": "BiMediX2: Bio-Medical EXpert LMM for Diverse Medical Modalities",
    "year": 2024,
    "published": "2024-12-10T18:59:35Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "We introduce BiMediX2, a bilingual (Arabic-English) Bio-Medical EXpert Large Multimodal Model that supports text-based and image-based medical interactions. It enables multi-turn conversation in Arabic and English and supports diverse medical imaging modalities, including radiology, CT, and histology. To train BiMediX2, we curate BiMed-V, an extensive Arabic-English bilingual healthcare dataset consisting of 1.6M samples of diverse medical interactions. This dataset supports a range of medical L",
    "arxiv_url": "https://arxiv.org/abs/2412.07769v2",
    "pdf_url": "https://arxiv.org/pdf/2412.07769v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2412.07769",
    "arxiv_authors": [
      "Sahal Shaji Mullappilly",
      "Mohammed Irfan Kurpath",
      "Sara Pieri",
      "Saeed Yahya Alseiari",
      "Shanavas Cholakkal",
      "Khaled Aldahmani",
      "Fahad Khan",
      "Rao Anwer",
      "Salman Khan",
      "Timothy Baldwin",
      "Hisham Cholakkal"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=BiMediX2%3A+Bio-Medical+EXpert+LMM+for+Diverse+Medical+Modalities+Sahal+Shaji+Mullappilly+Mohammed+Irfan+Kurpath+Sara+Pieri+Saeed+Yahya+Alseiari+Shanavas+Cholakkal",
    "gs_search_success": true,
    "gs_authors": [
      "LJWxVpUAAAAJ",
      "jLNKLsgAAAAJ"
    ],
    "citation_count": 7,
    "gs_author_count": 9
  },
  {
    "arxiv_id": "2412.13010",
    "title": "Measurement of Medial Elbow Joint Space using Landmark Detection",
    "year": 2024,
    "published": "2024-12-17T15:32:12Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Ultrasound imaging of the medial elbow is crucial for the early diagnosis of Ulnar Collateral Ligament (UCL) injuries. Specifically, measuring the elbow joint space in ultrasound images is used to assess the valgus instability of the elbow caused by UCL injuries. To automate this measurement, a model trained on a precisely annotated dataset is necessary; however, no publicly available dataset exists to date. This study introduces a novel ultrasound medial elbow dataset to measure the joint space",
    "arxiv_url": "https://arxiv.org/abs/2412.13010v3",
    "pdf_url": "https://arxiv.org/pdf/2412.13010v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2412.13010",
    "arxiv_authors": [
      "Shizuka Akahori",
      "Shotaro Teruya",
      "Pragyan Shrestha",
      "Yuichi Yoshii",
      "Ryuhei Michinobu",
      "Satoshi Iizuka",
      "Itaru Kitahara"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Measurement+of+Medial+Elbow+Joint+Space+using+Landmark+Detection+Shizuka+Akahori+Shotaro+Teruya+Pragyan+Shrestha+Yuichi+Yoshii+Ryuhei+Michinobu",
    "gs_search_success": true,
    "gs_authors": [
      "OuNRZb8AAAAJ",
      "tg_tPbwAAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 7
  },
  {
    "arxiv_id": "2306.04202",
    "title": "Video Compression with Arbitrary Rescaling Network",
    "year": 2023,
    "published": "2023-06-07T07:15:18Z",
    "categories": [
      "cs.MM",
      "cs.CV",
      "eess.IV"
    ],
    "abstract": "Most video platforms provide video streaming services with different qualities, and the quality of the services is usually adjusted by the resolution of the videos. So high-resolution videos need to be downsampled for compression. In order to solve the problem of video coding at different resolutions, we propose a rate-guided arbitrary rescaling network (RARN) for video resizing before encoding. To help the RARN be compatible with standard codecs and generate compression-friendly results, an ite",
    "arxiv_url": "https://arxiv.org/abs/2306.04202v1",
    "pdf_url": "https://arxiv.org/pdf/2306.04202v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2306.04202",
    "arxiv_authors": [
      "Mengxi Guo",
      "Shijie Zhao",
      "Hao Jiang",
      "Junlin Li",
      "Li Zhang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Video+Compression+with+Arbitrary+Rescaling+Network+Mengxi+Guo+Shijie+Zhao+Hao+Jiang+Junlin+Li+Li+Zhang",
    "gs_search_success": true,
    "gs_authors": [
      "daHqpg8AAAAJ",
      "6aQe5dUAAAAJ",
      "8G5-2OMAAAAJ"
    ],
    "citation_count": 6,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2406.07937",
    "title": "IFTD: Image Feature Triangle Descriptor for Loop Detection in Driving Scenes",
    "year": 2024,
    "published": "2024-06-12T06:59:58Z",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "abstract": "In this work, we propose a fast and robust Image Feature Triangle Descriptor (IFTD) based on the STD method, aimed at improving the efficiency and accuracy of place recognition in driving scenarios. We extract keypoints from BEV projection image of point cloud and construct these keypoints into triangle descriptors. By matching these feature triangles, we achieved precise place recognition and calculated the 4-DOF pose estimation between two keyframes. Furthermore, we employ image similarity ins",
    "arxiv_url": "https://arxiv.org/abs/2406.07937v1",
    "pdf_url": "https://arxiv.org/pdf/2406.07937v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2406.07937",
    "arxiv_authors": [
      "Fengtian Lang",
      "Ruiye Ming",
      "Zikang Yuan",
      "Xin Yang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=IFTD%3A+Image+Feature+Triangle+Descriptor+for+Loop+Detection+in+Driving+Scenes+Fengtian+Lang+Ruiye+Ming+Zikang+Yuan+Xin+Yang",
    "gs_search_success": true,
    "gs_authors": [
      "FrQLlCYAAAAJ",
      "zwgGSkEAAAAJ",
      "acxdM9gAAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2409.06309",
    "title": "PPMamba: A Pyramid Pooling Local Auxiliary SSM-Based Model for Remote Sensing Image Semantic Segmentation",
    "year": 2024,
    "published": "2024-09-10T08:08:50Z",
    "categories": [
      "cs.CV",
      "eess.IV"
    ],
    "abstract": "Semantic segmentation is a vital task in the field of remote sensing (RS). However, conventional convolutional neural network (CNN) and transformer-based models face limitations in capturing long-range dependencies or are often computationally intensive. Recently, an advanced state space model (SSM), namely Mamba, was introduced, offering linear computational complexity while effectively establishing long-distance dependencies. Despite their advantages, Mamba-based methods encounter challenges i",
    "arxiv_url": "https://arxiv.org/abs/2409.06309v1",
    "pdf_url": "https://arxiv.org/pdf/2409.06309v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2409.06309",
    "arxiv_authors": [
      "Yin Hu",
      "Xianping Ma",
      "Jialu Sui",
      "Man-On Pun"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=PPMamba%3A+A+Pyramid+Pooling+Local+Auxiliary+SSM-Based+Model+for+Remote+Sensing+Image+Semantic+Segmentation+Yin+Hu+Xianping+Ma+Jialu+Sui+Man-On+Pun",
    "gs_search_success": true,
    "gs_authors": [
      "YpIBfOkAAAAJ",
      "S--vtB8AAAAJ",
      "eVvwRuIAAAAJ",
      "h82gvswAAAAJ"
    ],
    "citation_count": 18,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2401.03131",
    "title": "A Physics-guided Generative AI Toolkit for Geophysical Monitoring",
    "year": 2024,
    "published": "2024-01-06T06:09:05Z",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "eess.SP",
      "physics.geo-ph"
    ],
    "abstract": "Full-waveform inversion (FWI) plays a vital role in geoscience to explore the subsurface. It utilizes the seismic wave to image the subsurface velocity map. As the machine learning (ML) technique evolves, the data-driven approaches using ML for FWI tasks have emerged, offering enhanced accuracy and reduced computational cost compared to traditional physics-based methods. However, a common challenge in geoscience, the unprivileged data, severely limits ML effectiveness. The issue becomes even wor",
    "arxiv_url": "https://arxiv.org/abs/2401.03131v1",
    "pdf_url": "https://arxiv.org/pdf/2401.03131v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2401.03131",
    "arxiv_authors": [
      "Junhuan Yang",
      "Hanchen Wang",
      "Yi Sheng",
      "Youzuo Lin",
      "Lei Yang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=A+Physics-guided+Generative+AI+Toolkit+for+Geophysical+Monitoring+Junhuan+Yang+Hanchen+Wang+Yi+Sheng+Youzuo+Lin+Lei+Yang",
    "gs_search_success": true,
    "gs_authors": [
      "SN69Jk0AAAAJ",
      "UTTE_wEAAAAJ",
      "CMXuHYgAAAAJ",
      "otFECasAAAAJ"
    ],
    "citation_count": 6,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2401.02126",
    "title": "Unified Diffusion-Based Rigid and Non-Rigid Editing with Text and Image Guidance",
    "year": 2024,
    "published": "2024-01-04T08:21:30Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Existing text-to-image editing methods tend to excel either in rigid or non-rigid editing but encounter challenges when combining both, resulting in misaligned outputs with the provided text prompts. In addition, integrating reference images for control remains challenging. To address these issues, we present a versatile image editing framework capable of executing both rigid and non-rigid edits, guided by either textual prompts or reference images. We leverage a dual-path injection scheme to ha",
    "arxiv_url": "https://arxiv.org/abs/2401.02126v1",
    "pdf_url": "https://arxiv.org/pdf/2401.02126v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2401.02126",
    "arxiv_authors": [
      "Jiacheng Wang",
      "Ping Liu",
      "Wei Xu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Unified+Diffusion-Based+Rigid+and+Non-Rigid+Editing+with+Text+and+Image+Guidance+Jiacheng+Wang+Ping+Liu+Wei+Xu",
    "gs_search_success": true,
    "gs_authors": [
      "KRz4JecAAAAJ"
    ],
    "citation_count": 6,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2503.17978",
    "title": "PIM: Physics-Informed Multi-task Pre-training for Improving Inertial Sensor-Based Human Activity Recognition",
    "year": 2025,
    "published": "2025-03-23T08:16:01Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Human activity recognition (HAR) with deep learning models relies on large amounts of labeled data, often challenging to obtain due to associated cost, time, and labor. Self-supervised learning (SSL) has emerged as an effective approach to leverage unlabeled data through pretext tasks, such as masked reconstruction and multitask learning with signal processing-based data augmentations, to pre-train encoder models. However, such methods are often derived from computer vision approaches that disre",
    "arxiv_url": "https://arxiv.org/abs/2503.17978v1",
    "pdf_url": "https://arxiv.org/pdf/2503.17978v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2503.17978",
    "arxiv_authors": [
      "Dominique Nshimyimana",
      "Vitor Fortes Rey",
      "Sungho Suh",
      "Bo Zhou",
      "Paul Lukowicz"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=PIM%3A+Physics-Informed+Multi-task+Pre-training+for+Improving+Inertial+Sensor-Based+Human+Activity+Recognition+Dominique+Nshimyimana+Vitor+Fortes+Rey+Sungho+Suh+Bo+Zhou+Paul+Lukowicz",
    "gs_search_success": true,
    "gs_authors": [
      "rjJwgO0AAAAJ",
      "1FIyc9kAAAAJ",
      "DMjDOYwAAAAJ",
      "WQT1MfIAAAAJ",
      "PCvtW3gAAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2408.02226",
    "title": "ProCreate, Don't Reproduce! Propulsive Energy Diffusion for Creative Generation",
    "year": 2024,
    "published": "2024-08-05T04:10:52Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "In this paper, we propose ProCreate, a simple and easy-to-implement method to improve sample diversity and creativity of diffusion-based image generative models and to prevent training data reproduction. ProCreate operates on a set of reference images and actively propels the generated image embedding away from the reference embeddings during the generation process. We propose FSCG-8 (Few-Shot Creative Generation 8), a few-shot creative generation dataset on eight different categories -- encompa",
    "arxiv_url": "https://arxiv.org/abs/2408.02226v2",
    "pdf_url": "https://arxiv.org/pdf/2408.02226v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2408.02226",
    "arxiv_authors": [
      "Jack Lu",
      "Ryan Teehan",
      "Mengye Ren"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=ProCreate%2C+Don%27t+Reproduce%21+Propulsive+Energy+Diffusion+for+Creative+Generation+Jack+Lu+Ryan+Teehan+Mengye+Ren",
    "gs_search_success": true,
    "gs_authors": [
      "mXo4IYQAAAAJ",
      "iN5SFdsAAAAJ",
      "XcQ9WqMAAAAJ"
    ],
    "citation_count": 11,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2403.14121",
    "title": "External Knowledge Enhanced 3D Scene Generation from Sketch",
    "year": 2024,
    "published": "2024-03-21T04:24:49Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Generating realistic 3D scenes is challenging due to the complexity of room layouts and object geometries.We propose a sketch based knowledge enhanced diffusion architecture (SEK) for generating customized, diverse, and plausible 3D scenes. SEK conditions the denoising process with a hand-drawn sketch of the target scene and cues from an object relationship knowledge base. We first construct an external knowledge base containing object relationships and then leverage knowledge enhanced graph rea",
    "arxiv_url": "https://arxiv.org/abs/2403.14121v2",
    "pdf_url": "https://arxiv.org/pdf/2403.14121v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2403.14121",
    "arxiv_authors": [
      "Zijie Wu",
      "Mingtao Feng",
      "Yaonan Wang",
      "He Xie",
      "Weisheng Dong",
      "Bo Miao",
      "Ajmal Mian"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=External+Knowledge+Enhanced+3D+Scene+Generation+from+Sketch+Zijie+Wu+Mingtao+Feng+Yaonan+Wang+He+Xie+Weisheng+Dong",
    "gs_search_success": true,
    "gs_authors": [
      "-g58LsoAAAAJ",
      "X589yaIAAAAJ",
      "AFsLiBcAAAAJ",
      "Sv3hHu8AAAAJ"
    ],
    "citation_count": 9,
    "gs_author_count": 7
  },
  {
    "arxiv_id": "2306.15111",
    "title": "Self-Supervised Image Captioning with CLIP",
    "year": 2023,
    "published": "2023-06-26T23:29:16Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Image captioning, a fundamental task in vision-language understanding, seeks to generate accurate natural language descriptions for provided images. Current image captioning approaches heavily rely on high-quality image-caption pairs, which can be hard to obtain for many domains. To address this, we introduce a self-supervised image captioning method. After learning an initial signal from a small labeled dataset, our method transitions to self-supervised learning on unlabeled data, leveraging th",
    "arxiv_url": "https://arxiv.org/abs/2306.15111v2",
    "pdf_url": "https://arxiv.org/pdf/2306.15111v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2306.15111",
    "arxiv_authors": [
      "Chuanyang Jin"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Self-Supervised+Image+Captioning+with+CLIP+Chuanyang+Jin",
    "gs_search_success": true,
    "gs_authors": [],
    "citation_count": 1,
    "gs_author_count": 1
  },
  {
    "arxiv_id": "2312.05984",
    "title": "Accurate Differential Operators for Hybrid Neural Fields",
    "year": 2023,
    "published": "2023-12-10T20:14:58Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR",
      "cs.LG"
    ],
    "abstract": "Neural fields have become widely used in various fields, from shape representation to neural rendering, and for solving partial differential equations (PDEs). With the advent of hybrid neural field representations like Instant NGP that leverage small MLPs and explicit representations, these models train quickly and can fit large scenes. Yet in many applications like rendering and simulation, hybrid neural fields can cause noticeable and unreasonable artifacts. This is because they do not yield a",
    "arxiv_url": "https://arxiv.org/abs/2312.05984v2",
    "pdf_url": "https://arxiv.org/pdf/2312.05984v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2312.05984",
    "arxiv_authors": [
      "Aditya Chetan",
      "Guandao Yang",
      "Zichen Wang",
      "Steve Marschner",
      "Bharath Hariharan"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Accurate+Differential+Operators+for+Hybrid+Neural+Fields+Aditya+Chetan+Guandao+Yang+Zichen+Wang+Steve+Marschner+Bharath+Hariharan",
    "gs_search_success": true,
    "gs_authors": [
      "_kElCmMAAAAJ",
      "llo3F3QAAAAJ",
      "6W_ACEEAAAAJ",
      "TpglobcAAAAJ",
      "-wsZhDAAAAAJ"
    ],
    "citation_count": 6,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2406.04844",
    "title": "Multi-Granularity Language-Guided Training for Multi-Object Tracking",
    "year": 2024,
    "published": "2024-06-07T11:18:40Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Most existing multi-object tracking methods typically learn visual tracking features via maximizing dis-similarities of different instances and minimizing similarities of the same instance. While such a feature learning scheme achieves promising performance, learning discriminative features solely based on visual information is challenging especially in case of environmental interference such as occlusion, blur and domain variance. In this work, we argue that multi-modal language-driven features",
    "arxiv_url": "https://arxiv.org/abs/2406.04844v2",
    "pdf_url": "https://arxiv.org/pdf/2406.04844v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2406.04844",
    "arxiv_authors": [
      "Yuhao Li",
      "Jiale Cao",
      "Muzammal Naseer",
      "Yu Zhu",
      "Jinqiu Sun",
      "Yanning Zhang",
      "Fahad Shahbaz Khan"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Multi-Granularity+Language-Guided+Training+for+Multi-Object+Tracking+Yuhao+Li+Jiale+Cao+Muzammal+Naseer+Yu+Zhu+Jinqiu+Sun",
    "gs_search_success": true,
    "gs_authors": [
      "zvaeYnUAAAAJ",
      "tM9xKA8AAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 7
  },
  {
    "arxiv_id": "2302.05621",
    "title": "Dive into the Resolution Augmentations and Metrics in Low Resolution Face Recognition: A Plain yet Effective New Baseline",
    "year": 2023,
    "published": "2023-02-11T07:31:47Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Although deep learning has significantly improved Face Recognition (FR), dramatic performance deterioration may occur when processing Low Resolution (LR) faces. To alleviate this, approaches based on unified feature space are proposed with the sacrifice under High Resolution (HR) circumstances. To deal with the huge domain gap between HR and LR domains and achieve the best on both domains, we first took a closer look at the impacts of several resolution augmentations and then analyzed the diffic",
    "arxiv_url": "https://arxiv.org/abs/2302.05621v1",
    "pdf_url": "https://arxiv.org/pdf/2302.05621v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2302.05621",
    "arxiv_authors": [
      "Xu Ling",
      "Yichen Lu",
      "Wenqi Xu",
      "Weihong Deng",
      "Yingjie Zhang",
      "Xingchen Cui",
      "Hongzhi Shi",
      "Dongchao Wen"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Dive+into+the+Resolution+Augmentations+and+Metrics+in+Low+Resolution+Face+Recognition%3A+A+Plain+yet+Effective+New+Baseline+Xu+Ling+Yichen+Lu+Wenqi+Xu+Weihong+Deng+Yingjie+Zhang",
    "gs_search_success": true,
    "gs_authors": [
      "CIEFqfYAAAAJ",
      "Ld2hgWQAAAAJ",
      "1rhBlUEAAAAJ"
    ],
    "citation_count": 6,
    "gs_author_count": 8
  },
  {
    "arxiv_id": "2304.04579",
    "title": "Coherent Concept-based Explanations in Medical Image and Its Application to Skin Lesion Diagnosis",
    "year": 2023,
    "published": "2023-04-10T13:32:04Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Early detection of melanoma is crucial for preventing severe complications and increasing the chances of successful treatment. Existing deep learning approaches for melanoma skin lesion diagnosis are deemed black-box models, as they omit the rationale behind the model prediction, compromising the trustworthiness and acceptability of these diagnostic methods. Attempts to provide concept-based explanations are based on post-hoc approaches, which depend on an additional model to derive interpretati",
    "arxiv_url": "https://arxiv.org/abs/2304.04579v2",
    "pdf_url": "https://arxiv.org/pdf/2304.04579v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2304.04579",
    "arxiv_authors": [
      "Cristiano Patrício",
      "João C. Neves",
      "Luís F. Teixeira"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Coherent+Concept-based+Explanations+in+Medical+Image+and+Its+Application+to+Skin+Lesion+Diagnosis+Cristiano+Patr%C3%ADcio+Jo%C3%A3o+C.+Neves+Lu%C3%ADs+F.+Teixeira",
    "gs_search_success": false,
    "gs_authors": [],
    "citation_count": null,
    "error_type": "fetch_failed",
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2312.17251",
    "title": "Semantic segmentation of SEM images of lower bainitic and tempered martensitic steels",
    "year": 2023,
    "published": "2023-12-02T05:11:34Z",
    "categories": [
      "cs.CV",
      "cond-mat.mtrl-sci",
      "cs.LG"
    ],
    "abstract": "This study employs deep learning techniques to segment scanning electron microscope images, enabling a quantitative analysis of carbide precipitates in lower bainite and tempered martensite steels with comparable strength. Following segmentation, carbides are investigated, and their volume percentage, size distribution, and orientations are probed within the image dataset. Our findings reveal that lower bainite and tempered martensite exhibit comparable volume percentages of carbides, albeit wit",
    "arxiv_url": "https://arxiv.org/abs/2312.17251v2",
    "pdf_url": "https://arxiv.org/pdf/2312.17251v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2312.17251",
    "arxiv_authors": [
      "Xiaohan Bie",
      "Manoj Arthanari",
      "Evelin Barbosa de Melo",
      "Baihua Ren",
      "Juancheng Li",
      "Stephen Yue",
      "Salim Brahimi",
      "Jun Song"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Semantic+segmentation+of+SEM+images+of+lower+bainitic+and+tempered+martensitic+steels+Xiaohan+Bie+Manoj+Arthanari+Evelin+Barbosa+de+Melo+Baihua+Ren+Juancheng+Li",
    "gs_search_success": true,
    "gs_authors": [
      "fIXnqbkAAAAJ",
      "5aLhwjQAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 8
  },
  {
    "arxiv_id": "2407.20868",
    "title": "A Comparative Study of Neural Surface Reconstruction for Scientific Visualization",
    "year": 2024,
    "published": "2024-07-30T14:43:54Z",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "abstract": "This comparative study evaluates various neural surface reconstruction methods, particularly focusing on their implications for scientific visualization through reconstructing 3D surfaces via multi-view rendering images. We categorize ten methods into neural radiance fields and neural implicit surfaces, uncovering the benefits of leveraging distance functions (i.e., SDFs and UDFs) to enhance the accuracy and smoothness of the reconstructed surfaces. Our findings highlight the efficiency and qual",
    "arxiv_url": "https://arxiv.org/abs/2407.20868v1",
    "pdf_url": "https://arxiv.org/pdf/2407.20868v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2407.20868",
    "arxiv_authors": [
      "Siyuan Yao",
      "Weixi Song",
      "Chaoli Wang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=A+Comparative+Study+of+Neural+Surface+Reconstruction+for+Scientific+Visualization+Siyuan+Yao+Weixi+Song+Chaoli+Wang",
    "gs_search_success": true,
    "gs_authors": [
      "z13bIdkAAAAJ",
      "YsCdxL4AAAAJ",
      "fvP8SGcAAAAJ"
    ],
    "citation_count": 2,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2505.23675",
    "title": "ImmunoDiff: A Diffusion Model for Immunotherapy Response Prediction in Lung Cancer",
    "year": 2025,
    "published": "2025-05-29T17:19:40Z",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "abstract": "Accurately predicting immunotherapy response in Non-Small Cell Lung Cancer (NSCLC) remains a critical unmet need. Existing radiomics and deep learning-based predictive models rely primarily on pre-treatment imaging to predict categorical response outcomes, limiting their ability to capture the complex morphological and textural transformations induced by immunotherapy. This study introduces ImmunoDiff, an anatomy-aware diffusion model designed to synthesize post-treatment CT scans from baseline ",
    "arxiv_url": "https://arxiv.org/abs/2505.23675v1",
    "pdf_url": "https://arxiv.org/pdf/2505.23675v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2505.23675",
    "arxiv_authors": [
      "Moinak Bhattacharya",
      "Judy Huang",
      "Amna F. Sher",
      "Gagandeep Singh",
      "Chao Chen",
      "Prateek Prasanna"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=ImmunoDiff%3A+A+Diffusion+Model+for+Immunotherapy+Response+Prediction+in+Lung+Cancer+Moinak+Bhattacharya+Judy+Huang+Amna+F.+Sher+Gagandeep+Singh+Chao+Chen",
    "gs_search_success": true,
    "gs_authors": [
      "uyA1Q18AAAAJ",
      "MmyDV2EAAAAJ",
      "ejucgiUAAAAJ"
    ],
    "citation_count": 2,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2307.06350",
    "title": "T2I-CompBench++: An Enhanced and Comprehensive Benchmark for Compositional Text-to-image Generation",
    "year": 2023,
    "published": "2023-07-12T17:59:42Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Despite the impressive advances in text-to-image models, they often struggle to effectively compose complex scenes with multiple objects, displaying various attributes and relationships. To address this challenge, we present T2I-CompBench++, an enhanced benchmark for compositional text-to-image generation. T2I-CompBench++ comprises 8,000 compositional text prompts categorized into four primary groups: attribute binding, object relationships, generative numeracy, and complex compositions. These a",
    "arxiv_url": "https://arxiv.org/abs/2307.06350v3",
    "pdf_url": "https://arxiv.org/pdf/2307.06350v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2307.06350",
    "arxiv_authors": [
      "Kaiyi Huang",
      "Chengqi Duan",
      "Kaiyue Sun",
      "Enze Xie",
      "Zhenguo Li",
      "Xihui Liu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=T2I-CompBench%2B%2B%3A+An+Enhanced+and+Comprehensive+Benchmark+for+Compositional+Text-to-image+Generation+Kaiyi+Huang+Chengqi+Duan+Kaiyue+Sun+Enze+Xie+Zhenguo+Li",
    "gs_search_success": true,
    "gs_authors": [
      "4YL23GMAAAAJ",
      "XboZC1AAAAAJ",
      "dB86D_cAAAAJ",
      "r9qb4ZwAAAAJ",
      "mieuBzUAAAAJ",
      "42MVVPgAAAAJ"
    ],
    "citation_count": 68,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2401.00663",
    "title": "1st Place Solution for 5th LSVOS Challenge: Referring Video Object Segmentation",
    "year": 2024,
    "published": "2024-01-01T04:24:48Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "The recent transformer-based models have dominated the Referring Video Object Segmentation (RVOS) task due to the superior performance. Most prior works adopt unified DETR framework to generate segmentation masks in query-to-instance manner. In this work, we integrate strengths of that leading RVOS models to build up an effective paradigm. We first obtain binary mask sequences from the RVOS models. To improve the consistency and quality of masks, we propose Two-Stage Multi-Model Fusion strategy.",
    "arxiv_url": "https://arxiv.org/abs/2401.00663v1",
    "pdf_url": "https://arxiv.org/pdf/2401.00663v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2401.00663",
    "arxiv_authors": [
      "Zhuoyan Luo",
      "Yicheng Xiao",
      "Yong Liu",
      "Yitong Wang",
      "Yansong Tang",
      "Xiu Li",
      "Yujiu Yang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=1st+Place+Solution+for+5th+LSVOS+Challenge%3A+Referring+Video+Object+Segmentation+Zhuoyan+Luo+Yicheng+Xiao+Yong+Liu+Yitong+Wang+Yansong+Tang",
    "gs_search_success": true,
    "gs_authors": [
      "4gH3sxsAAAAJ",
      "oakZP0cAAAAJ",
      "i9keb3IAAAAJ",
      "mKQhEsIAAAAJ",
      "NfFTKfYAAAAJ",
      "TIbistUAAAAJ"
    ],
    "citation_count": 3,
    "gs_author_count": 7
  },
  {
    "arxiv_id": "2406.04332",
    "title": "Coarse-To-Fine Tensor Trains for Compact Visual Representations",
    "year": 2024,
    "published": "2024-06-06T17:59:23Z",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "The ability to learn compact, high-quality, and easy-to-optimize representations for visual data is paramount to many applications such as novel view synthesis and 3D reconstruction. Recent work has shown substantial success in using tensor networks to design such compact and high-quality representations. However, the ability to optimize tensor-based representations, and in particular, the highly compact tensor train representation, is still lacking. This has prevented practitioners from deployi",
    "arxiv_url": "https://arxiv.org/abs/2406.04332v1",
    "pdf_url": "https://arxiv.org/pdf/2406.04332v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2406.04332",
    "arxiv_authors": [
      "Sebastian Loeschcke",
      "Dan Wang",
      "Christian Leth-Espensen",
      "Serge Belongie",
      "Michael J. Kastoryano",
      "Sagie Benaim"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Coarse-To-Fine+Tensor+Trains+for+Compact+Visual+Representations+Sebastian+Loeschcke+Dan+Wang+Christian+Leth-Espensen+Serge+Belongie+Michael+J.+Kastoryano",
    "gs_search_success": true,
    "gs_authors": [
      "-zSM2I8AAAAJ",
      "_aM-ud8AAAAJ",
      "tHbMyNoAAAAJ",
      "chD5XxkAAAAJ",
      "2roUPxkAAAAJ"
    ],
    "citation_count": 2,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2412.20538",
    "title": "Exploiting Aggregation and Segregation of Representations for Domain Adaptive Human Pose Estimation",
    "year": 2024,
    "published": "2024-12-29T17:59:45Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Human pose estimation (HPE) has received increasing attention recently due to its wide application in motion analysis, virtual reality, healthcare, etc. However, it suffers from the lack of labeled diverse real-world datasets due to the time- and labor-intensive annotation. To cope with the label deficiency issue, one common solution is to train the HPE models with easily available synthetic datasets (source) and apply them to real-world data (target) through domain adaptation (DA). Unfortunatel",
    "arxiv_url": "https://arxiv.org/abs/2412.20538v1",
    "pdf_url": "https://arxiv.org/pdf/2412.20538v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2412.20538",
    "arxiv_authors": [
      "Qucheng Peng",
      "Ce Zheng",
      "Zhengming Ding",
      "Pu Wang",
      "Chen Chen"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Exploiting+Aggregation+and+Segregation+of+Representations+for+Domain+Adaptive+Human+Pose+Estimation+Qucheng+Peng+Ce+Zheng+Zhengming+Ding+Pu+Wang+Chen+Chen",
    "gs_search_success": true,
    "gs_authors": [
      "IFqidw4AAAAJ",
      "YFKLC58AAAAJ",
      "TuEwcZ0AAAAJ",
      "TKbyRRsAAAAJ",
      "0buJlAUAAAAJ"
    ],
    "citation_count": 23,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2308.08157",
    "title": "Learning to Generate Semantic Layouts for Higher Text-Image Correspondence in Text-to-Image Synthesis",
    "year": 2023,
    "published": "2023-08-16T05:59:33Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Existing text-to-image generation approaches have set high standards for photorealism and text-image correspondence, largely benefiting from web-scale text-image datasets, which can include up to 5~billion pairs. However, text-to-image generation models trained on domain-specific datasets, such as urban scenes, medical images, and faces, still suffer from low text-image correspondence due to the lack of text-image pairs. Additionally, collecting billions of text-image pairs for a specific domain",
    "arxiv_url": "https://arxiv.org/abs/2308.08157v1",
    "pdf_url": "https://arxiv.org/pdf/2308.08157v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2308.08157",
    "arxiv_authors": [
      "Minho Park",
      "Jooyeol Yun",
      "Seunghwan Choi",
      "Jaegul Choo"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Learning+to+Generate+Semantic+Layouts+for+Higher+Text-Image+Correspondence+in+Text-to-Image+Synthesis+Minho+Park+Jooyeol+Yun+Seunghwan+Choi+Jaegul+Choo",
    "gs_search_success": true,
    "gs_authors": [
      "GHJYsLEAAAAJ",
      "gPVvl0sAAAAJ",
      "uFjBq8wAAAAJ",
      "gsCV7hMAAAAJ"
    ],
    "citation_count": 18,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2312.13150",
    "title": "Splatter Image: Ultra-Fast Single-View 3D Reconstruction",
    "year": 2023,
    "published": "2023-12-20T16:14:58Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "We introduce the \\method, an ultra-efficient approach for monocular 3D object reconstruction. Splatter Image is based on Gaussian Splatting, which allows fast and high-quality reconstruction of 3D scenes from multiple images. We apply Gaussian Splatting to monocular reconstruction by learning a neural network that, at test time, performs reconstruction in a feed-forward manner, at 38 FPS. Our main innovation is the surprisingly straightforward design of this network, which, using 2D operators, m",
    "arxiv_url": "https://arxiv.org/abs/2312.13150v2",
    "pdf_url": "https://arxiv.org/pdf/2312.13150v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2312.13150",
    "arxiv_authors": [
      "Stanislaw Szymanowicz",
      "Christian Rupprecht",
      "Andrea Vedaldi"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Splatter+Image%3A+Ultra-Fast+Single-View+3D+Reconstruction+Stanislaw+Szymanowicz+Christian+Rupprecht+Andrea+Vedaldi",
    "gs_search_success": true,
    "gs_authors": [
      "IrYlproAAAAJ",
      "kPJKnRIAAAAJ",
      "bRT7t28AAAAJ"
    ],
    "citation_count": 324,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2310.06196",
    "title": "DiPS: Discriminative Pseudo-Label Sampling with Self-Supervised Transformers for Weakly Supervised Object Localization",
    "year": 2023,
    "published": "2023-10-09T22:52:43Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Self-supervised vision transformers (SSTs) have shown great potential to yield rich localization maps that highlight different objects in an image. However, these maps remain class-agnostic since the model is unsupervised. They often tend to decompose the image into multiple maps containing different objects while being unable to distinguish the object of interest from background noise objects. In this paper, Discriminative Pseudo-label Sampling (DiPS) is introduced to leverage these class-agnos",
    "arxiv_url": "https://arxiv.org/abs/2310.06196v2",
    "pdf_url": "https://arxiv.org/pdf/2310.06196v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2310.06196",
    "arxiv_authors": [
      "Shakeeb Murtaza",
      "Soufiane Belharbi",
      "Marco Pedersoli",
      "Aydin Sarraf",
      "Eric Granger"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=DiPS%3A+Discriminative+Pseudo-Label+Sampling+with+Self-Supervised+Transformers+for+Weakly+Supervised+Object+Localization+Shakeeb+Murtaza+Soufiane+Belharbi+Marco+Pedersoli+Aydin+Sarraf+Eric+Granger",
    "gs_search_success": true,
    "gs_authors": [
      "SI0YVpAAAAAJ",
      "BYMo6wkAAAAJ",
      "TmfbdagAAAAJ",
      "aVfyPAoAAAAJ",
      "br7lo4MAAAAJ"
    ],
    "citation_count": 8,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2408.17267",
    "title": "UrBench: A Comprehensive Benchmark for Evaluating Large Multimodal Models in Multi-View Urban Scenarios",
    "year": 2024,
    "published": "2024-08-30T13:13:35Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Recent evaluations of Large Multimodal Models (LMMs) have explored their capabilities in various domains, with only few benchmarks specifically focusing on urban environments. Moreover, existing urban benchmarks have been limited to evaluating LMMs with basic region-level urban tasks under singular views, leading to incomplete evaluations of LMMs' abilities in urban environments. To address these issues, we present UrBench, a comprehensive benchmark designed for evaluating LMMs in complex multi-",
    "arxiv_url": "https://arxiv.org/abs/2408.17267v3",
    "pdf_url": "https://arxiv.org/pdf/2408.17267v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2408.17267",
    "arxiv_authors": [
      "Baichuan Zhou",
      "Haote Yang",
      "Dairong Chen",
      "Junyan Ye",
      "Tianyi Bai",
      "Jinhua Yu",
      "Songyang Zhang",
      "Dahua Lin",
      "Conghui He",
      "Weijia Li"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=UrBench%3A+A+Comprehensive+Benchmark+for+Evaluating+Large+Multimodal+Models+in+Multi-View+Urban+Scenarios+Baichuan+Zhou+Haote+Yang+Dairong+Chen+Junyan+Ye+Tianyi+Bai",
    "gs_search_success": true,
    "gs_authors": [
      "s3cJW70AAAAJ",
      "ByagRzIAAAAJ",
      "R6Rnh9IAAAAJ",
      "sv_N2PwAAAAJ",
      "9p8R9GYAAAAJ",
      "8XQPi7YAAAAJ",
      "radsfXwAAAAJ",
      "PopTv7kAAAAJ",
      "GMzzRRUAAAAJ",
      "6IbGkd4AAAAJ"
    ],
    "citation_count": 28,
    "gs_author_count": 9
  },
  {
    "arxiv_id": "2411.19231",
    "title": "Z-STAR+: A Zero-shot Style Transfer Method via Adjusting Style Distribution",
    "year": 2024,
    "published": "2024-11-28T15:56:17Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Style transfer presents a significant challenge, primarily centered on identifying an appropriate style representation. Conventional methods employ style loss, derived from second-order statistics or contrastive learning, to constrain style representation in the stylized result. However, these pre-defined style representations often limit stylistic expression, leading to artifacts. In contrast to existing approaches, we have discovered that latent features in vanilla diffusion models inherently ",
    "arxiv_url": "https://arxiv.org/abs/2411.19231v1",
    "pdf_url": "https://arxiv.org/pdf/2411.19231v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2411.19231",
    "arxiv_authors": [
      "Yingying Deng",
      "Xiangyu He",
      "Fan Tang",
      "Weiming Dong"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Z-STAR%2B%3A+A+Zero-shot+Style+Transfer+Method+via+Adjusting+Style+Distribution+Yingying+Deng+Xiangyu+He+Fan+Tang+Weiming+Dong",
    "gs_search_success": true,
    "gs_authors": [
      "WKGx4k8AAAAJ",
      "PdKElfwAAAAJ",
      "cDja050AAAAJ",
      "8N8lbJQAAAAJ"
    ],
    "citation_count": 2,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2303.10610",
    "title": "DiffMIC: Dual-Guidance Diffusion Network for Medical Image Classification",
    "year": 2023,
    "published": "2023-03-19T09:15:45Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Diffusion Probabilistic Models have recently shown remarkable performance in generative image modeling, attracting significant attention in the computer vision community. However, while a substantial amount of diffusion-based research has focused on generative tasks, few studies have applied diffusion models to general medical image classification. In this paper, we propose the first diffusion-based model (named DiffMIC) to address general medical image classification by eliminating unexpected n",
    "arxiv_url": "https://arxiv.org/abs/2303.10610v3",
    "pdf_url": "https://arxiv.org/pdf/2303.10610v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2303.10610",
    "arxiv_authors": [
      "Yijun Yang",
      "Huazhu Fu",
      "Angelica I. Aviles-Rivero",
      "Carola-Bibiane Schönlieb",
      "Lei Zhu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=DiffMIC%3A+Dual-Guidance+Diffusion+Network+for+Medical+Image+Classification+Yijun+Yang+Huazhu+Fu+Angelica+I.+Aviles-Rivero+Carola-Bibiane+Sch%C3%B6nlieb+Lei+Zhu",
    "gs_search_success": true,
    "gs_authors": [
      "MiKp_voAAAAJ",
      "q5AA4lEAAAAJ",
      "jCvUBYMAAAAJ",
      "nPeOXjwAAAAJ",
      "AQtqhaYAAAAJ"
    ],
    "citation_count": 128,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2301.12247",
    "title": "SEGA: Instructing Text-to-Image Models using Semantic Guidance",
    "year": 2023,
    "published": "2023-01-28T16:43:07Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "abstract": "Text-to-image diffusion models have recently received a lot of interest for their astonishing ability to produce high-fidelity images from text only. However, achieving one-shot generation that aligns with the user's intent is nearly impossible, yet small changes to the input prompt often result in very different images. This leaves the user with little semantic control. To put the user in control, we show how to interact with the diffusion process to flexibly steer it along semantic directions.",
    "arxiv_url": "https://arxiv.org/abs/2301.12247v2",
    "pdf_url": "https://arxiv.org/pdf/2301.12247v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2301.12247",
    "arxiv_authors": [
      "Manuel Brack",
      "Felix Friedrich",
      "Dominik Hintersdorf",
      "Lukas Struppek",
      "Patrick Schramowski",
      "Kristian Kersting"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=SEGA%3A+Instructing+Text-to-Image+Models+using+Semantic+Guidance+Manuel+Brack+Felix+Friedrich+Dominik+Hintersdorf+Lukas+Struppek+Patrick+Schramowski",
    "gs_search_success": true,
    "gs_authors": [
      "tU8K5qsAAAAJ",
      "DKITUfsAAAAJ",
      "kJ9Abf8AAAAJ",
      "RfM9ud0AAAAJ",
      "GD481RkAAAAJ",
      "QY-earAAAAAJ"
    ],
    "citation_count": 93,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2309.11928",
    "title": "Video Scene Location Recognition with Neural Networks",
    "year": 2023,
    "published": "2023-09-21T09:42:39Z",
    "categories": [
      "cs.CV",
      "cs.NE"
    ],
    "abstract": "This paper provides an insight into the possibility of scene recognition from a video sequence with a small set of repeated shooting locations (such as in television series) using artificial neural networks. The basic idea of the presented approach is to select a set of frames from each scene, transform them by a pre-trained singleimage pre-processing convolutional network, and classify the scene location with subsequent layers of the neural network. The considered networks have been tested and ",
    "arxiv_url": "https://arxiv.org/abs/2309.11928v1",
    "pdf_url": "https://arxiv.org/pdf/2309.11928v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2309.11928",
    "arxiv_authors": [
      "Lukáš Korel",
      "Petr Pulc",
      "Jiří Tumpach",
      "Martin Holeňa"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Video+Scene+Location+Recognition+with+Neural+Networks+Luk%C3%A1%C5%A1+Korel+Petr+Pulc+Ji%C5%99%C3%AD+Tumpach+Martin+Hole%C5%88a",
    "gs_search_success": true,
    "gs_authors": [
      "hfyjgFwAAAAJ",
      "crSZPAkAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2412.11596",
    "title": "MeshArt: Generating Articulated Meshes with Structure-Guided Transformers",
    "year": 2024,
    "published": "2024-12-16T09:35:08Z",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "abstract": "Articulated 3D object generation is fundamental for creating realistic, functional, and interactable virtual assets which are not simply static. We introduce MeshArt, a hierarchical transformer-based approach to generate articulated 3D meshes with clean, compact geometry, reminiscent of human-crafted 3D models. We approach articulated mesh generation in a part-by-part fashion across two stages. First, we generate a high-level articulation-aware object structure; then, based on this structural in",
    "arxiv_url": "https://arxiv.org/abs/2412.11596v2",
    "pdf_url": "https://arxiv.org/pdf/2412.11596v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2412.11596",
    "arxiv_authors": [
      "Daoyi Gao",
      "Yawar Siddiqui",
      "Lei Li",
      "Angela Dai"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=MeshArt%3A+Generating+Articulated+Meshes+with+Structure-Guided+Transformers+Daoyi+Gao+Yawar+Siddiqui+Lei+Li+Angela+Dai",
    "gs_search_success": true,
    "gs_authors": [
      "g-tGztMAAAAJ",
      "jbRrxncAAAAJ",
      "u26UK5QAAAAJ",
      "uzh8LlIAAAAJ"
    ],
    "citation_count": 11,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2303.11717",
    "title": "A Complete Survey on Generative AI (AIGC): Is ChatGPT from GPT-4 to GPT-5 All You Need?",
    "year": 2023,
    "published": "2023-03-21T10:09:47Z",
    "categories": [
      "cs.AI",
      "cs.CV",
      "cs.LG",
      "cs.MM"
    ],
    "abstract": "As ChatGPT goes viral, generative AI (AIGC, a.k.a AI-generated content) has made headlines everywhere because of its ability to analyze and create text, images, and beyond. With such overwhelming media coverage, it is almost impossible for us to miss the opportunity to glimpse AIGC from a certain angle. In the era of AI transitioning from pure analysis to creation, it is worth noting that ChatGPT, with its most recent language model GPT-4, is just a tool out of numerous AIGC tasks. Impressed by ",
    "arxiv_url": "https://arxiv.org/abs/2303.11717v1",
    "pdf_url": "https://arxiv.org/pdf/2303.11717v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2303.11717",
    "arxiv_authors": [
      "Chaoning Zhang",
      "Chenshuang Zhang",
      "Sheng Zheng",
      "Yu Qiao",
      "Chenghao Li",
      "Mengchun Zhang",
      "Sumit Kumar Dam",
      "Chu Myaet Thwal",
      "Ye Lin Tun",
      "Le Luang Huy",
      "Donguk kim",
      "Sung-Ho Bae",
      "Lik-Hang Lee",
      "Yang Yang",
      "Heng Tao Shen",
      "In So Kweon",
      "Choong Seon Hong"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=A+Complete+Survey+on+Generative+AI+%28AIGC%29%3A+Is+ChatGPT+from+GPT-4+to+GPT-5+All+You+Need%3F+Chaoning+Zhang+Chenshuang+Zhang+Sheng+Zheng+Yu+Qiao+Chenghao+Li",
    "gs_search_success": false,
    "gs_authors": [],
    "citation_count": null,
    "error_type": "fetch_failed",
    "gs_author_count": 9
  },
  {
    "arxiv_id": "2309.15812",
    "title": "Convolutional Networks with Oriented 1D Kernels",
    "year": 2023,
    "published": "2023-09-27T17:36:19Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "In computer vision, 2D convolution is arguably the most important operation performed by a ConvNet. Unsurprisingly, it has been the focus of intense software and hardware optimization and enjoys highly efficient implementations. In this work, we ask an intriguing question: can we make a ConvNet work without 2D convolutions? Surprisingly, we find that the answer is yes -- we show that a ConvNet consisting entirely of 1D convolutions can do just as well as 2D on ImageNet classification. Specifical",
    "arxiv_url": "https://arxiv.org/abs/2309.15812v1",
    "pdf_url": "https://arxiv.org/pdf/2309.15812v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2309.15812",
    "arxiv_authors": [
      "Alexandre Kirchmeyer",
      "Jia Deng"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Convolutional+Networks+with+Oriented+1D+Kernels+Alexandre+Kirchmeyer+Jia+Deng",
    "gs_search_success": true,
    "gs_authors": [
      "E-dauEcAAAAJ",
      "U3Eub-EAAAAJ"
    ],
    "citation_count": 11,
    "gs_author_count": 2
  },
  {
    "arxiv_id": "2408.09734",
    "title": "Mutually-Aware Feature Learning for Few-Shot Object Counting",
    "year": 2024,
    "published": "2024-08-19T06:46:24Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Few-shot object counting has garnered significant attention for its practicality as it aims to count target objects in a query image based on given exemplars without the need for additional training. However, there is a shortcoming in the prevailing extract-and-match approach: query and exemplar features lack interaction during feature extraction since they are extracted unaware of each other and later correlated based on similarity. This can lead to insufficient target awareness of the extracte",
    "arxiv_url": "https://arxiv.org/abs/2408.09734v1",
    "pdf_url": "https://arxiv.org/pdf/2408.09734v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2408.09734",
    "arxiv_authors": [
      "Yerim Jeon",
      "Subeen Lee",
      "Jihwan Kim",
      "Jae-Pil Heo"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Mutually-Aware+Feature+Learning+for+Few-Shot+Object+Counting+Yerim+Jeon+Subeen+Lee+Jihwan+Kim+Jae-Pil+Heo",
    "gs_search_success": true,
    "gs_authors": [
      "VXyJ_ssAAAAJ",
      "NAr1iIYAAAAJ",
      "pPtv7yYAAAAJ",
      "ENwf2OkAAAAJ"
    ],
    "citation_count": 3,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2312.01841",
    "title": "VividTalk: One-Shot Audio-Driven Talking Head Generation Based on 3D Hybrid Prior",
    "year": 2023,
    "published": "2023-12-04T12:25:37Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Audio-driven talking head generation has drawn much attention in recent years, and many efforts have been made in lip-sync, expressive facial expressions, natural head pose generation, and high video quality. However, no model has yet led or tied on all these metrics due to the one-to-many mapping between audio and motion. In this paper, we propose VividTalk, a two-stage generic framework that supports generating high-visual quality talking head videos with all the above properties. Specifically",
    "arxiv_url": "https://arxiv.org/abs/2312.01841v2",
    "pdf_url": "https://arxiv.org/pdf/2312.01841v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2312.01841",
    "arxiv_authors": [
      "Xusen Sun",
      "Longhao Zhang",
      "Hao Zhu",
      "Peng Zhang",
      "Bang Zhang",
      "Xinya Ji",
      "Kangneng Zhou",
      "Daiheng Gao",
      "Liefeng Bo",
      "Xun Cao"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=VividTalk%3A+One-Shot+Audio-Driven+Talking+Head+Generation+Based+on+3D+Hybrid+Prior+Xusen+Sun+Longhao+Zhang+Hao+Zhu+Peng+Zhang+Bang+Zhang",
    "gs_search_success": true,
    "gs_authors": [
      "sy_WtmcAAAAJ",
      "QTgxKmkAAAAJ",
      "Rogn7mcAAAAJ",
      "Y-ql3zMAAAAJ",
      "YRxe0FkAAAAJ",
      "FJwtMf0AAAAJ",
      "y1vvxWYAAAAJ"
    ],
    "citation_count": 45,
    "gs_author_count": 9
  },
  {
    "arxiv_id": "2504.03536",
    "title": "HumanDreamer-X: Photorealistic Single-image Human Avatars Reconstruction via Gaussian Restoration",
    "year": 2025,
    "published": "2025-04-04T15:35:14Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Single-image human reconstruction is vital for digital human modeling applications but remains an extremely challenging task. Current approaches rely on generative models to synthesize multi-view images for subsequent 3D reconstruction and animation. However, directly generating multiple views from a single human image suffers from geometric inconsistencies, resulting in issues like fragmented or blurred limbs in the reconstructed models. To tackle these limitations, we introduce \\textbf{HumanDr",
    "arxiv_url": "https://arxiv.org/abs/2504.03536v2",
    "pdf_url": "https://arxiv.org/pdf/2504.03536v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2504.03536",
    "arxiv_authors": [
      "Boyuan Wang",
      "Runqi Ouyang",
      "Xiaofeng Wang",
      "Zheng Zhu",
      "Guosheng Zhao",
      "Chaojun Ni",
      "Xiaopei Zhang",
      "Guan Huang",
      "Yijie Ren",
      "Lihong Liu",
      "Xingang Wang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=HumanDreamer-X%3A+Photorealistic+Single-image+Human+Avatars+Reconstruction+via+Gaussian+Restoration+Boyuan+Wang+Runqi+Ouyang+Xiaofeng+Wang+Zheng+Zhu+Guosheng+Zhao",
    "gs_search_success": true,
    "gs_authors": [
      "C3BbrU4AAAAJ",
      "Wp6EW3kAAAAJ",
      "5IJ0Yg4AAAAJ",
      "Wp7sUlIAAAAJ",
      "NmwjI0AAAAAJ"
    ],
    "citation_count": 6,
    "gs_author_count": 9
  },
  {
    "arxiv_id": "2501.18880",
    "title": "RLS3: RL-Based Synthetic Sample Selection to Enhance Spatial Reasoning in Vision-Language Models for Indoor Autonomous Perception",
    "year": 2025,
    "published": "2025-01-31T04:30:42Z",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "Vision-language model (VLM) fine-tuning for application-specific visual grounding based on natural language instructions has become one of the most popular approaches for learning-enabled autonomous systems. However, such fine-tuning relies heavily on high-quality datasets to achieve successful performance in various downstream tasks. Additionally, VLMs often encounter limitations due to insufficient and imbalanced fine-tuning data. To address these issues, we propose a new generalizable framewo",
    "arxiv_url": "https://arxiv.org/abs/2501.18880v1",
    "pdf_url": "https://arxiv.org/pdf/2501.18880v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2501.18880",
    "arxiv_authors": [
      "Joshua R. Waite",
      "Md. Zahid Hasan",
      "Qisai Liu",
      "Zhanhong Jiang",
      "Chinmay Hegde",
      "Soumik Sarkar"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=RLS3%3A+RL-Based+Synthetic+Sample+Selection+to+Enhance+Spatial+Reasoning+in+Vision-Language+Models+for+Indoor+Autonomous+Perception+Joshua+R.+Waite+Md.+Zahid+Hasan+Qisai+Liu+Zhanhong+Jiang+Chinmay+Hegde",
    "gs_search_success": true,
    "gs_authors": [
      "skX2i7kAAAAJ",
      "NQnok3sAAAAJ",
      "iNYmolkAAAAJ",
      "eJAV17IAAAAJ",
      "-rmRjqIAAAAJ",
      "zRRvPRMAAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2503.04500",
    "title": "ReynoldsFlow: Exquisite Flow Estimation via Reynolds Transport Theorem",
    "year": 2025,
    "published": "2025-03-06T14:49:28Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Optical flow is a fundamental technique for motion estimation, widely applied in video stabilization, interpolation, and object tracking. Traditional optical flow estimation methods rely on restrictive assumptions like brightness constancy and slow motion constraints. Recent deep learning-based flow estimations require extensive training on large domain-specific datasets, making them computationally demanding. Also, artificial intelligence (AI) advances have enabled deep learning models to take ",
    "arxiv_url": "https://arxiv.org/abs/2503.04500v2",
    "pdf_url": "https://arxiv.org/pdf/2503.04500v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2503.04500",
    "arxiv_authors": [
      "Yu-Hsi Chen",
      "Chin-Tien Wu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=ReynoldsFlow%3A+Exquisite+Flow+Estimation+via+Reynolds+Transport+Theorem+Yu-Hsi+Chen+Chin-Tien+Wu",
    "gs_search_success": true,
    "gs_authors": [
      "kcD42qYAAAAJ"
    ],
    "citation_count": 3,
    "gs_author_count": 2
  },
  {
    "arxiv_id": "2302.02483",
    "title": "Multi-Task Self-Supervised Learning for Image Segmentation Task",
    "year": 2023,
    "published": "2023-02-05T21:25:59Z",
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "abstract": "Thanks to breakthroughs in AI and Deep learning methodology, Computer vision techniques are rapidly improving. Most computer vision applications require sophisticated image segmentation to comprehend what is image and to make an analysis of each section easier. Training deep learning networks for semantic segmentation required a large amount of annotated data, which presents a major challenge in practice as it is expensive and labor-intensive to produce such data. The paper presents 1. Self-supe",
    "arxiv_url": "https://arxiv.org/abs/2302.02483v1",
    "pdf_url": "https://arxiv.org/pdf/2302.02483v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2302.02483",
    "arxiv_authors": [
      "Lichun Gao",
      "Chinmaya Khamesra",
      "Uday Kumbhar",
      "Ashay Aglawe"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Multi-Task+Self-Supervised+Learning+for+Image+Segmentation+Task+Lichun+Gao+Chinmaya+Khamesra+Uday+Kumbhar+Ashay+Aglawe",
    "gs_search_success": true,
    "gs_authors": [],
    "citation_count": 3,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2304.08809",
    "title": "SViTT: Temporal Learning of Sparse Video-Text Transformers",
    "year": 2023,
    "published": "2023-04-18T08:17:58Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Do video-text transformers learn to model temporal relationships across frames? Despite their immense capacity and the abundance of multimodal training data, recent work has revealed the strong tendency of video-text models towards frame-based spatial representations, while temporal reasoning remains largely unsolved. In this work, we identify several key challenges in temporal learning of video-text transformers: the spatiotemporal trade-off from limited network size; the curse of dimensionalit",
    "arxiv_url": "https://arxiv.org/abs/2304.08809v1",
    "pdf_url": "https://arxiv.org/pdf/2304.08809v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2304.08809",
    "arxiv_authors": [
      "Yi Li",
      "Kyle Min",
      "Subarna Tripathi",
      "Nuno Vasconcelos"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=SViTT%3A+Temporal+Learning+of+Sparse+Video-Text+Transformers+Yi+Li+Kyle+Min+Subarna+Tripathi+Nuno+Vasconcelos",
    "gs_search_success": true,
    "gs_authors": [
      "nY-HwbwAAAAJ",
      "Fykyo9gAAAAJ",
      "OEM7mQUAAAAJ",
      "HPfNU94AAAAJ"
    ],
    "citation_count": 26,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2407.04542",
    "title": "Rethinking Image Compression on the Web with Generative AI",
    "year": 2024,
    "published": "2024-07-05T14:29:12Z",
    "categories": [
      "cs.NI",
      "cs.CV",
      "cs.LG",
      "eess.IV"
    ],
    "abstract": "The rapid growth of the Internet, driven by social media, web browsing, and video streaming, has made images central to the Web experience, resulting in significant data transfer and increased webpage sizes. Traditional image compression methods, while reducing bandwidth, often degrade image quality. This paper explores a novel approach using generative AI to reconstruct images at the edge or client-side. We develop a framework that leverages text prompts and provides additional conditioning inp",
    "arxiv_url": "https://arxiv.org/abs/2407.04542v1",
    "pdf_url": "https://arxiv.org/pdf/2407.04542v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2407.04542",
    "arxiv_authors": [
      "Shayan Ali Hassan",
      "Danish Humair",
      "Ihsan Ayyub Qazi",
      "Zafar Ayyub Qazi"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Rethinking+Image+Compression+on+the+Web+with+Generative+AI+Shayan+Ali+Hassan+Danish+Humair+Ihsan+Ayyub+Qazi+Zafar+Ayyub+Qazi",
    "gs_search_success": true,
    "gs_authors": [
      "lVzlcmUAAAAJ",
      "O5uXfioAAAAJ",
      "ndJQTE0AAAAJ",
      "iaW_t6EAAAAJ"
    ],
    "citation_count": 4,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2404.01294",
    "title": "CosmicMan: A Text-to-Image Foundation Model for Humans",
    "year": 2024,
    "published": "2024-04-01T17:59:05Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "We present CosmicMan, a text-to-image foundation model specialized for generating high-fidelity human images. Unlike current general-purpose foundation models that are stuck in the dilemma of inferior quality and text-image misalignment for humans, CosmicMan enables generating photo-realistic human images with meticulous appearance, reasonable structure, and precise text-image alignment with detailed dense descriptions. At the heart of CosmicMan's success are the new reflections and perspectives",
    "arxiv_url": "https://arxiv.org/abs/2404.01294v1",
    "pdf_url": "https://arxiv.org/pdf/2404.01294v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2404.01294",
    "arxiv_authors": [
      "Shikai Li",
      "Jianglin Fu",
      "Kaiyuan Liu",
      "Wentao Wang",
      "Kwan-Yee Lin",
      "Wayne Wu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=CosmicMan%3A+A+Text-to-Image+Foundation+Model+for+Humans+Shikai+Li+Jianglin+Fu+Kaiyuan+Liu+Wentao+Wang+Kwan-Yee+Lin",
    "gs_search_success": true,
    "gs_authors": [
      "beGt3cAAAAAJ",
      "WXGg2rgAAAAJ",
      "uWfZKz4AAAAJ",
      "CX-o0V4AAAAJ",
      "IJT2bOcAAAAJ"
    ],
    "citation_count": 36,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2311.03762",
    "title": "Image change detection with only a few samples",
    "year": 2023,
    "published": "2023-11-07T07:01:35Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "This paper considers image change detection with only a small number of samples, which is a significant problem in terms of a few annotations available. A major impediment of image change detection task is the lack of large annotated datasets covering a wide variety of scenes. Change detection models trained on insufficient datasets have shown poor generalization capability. To address the poor generalization issue, we propose using simple image processing methods for generating synthetic but in",
    "arxiv_url": "https://arxiv.org/abs/2311.03762v1",
    "pdf_url": "https://arxiv.org/pdf/2311.03762v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2311.03762",
    "arxiv_authors": [
      "Ke Liu",
      "Zhaoyi Song",
      "Haoyue Bai"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Image+change+detection+with+only+a+few+samples+Ke+Liu+Zhaoyi+Song+Haoyue+Bai",
    "gs_search_success": false,
    "gs_authors": [],
    "citation_count": null,
    "error_type": "fetch_failed",
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2503.11051",
    "title": "Towards Privacy-preserved Pre-training of Remote Sensing Foundation Models with Federated Mutual-guidance Learning",
    "year": 2025,
    "published": "2025-03-14T03:38:49Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Traditional Remote Sensing Foundation models (RSFMs) are pre-trained with a data-centralized paradigm, through self-supervision on large-scale curated remote sensing data. For each institution, however, pre-training RSFMs with limited data in a standalone manner may lead to suboptimal performance, while aggregating remote sensing data from multiple institutions for centralized pre-training raises privacy concerns. Seeking for collaboration is a promising solution to resolve this dilemma, where m",
    "arxiv_url": "https://arxiv.org/abs/2503.11051v1",
    "pdf_url": "https://arxiv.org/pdf/2503.11051v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2503.11051",
    "arxiv_authors": [
      "Jieyi Tan",
      "Chengwei Zhang",
      "Bo Dang",
      "Yansheng Li"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Towards+Privacy-preserved+Pre-training+of+Remote+Sensing+Foundation+Models+with+Federated+Mutual-guidance+Learning+Jieyi+Tan+Chengwei+Zhang+Bo+Dang+Yansheng+Li",
    "gs_search_success": true,
    "gs_authors": [
      "wn9hc6UAAAAJ",
      "AXlR4OwAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2505.17971",
    "title": "Explainable Anatomy-Guided AI for Prostate MRI: Foundation Models and In Silico Clinical Trials for Virtual Biopsy-based Risk Assessment",
    "year": 2025,
    "published": "2025-05-23T14:40:09Z",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "abstract": "We present a fully automated, anatomically guided deep learning pipeline for prostate cancer (PCa) risk stratification using routine MRI. The pipeline integrates three key components: an nnU-Net module for segmenting the prostate gland and its zones on axial T2-weighted MRI; a classification module based on the UMedPT Swin Transformer foundation model, fine-tuned on 3D patches with optional anatomical priors and clinical data; and a VAE-GAN framework for generating counterfactual heatmaps that l",
    "arxiv_url": "https://arxiv.org/abs/2505.17971v1",
    "pdf_url": "https://arxiv.org/pdf/2505.17971v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2505.17971",
    "arxiv_authors": [
      "Danial Khan",
      "Zohaib Salahuddin",
      "Yumeng Zhang",
      "Sheng Kuang",
      "Shruti Atul Mali",
      "Henry C. Woodruff",
      "Sina Amirrajab",
      "Rachel Cavill",
      "Eduardo Ibor-Crespo",
      "Ana Jimenez-Pastor",
      "Adrian Galiana-Bordera",
      "Paula Jimenez Gomez",
      "Luis Marti-Bonmati",
      "Philippe Lambin"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Explainable+Anatomy-Guided+AI+for+Prostate+MRI%3A+Foundation+Models+and+In+Silico+Clinical+Trials+for+Virtual+Biopsy-based+Risk+Assessment+Danial+Khan+Zohaib+Salahuddin+Yumeng+Zhang+Sheng+Kuang+Shruti+Atul+Mali",
    "gs_search_success": true,
    "gs_authors": [
      "m0TFXOoAAAAJ",
      "z6VfuPcAAAAJ",
      "YisQqGIAAAAJ",
      "Y8b7DiwAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 9
  },
  {
    "arxiv_id": "2305.18035",
    "title": "Physics-Informed Computer Vision: A Review and Perspectives",
    "year": 2023,
    "published": "2023-05-29T11:55:11Z",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "abstract": "The incorporation of physical information in machine learning frameworks is opening and transforming many application domains. Here the learning process is augmented through the induction of fundamental knowledge and governing physical laws. In this work, we explore their utility for computer vision tasks in interpreting and understanding visual data. We present a systematic literature review of more than 250 papers on formulation and approaches to computer vision tasks guided by physical laws. ",
    "arxiv_url": "https://arxiv.org/abs/2305.18035v3",
    "pdf_url": "https://arxiv.org/pdf/2305.18035v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2305.18035",
    "arxiv_authors": [
      "Chayan Banerjee",
      "Kien Nguyen",
      "Clinton Fookes",
      "George Karniadakis"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Physics-Informed+Computer+Vision%3A+A+Review+and+Perspectives+Chayan+Banerjee+Kien+Nguyen+Clinton+Fookes+George+Karniadakis",
    "gs_search_success": true,
    "gs_authors": [
      "yZ0-ywkAAAAJ",
      "18HsR5EAAAAJ",
      "65hqtGYAAAAJ",
      "VpaJsNQAAAAJ"
    ],
    "citation_count": 75,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2312.02625",
    "title": "Diffusion Noise Feature: Accurate and Fast Generated Image Detection",
    "year": 2023,
    "published": "2023-12-05T10:01:11Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Generative models now produce images with such stunning realism that they can easily deceive the human eye. While this progress unlocks vast creative potential, it also presents significant risks, such as the spread of misinformation. Consequently, detecting generated images has become a critical research challenge. However, current detection methods are often plagued by low accuracy and poor generalization. In this paper, to address these limitations and enhance the detection of generated image",
    "arxiv_url": "https://arxiv.org/abs/2312.02625v3",
    "pdf_url": "https://arxiv.org/pdf/2312.02625v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2312.02625",
    "arxiv_authors": [
      "Yichi Zhang",
      "Xiaogang Xu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Diffusion+Noise+Feature%3A+Accurate+and+Fast+Generated+Image+Detection+Yichi+Zhang+Xiaogang+Xu",
    "gs_search_success": true,
    "gs_authors": [
      "Cno6rFYAAAAJ",
      "R65xDQwAAAAJ"
    ],
    "citation_count": 23,
    "gs_author_count": 2
  },
  {
    "arxiv_id": "2402.06855",
    "title": "For Better or For Worse? Learning Minimum Variance Features With Label Augmentation",
    "year": 2024,
    "published": "2024-02-10T01:36:39Z",
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "abstract": "Data augmentation has been pivotal in successfully training deep learning models on classification tasks over the past decade. An important subclass of data augmentation techniques - which includes both label smoothing and Mixup - involves modifying not only the input data but also the input label during model training. In this work, we analyze the role played by the label augmentation aspect of such methods. We first prove that linear models on binary classification data trained with label augm",
    "arxiv_url": "https://arxiv.org/abs/2402.06855v3",
    "pdf_url": "https://arxiv.org/pdf/2402.06855v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2402.06855",
    "arxiv_authors": [
      "Muthu Chidambaram",
      "Rong Ge"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=For+Better+or+For+Worse%3F+Learning+Minimum+Variance+Features+With+Label+Augmentation+Muthu+Chidambaram+Rong+Ge",
    "gs_search_success": true,
    "gs_authors": [
      "s4f3sc0AAAAJ",
      "MVxcjEoAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 2
  },
  {
    "arxiv_id": "2411.09411",
    "title": "Building Height Estimation Using Shadow Length in Satellite Imagery",
    "year": 2024,
    "published": "2024-11-14T13:06:18Z",
    "categories": [
      "cs.CV",
      "eess.IV"
    ],
    "abstract": "Estimating building height from satellite imagery poses significant challenges, especially when monocular images are employed, resulting in a loss of essential 3D information during imaging. This loss of spatial depth further complicates the height estimation process. We addressed this issue by using shadow length as an additional cue to compensate for the loss of building height estimation using single-view imagery. We proposed a novel method that first localized a building and its shadow in th",
    "arxiv_url": "https://arxiv.org/abs/2411.09411v1",
    "pdf_url": "https://arxiv.org/pdf/2411.09411v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2411.09411",
    "arxiv_authors": [
      "Mahd Qureshi",
      "Shayaan Chaudhry",
      "Sana Jabba",
      "Murtaza Taj"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Building+Height+Estimation+Using+Shadow+Length+in+Satellite+Imagery+Mahd+Qureshi+Shayaan+Chaudhry+Sana+Jabba+Murtaza+Taj",
    "gs_search_success": true,
    "gs_authors": [
      "fm7aoYsAAAAJ",
      "Gk3G9N0AAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2505.22016",
    "title": "PanoWan: Lifting Diffusion Video Generation Models to 360° with Latitude/Longitude-aware Mechanisms",
    "year": 2025,
    "published": "2025-05-28T06:24:21Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Panoramic video generation enables immersive 360° content creation, valuable in applications that demand scene-consistent world exploration. However, existing panoramic video generation models struggle to leverage pre-trained generative priors from conventional text-to-video models for high-quality and diverse panoramic videos generation, due to limited dataset scale and the gap in spatial feature representations. In this paper, we introduce PanoWan to effectively lift pre-trained text-to-video ",
    "arxiv_url": "https://arxiv.org/abs/2505.22016v2",
    "pdf_url": "https://arxiv.org/pdf/2505.22016v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2505.22016",
    "arxiv_authors": [
      "Yifei Xia",
      "Shuchen Weng",
      "Siqi Yang",
      "Jingqi Liu",
      "Chengxuan Zhu",
      "Minggui Teng",
      "Zijian Jia",
      "Han Jiang",
      "Boxin Shi"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=PanoWan%3A+Lifting+Diffusion+Video+Generation+Models+to+360%C2%B0+with+Latitude%2FLongitude-aware+Mechanisms+Yifei+Xia+Shuchen+Weng+Siqi+Yang+Jingqi+Liu+Chengxuan+Zhu",
    "gs_search_success": true,
    "gs_authors": [
      "-5qVEQsAAAAJ",
      "0c_Zm0QAAAAJ",
      "BukbqcwAAAAJ",
      "kL7f39wAAAAJ"
    ],
    "citation_count": 3,
    "gs_author_count": 9
  },
  {
    "arxiv_id": "2403.04899",
    "title": "Towards Scene Graph Anticipation",
    "year": 2024,
    "published": "2024-03-07T21:08:51Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Spatio-temporal scene graphs represent interactions in a video by decomposing scenes into individual objects and their pair-wise temporal relationships. Long-term anticipation of the fine-grained pair-wise relationships between objects is a challenging problem. To this end, we introduce the task of Scene Graph Anticipation (SGA). We adapt state-of-the-art scene graph generation methods as baselines to anticipate future pair-wise relationships between objects and propose a novel approach SceneSay",
    "arxiv_url": "https://arxiv.org/abs/2403.04899v2",
    "pdf_url": "https://arxiv.org/pdf/2403.04899v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2403.04899",
    "arxiv_authors": [
      "Rohith Peddi",
      "Saksham Singh",
      "Saurabh",
      "Parag Singla",
      "Vibhav Gogate"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Towards+Scene+Graph+Anticipation+Rohith+Peddi+Saksham+Singh+Saurabh+Parag+Singla+Vibhav+Gogate",
    "gs_search_success": true,
    "gs_authors": [
      "gmdiMyoAAAAJ",
      "V49BsgMAAAAJ",
      "pm_dg3cAAAAJ",
      "MvhACDcAAAAJ"
    ],
    "citation_count": 11,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2409.15739",
    "title": "Teaching Tailored to Talent: Adverse Weather Restoration via Prompt Pool and Depth-Anything Constraint",
    "year": 2024,
    "published": "2024-09-24T04:46:18Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Recent advancements in adverse weather restoration have shown potential, yet the unpredictable and varied combinations of weather degradations in the real world pose significant challenges. Previous methods typically struggle with dynamically handling intricate degradation combinations and carrying on background reconstruction precisely, leading to performance and generalization limitations. Drawing inspiration from prompt learning and the \"Teaching Tailored to Talent\" concept, we introduce a no",
    "arxiv_url": "https://arxiv.org/abs/2409.15739v1",
    "pdf_url": "https://arxiv.org/pdf/2409.15739v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2409.15739",
    "arxiv_authors": [
      "Sixiang Chen",
      "Tian Ye",
      "Kai Zhang",
      "Zhaohu Xing",
      "Yunlong Lin",
      "Lei Zhu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Teaching+Tailored+to+Talent%3A+Adverse+Weather+Restoration+via+Prompt+Pool+and+Depth-Anything+Constraint+Sixiang+Chen+Tian+Ye+Kai+Zhang+Zhaohu+Xing+Yunlong+Lin",
    "gs_search_success": true,
    "gs_authors": [
      "5F3tICwAAAAJ",
      "O3x90PQAAAAJ",
      "1sGXZ-wAAAAJ",
      "EtljKSgAAAAJ",
      "AQtqhaYAAAAJ"
    ],
    "citation_count": 24,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2503.14975",
    "title": "Taming Flow Matching with Unbalanced Optimal Transport into Fast Pansharpening",
    "year": 2025,
    "published": "2025-03-19T08:10:49Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Pansharpening, a pivotal task in remote sensing for fusing high-resolution panchromatic and multispectral imagery, has garnered significant research interest. Recent advancements employing diffusion models based on stochastic differential equations (SDEs) have demonstrated state-of-the-art performance. However, the inherent multi-step sampling process of SDEs imposes substantial computational overhead, hindering practical deployment. While existing methods adopt efficient samplers, knowledge dis",
    "arxiv_url": "https://arxiv.org/abs/2503.14975v1",
    "pdf_url": "https://arxiv.org/pdf/2503.14975v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2503.14975",
    "arxiv_authors": [
      "Zihan Cao",
      "Yu Zhong",
      "Liang-Jian Deng"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Taming+Flow+Matching+with+Unbalanced+Optimal+Transport+into+Fast+Pansharpening+Zihan+Cao+Yu+Zhong+Liang-Jian+Deng",
    "gs_search_success": true,
    "gs_authors": [
      "TZs9NxkAAAAJ",
      "pv61p_EAAAAJ",
      "UgD8AtkAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2504.14736",
    "title": "ChronoRoot 2.0: An Open AI-Powered Platform for 2D Temporal Plant Phenotyping",
    "year": 2025,
    "published": "2025-04-20T20:56:25Z",
    "categories": [
      "cs.CV",
      "q-bio.QM"
    ],
    "abstract": "The analysis of plant developmental plasticity, including root system architecture, is fundamental to understanding plant adaptability and development, particularly in the context of climate change and agricultural sustainability. While significant advances have been made in plant phenotyping technologies, comprehensive temporal analysis of root development remains challenging, with most existing solutions providing either limited throughput or restricted structural analysis capabilities. Here, ",
    "arxiv_url": "https://arxiv.org/abs/2504.14736v1",
    "pdf_url": "https://arxiv.org/pdf/2504.14736v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2504.14736",
    "arxiv_authors": [
      "Nicolás Gaggion",
      "Rodrigo Bonazzola",
      "María Florencia Legascue",
      "María Florencia Mammarella",
      "Florencia Sol Rodriguez",
      "Federico Emanuel Aballay",
      "Florencia Belén Catulo",
      "Andana Barrios",
      "Franco Accavallo",
      "Santiago Nahuel Villarreal",
      "Martin Crespi",
      "Martiniano María Ricardi",
      "Ezequiel Petrillo",
      "Thomas Blein",
      "Federico Ariel",
      "Enzo Ferrante"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=ChronoRoot+2.0%3A+An+Open+AI-Powered+Platform+for+2D+Temporal+Plant+Phenotyping+Nicol%C3%A1s+Gaggion+Rodrigo+Bonazzola+Mar%C3%ADa+Florencia+Legascue+Mar%C3%ADa+Florencia+Mammarella+Florencia+Sol+Rodriguez",
    "gs_search_success": true,
    "gs_authors": [
      "CnwAbLMAAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 9
  },
  {
    "arxiv_id": "2411.13490",
    "title": "Efficient Brain Imaging Analysis for Alzheimer's and Dementia Detection Using Convolution-Derivative Operations",
    "year": 2024,
    "published": "2024-11-20T17:38:34Z",
    "categories": [
      "eess.IV",
      "cs.CV",
      "cs.NE",
      "cs.PF"
    ],
    "abstract": "Alzheimer's disease (AD) is characterized by progressive neurodegeneration and results in detrimental structural changes in human brains. Detecting these changes is crucial for early diagnosis and timely intervention of disease progression. Jacobian maps, derived from spatial normalization in voxel-based morphometry (VBM), have been instrumental in interpreting volume alterations associated with AD. However, the computational cost of generating Jacobian maps limits its clinical adoption. In this",
    "arxiv_url": "https://arxiv.org/abs/2411.13490v2",
    "pdf_url": "https://arxiv.org/pdf/2411.13490v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2411.13490",
    "arxiv_authors": [
      "Yasmine Mustafa",
      "Mohamed Elmahallawy",
      "Tie Luo"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Efficient+Brain+Imaging+Analysis+for+Alzheimer%27s+and+Dementia+Detection+Using+Convolution-Derivative+Operations+Yasmine+Mustafa+Mohamed+Elmahallawy+Tie+Luo",
    "gs_search_success": true,
    "gs_authors": [
      "4F60wpoAAAAJ",
      "NptWuzoAAAAJ",
      "xr7LozIAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2412.05897",
    "title": "Epistemic Uncertainty for Generated Image Detection",
    "year": 2024,
    "published": "2024-12-08T11:32:25Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "We introduce a novel framework for AI-generated image detection through epistemic uncertainty, aiming to address critical security concerns in the era of generative models. Our key insight stems from the observation that distributional discrepancies between training and testing data manifest distinctively in the epistemic uncertainty space of machine learning models. In this context, the distribution shift between natural and generated images leads to elevated epistemic uncertainty in models tra",
    "arxiv_url": "https://arxiv.org/abs/2412.05897v2",
    "pdf_url": "https://arxiv.org/pdf/2412.05897v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2412.05897",
    "arxiv_authors": [
      "Jun Nie",
      "Yonggang Zhang",
      "Tongliang Liu",
      "Yiu-ming Cheung",
      "Bo Han",
      "Xinmei Tian"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Epistemic+Uncertainty+for+Generated+Image+Detection+Jun+Nie+Yonggang+Zhang+Tongliang+Liu+Yiu-ming+Cheung+Bo+Han",
    "gs_search_success": false,
    "gs_authors": [],
    "citation_count": null,
    "error_type": "fetch_failed",
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2304.04211",
    "title": "AGAD: Adversarial Generative Anomaly Detection",
    "year": 2023,
    "published": "2023-04-09T10:40:02Z",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "Anomaly detection suffered from the lack of anomalies due to the diversity of abnormalities and the difficulties of obtaining large-scale anomaly data. Semi-supervised anomaly detection methods are often used to solely leverage normal data to detect abnormalities that deviated from the learnt normality distributions. Meanwhile, given the fact that limited anomaly data can be obtained with a minor cost in practice, some researches also investigated anomaly detection methods under supervised scena",
    "arxiv_url": "https://arxiv.org/abs/2304.04211v1",
    "pdf_url": "https://arxiv.org/pdf/2304.04211v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2304.04211",
    "arxiv_authors": [
      "Jian Shi",
      "Ni Zhang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=AGAD%3A+Adversarial+Generative+Anomaly+Detection+Jian+Shi+Ni+Zhang",
    "gs_search_success": true,
    "gs_authors": [
      "LFlRgLEAAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 2
  },
  {
    "arxiv_id": "2308.02669",
    "title": "ConceptLab: Creative Concept Generation using VLM-Guided Diffusion Prior Constraints",
    "year": 2023,
    "published": "2023-08-03T17:04:41Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Recent text-to-image generative models have enabled us to transform our words into vibrant, captivating imagery. The surge of personalization techniques that has followed has also allowed us to imagine unique concepts in new scenes. However, an intriguing question remains: How can we generate a new, imaginary concept that has never been seen before? In this paper, we present the task of creative text-to-image generation, where we seek to generate new members of a broad category (e.g., generating",
    "arxiv_url": "https://arxiv.org/abs/2308.02669v2",
    "pdf_url": "https://arxiv.org/pdf/2308.02669v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2308.02669",
    "arxiv_authors": [
      "Elad Richardson",
      "Kfir Goldberg",
      "Yuval Alaluf",
      "Daniel Cohen-Or"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=ConceptLab%3A+Creative+Concept+Generation+using+VLM-Guided+Diffusion+Prior+Constraints+Elad+Richardson+Kfir+Goldberg+Yuval+Alaluf+Daniel+Cohen-Or",
    "gs_search_success": true,
    "gs_authors": [
      "9npMV2kAAAAJ",
      "zvmGNjEAAAAJ",
      "fAxws1sAAAAJ",
      "uvaPP80AAAAJ"
    ],
    "citation_count": 20,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2410.08326",
    "title": "Neural Architecture Search of Hybrid Models for NPU-CIM Heterogeneous AR/VR Devices",
    "year": 2024,
    "published": "2024-10-10T19:30:34Z",
    "categories": [
      "cs.CV",
      "cs.AR",
      "cs.LG",
      "cs.PF"
    ],
    "abstract": "Low-Latency and Low-Power Edge AI is essential for Virtual Reality and Augmented Reality applications. Recent advances show that hybrid models, combining convolution layers (CNN) and transformers (ViT), often achieve superior accuracy/performance tradeoff on various computer vision and machine learning (ML) tasks. However, hybrid ML models can pose system challenges for latency and energy-efficiency due to their diverse nature in dataflow and memory access patterns. In this work, we leverage the",
    "arxiv_url": "https://arxiv.org/abs/2410.08326v1",
    "pdf_url": "https://arxiv.org/pdf/2410.08326v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2410.08326",
    "arxiv_authors": [
      "Yiwei Zhao",
      "Ziyun Li",
      "Win-San Khwa",
      "Xiaoyu Sun",
      "Sai Qian Zhang",
      "Syed Shakib Sarwar",
      "Kleber Hugo Stangherlin",
      "Yi-Lun Lu",
      "Jorge Tomas Gomez",
      "Jae-Sun Seo",
      "Phillip B. Gibbons",
      "Barbara De Salvo",
      "Chiao Liu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Neural+Architecture+Search+of+Hybrid+Models+for+NPU-CIM+Heterogeneous+AR%2FVR+Devices+Yiwei+Zhao+Ziyun+Li+Win-San+Khwa+Xiaoyu+Sun+Sai+Qian+Zhang",
    "gs_search_success": true,
    "gs_authors": [
      "aIu67IkAAAAJ",
      "kcCZkTwAAAAJ",
      "jabbdXIAAAAJ",
      "rd9OFHwAAAAJ",
      "dbOGRFoAAAAJ",
      "ccDp8goAAAAJ",
      "kldNhnYAAAAJ",
      "4quSKooAAAAJ"
    ],
    "citation_count": 5,
    "gs_author_count": 9
  },
  {
    "arxiv_id": "2403.00353",
    "title": "MS-Net: A Multi-Path Sparse Model for Motion Prediction in Multi-Scenes",
    "year": 2024,
    "published": "2024-03-01T08:32:12Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.RO"
    ],
    "abstract": "The multi-modality and stochastic characteristics of human behavior make motion prediction a highly challenging task, which is critical for autonomous driving. While deep learning approaches have demonstrated their great potential in this area, it still remains unsolved to establish a connection between multiple driving scenes (e.g., merging, roundabout, intersection) and the design of deep learning models. Current learning-based methods typically use one unified model to predict trajectories in",
    "arxiv_url": "https://arxiv.org/abs/2403.00353v1",
    "pdf_url": "https://arxiv.org/pdf/2403.00353v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2403.00353",
    "arxiv_authors": [
      "Xiaqiang Tang",
      "Weigao Sun",
      "Siyuan Hu",
      "Yiyang Sun",
      "Yafeng Guo"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=MS-Net%3A+A+Multi-Path+Sparse+Model+for+Motion+Prediction+in+Multi-Scenes+Xiaqiang+Tang+Weigao+Sun+Siyuan+Hu+Yiyang+Sun+Yafeng+Guo",
    "gs_search_success": true,
    "gs_authors": [
      "wp1ePfYAAAAJ",
      "HGQ7pdkAAAAJ"
    ],
    "citation_count": 7,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2307.10305",
    "title": "Tapestry of Time and Actions: Modeling Human Activity Sequences using Temporal Point Process Flows",
    "year": 2023,
    "published": "2023-07-13T19:17:54Z",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "Human beings always engage in a vast range of activities and tasks that demonstrate their ability to adapt to different scenarios. Any human activity can be represented as a temporal sequence of actions performed to achieve a certain goal. Unlike the time series datasets extracted from electronics or machines, these action sequences are highly disparate in their nature -- the time to finish a sequence of actions can vary between different persons. Therefore, understanding the dynamics of these s",
    "arxiv_url": "https://arxiv.org/abs/2307.10305v1",
    "pdf_url": "https://arxiv.org/pdf/2307.10305v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2307.10305",
    "arxiv_authors": [
      "Vinayak Gupta",
      "Srikanta Bedathur"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Tapestry+of+Time+and+Actions%3A+Modeling+Human+Activity+Sequences+using+Temporal+Point+Process+Flows+Vinayak+Gupta+Srikanta+Bedathur",
    "gs_search_success": true,
    "gs_authors": [
      "tQuRm1AAAAAJ",
      "ngfF2oAAAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 2
  },
  {
    "arxiv_id": "2310.08948",
    "title": "Federated Class-Incremental Learning with Prompting",
    "year": 2023,
    "published": "2023-10-13T08:35:02Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "As Web technology continues to develop, it has become increasingly common to use data stored on different clients. At the same time, federated learning has received widespread attention due to its ability to protect data privacy when let models learn from data which is distributed across various clients. However, most existing works assume that the client's data are fixed. In real-world scenarios, such an assumption is most likely not true as data may be continuously generated and new classes ma",
    "arxiv_url": "https://arxiv.org/abs/2310.08948v2",
    "pdf_url": "https://arxiv.org/pdf/2310.08948v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2310.08948",
    "arxiv_authors": [
      "Xin Luo",
      "Fang-Yi Liang",
      "Jiale Liu",
      "Yu-Wei Zhan",
      "Zhen-Duo Chen",
      "Xin-Shun Xu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Federated+Class-Incremental+Learning+with+Prompting+Xin+Luo+Fang-Yi+Liang+Jiale+Liu+Yu-Wei+Zhan+Zhen-Duo+Chen",
    "gs_search_success": true,
    "gs_authors": [
      "ZaCsoy0AAAAJ",
      "iM_-4-sAAAAJ",
      "ICzwFaIAAAAJ"
    ],
    "citation_count": 7,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2310.01842",
    "title": "SelfGraphVQA: A Self-Supervised Graph Neural Network for Scene-based Question Answering",
    "year": 2023,
    "published": "2023-10-03T07:14:53Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "The intersection of vision and language is of major interest due to the increased focus on seamless integration between recognition and reasoning. Scene graphs (SGs) have emerged as a useful tool for multimodal image analysis, showing impressive performance in tasks such as Visual Question Answering (VQA). In this work, we demonstrate that despite the effectiveness of scene graphs in VQA tasks, current methods that utilize idealized annotated scene graphs struggle to generalize when using predic",
    "arxiv_url": "https://arxiv.org/abs/2310.01842v1",
    "pdf_url": "https://arxiv.org/pdf/2310.01842v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2310.01842",
    "arxiv_authors": [
      "Bruno Souza",
      "Marius Aasan",
      "Helio Pedrini",
      "Adín Ramírez Rivera"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=SelfGraphVQA%3A+A+Self-Supervised+Graph+Neural+Network+for+Scene-based+Question+Answering+Bruno+Souza+Marius+Aasan+Helio+Pedrini+Ad%C3%ADn+Ram%C3%ADrez+Rivera",
    "gs_search_success": true,
    "gs_authors": [
      "kc-wB9QAAAAJ",
      "p2aLoZAAAAAJ",
      "i9mjbNcAAAAJ",
      "MurMuK0AAAAJ"
    ],
    "citation_count": 8,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2405.05216",
    "title": "FinePOSE: Fine-Grained Prompt-Driven 3D Human Pose Estimation via Diffusion Models",
    "year": 2024,
    "published": "2024-05-08T17:09:03Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "The 3D Human Pose Estimation (3D HPE) task uses 2D images or videos to predict human joint coordinates in 3D space. Despite recent advancements in deep learning-based methods, they mostly ignore the capability of coupling accessible texts and naturally feasible knowledge of humans, missing out on valuable implicit supervision to guide the 3D HPE task. Moreover, previous efforts often study this task from the perspective of the whole human body, neglecting fine-grained guidance hidden in differen",
    "arxiv_url": "https://arxiv.org/abs/2405.05216v1",
    "pdf_url": "https://arxiv.org/pdf/2405.05216v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2405.05216",
    "arxiv_authors": [
      "Jinglin Xu",
      "Yijie Guo",
      "Yuxin Peng"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=FinePOSE%3A+Fine-Grained+Prompt-Driven+3D+Human+Pose+Estimation+via+Diffusion+Models+Jinglin+Xu+Yijie+Guo+Yuxin+Peng",
    "gs_search_success": true,
    "gs_authors": [
      "GXf4uv0AAAAJ",
      "mFsXPNYAAAAJ",
      "SVFP5XEAAAAJ"
    ],
    "citation_count": 46,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2405.15274",
    "title": "Talk to Parallel LiDARs: A Human-LiDAR Interaction Method Based on 3D Visual Grounding",
    "year": 2024,
    "published": "2024-05-24T07:00:45Z",
    "categories": [
      "cs.CV",
      "cs.HC"
    ],
    "abstract": "LiDAR sensors play a crucial role in various applications, especially in autonomous driving. Current research primarily focuses on optimizing perceptual models with point cloud data as input, while the exploration of deeper cognitive intelligence remains relatively limited. To address this challenge, parallel LiDARs have emerged as a novel theoretical framework for the next-generation intelligent LiDAR systems, which tightly integrate physical, digital, and social systems. To endow LiDAR systems",
    "arxiv_url": "https://arxiv.org/abs/2405.15274v1",
    "pdf_url": "https://arxiv.org/pdf/2405.15274v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2405.15274",
    "arxiv_authors": [
      "Yuhang Liu",
      "Boyi Sun",
      "Guixu Zheng",
      "Yishuo Wang",
      "Jing Wang",
      "Fei-Yue Wang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Talk+to+Parallel+LiDARs%3A+A+Human-LiDAR+Interaction+Method+Based+on+3D+Visual+Grounding+Yuhang+Liu+Boyi+Sun+Guixu+Zheng+Yishuo+Wang+Jing+Wang",
    "gs_search_success": true,
    "gs_authors": [
      "J0IIyNgAAAAJ",
      "9YITMcoAAAAJ",
      "3TTXGAoAAAAJ",
      "nKjU9K4AAAAJ"
    ],
    "citation_count": 5,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2304.06114",
    "title": "TopTrack: Tracking Objects By Their Top",
    "year": 2023,
    "published": "2023-04-12T19:00:12Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "In recent years, the joint detection-and-tracking paradigm has been a very popular way of tackling the multi-object tracking (MOT) task. Many of the methods following this paradigm use the object center keypoint for detection. However, we argue that the center point is not optimal since it is often not visible in crowded scenarios, which results in many missed detections when the objects are partially occluded. We propose TopTrack, a joint detection-and-tracking method that uses the top of the o",
    "arxiv_url": "https://arxiv.org/abs/2304.06114v1",
    "pdf_url": "https://arxiv.org/pdf/2304.06114v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2304.06114",
    "arxiv_authors": [
      "Jacob Meilleur",
      "Guillaume-Alexandre Bilodeau"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=TopTrack%3A+Tracking+Objects+By+Their+Top+Jacob+Meilleur+Guillaume-Alexandre+Bilodeau",
    "gs_search_success": true,
    "gs_authors": [
      "OU5TZScAAAAJ"
    ],
    "citation_count": 2,
    "gs_author_count": 2
  },
  {
    "arxiv_id": "2305.15781",
    "title": "VanillaKD: Revisit the Power of Vanilla Knowledge Distillation from Small Scale to Large Scale",
    "year": 2023,
    "published": "2023-05-25T06:50:08Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "The tremendous success of large models trained on extensive datasets demonstrates that scale is a key ingredient in achieving superior results. Therefore, the reflection on the rationality of designing knowledge distillation (KD) approaches for limited-capacity architectures solely based on small-scale datasets is now deemed imperative. In this paper, we identify the \\emph{small data pitfall} that presents in previous KD methods, which results in the underestimation of the power of vanilla KD fr",
    "arxiv_url": "https://arxiv.org/abs/2305.15781v1",
    "pdf_url": "https://arxiv.org/pdf/2305.15781v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2305.15781",
    "arxiv_authors": [
      "Zhiwei Hao",
      "Jianyuan Guo",
      "Kai Han",
      "Han Hu",
      "Chang Xu",
      "Yunhe Wang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=VanillaKD%3A+Revisit+the+Power+of+Vanilla+Knowledge+Distillation+from+Small+Scale+to+Large+Scale+Zhiwei+Hao+Jianyuan+Guo+Kai+Han+Han+Hu+Chang+Xu",
    "gs_search_success": true,
    "gs_authors": [
      "N4F_3eoAAAAJ",
      "vThoBVcAAAAJ",
      "UnAbd4gAAAAJ",
      "isizOkYAAAAJ",
      "MwDSTNAAAAAJ"
    ],
    "citation_count": 25,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2405.08813",
    "title": "CinePile: A Long Video Question Answering Dataset and Benchmark",
    "year": 2024,
    "published": "2024-05-14T17:59:02Z",
    "categories": [
      "cs.CV",
      "cs.LG",
      "cs.MM"
    ],
    "abstract": "Current datasets for long-form video understanding often fall short of providing genuine long-form comprehension challenges, as many tasks derived from these datasets can be successfully tackled by analyzing just one or a few random frames from a video. To address this issue, we present a novel dataset and benchmark, CinePile, specifically designed for authentic long-form video understanding. This paper details our innovative approach for creating a question-answer dataset, utilizing advanced LL",
    "arxiv_url": "https://arxiv.org/abs/2405.08813v3",
    "pdf_url": "https://arxiv.org/pdf/2405.08813v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2405.08813",
    "arxiv_authors": [
      "Ruchit Rawal",
      "Khalid Saifullah",
      "Miquel Farré",
      "Ronen Basri",
      "David Jacobs",
      "Gowthami Somepalli",
      "Tom Goldstein"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=CinePile%3A+A+Long+Video+Question+Answering+Dataset+and+Benchmark+Ruchit+Rawal+Khalid+Saifullah+Miquel+Farr%C3%A9+Ronen+Basri+David+Jacobs",
    "gs_search_success": true,
    "gs_authors": [
      "KmSuVtgAAAAJ",
      "WH2KmRgAAAAJ",
      "0_1BRB0AAAAJ",
      "T2ezBDsAAAAJ",
      "d6vuvHIAAAAJ",
      "NNEbBIQAAAAJ",
      "ilDc7ssAAAAJ"
    ],
    "citation_count": 84,
    "gs_author_count": 7
  },
  {
    "arxiv_id": "2501.09826",
    "title": "PIXELS: Progressive Image Xemplar-based Editing with Latent Surgery",
    "year": 2025,
    "published": "2025-01-16T20:26:30Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Recent advancements in language-guided diffusion models for image editing are often bottle-necked by cumbersome prompt engineering to precisely articulate desired changes. An intuitive alternative calls on guidance from in-the-wild image exemplars to help users bring their imagined edits to life. Contemporary exemplar-based editing methods shy away from leveraging the rich latent space learnt by pre-existing large text-to-image (TTI) models and fall back on training with curated objective functi",
    "arxiv_url": "https://arxiv.org/abs/2501.09826v1",
    "pdf_url": "https://arxiv.org/pdf/2501.09826v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2501.09826",
    "arxiv_authors": [
      "Shristi Das Biswas",
      "Matthew Shreve",
      "Xuelu Li",
      "Prateek Singhal",
      "Kaushik Roy"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=PIXELS%3A+Progressive+Image+Xemplar-based+Editing+with+Latent+Surgery+Shristi+Das+Biswas+Matthew+Shreve+Xuelu+Li+Prateek+Singhal+Kaushik+Roy",
    "gs_search_success": true,
    "gs_authors": [
      "sqU8ApgAAAAJ",
      "9PITVmIAAAAJ",
      "to4P8KgAAAAJ",
      "CrjxYc0AAAAJ"
    ],
    "citation_count": 2,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2302.10523",
    "title": "I2V: Towards Texture-Aware Self-Supervised Blind Denoising using Self-Residual Learning for Real-World Images",
    "year": 2023,
    "published": "2023-02-21T08:51:17Z",
    "categories": [
      "cs.CV",
      "eess.IV"
    ],
    "abstract": "Although the advances of self-supervised blind denoising are significantly superior to conventional approaches without clean supervision in synthetic noise scenarios, it shows poor quality in real-world images due to spatially correlated noise corruption. Recently, pixel-shuffle downsampling (PD) has been proposed to eliminate the spatial correlation of noise. A study combining a blind spot network (BSN) and asymmetric PD (AP) successfully demonstrated that self-supervised blind denoising is app",
    "arxiv_url": "https://arxiv.org/abs/2302.10523v1",
    "pdf_url": "https://arxiv.org/pdf/2302.10523v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2302.10523",
    "arxiv_authors": [
      "Kanggeun Lee",
      "Kyungryun Lee",
      "Won-Ki Jeong"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=I2V%3A+Towards+Texture-Aware+Self-Supervised+Blind+Denoising+using+Self-Residual+Learning+for+Real-World+Images+Kanggeun+Lee+Kyungryun+Lee+Won-Ki+Jeong",
    "gs_search_success": true,
    "gs_authors": [
      "bnyKqkwAAAAJ",
      "OvRs1iwAAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2505.05672",
    "title": "TeGA: Texture Space Gaussian Avatars for High-Resolution Dynamic Head Modeling",
    "year": 2025,
    "published": "2025-05-08T22:10:27Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Sparse volumetric reconstruction and rendering via 3D Gaussian splatting have recently enabled animatable 3D head avatars that are rendered under arbitrary viewpoints with impressive photorealism. Today, such photoreal avatars are seen as a key component in emerging applications in telepresence, extended reality, and entertainment. Building a photoreal avatar requires estimating the complex non-rigid motion of different facial components as seen in input video images; due to inaccurate motion es",
    "arxiv_url": "https://arxiv.org/abs/2505.05672v1",
    "pdf_url": "https://arxiv.org/pdf/2505.05672v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2505.05672",
    "arxiv_authors": [
      "Gengyan Li",
      "Paulo Gotardo",
      "Timo Bolkart",
      "Stephan Garbin",
      "Kripasindhu Sarkar",
      "Abhimitra Meka",
      "Alexandros Lattas",
      "Thabo Beeler"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=TeGA%3A+Texture+Space+Gaussian+Avatars+for+High-Resolution+Dynamic+Head+Modeling+Gengyan+Li+Paulo+Gotardo+Timo+Bolkart+Stephan+Garbin+Kripasindhu+Sarkar",
    "gs_search_success": true,
    "gs_authors": [
      "q5y44h8AAAAJ",
      "RbfmHUgAAAAJ",
      "Rcv1-EIAAAAJ",
      "fNteVjwAAAAJ",
      "riuczTYAAAAJ",
      "FWmY0f4AAAAJ",
      "0wJRUlsAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 8
  },
  {
    "arxiv_id": "2308.06945",
    "title": "Semantic-aware Network for Aerial-to-Ground Image Synthesis",
    "year": 2023,
    "published": "2023-08-14T05:37:07Z",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "Aerial-to-ground image synthesis is an emerging and challenging problem that aims to synthesize a ground image from an aerial image. Due to the highly different layout and object representation between the aerial and ground images, existing approaches usually fail to transfer the components of the aerial scene into the ground scene. In this paper, we propose a novel framework to explore the challenges by imposing enhanced structural alignment and semantic awareness. We introduce a novel semantic",
    "arxiv_url": "https://arxiv.org/abs/2308.06945v1",
    "pdf_url": "https://arxiv.org/pdf/2308.06945v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2308.06945",
    "arxiv_authors": [
      "Jinhyun Jang",
      "Taeyong Song",
      "Kwanghoon Sohn"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Semantic-aware+Network+for+Aerial-to-Ground+Image+Synthesis+Jinhyun+Jang+Taeyong+Song+Kwanghoon+Sohn",
    "gs_search_success": true,
    "gs_authors": [
      "VH9dtGsAAAAJ",
      "awzjSPcAAAAJ",
      "zEtk0QsAAAAJ"
    ],
    "citation_count": 2,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2502.16907",
    "title": "MambaFlow: A Novel and Flow-guided State Space Model for Scene Flow Estimation",
    "year": 2025,
    "published": "2025-02-24T07:05:49Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Scene flow estimation aims to predict 3D motion from consecutive point cloud frames, which is of great interest in autonomous driving field. Existing methods face challenges such as insufficient spatio-temporal modeling and inherent loss of fine-grained feature during voxelization. However, the success of Mamba, a representative state space model (SSM) that enables global modeling with linear complexity, provides a promising solution. In this paper, we propose MambaFlow, a novel scene flow estim",
    "arxiv_url": "https://arxiv.org/abs/2502.16907v1",
    "pdf_url": "https://arxiv.org/pdf/2502.16907v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2502.16907",
    "arxiv_authors": [
      "Jiehao Luo",
      "Jintao Cheng",
      "Xiaoyu Tang",
      "Qingwen Zhang",
      "Bohuan Xue",
      "Rui Fan"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=MambaFlow%3A+A+Novel+and+Flow-guided+State+Space+Model+for+Scene+Flow+Estimation+Jiehao+Luo+Jintao+Cheng+Xiaoyu+Tang+Qingwen+Zhang+Bohuan+Xue",
    "gs_search_success": true,
    "gs_authors": [
      "1acIcv4AAAAJ",
      "TRrNPowAAAAJ",
      "3Pcn53kAAAAJ",
      "P5AJTXcAAAAJ"
    ],
    "citation_count": 5,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2503.18406",
    "title": "Instruct-CLIP: Improving Instruction-Guided Image Editing with Automated Data Refinement Using Contrastive Learning",
    "year": 2025,
    "published": "2025-03-24T07:25:44Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Although natural language instructions offer an intuitive way to guide automated image editing, deep-learning models often struggle to achieve high-quality results, largely due to the difficulty of creating large, high-quality training datasets. To do this, previous approaches have typically relied on text-to-image (T2I) generative models to produce pairs of original and edited images that simulate the input/output of an instruction-guided image-editing model. However, these image pairs often fa",
    "arxiv_url": "https://arxiv.org/abs/2503.18406v2",
    "pdf_url": "https://arxiv.org/pdf/2503.18406v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2503.18406",
    "arxiv_authors": [
      "Sherry X. Chen",
      "Misha Sra",
      "Pradeep Sen"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Instruct-CLIP%3A+Improving+Instruction-Guided+Image+Editing+with+Automated+Data+Refinement+Using+Contrastive+Learning+Sherry+X.+Chen+Misha+Sra+Pradeep+Sen",
    "gs_search_success": true,
    "gs_authors": [
      "9_H57VkAAAAJ",
      "yDkV9BsAAAAJ",
      "ijlvzXsAAAAJ"
    ],
    "citation_count": 2,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2308.12462",
    "title": "Overcoming Generic Knowledge Loss with Selective Parameter Update",
    "year": 2023,
    "published": "2023-08-23T22:55:45Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Foundation models encompass an extensive knowledge base and offer remarkable transferability. However, this knowledge becomes outdated or insufficient over time. The challenge lies in continuously updating foundation models to accommodate novel information while retaining their original capabilities. Leveraging the fact that foundation models have initial knowledge on various tasks and domains, we propose a novel approach that, instead of updating all parameters equally, localizes the updates to",
    "arxiv_url": "https://arxiv.org/abs/2308.12462v4",
    "pdf_url": "https://arxiv.org/pdf/2308.12462v4",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2308.12462",
    "arxiv_authors": [
      "Wenxuan Zhang",
      "Paul Janson",
      "Rahaf Aljundi",
      "Mohamed Elhoseiny"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Overcoming+Generic+Knowledge+Loss+with+Selective+Parameter+Update+Wenxuan+Zhang+Paul+Janson+Rahaf+Aljundi+Mohamed+Elhoseiny",
    "gs_search_success": true,
    "gs_authors": [
      "iRBUTOAAAAAJ",
      "wfKn1W0AAAAJ",
      "YLh7yrwAAAAJ",
      "GzBuFCAAAAAJ"
    ],
    "citation_count": 23,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2407.06504",
    "title": "Reprogramming Distillation for Medical Foundation Models",
    "year": 2024,
    "published": "2024-07-09T02:17:51Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Medical foundation models pre-trained on large-scale datasets have demonstrated powerful versatile capabilities for various tasks. However, due to the gap between pre-training tasks (or modalities) and downstream tasks (or modalities), the real-world computation and speed constraints, it might not be straightforward to apply medical foundation models in the downstream scenarios. Previous methods, such as parameter efficient fine-tuning (PEFT) methods and knowledge distillation (KD) methods, are ",
    "arxiv_url": "https://arxiv.org/abs/2407.06504v1",
    "pdf_url": "https://arxiv.org/pdf/2407.06504v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2407.06504",
    "arxiv_authors": [
      "Yuhang Zhou",
      "Siyuan Du",
      "Haolin Li",
      "Jiangchao Yao",
      "Ya Zhang",
      "Yanfeng Wang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Reprogramming+Distillation+for+Medical+Foundation+Models+Yuhang+Zhou+Siyuan+Du+Haolin+Li+Jiangchao+Yao+Ya+Zhang",
    "gs_search_success": true,
    "gs_authors": [
      "bmaf6p4AAAAJ",
      "x_sgJskAAAAJ",
      "9WvLlkIAAAAJ",
      "w8oDh9QAAAAJ",
      "pbjw9sMAAAAJ"
    ],
    "citation_count": 6,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2505.24600",
    "title": "SARD: A Large-Scale Synthetic Arabic OCR Dataset for Book-Style Text Recognition",
    "year": 2025,
    "published": "2025-05-30T13:47:54Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Arabic Optical Character Recognition (OCR) is essential for converting vast amounts of Arabic print media into digital formats. However, training modern OCR models, especially powerful vision-language models, is hampered by the lack of large, diverse, and well-structured datasets that mimic real-world book layouts. Existing Arabic OCR datasets often focus on isolated words or lines or are limited in scale, typographic variety, or structural complexity found in books. To address this significant ",
    "arxiv_url": "https://arxiv.org/abs/2505.24600v1",
    "pdf_url": "https://arxiv.org/pdf/2505.24600v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2505.24600",
    "arxiv_authors": [
      "Omer Nacar",
      "Yasser Al-Habashi",
      "Serry Sibaee",
      "Adel Ammar",
      "Wadii Boulila"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=SARD%3A+A+Large-Scale+Synthetic+Arabic+OCR+Dataset+for+Book-Style+Text+Recognition+Omer+Nacar+Yasser+Al-Habashi+Serry+Sibaee+Adel+Ammar+Wadii+Boulila",
    "gs_search_success": true,
    "gs_authors": [
      "pezf5FYAAAAJ",
      "ubl6WkoAAAAJ",
      "ZSfcQhkAAAAJ",
      "rPfLwd0AAAAJ"
    ],
    "citation_count": 2,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2307.15480",
    "title": "Non-invasive Diabetes Detection using Gabor Filter: A Comparative Analysis of Different Cameras",
    "year": 2023,
    "published": "2023-07-28T11:08:01Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "This paper compares and explores the performance of both mobile device camera and laptop camera as convenient tool for capturing images for non-invasive detection of Diabetes Mellitus (DM) using facial block texture features. Participants within age bracket 20 to 79 years old were chosen for the dataset. 12mp and 7mp mobile cameras, and a laptop camera were used to take the photo under normal lighting condition. Extracted facial blocks were classified using k-Nearest Neighbors (k-NN) and Support",
    "arxiv_url": "https://arxiv.org/abs/2307.15480v1",
    "pdf_url": "https://arxiv.org/pdf/2307.15480v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2307.15480",
    "arxiv_authors": [
      "Christina A. Garcia",
      "Patricia Angela R. Abu",
      "Rosula SJ. Reyes"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Non-invasive+Diabetes+Detection+using+Gabor+Filter%3A+A+Comparative+Analysis+of+Different+Cameras+Christina+A.+Garcia+Patricia+Angela+R.+Abu+Rosula+SJ.+Reyes",
    "gs_search_success": true,
    "gs_authors": [
      "6EUB_l8AAAAJ",
      "z1chGPsAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2308.15942",
    "title": "Stage-by-stage Wavelet Optimization Refinement Diffusion Model for Sparse-View CT Reconstruction",
    "year": 2023,
    "published": "2023-08-30T10:48:53Z",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "abstract": "Diffusion models have emerged as potential tools to tackle the challenge of sparse-view CT reconstruction, displaying superior performance compared to conventional methods. Nevertheless, these prevailing diffusion models predominantly focus on the sinogram or image domains, which can lead to instability during model training, potentially culminating in convergence towards local minimal solutions. The wavelet trans-form serves to disentangle image contents and features into distinct frequency-com",
    "arxiv_url": "https://arxiv.org/abs/2308.15942v2",
    "pdf_url": "https://arxiv.org/pdf/2308.15942v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2308.15942",
    "arxiv_authors": [
      "Kai Xu",
      "Shiyu Lu",
      "Bin Huang",
      "Weiwen Wu",
      "Qiegen Liu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Stage-by-stage+Wavelet+Optimization+Refinement+Diffusion+Model+for+Sparse-View+CT+Reconstruction+Kai+Xu+Shiyu+Lu+Bin+Huang+Weiwen+Wu+Qiegen+Liu",
    "gs_search_success": true,
    "gs_authors": [
      "T00zMvIAAAAJ",
      "T4o77REAAAAJ"
    ],
    "citation_count": 52,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2310.06080",
    "title": "Advancing Diagnostic Precision: Leveraging Machine Learning Techniques for Accurate Detection of Covid-19, Pneumonia, and Tuberculosis in Chest X-Ray Images",
    "year": 2023,
    "published": "2023-10-09T18:38:49Z",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "abstract": "Lung diseases such as COVID-19, tuberculosis (TB), and pneumonia continue to be serious global health concerns that affect millions of people worldwide. In medical practice, chest X-ray examinations have emerged as the norm for diagnosing diseases, particularly chest infections such as COVID-19. Paramedics and scientists are working intensively to create a reliable and precise approach for early-stage COVID-19 diagnosis in order to save lives. But with a variety of symptoms, medical diagnosis of",
    "arxiv_url": "https://arxiv.org/abs/2310.06080v1",
    "pdf_url": "https://arxiv.org/pdf/2310.06080v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2310.06080",
    "arxiv_authors": [
      "Aditya Kulkarni",
      "Guruprasad Parasnis",
      "Harish Balasubramanian",
      "Vansh Jain",
      "Anmol Chokshi",
      "Reena Sonkusare"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Advancing+Diagnostic+Precision%3A+Leveraging+Machine+Learning+Techniques+for+Accurate+Detection+of+Covid-19%2C+Pneumonia%2C+and+Tuberculosis+in+Chest+X-Ray+Images+Aditya+Kulkarni+Guruprasad+Parasnis+Harish+Balasubramanian+Vansh+Jain+Anmol+Chokshi",
    "gs_search_success": true,
    "gs_authors": [
      "4iBhqOkAAAAJ",
      "_vE4mIwAAAAJ",
      "E--wCN0AAAAJ",
      "_h55xlsAAAAJ",
      "G1nzEuIAAAAJ"
    ],
    "citation_count": 7,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2407.03550",
    "title": "CoMix: A Comprehensive Benchmark for Multi-Task Comic Understanding",
    "year": 2024,
    "published": "2024-07-04T00:07:50Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "The comic domain is rapidly advancing with the development of single-page analysis and synthesis models. However, evaluation metrics and datasets lag behind, often limited to small-scale or single-style test sets. We introduce a novel benchmark, CoMix, designed to evaluate the multi-task capabilities of models in comic analysis. Unlike existing benchmarks that focus on isolated tasks such as object detection or text recognition, CoMix addresses a broader range of tasks including object detection",
    "arxiv_url": "https://arxiv.org/abs/2407.03550v2",
    "pdf_url": "https://arxiv.org/pdf/2407.03550v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2407.03550",
    "arxiv_authors": [
      "Emanuele Vivoli",
      "Marco Bertini",
      "Dimosthenis Karatzas"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=CoMix%3A+A+Comprehensive+Benchmark+for+Multi-Task+Comic+Understanding+Emanuele+Vivoli+Marco+Bertini+Dimosthenis+Karatzas",
    "gs_search_success": true,
    "gs_authors": [
      "xASEtrUAAAAJ",
      "BCzPjawAAAAJ",
      "SBm9ZpYAAAAJ"
    ],
    "citation_count": 8,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2305.16465",
    "title": "An AI-Ready Multiplex Staining Dataset for Reproducible and Accurate Characterization of Tumor Immune Microenvironment",
    "year": 2023,
    "published": "2023-05-25T20:42:23Z",
    "categories": [
      "eess.IV",
      "cs.CV",
      "q-bio.QM"
    ],
    "abstract": "We introduce a new AI-ready computational pathology dataset containing restained and co-registered digitized images from eight head-and-neck squamous cell carcinoma patients. Specifically, the same tumor sections were stained with the expensive multiplex immunofluorescence (mIF) assay first and then restained with cheaper multiplex immunohistochemistry (mIHC). This is a first public dataset that demonstrates the equivalence of these two staining methods which in turn allows several use cases; du",
    "arxiv_url": "https://arxiv.org/abs/2305.16465v1",
    "pdf_url": "https://arxiv.org/pdf/2305.16465v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2305.16465",
    "arxiv_authors": [
      "Parmida Ghahremani",
      "Joseph Marino",
      "Juan Hernandez-Prera",
      "Janis V. de la Iglesia",
      "Robbert JC Slebos",
      "Christine H. Chung",
      "Saad Nadeem"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=An+AI-Ready+Multiplex+Staining+Dataset+for+Reproducible+and+Accurate+Characterization+of+Tumor+Immune+Microenvironment+Parmida+Ghahremani+Joseph+Marino+Juan+Hernandez-Prera+Janis+V.+de+la+Iglesia+Robbert+JC+Slebos",
    "gs_search_success": true,
    "gs_authors": [
      "NcEH2A4AAAAJ",
      "OGIlZrMAAAAJ",
      "Wby5oQcAAAAJ"
    ],
    "citation_count": 9,
    "gs_author_count": 7
  },
  {
    "arxiv_id": "2504.13645",
    "title": "Efficient Parameter Adaptation for Multi-Modal Medical Image Segmentation and Prognosis",
    "year": 2025,
    "published": "2025-04-18T11:52:21Z",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "Cancer detection and prognosis relies heavily on medical imaging, particularly CT and PET scans. Deep Neural Networks (DNNs) have shown promise in tumor segmentation by fusing information from these modalities. However, a critical bottleneck exists: the dependency on CT-PET data concurrently for training and inference, posing a challenge due to the limited availability of PET scans. Hence, there is a clear need for a flexible and efficient framework that can be trained with the widely available ",
    "arxiv_url": "https://arxiv.org/abs/2504.13645v1",
    "pdf_url": "https://arxiv.org/pdf/2504.13645v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2504.13645",
    "arxiv_authors": [
      "Numan Saeed",
      "Shahad Hardan",
      "Muhammad Ridzuan",
      "Nada Saadi",
      "Karthik Nandakumar",
      "Mohammad Yaqub"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Efficient+Parameter+Adaptation+for+Multi-Modal+Medical+Image+Segmentation+and+Prognosis+Numan+Saeed+Shahad+Hardan+Muhammad+Ridzuan+Nada+Saadi+Karthik+Nandakumar",
    "gs_search_success": false,
    "gs_authors": [],
    "citation_count": null,
    "error_type": "fetch_failed",
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2502.14061",
    "title": "EfficientPose 6D: Scalable and Efficient 6D Object Pose Estimation",
    "year": 2025,
    "published": "2025-02-19T19:21:23Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "abstract": "In industrial applications requiring real-time feedback, such as quality control and robotic manipulation, the demand for high-speed and accurate pose estimation remains critical. Despite advances improving speed and accuracy in pose estimation, finding a balance between computational efficiency and accuracy poses significant challenges in dynamic environments. Most current algorithms lack scalability in estimation time, especially for diverse datasets, and the state-of-the-art (SOTA) methods ar",
    "arxiv_url": "https://arxiv.org/abs/2502.14061v1",
    "pdf_url": "https://arxiv.org/pdf/2502.14061v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2502.14061",
    "arxiv_authors": [
      "Zixuan Fang",
      "Thomas Pöllabauer",
      "Tristan Wirth",
      "Sarah Berkei",
      "Volker Knauthe",
      "Arjan Kuijper"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=EfficientPose+6D%3A+Scalable+and+Efficient+6D+Object+Pose+Estimation+Zixuan+Fang+Thomas+P%C3%B6llabauer+Tristan+Wirth+Sarah+Berkei+Volker+Knauthe",
    "gs_search_success": true,
    "gs_authors": [
      "wWzp0lgAAAAJ",
      "Fim94JsAAAAJ",
      "QPLXnaoAAAAJ",
      "HL14YPkAAAAJ",
      "yy68pbIAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2402.02431",
    "title": "Learning Mutual Excitation for Hand-to-Hand and Human-to-Human Interaction Recognition",
    "year": 2024,
    "published": "2024-02-04T10:00:00Z",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "Recognizing interactive actions, including hand-to-hand interaction and human-to-human interaction, has attracted increasing attention for various applications in the field of video analysis and human-robot interaction. Considering the success of graph convolution in modeling topology-aware features from skeleton data, recent methods commonly operate graph convolution on separate entities and use late fusion for interactive action recognition, which can barely model the mutual semantic relations",
    "arxiv_url": "https://arxiv.org/abs/2402.02431v2",
    "pdf_url": "https://arxiv.org/pdf/2402.02431v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2402.02431",
    "arxiv_authors": [
      "Mengyuan Liu",
      "Chen Chen",
      "Songtao Wu",
      "Fanyang Meng",
      "Hong Liu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Learning+Mutual+Excitation+for+Hand-to-Hand+and+Human-to-Human+Interaction+Recognition+Mengyuan+Liu+Chen+Chen+Songtao+Wu+Fanyang+Meng+Hong+Liu",
    "gs_search_success": false,
    "gs_authors": [],
    "citation_count": null,
    "error_type": "fetch_failed",
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2412.06016",
    "title": "Track4Gen: Teaching Video Diffusion Models to Track Points Improves Video Generation",
    "year": 2024,
    "published": "2024-12-08T18:21:00Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "abstract": "While recent foundational video generators produce visually rich output, they still struggle with appearance drift, where objects gradually degrade or change inconsistently across frames, breaking visual coherence. We hypothesize that this is because there is no explicit supervision in terms of spatial tracking at the feature level. We propose Track4Gen, a spatially aware video generator that combines video diffusion loss with point tracking across frames, providing enhanced spatial supervision ",
    "arxiv_url": "https://arxiv.org/abs/2412.06016v3",
    "pdf_url": "https://arxiv.org/pdf/2412.06016v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2412.06016",
    "arxiv_authors": [
      "Hyeonho Jeong",
      "Chun-Hao Paul Huang",
      "Jong Chul Ye",
      "Niloy Mitra",
      "Duygu Ceylan"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Track4Gen%3A+Teaching+Video+Diffusion+Models+to+Track+Points+Improves+Video+Generation+Hyeonho+Jeong+Chun-Hao+Paul+Huang+Jong+Chul+Ye+Niloy+Mitra+Duygu+Ceylan",
    "gs_search_success": true,
    "gs_authors": [
      "HNMjoNEAAAAJ",
      "56Kj2QoAAAAJ",
      "LphRgywAAAAJ",
      "Pa7EfsEAAAAJ",
      "dPrZJWMAAAAJ"
    ],
    "citation_count": 19,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2309.02617",
    "title": "Compressing Vision Transformers for Low-Resource Visual Learning",
    "year": 2023,
    "published": "2023-09-05T23:33:39Z",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "Vision transformer (ViT) and its variants have swept through visual learning leaderboards and offer state-of-the-art accuracy in tasks such as image classification, object detection, and semantic segmentation by attending to different parts of the visual input and capturing long-range spatial dependencies. However, these models are large and computation-heavy. For instance, the recently proposed ViT-B model has 86M parameters making it impractical for deployment on resource-constrained devices. ",
    "arxiv_url": "https://arxiv.org/abs/2309.02617v1",
    "pdf_url": "https://arxiv.org/pdf/2309.02617v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2309.02617",
    "arxiv_authors": [
      "Eric Youn",
      "Sai Mitheran J",
      "Sanjana Prabhu",
      "Siyuan Chen"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Compressing+Vision+Transformers+for+Low-Resource+Visual+Learning+Eric+Youn+Sai+Mitheran+J+Sanjana+Prabhu+Siyuan+Chen",
    "gs_search_success": true,
    "gs_authors": [
      "MhusYc8AAAAJ",
      "zlRkEowAAAAJ"
    ],
    "citation_count": 7,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2304.05051",
    "title": "FashionSAP: Symbols and Attributes Prompt for Fine-grained Fashion Vision-Language Pre-training",
    "year": 2023,
    "published": "2023-04-11T08:20:17Z",
    "categories": [
      "cs.CV",
      "cs.CL"
    ],
    "abstract": "Fashion vision-language pre-training models have shown efficacy for a wide range of downstream tasks. However, general vision-language pre-training models pay less attention to fine-grained domain features, while these features are important in distinguishing the specific domain tasks from general tasks. We propose a method for fine-grained fashion vision-language pre-training based on fashion Symbols and Attributes Prompt (FashionSAP) to model fine-grained multi-modalities fashion attributes an",
    "arxiv_url": "https://arxiv.org/abs/2304.05051v1",
    "pdf_url": "https://arxiv.org/pdf/2304.05051v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2304.05051",
    "arxiv_authors": [
      "Yunpeng Han",
      "Lisai Zhang",
      "Qingcai Chen",
      "Zhijian Chen",
      "Zhonghua Li",
      "Jianxin Yang",
      "Zhao Cao"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=FashionSAP%3A+Symbols+and+Attributes+Prompt+for+Fine-grained+Fashion+Vision-Language+Pre-training+Yunpeng+Han+Lisai+Zhang+Qingcai+Chen+Zhijian+Chen+Zhonghua+Li",
    "gs_search_success": true,
    "gs_authors": [
      "aJmTPaoAAAAJ",
      "Q3SVOv4AAAAJ",
      "7aR5D4sAAAAJ",
      "9hXjJMcAAAAJ",
      "L6RXB5AAAAAJ"
    ],
    "citation_count": 30,
    "gs_author_count": 7
  },
  {
    "arxiv_id": "2307.11141",
    "title": "Towards General Game Representations: Decomposing Games Pixels into Content and Style",
    "year": 2023,
    "published": "2023-07-20T17:53:04Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "On-screen game footage contains rich contextual information that players process when playing and experiencing a game. Learning pixel representations of games can benefit artificial intelligence across several downstream tasks including game-playing agents, procedural content generation, and player modelling. The generalizability of these methods, however, remains a challenge, as learned representations should ideally be shared across games with similar game mechanics. This could allow, for inst",
    "arxiv_url": "https://arxiv.org/abs/2307.11141v1",
    "pdf_url": "https://arxiv.org/pdf/2307.11141v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2307.11141",
    "arxiv_authors": [
      "Chintan Trivedi",
      "Konstantinos Makantasis",
      "Antonios Liapis",
      "Georgios N. Yannakakis"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Towards+General+Game+Representations%3A+Decomposing+Games+Pixels+into+Content+and+Style+Chintan+Trivedi+Konstantinos+Makantasis+Antonios+Liapis+Georgios+N.+Yannakakis",
    "gs_search_success": true,
    "gs_authors": [
      "nj4bkJkAAAAJ",
      "GYOp9v8AAAAJ",
      "UVd3SBYAAAAJ",
      "GYdcv54AAAAJ"
    ],
    "citation_count": 4,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2307.14126",
    "title": "Multi-modal Learning with Missing Modality via Shared-Specific Feature Modelling",
    "year": 2023,
    "published": "2023-07-26T11:45:39Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "The missing modality issue is critical but non-trivial to be solved by multi-modal models. Current methods aiming to handle the missing modality problem in multi-modal tasks, either deal with missing modalities only during evaluation or train separate models to handle specific missing modality settings. In addition, these models are designed for specific tasks, so for example, classification models are not easily adapted to segmentation tasks and vice versa. In this paper, we propose the Shared-",
    "arxiv_url": "https://arxiv.org/abs/2307.14126v2",
    "pdf_url": "https://arxiv.org/pdf/2307.14126v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2307.14126",
    "arxiv_authors": [
      "Hu Wang",
      "Yuanhong Chen",
      "Congbo Ma",
      "Jodie Avery",
      "Louise Hull",
      "Gustavo Carneiro"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Multi-modal+Learning+with+Missing+Modality+via+Shared-Specific+Feature+Modelling+Hu+Wang+Yuanhong+Chen+Congbo+Ma+Jodie+Avery+Louise+Hull",
    "gs_search_success": true,
    "gs_authors": [
      "i1ASOXsAAAAJ",
      "K_6dgCgAAAAJ",
      "yssGivYAAAAJ",
      "Qx3QSWgAAAAJ",
      "PiWKAx0AAAAJ",
      "E0TtOWAAAAAJ"
    ],
    "citation_count": 214,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2407.03291",
    "title": "VCHAR:Variance-Driven Complex Human Activity Recognition framework with Generative Representation",
    "year": 2024,
    "published": "2024-07-03T17:24:36Z",
    "categories": [
      "cs.AI",
      "cs.CV",
      "cs.HC",
      "eess.SP"
    ],
    "abstract": "Complex human activity recognition (CHAR) remains a pivotal challenge within ubiquitous computing, especially in the context of smart environments. Existing studies typically require meticulous labeling of both atomic and complex activities, a task that is labor-intensive and prone to errors due to the scarcity and inaccuracies of available datasets. Most prior research has focused on datasets that either precisely label atomic activities or, at minimum, their sequence approaches that are often ",
    "arxiv_url": "https://arxiv.org/abs/2407.03291v2",
    "pdf_url": "https://arxiv.org/pdf/2407.03291v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2407.03291",
    "arxiv_authors": [
      "Yuan Sun",
      "Navid Salami Pargoo",
      "Taqiya Ehsan",
      "Zhao Zhang",
      "Jorge Ortiz"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=VCHAR%3AVariance-Driven+Complex+Human+Activity+Recognition+framework+with+Generative+Representation+Yuan+Sun+Navid+Salami+Pargoo+Taqiya+Ehsan+Zhao+Zhang+Jorge+Ortiz",
    "gs_search_success": true,
    "gs_authors": [
      "rS8vJcEAAAAJ",
      "n4yBc-YAAAAJ",
      "AcyG538AAAAJ"
    ],
    "citation_count": 9,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2303.07872",
    "title": "Object-based SLAM utilizing unambiguous pose parameters considering general symmetry types",
    "year": 2023,
    "published": "2023-03-13T03:07:59Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Existence of symmetric objects, whose observation at different viewpoints can be identical, can deteriorate the performance of simultaneous localization and mapping(SLAM). This work proposes a system for robustly optimizing the pose of cameras and objects even in the presence of symmetric objects. We classify objects into three categories depending on their symmetry characteristics, which is efficient and effective in that it allows to deal with general objects and the objects in the same catego",
    "arxiv_url": "https://arxiv.org/abs/2303.07872v1",
    "pdf_url": "https://arxiv.org/pdf/2303.07872v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2303.07872",
    "arxiv_authors": [
      "Taekbeom Lee",
      "Youngseok Jang",
      "H. Jin Kim"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Object-based+SLAM+utilizing+unambiguous+pose+parameters+considering+general+symmetry+types+Taekbeom+Lee+Youngseok+Jang+H.+Jin+Kim",
    "gs_search_success": true,
    "gs_authors": [
      "SFbP9n0AAAAJ",
      "_3SZKckAAAAJ",
      "7pvJoZoAAAAJ"
    ],
    "citation_count": 5,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2405.18774",
    "title": "LLaMA-Reg: Using LLaMA 2 for Unsupervised Medical Image Registration",
    "year": 2024,
    "published": "2024-05-29T05:26:25Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Medical image registration is an essential topic in medical image analysis. In this paper, we propose a method for medical image registration using a pretrained large language model. We find that using the pretrained large language model to encode deep features of the medical images in the registration model can effectively improve image registration accuracy, indicating the great potential of the large language model in medical image registration tasks. We use dual encoders to perform deep feat",
    "arxiv_url": "https://arxiv.org/abs/2405.18774v1",
    "pdf_url": "https://arxiv.org/pdf/2405.18774v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2405.18774",
    "arxiv_authors": [
      "Mingrui Ma",
      "Yu Yang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=LLaMA-Reg%3A+Using+LLaMA+2+for+Unsupervised+Medical+Image+Registration+Mingrui+Ma+Yu+Yang",
    "gs_search_success": true,
    "gs_authors": [
      "h9SxaDkAAAAJ"
    ],
    "citation_count": 3,
    "gs_author_count": 2
  },
  {
    "arxiv_id": "2409.05636",
    "title": "3D-SAR Tomography and Machine Learning for High-Resolution Tree Height Estimation",
    "year": 2024,
    "published": "2024-09-09T14:07:38Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Accurately estimating forest biomass is crucial for global carbon cycle modelling and climate change mitigation. Tree height, a key factor in biomass calculations, can be measured using Synthetic Aperture Radar (SAR) technology. This study applies machine learning to extract forest height data from two SAR products: Single Look Complex (SLC) images and tomographic cubes, in preparation for the ESA Biomass Satellite mission. We use the TomoSense dataset, containing SAR and LiDAR data from Germany",
    "arxiv_url": "https://arxiv.org/abs/2409.05636v1",
    "pdf_url": "https://arxiv.org/pdf/2409.05636v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2409.05636",
    "arxiv_authors": [
      "Grace Colverd",
      "Jumpei Takami",
      "Laura Schade",
      "Karol Bot",
      "Joseph A. Gallego-Mejia"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=3D-SAR+Tomography+and+Machine+Learning+for+High-Resolution+Tree+Height+Estimation+Grace+Colverd+Jumpei+Takami+Laura+Schade+Karol+Bot+Joseph+A.+Gallego-Mejia",
    "gs_search_success": true,
    "gs_authors": [
      "DS0IfX4AAAAJ",
      "KaR7WucAAAAJ",
      "Kks1bcIAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2412.13180",
    "title": "Feather the Throttle: Revisiting Visual Token Pruning for Vision-Language Model Acceleration",
    "year": 2024,
    "published": "2024-12-17T18:56:50Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Recent works on accelerating Vision-Language Models achieve strong performance across a variety of vision-language tasks despite highly compressing visual information. In this work, we examine the popular acceleration approach of early pruning of visual tokens inside the language model. Surprisingly, we find that while strong performance is maintained across many tasks, it exhibits drastically different behavior for a subset of vision-centric tasks such as localization. Upon further investigatio",
    "arxiv_url": "https://arxiv.org/abs/2412.13180v2",
    "pdf_url": "https://arxiv.org/pdf/2412.13180v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2412.13180",
    "arxiv_authors": [
      "Mark Endo",
      "Xiaohan Wang",
      "Serena Yeung-Levy"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Feather+the+Throttle%3A+Revisiting+Visual+Token+Pruning+for+Vision-Language+Model+Acceleration+Mark+Endo+Xiaohan+Wang+Serena+Yeung-Levy",
    "gs_search_success": true,
    "gs_authors": [
      "DJd9uDcAAAAJ",
      "Tw2m5kUAAAAJ",
      "iGA10XoAAAAJ"
    ],
    "citation_count": 8,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2306.16269",
    "title": "RSPrompter: Learning to Prompt for Remote Sensing Instance Segmentation based on Visual Foundation Model",
    "year": 2023,
    "published": "2023-06-28T14:51:34Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Leveraging the extensive training data from SA-1B, the Segment Anything Model (SAM) demonstrates remarkable generalization and zero-shot capabilities. However, as a category-agnostic instance segmentation method, SAM heavily relies on prior manual guidance, including points, boxes, and coarse-grained masks. Furthermore, its performance in remote sensing image segmentation tasks remains largely unexplored and unproven. In this paper, we aim to develop an automated instance segmentation approach f",
    "arxiv_url": "https://arxiv.org/abs/2306.16269v2",
    "pdf_url": "https://arxiv.org/pdf/2306.16269v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2306.16269",
    "arxiv_authors": [
      "Keyan Chen",
      "Chenyang Liu",
      "Hao Chen",
      "Haotian Zhang",
      "Wenyuan Li",
      "Zhengxia Zou",
      "Zhenwei Shi"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=RSPrompter%3A+Learning+to+Prompt+for+Remote+Sensing+Instance+Segmentation+based+on+Visual+Foundation+Model+Keyan+Chen+Chenyang+Liu+Hao+Chen+Haotian+Zhang+Wenyuan+Li",
    "gs_search_success": false,
    "gs_authors": [],
    "citation_count": null,
    "error_type": "fetch_failed",
    "gs_author_count": 7
  },
  {
    "arxiv_id": "2411.17235",
    "title": "MLI-NeRF: Multi-Light Intrinsic-Aware Neural Radiance Fields",
    "year": 2024,
    "published": "2024-11-26T08:57:38Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Current methods for extracting intrinsic image components, such as reflectance and shading, primarily rely on statistical priors. These methods focus mainly on simple synthetic scenes and isolated objects and struggle to perform well on challenging real-world data. To address this issue, we propose MLI-NeRF, which integrates \\textbf{M}ultiple \\textbf{L}ight information in \\textbf{I}ntrinsic-aware \\textbf{Ne}ural \\textbf{R}adiance \\textbf{F}ields. By leveraging scene information provided by diffe",
    "arxiv_url": "https://arxiv.org/abs/2411.17235v1",
    "pdf_url": "https://arxiv.org/pdf/2411.17235v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2411.17235",
    "arxiv_authors": [
      "Yixiong Yang",
      "Shilin Hu",
      "Haoyu Wu",
      "Ramon Baldrich",
      "Dimitris Samaras",
      "Maria Vanrell"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=MLI-NeRF%3A+Multi-Light+Intrinsic-Aware+Neural+Radiance+Fields+Yixiong+Yang+Shilin+Hu+Haoyu+Wu+Ramon+Baldrich+Dimitris+Samaras",
    "gs_search_success": true,
    "gs_authors": [
      "8AypXfoAAAAJ",
      "LjE0U1IAAAAJ",
      "HGyvslEAAAAJ",
      "IIUoUDQAAAAJ",
      "BxbKTYkAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2410.09575",
    "title": "Reconstructive Visual Instruction Tuning",
    "year": 2024,
    "published": "2024-10-12T15:54:29Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "abstract": "This paper introduces reconstructive visual instruction tuning (ROSS), a family of Large Multimodal Models (LMMs) that exploit vision-centric supervision signals. In contrast to conventional visual instruction tuning approaches that exclusively supervise text outputs, ROSS prompts LMMs to supervise visual outputs via reconstructing input images. By doing so, it capitalizes on the inherent richness and detail present within input images themselves, which are often lost in pure text supervision. H",
    "arxiv_url": "https://arxiv.org/abs/2410.09575v2",
    "pdf_url": "https://arxiv.org/pdf/2410.09575v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2410.09575",
    "arxiv_authors": [
      "Haochen Wang",
      "Anlin Zheng",
      "Yucheng Zhao",
      "Tiancai Wang",
      "Zheng Ge",
      "Xiangyu Zhang",
      "Zhaoxiang Zhang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Reconstructive+Visual+Instruction+Tuning+Haochen+Wang+Anlin+Zheng+Yucheng+Zhao+Tiancai+Wang+Zheng+Ge",
    "gs_search_success": true,
    "gs_authors": [
      "qxWfV6cAAAAJ",
      "yuB-cfoAAAAJ",
      "hJ-VrrIAAAAJ",
      "YI0sRroAAAAJ",
      "oNlpTdcAAAAJ",
      "QWemjjQAAAAJ"
    ],
    "citation_count": 23,
    "gs_author_count": 7
  },
  {
    "arxiv_id": "2309.02702",
    "title": "Gene-induced Multimodal Pre-training for Image-omic Classification",
    "year": 2023,
    "published": "2023-09-06T04:30:15Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Histology analysis of the tumor micro-environment integrated with genomic assays is the gold standard for most cancers in modern medicine. This paper proposes a Gene-induced Multimodal Pre-training (GiMP) framework, which jointly incorporates genomics and Whole Slide Images (WSIs) for classification tasks. Our work aims at dealing with the main challenges of multi-modality image-omic classification w.r.t. (1) the patient-level feature extraction difficulties from gigapixel WSIs and tens of thous",
    "arxiv_url": "https://arxiv.org/abs/2309.02702v1",
    "pdf_url": "https://arxiv.org/pdf/2309.02702v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2309.02702",
    "arxiv_authors": [
      "Ting Jin",
      "Xingran Xie",
      "Renjie Wan",
      "Qingli Li",
      "Yan Wang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Gene-induced+Multimodal+Pre-training+for+Image-omic+Classification+Ting+Jin+Xingran+Xie+Renjie+Wan+Qingli+Li+Yan+Wang",
    "gs_search_success": true,
    "gs_authors": [
      "S8_ES4MAAAAJ",
      "5a1Cmk0AAAAJ"
    ],
    "citation_count": 15,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2501.08286",
    "title": "VINGS-Mono: Visual-Inertial Gaussian Splatting Monocular SLAM in Large Scenes",
    "year": 2025,
    "published": "2025-01-14T18:01:15Z",
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "abstract": "VINGS-Mono is a monocular (inertial) Gaussian Splatting (GS) SLAM framework designed for large scenes. The framework comprises four main components: VIO Front End, 2D Gaussian Map, NVS Loop Closure, and Dynamic Eraser. In the VIO Front End, RGB frames are processed through dense bundle adjustment and uncertainty estimation to extract scene geometry and poses. Based on this output, the mapping module incrementally constructs and maintains a 2D Gaussian map. Key components of the 2D Gaussian Map i",
    "arxiv_url": "https://arxiv.org/abs/2501.08286v1",
    "pdf_url": "https://arxiv.org/pdf/2501.08286v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2501.08286",
    "arxiv_authors": [
      "Ke Wu",
      "Zicheng Zhang",
      "Muer Tie",
      "Ziqing Ai",
      "Zhongxue Gan",
      "Wenchao Ding"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=VINGS-Mono%3A+Visual-Inertial+Gaussian+Splatting+Monocular+SLAM+in+Large+Scenes+Ke+Wu+Zicheng+Zhang+Muer+Tie+Ziqing+Ai+Zhongxue+Gan",
    "gs_search_success": true,
    "gs_authors": [
      "44f1ubYAAAAJ",
      "gJAbHdAAAAAJ"
    ],
    "citation_count": 11,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2310.18653",
    "title": "Feature Guided Masked Autoencoder for Self-supervised Learning in Remote Sensing",
    "year": 2023,
    "published": "2023-10-28T09:43:13Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Self-supervised learning guided by masked image modelling, such as Masked AutoEncoder (MAE), has attracted wide attention for pretraining vision transformers in remote sensing. However, MAE tends to excessively focus on pixel details, thereby limiting the model's capacity for semantic understanding, in particular for noisy SAR images. In this paper, we explore spectral and spatial remote sensing image features as improved MAE-reconstruction targets. We first conduct a study on reconstructing var",
    "arxiv_url": "https://arxiv.org/abs/2310.18653v1",
    "pdf_url": "https://arxiv.org/pdf/2310.18653v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2310.18653",
    "arxiv_authors": [
      "Yi Wang",
      "Hugo Hernández Hernández",
      "Conrad M Albrecht",
      "Xiao Xiang Zhu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Feature+Guided+Masked+Autoencoder+for+Self-supervised+Learning+in+Remote+Sensing+Yi+Wang+Hugo+Hern%C3%A1ndez+Hern%C3%A1ndez+Conrad+M+Albrecht+Xiao+Xiang+Zhu",
    "gs_search_success": true,
    "gs_authors": [
      "CNakdIgAAAAJ",
      "rEADuMEAAAAJ"
    ],
    "citation_count": 44,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2409.10080",
    "title": "DAE-Fuse: An Adaptive Discriminative Autoencoder for Multi-Modality Image Fusion",
    "year": 2024,
    "published": "2024-09-16T08:37:09Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "In extreme scenarios such as nighttime or low-visibility environments, achieving reliable perception is critical for applications like autonomous driving, robotics, and surveillance. Multi-modality image fusion, particularly integrating infrared imaging, offers a robust solution by combining complementary information from different modalities to enhance scene understanding and decision-making. However, current methods face significant limitations: GAN-based approaches often produce blurry images",
    "arxiv_url": "https://arxiv.org/abs/2409.10080v3",
    "pdf_url": "https://arxiv.org/pdf/2409.10080v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2409.10080",
    "arxiv_authors": [
      "Yuchen Guo",
      "Ruoxiang Xu",
      "Rongcheng Li",
      "Weifeng Su"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=DAE-Fuse%3A+An+Adaptive+Discriminative+Autoencoder+for+Multi-Modality+Image+Fusion+Yuchen+Guo+Ruoxiang+Xu+Rongcheng+Li+Weifeng+Su",
    "gs_search_success": true,
    "gs_authors": [
      "PuLs4t4AAAAJ",
      "2wsaCVoAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2307.08316",
    "title": "Bridging the Gap: Multi-Level Cross-Modality Joint Alignment for Visible-Infrared Person Re-Identification",
    "year": 2023,
    "published": "2023-07-17T08:24:05Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Visible-Infrared person Re-IDentification (VI-ReID) is a challenging cross-modality image retrieval task that aims to match pedestrians' images across visible and infrared cameras. To solve the modality gap, existing mainstream methods adopt a learning paradigm converting the image retrieval task into an image classification task with cross-entropy loss and auxiliary metric learning losses. These losses follow the strategy of adjusting the distribution of extracted embeddings to reduce the intra",
    "arxiv_url": "https://arxiv.org/abs/2307.08316v1",
    "pdf_url": "https://arxiv.org/pdf/2307.08316v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2307.08316",
    "arxiv_authors": [
      "Tengfei Liang",
      "Yi Jin",
      "Wu Liu",
      "Tao Wang",
      "Songhe Feng",
      "Yidong Li"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Bridging+the+Gap%3A+Multi-Level+Cross-Modality+Joint+Alignment+for+Visible-Infrared+Person+Re-Identification+Tengfei+Liang+Yi+Jin+Wu+Liu+Tao+Wang+Songhe+Feng",
    "gs_search_success": true,
    "gs_authors": [
      "YE6fPvgAAAAJ",
      "NQAenU0AAAAJ",
      "K5lqMYgAAAAJ",
      "dvjRjHoAAAAJ",
      "rQpizr0AAAAJ"
    ],
    "citation_count": 24,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2406.01078",
    "title": "Unseen Visual Anomaly Generation",
    "year": 2024,
    "published": "2024-06-03T07:58:09Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Visual anomaly detection (AD) presents significant challenges due to the scarcity of anomalous data samples. While numerous works have been proposed to synthesize anomalous samples, these synthetic anomalies often lack authenticity or require extensive training data, limiting their applicability in real-world scenarios. In this work, we propose Anomaly Anything (AnomalyAny), a novel framework that leverages Stable Diffusion (SD)'s image generation capabilities to generate diverse and realistic u",
    "arxiv_url": "https://arxiv.org/abs/2406.01078v4",
    "pdf_url": "https://arxiv.org/pdf/2406.01078v4",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2406.01078",
    "arxiv_authors": [
      "Han Sun",
      "Yunkang Cao",
      "Hao Dong",
      "Olga Fink"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Unseen+Visual+Anomaly+Generation+Han+Sun+Yunkang+Cao+Hao+Dong+Olga+Fink",
    "gs_search_success": false,
    "gs_authors": [],
    "citation_count": null,
    "error_type": "fetch_failed",
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2406.03702",
    "title": "DSNet: A Novel Way to Use Atrous Convolutions in Semantic Segmentation",
    "year": 2024,
    "published": "2024-06-06T02:51:57Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Atrous convolutions are employed as a method to increase the receptive field in semantic segmentation tasks. However, in previous works of semantic segmentation, it was rarely employed in the shallow layers of the model. We revisit the design of atrous convolutions in modern convolutional neural networks (CNNs), and demonstrate that the concept of using large kernels to apply atrous convolutions could be a more powerful paradigm. We propose three guidelines to apply atrous convolutions more effi",
    "arxiv_url": "https://arxiv.org/abs/2406.03702v1",
    "pdf_url": "https://arxiv.org/pdf/2406.03702v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2406.03702",
    "arxiv_authors": [
      "Zilu Guo",
      "Liuyang Bian",
      "Xuan Huang",
      "Hu Wei",
      "Jingyu Li",
      "Huasheng Ni"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=DSNet%3A+A+Novel+Way+to+Use+Atrous+Convolutions+in+Semantic+Segmentation+Zilu+Guo+Liuyang+Bian+Xuan+Huang+Hu+Wei+Jingyu+Li",
    "gs_search_success": true,
    "gs_authors": [
      "tjoeoYQAAAAJ",
      "FqMs9EoAAAAJ"
    ],
    "citation_count": 42,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2311.03240",
    "title": "Machine Learning-Based Tea Leaf Disease Detection: A Comprehensive Review",
    "year": 2023,
    "published": "2023-11-06T16:30:40Z",
    "categories": [
      "cs.CV",
      "cs.LG",
      "eess.IV"
    ],
    "abstract": "Tea leaf diseases are a major challenge to agricultural productivity, with far-reaching implications for yield and quality in the tea industry. The rise of machine learning has enabled the development of innovative approaches to combat these diseases. Early detection and diagnosis are crucial for effective crop management. For predicting tea leaf disease, several automated systems have already been developed using different image processing techniques. This paper delivers a systematic review of ",
    "arxiv_url": "https://arxiv.org/abs/2311.03240v1",
    "pdf_url": "https://arxiv.org/pdf/2311.03240v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2311.03240",
    "arxiv_authors": [
      "Faruk Ahmed",
      "Md. Taimur Ahad",
      "Yousuf Rayhan Emon"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Machine+Learning-Based+Tea+Leaf+Disease+Detection%3A+A+Comprehensive+Review+Faruk+Ahmed+Md.+Taimur+Ahad+Yousuf+Rayhan+Emon",
    "gs_search_success": true,
    "gs_authors": [
      "16_BMKUAAAAJ",
      "PjjriocAAAAJ",
      "8jcz_7IAAAAJ"
    ],
    "citation_count": 22,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2502.12742",
    "title": "3D Shape-to-Image Brownian Bridge Diffusion for Brain MRI Synthesis from Cortical Surfaces",
    "year": 2025,
    "published": "2025-02-18T10:59:04Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Despite recent advances in medical image generation, existing methods struggle to produce anatomically plausible 3D structures. In synthetic brain magnetic resonance images (MRIs), characteristic fissures are often missing, and reconstructed cortical surfaces appear scattered rather than densely convoluted. To address this issue, we introduce Cor2Vox, the first diffusion model-based method that translates continuous cortical shape priors to synthetic brain MRIs. To achieve this, we leverage a Br",
    "arxiv_url": "https://arxiv.org/abs/2502.12742v1",
    "pdf_url": "https://arxiv.org/pdf/2502.12742v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2502.12742",
    "arxiv_authors": [
      "Fabian Bongratz",
      "Yitong Li",
      "Sama Elbaroudy",
      "Christian Wachinger"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=3D+Shape-to-Image+Brownian+Bridge+Diffusion+for+Brain+MRI+Synthesis+from+Cortical+Surfaces+Fabian+Bongratz+Yitong+Li+Sama+Elbaroudy+Christian+Wachinger",
    "gs_search_success": true,
    "gs_authors": [
      "U4y8_g4AAAAJ",
      "8rYJ5qAAAAAJ",
      "UOIBNdUAAAAJ",
      "aQzZhi8AAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2310.02251",
    "title": "Talk2BEV: Language-enhanced Bird's-eye View Maps for Autonomous Driving",
    "year": 2023,
    "published": "2023-10-03T17:53:51Z",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "abstract": "Talk2BEV is a large vision-language model (LVLM) interface for bird's-eye view (BEV) maps in autonomous driving contexts. While existing perception systems for autonomous driving scenarios have largely focused on a pre-defined (closed) set of object categories and driving scenarios, Talk2BEV blends recent advances in general-purpose language and vision models with BEV-structured map representations, eliminating the need for task-specific models. This enables a single system to cater to a variety",
    "arxiv_url": "https://arxiv.org/abs/2310.02251v2",
    "pdf_url": "https://arxiv.org/pdf/2310.02251v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2310.02251",
    "arxiv_authors": [
      "Tushar Choudhary",
      "Vikrant Dewangan",
      "Shivam Chandhok",
      "Shubham Priyadarshan",
      "Anushka Jain",
      "Arun K. Singh",
      "Siddharth Srivastava",
      "Krishna Murthy Jatavallabhula",
      "K. Madhava Krishna"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Talk2BEV%3A+Language-enhanced+Bird%27s-eye+View+Maps+for+Autonomous+Driving+Tushar+Choudhary+Vikrant+Dewangan+Shivam+Chandhok+Shubham+Priyadarshan+Anushka+Jain",
    "gs_search_success": true,
    "gs_authors": [
      "tMpFlPIAAAAJ",
      "Q6H54QEAAAAJ",
      "ZER2BeIAAAAJ",
      "K5jGqSQAAAAJ",
      "0zgDoIEAAAAJ"
    ],
    "citation_count": 110,
    "gs_author_count": 9
  },
  {
    "arxiv_id": "2412.10718",
    "title": "Grid: Omni Visual Generation",
    "year": 2024,
    "published": "2024-12-14T07:22:03Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Visual generation has witnessed remarkable progress in single-image tasks, yet extending these capabilities to temporal sequences remains challenging. Current approaches either build specialized video models from scratch with enormous computational costs or add separate motion modules to image generators, both requiring learning temporal dynamics anew. We observe that modern image generation models possess underutilized potential in handling structured layouts with implicit temporal understandin",
    "arxiv_url": "https://arxiv.org/abs/2412.10718v5",
    "pdf_url": "https://arxiv.org/pdf/2412.10718v5",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2412.10718",
    "arxiv_authors": [
      "Cong Wan",
      "Xiangyang Luo",
      "Hao Luo",
      "Zijian Cai",
      "Yiren Song",
      "Yunlong Zhao",
      "Yifan Bai",
      "Fan Wang",
      "Yuhang He",
      "Yihong Gong"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Grid%3A+Omni+Visual+Generation+Cong+Wan+Xiangyang+Luo+Hao+Luo+Zijian+Cai+Yiren+Song",
    "gs_search_success": true,
    "gs_authors": [
      "L2YS0jgAAAAJ",
      "JdQnaesAAAAJ",
      "9VCIiVcAAAAJ",
      "L-3A3JgAAAAJ",
      "zFi57pEAAAAJ",
      "x2xdU7gAAAAJ"
    ],
    "citation_count": 2,
    "gs_author_count": 9
  },
  {
    "arxiv_id": "2411.14901",
    "title": "ReVisionLLM: Recursive Vision-Language Model for Temporal Grounding in Hour-Long Videos",
    "year": 2024,
    "published": "2024-11-22T12:46:50Z",
    "categories": [
      "cs.CV",
      "cs.CL"
    ],
    "abstract": "Large language models (LLMs) excel at retrieving information from lengthy text, but their vision-language counterparts (VLMs) face difficulties with hour-long videos, especially for temporal grounding. Specifically, these VLMs are constrained by frame limitations, often losing essential temporal details needed for accurate event localization in extended video content. We propose ReVisionLLM, a recursive vision-language model designed to locate events in hour-long videos. Inspired by human search",
    "arxiv_url": "https://arxiv.org/abs/2411.14901v1",
    "pdf_url": "https://arxiv.org/pdf/2411.14901v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2411.14901",
    "arxiv_authors": [
      "Tanveer Hannan",
      "Md Mohaiminul Islam",
      "Jindong Gu",
      "Thomas Seidl",
      "Gedas Bertasius"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=ReVisionLLM%3A+Recursive+Vision-Language+Model+for+Temporal+Grounding+in+Hour-Long+Videos+Tanveer+Hannan+Md+Mohaiminul+Islam+Jindong+Gu+Thomas+Seidl+Gedas+Bertasius",
    "gs_search_success": true,
    "gs_authors": [
      "nBHX6u8AAAAJ",
      "0fOi9KkAAAAJ",
      "VSBucJ4AAAAJ",
      "mj3ff80AAAAJ",
      "8FWkjw8AAAAJ"
    ],
    "citation_count": 6,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2504.06838",
    "title": "ZIP: An Efficient Zeroth-order Prompt Tuning for Black-box Vision-Language Models",
    "year": 2025,
    "published": "2025-04-09T12:56:22Z",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "Recent studies have introduced various approaches for prompt-tuning black-box vision-language models, referred to as black-box prompt-tuning (BBPT). While BBPT has demonstrated considerable potential, it is often found that many existing methods require an excessive number of queries (i.e., function evaluations), which poses a significant challenge in real-world scenarios where the number of allowed queries is limited. To tackle this issue, we propose Zeroth-order Intrinsic-dimensional Prompt-tu",
    "arxiv_url": "https://arxiv.org/abs/2504.06838v1",
    "pdf_url": "https://arxiv.org/pdf/2504.06838v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2504.06838",
    "arxiv_authors": [
      "Seonghwan Park",
      "Jaehyeon Jeong",
      "Yongjun Kim",
      "Jaeho Lee",
      "Namhoon Lee"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=ZIP%3A+An+Efficient+Zeroth-order+Prompt+Tuning+for+Black-box+Vision-Language+Models+Seonghwan+Park+Jaehyeon+Jeong+Yongjun+Kim+Jaeho+Lee+Namhoon+Lee",
    "gs_search_success": true,
    "gs_authors": [
      "wi9q5T8AAAAJ",
      "cBCB-e8AAAAJ",
      "t91zoQMAAAAJ",
      "bVUvWmwAAAAJ"
    ],
    "citation_count": 3,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2407.13341",
    "title": "Hybrid Deep Learning-Based for Enhanced Occlusion Segmentation in PICU Patient Monitoring",
    "year": 2024,
    "published": "2024-07-18T09:37:55Z",
    "categories": [
      "cs.CV",
      "eess.SP"
    ],
    "abstract": "Remote patient monitoring has emerged as a prominent non-invasive method, using digital technologies and computer vision (CV) to replace traditional invasive monitoring. While neonatal and pediatric departments embrace this approach, Pediatric Intensive Care Units (PICUs) face the challenge of occlusions hindering accurate image analysis and interpretation. \\textit{Objective}: In this study, we propose a hybrid approach to effectively segment common occlusions encountered in remote monitoring ap",
    "arxiv_url": "https://arxiv.org/abs/2407.13341v1",
    "pdf_url": "https://arxiv.org/pdf/2407.13341v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2407.13341",
    "arxiv_authors": [
      "Mario Francisco Munoz",
      "Hoang Vu Huy",
      "Thanh-Dung Le"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Hybrid+Deep+Learning-Based+for+Enhanced+Occlusion+Segmentation+in+PICU+Patient+Monitoring+Mario+Francisco+Munoz+Hoang+Vu+Huy+Thanh-Dung+Le",
    "gs_search_success": true,
    "gs_authors": [
      "mSdI7nMAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2403.03539",
    "title": "Gadolinium dose reduction for brain MRI using conditional deep learning",
    "year": 2024,
    "published": "2024-03-06T08:35:29Z",
    "categories": [
      "eess.IV",
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "Recently, deep learning (DL)-based methods have been proposed for the computational reduction of gadolinium-based contrast agents (GBCAs) to mitigate adverse side effects while preserving diagnostic value. Currently, the two main challenges for these approaches are the accurate prediction of contrast enhancement and the synthesis of realistic images. In this work, we address both challenges by utilizing the contrast signal encoded in the subtraction images of pre-contrast and post-contrast image",
    "arxiv_url": "https://arxiv.org/abs/2403.03539v1",
    "pdf_url": "https://arxiv.org/pdf/2403.03539v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2403.03539",
    "arxiv_authors": [
      "Thomas Pinetz",
      "Erich Kobler",
      "Robert Haase",
      "Julian A. Luetkens",
      "Mathias Meetschen",
      "Johannes Haubold",
      "Cornelius Deuschl",
      "Alexander Radbruch",
      "Katerina Deike",
      "Alexander Effland"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Gadolinium+dose+reduction+for+brain+MRI+using+conditional+deep+learning+Thomas+Pinetz+Erich+Kobler+Robert+Haase+Julian+A.+Luetkens+Mathias+Meetschen",
    "gs_search_success": false,
    "gs_authors": [],
    "citation_count": null,
    "error_type": "fetch_failed",
    "gs_author_count": 9
  },
  {
    "arxiv_id": "2503.14523",
    "title": "SDF-TopoNet: A Two-Stage Framework for Tubular Structure Segmentation via SDF Pre-training and Topology-Aware Fine-Tuning",
    "year": 2025,
    "published": "2025-03-14T23:54:38Z",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "abstract": "Accurate segmentation of tubular and curvilinear structures, such as blood vessels, neurons, and road networks, is crucial in various applications. A key challenge is ensuring topological correctness while maintaining computational efficiency. Existing approaches often employ topological loss functions based on persistent homology, such as Betti error, to enforce structural consistency. However, these methods suffer from high computational costs and are insensitive to pixel-level accuracy, often",
    "arxiv_url": "https://arxiv.org/abs/2503.14523v2",
    "pdf_url": "https://arxiv.org/pdf/2503.14523v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2503.14523",
    "arxiv_authors": [
      "Siyi Wu",
      "Leyi Zhao",
      "Haotian Ma",
      "Xinyuan Song"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=SDF-TopoNet%3A+A+Two-Stage+Framework+for+Tubular+Structure+Segmentation+via+SDF+Pre-training+and+Topology-Aware+Fine-Tuning+Siyi+Wu+Leyi+Zhao+Haotian+Ma+Xinyuan+Song",
    "gs_search_success": true,
    "gs_authors": [
      "Dr3wCh4AAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2310.20316",
    "title": "HWD: A Novel Evaluation Score for Styled Handwritten Text Generation",
    "year": 2023,
    "published": "2023-10-31T09:44:27Z",
    "categories": [
      "cs.CV",
      "cs.DL"
    ],
    "abstract": "Styled Handwritten Text Generation (Styled HTG) is an important task in document analysis, aiming to generate text images with the handwriting of given reference images. In recent years, there has been significant progress in the development of deep learning models for tackling this task. Being able to measure the performance of HTG models via a meaningful and representative criterion is key for fostering the development of this research topic. However, despite the current adoption of scores for",
    "arxiv_url": "https://arxiv.org/abs/2310.20316v1",
    "pdf_url": "https://arxiv.org/pdf/2310.20316v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2310.20316",
    "arxiv_authors": [
      "Vittorio Pippi",
      "Fabio Quattrini",
      "Silvia Cascianelli",
      "Rita Cucchiara"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=HWD%3A+A+Novel+Evaluation+Score+for+Styled+Handwritten+Text+Generation+Vittorio+Pippi+Fabio+Quattrini+Silvia+Cascianelli+Rita+Cucchiara",
    "gs_search_success": true,
    "gs_authors": [
      "OHmt2vUAAAAJ",
      "utmt89wAAAAJ",
      "mx7S6J8AAAAJ",
      "OM3sZEoAAAAJ"
    ],
    "citation_count": 22,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2412.07693",
    "title": "Leveraging Content and Context Cues for Low-Light Image Enhancement",
    "year": 2024,
    "published": "2024-12-10T17:32:09Z",
    "categories": [
      "cs.CV",
      "eess.IV"
    ],
    "abstract": "Low-light conditions have an adverse impact on machine cognition, limiting the performance of computer vision systems in real life. Since low-light data is limited and difficult to annotate, we focus on image processing to enhance low-light images and improve the performance of any downstream task model, instead of fine-tuning each of the models which can be prohibitively expensive. We propose to improve the existing zero-reference low-light enhancement by leveraging the CLIP model to capture im",
    "arxiv_url": "https://arxiv.org/abs/2412.07693v1",
    "pdf_url": "https://arxiv.org/pdf/2412.07693v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2412.07693",
    "arxiv_authors": [
      "Igor Morawski",
      "Kai He",
      "Shusil Dangi",
      "Winston H. Hsu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Leveraging+Content+and+Context+Cues+for+Low-Light+Image+Enhancement+Igor+Morawski+Kai+He+Shusil+Dangi+Winston+H.+Hsu",
    "gs_search_success": true,
    "gs_authors": [
      "w0hu2zUAAAAJ",
      "NOvDH3QAAAAJ",
      "QXzeW1kAAAAJ",
      "h12ifugAAAAJ"
    ],
    "citation_count": 4,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2303.15999",
    "title": "Thread Counting in Plain Weave for Old Paintings Using Semi-Supervised Regression Deep Learning Models",
    "year": 2023,
    "published": "2023-03-28T14:15:13Z",
    "categories": [
      "cs.CV",
      "cs.LG",
      "eess.IV"
    ],
    "abstract": "In this work, the authors develop regression approaches based on deep learning to perform thread density estimation for plain weave canvas analysis. Previous approaches were based on Fourier analysis, which is quite robust for some scenarios but fails in some others, in machine learning tools, that involve pre-labeling of the painting at hand, or the segmentation of thread crossing points, that provides good estimations in all scenarios with no need of pre-labeling. The segmentation approach is ",
    "arxiv_url": "https://arxiv.org/abs/2303.15999v3",
    "pdf_url": "https://arxiv.org/pdf/2303.15999v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2303.15999",
    "arxiv_authors": [
      "A. D. Bejarano",
      "Juan J. Murillo-Fuentes",
      "Laura Alba-Carcelen"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Thread+Counting+in+Plain+Weave+for+Old+Paintings+Using+Semi-Supervised+Regression+Deep+Learning+Models+A.+D.+Bejarano+Juan+J.+Murillo-Fuentes+Laura+Alba-Carcelen",
    "gs_search_success": true,
    "gs_authors": [
      "bKyE_TgAAAAJ"
    ],
    "citation_count": 3,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2408.02033",
    "title": "Enhancing Human Action Recognition and Violence Detection Through Deep Learning Audiovisual Fusion",
    "year": 2024,
    "published": "2024-08-04T13:51:18Z",
    "categories": [
      "cs.CV",
      "cs.LG",
      "cs.MM",
      "eess.IV"
    ],
    "abstract": "This paper proposes a hybrid fusion-based deep learning approach based on two different modalities, audio and video, to improve human activity recognition and violence detection in public places. To take advantage of audiovisual fusion, late fusion, intermediate fusion, and hybrid fusion-based deep learning (HFBDL) are used and compared. Since the objective is to detect and recognize human violence in public places, Real-life violence situation (RLVS) dataset is expanded and used. Simulating res",
    "arxiv_url": "https://arxiv.org/abs/2408.02033v1",
    "pdf_url": "https://arxiv.org/pdf/2408.02033v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2408.02033",
    "arxiv_authors": [
      "Pooya Janani",
      "Amirabolfazl Suratgar",
      "Afshin Taghvaeipour"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Enhancing+Human+Action+Recognition+and+Violence+Detection+Through+Deep+Learning+Audiovisual+Fusion+Pooya+Janani+Amirabolfazl+Suratgar+Afshin+Taghvaeipour",
    "gs_search_success": true,
    "gs_authors": [
      "7d0B6M8AAAAJ",
      "2LKy5jgAAAAJ"
    ],
    "citation_count": 6,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2401.11704",
    "title": "EK-Net:Real-time Scene Text Detection with Expand Kernel Distance",
    "year": 2024,
    "published": "2024-01-22T06:05:26Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Recently, scene text detection has received significant attention due to its wide application. However, accurate detection in complex scenes of multiple scales, orientations, and curvature remains a challenge. Numerous detection methods adopt the Vatti clipping (VC) algorithm for multiple-instance training to address the issue of arbitrary-shaped text. Yet we identify several bias results from these approaches called the \"shrinked kernel\". Specifically, it refers to a decrease in accuracy result",
    "arxiv_url": "https://arxiv.org/abs/2401.11704v1",
    "pdf_url": "https://arxiv.org/pdf/2401.11704v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2401.11704",
    "arxiv_authors": [
      "Boyuan Zhu",
      "Fagui Liu",
      "Xi Chen",
      "Quan Tang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=EK-Net%3AReal-time+Scene+Text+Detection+with+Expand+Kernel+Distance+Boyuan+Zhu+Fagui+Liu+Xi+Chen+Quan+Tang",
    "gs_search_success": true,
    "gs_authors": [
      "ElL4N6wAAAAJ"
    ],
    "citation_count": 5,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2408.08332",
    "title": "TurboEdit: Instant text-based image editing",
    "year": 2024,
    "published": "2024-08-14T18:02:24Z",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "We address the challenges of precise image inversion and disentangled image editing in the context of few-step diffusion models. We introduce an encoder based iterative inversion technique. The inversion network is conditioned on the input image and the reconstructed image from the previous step, allowing for correction of the next reconstruction towards the input image. We demonstrate that disentangled controls can be easily achieved in the few-step diffusion model by conditioning on an (automa",
    "arxiv_url": "https://arxiv.org/abs/2408.08332v1",
    "pdf_url": "https://arxiv.org/pdf/2408.08332v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2408.08332",
    "arxiv_authors": [
      "Zongze Wu",
      "Nicholas Kolkin",
      "Jonathan Brandt",
      "Richard Zhang",
      "Eli Shechtman"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=TurboEdit%3A+Instant+text-based+image+editing+Zongze+Wu+Nicholas+Kolkin+Jonathan+Brandt+Richard+Zhang+Eli+Shechtman",
    "gs_search_success": true,
    "gs_authors": [
      "B_FTboQAAAAJ",
      "LW8ze_UAAAAJ",
      "V8FwQGkAAAAJ",
      "MqWYTj0AAAAJ"
    ],
    "citation_count": 26,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2411.14279",
    "title": "Looking Beyond Text: Reducing Language bias in Large Vision-Language Models via Multimodal Dual-Attention and Soft-Image Guidance",
    "year": 2024,
    "published": "2024-11-21T16:33:30Z",
    "categories": [
      "cs.CV",
      "cs.CL"
    ],
    "abstract": "Large vision-language models (LVLMs) have achieved impressive results in various vision-language tasks. However, despite showing promising performance, LVLMs suffer from hallucinations caused by language bias, leading to diminished focus on images and ineffective visual comprehension. We identify two primary reasons for this bias: 1. Different scales of training data between the pretraining stage of LLM and multimodal alignment stage. 2. The learned inference bias due to short-term dependency of",
    "arxiv_url": "https://arxiv.org/abs/2411.14279v1",
    "pdf_url": "https://arxiv.org/pdf/2411.14279v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2411.14279",
    "arxiv_authors": [
      "Haozhe Zhao",
      "Shuzheng Si",
      "Liang Chen",
      "Yichi Zhang",
      "Maosong Sun",
      "Mingjia Zhang",
      "Baobao Chang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Looking+Beyond+Text%3A+Reducing+Language+bias+in+Large+Vision-Language+Models+via+Multimodal+Dual-Attention+and+Soft-Image+Guidance+Haozhe+Zhao+Shuzheng+Si+Liang+Chen+Yichi+Zhang+Maosong+Sun",
    "gs_search_success": true,
    "gs_authors": [
      "lMKPaTYAAAAJ",
      "qcdWjbgAAAAJ",
      "skIXywUAAAAJ",
      "LaKNyhQAAAAJ",
      "zO2XyZUAAAAJ",
      "zIgT0HMAAAAJ"
    ],
    "citation_count": 6,
    "gs_author_count": 7
  },
  {
    "arxiv_id": "2308.10446",
    "title": "LDCSF: Local depth convolution-based Swim framework for classifying multi-label histopathology images",
    "year": 2023,
    "published": "2023-08-21T03:44:54Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Histopathological images are the gold standard for diagnosing liver cancer. However, the accuracy of fully digital diagnosis in computational pathology needs to be improved. In this paper, in order to solve the problem of multi-label and low classification accuracy of histopathology images, we propose a locally deep convolutional Swim framework (LDCSF) to classify multi-label histopathology images. In order to be able to provide local field of view diagnostic results, we propose the LDCSF model,",
    "arxiv_url": "https://arxiv.org/abs/2308.10446v1",
    "pdf_url": "https://arxiv.org/pdf/2308.10446v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2308.10446",
    "arxiv_authors": [
      "Liangrui Pan",
      "Yutao Dou",
      "Zhichao Feng",
      "Liwen Xu",
      "Shaoliang Peng"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=LDCSF%3A+Local+depth+convolution-based+Swim+framework+for+classifying+multi-label+histopathology+images+Liangrui+Pan+Yutao+Dou+Zhichao+Feng+Liwen+Xu+Shaoliang+Peng",
    "gs_search_success": true,
    "gs_authors": [
      "9r36564AAAAJ",
      "-zSEPX0AAAAJ",
      "ouriG7cAAAAJ",
      "Ymn8pYwAAAAJ"
    ],
    "citation_count": 5,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2310.00670",
    "title": "A Hierarchical Graph-based Approach for Recognition and Description Generation of Bimanual Actions in Videos",
    "year": 2023,
    "published": "2023-10-01T13:45:48Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Nuanced understanding and the generation of detailed descriptive content for (bimanual) manipulation actions in videos is important for disciplines such as robotics, human-computer interaction, and video content analysis. This study describes a novel method, integrating graph based modeling with layered hierarchical attention mechanisms, resulting in higher precision and better comprehensiveness of video descriptions. To achieve this, we encode, first, the spatio-temporal inter dependencies betw",
    "arxiv_url": "https://arxiv.org/abs/2310.00670v1",
    "pdf_url": "https://arxiv.org/pdf/2310.00670v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2310.00670",
    "arxiv_authors": [
      "Fatemeh Ziaeetabar",
      "Reza Safabakhsh",
      "Saeedeh Momtazi",
      "Minija Tamosiunaite",
      "Florentin Wörgötter"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=A+Hierarchical+Graph-based+Approach+for+Recognition+and+Description+Generation+of+Bimanual+Actions+in+Videos+Fatemeh+Ziaeetabar+Reza+Safabakhsh+Saeedeh+Momtazi+Minija+Tamosiunaite+Florentin+W%C3%B6rg%C3%B6tter",
    "gs_search_success": true,
    "gs_authors": [
      "QxfijdkAAAAJ",
      "6Y4L5EAAAAAJ"
    ],
    "citation_count": 8,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2403.07339",
    "title": "IM-Unpack: Training and Inference with Arbitrarily Low Precision Integers",
    "year": 2024,
    "published": "2024-03-12T05:44:27Z",
    "categories": [
      "cs.LG",
      "cs.CL",
      "cs.CV"
    ],
    "abstract": "GEneral Matrix Multiply (GEMM) is a central operation in deep learning and corresponds to the largest chunk of the compute footprint. Therefore, improving its efficiency is an active topic of ongoing research. A popular strategy is the use of low bit-width integers to approximate the original entries in a matrix. This allows efficiency gains, but often requires sophisticated techniques to control the rounding error incurred. In this work, we first verify/check that when the low bit-width restric",
    "arxiv_url": "https://arxiv.org/abs/2403.07339v1",
    "pdf_url": "https://arxiv.org/pdf/2403.07339v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2403.07339",
    "arxiv_authors": [
      "Zhanpeng Zeng",
      "Karthikeyan Sankaralingam",
      "Vikas Singh"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=IM-Unpack%3A+Training+and+Inference+with+Arbitrarily+Low+Precision+Integers+Zhanpeng+Zeng+Karthikeyan+Sankaralingam+Vikas+Singh",
    "gs_search_success": true,
    "gs_authors": [
      "O0W9jEQAAAAJ",
      "P9ctuRUAAAAJ",
      "d32BmwcAAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2504.17628",
    "title": "Beyond Labels: Zero-Shot Diabetic Foot Ulcer Wound Segmentation with Self-attention Diffusion Models and the Potential for Text-Guided Customization",
    "year": 2025,
    "published": "2025-04-24T14:50:10Z",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "abstract": "Diabetic foot ulcers (DFUs) pose a significant challenge in healthcare, requiring precise and efficient wound assessment to enhance patient outcomes. This study introduces the Attention Diffusion Zero-shot Unsupervised System (ADZUS), a novel text-guided diffusion model that performs wound segmentation without relying on labeled training data. Unlike conventional deep learning models, which require extensive annotation, ADZUS leverages zero-shot learning to dynamically adapt segmentation based o",
    "arxiv_url": "https://arxiv.org/abs/2504.17628v1",
    "pdf_url": "https://arxiv.org/pdf/2504.17628v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2504.17628",
    "arxiv_authors": [
      "Abderrachid Hamrani",
      "Daniela Leizaola",
      "Renato Sousa",
      "Jose P. Ponce",
      "Stanley Mathis",
      "David G. Armstrong",
      "Anuradha Godavarty"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Beyond+Labels%3A+Zero-Shot+Diabetic+Foot+Ulcer+Wound+Segmentation+with+Self-attention+Diffusion+Models+and+the+Potential+for+Text-Guided+Customization+Abderrachid+Hamrani+Daniela+Leizaola+Renato+Sousa+Jose+P.+Ponce+Stanley+Mathis",
    "gs_search_success": true,
    "gs_authors": [
      "7qcJ5rMAAAAJ",
      "xaxdppQAAAAJ",
      "CakCCY4AAAAJ",
      "kXhLLXIAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 7
  },
  {
    "arxiv_id": "2303.15126",
    "title": "NeuralPCI: Spatio-temporal Neural Field for 3D Point Cloud Multi-frame Non-linear Interpolation",
    "year": 2023,
    "published": "2023-03-27T11:58:52Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "In recent years, there has been a significant increase in focus on the interpolation task of computer vision. Despite the tremendous advancement of video interpolation, point cloud interpolation remains insufficiently explored. Meanwhile, the existence of numerous nonlinear large motions in real-world scenarios makes the point cloud interpolation task more challenging. In light of these issues, we present NeuralPCI: an end-to-end 4D spatio-temporal Neural field for 3D Point Cloud Interpolation, ",
    "arxiv_url": "https://arxiv.org/abs/2303.15126v1",
    "pdf_url": "https://arxiv.org/pdf/2303.15126v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2303.15126",
    "arxiv_authors": [
      "Zehan Zheng",
      "Danni Wu",
      "Ruisi Lu",
      "Fan Lu",
      "Guang Chen",
      "Changjun Jiang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=NeuralPCI%3A+Spatio-temporal+Neural+Field+for+3D+Point+Cloud+Multi-frame+Non-linear+Interpolation+Zehan+Zheng+Danni+Wu+Ruisi+Lu+Fan+Lu+Guang+Chen",
    "gs_search_success": true,
    "gs_authors": [
      "DyEUPFUAAAAJ",
      "Pig6X6MAAAAJ",
      "kBhIyv4AAAAJ"
    ],
    "citation_count": 29,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2311.18675",
    "title": "Cascaded Interaction with Eroded Deep Supervision for Salient Object Detection",
    "year": 2023,
    "published": "2023-11-30T16:20:54Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Deep convolutional neural networks have been widely applied in salient object detection and have achieved remarkable results in this field. However, existing models suffer from information distortion caused by interpolation during up-sampling and down-sampling. In response to this drawback, this article starts from two directions in the network: feature and label. On the one hand, a novel cascaded interaction network with a guidance module named global-local aligned attention (GAA) is designed t",
    "arxiv_url": "https://arxiv.org/abs/2311.18675v1",
    "pdf_url": "https://arxiv.org/pdf/2311.18675v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2311.18675",
    "arxiv_authors": [
      "Hewen Xiao",
      "Jie Mei",
      "Guangfu Ma",
      "Weiren Wu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Cascaded+Interaction+with+Eroded+Deep+Supervision+for+Salient+Object+Detection+Hewen+Xiao+Jie+Mei+Guangfu+Ma+Weiren+Wu",
    "gs_search_success": true,
    "gs_authors": [
      "tyQm5IkAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2504.01955",
    "title": "Scene-Centric Unsupervised Panoptic Segmentation",
    "year": 2025,
    "published": "2025-04-02T17:58:46Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Unsupervised panoptic segmentation aims to partition an image into semantically meaningful regions and distinct object instances without training on manually annotated data. In contrast to prior work on unsupervised panoptic scene understanding, we eliminate the need for object-centric training data, enabling the unsupervised understanding of complex scenes. To that end, we present the first unsupervised panoptic method that directly trains on scene-centric imagery. In particular, we propose an ",
    "arxiv_url": "https://arxiv.org/abs/2504.01955v1",
    "pdf_url": "https://arxiv.org/pdf/2504.01955v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2504.01955",
    "arxiv_authors": [
      "Oliver Hahn",
      "Christoph Reich",
      "Nikita Araslanov",
      "Daniel Cremers",
      "Christian Rupprecht",
      "Stefan Roth"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Scene-Centric+Unsupervised+Panoptic+Segmentation+Oliver+Hahn+Christoph+Reich+Nikita+Araslanov+Daniel+Cremers+Christian+Rupprecht",
    "gs_search_success": true,
    "gs_authors": [
      "RdMFioAAAAAJ",
      "cXQciMEAAAAJ",
      "0OYstzAAAAAJ",
      "IrYlproAAAAJ",
      "0yDoR0AAAAAJ",
      "MFjlVQIAAAAJ"
    ],
    "citation_count": 3,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2505.22046",
    "title": "LatentMove: Towards Complex Human Movement Video Generation",
    "year": 2025,
    "published": "2025-05-28T07:10:49Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Image-to-video (I2V) generation seeks to produce realistic motion sequences from a single reference image. Although recent methods exhibit strong temporal consistency, they often struggle when dealing with complex, non-repetitive human movements, leading to unnatural deformations. To tackle this issue, we present LatentMove, a DiT-based framework specifically tailored for highly dynamic human animation. Our architecture incorporates a conditional control branch and learnable face/body tokens to ",
    "arxiv_url": "https://arxiv.org/abs/2505.22046v2",
    "pdf_url": "https://arxiv.org/pdf/2505.22046v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2505.22046",
    "arxiv_authors": [
      "Ashkan Taghipour",
      "Morteza Ghahremani",
      "Mohammed Bennamoun",
      "Farid Boussaid",
      "Aref Miri Rekavandi",
      "Zinuo Li",
      "Qiuhong Ke",
      "Hamid Laga"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=LatentMove%3A+Towards+Complex+Human+Movement+Video+Generation+Ashkan+Taghipour+Morteza+Ghahremani+Mohammed+Bennamoun+Farid+Boussaid+Aref+Miri+Rekavandi",
    "gs_search_success": true,
    "gs_authors": [
      "yhXUlXsAAAAJ",
      "Qxmqp-0AAAAJ",
      "6cV-GesAAAAJ",
      "84qxdhsAAAAJ",
      "SacY05oAAAAJ",
      "ylX5MEAAAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 8
  },
  {
    "arxiv_id": "2407.07605",
    "title": "Early Explorations of Lightweight Models for Wound Segmentation on Mobile Devices",
    "year": 2024,
    "published": "2024-07-10T12:44:22Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "The aging population poses numerous challenges to healthcare, including the increase in chronic wounds in the elderly. The current approach to wound assessment by therapists based on photographic documentation is subjective, highlighting the need for computer-aided wound recognition from smartphone photos. This offers objective and convenient therapy monitoring, while being accessible to patients from their home at any time. However, despite research in mobile image segmentation, there is a lack",
    "arxiv_url": "https://arxiv.org/abs/2407.07605v3",
    "pdf_url": "https://arxiv.org/pdf/2407.07605v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2407.07605",
    "arxiv_authors": [
      "Vanessa Borst",
      "Timo Dittus",
      "Konstantin Müller",
      "Samuel Kounev"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Early+Explorations+of+Lightweight+Models+for+Wound+Segmentation+on+Mobile+Devices+Vanessa+Borst+Timo+Dittus+Konstantin+M%C3%BCller+Samuel+Kounev",
    "gs_search_success": false,
    "gs_authors": [],
    "citation_count": null,
    "error_type": "fetch_failed",
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2412.05781",
    "title": "Open-Source Acceleration of Stable-Diffusion.cpp Deployable on All Devices",
    "year": 2024,
    "published": "2024-12-08T02:27:17Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "abstract": "Stable diffusion plays a crucial role in generating high-quality images. However, image generation is time-consuming and memory-intensive. To address this, stable-diffusion.cpp (Sdcpp) emerges as an efficient inference framework to accelerate the diffusion models. Although it is lightweight, the current implementation of ggml_conv_2d operator in Sdcpp is suboptimal, exhibiting both high inference latency and massive memory usage. To address this, in this work, we present an optimized version of ",
    "arxiv_url": "https://arxiv.org/abs/2412.05781v3",
    "pdf_url": "https://arxiv.org/pdf/2412.05781v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2412.05781",
    "arxiv_authors": [
      "Jingxu Ng",
      "Cheng Lv",
      "Pu Zhao",
      "Wei Niu",
      "Juyi Lin",
      "Minzhou Pan",
      "Yun Liang",
      "Yanzhi Wang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Open-Source+Acceleration+of+Stable-Diffusion.cpp+Deployable+on+All+Devices+Jingxu+Ng+Cheng+Lv+Pu+Zhao+Wei+Niu+Juyi+Lin",
    "gs_search_success": true,
    "gs_authors": [
      "a7akgIEAAAAJ",
      "w1RoaOMAAAAJ",
      "rWZLnpwAAAAJ",
      "pxpxmkkAAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 8
  },
  {
    "arxiv_id": "2406.09409",
    "title": "CodedEvents: Optimal Point-Spread-Function Engineering for 3D-Tracking with Event Cameras",
    "year": 2024,
    "published": "2024-06-13T17:59:46Z",
    "categories": [
      "cs.CV",
      "eess.IV"
    ],
    "abstract": "Point-spread-function (PSF) engineering is a well-established computational imaging technique that uses phase masks and other optical elements to embed extra information (e.g., depth) into the images captured by conventional CMOS image sensors. To date, however, PSF-engineering has not been applied to neuromorphic event cameras; a powerful new image sensing technology that responds to changes in the log-intensity of light.   This paper establishes theoretical limits (Cramér Rao bounds) on 3D poi",
    "arxiv_url": "https://arxiv.org/abs/2406.09409v1",
    "pdf_url": "https://arxiv.org/pdf/2406.09409v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2406.09409",
    "arxiv_authors": [
      "Sachin Shah",
      "Matthew Albert Chan",
      "Haoming Cai",
      "Jingxi Chen",
      "Sakshum Kulshrestha",
      "Chahat Deep Singh",
      "Yiannis Aloimonos",
      "Christopher Metzler"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=CodedEvents%3A+Optimal+Point-Spread-Function+Engineering+for+3D-Tracking+with+Event+Cameras+Sachin+Shah+Matthew+Albert+Chan+Haoming+Cai+Jingxi+Chen+Sakshum+Kulshrestha",
    "gs_search_success": true,
    "gs_authors": [
      "ASaUMpsAAAAJ",
      "on7GFpYAAAAJ",
      "kWq6VHIAAAAJ",
      "jcfUmocAAAAJ",
      "Jzs37TkAAAAJ",
      "7QmEsOwAAAAJ",
      "Exb9CHIAAAAJ",
      "vPPdjg0AAAAJ"
    ],
    "citation_count": 6,
    "gs_author_count": 8
  },
  {
    "arxiv_id": "2412.07626",
    "title": "OmniDocBench: Benchmarking Diverse PDF Document Parsing with Comprehensive Annotations",
    "year": 2024,
    "published": "2024-12-10T16:05:56Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.IR"
    ],
    "abstract": "Document content extraction is a critical task in computer vision, underpinning the data needs of large language models (LLMs) and retrieval-augmented generation (RAG) systems. Despite recent progress, current document parsing methods have not been fairly and comprehensively evaluated due to the narrow coverage of document types and the simplified, unrealistic evaluation procedures in existing benchmarks. To address these gaps, we introduce OmniDocBench, a novel benchmark featuring high-quality ",
    "arxiv_url": "https://arxiv.org/abs/2412.07626v2",
    "pdf_url": "https://arxiv.org/pdf/2412.07626v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2412.07626",
    "arxiv_authors": [
      "Linke Ouyang",
      "Yuan Qu",
      "Hongbin Zhou",
      "Jiawei Zhu",
      "Rui Zhang",
      "Qunshu Lin",
      "Bin Wang",
      "Zhiyuan Zhao",
      "Man Jiang",
      "Xiaomeng Zhao",
      "Jin Shi",
      "Fan Wu",
      "Pei Chu",
      "Minghao Liu",
      "Zhenxiang Li",
      "Chao Xu",
      "Bo Zhang",
      "Botian Shi",
      "Zhongying Tu",
      "Conghui He"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=OmniDocBench%3A+Benchmarking+Diverse+PDF+Document+Parsing+with+Comprehensive+Annotations+Linke+Ouyang+Yuan+Qu+Hongbin+Zhou+Jiawei+Zhu+Rui+Zhang",
    "gs_search_success": true,
    "gs_authors": [
      "rDaVSiAAAAAJ",
      "UoAEwT0AAAAJ",
      "fb_WgAEAAAAJ",
      "BdakD8YAAAAJ",
      "K0BNv0YAAAAJ",
      "ZeGQMToAAAAJ",
      "WljXYoYAAAAJ"
    ],
    "citation_count": 23,
    "gs_author_count": 9
  },
  {
    "arxiv_id": "2409.00313",
    "title": "Training-Free Sketch-Guided Diffusion with Latent Optimization",
    "year": 2024,
    "published": "2024-08-31T00:44:03Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Based on recent advanced diffusion models, Text-to-image (T2I) generation models have demonstrated their capabilities to generate diverse and high-quality images. However, leveraging their potential for real-world content creation, particularly in providing users with precise control over the image generation result, poses a significant challenge. In this paper, we propose an innovative training-free pipeline that extends existing text-to-image generation models to incorporate a sketch as an add",
    "arxiv_url": "https://arxiv.org/abs/2409.00313v2",
    "pdf_url": "https://arxiv.org/pdf/2409.00313v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2409.00313",
    "arxiv_authors": [
      "Sandra Zhang Ding",
      "Jiafeng Mao",
      "Kiyoharu Aizawa"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Training-Free+Sketch-Guided+Diffusion+with+Latent+Optimization+Sandra+Zhang+Ding+Jiafeng+Mao+Kiyoharu+Aizawa",
    "gs_search_success": true,
    "gs_authors": [
      "CJRhhi0AAAAJ",
      "K6pA2u0AAAAJ"
    ],
    "citation_count": 4,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2502.11859",
    "title": "Defining and Evaluating Visual Language Models' Basic Spatial Abilities: A Perspective from Psychometrics",
    "year": 2025,
    "published": "2025-02-17T14:50:53Z",
    "categories": [
      "cs.CV",
      "cs.CL"
    ],
    "abstract": "The Theory of Multiple Intelligences underscores the hierarchical nature of cognitive capabilities. To advance Spatial Artificial Intelligence, we pioneer a psychometric framework defining five Basic Spatial Abilities (BSAs) in Visual Language Models (VLMs): Spatial Perception, Spatial Relation, Spatial Orientation, Mental Rotation, and Spatial Visualization. Benchmarking 13 mainstream VLMs through nine validated psychometric experiments reveals significant gaps versus humans (average score 24.9",
    "arxiv_url": "https://arxiv.org/abs/2502.11859v2",
    "pdf_url": "https://arxiv.org/pdf/2502.11859v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2502.11859",
    "arxiv_authors": [
      "Wenrui Xu",
      "Dalin Lyu",
      "Weihang Wang",
      "Jie Feng",
      "Chen Gao",
      "Yong Li"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Defining+and+Evaluating+Visual+Language+Models%27+Basic+Spatial+Abilities%3A+A+Perspective+from+Psychometrics+Wenrui+Xu+Dalin+Lyu+Weihang+Wang+Jie+Feng+Chen+Gao",
    "gs_search_success": true,
    "gs_authors": [
      "wrPOVnkAAAAJ",
      "igAQ0ZsAAAAJ",
      "uvLx-GAAAAAJ",
      "Af60_cEAAAAJ",
      "kmgzPeQAAAAJ"
    ],
    "citation_count": 14,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2410.10279",
    "title": "Exploring Semi-Supervised Learning for Online Mapping",
    "year": 2024,
    "published": "2024-10-14T08:31:08Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "The ability to generate online maps using only onboard sensory information is crucial for enabling autonomous driving beyond well-mapped areas. Training models for this task -- predicting lane markers, road edges, and pedestrian crossings -- traditionally require extensive labelled data, which is expensive and labour-intensive to obtain. While semi-supervised learning (SSL) has shown promise in other domains, its potential for online mapping remains largely underexplored. In this work, we bridge",
    "arxiv_url": "https://arxiv.org/abs/2410.10279v2",
    "pdf_url": "https://arxiv.org/pdf/2410.10279v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2410.10279",
    "arxiv_authors": [
      "Adam Lilja",
      "Erik Wallin",
      "Junsheng Fu",
      "Lars Hammarstrand"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Exploring+Semi-Supervised+Learning+for+Online+Mapping+Adam+Lilja+Erik+Wallin+Junsheng+Fu+Lars+Hammarstrand",
    "gs_search_success": false,
    "gs_authors": [],
    "citation_count": null,
    "error_type": "fetch_failed",
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2407.07090",
    "title": "3D Gaussian Ray Tracing: Fast Tracing of Particle Scenes",
    "year": 2024,
    "published": "2024-07-09T17:59:30Z",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "abstract": "Particle-based representations of radiance fields such as 3D Gaussian Splatting have found great success for reconstructing and re-rendering of complex scenes. Most existing methods render particles via rasterization, projecting them to screen space tiles for processing in a sorted order. This work instead considers ray tracing the particles, building a bounding volume hierarchy and casting a ray for each pixel using high-performance GPU ray tracing hardware. To efficiently handle large numbers ",
    "arxiv_url": "https://arxiv.org/abs/2407.07090v3",
    "pdf_url": "https://arxiv.org/pdf/2407.07090v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2407.07090",
    "arxiv_authors": [
      "Nicolas Moenne-Loccoz",
      "Ashkan Mirzaei",
      "Or Perel",
      "Riccardo de Lutio",
      "Janick Martinez Esturo",
      "Gavriel State",
      "Sanja Fidler",
      "Nicholas Sharp",
      "Zan Gojcic"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=3D+Gaussian+Ray+Tracing%3A+Fast+Tracing+of+Particle+Scenes+Nicolas+Moenne-Loccoz+Ashkan+Mirzaei+Or+Perel+Riccardo+de+Lutio+Janick+Martinez+Esturo",
    "gs_search_success": true,
    "gs_authors": [
      "kcZpr18AAAAJ",
      "fBP5RngAAAAJ",
      "z8GwuTgAAAAJ",
      "E2y2UqYAAAAJ",
      "h8jV18QAAAAJ",
      "CUlqK5EAAAAJ"
    ],
    "citation_count": 89,
    "gs_author_count": 9
  },
  {
    "arxiv_id": "2306.14725",
    "title": "Error correcting 2D-3D cascaded network for myocardial infarct scar segmentation on late gadolinium enhancement cardiac magnetic resonance images",
    "year": 2023,
    "published": "2023-06-26T14:21:18Z",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "abstract": "Late gadolinium enhancement (LGE) cardiac magnetic resonance (CMR) imaging is considered the in vivo reference standard for assessing infarct size (IS) and microvascular obstruction (MVO) in ST-elevation myocardial infarction (STEMI) patients. However, the exact quantification of those markers of myocardial infarct severity remains challenging and very time-consuming. As LGE distribution patterns can be quite complex and hard to delineate from the blood pool or epicardial fat, automatic segmenta",
    "arxiv_url": "https://arxiv.org/abs/2306.14725v2",
    "pdf_url": "https://arxiv.org/pdf/2306.14725v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2306.14725",
    "arxiv_authors": [
      "Matthias Schwab",
      "Mathias Pamminger",
      "Christian Kremser",
      "Daniel Obmann",
      "Markus Haltmeier",
      "Agnes Mayr"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Error+correcting+2D-3D+cascaded+network+for+myocardial+infarct+scar+segmentation+on+late+gadolinium+enhancement+cardiac+magnetic+resonance+images+Matthias+Schwab+Mathias+Pamminger+Christian+Kremser+Daniel+Obmann+Markus+Haltmeier",
    "gs_search_success": true,
    "gs_authors": [
      "K-Apf0sAAAAJ",
      "3zuUjawAAAAJ",
      "sR-MDdgAAAAJ"
    ],
    "citation_count": 4,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2503.14979",
    "title": "One-Shot Medical Video Object Segmentation via Temporal Contrastive Memory Networks",
    "year": 2025,
    "published": "2025-03-19T08:17:48Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Video object segmentation is crucial for the efficient analysis of complex medical video data, yet it faces significant challenges in data availability and annotation. We introduce the task of one-shot medical video object segmentation, which requires separating foreground and background pixels throughout a video given only the mask annotation of the first frame. To address this problem, we propose a temporal contrastive memory network comprising image and mask encoders to learn feature represen",
    "arxiv_url": "https://arxiv.org/abs/2503.14979v1",
    "pdf_url": "https://arxiv.org/pdf/2503.14979v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2503.14979",
    "arxiv_authors": [
      "Yaxiong Chen",
      "Junjian Hu",
      "Chunlei Li",
      "Zixuan Zheng",
      "Jingliang Hu",
      "Yilei Shi",
      "Shengwu Xiong",
      "Xiao Xiang Zhu",
      "Lichao Mou"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=One-Shot+Medical+Video+Object+Segmentation+via+Temporal+Contrastive+Memory+Networks+Yaxiong+Chen+Junjian+Hu+Chunlei+Li+Zixuan+Zheng+Jingliang+Hu",
    "gs_search_success": true,
    "gs_authors": [
      "wWqRC4IAAAAJ",
      "CNakdIgAAAAJ",
      "7k8GAaEAAAAJ",
      "4ykAWEcAAAAJ",
      "l0zg1_MAAAAJ",
      "WOSX798AAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 9
  },
  {
    "arxiv_id": "2412.14596",
    "title": "LDP: Generalizing to Multilingual Visual Information Extraction by Language Decoupled Pretraining",
    "year": 2024,
    "published": "2024-12-19T07:31:40Z",
    "categories": [
      "cs.CV",
      "cs.CL",
      "cs.LG"
    ],
    "abstract": "Visual Information Extraction (VIE) plays a crucial role in the comprehension of semi-structured documents, and several pre-trained models have been developed to enhance performance. However, most of these works are monolingual (usually English). Due to the extremely unbalanced quantity and quality of pre-training corpora between English and other languages, few works can extend to non-English scenarios. In this paper, we conduct systematic experiments to show that vision and layout modality hol",
    "arxiv_url": "https://arxiv.org/abs/2412.14596v1",
    "pdf_url": "https://arxiv.org/pdf/2412.14596v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2412.14596",
    "arxiv_authors": [
      "Huawen Shen",
      "Gengluo Li",
      "Jinwen Zhong",
      "Yu Zhou"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=LDP%3A+Generalizing+to+Multilingual+Visual+Information+Extraction+by+Language+Decoupled+Pretraining+Huawen+Shen+Gengluo+Li+Jinwen+Zhong+Yu+Zhou",
    "gs_search_success": true,
    "gs_authors": [
      "3qdGxygAAAAJ",
      "Uroi5mgAAAAJ",
      "FNfBHg8AAAAJ"
    ],
    "citation_count": 7,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2412.09844",
    "title": "Real-time Identity Defenses against Malicious Personalization of Diffusion Models",
    "year": 2024,
    "published": "2024-12-13T04:27:08Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Personalized generative diffusion models, capable of synthesizing highly realistic images based on a few reference portraits, may pose substantial social, ethical, and legal risks via identity replication. Existing defense mechanisms rely on computationally intensive adversarial perturbations tailored to individual images, rendering them impractical for real-world deployment. This study introduces the Real-time Identity Defender (RID), a neural network designed to generate adversarial perturbati",
    "arxiv_url": "https://arxiv.org/abs/2412.09844v2",
    "pdf_url": "https://arxiv.org/pdf/2412.09844v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2412.09844",
    "arxiv_authors": [
      "Hanzhong Guo",
      "Shen Nie",
      "Chao Du",
      "Tianyu Pang",
      "Hao Sun",
      "Chongxuan Li"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Real-time+Identity+Defenses+against+Malicious+Personalization+of+Diffusion+Models+Hanzhong+Guo+Shen+Nie+Chao+Du+Tianyu+Pang+Hao+Sun",
    "gs_search_success": false,
    "gs_authors": [],
    "citation_count": null,
    "error_type": "fetch_failed",
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2311.13128",
    "title": "P2RBox: Point Prompt Oriented Object Detection with SAM",
    "year": 2023,
    "published": "2023-11-22T03:33:00Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Single-point annotation in oriented object detection of remote sensing scenarios is gaining increasing attention due to its cost-effectiveness. However, due to the granularity ambiguity of points, there is a significant performance gap between previous methods and those with fully supervision. In this study, we introduce P2RBox, which employs point prompt to generate rotated box (RBox) annotation for oriented object detection. P2RBox employs the SAM model to generate high-quality mask proposals.",
    "arxiv_url": "https://arxiv.org/abs/2311.13128v2",
    "pdf_url": "https://arxiv.org/pdf/2311.13128v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2311.13128",
    "arxiv_authors": [
      "Guangming Cao",
      "Xuehui Yu",
      "Wenwen Yu",
      "Xumeng Han",
      "Xue Yang",
      "Guorong Li",
      "Jianbin Jiao",
      "Zhenjun Han"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=P2RBox%3A+Point+Prompt+Oriented+Object+Detection+with+SAM+Guangming+Cao+Xuehui+Yu+Wenwen+Yu+Xumeng+Han+Xue+Yang",
    "gs_search_success": true,
    "gs_authors": [
      "XqdpqNcAAAAJ",
      "2xTlvV0AAAAJ",
      "0rK4yTcAAAAJ",
      "WYrxoBEAAAAJ",
      "LWu_FiQAAAAJ"
    ],
    "citation_count": 7,
    "gs_author_count": 8
  },
  {
    "arxiv_id": "2405.11180",
    "title": "GestFormer: Multiscale Wavelet Pooling Transformer Network for Dynamic Hand Gesture Recognition",
    "year": 2024,
    "published": "2024-05-18T05:16:32Z",
    "categories": [
      "cs.CV",
      "cs.HC"
    ],
    "abstract": "Transformer model have achieved state-of-the-art results in many applications like NLP, classification, etc. But their exploration in gesture recognition task is still limited. So, we propose a novel GestFormer architecture for dynamic hand gesture recognition. The motivation behind this design is to propose a resource efficient transformer model, since transformers are computationally expensive and very complex. So, we propose to use a pooling based token mixer named PoolFormer, since it uses o",
    "arxiv_url": "https://arxiv.org/abs/2405.11180v1",
    "pdf_url": "https://arxiv.org/pdf/2405.11180v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2405.11180",
    "arxiv_authors": [
      "Mallika Garg",
      "Debashis Ghosh",
      "Pyari Mohan Pradhan"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=GestFormer%3A+Multiscale+Wavelet+Pooling+Transformer+Network+for+Dynamic+Hand+Gesture+Recognition+Mallika+Garg+Debashis+Ghosh+Pyari+Mohan+Pradhan",
    "gs_search_success": true,
    "gs_authors": [
      "_eIpqasAAAAJ",
      "hlPKfaQAAAAJ"
    ],
    "citation_count": 29,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2412.16717",
    "title": "GANFusion: Feed-Forward Text-to-3D with Diffusion in GAN Space",
    "year": 2024,
    "published": "2024-12-21T17:59:17Z",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "We train a feed-forward text-to-3D diffusion generator for human characters using only single-view 2D data for supervision. Existing 3D generative models cannot yet match the fidelity of image or video generative models. State-of-the-art 3D generators are either trained with explicit 3D supervision and are thus limited by the volume and diversity of existing 3D data. Meanwhile, generators that can be trained with only 2D data as supervision typically produce coarser results, cannot be text-condi",
    "arxiv_url": "https://arxiv.org/abs/2412.16717v1",
    "pdf_url": "https://arxiv.org/pdf/2412.16717v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2412.16717",
    "arxiv_authors": [
      "Souhaib Attaiki",
      "Paul Guerrero",
      "Duygu Ceylan",
      "Niloy J. Mitra",
      "Maks Ovsjanikov"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=GANFusion%3A+Feed-Forward+Text-to-3D+with+Diffusion+in+GAN+Space+Souhaib+Attaiki+Paul+Guerrero+Duygu+Ceylan+Niloy+J.+Mitra+Maks+Ovsjanikov",
    "gs_search_success": true,
    "gs_authors": [
      "0IsSPNEAAAAJ",
      "hNjubvkAAAAJ",
      "56Kj2QoAAAAJ",
      "dPrZJWMAAAAJ",
      "REUg_ToAAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2503.06569",
    "title": "Global-Aware Monocular Semantic Scene Completion with State Space Models",
    "year": 2025,
    "published": "2025-03-09T11:55:40Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Monocular Semantic Scene Completion (MonoSSC) reconstructs and interprets 3D environments from a single image, enabling diverse real-world applications. However, existing methods are often constrained by the local receptive field of Convolutional Neural Networks (CNNs), making it challenging to handle the non-uniform distribution of projected points (Fig. \\ref{fig:perspective}) and effectively reconstruct missing information caused by the 3D-to-2D projection. In this work, we introduce GA-MonoSS",
    "arxiv_url": "https://arxiv.org/abs/2503.06569v1",
    "pdf_url": "https://arxiv.org/pdf/2503.06569v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2503.06569",
    "arxiv_authors": [
      "Shijie Li",
      "Zhongyao Cheng",
      "Rong Li",
      "Shuai Li",
      "Juergen Gall",
      "Xun Xu",
      "Xulei Yang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Global-Aware+Monocular+Semantic+Scene+Completion+with+State+Space+Models+Shijie+Li+Zhongyao+Cheng+Rong+Li+Shuai+Li+Juergen+Gall",
    "gs_search_success": true,
    "gs_authors": [
      "tXkwIK8AAAAJ",
      "1CLaPMEAAAAJ",
      "M68wBgkAAAAJ",
      "pi0SGQUAAAAJ",
      "kl3niOYAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 7
  },
  {
    "arxiv_id": "2403.15032",
    "title": "An Integrated Neighborhood and Scale Information Network for Open-Pit Mine Change Detection in High-Resolution Remote Sensing Images",
    "year": 2024,
    "published": "2024-03-22T08:27:25Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Open-pit mine change detection (CD) in high-resolution (HR) remote sensing images plays a crucial role in mineral development and environmental protection. Significant progress has been made in this field in recent years, largely due to the advancement of deep learning techniques. However, existing deep-learning-based CD methods encounter challenges in effectively integrating neighborhood and scale information, resulting in suboptimal performance. Therefore, by exploring the influence patterns o",
    "arxiv_url": "https://arxiv.org/abs/2403.15032v1",
    "pdf_url": "https://arxiv.org/pdf/2403.15032v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2403.15032",
    "arxiv_authors": [
      "Zilin Xie",
      "Kangning Li",
      "Jinbao Jiang",
      "Jinzhong Yang",
      "Xiaojun Qiao",
      "Deshuai Yuan",
      "Cheng Nie"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=An+Integrated+Neighborhood+and+Scale+Information+Network+for+Open-Pit+Mine+Change+Detection+in+High-Resolution+Remote+Sensing+Images+Zilin+Xie+Kangning+Li+Jinbao+Jiang+Jinzhong+Yang+Xiaojun+Qiao",
    "gs_search_success": true,
    "gs_authors": [
      "3wpLoqMAAAAJ"
    ],
    "citation_count": 2,
    "gs_author_count": 7
  },
  {
    "arxiv_id": "2410.05984",
    "title": "Are Minimal Radial Distortion Solvers Necessary for Relative Pose Estimation?",
    "year": 2024,
    "published": "2024-10-08T12:30:29Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Estimating the relative pose between two cameras is a fundamental step in many applications such as Structure-from-Motion. The common approach to relative pose estimation is to apply a minimal solver inside a RANSAC loop. Highly efficient solvers exist for pinhole cameras. Yet, (nearly) all cameras exhibit radial distortion. Not modeling radial distortion leads to (significantly) worse results. However, minimal radial distortion solvers are significantly more complex than pinhole solvers, both i",
    "arxiv_url": "https://arxiv.org/abs/2410.05984v2",
    "pdf_url": "https://arxiv.org/pdf/2410.05984v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2410.05984",
    "arxiv_authors": [
      "Charalambos Tzamos",
      "Viktor Kocur",
      "Yaqing Ding",
      "Torsten Sattler",
      "Zuzana Kukelova"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Are+Minimal+Radial+Distortion+Solvers+Necessary+for+Relative+Pose+Estimation%3F+Charalambos+Tzamos+Viktor+Kocur+Yaqing+Ding+Torsten+Sattler+Zuzana+Kukelova",
    "gs_search_success": true,
    "gs_authors": [
      "J5-_VNEAAAAJ",
      "9vesUuQAAAAJ",
      "DkrdHFgAAAAJ",
      "M4a3VyYAAAAJ",
      "jzx6_ZIAAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2404.11299",
    "title": "Learning from Unlabelled Data with Transformers: Domain Adaptation for Semantic Segmentation of High Resolution Aerial Images",
    "year": 2024,
    "published": "2024-04-17T12:12:48Z",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "Data from satellites or aerial vehicles are most of the times unlabelled. Annotating such data accurately is difficult, requires expertise, and is costly in terms of time. Even if Earth Observation (EO) data were correctly labelled, labels might change over time. Learning from unlabelled data within a semi-supervised learning framework for segmentation of aerial images is challenging. In this paper, we develop a new model for semantic segmentation of unlabelled images, the Non-annotated Earth Ob",
    "arxiv_url": "https://arxiv.org/abs/2404.11299v1",
    "pdf_url": "https://arxiv.org/pdf/2404.11299v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2404.11299",
    "arxiv_authors": [
      "Nikolaos Dionelis",
      "Francesco Pro",
      "Luca Maiano",
      "Irene Amerini",
      "Bertrand Le Saux"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Learning+from+Unlabelled+Data+with+Transformers%3A+Domain+Adaptation+for+Semantic+Segmentation+of+High+Resolution+Aerial+Images+Nikolaos+Dionelis+Francesco+Pro+Luca+Maiano+Irene+Amerini+Bertrand+Le+Saux",
    "gs_search_success": true,
    "gs_authors": [
      "2UweGHoAAAAJ",
      "SiGd2-YAAAAJ",
      "58SvrEgAAAAJ",
      "4ZDhr6UAAAAJ",
      "FZyBVqkAAAAJ"
    ],
    "citation_count": 5,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2411.17040",
    "title": "Multimodal Alignment and Fusion: A Survey",
    "year": 2024,
    "published": "2024-11-26T02:10:27Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "This survey provides a comprehensive overview of recent advances in multimodal alignment and fusion within the field of machine learning, driven by the increasing availability and diversity of data modalities such as text, images, audio, and video. Unlike previous surveys that often focus on specific modalities or limited fusion strategies, our work presents a structure-centric and method-driven framework that emphasizes generalizable techniques. We systematically categorize and analyze key appr",
    "arxiv_url": "https://arxiv.org/abs/2411.17040v2",
    "pdf_url": "https://arxiv.org/pdf/2411.17040v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2411.17040",
    "arxiv_authors": [
      "Songtao Li",
      "Hao Tang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Multimodal+Alignment+and+Fusion%3A+A+Survey+Songtao+Li+Hao+Tang",
    "gs_search_success": true,
    "gs_authors": [
      "9zJkeEMAAAAJ"
    ],
    "citation_count": 68,
    "gs_author_count": 2
  },
  {
    "arxiv_id": "2410.12673",
    "title": "MambaBEV: An efficient 3D detection model with Mamba2",
    "year": 2024,
    "published": "2024-10-16T15:37:29Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Accurate 3D object detection in autonomous driving relies on Bird's Eye View (BEV) perception and effective temporal fusion.However, existing fusion strategies based on convolutional layers or deformable self attention struggle with global context modeling in BEV space,leading to lower accuracy for large objects. To address this, we introduce MambaBEV, a novel BEV based 3D object detection model that leverages Mamba2, an advanced state space model (SSM) optimized for long sequence processing.Our",
    "arxiv_url": "https://arxiv.org/abs/2410.12673v2",
    "pdf_url": "https://arxiv.org/pdf/2410.12673v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2410.12673",
    "arxiv_authors": [
      "Zihan You",
      "Ni Wang",
      "Hao Wang",
      "Qichao Zhao",
      "Jinxiang Wang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=MambaBEV%3A+An+efficient+3D+detection+model+with+Mamba2+Zihan+You+Ni+Wang+Hao+Wang+Qichao+Zhao+Jinxiang+Wang",
    "gs_search_success": true,
    "gs_authors": [
      "LnvFQJUAAAAJ"
    ],
    "citation_count": 7,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2404.00330",
    "title": "Memory-Scalable and Simplified Functional Map Learning",
    "year": 2024,
    "published": "2024-03-30T12:01:04Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Deep functional maps have emerged in recent years as a prominent learning-based framework for non-rigid shape matching problems. While early methods in this domain only focused on learning in the functional domain, the latest techniques have demonstrated that by promoting consistency between functional and pointwise maps leads to significant improvements in accuracy. Unfortunately, existing approaches rely heavily on the computation of large dense matrices arising from soft pointwise maps, which",
    "arxiv_url": "https://arxiv.org/abs/2404.00330v1",
    "pdf_url": "https://arxiv.org/pdf/2404.00330v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2404.00330",
    "arxiv_authors": [
      "Robin Magnet",
      "Maks Ovsjanikov"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Memory-Scalable+and+Simplified+Functional+Map+Learning+Robin+Magnet+Maks+Ovsjanikov",
    "gs_search_success": true,
    "gs_authors": [
      "0IsSPNEAAAAJ",
      "t4_TpDQAAAAJ"
    ],
    "citation_count": 13,
    "gs_author_count": 2
  },
  {
    "arxiv_id": "2503.00513",
    "title": "Inst3D-LMM: Instance-Aware 3D Scene Understanding with Multi-modal Instruction Tuning",
    "year": 2025,
    "published": "2025-03-01T14:38:42Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Despite encouraging progress in 3D scene understanding, it remains challenging to develop an effective Large Multi-modal Model (LMM) that is capable of understanding and reasoning in complex 3D environments. Most previous methods typically encode 3D point and 2D image features separately, neglecting interactions between 2D semantics and 3D object properties, as well as the spatial relationships within the 3D environment. This limitation not only hinders comprehensive representations of 3D scene,",
    "arxiv_url": "https://arxiv.org/abs/2503.00513v2",
    "pdf_url": "https://arxiv.org/pdf/2503.00513v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2503.00513",
    "arxiv_authors": [
      "Hanxun Yu",
      "Wentong Li",
      "Song Wang",
      "Junbo Chen",
      "Jianke Zhu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Inst3D-LMM%3A+Instance-Aware+3D+Scene+Understanding+with+Multi-modal+Instruction+Tuning+Hanxun+Yu+Wentong+Li+Song+Wang+Junbo+Chen+Jianke+Zhu",
    "gs_search_success": true,
    "gs_authors": [
      "DruMxKYAAAAJ",
      "SC-WmzwAAAAJ",
      "4YOIYGwAAAAJ",
      "Jj0jbL8AAAAJ",
      "MJjM6BcAAAAJ"
    ],
    "citation_count": 16,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2311.08530",
    "title": "SceneScore: Learning a Cost Function for Object Arrangement",
    "year": 2023,
    "published": "2023-11-14T20:55:40Z",
    "categories": [
      "cs.RO",
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "Arranging objects correctly is a key capability for robots which unlocks a wide range of useful tasks. A prerequisite for creating successful arrangements is the ability to evaluate the desirability of a given arrangement. Our method \"SceneScore\" learns a cost function for arrangements, such that desirable, human-like arrangements have a low cost. We learn the distribution of training arrangements offline using an energy-based model, solely from example images without requiring environment inter",
    "arxiv_url": "https://arxiv.org/abs/2311.08530v1",
    "pdf_url": "https://arxiv.org/pdf/2311.08530v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2311.08530",
    "arxiv_authors": [
      "Ivan Kapelyukh",
      "Edward Johns"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=SceneScore%3A+Learning+a+Cost+Function+for+Object+Arrangement+Ivan+Kapelyukh+Edward+Johns",
    "gs_search_success": true,
    "gs_authors": [
      "dHec-LkAAAAJ",
      "DkNQTkoAAAAJ"
    ],
    "citation_count": 5,
    "gs_author_count": 2
  },
  {
    "arxiv_id": "2410.23330",
    "title": "CLIPErase: Efficient Unlearning of Visual-Textual Associations in CLIP",
    "year": 2024,
    "published": "2024-10-30T17:51:31Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "abstract": "Machine unlearning (MU) has gained significant attention as a means to remove specific data from trained models without requiring a full retraining process. While progress has been made in unimodal domains like text and image classification, unlearning in multimodal models remains relatively underexplored. In this work, we address the unique challenges of unlearning in CLIP, a prominent multimodal model that aligns visual and textual representations. We introduce CLIPErase, a novel approach that",
    "arxiv_url": "https://arxiv.org/abs/2410.23330v2",
    "pdf_url": "https://arxiv.org/pdf/2410.23330v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2410.23330",
    "arxiv_authors": [
      "Tianyu Yang",
      "Lisen Dai",
      "Xiangqi Wang",
      "Minhao Cheng",
      "Yapeng Tian",
      "Xiangliang Zhang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=CLIPErase%3A+Efficient+Unlearning+of+Visual-Textual+Associations+in+CLIP+Tianyu+Yang+Lisen+Dai+Xiangqi+Wang+Minhao+Cheng+Yapeng+Tian",
    "gs_search_success": true,
    "gs_authors": [
      "lxCqdpoAAAAJ",
      "7jBSnlgAAAAJ",
      "BhRJe4wAAAAJ",
      "bdEcg6cAAAAJ",
      "_LkC1yoAAAAJ"
    ],
    "citation_count": 15,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2311.05371",
    "title": "Training Robust Deep Physiological Measurement Models with Synthetic Video-based Data",
    "year": 2023,
    "published": "2023-11-09T13:55:45Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Recent advances in supervised deep learning techniques have demonstrated the possibility to remotely measure human physiological vital signs (e.g., photoplethysmograph, heart rate) just from facial videos. However, the performance of these methods heavily relies on the availability and diversity of real labeled data. Yet, collecting large-scale real-world data with high-quality labels is typically challenging and resource intensive, which also raises privacy concerns when storing personal bio-me",
    "arxiv_url": "https://arxiv.org/abs/2311.05371v2",
    "pdf_url": "https://arxiv.org/pdf/2311.05371v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2311.05371",
    "arxiv_authors": [
      "Yuxuan Ou",
      "Yuzhe Zhang",
      "Yuntang Wang",
      "Shwetak Patel",
      "Daniel McDuf",
      "Yuzhe Yang",
      "Xin Liu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Training+Robust+Deep+Physiological+Measurement+Models+with+Synthetic+Video-based+Data+Yuxuan+Ou+Yuzhe+Zhang+Yuntang+Wang+Shwetak+Patel+Daniel+McDuf",
    "gs_search_success": true,
    "gs_authors": [
      "p9F83HoAAAAJ",
      "z4S5rC0AAAAJ",
      "ReUowqIAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 7
  },
  {
    "arxiv_id": "2406.02842",
    "title": "DiffCut: Catalyzing Zero-Shot Semantic Segmentation with Diffusion Features and Recursive Normalized Cut",
    "year": 2024,
    "published": "2024-06-05T01:32:31Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Foundation models have emerged as powerful tools across various domains including language, vision, and multimodal tasks. While prior works have addressed unsupervised image segmentation, they significantly lag behind supervised models. In this paper, we use a diffusion UNet encoder as a foundation vision encoder and introduce DiffCut, an unsupervised zero-shot segmentation method that solely harnesses the output features from the final self-attention block. Through extensive experimentation, we",
    "arxiv_url": "https://arxiv.org/abs/2406.02842v4",
    "pdf_url": "https://arxiv.org/pdf/2406.02842v4",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2406.02842",
    "arxiv_authors": [
      "Paul Couairon",
      "Mustafa Shukor",
      "Jean-Emmanuel Haugeard",
      "Matthieu Cord",
      "Nicolas Thome"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=DiffCut%3A+Catalyzing+Zero-Shot+Semantic+Segmentation+with+Diffusion+Features+and+Recursive+Normalized+Cut+Paul+Couairon+Mustafa+Shukor+Jean-Emmanuel+Haugeard+Matthieu+Cord+Nicolas+Thome",
    "gs_search_success": true,
    "gs_authors": [
      "lhp9mRgAAAAJ",
      "yQRnP7YAAAAJ"
    ],
    "citation_count": 16,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2302.04233",
    "title": "SkyEye: Self-Supervised Bird's-Eye-View Semantic Mapping Using Monocular Frontal View Images",
    "year": 2023,
    "published": "2023-02-08T18:02:09Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.RO"
    ],
    "abstract": "Bird's-Eye-View (BEV) semantic maps have become an essential component of automated driving pipelines due to the rich representation they provide for decision-making tasks. However, existing approaches for generating these maps still follow a fully supervised training paradigm and hence rely on large amounts of annotated BEV data. In this work, we address this limitation by proposing the first self-supervised approach for generating a BEV semantic map using a single monocular image from the fron",
    "arxiv_url": "https://arxiv.org/abs/2302.04233v1",
    "pdf_url": "https://arxiv.org/pdf/2302.04233v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2302.04233",
    "arxiv_authors": [
      "Nikhil Gosala",
      "Kürsat Petek",
      "Paulo L. J. Drews-Jr",
      "Wolfram Burgard",
      "Abhinav Valada"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=SkyEye%3A+Self-Supervised+Bird%27s-Eye-View+Semantic+Mapping+Using+Monocular+Frontal+View+Images+Nikhil+Gosala+K%C3%BCrsat+Petek+Paulo+L.+J.+Drews-Jr+Wolfram+Burgard+Abhinav+Valada",
    "gs_search_success": true,
    "gs_authors": [
      "4ARz8DwAAAAJ",
      "zj6FavAAAAAJ",
      "IeIst7QAAAAJ",
      "96SlLi8AAAAJ",
      "LcARjz0AAAAJ"
    ],
    "citation_count": 45,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2403.18180",
    "title": "Multi-Layer Dense Attention Decoder for Polyp Segmentation",
    "year": 2024,
    "published": "2024-03-27T01:15:05Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Detecting and segmenting polyps is crucial for expediting the diagnosis of colon cancer. This is a challenging task due to the large variations of polyps in color, texture, and lighting conditions, along with subtle differences between the polyp and its surrounding area. Recently, vision Transformers have shown robust abilities in modeling global context for polyp segmentation. However, they face two major limitations: the inability to learn local relations among multi-level layers and inadequat",
    "arxiv_url": "https://arxiv.org/abs/2403.18180v1",
    "pdf_url": "https://arxiv.org/pdf/2403.18180v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2403.18180",
    "arxiv_authors": [
      "Krushi Patel",
      "Fengjun Li",
      "Guanghui Wang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Multi-Layer+Dense+Attention+Decoder+for+Polyp+Segmentation+Krushi+Patel+Fengjun+Li+Guanghui+Wang",
    "gs_search_success": true,
    "gs_authors": [
      "OrkVjT4AAAAJ",
      "7P9MSzAAAAAJ",
      "I_5aoAwAAAAJ"
    ],
    "citation_count": 7,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2412.10604",
    "title": "EvalGIM: A Library for Evaluating Generative Image Models",
    "year": 2024,
    "published": "2024-12-13T23:15:35Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "As the use of text-to-image generative models increases, so does the adoption of automatic benchmarking methods used in their evaluation. However, while metrics and datasets abound, there are few unified benchmarking libraries that provide a framework for performing evaluations across many datasets and metrics. Furthermore, the rapid introduction of increasingly robust benchmarking methods requires that evaluation libraries remain flexible to new datasets and metrics. Finally, there remains a ga",
    "arxiv_url": "https://arxiv.org/abs/2412.10604v2",
    "pdf_url": "https://arxiv.org/pdf/2412.10604v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2412.10604",
    "arxiv_authors": [
      "Melissa Hall",
      "Oscar Mañas",
      "Reyhane Askari-Hemmat",
      "Mark Ibrahim",
      "Candace Ross",
      "Pietro Astolfi",
      "Tariq Berrada Ifriqi",
      "Marton Havasi",
      "Yohann Benchetrit",
      "Karen Ullrich",
      "Carolina Braga",
      "Abhishek Charnalia",
      "Maeve Ryan",
      "Mike Rabbat",
      "Michal Drozdzal",
      "Jakob Verbeek",
      "Adriana Romero-Soriano"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=EvalGIM%3A+A+Library+for+Evaluating+Generative+Image+Models+Melissa+Hall+Oscar+Ma%C3%B1as+Reyhane+Askari-Hemmat+Mark+Ibrahim+Candace+Ross",
    "gs_search_success": false,
    "gs_authors": [],
    "citation_count": null,
    "error_type": "fetch_failed",
    "gs_author_count": 9
  },
  {
    "arxiv_id": "2405.08794",
    "title": "Ambiguous Annotations: When is a Pedestrian not a Pedestrian?",
    "year": 2024,
    "published": "2024-05-14T17:44:34Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Datasets labelled by human annotators are widely used in the training and testing of machine learning models. In recent years, researchers are increasingly paying attention to label quality. However, it is not always possible to objectively determine whether an assigned label is correct or not. The present work investigates this ambiguity in the annotation of autonomous driving datasets as an important dimension of data quality. Our experiments show that excluding highly ambiguous data from the ",
    "arxiv_url": "https://arxiv.org/abs/2405.08794v1",
    "pdf_url": "https://arxiv.org/pdf/2405.08794v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2405.08794",
    "arxiv_authors": [
      "Luisa Schwirten",
      "Jannes Scholz",
      "Daniel Kondermann",
      "Janis Keuper"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Ambiguous+Annotations%3A+When+is+a+Pedestrian+not+a+Pedestrian%3F+Luisa+Schwirten+Jannes+Scholz+Daniel+Kondermann+Janis+Keuper",
    "gs_search_success": true,
    "gs_authors": [
      "4I1TxJcAAAAJ",
      "BUkDvU0AAAAJ"
    ],
    "citation_count": 5,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2403.01993",
    "title": "Physics-Informed Learning for Time-Resolved Angiographic Contrast Agent Concentration Reconstruction",
    "year": 2024,
    "published": "2024-03-04T12:37:52Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Three-dimensional Digital Subtraction Angiography (3D-DSA) is a well-established X-ray-based technique for visualizing vascular anatomy. Recently, four-dimensional DSA (4D-DSA) reconstruction algorithms have been developed to enable the visualization of volumetric contrast flow dynamics through time-series of volumes. . This reconstruction problem is ill-posed mainly due to vessel overlap in the projection direction and geometric vessel foreshortening, which leads to information loss in the reco",
    "arxiv_url": "https://arxiv.org/abs/2403.01993v1",
    "pdf_url": "https://arxiv.org/pdf/2403.01993v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2403.01993",
    "arxiv_authors": [
      "Noah Maul",
      "Annette Birkhold",
      "Fabian Wagner",
      "Mareike Thies",
      "Maximilian Rohleder",
      "Philipp Berg",
      "Markus Kowarschik",
      "Andreas Maier"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Physics-Informed+Learning+for+Time-Resolved+Angiographic+Contrast+Agent+Concentration+Reconstruction+Noah+Maul+Annette+Birkhold+Fabian+Wagner+Mareike+Thies+Maximilian+Rohleder",
    "gs_search_success": true,
    "gs_authors": [
      "MA6SDuEAAAAJ",
      "LLbsf-cAAAAJ",
      "XT9uWWUAAAAJ",
      "aE2KeAEAAAAJ",
      "UQn_lTAAAAAJ",
      "TwDoUfEAAAAJ",
      "utr0qlUAAAAJ",
      "vReFRlIAAAAJ"
    ],
    "citation_count": 4,
    "gs_author_count": 8
  },
  {
    "arxiv_id": "2401.02162",
    "title": "Frequency Domain Nuances Mining for Visible-Infrared Person Re-identification",
    "year": 2024,
    "published": "2024-01-04T09:19:54Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "The key of visible-infrared person re-identification (VIReID) lies in how to minimize the modality discrepancy between visible and infrared images. Existing methods mainly exploit the spatial information while ignoring the discriminative frequency information. To address this issue, this paper aims to reduce the modality discrepancy from the frequency domain perspective. Specifically, we propose a novel Frequency Domain Nuances Mining (FDNM) method to explore the cross-modality frequency domain ",
    "arxiv_url": "https://arxiv.org/abs/2401.02162v2",
    "pdf_url": "https://arxiv.org/pdf/2401.02162v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2401.02162",
    "arxiv_authors": [
      "Yukang Zhang",
      "Yang Lu",
      "Yan Yan",
      "Hanzi Wang",
      "Xuelong Li"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Frequency+Domain+Nuances+Mining+for+Visible-Infrared+Person+Re-identification+Yukang+Zhang+Yang+Lu+Yan+Yan+Hanzi+Wang+Xuelong+Li",
    "gs_search_success": true,
    "gs_authors": [
      "Ma51U80AAAAJ",
      "ahUibskAAAAJ",
      "r7r4FGwAAAAJ",
      "AmJaPdUAAAAJ",
      "g-cFsfkAAAAJ"
    ],
    "citation_count": 19,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2312.10978",
    "title": "Collaborative Learning for Annotation-Efficient Volumetric MR Image Segmentation",
    "year": 2023,
    "published": "2023-12-18T07:02:37Z",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "abstract": "Background: Deep learning has presented great potential in accurate MR image segmentation when enough labeled data are provided for network optimization. However, manually annotating 3D MR images is tedious and time-consuming, requiring experts with rich domain knowledge and experience. Purpose: To build a deep learning method exploring sparse annotations, namely only a single 2D slice label for each 3D training MR image. Population: 3D MR images of 150 subjects from two publicly available datas",
    "arxiv_url": "https://arxiv.org/abs/2312.10978v1",
    "pdf_url": "https://arxiv.org/pdf/2312.10978v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2312.10978",
    "arxiv_authors": [
      "Yousuf Babiker M. Osman",
      "Cheng Li",
      "Weijian Huang",
      "Shanshan Wang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Collaborative+Learning+for+Annotation-Efficient+Volumetric+MR+Image+Segmentation+Yousuf+Babiker+M.+Osman+Cheng+Li+Weijian+Huang+Shanshan+Wang",
    "gs_search_success": true,
    "gs_authors": [
      "-dT6ungAAAAJ",
      "8pnz5L4AAAAJ",
      "z6UNKJUAAAAJ",
      "dXgK6A4AAAAJ"
    ],
    "citation_count": 6,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2503.18016",
    "title": "Retrieval Augmented Generation and Understanding in Vision: A Survey and New Outlook",
    "year": 2025,
    "published": "2025-03-23T10:33:28Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Retrieval-augmented generation (RAG) has emerged as a pivotal technique in artificial intelligence (AI), particularly in enhancing the capabilities of large language models (LLMs) by enabling access to external, reliable, and up-to-date knowledge sources. In the context of AI-Generated Content (AIGC), RAG has proven invaluable by augmenting model outputs with supplementary, relevant information, thus improving their quality. Recently, the potential of RAG has extended beyond natural language pro",
    "arxiv_url": "https://arxiv.org/abs/2503.18016v1",
    "pdf_url": "https://arxiv.org/pdf/2503.18016v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2503.18016",
    "arxiv_authors": [
      "Xu Zheng",
      "Ziqiao Weng",
      "Yuanhuiyi Lyu",
      "Lutao Jiang",
      "Haiwei Xue",
      "Bin Ren",
      "Danda Paudel",
      "Nicu Sebe",
      "Luc Van Gool",
      "Xuming Hu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Retrieval+Augmented+Generation+and+Understanding+in+Vision%3A+A+Survey+and+New+Outlook+Xu+Zheng+Ziqiao+Weng+Yuanhuiyi+Lyu+Lutao+Jiang+Haiwei+Xue",
    "gs_search_success": true,
    "gs_authors": [
      "uTYYRS8AAAAJ",
      "Ii1c51QAAAAJ",
      "lwyU9_IAAAAJ",
      "TwMib_QAAAAJ",
      "6_lvla4AAAAJ",
      "stFCYOAAAAAJ",
      "Md9maLYAAAAJ",
      "dbBKbXoAAAAJ",
      "W43pvPkAAAAJ"
    ],
    "citation_count": 29,
    "gs_author_count": 9
  },
  {
    "arxiv_id": "2503.14562",
    "title": "Analysis of human visual field information using machine learning methods and assessment of their accuracy",
    "year": 2025,
    "published": "2025-03-18T07:39:41Z",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "abstract": "Subject of research: is the study of methods for analyzing perimetric images for the diagnosis and control of glaucoma diseases. Objects of research: is a dataset collected on the ophthalmological perimeter with the results of various patient pathologies, since the ophthalmological community is acutely aware of the issue of disease control and import substitution. [5]. Purpose of research: is to consider various machine learning methods that can classify glaucoma. This is possible thanks to the ",
    "arxiv_url": "https://arxiv.org/abs/2503.14562v1",
    "pdf_url": "https://arxiv.org/pdf/2503.14562v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2503.14562",
    "arxiv_authors": [
      "A. I. Medvedeva",
      "V. V. Bakutkin"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Analysis+of+human+visual+field+information+using+machine+learning+methods+and+assessment+of+their+accuracy+A.+I.+Medvedeva+V.+V.+Bakutkin",
    "gs_search_success": true,
    "gs_authors": [],
    "citation_count": null,
    "gs_author_count": 2
  },
  {
    "arxiv_id": "2504.00870",
    "title": "Data-free Knowledge Distillation with Diffusion Models",
    "year": 2025,
    "published": "2025-04-01T15:00:33Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Recently Data-Free Knowledge Distillation (DFKD) has garnered attention and can transfer knowledge from a teacher neural network to a student neural network without requiring any access to training data. Although diffusion models are adept at synthesizing high-fidelity photorealistic images across various domains, existing methods cannot be easiliy implemented to DFKD. To bridge that gap, this paper proposes a novel approach based on diffusion models, DiffDFKD. Specifically, DiffDFKD involves ta",
    "arxiv_url": "https://arxiv.org/abs/2504.00870v1",
    "pdf_url": "https://arxiv.org/pdf/2504.00870v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2504.00870",
    "arxiv_authors": [
      "Xiaohua Qi",
      "Renda Li",
      "Long Peng",
      "Qiang Ling",
      "Jun Yu",
      "Ziyi Chen",
      "Peng Chang",
      "Mei Han",
      "Jing Xiao"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Data-free+Knowledge+Distillation+with+Diffusion+Models+Xiaohua+Qi+Renda+Li+Long+Peng+Qiang+Ling+Jun+Yu",
    "gs_search_success": true,
    "gs_authors": [
      "7YO-oQoAAAAJ",
      "efZyqyQAAAAJ",
      "feoBnmkAAAAJ",
      "4yMS3J8AAAAJ",
      "ibsno48AAAAJ"
    ],
    "citation_count": 6,
    "gs_author_count": 9
  },
  {
    "arxiv_id": "2401.14510",
    "title": "RPNR: Robust-Perception Neural Reshading",
    "year": 2024,
    "published": "2024-01-25T21:06:23Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Augmented Reality (AR) applications necessitates methods of inserting needed objects into scenes captured by cameras in a way that is coherent with the surroundings. Common AR applications require the insertion of predefined 3D objects with known properties and shape. This simplifies the problem since it is reduced to extracting an illumination model for the object in that scene by understanding the surrounding light sources. However, it is often not the case that we have information about the p",
    "arxiv_url": "https://arxiv.org/abs/2401.14510v1",
    "pdf_url": "https://arxiv.org/pdf/2401.14510v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2401.14510",
    "arxiv_authors": [
      "Fouad Afiouni",
      "Mohamad Fakih",
      "Joey Sleiman"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=RPNR%3A+Robust-Perception+Neural+Reshading+Fouad+Afiouni+Mohamad+Fakih+Joey+Sleiman",
    "gs_search_success": true,
    "gs_authors": [
      "dJbjUNwAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2309.13303",
    "title": "C$^2$VAE: Gaussian Copula-based VAE Differing Disentangled from Coupled Representations with Contrastive Posterior",
    "year": 2023,
    "published": "2023-09-23T08:33:48Z",
    "categories": [
      "cs.LG",
      "cs.CV",
      "stat.ML"
    ],
    "abstract": "We present a self-supervised variational autoencoder (VAE) to jointly learn disentangled and dependent hidden factors and then enhance disentangled representation learning by a self-supervised classifier to eliminate coupled representations in a contrastive manner. To this end, a Contrastive Copula VAE (C$^2$VAE) is introduced without relying on prior knowledge about data in the probabilistic principle and involving strong modeling assumptions on the posterior in the neural architecture. C$^2$VA",
    "arxiv_url": "https://arxiv.org/abs/2309.13303v1",
    "pdf_url": "https://arxiv.org/pdf/2309.13303v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2309.13303",
    "arxiv_authors": [
      "Zhangkai Wu",
      "Longbing Cao"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=C%24%5E2%24VAE%3A+Gaussian+Copula-based+VAE+Differing+Disentangled+from+Coupled+Representations+with+Contrastive+Posterior+Zhangkai+Wu+Longbing+Cao",
    "gs_search_success": true,
    "gs_authors": [
      "543HJzoAAAAJ",
      "cDs3DM8AAAAJ"
    ],
    "citation_count": 4,
    "gs_author_count": 2
  },
  {
    "arxiv_id": "2408.07393",
    "title": "Segment Using Just One Example",
    "year": 2024,
    "published": "2024-08-14T09:13:06Z",
    "categories": [
      "cs.CV",
      "eess.IV"
    ],
    "abstract": "Semantic segmentation is an important topic in computer vision with many relevant application in Earth observation. While supervised methods exist, the constraints of limited annotated data has encouraged development of unsupervised approaches. However, existing unsupervised methods resemble clustering and cannot be directly mapped to explicit target classes. In this paper, we deal with single shot semantic segmentation, where one example for the target class is provided, which is used to segmen",
    "arxiv_url": "https://arxiv.org/abs/2408.07393v1",
    "pdf_url": "https://arxiv.org/pdf/2408.07393v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2408.07393",
    "arxiv_authors": [
      "Pratik Vora",
      "Sudipan Saha"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Segment+Using+Just+One+Example+Pratik+Vora+Sudipan+Saha",
    "gs_search_success": true,
    "gs_authors": [
      "CTqHgl4AAAAJ",
      "C1_Ukv4AAAAJ"
    ],
    "citation_count": 2,
    "gs_author_count": 2
  },
  {
    "arxiv_id": "2412.12740",
    "title": "Open-World Panoptic Segmentation",
    "year": 2024,
    "published": "2024-12-17T10:03:39Z",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "abstract": "Perception is a key building block of autonomously acting vision systems such as autonomous vehicles. It is crucial that these systems are able to understand their surroundings in order to operate safely and robustly. Additionally, autonomous systems deployed in unconstrained real-world scenarios must be able of dealing with novel situations and object that have never been seen before. In this article, we tackle the problem of open-world panoptic segmentation, i.e., the task of discovering new s",
    "arxiv_url": "https://arxiv.org/abs/2412.12740v1",
    "pdf_url": "https://arxiv.org/pdf/2412.12740v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2412.12740",
    "arxiv_authors": [
      "Matteo Sodano",
      "Federico Magistri",
      "Jens Behley",
      "Cyrill Stachniss"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Open-World+Panoptic+Segmentation+Matteo+Sodano+Federico+Magistri+Jens+Behley+Cyrill+Stachniss",
    "gs_search_success": true,
    "gs_authors": [
      "L4LZHXsAAAAJ",
      "8vib2lAAAAAJ",
      "Vtd43l8AAAAJ",
      "irT8pKcAAAAJ"
    ],
    "citation_count": 3,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2308.01300",
    "title": "Revisiting DETR Pre-training for Object Detection",
    "year": 2023,
    "published": "2023-08-02T17:39:30Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Motivated by the remarkable achievements of DETR-based approaches on COCO object detection and segmentation benchmarks, recent endeavors have been directed towards elevating their performance through self-supervised pre-training of Transformers while preserving a frozen backbone. Noteworthy advancements in accuracy have been documented in certain studies. Our investigation delved deeply into a representative approach, DETReg, and its performance assessment in the context of emerging models like ",
    "arxiv_url": "https://arxiv.org/abs/2308.01300v2",
    "pdf_url": "https://arxiv.org/pdf/2308.01300v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2308.01300",
    "arxiv_authors": [
      "Yan Ma",
      "Weicong Liang",
      "Bohan Chen",
      "Yiduo Hao",
      "Bojian Hou",
      "Xiangyu Yue",
      "Chao Zhang",
      "Yuhui Yuan"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Revisiting+DETR+Pre-training+for+Object+Detection+Yan+Ma+Weicong+Liang+Bohan+Chen+Yiduo+Hao+Bojian+Hou",
    "gs_search_success": true,
    "gs_authors": [
      "eDy7754AAAAJ",
      "MgppzFwAAAAJ",
      "QvHDIygAAAAJ",
      "-xQ-C1sAAAAJ",
      "yFVd81sAAAAJ",
      "PzyvzksAAAAJ"
    ],
    "citation_count": 7,
    "gs_author_count": 8
  },
  {
    "arxiv_id": "2306.14300",
    "title": "Screening Autism Spectrum Disorder in childrens using Deep Learning Approach : Evaluating the classification model of YOLOv8 by comparing with other models",
    "year": 2023,
    "published": "2023-06-25T18:02:01Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Autism spectrum disorder (ASD) is a developmental condition that presents significant challenges in social interaction, communication, and behavior. Early intervention plays a pivotal role in enhancing cognitive abilities and reducing autistic symptoms in children with ASD. Numerous clinical studies have highlighted distinctive facial characteristics that distinguish ASD children from typically developing (TD) children. In this study, we propose a practical solution for ASD screening using facia",
    "arxiv_url": "https://arxiv.org/abs/2306.14300v1",
    "pdf_url": "https://arxiv.org/pdf/2306.14300v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2306.14300",
    "arxiv_authors": [
      "Subash Gautam",
      "Prabin Sharma",
      "Kisan Thapa",
      "Mala Deep Upadhaya",
      "Dikshya Thapa",
      "Salik Ram Khanal",
      "Vítor Manuel de Jesus Filipe"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Screening+Autism+Spectrum+Disorder+in+childrens+using+Deep+Learning+Approach+%3A+Evaluating+the+classification+model+of+YOLOv8+by+comparing+with+other+models+Subash+Gautam+Prabin+Sharma+Kisan+Thapa+Mala+Deep+Upadhaya+Dikshya+Thapa",
    "gs_search_success": true,
    "gs_authors": [
      "ZzKfXn8AAAAJ",
      "PvDIuZoAAAAJ",
      "rhTXLtwAAAAJ",
      "61GDeEAAAAAJ",
      "4Qs3ndUAAAAJ"
    ],
    "citation_count": 7,
    "gs_author_count": 7
  },
  {
    "arxiv_id": "2402.14015",
    "title": "Corrective Machine Unlearning",
    "year": 2024,
    "published": "2024-02-21T18:54:37Z",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR",
      "cs.CV"
    ],
    "abstract": "Machine Learning models increasingly face data integrity challenges due to the use of large-scale training datasets drawn from the Internet. We study what model developers can do if they detect that some data was manipulated or incorrect. Such manipulated data can cause adverse effects including vulnerability to backdoored samples, systemic biases, and reduced accuracy on certain input domains. Realistically, all manipulated training samples cannot be identified, and only a small, representative",
    "arxiv_url": "https://arxiv.org/abs/2402.14015v2",
    "pdf_url": "https://arxiv.org/pdf/2402.14015v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2402.14015",
    "arxiv_authors": [
      "Shashwat Goel",
      "Ameya Prabhu",
      "Philip Torr",
      "Ponnurangam Kumaraguru",
      "Amartya Sanyal"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Corrective+Machine+Unlearning+Shashwat+Goel+Ameya+Prabhu+Philip+Torr+Ponnurangam+Kumaraguru+Amartya+Sanyal",
    "gs_search_success": true,
    "gs_authors": [
      "exaNV-0AAAAJ",
      "0kK7sSAAAAAJ",
      "MfzQyP8AAAAJ",
      "cRLqsyYAAAAJ",
      "kPxa2w0AAAAJ"
    ],
    "citation_count": 45,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2412.01255",
    "title": "Merging synthetic and real embryo data for advanced AI predictions",
    "year": 2024,
    "published": "2024-12-02T08:24:49Z",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "abstract": "Accurate embryo morphology assessment is essential in assisted reproductive technology for selecting the most viable embryo. Artificial intelligence has the potential to enhance this process. However, the limited availability of embryo data presents challenges for training deep learning models. To address this, we trained two generative models using two datasets-one we created and made publicly available, and one existing public dataset-to generate synthetic embryo images at various cell stages,",
    "arxiv_url": "https://arxiv.org/abs/2412.01255v2",
    "pdf_url": "https://arxiv.org/pdf/2412.01255v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2412.01255",
    "arxiv_authors": [
      "Oriana Presacan",
      "Alexandru Dorobantiu",
      "Vajira Thambawita",
      "Michael A. Riegler",
      "Mette H. Stensen",
      "Mario Iliceto",
      "Alexandru C. Aldea",
      "Akriti Sharma"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Merging+synthetic+and+real+embryo+data+for+advanced+AI+predictions+Oriana+Presacan+Alexandru+Dorobantiu+Vajira+Thambawita+Michael+A.+Riegler+Mette+H.+Stensen",
    "gs_search_success": true,
    "gs_authors": [
      "Uni9jIUAAAAJ",
      "bzpPuSkAAAAJ",
      "hSe42z0AAAAJ",
      "Vd_ApDoAAAAJ",
      "Ex-rjyIAAAAJ"
    ],
    "citation_count": 4,
    "gs_author_count": 8
  },
  {
    "arxiv_id": "2407.16354",
    "title": "Strike a Balance in Continual Panoptic Segmentation",
    "year": 2024,
    "published": "2024-07-23T09:58:20Z",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "This study explores the emerging area of continual panoptic segmentation, highlighting three key balances. First, we introduce past-class backtrace distillation to balance the stability of existing knowledge with the adaptability to new information. This technique retraces the features associated with past classes based on the final label assignment results, performing knowledge distillation targeting these specific features from the previous model while allowing other features to flexibly adapt",
    "arxiv_url": "https://arxiv.org/abs/2407.16354v1",
    "pdf_url": "https://arxiv.org/pdf/2407.16354v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2407.16354",
    "arxiv_authors": [
      "Jinpeng Chen",
      "Runmin Cong",
      "Yuxuan Luo",
      "Horace Ho Shing Ip",
      "Sam Kwong"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Strike+a+Balance+in+Continual+Panoptic+Segmentation+Jinpeng+Chen+Runmin+Cong+Yuxuan+Luo+Horace+Ho+Shing+Ip+Sam+Kwong",
    "gs_search_success": true,
    "gs_authors": [
      "HdnFJ5kAAAAJ",
      "-VrKJ0EAAAAJ",
      "_PVI6EAAAAAJ",
      "4ERl0ygAAAAJ",
      "RkGFFUkAAAAJ"
    ],
    "citation_count": 9,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2404.10841",
    "title": "Gasformer: A Transformer-based Architecture for Segmenting Methane Emissions from Livestock in Optical Gas Imaging",
    "year": 2024,
    "published": "2024-04-16T18:38:23Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Methane emissions from livestock, particularly cattle, significantly contribute to climate change. Effective methane emission mitigation strategies are crucial as the global population and demand for livestock products increase. We introduce Gasformer, a novel semantic segmentation architecture for detecting low-flow rate methane emissions from livestock, and controlled release experiments using optical gas imaging. We present two unique datasets captured with a FLIR GF77 OGI camera. Gasformer l",
    "arxiv_url": "https://arxiv.org/abs/2404.10841v1",
    "pdf_url": "https://arxiv.org/pdf/2404.10841v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2404.10841",
    "arxiv_authors": [
      "Toqi Tahamid Sarker",
      "Mohamed G Embaby",
      "Khaled R Ahmed",
      "Amer AbuGhazaleh"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Gasformer%3A+A+Transformer-based+Architecture+for+Segmenting+Methane+Emissions+from+Livestock+in+Optical+Gas+Imaging+Toqi+Tahamid+Sarker+Mohamed+G+Embaby+Khaled+R+Ahmed+Amer+AbuGhazaleh",
    "gs_search_success": false,
    "gs_authors": [],
    "citation_count": null,
    "error_type": "fetch_failed",
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2303.02737",
    "title": "SePaint: Semantic Map Inpainting via Multinomial Diffusion",
    "year": 2023,
    "published": "2023-03-05T18:04:28Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Prediction beyond partial observations is crucial for robots to navigate in unknown environments because it can provide extra information regarding the surroundings beyond the current sensing range or resolution. In this work, we consider the inpainting of semantic Bird's-Eye-View maps. We propose SePaint, an inpainting model for semantic data based on generative multinomial diffusion. To maintain semantic consistency, we need to condition the prediction for the missing regions on the known regi",
    "arxiv_url": "https://arxiv.org/abs/2303.02737v1",
    "pdf_url": "https://arxiv.org/pdf/2303.02737v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2303.02737",
    "arxiv_authors": [
      "Zheng Chen",
      "Deepak Duggirala",
      "David Crandall",
      "Lei Jiang",
      "Lantao Liu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=SePaint%3A+Semantic+Map+Inpainting+via+Multinomial+Diffusion+Zheng+Chen+Deepak+Duggirala+David+Crandall+Lei+Jiang+Lantao+Liu",
    "gs_search_success": true,
    "gs_authors": [
      "X6MkScIAAAAJ",
      "8bQRH5YAAAAJ",
      "rUyfNfQAAAAJ",
      "L5dHk5cAAAAJ",
      "-1sXorAAAAAJ"
    ],
    "citation_count": 4,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2304.04336",
    "title": "Split, Merge, and Refine: Fitting Tight Bounding Boxes via Over-Segmentation and Iterative Search",
    "year": 2023,
    "published": "2023-04-10T00:25:15Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Achieving tight bounding boxes of a shape while guaranteeing complete boundness is an essential task for efficient geometric operations and unsupervised semantic part detection. But previous methods fail to achieve both full coverage and tightness. Neural-network-based methods are not suitable for these goals due to the non-differentiability of the objective, while classic iterative search methods suffer from their sensitivity to the initialization. We propose a novel framework for finding a set",
    "arxiv_url": "https://arxiv.org/abs/2304.04336v3",
    "pdf_url": "https://arxiv.org/pdf/2304.04336v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2304.04336",
    "arxiv_authors": [
      "Chanhyeok Park",
      "Minhyuk Sung"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Split%2C+Merge%2C+and+Refine%3A+Fitting+Tight+Bounding+Boxes+via+Over-Segmentation+and+Iterative+Search+Chanhyeok+Park+Minhyuk+Sung",
    "gs_search_success": true,
    "gs_authors": [
      "XrShcqcAAAAJ",
      "PcIYMp4AAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 2
  },
  {
    "arxiv_id": "2408.12077",
    "title": "Through-the-Wall Radar Human Activity Micro-Doppler Signature Representation Method Based on Joint Boulic-Sinusoidal Pendulum Model",
    "year": 2024,
    "published": "2024-08-22T02:33:29Z",
    "categories": [
      "eess.SP",
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "With the help of micro-Doppler signature, ultra-wideband (UWB) through-the-wall radar (TWR) enables the reconstruction of range and velocity information of limb nodes to accurately identify indoor human activities. However, existing methods are usually trained and validated directly using range-time maps (RTM) and Doppler-time maps (DTM), which have high feature redundancy and poor generalization ability. In order to solve this problem, this paper proposes a human activity micro-Doppler signatur",
    "arxiv_url": "https://arxiv.org/abs/2408.12077v1",
    "pdf_url": "https://arxiv.org/pdf/2408.12077v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2408.12077",
    "arxiv_authors": [
      "Xiaopeng Yang",
      "Weicheng Gao",
      "Xiaodong Qu",
      "Zeyu Ma",
      "Hao Zhang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Through-the-Wall+Radar+Human+Activity+Micro-Doppler+Signature+Representation+Method+Based+on+Joint+Boulic-Sinusoidal+Pendulum+Model+Xiaopeng+Yang+Weicheng+Gao+Xiaodong+Qu+Zeyu+Ma+Hao+Zhang",
    "gs_search_success": true,
    "gs_authors": [
      "mLB5_aYAAAAJ"
    ],
    "citation_count": 8,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2311.04938",
    "title": "Improved DDIM Sampling with Moment Matching Gaussian Mixtures",
    "year": 2023,
    "published": "2023-11-08T00:24:50Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "abstract": "We propose using a Gaussian Mixture Model (GMM) as reverse transition operator (kernel) within the Denoising Diffusion Implicit Models (DDIM) framework, which is one of the most widely used approaches for accelerated sampling from pre-trained Denoising Diffusion Probabilistic Models (DDPM). Specifically we match the first and second order central moments of the DDPM forward marginals by constraining the parameters of the GMM. We see that moment matching is sufficient to obtain samples with equal",
    "arxiv_url": "https://arxiv.org/abs/2311.04938v4",
    "pdf_url": "https://arxiv.org/pdf/2311.04938v4",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2311.04938",
    "arxiv_authors": [
      "Prasad Gabbur"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Improved+DDIM+Sampling+with+Moment+Matching+Gaussian+Mixtures+Prasad+Gabbur",
    "gs_search_success": true,
    "gs_authors": [
      "Mk9iFM8AAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 1
  },
  {
    "arxiv_id": "2311.14746",
    "title": "All in One: RGB, RGB-D, and RGB-T Salient Object Detection",
    "year": 2023,
    "published": "2023-11-23T03:34:41Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Salient object detection (SOD) aims to identify the most attractive objects within an image. Depending on the type of data being detected, SOD can be categorized into various forms, including RGB, RGB-D (Depth), RGB-T (Thermal) and light field SOD. Previous researches have focused on saliency detection with individual data type. If the RGB-D SOD model is forced to detect RGB-T data it will perform poorly. We propose an innovative model framework that provides a unified solution for the salient o",
    "arxiv_url": "https://arxiv.org/abs/2311.14746v1",
    "pdf_url": "https://arxiv.org/pdf/2311.14746v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2311.14746",
    "arxiv_authors": [
      "Xingzhao Jia",
      "Zhongqiu Zhao",
      "Changlei Dongye",
      "Zhao Zhang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=All+in+One%3A+RGB%2C+RGB-D%2C+and+RGB-T+Salient+Object+Detection+Xingzhao+Jia+Zhongqiu+Zhao+Changlei+Dongye+Zhao+Zhang",
    "gs_search_success": true,
    "gs_authors": [
      "yELU6JcAAAAJ",
      "h6SCUNwAAAAJ"
    ],
    "citation_count": 5,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2405.07155",
    "title": "Meta-Learned Modality-Weighted Knowledge Distillation for Robust Multi-Modal Learning with Missing Data",
    "year": 2024,
    "published": "2024-05-12T04:18:10Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "In multi-modal learning, some modalities are more influential than others, and their absence can have a significant impact on classification/segmentation accuracy. Addressing this challenge, we propose a novel approach called Meta-learned Modality-weighted Knowledge Distillation (MetaKD), which enables multi-modal models to maintain high accuracy even when key modalities are missing. MetaKD adaptively estimates the importance weight of each modality through a meta-learning process. These learned",
    "arxiv_url": "https://arxiv.org/abs/2405.07155v4",
    "pdf_url": "https://arxiv.org/pdf/2405.07155v4",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2405.07155",
    "arxiv_authors": [
      "Hu Wang",
      "Salma Hassan",
      "Yuyuan Liu",
      "Congbo Ma",
      "Yuanhong Chen",
      "Qing Li",
      "Jiahui Geng",
      "Bingjie Wang",
      "Yu Tian",
      "Yutong Xie",
      "Jodie Avery",
      "Louise Hull",
      "Ian Reid",
      "Mohammad Yaqub",
      "Gustavo Carneiro"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Meta-Learned+Modality-Weighted+Knowledge+Distillation+for+Robust+Multi-Modal+Learning+with+Missing+Data+Hu+Wang+Salma+Hassan+Yuyuan+Liu+Congbo+Ma+Yuanhong+Chen",
    "gs_search_success": true,
    "gs_authors": [
      "i1ASOXsAAAAJ",
      "K_6dgCgAAAAJ",
      "yssGivYAAAAJ",
      "ddDL9HMAAAAJ",
      "0DhSy_EAAAAJ"
    ],
    "citation_count": 3,
    "gs_author_count": 9
  },
  {
    "arxiv_id": "2401.01175",
    "title": "Learning Surface Scattering Parameters From SAR Images Using Differentiable Ray Tracing",
    "year": 2024,
    "published": "2024-01-02T12:09:06Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Simulating high-resolution Synthetic Aperture Radar (SAR) images in complex scenes has consistently presented a significant research challenge. The development of a microwave-domain surface scattering model and its reversibility are poised to play a pivotal role in enhancing the authenticity of SAR image simulations and facilitating the reconstruction of target parameters. Drawing inspiration from the field of computer graphics, this paper proposes a surface microwave rendering model that compre",
    "arxiv_url": "https://arxiv.org/abs/2401.01175v1",
    "pdf_url": "https://arxiv.org/pdf/2401.01175v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2401.01175",
    "arxiv_authors": [
      "Jiangtao Wei",
      "Yixiang Luomei",
      "Xu Zhang",
      "Feng Xu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Learning+Surface+Scattering+Parameters+From+SAR+Images+Using+Differentiable+Ray+Tracing+Jiangtao+Wei+Yixiang+Luomei+Xu+Zhang+Feng+Xu",
    "gs_search_success": true,
    "gs_authors": [
      "i-N1K9oAAAAJ",
      "FPyqC3gAAAAJ",
      "nW2U70EAAAAJ"
    ],
    "citation_count": 10,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2310.04299",
    "title": "Convergent ADMM Plug and Play PET Image Reconstruction",
    "year": 2023,
    "published": "2023-10-06T15:01:32Z",
    "categories": [
      "eess.IV",
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "In this work, we investigate hybrid PET reconstruction algorithms based on coupling a model-based variational reconstruction and the application of a separately learnt Deep Neural Network operator (DNN) in an ADMM Plug and Play framework. Following recent results in optimization, fixed point convergence of the scheme can be achieved by enforcing an additional constraint on network parameters during learning. We propose such an ADMM algorithm and show in a realistic [18F]-FDG synthetic brain exam",
    "arxiv_url": "https://arxiv.org/abs/2310.04299v1",
    "pdf_url": "https://arxiv.org/pdf/2310.04299v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2310.04299",
    "arxiv_authors": [
      "Florent Sureau",
      "Mahdi Latreche",
      "Marion Savanier",
      "Claude Comtat"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Convergent+ADMM+Plug+and+Play+PET+Image+Reconstruction+Florent+Sureau+Mahdi+Latreche+Marion+Savanier+Claude+Comtat",
    "gs_search_success": true,
    "gs_authors": [
      "ZKahH9oAAAAJ"
    ],
    "citation_count": 7,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2407.02014",
    "title": "Multi-Grained Contrast for Data-Efficient Unsupervised Representation Learning",
    "year": 2024,
    "published": "2024-07-02T07:35:21Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "The existing contrastive learning methods mainly focus on single-grained representation learning, e.g., part-level, object-level or scene-level ones, thus inevitably neglecting the transferability of representations on other granularity levels. In this paper, we aim to learn multi-grained representations, which can effectively describe the image on various granularity levels, thus improving generalization on extensive downstream tasks. To this end, we propose a novel Multi-Grained Contrast metho",
    "arxiv_url": "https://arxiv.org/abs/2407.02014v1",
    "pdf_url": "https://arxiv.org/pdf/2407.02014v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2407.02014",
    "arxiv_authors": [
      "Chengchao Shen",
      "Jianzhong Chen",
      "Jianxin Wang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Multi-Grained+Contrast+for+Data-Efficient+Unsupervised+Representation+Learning+Chengchao+Shen+Jianzhong+Chen+Jianxin+Wang",
    "gs_search_success": true,
    "gs_authors": [
      "lQ8tqZ4AAAAJ"
    ],
    "citation_count": 5,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2409.14063",
    "title": "Recovering Global Data Distribution Locally in Federated Learning",
    "year": 2024,
    "published": "2024-09-21T08:35:04Z",
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "abstract": "Federated Learning (FL) is a distributed machine learning paradigm that enables collaboration among multiple clients to train a shared model without sharing raw data. However, a major challenge in FL is the label imbalance, where clients may exclusively possess certain classes while having numerous minority and missing classes. Previous works focus on optimizing local updates or global aggregation but ignore the underlying imbalanced label distribution across clients. In this paper, we propose a",
    "arxiv_url": "https://arxiv.org/abs/2409.14063v1",
    "pdf_url": "https://arxiv.org/pdf/2409.14063v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2409.14063",
    "arxiv_authors": [
      "Ziyu Yao"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Recovering+Global+Data+Distribution+Locally+in+Federated+Learning+Ziyu+Yao",
    "gs_search_success": true,
    "gs_authors": [
      "3eePDdIAAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 1
  },
  {
    "arxiv_id": "2308.03008",
    "title": "Early Detection and Localization of Pancreatic Cancer by Label-Free Tumor Synthesis",
    "year": 2023,
    "published": "2023-08-06T03:37:34Z",
    "categories": [
      "eess.IV",
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "Early detection and localization of pancreatic cancer can increase the 5-year survival rate for patients from 8.5% to 20%. Artificial intelligence (AI) can potentially assist radiologists in detecting pancreatic tumors at an early stage. Training AI models require a vast number of annotated examples, but the availability of CT scans obtaining early-stage tumors is constrained. This is because early-stage tumors may not cause any symptoms, which can delay detection, and the tumors are relatively ",
    "arxiv_url": "https://arxiv.org/abs/2308.03008v1",
    "pdf_url": "https://arxiv.org/pdf/2308.03008v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2308.03008",
    "arxiv_authors": [
      "Bowen Li",
      "Yu-Cheng Chou",
      "Shuwen Sun",
      "Hualin Qiao",
      "Alan Yuille",
      "Zongwei Zhou"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Early+Detection+and+Localization+of+Pancreatic+Cancer+by+Label-Free+Tumor+Synthesis+Bowen+Li+Yu-Cheng+Chou+Shuwen+Sun+Hualin+Qiao+Alan+Yuille",
    "gs_search_success": true,
    "gs_authors": [
      "UfINwO0AAAAJ",
      "JVOeczAAAAAJ",
      "YVNRBTcAAAAJ",
      "FJ-huxgAAAAJ",
      "7-7-BF0AAAAJ"
    ],
    "citation_count": 28,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2301.13151",
    "title": "Convolutional Neural Network-Based Automatic Classification of Colorectal and Prostate Tumor Biopsies Using Multispectral Imagery: System Development Study",
    "year": 2023,
    "published": "2023-01-30T18:28:25Z",
    "categories": [
      "eess.IV",
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "Colorectal and prostate cancers are the most common types of cancer in men worldwide. To diagnose colorectal and prostate cancer, a pathologist performs a histological analysis on needle biopsy samples. This manual process is time-consuming and error-prone, resulting in high intra and interobserver variability, which affects diagnosis reliability. This study aims to develop an automatic computerized system for diagnosing colorectal and prostate tumors by using images of biopsy samples to reduce ",
    "arxiv_url": "https://arxiv.org/abs/2301.13151v1",
    "pdf_url": "https://arxiv.org/pdf/2301.13151v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2301.13151",
    "arxiv_authors": [
      "Remy Peyret",
      "Duaa alSaeed",
      "Fouad Khelifi",
      "Nadia Al-Ghreimil",
      "Heyam Al-Baity",
      "Ahmed Bouridane"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Convolutional+Neural+Network-Based+Automatic+Classification+of+Colorectal+and+Prostate+Tumor+Biopsies+Using+Multispectral+Imagery%3A+System+Development+Study+Remy+Peyret+Duaa+alSaeed+Fouad+Khelifi+Nadia+Al-Ghreimil+Heyam+Al-Baity",
    "gs_search_success": true,
    "gs_authors": [
      "3ZRpXO0AAAAJ",
      "dDjIW94AAAAJ",
      "vHDxrfYAAAAJ",
      "WsFFZGAAAAAJ",
      "4gdwLMYAAAAJ"
    ],
    "citation_count": 4,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2410.15364",
    "title": "Scene Graph Generation with Role-Playing Large Language Models",
    "year": 2024,
    "published": "2024-10-20T11:40:31Z",
    "categories": [
      "cs.CV",
      "cs.MM"
    ],
    "abstract": "Current approaches for open-vocabulary scene graph generation (OVSGG) use vision-language models such as CLIP and follow a standard zero-shot pipeline -- computing similarity between the query image and the text embeddings for each category (i.e., text classifiers). In this work, we argue that the text classifiers adopted by existing OVSGG methods, i.e., category-/part-level prompts, are scene-agnostic as they remain unchanged across contexts. Using such fixed text classifiers not only struggles",
    "arxiv_url": "https://arxiv.org/abs/2410.15364v1",
    "pdf_url": "https://arxiv.org/pdf/2410.15364v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2410.15364",
    "arxiv_authors": [
      "Guikun Chen",
      "Jin Li",
      "Wenguan Wang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Scene+Graph+Generation+with+Role-Playing+Large+Language+Models+Guikun+Chen+Jin+Li+Wenguan+Wang",
    "gs_search_success": true,
    "gs_authors": [
      "CqAQQkgAAAAJ",
      "I1TOdpkAAAAJ"
    ],
    "citation_count": 24,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2505.09746",
    "title": "A Computational Pipeline for Advanced Analysis of 4D Flow MRI in the Left Atrium",
    "year": 2025,
    "published": "2025-05-14T19:09:17Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "The left atrium (LA) plays a pivotal role in modulating left ventricular filling, but our comprehension of its hemodynamics is significantly limited by the constraints of conventional ultrasound analysis. 4D flow magnetic resonance imaging (4D Flow MRI) holds promise for enhancing our understanding of atrial hemodynamics. However, the low velocities within the LA and the limited spatial resolution of 4D Flow MRI make analyzing this chamber challenging. Furthermore, the absence of dedicated compu",
    "arxiv_url": "https://arxiv.org/abs/2505.09746v1",
    "pdf_url": "https://arxiv.org/pdf/2505.09746v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2505.09746",
    "arxiv_authors": [
      "Xabier Morales",
      "Ayah Elsayed",
      "Debbie Zhao",
      "Filip Loncaric",
      "Ainhoa Aguado",
      "Mireia Masias",
      "Gina Quill",
      "Marc Ramos",
      "Ada Doltra",
      "Ana Garcia",
      "Marta Sitges",
      "David Marlevi",
      "Alistair Young",
      "Martyn Nash",
      "Bart Bijnens",
      "Oscar Camara"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=A+Computational+Pipeline+for+Advanced+Analysis+of+4D+Flow+MRI+in+the+Left+Atrium+Xabier+Morales+Ayah+Elsayed+Debbie+Zhao+Filip+Loncaric+Ainhoa+Aguado",
    "gs_search_success": true,
    "gs_authors": [
      "nLbLPfcAAAAJ",
      "Ue1ZkSoAAAAJ",
      "19GmxK8AAAAJ",
      "huCe7uEAAAAJ",
      "4ViZDKwAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 9
  },
  {
    "arxiv_id": "2312.14985",
    "title": "UniHuman: A Unified Model for Editing Human Images in the Wild",
    "year": 2023,
    "published": "2023-12-22T05:00:30Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Human image editing includes tasks like changing a person's pose, their clothing, or editing the image according to a text prompt. However, prior work often tackles these tasks separately, overlooking the benefit of mutual reinforcement from learning them jointly. In this paper, we propose UniHuman, a unified model that addresses multiple facets of human image editing in real-world settings. To enhance the model's generation quality and generalization capacity, we leverage guidance from human vi",
    "arxiv_url": "https://arxiv.org/abs/2312.14985v2",
    "pdf_url": "https://arxiv.org/pdf/2312.14985v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2312.14985",
    "arxiv_authors": [
      "Nannan Li",
      "Qing Liu",
      "Krishna Kumar Singh",
      "Yilin Wang",
      "Jianming Zhang",
      "Bryan A. Plummer",
      "Zhe Lin"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=UniHuman%3A+A+Unified+Model+for+Editing+Human+Images+in+the+Wild+Nannan+Li+Qing+Liu+Krishna+Kumar+Singh+Yilin+Wang+Jianming+Zhang",
    "gs_search_success": true,
    "gs_authors": [
      "R0bnqaAAAAAJ",
      "AJzL98UAAAAJ",
      "3TMipekAAAAJ",
      "1ytghtEAAAAJ",
      "329B_BEAAAAJ",
      "TkVHKDgAAAAJ",
      "fYqdLx4AAAAJ"
    ],
    "citation_count": 16,
    "gs_author_count": 7
  },
  {
    "arxiv_id": "2407.14944",
    "title": "Automatic Generation of Fashion Images using Prompting in Generative Machine Learning Models",
    "year": 2024,
    "published": "2024-07-20T17:37:51Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "The advent of artificial intelligence has contributed in a groundbreaking transformation of the fashion industry, redefining creativity and innovation in unprecedented ways. This work investigates methodologies for generating tailored fashion descriptions using two distinct Large Language Models and a Stable Diffusion model for fashion image creation. Emphasizing adaptability in AI-driven fashion creativity, we depart from traditional approaches and focus on prompting techniques, such as zero-sh",
    "arxiv_url": "https://arxiv.org/abs/2407.14944v1",
    "pdf_url": "https://arxiv.org/pdf/2407.14944v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2407.14944",
    "arxiv_authors": [
      "Georgia Argyrou",
      "Angeliki Dimitriou",
      "Maria Lymperaiou",
      "Giorgos Filandrianos",
      "Giorgos Stamou"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Automatic+Generation+of+Fashion+Images+using+Prompting+in+Generative+Machine+Learning+Models+Georgia+Argyrou+Angeliki+Dimitriou+Maria+Lymperaiou+Giorgos+Filandrianos+Giorgos+Stamou",
    "gs_search_success": true,
    "gs_authors": [
      "YNikyhIAAAAJ",
      "R3y5dxMAAAAJ",
      "pyiokhkAAAAJ",
      "oPIyXYcAAAAJ"
    ],
    "citation_count": 9,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2408.11150",
    "title": "An Interpretable Deep Learning Approach for Morphological Script Type Analysis",
    "year": 2024,
    "published": "2024-08-20T19:15:06Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Defining script types and establishing classification criteria for medieval handwriting is a central aspect of palaeographical analysis. However, existing typologies often encounter methodological challenges, such as descriptive limitations and subjective criteria. We propose an interpretable deep learning-based approach to morphological script type analysis, which enables systematic and objective analysis and contributes to bridging the gap between qualitative observations and quantitative meas",
    "arxiv_url": "https://arxiv.org/abs/2408.11150v1",
    "pdf_url": "https://arxiv.org/pdf/2408.11150v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2408.11150",
    "arxiv_authors": [
      "Malamatenia Vlachou-Efstathiou",
      "Ioannis Siglidis",
      "Dominique Stutzmann",
      "Mathieu Aubry"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=An+Interpretable+Deep+Learning+Approach+for+Morphological+Script+Type+Analysis+Malamatenia+Vlachou-Efstathiou+Ioannis+Siglidis+Dominique+Stutzmann+Mathieu+Aubry",
    "gs_search_success": false,
    "gs_authors": [],
    "citation_count": null,
    "error_type": "fetch_failed",
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2403.19128",
    "title": "OmniParser: A Unified Framework for Text Spotting, Key Information Extraction and Table Recognition",
    "year": 2024,
    "published": "2024-03-28T03:51:14Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Recently, visually-situated text parsing (VsTP) has experienced notable advancements, driven by the increasing demand for automated document understanding and the emergence of Generative Large Language Models (LLMs) capable of processing document-based questions. Various methods have been proposed to address the challenging problem of VsTP. However, due to the diversified targets and heterogeneous schemas, previous works usually design task-specific architectures and objectives for individual ta",
    "arxiv_url": "https://arxiv.org/abs/2403.19128v1",
    "pdf_url": "https://arxiv.org/pdf/2403.19128v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2403.19128",
    "arxiv_authors": [
      "Jianqiang Wan",
      "Sibo Song",
      "Wenwen Yu",
      "Yuliang Liu",
      "Wenqing Cheng",
      "Fei Huang",
      "Xiang Bai",
      "Cong Yao",
      "Zhibo Yang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=OmniParser%3A+A+Unified+Framework+for+Text+Spotting%2C+Key+Information+Extraction+and+Table+Recognition+Jianqiang+Wan+Sibo+Song+Wenwen+Yu+Yuliang+Liu+Wenqing+Cheng",
    "gs_search_success": true,
    "gs_authors": [
      "UeltiQ4AAAAJ",
      "gEXBFvYAAAAJ",
      "9r98PpoAAAAJ",
      "X3K4jQwAAAAJ",
      "V06mvgQAAAAJ",
      "kwWyE2cAAAAJ",
      "IpmnLFcAAAAJ"
    ],
    "citation_count": 74,
    "gs_author_count": 9
  },
  {
    "arxiv_id": "2412.03887",
    "title": "MOANA: Multi-Radar Dataset for Maritime Odometry and Autonomous Navigation Application",
    "year": 2024,
    "published": "2024-12-05T05:40:40Z",
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "abstract": "Maritime environmental sensing requires overcoming challenges from complex conditions such as harsh weather, platform perturbations, large dynamic objects, and the requirement for long detection ranges. While cameras and LiDAR are commonly used in ground vehicle navigation, their applicability in maritime settings is limited by range constraints and hardware maintenance issues. Radar sensors, however, offer robust long-range detection capabilities and resilience to physical contamination from we",
    "arxiv_url": "https://arxiv.org/abs/2412.03887v4",
    "pdf_url": "https://arxiv.org/pdf/2412.03887v4",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2412.03887",
    "arxiv_authors": [
      "Hyesu Jang",
      "Wooseong Yang",
      "Hanguen Kim",
      "Dongje Lee",
      "Yongjin Kim",
      "Jinbum Park",
      "Minsoo Jeon",
      "Jaeseong Koh",
      "Yejin Kang",
      "Minwoo Jung",
      "Sangwoo Jung",
      "Chng Zhen Hao",
      "Wong Yu Hin",
      "Chew Yihang",
      "Ayoung Kim"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=MOANA%3A+Multi-Radar+Dataset+for+Maritime+Odometry+and+Autonomous+Navigation+Application+Hyesu+Jang+Wooseong+Yang+Hanguen+Kim+Dongje+Lee+Yongjin+Kim",
    "gs_search_success": true,
    "gs_authors": [
      "aKPTi7gAAAAJ",
      "liSzSegAAAAJ",
      "I2pNZDkAAAAJ",
      "lh2KUKMAAAAJ"
    ],
    "citation_count": 3,
    "gs_author_count": 9
  },
  {
    "arxiv_id": "2411.00688",
    "title": "Why do we regularise in every iteration for imaging inverse problems?",
    "year": 2024,
    "published": "2024-11-01T15:50:05Z",
    "categories": [
      "math.NA",
      "cs.CV",
      "math.OC"
    ],
    "abstract": "Regularisation is commonly used in iterative methods for solving imaging inverse problems. Many algorithms involve the evaluation of the proximal operator of the regularisation term in every iteration, leading to a significant computational overhead since such evaluation can be costly. In this context, the ProxSkip algorithm, recently proposed for federated learning purposes, emerges as an solution. It randomly skips regularisation steps, reducing the computational time of an iterative algorithm",
    "arxiv_url": "https://arxiv.org/abs/2411.00688v1",
    "pdf_url": "https://arxiv.org/pdf/2411.00688v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2411.00688",
    "arxiv_authors": [
      "Evangelos Papoutsellis",
      "Zeljko Kereta",
      "Kostas Papafitsoros"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Why+do+we+regularise+in+every+iteration+for+imaging+inverse+problems%3F+Evangelos+Papoutsellis+Zeljko+Kereta+Kostas+Papafitsoros",
    "gs_search_success": true,
    "gs_authors": [
      "MnJqcyQAAAAJ",
      "fgSULeUAAAAJ"
    ],
    "citation_count": 2,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2402.14780",
    "title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video Diffusion Models",
    "year": 2024,
    "published": "2024-02-22T18:38:48Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Image customization has been extensively studied in text-to-image (T2I) diffusion models, leading to impressive outcomes and applications. With the emergence of text-to-video (T2V) diffusion models, its temporal counterpart, motion customization, has not yet been well investigated. To address the challenge of one-shot video motion customization, we propose Customize-A-Video that models the motion from a single reference video and adapts it to new subjects and scenes with both spatial and tempora",
    "arxiv_url": "https://arxiv.org/abs/2402.14780v3",
    "pdf_url": "https://arxiv.org/pdf/2402.14780v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2402.14780",
    "arxiv_authors": [
      "Yixuan Ren",
      "Yang Zhou",
      "Jimei Yang",
      "Jing Shi",
      "Difan Liu",
      "Feng Liu",
      "Mingi Kwon",
      "Abhinav Shrivastava"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Customize-A-Video%3A+One-Shot+Motion+Customization+of+Text-to-Video+Diffusion+Models+Yixuan+Ren+Yang+Zhou+Jimei+Yang+Jing+Shi+Difan+Liu",
    "gs_search_success": true,
    "gs_authors": [
      "FHtM5MUAAAAJ",
      "TFpdak8AAAAJ",
      "uiqXutMAAAAJ",
      "mIF9BowAAAAJ",
      "UuwugFEAAAAJ",
      "GPDTa9sAAAAJ",
      "W8vK8BwAAAAJ"
    ],
    "citation_count": 65,
    "gs_author_count": 8
  },
  {
    "arxiv_id": "2407.15683",
    "title": "Enhancing Transferability of Targeted Adversarial Examples: A Self-Universal Perspective",
    "year": 2024,
    "published": "2024-07-22T14:51:28Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Transfer-based targeted adversarial attacks against black-box deep neural networks (DNNs) have been proven to be significantly more challenging than untargeted ones. The impressive transferability of current SOTA, the generative methods, comes at the cost of requiring massive amounts of additional data and time-consuming training for each targeted label. This results in limited efficiency and flexibility, significantly hindering their deployment in practical applications. In this paper, we offer",
    "arxiv_url": "https://arxiv.org/abs/2407.15683v1",
    "pdf_url": "https://arxiv.org/pdf/2407.15683v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2407.15683",
    "arxiv_authors": [
      "Bowen Peng",
      "Li Liu",
      "Tianpeng Liu",
      "Zhen Liu",
      "Yongxiang Liu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Enhancing+Transferability+of+Targeted+Adversarial+Examples%3A+A+Self-Universal+Perspective+Bowen+Peng+Li+Liu+Tianpeng+Liu+Zhen+Liu+Yongxiang+Liu",
    "gs_search_success": true,
    "gs_authors": [
      "I8QD_w0AAAAJ",
      "a9tTHSEAAAAJ",
      "9cMQrVsAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2303.10062",
    "title": "Confidence-aware 3D Gaze Estimation and Evaluation Metric",
    "year": 2023,
    "published": "2023-03-17T15:44:44Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Deep learning appearance-based 3D gaze estimation is gaining popularity due to its minimal hardware requirements and being free of constraint. Unreliable and overconfident inferences, however, still limit the adoption of this gaze estimation method. To address the unreliable and overconfident issues, we introduce a confidence-aware model that predicts uncertainties together with gaze angle estimations. We also introduce a novel effectiveness evaluation method based on the causality between eye f",
    "arxiv_url": "https://arxiv.org/abs/2303.10062v2",
    "pdf_url": "https://arxiv.org/pdf/2303.10062v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2303.10062",
    "arxiv_authors": [
      "Qiaojie Zheng",
      "Xiaoli Zhang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Confidence-aware+3D+Gaze+Estimation+and+Evaluation+Metric+Qiaojie+Zheng+Xiaoli+Zhang",
    "gs_search_success": true,
    "gs_authors": [
      "PI-uT-sAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 2
  },
  {
    "arxiv_id": "2303.10087",
    "title": "Neural Refinement for Absolute Pose Regression with Feature Synthesis",
    "year": 2023,
    "published": "2023-03-17T16:10:50Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Absolute Pose Regression (APR) methods use deep neural networks to directly regress camera poses from RGB images. However, the predominant APR architectures only rely on 2D operations during inference, resulting in limited accuracy of pose estimation due to the lack of 3D geometry constraints or priors. In this work, we propose a test-time refinement pipeline that leverages implicit geometric constraints using a robust feature field to enhance the ability of APR methods to use 3D information dur",
    "arxiv_url": "https://arxiv.org/abs/2303.10087v2",
    "pdf_url": "https://arxiv.org/pdf/2303.10087v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2303.10087",
    "arxiv_authors": [
      "Shuai Chen",
      "Yash Bhalgat",
      "Xinghui Li",
      "Jiawang Bian",
      "Kejie Li",
      "Zirui Wang",
      "Victor Adrian Prisacariu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Neural+Refinement+for+Absolute+Pose+Regression+with+Feature+Synthesis+Shuai+Chen+Yash+Bhalgat+Xinghui+Li+Jiawang+Bian+Kejie+Li",
    "gs_search_success": true,
    "gs_authors": [
      "XLlgbBoAAAAJ",
      "c0xTh_YAAAAJ",
      "JBwsoCUAAAAJ",
      "zeGz5JcAAAAJ",
      "GmWA-LoAAAAJ",
      "zCBKqa8AAAAJ",
      "q0VSEHYAAAAJ"
    ],
    "citation_count": 33,
    "gs_author_count": 7
  },
  {
    "arxiv_id": "2311.10207",
    "title": "Stella Nera: A Differentiable Maddness-Based Hardware Accelerator for Efficient Approximate Matrix Multiplication",
    "year": 2023,
    "published": "2023-11-16T21:43:05Z",
    "categories": [
      "cs.AR",
      "cs.CV",
      "cs.LG",
      "stat.ML"
    ],
    "abstract": "Artificial intelligence has surged in recent years, with advancements in machine learning rapidly impacting nearly every area of life. However, the growing complexity of these models has far outpaced advancements in available hardware accelerators, leading to significant computational and energy demands, primarily due to matrix multiplications, which dominate the compute workload. Maddness (i.e., Multiply-ADDitioN-lESS) presents a hash-based version of product quantization, which renders matrix ",
    "arxiv_url": "https://arxiv.org/abs/2311.10207v2",
    "pdf_url": "https://arxiv.org/pdf/2311.10207v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2311.10207",
    "arxiv_authors": [
      "Jannis Schönleber",
      "Lukas Cavigelli",
      "Matteo Perotti",
      "Luca Benini",
      "Renzo Andri"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Stella+Nera%3A+A+Differentiable+Maddness-Based+Hardware+Accelerator+for+Efficient+Approximate+Matrix+Multiplication+Jannis+Sch%C3%B6nleber+Lukas+Cavigelli+Matteo+Perotti+Luca+Benini+Renzo+Andri",
    "gs_search_success": false,
    "gs_authors": [],
    "citation_count": null,
    "error_type": "fetch_failed",
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2404.04736",
    "title": "ProtoAL: Interpretable Deep Active Learning with prototypes for medical imaging",
    "year": 2024,
    "published": "2024-04-06T21:39:49Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "abstract": "The adoption of Deep Learning algorithms in the medical imaging field is a prominent area of research, with high potential for advancing AI-based Computer-aided diagnosis (AI-CAD) solutions. However, current solutions face challenges due to a lack of interpretability features and high data demands, prompting recent efforts to address these issues. In this study, we propose the ProtoAL method, where we integrate an interpretable DL model into the Deep Active Learning (DAL) framework. This approac",
    "arxiv_url": "https://arxiv.org/abs/2404.04736v1",
    "pdf_url": "https://arxiv.org/pdf/2404.04736v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2404.04736",
    "arxiv_authors": [
      "Iury B. de A. Santos",
      "André C. P. L. F. de Carvalho"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=ProtoAL%3A+Interpretable+Deep+Active+Learning+with+prototypes+for+medical+imaging+Iury+B.+de+A.+Santos+Andr%C3%A9+C.+P.+L.+F.+de+Carvalho",
    "gs_search_success": true,
    "gs_authors": [
      "QR7LF_oAAAAJ",
      "Jx_5GrgAAAAJ"
    ],
    "citation_count": 2,
    "gs_author_count": 2
  },
  {
    "arxiv_id": "2405.09459",
    "title": "Fourier Boundary Features Network with Wider Catchers for Glass Segmentation",
    "year": 2024,
    "published": "2024-05-15T15:52:27Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Glass largely blurs the boundary between the real world and the reflection. The special transmittance and reflectance quality have confused the semantic tasks related to machine vision. Therefore, how to clear the boundary built by glass, and avoid over-capturing features as false positive information in deep structure, matters for constraining the segmentation of reflection surface and penetrating glass. We proposed the Fourier Boundary Features Network with Wider Catchers (FBWC), which might b",
    "arxiv_url": "https://arxiv.org/abs/2405.09459v2",
    "pdf_url": "https://arxiv.org/pdf/2405.09459v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2405.09459",
    "arxiv_authors": [
      "Xiaolin Qin",
      "Jiacen Liu",
      "Qianlei Wang",
      "Shaolin Zhang",
      "Fei Zhu",
      "Zhang Yi"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Fourier+Boundary+Features+Network+with+Wider+Catchers+for+Glass+Segmentation+Xiaolin+Qin+Jiacen+Liu+Qianlei+Wang+Shaolin+Zhang+Fei+Zhu",
    "gs_search_success": true,
    "gs_authors": [
      "fjZ1CBwAAAAJ",
      "RaqG37kAAAAJ",
      "z_9KcmsAAAAJ",
      "N9FhhGQAAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2406.15349",
    "title": "NAVSIM: Data-Driven Non-Reactive Autonomous Vehicle Simulation and Benchmarking",
    "year": 2024,
    "published": "2024-06-21T17:59:02Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "cs.RO"
    ],
    "abstract": "Benchmarking vision-based driving policies is challenging. On one hand, open-loop evaluation with real data is easy, but these results do not reflect closed-loop performance. On the other, closed-loop evaluation is possible in simulation, but is hard to scale due to its significant computational demands. Further, the simulators available today exhibit a large domain gap to real data. This has resulted in an inability to draw clear conclusions from the rapidly growing body of research on end-to-e",
    "arxiv_url": "https://arxiv.org/abs/2406.15349v2",
    "pdf_url": "https://arxiv.org/pdf/2406.15349v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2406.15349",
    "arxiv_authors": [
      "Daniel Dauner",
      "Marcel Hallgarten",
      "Tianyu Li",
      "Xinshuo Weng",
      "Zhiyu Huang",
      "Zetong Yang",
      "Hongyang Li",
      "Igor Gilitschenski",
      "Boris Ivanovic",
      "Marco Pavone",
      "Andreas Geiger",
      "Kashyap Chitta"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=NAVSIM%3A+Data-Driven+Non-Reactive+Autonomous+Vehicle+Simulation+and+Benchmarking+Daniel+Dauner+Marcel+Hallgarten+Tianyu+Li+Xinshuo+Weng+Zhiyu+Huang",
    "gs_search_success": true,
    "gs_authors": [
      "tZqIYDcAAAAJ",
      "aLZEVCsAAAAJ",
      "oPiZSVYAAAAJ",
      "we6nXHoAAAAJ",
      "Hfrih1EAAAAJ",
      "ey9AQcEAAAAJ",
      "SpOCUvYAAAAJ",
      "Nuw1Y4oAAAAJ",
      "dthSEsoAAAAJ"
    ],
    "citation_count": 124,
    "gs_author_count": 9
  },
  {
    "arxiv_id": "2501.09465",
    "title": "RE-POSE: Synergizing Reinforcement Learning-Based Partitioning and Offloading for Edge Object Detection",
    "year": 2025,
    "published": "2025-01-16T10:56:45Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.DC"
    ],
    "abstract": "Object detection plays a crucial role in smart video analysis, with applications ranging from autonomous driving and security to smart cities. However, achieving real-time object detection on edge devices presents significant challenges due to their limited computational resources and the high demands of deep neural network (DNN)-based detection models, particularly when processing high-resolution video. Conventional strategies, such as input down-sampling and network up-scaling, often compromis",
    "arxiv_url": "https://arxiv.org/abs/2501.09465v1",
    "pdf_url": "https://arxiv.org/pdf/2501.09465v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2501.09465",
    "arxiv_authors": [
      "Jianrui Shi",
      "Yong Zhao",
      "Zeyang Cui",
      "Xiaoming Shen",
      "Minhang Zeng",
      "Xiaojie Liu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=RE-POSE%3A+Synergizing+Reinforcement+Learning-Based+Partitioning+and+Offloading+for+Edge+Object+Detection+Jianrui+Shi+Yong+Zhao+Zeyang+Cui+Xiaoming+Shen+Minhang+Zeng",
    "gs_search_success": false,
    "gs_authors": [],
    "citation_count": null,
    "error_type": "fetch_failed",
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2404.08549",
    "title": "Practical Guidelines for Cell Segmentation Models Under Optical Aberrations in Microscopy",
    "year": 2024,
    "published": "2024-04-12T15:45:26Z",
    "categories": [
      "eess.IV",
      "cs.CV",
      "physics.bio-ph"
    ],
    "abstract": "Cell segmentation is essential in biomedical research for analyzing cellular morphology and behavior. Deep learning methods, particularly convolutional neural networks (CNNs), have revolutionized cell segmentation by extracting intricate features from images. However, the robustness of these methods under microscope optical aberrations remains a critical challenge. This study evaluates cell image segmentation models under optical aberrations from fluorescence and bright field microscopy. By simu",
    "arxiv_url": "https://arxiv.org/abs/2404.08549v2",
    "pdf_url": "https://arxiv.org/pdf/2404.08549v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2404.08549",
    "arxiv_authors": [
      "Boyuan Peng",
      "Jiaju Chen",
      "P. Bilha Githinji",
      "Ijaz Gul",
      "Qihui Ye",
      "Minjiang Chen",
      "Peiwu Qin",
      "Xingru Huang",
      "Chenggang Yan",
      "Dongmei Yu",
      "Jiansong Ji",
      "Zhenglin Chen"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Practical+Guidelines+for+Cell+Segmentation+Models+Under+Optical+Aberrations+in+Microscopy+Boyuan+Peng+Jiaju+Chen+P.+Bilha+Githinji+Ijaz+Gul+Qihui+Ye",
    "gs_search_success": true,
    "gs_authors": [
      "7YjtKMcAAAAJ",
      "47mQF1wAAAAJ",
      "acPALtAAAAAJ",
      "nRXE2F4AAAAJ",
      "yD3IOXkAAAAJ",
      "FoCc2k4AAAAJ",
      "Tm3T-scAAAAJ",
      "IBb_duAAAAAJ"
    ],
    "citation_count": 39,
    "gs_author_count": 9
  },
  {
    "arxiv_id": "2505.09306",
    "title": "Predicting butterfly species presence from satellite imagery using soft contrastive regularisation",
    "year": 2025,
    "published": "2025-05-14T11:42:09Z",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "The growing demand for scalable biodiversity monitoring methods has fuelled interest in remote sensing data, due to its widespread availability and extensive coverage. Traditionally, the application of remote sensing to biodiversity research has focused on mapping and monitoring habitats, but with increasing availability of large-scale citizen-science wildlife observation data, recent methods have started to explore predicting multi-species presence directly from satellite images. This paper pre",
    "arxiv_url": "https://arxiv.org/abs/2505.09306v1",
    "pdf_url": "https://arxiv.org/pdf/2505.09306v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2505.09306",
    "arxiv_authors": [
      "Thijs L van der Plas",
      "Stephen Law",
      "Michael JO Pocock"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Predicting+butterfly+species+presence+from+satellite+imagery+using+soft+contrastive+regularisation+Thijs+L+van+der+Plas+Stephen+Law+Michael+JO+Pocock",
    "gs_search_success": true,
    "gs_authors": [
      "UynY1xQAAAAJ",
      "Gr1QKbMAAAAJ",
      "IBnzFskAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2312.08168",
    "title": "Chat-Scene: Bridging 3D Scene and Large Language Models with Object Identifiers",
    "year": 2023,
    "published": "2023-12-13T14:27:45Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Recent advancements in 3D Large Language Models (LLMs) have demonstrated promising capabilities for 3D scene understanding. However, previous methods exhibit deficiencies in general referencing and grounding capabilities for intricate scene comprehension. In this paper, we introduce the use of object identifiers and object-centric representations to interact with scenes at the object level. Specifically, we decompose the input 3D scene into a set of object proposals, each assigned a unique ident",
    "arxiv_url": "https://arxiv.org/abs/2312.08168v4",
    "pdf_url": "https://arxiv.org/pdf/2312.08168v4",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2312.08168",
    "arxiv_authors": [
      "Haifeng Huang",
      "Yilun Chen",
      "Zehan Wang",
      "Rongjie Huang",
      "Runsen Xu",
      "Tai Wang",
      "Luping Liu",
      "Xize Cheng",
      "Yang Zhao",
      "Jiangmiao Pang",
      "Zhou Zhao"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Chat-Scene%3A+Bridging+3D+Scene+and+Large+Language+Models+with+Object+Identifiers+Haifeng+Huang+Yilun+Chen+Zehan+Wang+Rongjie+Huang+Runsen+Xu",
    "gs_search_success": true,
    "gs_authors": [
      "7w1U0l4AAAAJ",
      "MOobrCcAAAAJ",
      "oUm2gZUAAAAJ",
      "ssSfKpAAAAAJ",
      "iRHBUsgAAAAJ",
      "uPmTOHAAAAAJ",
      "gKXC9Q8AAAAJ",
      "DIHzHQYAAAAJ",
      "JmbbZWIAAAAJ",
      "IIoFY90AAAAJ",
      "euXK0lkAAAAJ"
    ],
    "citation_count": 94,
    "gs_author_count": 9
  },
  {
    "arxiv_id": "2309.17342",
    "title": "Towards Free Data Selection with General-Purpose Models",
    "year": 2023,
    "published": "2023-09-29T15:50:14Z",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "A desirable data selection algorithm can efficiently choose the most informative samples to maximize the utility of limited annotation budgets. However, current approaches, represented by active learning methods, typically follow a cumbersome pipeline that iterates the time-consuming model training and batch data selection repeatedly. In this paper, we challenge this status quo by designing a distinct data selection pipeline that utilizes existing general-purpose models to select data from vario",
    "arxiv_url": "https://arxiv.org/abs/2309.17342v2",
    "pdf_url": "https://arxiv.org/pdf/2309.17342v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2309.17342",
    "arxiv_authors": [
      "Yichen Xie",
      "Mingyu Ding",
      "Masayoshi Tomizuka",
      "Wei Zhan"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Towards+Free+Data+Selection+with+General-Purpose+Models+Yichen+Xie+Mingyu+Ding+Masayoshi+Tomizuka+Wei+Zhan",
    "gs_search_success": true,
    "gs_authors": [
      "xVN3UxYAAAAJ",
      "SdX6DaEAAAAJ",
      "w4yTWwoAAAAJ",
      "8m8taGEAAAAJ"
    ],
    "citation_count": 18,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2307.04231",
    "title": "Mx2M: Masked Cross-Modality Modeling in Domain Adaptation for 3D Semantic Segmentation",
    "year": 2023,
    "published": "2023-07-09T17:13:14Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Existing methods of cross-modal domain adaptation for 3D semantic segmentation predict results only via 2D-3D complementarity that is obtained by cross-modal feature matching. However, as lacking supervision in the target domain, the complementarity is not always reliable. The results are not ideal when the domain gap is large. To solve the problem of lacking supervision, we introduce masked modeling into this task and propose a method Mx2M, which utilizes masked cross-modality modeling to reduc",
    "arxiv_url": "https://arxiv.org/abs/2307.04231v1",
    "pdf_url": "https://arxiv.org/pdf/2307.04231v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2307.04231",
    "arxiv_authors": [
      "Boxiang Zhang",
      "Zunran Wang",
      "Yonggen Ling",
      "Yuanyuan Guan",
      "Shenghao Zhang",
      "Wenhui Li"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Mx2M%3A+Masked+Cross-Modality+Modeling+in+Domain+Adaptation+for+3D+Semantic+Segmentation+Boxiang+Zhang+Zunran+Wang+Yonggen+Ling+Yuanyuan+Guan+Shenghao+Zhang",
    "gs_search_success": true,
    "gs_authors": [
      "P2HB2bsAAAAJ",
      "EE6lyhcAAAAJ",
      "4Tu7AgQAAAAJ"
    ],
    "citation_count": 15,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2405.03702",
    "title": "Leafy Spurge Dataset: Real-world Weed Classification Within Aerial Drone Imagery",
    "year": 2024,
    "published": "2024-05-02T23:53:29Z",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "Invasive plant species are detrimental to the ecology of both agricultural and wildland areas. Euphorbia esula, or leafy spurge, is one such plant that has spread through much of North America from Eastern Europe. When paired with contemporary computer vision systems, unmanned aerial vehicles, or drones, offer the means to track expansion of problem plants, such as leafy spurge, and improve chances of controlling these weeds. We gathered a dataset of leafy spurge presence and absence in grasslan",
    "arxiv_url": "https://arxiv.org/abs/2405.03702v2",
    "pdf_url": "https://arxiv.org/pdf/2405.03702v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2405.03702",
    "arxiv_authors": [
      "Kyle Doherty",
      "Max Gurinas",
      "Erik Samsoe",
      "Charles Casper",
      "Beau Larkin",
      "Philip Ramsey",
      "Brandon Trabucco",
      "Ruslan Salakhutdinov"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Leafy+Spurge+Dataset%3A+Real-world+Weed+Classification+Within+Aerial+Drone+Imagery+Kyle+Doherty+Max+Gurinas+Erik+Samsoe+Charles+Casper+Beau+Larkin",
    "gs_search_success": true,
    "gs_authors": [
      "ptNTEnMAAAAJ",
      "oiESPrEAAAAJ",
      "jOx6tvIAAAAJ",
      "aLquhd4AAAAJ",
      "iE1E44kAAAAJ"
    ],
    "citation_count": 3,
    "gs_author_count": 8
  },
  {
    "arxiv_id": "2303.12940",
    "title": "Cryptocurrency wallets: assessment and security",
    "year": 2023,
    "published": "2023-03-06T08:52:01Z",
    "categories": [
      "cs.CR",
      "cs.CV",
      "cs.DC",
      "cs.IT"
    ],
    "abstract": "Digital wallet as a software program or a digital device allows users to conduct various transactions. Hot and cold digital wallets are considered as two types of this wallet. Digital wallets need an online connection fall into the first group, whereas digital wallets can operate without internet connection belong to the second group. Prior to buying a digital wallet, it is important to define for what purpose it will be utilized. The ease with which a mobile phone transaction may be completed i",
    "arxiv_url": "https://arxiv.org/abs/2303.12940v1",
    "pdf_url": "https://arxiv.org/pdf/2303.12940v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2303.12940",
    "arxiv_authors": [
      "Ehsan Nowroozi",
      "Seyedsadra Seyedshoari",
      "Yassine Mekdad",
      "Erkay Savas",
      "Mauro Conti"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Cryptocurrency+wallets%3A+assessment+and+security+Ehsan+Nowroozi+Seyedsadra+Seyedshoari+Yassine+Mekdad+Erkay+Savas+Mauro+Conti",
    "gs_search_success": true,
    "gs_authors": [
      "C0bNkP8AAAAJ",
      "LaSyv7EAAAAJ",
      "biKy5tsAAAAJ",
      "0BcsOY8AAAAJ"
    ],
    "citation_count": 15,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2412.09959",
    "title": "Towards Consistent and Efficient Dataset Distillation via Diffusion-Driven Selection",
    "year": 2024,
    "published": "2024-12-13T08:34:46Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Dataset distillation provides an effective approach to reduce memory and computational costs by optimizing a compact dataset that achieves performance comparable to the full original. However, for large-scale datasets and complex deep networks (e.g., ImageNet-1K with ResNet-101), the vast optimization space hinders distillation effectiveness, limiting practical applications. Recent methods leverage pre-trained diffusion models to directly generate informative images, thereby bypassing pixel-leve",
    "arxiv_url": "https://arxiv.org/abs/2412.09959v4",
    "pdf_url": "https://arxiv.org/pdf/2412.09959v4",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2412.09959",
    "arxiv_authors": [
      "Xinhao Zhong",
      "Shuoyang Sun",
      "Xulin Gu",
      "Zhaoyang Xu",
      "Yaowei Wang",
      "Min Zhang",
      "Bin Chen"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Towards+Consistent+and+Efficient+Dataset+Distillation+via+Diffusion-Driven+Selection+Xinhao+Zhong+Shuoyang+Sun+Xulin+Gu+Zhaoyang+Xu+Yaowei+Wang",
    "gs_search_success": true,
    "gs_authors": [
      "739oeSkAAAAJ",
      "jxmwDaQAAAAJ",
      "Yl0wv7AAAAAJ",
      "RFvD0Q4AAAAJ"
    ],
    "citation_count": 8,
    "gs_author_count": 7
  },
  {
    "arxiv_id": "2502.13883",
    "title": "Multi-view Video-Pose Pretraining for Operating Room Surgical Activity Recognition",
    "year": 2025,
    "published": "2025-02-19T17:08:04Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Understanding the workflow of surgical procedures in complex operating rooms requires a deep understanding of the interactions between clinicians and their environment. Surgical activity recognition (SAR) is a key computer vision task that detects activities or phases from multi-view camera recordings. Existing SAR models often fail to account for fine-grained clinician movements and multi-view knowledge, or they require calibrated multi-view camera setups and advanced point-cloud processing to ",
    "arxiv_url": "https://arxiv.org/abs/2502.13883v1",
    "pdf_url": "https://arxiv.org/pdf/2502.13883v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2502.13883",
    "arxiv_authors": [
      "Idris Hamoud",
      "Vinkle Srivastav",
      "Muhammad Abdullah Jamal",
      "Didier Mutter",
      "Omid Mohareri",
      "Nicolas Padoy"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Multi-view+Video-Pose+Pretraining+for+Operating+Room+Surgical+Activity+Recognition+Idris+Hamoud+Vinkle+Srivastav+Muhammad+Abdullah+Jamal+Didier+Mutter+Omid+Mohareri",
    "gs_search_success": true,
    "gs_authors": [
      "gWNpXlMAAAAJ",
      "FDWwdoQAAAAJ",
      "aj84iHAAAAAJ",
      "xwd0d-EAAAAJ",
      "-t_yjBIAAAAJ",
      "Af14BLoAAAAJ"
    ],
    "citation_count": 3,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2501.05688",
    "title": "eKalibr: Dynamic Intrinsic Calibration for Event Cameras From First Principles of Events",
    "year": 2025,
    "published": "2025-01-10T03:41:03Z",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "abstract": "The bio-inspired event camera has garnered extensive research attention in recent years, owing to its significant potential derived from its high dynamic range and low latency characteristics. Similar to the standard camera, the event camera requires precise intrinsic calibration to facilitate further high-level visual applications, such as pose estimation and mapping. While several calibration methods for event cameras have been proposed, most of them are either (i) engineering-driven, heavily ",
    "arxiv_url": "https://arxiv.org/abs/2501.05688v2",
    "pdf_url": "https://arxiv.org/pdf/2501.05688v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2501.05688",
    "arxiv_authors": [
      "Shuolong Chen",
      "Xingxing Li",
      "Liu Yuan",
      "Ziao Liu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=eKalibr%3A+Dynamic+Intrinsic+Calibration+for+Event+Cameras+From+First+Principles+of+Events+Shuolong+Chen+Xingxing+Li+Liu+Yuan+Ziao+Liu",
    "gs_search_success": true,
    "gs_authors": [
      "pyQbVf4AAAAJ"
    ],
    "citation_count": 3,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2409.19051",
    "title": "Multimodal Markup Document Models for Graphic Design Completion",
    "year": 2024,
    "published": "2024-09-27T18:00:01Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.MM"
    ],
    "abstract": "This paper presents multimodal markup document models (MarkupDM) that can generate both markup language and images within interleaved multimodal documents. Unlike existing vision-and-language multimodal models, our MarkupDM tackles unique challenges critical to graphic design tasks: generating partial images that contribute to the overall appearance, often involving transparency and varying sizes, and understanding the syntax and semantics of markup languages, which play a fundamental role as a ",
    "arxiv_url": "https://arxiv.org/abs/2409.19051v1",
    "pdf_url": "https://arxiv.org/pdf/2409.19051v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2409.19051",
    "arxiv_authors": [
      "Kotaro Kikuchi",
      "Naoto Inoue",
      "Mayu Otani",
      "Edgar Simo-Serra",
      "Kota Yamaguchi"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Multimodal+Markup+Document+Models+for+Graphic+Design+Completion+Kotaro+Kikuchi+Naoto+Inoue+Mayu+Otani+Edgar+Simo-Serra+Kota+Yamaguchi",
    "gs_search_success": true,
    "gs_authors": [
      "PfX-_GkAAAAJ",
      "2bDu3ecAAAAJ",
      "hTqGOekAAAAJ",
      "Cf48JmIAAAAJ",
      "mYDpPnYAAAAJ",
      "V5eSgKUAAAAJ"
    ],
    "citation_count": 8,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2407.08572",
    "title": "Boosting Adversarial Transferability for Skeleton-based Action Recognition via Exploring the Model Posterior Space",
    "year": 2024,
    "published": "2024-07-11T14:59:31Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Skeletal motion plays a pivotal role in human activity recognition (HAR). Recently, attack methods have been proposed to identify the universal vulnerability of skeleton-based HAR(S-HAR). However, the research of adversarial transferability on S-HAR is largely missing. More importantly, existing attacks all struggle in transfer across unknown S-HAR models. We observed that the key reason is that the loss landscape of the action recognizers is rugged and sharp. Given the established correlation i",
    "arxiv_url": "https://arxiv.org/abs/2407.08572v2",
    "pdf_url": "https://arxiv.org/pdf/2407.08572v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2407.08572",
    "arxiv_authors": [
      "Yunfeng Diao",
      "Baiqi Wu",
      "Ruixuan Zhang",
      "Xun Yang",
      "Meng Wang",
      "He Wang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Boosting+Adversarial+Transferability+for+Skeleton-based+Action+Recognition+via+Exploring+the+Model+Posterior+Space+Yunfeng+Diao+Baiqi+Wu+Ruixuan+Zhang+Xun+Yang+Meng+Wang",
    "gs_search_success": true,
    "gs_authors": [],
    "citation_count": null,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2503.08497",
    "title": "MMRL: Multi-Modal Representation Learning for Vision-Language Models",
    "year": 2025,
    "published": "2025-03-11T14:48:01Z",
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "abstract": "Large-scale pre-trained Vision-Language Models (VLMs) have become essential for transfer learning across diverse tasks. However, adapting these models with limited few-shot data often leads to overfitting, diminishing their performance on new tasks. To tackle this issue, we propose a novel Multi-Modal Representation Learning (MMRL) framework that introduces a shared, learnable, and modality-agnostic representation space. MMRL projects the space tokens to text and image representation tokens, fac",
    "arxiv_url": "https://arxiv.org/abs/2503.08497v2",
    "pdf_url": "https://arxiv.org/pdf/2503.08497v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2503.08497",
    "arxiv_authors": [
      "Yuncheng Guo",
      "Xiaodong Gu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=MMRL%3A+Multi-Modal+Representation+Learning+for+Vision-Language+Models+Yuncheng+Guo+Xiaodong+Gu",
    "gs_search_success": true,
    "gs_authors": [],
    "citation_count": 18,
    "gs_author_count": 2
  },
  {
    "arxiv_id": "2503.23355",
    "title": "DSPFusion: Image Fusion via Degradation and Semantic Dual-Prior Guidance",
    "year": 2025,
    "published": "2025-03-30T08:18:50Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Existing fusion methods are tailored for high-quality images but struggle with degraded images captured under harsh circumstances, thus limiting the practical potential of image fusion. This work presents a \\textbf{D}egradation and \\textbf{S}emantic \\textbf{P}rior dual-guided framework for degraded image \\textbf{Fusion} (\\textbf{DSPFusion}), utilizing degradation priors and high-quality scene semantic priors restored via diffusion models to guide both information recovery and fusion in a unified",
    "arxiv_url": "https://arxiv.org/abs/2503.23355v1",
    "pdf_url": "https://arxiv.org/pdf/2503.23355v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2503.23355",
    "arxiv_authors": [
      "Linfeng Tang",
      "Chunyu Li",
      "Guoqing Wang",
      "Yixuan Yuan",
      "Jiayi Ma"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=DSPFusion%3A+Image+Fusion+via+Degradation+and+Semantic+Dual-Prior+Guidance+Linfeng+Tang+Chunyu+Li+Guoqing+Wang+Yixuan+Yuan+Jiayi+Ma",
    "gs_search_success": true,
    "gs_authors": [
      "V08v5OEAAAAJ",
      "73trMQkAAAAJ",
      "Aho5Jv8AAAAJ",
      "PyRqpAsAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2407.12401",
    "title": "Geometric Remove-and-Retrain (GOAR): Coordinate-Invariant eXplainable AI Assessment",
    "year": 2024,
    "published": "2024-07-17T08:28:53Z",
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "abstract": "Identifying the relevant input features that have a critical influence on the output results is indispensable for the development of explainable artificial intelligence (XAI). Remove-and-Retrain (ROAR) is a widely accepted approach for assessing the importance of individual pixels by measuring changes in accuracy following their removal and subsequent retraining of the modified dataset. However, we uncover notable limitations in pixel-perturbation strategies. When viewed from a geometric perspec",
    "arxiv_url": "https://arxiv.org/abs/2407.12401v1",
    "pdf_url": "https://arxiv.org/pdf/2407.12401v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2407.12401",
    "arxiv_authors": [
      "Yong-Hyun Park",
      "Junghoon Seo",
      "Bomseok Park",
      "Seongsu Lee",
      "Junghyo Jo"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Geometric+Remove-and-Retrain+%28GOAR%29%3A+Coordinate-Invariant+eXplainable+AI+Assessment+Yong-Hyun+Park+Junghoon+Seo+Bomseok+Park+Seongsu+Lee+Junghyo+Jo",
    "gs_search_success": true,
    "gs_authors": [
      "h1QXLx0AAAAJ",
      "H8N2tHgAAAAJ",
      "9KBQk-YAAAAJ"
    ],
    "citation_count": 2,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2305.18769",
    "title": "DualVAE: Controlling Colours of Generated and Real Images",
    "year": 2023,
    "published": "2023-05-30T06:04:30Z",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "Colour controlled image generation and manipulation are of interest to artists and graphic designers. Vector Quantised Variational AutoEncoders (VQ-VAEs) with autoregressive (AR) prior are able to produce high quality images, but lack an explicit representation mechanism to control colour attributes. We introduce DualVAE, a hybrid representation model that provides such control by learning disentangled representations for colour and geometry. The geometry is represented by an image intensity map",
    "arxiv_url": "https://arxiv.org/abs/2305.18769v1",
    "pdf_url": "https://arxiv.org/pdf/2305.18769v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2305.18769",
    "arxiv_authors": [
      "Keerth Rathakumar",
      "David Liebowitz",
      "Christian Walder",
      "Kristen Moore",
      "Salil S. Kanhere"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=DualVAE%3A+Controlling+Colours+of+Generated+and+Real+Images+Keerth+Rathakumar+David+Liebowitz+Christian+Walder+Kristen+Moore+Salil+S.+Kanhere",
    "gs_search_success": true,
    "gs_authors": [
      "ugH_Wg4AAAAJ",
      "sgqmaPMAAAAJ",
      "uI20HykAAAAJ",
      "5hhvj-EAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2405.16947",
    "title": "Zero-Shot Video Semantic Segmentation based on Pre-Trained Diffusion Models",
    "year": 2024,
    "published": "2024-05-27T08:39:38Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "We introduce the first zero-shot approach for Video Semantic Segmentation (VSS) based on pre-trained diffusion models. A growing research direction attempts to employ diffusion models to perform downstream vision tasks by exploiting their deep understanding of image semantics. Yet, the majority of these approaches have focused on image-related tasks like semantic correspondence and segmentation, with less emphasis on video tasks such as VSS. Ideally, diffusion-based image semantic segmentation a",
    "arxiv_url": "https://arxiv.org/abs/2405.16947v1",
    "pdf_url": "https://arxiv.org/pdf/2405.16947v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2405.16947",
    "arxiv_authors": [
      "Qian Wang",
      "Abdelrahman Eldesokey",
      "Mohit Mendiratta",
      "Fangneng Zhan",
      "Adam Kortylewski",
      "Christian Theobalt",
      "Peter Wonka"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Zero-Shot+Video+Semantic+Segmentation+based+on+Pre-Trained+Diffusion+Models+Qian+Wang+Abdelrahman+Eldesokey+Mohit+Mendiratta+Fangneng+Zhan+Adam+Kortylewski",
    "gs_search_success": true,
    "gs_authors": [
      "eIWg8NMAAAAJ",
      "tRLUOBIAAAAJ",
      "9tP8jsUAAAAJ",
      "8zbcfzAAAAAJ",
      "Y3Bc8isAAAAJ",
      "0EKXSXgAAAAJ",
      "IhU9wjAAAAAJ"
    ],
    "citation_count": 6,
    "gs_author_count": 7
  },
  {
    "arxiv_id": "2309.02119",
    "title": "Hierarchical Masked 3D Diffusion Model for Video Outpainting",
    "year": 2023,
    "published": "2023-09-05T10:52:21Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Video outpainting aims to adequately complete missing areas at the edges of video frames. Compared to image outpainting, it presents an additional challenge as the model should maintain the temporal consistency of the filled area. In this paper, we introduce a masked 3D diffusion model for video outpainting. We use the technique of mask modeling to train the 3D diffusion model. This allows us to use multiple guide frames to connect the results of multiple video clip inferences, thus ensuring tem",
    "arxiv_url": "https://arxiv.org/abs/2309.02119v3",
    "pdf_url": "https://arxiv.org/pdf/2309.02119v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2309.02119",
    "arxiv_authors": [
      "Fanda Fan",
      "Chaoxu Guo",
      "Litong Gong",
      "Biao Wang",
      "Tiezheng Ge",
      "Yuning Jiang",
      "Chunjie Luo",
      "Jianfeng Zhan"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Hierarchical+Masked+3D+Diffusion+Model+for+Video+Outpainting+Fanda+Fan+Chaoxu+Guo+Litong+Gong+Biao+Wang+Tiezheng+Ge",
    "gs_search_success": true,
    "gs_authors": [
      "eqwfFyYAAAAJ",
      "Ltq_kHEAAAAJ",
      "6zI9GL4AAAAJ",
      "09N7Ql0AAAAJ",
      "Z7v-NA4AAAAJ",
      "_X_2OOQAAAAJ",
      "SZ7qRZMAAAAJ",
      "db5ZTlMAAAAJ"
    ],
    "citation_count": 26,
    "gs_author_count": 8
  },
  {
    "arxiv_id": "2407.04859",
    "title": "Hybrid Primal Sketch: Combining Analogy, Qualitative Representations, and Computer Vision for Scene Understanding",
    "year": 2024,
    "published": "2024-07-05T20:44:35Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "One of the purposes of perception is to bridge between sensors and conceptual understanding. Marr's Primal Sketch combined initial edge-finding with multiple downstream processes to capture aspects of visual perception such as grouping and stereopsis. Given the progress made in multiple areas of AI since then, we have developed a new framework inspired by Marr's work, the Hybrid Primal Sketch, which combines computer vision components into an ensemble to produce sketch-like entities which are th",
    "arxiv_url": "https://arxiv.org/abs/2407.04859v1",
    "pdf_url": "https://arxiv.org/pdf/2407.04859v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2407.04859",
    "arxiv_authors": [
      "Kenneth D. Forbus",
      "Kezhen Chen",
      "Wangcheng Xu",
      "Madeline Usher"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Hybrid+Primal+Sketch%3A+Combining+Analogy%2C+Qualitative+Representations%2C+and+Computer+Vision+for+Scene+Understanding+Kenneth+D.+Forbus+Kezhen+Chen+Wangcheng+Xu+Madeline+Usher",
    "gs_search_success": true,
    "gs_authors": [
      "GOE2PJkAAAAJ",
      "IdRYT-8AAAAJ"
    ],
    "citation_count": 2,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2307.04956",
    "title": "PKU-GoodsAD: A Supermarket Goods Dataset for Unsupervised Anomaly Detection and Segmentation",
    "year": 2023,
    "published": "2023-07-11T01:17:00Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Visual anomaly detection is essential and commonly used for many tasks in the field of computer vision. Recent anomaly detection datasets mainly focus on industrial automated inspection, medical image analysis and video surveillance. In order to broaden the application and research of anomaly detection in unmanned supermarkets and smart manufacturing, we introduce the supermarket goods anomaly detection (GoodsAD) dataset. It contains 6124 high-resolution images of 484 different appearance goods ",
    "arxiv_url": "https://arxiv.org/abs/2307.04956v2",
    "pdf_url": "https://arxiv.org/pdf/2307.04956v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2307.04956",
    "arxiv_authors": [
      "Jian Zhang",
      "Runwei Ding",
      "Miaoju Ban",
      "Ge Yang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=PKU-GoodsAD%3A+A+Supermarket+Goods+Dataset+for+Unsupervised+Anomaly+Detection+and+Segmentation+Jian+Zhang+Runwei+Ding+Miaoju+Ban+Ge+Yang",
    "gs_search_success": true,
    "gs_authors": [
      "VFnZyPn_1sgC",
      "OrodefUAAAAJ",
      "RZorbGsAAAAJ"
    ],
    "citation_count": 23,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2307.11906",
    "title": "Unveiling Vulnerabilities in Interpretable Deep Learning Systems with Query-Efficient Black-box Attacks",
    "year": 2023,
    "published": "2023-07-21T21:09:54Z",
    "categories": [
      "cs.CV",
      "cs.CR",
      "cs.LG"
    ],
    "abstract": "Deep learning has been rapidly employed in many applications revolutionizing many industries, but it is known to be vulnerable to adversarial attacks. Such attacks pose a serious threat to deep learning-based systems compromising their integrity, reliability, and trust. Interpretable Deep Learning Systems (IDLSes) are designed to make the system more transparent and explainable, but they are also shown to be susceptible to attacks. In this work, we propose a novel microbial genetic algorithm-bas",
    "arxiv_url": "https://arxiv.org/abs/2307.11906v1",
    "pdf_url": "https://arxiv.org/pdf/2307.11906v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2307.11906",
    "arxiv_authors": [
      "Eldor Abdukhamidov",
      "Mohammed Abuhamad",
      "Simon S. Woo",
      "Eric Chan-Tin",
      "Tamer Abuhmed"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Unveiling+Vulnerabilities+in+Interpretable+Deep+Learning+Systems+with+Query-Efficient+Black-box+Attacks+Eldor+Abdukhamidov+Mohammed+Abuhamad+Simon+S.+Woo+Eric+Chan-Tin+Tamer+Abuhmed",
    "gs_search_success": true,
    "gs_authors": [
      "Eleytz0AAAAJ",
      "pLC4l6YAAAAJ",
      "m-IpfOMAAAAJ",
      "mHnj60cAAAAJ",
      "V7FWdNoAAAAJ"
    ],
    "citation_count": 3,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2312.06495",
    "title": "Detecting Events in Crowds Through Changes in Geometrical Dimensions of Pedestrians",
    "year": 2023,
    "published": "2023-12-11T16:18:56Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Security is an important topic in our contemporary world, and the ability to automate the detection of any events of interest that can take place in a crowd is of great interest to a population. We hypothesize that the detection of events in videos is correlated with significant changes in pedestrian behaviors. In this paper, we examine three different scenarios of crowd behavior, containing both the cases where an event triggers a change in the behavior of the crowd and two video sequences wher",
    "arxiv_url": "https://arxiv.org/abs/2312.06495v1",
    "pdf_url": "https://arxiv.org/pdf/2312.06495v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2312.06495",
    "arxiv_authors": [
      "Matheus Schreiner Homrich da Silva",
      "Paulo Brossard de Souza Pinto Neto",
      "Rodolfo Migon Favaretto",
      "Soraia Raupp Musse"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Detecting+Events+in+Crowds+Through+Changes+in+Geometrical+Dimensions+of+Pedestrians+Matheus+Schreiner+Homrich+da+Silva+Paulo+Brossard+de+Souza+Pinto+Neto+Rodolfo+Migon+Favaretto+Soraia+Raupp+Musse",
    "gs_search_success": true,
    "gs_authors": [
      "wpVGOLZ_XC0C"
    ],
    "citation_count": 1,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2503.12049",
    "title": "TACO: Taming Diffusion for in-the-wild Video Amodal Completion",
    "year": 2025,
    "published": "2025-03-15T08:47:45Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Humans can infer complete shapes and appearances of objects from limited visual cues, relying on extensive prior knowledge of the physical world. However, completing partially observable objects while ensuring consistency across video frames remains challenging for existing models, especially for unstructured, in-the-wild videos. This paper tackles the task of Video Amodal Completion (VAC), which aims to generate the complete object consistently throughout the video given a visual prompt specify",
    "arxiv_url": "https://arxiv.org/abs/2503.12049v2",
    "pdf_url": "https://arxiv.org/pdf/2503.12049v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2503.12049",
    "arxiv_authors": [
      "Ruijie Lu",
      "Yixin Chen",
      "Yu Liu",
      "Jiaxiang Tang",
      "Junfeng Ni",
      "Diwen Wan",
      "Gang Zeng",
      "Siyuan Huang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=TACO%3A+Taming+Diffusion+for+in-the-wild+Video+Amodal+Completion+Ruijie+Lu+Yixin+Chen+Yu+Liu+Jiaxiang+Tang+Junfeng+Ni",
    "gs_search_success": true,
    "gs_authors": [
      "t1-cWnoAAAAJ",
      "FCdDIOQAAAAJ",
      "gWWaiWYAAAAJ",
      "RuHyY6gAAAAJ",
      "lPZW7NAAAAAJ",
      "wxo8_VYAAAAJ",
      "zOoHsAcAAAAJ",
      "1NN7Ee8AAAAJ"
    ],
    "citation_count": 8,
    "gs_author_count": 8
  },
  {
    "arxiv_id": "2305.15562",
    "title": "Let There Be Order: Rethinking Ordering in Autoregressive Graph Generation",
    "year": 2023,
    "published": "2023-05-24T20:52:34Z",
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "abstract": "Conditional graph generation tasks involve training a model to generate a graph given a set of input conditions. Many previous studies employ autoregressive models to incrementally generate graph components such as nodes and edges. However, as graphs typically lack a natural ordering among their components, converting a graph into a sequence of tokens is not straightforward. While prior works mostly rely on conventional heuristics or graph traversal methods like breadth-first search (BFS) or dep",
    "arxiv_url": "https://arxiv.org/abs/2305.15562v1",
    "pdf_url": "https://arxiv.org/pdf/2305.15562v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2305.15562",
    "arxiv_authors": [
      "Jie Bu",
      "Kazi Sajeed Mehrab",
      "Anuj Karpatne"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Let+There+Be+Order%3A+Rethinking+Ordering+in+Autoregressive+Graph+Generation+Jie+Bu+Kazi+Sajeed+Mehrab+Anuj+Karpatne",
    "gs_search_success": true,
    "gs_authors": [
      "rdh3gVMAAAAJ",
      "ogW9GEoAAAAJ",
      "aJdbHAoAAAAJ"
    ],
    "citation_count": 5,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2307.13375",
    "title": "Towards Unifying Anatomy Segmentation: Automated Generation of a Full-body CT Dataset via Knowledge Aggregation and Anatomical Guidelines",
    "year": 2023,
    "published": "2023-07-25T09:48:13Z",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "abstract": "In this study, we present a method for generating automated anatomy segmentation datasets using a sequential process that involves nnU-Net-based pseudo-labeling and anatomy-guided pseudo-label refinement. By combining various fragmented knowledge bases, we generate a dataset of whole-body CT scans with $142$ voxel-level labels for 533 volumes providing comprehensive anatomical coverage which experts have approved. Our proposed procedure does not rely on manual annotation during the label aggrega",
    "arxiv_url": "https://arxiv.org/abs/2307.13375v1",
    "pdf_url": "https://arxiv.org/pdf/2307.13375v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2307.13375",
    "arxiv_authors": [
      "Alexander Jaus",
      "Constantin Seibold",
      "Kelsey Hermann",
      "Alexandra Walter",
      "Kristina Giske",
      "Johannes Haubold",
      "Jens Kleesiek",
      "Rainer Stiefelhagen"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Towards+Unifying+Anatomy+Segmentation%3A+Automated+Generation+of+a+Full-body+CT+Dataset+via+Knowledge+Aggregation+and+Anatomical+Guidelines+Alexander+Jaus+Constantin+Seibold+Kelsey+Hermann+Alexandra+Walter+Kristina+Giske",
    "gs_search_success": true,
    "gs_authors": [
      "gHj3oLwAAAAJ",
      "RF0xnfsAAAAJ",
      "rSuG-f4AAAAJ"
    ],
    "citation_count": 30,
    "gs_author_count": 8
  },
  {
    "arxiv_id": "2409.15132",
    "title": "FusionRF: High-Fidelity Satellite Neural Radiance Fields from Multispectral and Panchromatic Acquisitions",
    "year": 2024,
    "published": "2024-09-23T15:38:03Z",
    "categories": [
      "cs.CV",
      "eess.IV"
    ],
    "abstract": "We introduce FusionRF, a novel framework for digital surface reconstruction from satellite multispectral and panchromatic images. Current work has demonstrated the increased accuracy of neural photogrammetry for surface reconstruction from optical satellite images compared to algorithmic methods. Common satellites produce both a panchromatic and multispectral image, which contain high spatial and spectral information respectively. Current neural reconstruction methods require multispectral image",
    "arxiv_url": "https://arxiv.org/abs/2409.15132v3",
    "pdf_url": "https://arxiv.org/pdf/2409.15132v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2409.15132",
    "arxiv_authors": [
      "Michael Sprintson",
      "Rama Chellappa",
      "Cheng Peng"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=FusionRF%3A+High-Fidelity+Satellite+Neural+Radiance+Fields+from+Multispectral+and+Panchromatic+Acquisitions+Michael+Sprintson+Rama+Chellappa+Cheng+Peng",
    "gs_search_success": true,
    "gs_authors": [
      "HiQ4T3wAAAAJ",
      "qhOYMZsAAAAJ",
      "L60tuywAAAAJ"
    ],
    "citation_count": 3,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2309.08129",
    "title": "Increasing diversity of omni-directional images generated from single image using cGAN based on MLPMixer",
    "year": 2023,
    "published": "2023-09-15T03:43:29Z",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "abstract": "This paper proposes a novel approach to generating omni-directional images from a single snapshot picture. The previous method has relied on the generative adversarial networks based on convolutional neural networks (CNN). Although this method has successfully generated omni-directional images, CNN has two drawbacks for this task. First, since a convolutional layer only processes a local area, it is difficult to propagate the information of an input snapshot picture embedded in the center of the",
    "arxiv_url": "https://arxiv.org/abs/2309.08129v1",
    "pdf_url": "https://arxiv.org/pdf/2309.08129v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2309.08129",
    "arxiv_authors": [
      "Atsuya Nakata",
      "Ryuto Miyazaki",
      "Takao Yamanaka"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Increasing+diversity+of+omni-directional+images+generated+from+single+image+using+cGAN+based+on+MLPMixer+Atsuya+Nakata+Ryuto+Miyazaki+Takao+Yamanaka",
    "gs_search_success": true,
    "gs_authors": [
      "CU1iXLMAAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2305.03049",
    "title": "NeuralEditor: Editing Neural Radiance Fields via Manipulating Point Clouds",
    "year": 2023,
    "published": "2023-05-04T17:59:40Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "This paper proposes NeuralEditor that enables neural radiance fields (NeRFs) natively editable for general shape editing tasks. Despite their impressive results on novel-view synthesis, it remains a fundamental challenge for NeRFs to edit the shape of the scene. Our key insight is to exploit the explicit point cloud representation as the underlying structure to construct NeRFs, inspired by the intuitive interpretation of NeRF rendering as a process that projects or \"plots\" the associated 3D poin",
    "arxiv_url": "https://arxiv.org/abs/2305.03049v1",
    "pdf_url": "https://arxiv.org/pdf/2305.03049v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2305.03049",
    "arxiv_authors": [
      "Jun-Kun Chen",
      "Jipeng Lyu",
      "Yu-Xiong Wang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=NeuralEditor%3A+Editing+Neural+Radiance+Fields+via+Manipulating+Point+Clouds+Jun-Kun+Chen+Jipeng+Lyu+Yu-Xiong+Wang",
    "gs_search_success": true,
    "gs_authors": [
      "T_Q-xDkAAAAJ",
      "_m5__wUAAAAJ",
      "k-oNGZ0AAAAJ"
    ],
    "citation_count": 51,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2407.06116",
    "title": "Data-driven Nucleus Subclassification on Colon H&E using Style-transferred Digital Pathology",
    "year": 2024,
    "published": "2024-05-15T19:33:35Z",
    "categories": [
      "eess.IV",
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "Understanding the way cells communicate, co-locate, and interrelate is essential to furthering our understanding of how the body functions. H&E is widely available, however, cell subtyping often requires expert knowledge and the use of specialized stains. To reduce the annotation burden, AI has been proposed for the classification of cells on H&E. For example, the recent Colon Nucleus Identification and Classification (CoNIC) Challenge focused on labeling 6 cell types on H&E of the colon. Howeve",
    "arxiv_url": "https://arxiv.org/abs/2407.06116v1",
    "pdf_url": "https://arxiv.org/pdf/2407.06116v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2407.06116",
    "arxiv_authors": [
      "Lucas W. Remedios",
      "Shunxing Bao",
      "Samuel W. Remedios",
      "Ho Hin Lee",
      "Leon Y. Cai",
      "Thomas Li",
      "Ruining Deng",
      "Nancy R. Newlin",
      "Adam M. Saunders",
      "Can Cui",
      "Jia Li",
      "Qi Liu",
      "Ken S. Lau",
      "Joseph T. Roland",
      "Mary K Washington",
      "Lori A. Coburn",
      "Keith T. Wilson",
      "Yuankai Huo",
      "Bennett A. Landman"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Data-driven+Nucleus+Subclassification+on+Colon+H%26E+using+Style-transferred+Digital+Pathology+Lucas+W.+Remedios+Shunxing+Bao+Samuel+W.+Remedios+Ho+Hin+Lee+Leon+Y.+Cai",
    "gs_search_success": true,
    "gs_authors": [
      "cwyRnqwAAAAJ",
      "H6TdLnoAAAAJ",
      "1-rnyuoAAAAJ",
      "j_JQvacAAAAJ",
      "orIX5CwAAAAJ",
      "r6HOOmgAAAAJ",
      "B3UojrgAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 9
  },
  {
    "arxiv_id": "2505.19147",
    "title": "Shifting AI Efficiency From Model-Centric to Data-Centric Compression",
    "year": 2025,
    "published": "2025-05-25T13:51:17Z",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV"
    ],
    "abstract": "The advancement of large language models (LLMs) and multi-modal LLMs (MLLMs) has historically relied on scaling model parameters. However, as hardware limits constrain further model growth, the primary computational bottleneck has shifted to the quadratic cost of self-attention over increasingly long sequences by ultra-long text contexts, high-resolution images, and extended videos. In this position paper, \\textbf{we argue that the focus of research for efficient artificial intelligence (AI) is ",
    "arxiv_url": "https://arxiv.org/abs/2505.19147v3",
    "pdf_url": "https://arxiv.org/pdf/2505.19147v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2505.19147",
    "arxiv_authors": [
      "Xuyang Liu",
      "Zichen Wen",
      "Shaobo Wang",
      "Junjie Chen",
      "Zhishan Tao",
      "Yubo Wang",
      "Tailai Chen",
      "Xiangqi Jin",
      "Chang Zou",
      "Yiyu Wang",
      "Chenfei Liao",
      "Xu Zheng",
      "Honggang Chen",
      "Weijia Li",
      "Xuming Hu",
      "Conghui He",
      "Linfeng Zhang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Shifting+AI+Efficiency+From+Model-Centric+to+Data-Centric+Compression+Xuyang+Liu+Zichen+Wen+Shaobo+Wang+Junjie+Chen+Zhishan+Tao",
    "gs_search_success": true,
    "gs_authors": [
      "CZ-qXbAAAAAJ",
      "viFDWtwAAAAJ",
      "9VhMC1QAAAAJ",
      "2ZlT5o0AAAAJ",
      "q3NWGzUAAAAJ",
      "yWm0cJsAAAAJ"
    ],
    "citation_count": 16,
    "gs_author_count": 9
  },
  {
    "arxiv_id": "2405.05258",
    "title": "Multi-Modal Data-Efficient 3D Scene Understanding for Autonomous Driving",
    "year": 2024,
    "published": "2024-05-08T17:59:53Z",
    "categories": [
      "cs.CV",
      "cs.LG",
      "cs.RO"
    ],
    "abstract": "Efficient data utilization is crucial for advancing 3D scene understanding in autonomous driving, where reliance on heavily human-annotated LiDAR point clouds challenges fully supervised methods. Addressing this, our study extends into semi-supervised learning for LiDAR semantic segmentation, leveraging the intrinsic spatial priors of driving scenes and multi-sensor complements to augment the efficacy of unlabeled datasets. We introduce LaserMix++, an evolved framework that integrates laser beam",
    "arxiv_url": "https://arxiv.org/abs/2405.05258v2",
    "pdf_url": "https://arxiv.org/pdf/2405.05258v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2405.05258",
    "arxiv_authors": [
      "Lingdong Kong",
      "Xiang Xu",
      "Jiawei Ren",
      "Wenwei Zhang",
      "Liang Pan",
      "Kai Chen",
      "Wei Tsang Ooi",
      "Ziwei Liu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Multi-Modal+Data-Efficient+3D+Scene+Understanding+for+Autonomous+Driving+Lingdong+Kong+Xiang+Xu+Jiawei+Ren+Wenwei+Zhang+Liang+Pan",
    "gs_search_success": true,
    "gs_authors": [
      "1UHZkksAAAAJ",
      "YUKPVCoAAAAJ",
      "QDXADSEAAAAJ",
      "lSDISOcAAAAJ",
      "-j1j7TkAAAAJ",
      "lc45xlcAAAAJ",
      "nFP2ldkAAAAJ"
    ],
    "citation_count": 67,
    "gs_author_count": 8
  },
  {
    "arxiv_id": "2407.17219",
    "title": "Graph Neural Networks: A suitable Alternative to MLPs in Latent 3D Medical Image Classification?",
    "year": 2024,
    "published": "2024-07-24T12:19:39Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Recent studies have underscored the capabilities of natural imaging foundation models to serve as powerful feature extractors, even in a zero-shot setting for medical imaging data. Most commonly, a shallow multi-layer perceptron (MLP) is appended to the feature extractor to facilitate end-to-end learning and downstream prediction tasks such as classification, thus representing the de facto standard. However, as graph neural networks (GNNs) have become a practicable choice for various tasks in me",
    "arxiv_url": "https://arxiv.org/abs/2407.17219v1",
    "pdf_url": "https://arxiv.org/pdf/2407.17219v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2407.17219",
    "arxiv_authors": [
      "Johannes Kiechle",
      "Daniel M. Lang",
      "Stefan M. Fischer",
      "Lina Felsner",
      "Jan C. Peeken",
      "Julia A. Schnabel"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Graph+Neural+Networks%3A+A+suitable+Alternative+to+MLPs+in+Latent+3D+Medical+Image+Classification%3F+Johannes+Kiechle+Daniel+M.+Lang+Stefan+M.+Fischer+Lina+Felsner+Jan+C.+Peeken",
    "gs_search_success": true,
    "gs_authors": [
      "AV04Hs4AAAAJ",
      "4zTzQX4AAAAJ",
      "9Sy85BAAAAAJ",
      "QOKARtUAAAAJ"
    ],
    "citation_count": 2,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2310.16102",
    "title": "Learned, uncertainty-driven adaptive acquisition for photon-efficient scanning microscopy",
    "year": 2023,
    "published": "2023-10-24T18:06:03Z",
    "categories": [
      "eess.IV",
      "cs.CV",
      "physics.optics"
    ],
    "abstract": "Scanning microscopy systems, such as confocal and multiphoton microscopy, are powerful imaging tools for probing deep into biological tissue. However, scanning systems have an inherent trade-off between acquisition time, field of view, phototoxicity, and image quality, often resulting in noisy measurements when fast, large field of view, and/or gentle imaging is needed. Deep learning could be used to denoise noisy microscopy measurements, but these algorithms can be prone to hallucination, which",
    "arxiv_url": "https://arxiv.org/abs/2310.16102v2",
    "pdf_url": "https://arxiv.org/pdf/2310.16102v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2310.16102",
    "arxiv_authors": [
      "Cassandra Tong Ye",
      "Jiashu Han",
      "Kunzan Liu",
      "Anastasios Angelopoulos",
      "Linda Griffith",
      "Kristina Monakhova",
      "Sixian You"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Learned%2C+uncertainty-driven+adaptive+acquisition+for+photon-efficient+scanning+microscopy+Cassandra+Tong+Ye+Jiashu+Han+Kunzan+Liu+Anastasios+Angelopoulos+Linda+Griffith",
    "gs_search_success": true,
    "gs_authors": [
      "nfX25MMAAAAJ",
      "vEYl-MUAAAAJ",
      "fWCoyDcAAAAJ",
      "X71o1ykAAAAJ",
      "wzifqNkAAAAJ",
      "gZ-RhocAAAAJ"
    ],
    "citation_count": 6,
    "gs_author_count": 7
  },
  {
    "arxiv_id": "2403.10778",
    "title": "HCF-Net: Hierarchical Context Fusion Network for Infrared Small Object Detection",
    "year": 2024,
    "published": "2024-03-16T02:45:42Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Infrared small object detection is an important computer vision task involving the recognition and localization of tiny objects in infrared images, which usually contain only a few pixels. However, it encounters difficulties due to the diminutive size of the objects and the generally complex backgrounds in infrared images. In this paper, we propose a deep learning method, HCF-Net, that significantly improves infrared small object detection performance through multiple practical modules. Specific",
    "arxiv_url": "https://arxiv.org/abs/2403.10778v1",
    "pdf_url": "https://arxiv.org/pdf/2403.10778v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2403.10778",
    "arxiv_authors": [
      "Shibiao Xu",
      "ShuChen Zheng",
      "Wenhao Xu",
      "Rongtao Xu",
      "Changwei Wang",
      "Jiguang Zhang",
      "Xiaoqiang Teng",
      "Ao Li",
      "Li Guo"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=HCF-Net%3A+Hierarchical+Context+Fusion+Network+for+Infrared+Small+Object+Detection+Shibiao+Xu+ShuChen+Zheng+Wenhao+Xu+Rongtao+Xu+Changwei+Wang",
    "gs_search_success": true,
    "gs_authors": [
      "FXZfajQAAAAJ",
      "htmrWvUAAAAJ",
      "2eo4NBsAAAAJ",
      "DnJKQI8AAAAJ",
      "QWjKxMQAAAAJ",
      "_IUq7ooAAAAJ"
    ],
    "citation_count": 188,
    "gs_author_count": 9
  },
  {
    "arxiv_id": "2309.09078",
    "title": "Unsupervised Green Object Tracker (GOT) without Offline Pre-training",
    "year": 2023,
    "published": "2023-09-16T19:00:56Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Supervised trackers trained on labeled data dominate the single object tracking field for superior tracking accuracy. The labeling cost and the huge computational complexity hinder their applications on edge devices. Unsupervised learning methods have also been investigated to reduce the labeling cost but their complexity remains high. Aiming at lightweight high-performance tracking, feasibility without offline pre-training, and algorithmic transparency, we propose a new single object tracking m",
    "arxiv_url": "https://arxiv.org/abs/2309.09078v1",
    "pdf_url": "https://arxiv.org/pdf/2309.09078v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2309.09078",
    "arxiv_authors": [
      "Zhiruo Zhou",
      "Suya You",
      "C. -C. Jay Kuo"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Unsupervised+Green+Object+Tracker+%28GOT%29+without+Offline+Pre-training+Zhiruo+Zhou+Suya+You+C.+-C.+Jay+Kuo",
    "gs_search_success": true,
    "gs_authors": [
      "81d60okAAAAJ",
      "2CPCVLkAAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2309.15495",
    "title": "Investigating the changes in BOLD responses during viewing of images with varied complexity: An fMRI time-series based analysis on human vision",
    "year": 2023,
    "published": "2023-09-27T08:46:09Z",
    "categories": [
      "cs.CV",
      "eess.SP"
    ],
    "abstract": "Functional MRI (fMRI) is widely used to examine brain functionality by detecting alteration in oxygenated blood flow that arises with brain activity. This work aims to investigate the neurological variation of human brain responses during viewing of images with varied complexity using fMRI time series (TS) analysis. Publicly available BOLD5000 dataset is used for this purpose which contains fMRI scans while viewing 5254 distinct images of diverse categories, drawn from three standard computer vi",
    "arxiv_url": "https://arxiv.org/abs/2309.15495v1",
    "pdf_url": "https://arxiv.org/pdf/2309.15495v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2309.15495",
    "arxiv_authors": [
      "Naveen Kanigiri",
      "Manohar Suggula",
      "Debanjali Bhattacharya",
      "Neelam Sinha"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Investigating+the+changes+in+BOLD+responses+during+viewing+of+images+with+varied+complexity%3A+An+fMRI+time-series+based+analysis+on+human+vision+Naveen+Kanigiri+Manohar+Suggula+Debanjali+Bhattacharya+Neelam+Sinha",
    "gs_search_success": true,
    "gs_authors": [
      "6A_OhfoAAAAJ",
      "cOoPNCQAAAAJ",
      "a0Jwd_cAAAAJ"
    ],
    "citation_count": 2,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2402.18447",
    "title": "Prompt-Driven Dynamic Object-Centric Learning for Single Domain Generalization",
    "year": 2024,
    "published": "2024-02-28T16:16:51Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Single-domain generalization aims to learn a model from single source domain data to achieve generalized performance on other unseen target domains. Existing works primarily focus on improving the generalization ability of static networks. However, static networks are unable to dynamically adapt to the diverse variations in different image scenes, leading to limited generalization capability. Different scenes exhibit varying levels of complexity, and the complexity of images further varies signi",
    "arxiv_url": "https://arxiv.org/abs/2402.18447v1",
    "pdf_url": "https://arxiv.org/pdf/2402.18447v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2402.18447",
    "arxiv_authors": [
      "Deng Li",
      "Aming Wu",
      "Yaowei Wang",
      "Yahong Han"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Prompt-Driven+Dynamic+Object-Centric+Learning+for+Single+Domain+Generalization+Deng+Li+Aming+Wu+Yaowei+Wang+Yahong+Han",
    "gs_search_success": true,
    "gs_authors": [
      "t4283loAAAAJ",
      "o_DllmIAAAAJ",
      "xjGiOx0AAAAJ"
    ],
    "citation_count": 30,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2504.08452",
    "title": "Road Grip Uncertainty Estimation Through Surface State Segmentation",
    "year": 2025,
    "published": "2025-04-11T11:28:00Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Slippery road conditions pose significant challenges for autonomous driving. Beyond predicting road grip, it is crucial to estimate its uncertainty reliably to ensure safe vehicle control. In this work, we benchmark several uncertainty prediction methods to assess their effectiveness for grip uncertainty estimation. Additionally, we propose a novel approach that leverages road surface state segmentation to predict grip uncertainty. Our method estimates a pixel-wise grip probability distribution ",
    "arxiv_url": "https://arxiv.org/abs/2504.08452v1",
    "pdf_url": "https://arxiv.org/pdf/2504.08452v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2504.08452",
    "arxiv_authors": [
      "Jyri Maanpää",
      "Julius Pesonen",
      "Iaroslav Melekhov",
      "Heikki Hyyti",
      "Juha Hyyppä"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Road+Grip+Uncertainty+Estimation+Through+Surface+State+Segmentation+Jyri+Maanp%C3%A4%C3%A4+Julius+Pesonen+Iaroslav+Melekhov+Heikki+Hyyti+Juha+Hyypp%C3%A4",
    "gs_search_success": true,
    "gs_authors": [
      "c7hUrgwAAAAJ",
      "DJsNg-0AAAAJ",
      "tKnUIIcAAAAJ",
      "BXNprrsAAAAJ",
      "doUGlWAAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2407.15719",
    "title": "GFE-Mamba: Mamba-based AD Multi-modal Progression Assessment via Generative Feature Extraction from MCI",
    "year": 2024,
    "published": "2024-07-22T15:22:33Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Alzheimer's Disease (AD) is a progressive, irreversible neurodegenerative disorder that often originates from Mild Cognitive Impairment (MCI). This progression results in significant memory loss and severely affects patients' quality of life. Clinical trials have consistently shown that early and targeted interventions for individuals with MCI may slow or even prevent the advancement of AD. Research indicates that accurate medical classification requires diverse multimodal data, including detail",
    "arxiv_url": "https://arxiv.org/abs/2407.15719v3",
    "pdf_url": "https://arxiv.org/pdf/2407.15719v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2407.15719",
    "arxiv_authors": [
      "Zhaojie Fang",
      "Shenghao Zhu",
      "Yifei Chen",
      "Binfeng Zou",
      "Fan Jia",
      "Chang Liu",
      "Xiang Feng",
      "Linwei Qiu",
      "Feiwei Qin",
      "Jin Fan",
      "Changbiao Chu",
      "Changmiao Wang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=GFE-Mamba%3A+Mamba-based+AD+Multi-modal+Progression+Assessment+via+Generative+Feature+Extraction+from+MCI+Zhaojie+Fang+Shenghao+Zhu+Yifei+Chen+Binfeng+Zou+Fan+Jia",
    "gs_search_success": true,
    "gs_authors": [
      "2MJO9UEAAAAJ",
      "47KhMXEAAAAJ",
      "QpOCKMwAAAAJ",
      "0nLyK0kAAAAJ",
      "583w39sAAAAJ",
      "2Y06Jo4AAAAJ",
      "a0uWKuoAAAAJ"
    ],
    "citation_count": 10,
    "gs_author_count": 9
  },
  {
    "arxiv_id": "2405.00430",
    "title": "Continuous sPatial-Temporal Deformable Image Registration (CPT-DIR) for motion modelling in radiotherapy: beyond classic voxel-based methods",
    "year": 2024,
    "published": "2024-05-01T10:26:08Z",
    "categories": [
      "physics.med-ph",
      "cs.CV"
    ],
    "abstract": "Deformable image registration (DIR) is a crucial tool in radiotherapy for analyzing anatomical changes and motion patterns. Current DIR implementations rely on discrete volumetric motion representation, which often leads to compromised accuracy and uncertainty when handling significant anatomical changes and sliding boundaries. This limitation affects the reliability of subsequent contour propagation and dose accumulation procedures, particularly in regions with complex anatomical interfaces suc",
    "arxiv_url": "https://arxiv.org/abs/2405.00430v2",
    "pdf_url": "https://arxiv.org/pdf/2405.00430v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2405.00430",
    "arxiv_authors": [
      "Xia Li",
      "Runzhao Yang",
      "Muheng Li",
      "Xiangtai Li",
      "Antony J. Lomax",
      "Joachim M. Buhmann",
      "Ye Zhang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Continuous+sPatial-Temporal+Deformable+Image+Registration+%28CPT-DIR%29+for+motion+modelling+in+radiotherapy%3A+beyond+classic+voxel-based+methods+Xia+Li+Runzhao+Yang+Muheng+Li+Xiangtai+Li+Antony+J.+Lomax",
    "gs_search_success": true,
    "gs_authors": [
      "U3J4L6IAAAAJ",
      "zQWbCzYAAAAJ",
      "h4M2wyYAAAAJ",
      "FC5zGu4AAAAJ",
      "FL3ReD0AAAAJ"
    ],
    "citation_count": 8,
    "gs_author_count": 7
  },
  {
    "arxiv_id": "2308.04669",
    "title": "A General Implicit Framework for Fast NeRF Composition and Rendering",
    "year": 2023,
    "published": "2023-08-09T02:27:23Z",
    "categories": [
      "cs.CV",
      "cs.GR",
      "cs.LG"
    ],
    "abstract": "A variety of Neural Radiance Fields (NeRF) methods have recently achieved remarkable success in high render speed. However, current accelerating methods are specialized and incompatible with various implicit methods, preventing real-time composition over various types of NeRF works. Because NeRF relies on sampling along rays, it is possible to provide general guidance for acceleration. To that end, we propose a general implicit pipeline for composing NeRF objects quickly. Our method enables the ",
    "arxiv_url": "https://arxiv.org/abs/2308.04669v4",
    "pdf_url": "https://arxiv.org/pdf/2308.04669v4",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2308.04669",
    "arxiv_authors": [
      "Xinyu Gao",
      "Ziyi Yang",
      "Yunlu Zhao",
      "Yuxiang Sun",
      "Xiaogang Jin",
      "Changqing Zou"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=A+General+Implicit+Framework+for+Fast+NeRF+Composition+and+Rendering+Xinyu+Gao+Ziyi+Yang+Yunlu+Zhao+Yuxiang+Sun+Xiaogang+Jin",
    "gs_search_success": true,
    "gs_authors": [
      "kj5HiGgAAAAJ",
      "yryOvLwAAAAJ",
      "B0IyfqQAAAAJ"
    ],
    "citation_count": 7,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2501.05069",
    "title": "Commonsense Video Question Answering through Video-Grounded Entailment Tree Reasoning",
    "year": 2025,
    "published": "2025-01-09T08:44:42Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "This paper proposes the first video-grounded entailment tree reasoning method for commonsense video question answering (VQA). Despite the remarkable progress of large visual-language models (VLMs), there are growing concerns that they learn spurious correlations between videos and likely answers, reinforced by their black-box nature and remaining benchmarking biases. Our method explicitly grounds VQA tasks to video fragments in four steps: entailment tree construction, video-language entailment ",
    "arxiv_url": "https://arxiv.org/abs/2501.05069v2",
    "pdf_url": "https://arxiv.org/pdf/2501.05069v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2501.05069",
    "arxiv_authors": [
      "Huabin Liu",
      "Filip Ilievski",
      "Cees G. M. Snoek"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Commonsense+Video+Question+Answering+through+Video-Grounded+Entailment+Tree+Reasoning+Huabin+Liu+Filip+Ilievski+Cees+G.+M.+Snoek",
    "gs_search_success": true,
    "gs_authors": [
      "4ZScBc0AAAAJ",
      "hicoLSQAAAAJ",
      "0uKdbscAAAAJ"
    ],
    "citation_count": 3,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2303.10211",
    "title": "SITReg: Multi-resolution architecture for symmetric, inverse consistent, and topology preserving image registration",
    "year": 2023,
    "published": "2023-03-17T19:00:24Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Deep learning has emerged as a strong alternative for classical iterative methods for deformable medical image registration, where the goal is to find a mapping between the coordinate systems of two images. Popular classical image registration methods enforce the useful inductive biases of symmetricity, inverse consistency, and topology preservation by construction. However, while many deep learning registration methods encourage these properties via loss functions, no earlier methods enforce al",
    "arxiv_url": "https://arxiv.org/abs/2303.10211v5",
    "pdf_url": "https://arxiv.org/pdf/2303.10211v5",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2303.10211",
    "arxiv_authors": [
      "Joel Honkamaa",
      "Pekka Marttinen"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=SITReg%3A+Multi-resolution+architecture+for+symmetric%2C+inverse+consistent%2C+and+topology+preserving+image+registration+Joel+Honkamaa+Pekka+Marttinen",
    "gs_search_success": true,
    "gs_authors": [
      "id47-5cAAAAJ",
      "Da5sJakAAAAJ"
    ],
    "citation_count": 9,
    "gs_author_count": 2
  },
  {
    "arxiv_id": "2503.12827",
    "title": "GSBA$^K$: $top$-$K$ Geometric Score-based Black-box Attack",
    "year": 2025,
    "published": "2025-03-17T05:15:09Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Existing score-based adversarial attacks mainly focus on crafting $top$-1 adversarial examples against classifiers with single-label classification. Their attack success rate and query efficiency are often less than satisfactory, particularly under small perturbation requirements; moreover, the vulnerability of classifiers with multi-label learning is yet to be studied. In this paper, we propose a comprehensive surrogate free score-based attack, named \\b geometric \\b score-based \\b black-box \\b ",
    "arxiv_url": "https://arxiv.org/abs/2503.12827v3",
    "pdf_url": "https://arxiv.org/pdf/2503.12827v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2503.12827",
    "arxiv_authors": [
      "Md Farhamdur Reza",
      "Richeng Jin",
      "Tianfu Wu",
      "Huaiyu Dai"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=GSBA%24%5EK%24%3A+%24top%24-%24K%24+Geometric+Score-based+Black-box+Attack+Md+Farhamdur+Reza+Richeng+Jin+Tianfu+Wu+Huaiyu+Dai",
    "gs_search_success": true,
    "gs_authors": [
      "WXwrookAAAAJ",
      "8SeH2_8AAAAJ",
      "PgTnxeUAAAAJ"
    ],
    "citation_count": 2,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2306.06289",
    "title": "SegViTv2: Exploring Efficient and Continual Semantic Segmentation with Plain Vision Transformers",
    "year": 2023,
    "published": "2023-06-09T22:29:56Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "This paper investigates the capability of plain Vision Transformers (ViTs) for semantic segmentation using the encoder-decoder framework and introduces \\textbf{SegViTv2}. In this study, we introduce a novel Attention-to-Mask (\\atm) module to design a lightweight decoder effective for plain ViT. The proposed ATM converts the global attention map into semantic masks for high-quality segmentation results. Our decoder outperforms the popular decoder UPerNet using various ViT backbones while consumin",
    "arxiv_url": "https://arxiv.org/abs/2306.06289v2",
    "pdf_url": "https://arxiv.org/pdf/2306.06289v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2306.06289",
    "arxiv_authors": [
      "Bowen Zhang",
      "Liyang Liu",
      "Minh Hieu Phan",
      "Zhi Tian",
      "Chunhua Shen",
      "Yifan Liu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=SegViTv2%3A+Exploring+Efficient+and+Continual+Semantic+Segmentation+with+Plain+Vision+Transformers+Bowen+Zhang+Liyang+Liu+Minh+Hieu+Phan+Zhi+Tian+Chunhua+Shen",
    "gs_search_success": true,
    "gs_authors": [
      "xSF3BBoAAAAJ",
      "gSEw8EsAAAAJ",
      "Ljk2BvIAAAAJ",
      "ksQ4JnQAAAAJ",
      "1HjSeKgAAAAJ",
      "BdVznUwAAAAJ"
    ],
    "citation_count": 330,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2307.07393",
    "title": "L-DAWA: Layer-wise Divergence Aware Weight Aggregation in Federated Self-Supervised Visual Representation Learning",
    "year": 2023,
    "published": "2023-07-14T15:07:30Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "The ubiquity of camera-enabled devices has led to large amounts of unlabeled image data being produced at the edge. The integration of self-supervised learning (SSL) and federated learning (FL) into one coherent system can potentially offer data privacy guarantees while also advancing the quality and robustness of the learned visual representations without needing to move data around. However, client bias and divergence during FL aggregation caused by data heterogeneity limits the performance of",
    "arxiv_url": "https://arxiv.org/abs/2307.07393v1",
    "pdf_url": "https://arxiv.org/pdf/2307.07393v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2307.07393",
    "arxiv_authors": [
      "Yasar Abbas Ur Rehman",
      "Yan Gao",
      "Pedro Porto Buarque de Gusmão",
      "Mina Alibeigi",
      "Jiajun Shen",
      "Nicholas D. Lane"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=L-DAWA%3A+Layer-wise+Divergence+Aware+Weight+Aggregation+in+Federated+Self-Supervised+Visual+Representation+Learning+Yasar+Abbas+Ur+Rehman+Yan+Gao+Pedro+Porto+Buarque+de+Gusm%C3%A3o+Mina+Alibeigi+Jiajun+Shen",
    "gs_search_success": true,
    "gs_authors": [
      "TfdVttMAAAAJ",
      "qckHL1AAAAAJ",
      "gJA_rJYAAAAJ",
      "_im5GrcAAAAJ",
      "IleoLUgAAAAJ",
      "hDKFeWEAAAAJ"
    ],
    "citation_count": 43,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2304.06906",
    "title": "Swin3D: A Pretrained Transformer Backbone for 3D Indoor Scene Understanding",
    "year": 2023,
    "published": "2023-04-14T02:49:08Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "The use of pretrained backbones with fine-tuning has been successful for 2D vision and natural language processing tasks, showing advantages over task-specific networks. In this work, we introduce a pretrained 3D backbone, called {\\SST}, for 3D indoor scene understanding. We design a 3D Swin transformer as our backbone network, which enables efficient self-attention on sparse voxels with linear memory complexity, making the backbone scalable to large models and datasets. We also introduce a gene",
    "arxiv_url": "https://arxiv.org/abs/2304.06906v3",
    "pdf_url": "https://arxiv.org/pdf/2304.06906v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2304.06906",
    "arxiv_authors": [
      "Yu-Qi Yang",
      "Yu-Xiao Guo",
      "Jian-Yu Xiong",
      "Yang Liu",
      "Hao Pan",
      "Peng-Shuai Wang",
      "Xin Tong",
      "Baining Guo"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Swin3D%3A+A+Pretrained+Transformer+Backbone+for+3D+Indoor+Scene+Understanding+Yu-Qi+Yang+Yu-Xiao+Guo+Jian-Yu+Xiong+Yang+Liu+Hao+Pan",
    "gs_search_success": true,
    "gs_authors": [
      "ushTfxwAAAAJ",
      "-FcA6oIAAAAJ",
      "FCHG5J0AAAAJ",
      "0Z5Sdr8AAAAJ",
      "P91a-UQAAAAJ",
      "iczxr4UAAAAJ",
      "knqNKX0AAAAJ",
      "h4kYmRYAAAAJ"
    ],
    "citation_count": 151,
    "gs_author_count": 8
  },
  {
    "arxiv_id": "2406.01460",
    "title": "MLIP: Efficient Multi-Perspective Language-Image Pretraining with Exhaustive Data Utilization",
    "year": 2024,
    "published": "2024-06-03T15:49:11Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Contrastive Language-Image Pretraining (CLIP) has achieved remarkable success, leading to rapid advancements in multimodal studies. However, CLIP faces a notable challenge in terms of inefficient data utilization. It relies on a single contrastive supervision for each image-text pair during representation learning, disregarding a substantial amount of valuable information that could offer richer supervision. Additionally, the retention of non-informative tokens leads to increased computational d",
    "arxiv_url": "https://arxiv.org/abs/2406.01460v2",
    "pdf_url": "https://arxiv.org/pdf/2406.01460v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2406.01460",
    "arxiv_authors": [
      "Yu Zhang",
      "Qi Zhang",
      "Zixuan Gong",
      "Yiwei Shi",
      "Yepeng Liu",
      "Duoqian Miao",
      "Yang Liu",
      "Ke Liu",
      "Kun Yi",
      "Wei Fan",
      "Liang Hu",
      "Changwei Wang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=MLIP%3A+Efficient+Multi-Perspective+Language-Image+Pretraining+with+Exhaustive+Data+Utilization+Yu+Zhang+Qi+Zhang+Zixuan+Gong+Yiwei+Shi+Yepeng+Liu",
    "gs_search_success": true,
    "gs_authors": [
      "QvAC0OEAAAAJ",
      "nXaFcFUAAAAJ",
      "MhMZcIEAAAAJ",
      "DnJKQI8AAAAJ",
      "I7nvVHgAAAAJ",
      "cj6wAgYAAAAJ",
      "8UAk1p4AAAAJ"
    ],
    "citation_count": 6,
    "gs_author_count": 9
  },
  {
    "arxiv_id": "2301.08365",
    "title": "On Retrospective k-space Subsampling schemes For Deep MRI Reconstruction",
    "year": 2023,
    "published": "2023-01-20T00:05:18Z",
    "categories": [
      "eess.IV",
      "cs.CV",
      "physics.med-ph"
    ],
    "abstract": "Acquiring fully-sampled MRI $k$-space data is time-consuming, and collecting accelerated data can reduce the acquisition time. Employing 2D Cartesian-rectilinear subsampling schemes is a conventional approach for accelerated acquisitions; however, this often results in imprecise reconstructions, even with the use of Deep Learning (DL), especially at high acceleration factors. Non-rectilinear or non-Cartesian trajectories can be implemented in MRI scanners as alternative subsampling options. This",
    "arxiv_url": "https://arxiv.org/abs/2301.08365v5",
    "pdf_url": "https://arxiv.org/pdf/2301.08365v5",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2301.08365",
    "arxiv_authors": [
      "George Yiasemis",
      "Clara I. Sánchez",
      "Jan-Jakob Sonke",
      "Jonas Teuwen"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=On+Retrospective+k-space+Subsampling+schemes+For+Deep+MRI+Reconstruction+George+Yiasemis+Clara+I.+S%C3%A1nchez+Jan-Jakob+Sonke+Jonas+Teuwen",
    "gs_search_success": true,
    "gs_authors": [
      "GLGupPoAAAAJ",
      "Jz3tRZMAAAAJ",
      "YxMfdYEAAAAJ",
      "iyW85DwAAAAJ"
    ],
    "citation_count": 23,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2304.14252",
    "title": "FLAC: Fairness-Aware Representation Learning by Suppressing Attribute-Class Associations",
    "year": 2023,
    "published": "2023-04-27T15:10:46Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Bias in computer vision systems can perpetuate or even amplify discrimination against certain populations. Considering that bias is often introduced by biased visual datasets, many recent research efforts focus on training fair models using such data. However, most of them heavily rely on the availability of protected attribute labels in the dataset, which limits their applicability, while label-unaware approaches, i.e., approaches operating without such labels, exhibit considerably lower perfor",
    "arxiv_url": "https://arxiv.org/abs/2304.14252v2",
    "pdf_url": "https://arxiv.org/pdf/2304.14252v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2304.14252",
    "arxiv_authors": [
      "Ioannis Sarridis",
      "Christos Koutlis",
      "Symeon Papadopoulos",
      "Christos Diou"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=FLAC%3A+Fairness-Aware+Representation+Learning+by+Suppressing+Attribute-Class+Associations+Ioannis+Sarridis+Christos+Koutlis+Symeon+Papadopoulos+Christos+Diou",
    "gs_search_success": true,
    "gs_authors": [
      "USwu_AgAAAAJ",
      "mFQe8uQAAAAJ",
      "ymE72QIAAAAJ",
      "GuhyORoAAAAJ"
    ],
    "citation_count": 20,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2303.04203",
    "title": "What is Memory? A Homological Perspective",
    "year": 2023,
    "published": "2023-03-07T19:47:01Z",
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "abstract": "We introduce the delta-homology model of memory, a unified framework in which recall, learning, and prediction emerge from cycle closure, the completion of topologically constrained trajectories within the brain's latent manifold. A Dirac-like memory trace corresponds to a nontrivial homology generator, representing a sparse, irreducible attractor that reactivates only when inference trajectories close upon themselves. In this view, memory is not a static attractor landscape but a topological pr",
    "arxiv_url": "https://arxiv.org/abs/2303.04203v3",
    "pdf_url": "https://arxiv.org/pdf/2303.04203v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2303.04203",
    "arxiv_authors": [
      "Xin Li"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=What+is+Memory%3F+A+Homological+Perspective+Xin+Li",
    "gs_search_success": true,
    "gs_authors": [
      "h2SIH-QAAAAJ",
      "ZjuY8TEAAAAJ",
      "nQGCUhcAAAAJ",
      "PdCr1SUAAAAJ",
      "nlNpvqYAAAAJ",
      "gMBvzGoAAAAJ",
      "Ehr2BVQAAAAJ",
      "C361XpQAAAAJ",
      "Re4ML5UAAAAJ",
      "Z8ajcOMAAAAJ"
    ],
    "citation_count": 3,
    "gs_author_count": 1
  },
  {
    "arxiv_id": "2402.07243",
    "title": "PIVOT-Net: Heterogeneous Point-Voxel-Tree-based Framework for Point Cloud Compression",
    "year": 2024,
    "published": "2024-02-11T16:57:08Z",
    "categories": [
      "cs.CV",
      "eess.IV"
    ],
    "abstract": "The universality of the point cloud format enables many 3D applications, making the compression of point clouds a critical phase in practice. Sampled as discrete 3D points, a point cloud approximates 2D surface(s) embedded in 3D with a finite bit-depth. However, the point distribution of a practical point cloud changes drastically as its bit-depth increases, requiring different methodologies for effective consumption/analysis. In this regard, a heterogeneous point cloud compression (PCC) framewo",
    "arxiv_url": "https://arxiv.org/abs/2402.07243v1",
    "pdf_url": "https://arxiv.org/pdf/2402.07243v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2402.07243",
    "arxiv_authors": [
      "Jiahao Pang",
      "Kevin Bui",
      "Dong Tian"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=PIVOT-Net%3A+Heterogeneous+Point-Voxel-Tree-based+Framework+for+Point+Cloud+Compression+Jiahao+Pang+Kevin+Bui+Dong+Tian",
    "gs_search_success": true,
    "gs_authors": [
      "VsGLu-QAAAAJ",
      "JxRmaiwAAAAJ",
      "5JJknBgAAAAJ"
    ],
    "citation_count": 12,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2503.18420",
    "title": "Panorama Generation From NFoV Image Done Right",
    "year": 2025,
    "published": "2025-03-24T08:04:33Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Generating 360-degree panoramas from narrow field of view (NFoV) image is a promising computer vision task for Virtual Reality (VR) applications. Existing methods mostly assess the generated panoramas with InceptionNet or CLIP based metrics, which tend to perceive the image quality and is \\textbf{not suitable for evaluating the distortion}. In this work, we first propose a distortion-specific CLIP, named Distort-CLIP to accurately evaluate the panorama distortion and discover the \\textbf{``visua",
    "arxiv_url": "https://arxiv.org/abs/2503.18420v1",
    "pdf_url": "https://arxiv.org/pdf/2503.18420v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2503.18420",
    "arxiv_authors": [
      "Dian Zheng",
      "Cheng Zhang",
      "Xiao-Ming Wu",
      "Cao Li",
      "Chengfei Lv",
      "Jian-Fang Hu",
      "Wei-Shi Zheng"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Panorama+Generation+From+NFoV+Image+Done+Right+Dian+Zheng+Cheng+Zhang+Xiao-Ming+Wu+Cao+Li+Chengfei+Lv",
    "gs_search_success": true,
    "gs_authors": [
      "Li7oZpsAAAAJ",
      "AwqDDGoAAAAJ",
      "0dkD_dcAAAAJ",
      "2uEyZJQAAAAJ"
    ],
    "citation_count": 7,
    "gs_author_count": 7
  },
  {
    "arxiv_id": "2501.00358",
    "title": "Embodied VideoAgent: Persistent Memory from Egocentric Videos and Embodied Sensors Enables Dynamic Scene Understanding",
    "year": 2024,
    "published": "2024-12-31T09:22:38Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "This paper investigates the problem of understanding dynamic 3D scenes from egocentric observations, a key challenge in robotics and embodied AI. Unlike prior studies that explored this as long-form video understanding and utilized egocentric video only, we instead propose an LLM-based agent, Embodied VideoAgent, which constructs scene memory from both egocentric video and embodied sensory inputs (e.g. depth and pose sensing). We further introduce a VLM-based approach to automatically update the",
    "arxiv_url": "https://arxiv.org/abs/2501.00358v2",
    "pdf_url": "https://arxiv.org/pdf/2501.00358v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2501.00358",
    "arxiv_authors": [
      "Yue Fan",
      "Xiaojian Ma",
      "Rongpeng Su",
      "Jun Guo",
      "Rujie Wu",
      "Xi Chen",
      "Qing Li"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Embodied+VideoAgent%3A+Persistent+Memory+from+Egocentric+Videos+and+Embodied+Sensors+Enables+Dynamic+Scene+Understanding+Yue+Fan+Xiaojian+Ma+Rongpeng+Su+Jun+Guo+Rujie+Wu",
    "gs_search_success": true,
    "gs_authors": [
      "O7X6si8AAAAJ",
      "tuoTR7YAAAAJ",
      "xZec9goAAAAJ",
      "RP1FE2MAAAAJ",
      "qNmoYl8AAAAJ",
      "D1LEg-YAAAAJ",
      "rY-Ndl0AAAAJ"
    ],
    "citation_count": 5,
    "gs_author_count": 7
  },
  {
    "arxiv_id": "2312.12263",
    "title": "FedDiv: Collaborative Noise Filtering for Federated Learning with Noisy Labels",
    "year": 2023,
    "published": "2023-12-19T15:46:47Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Federated learning with noisy labels (F-LNL) aims at seeking an optimal server model via collaborative distributed learning by aggregating multiple client models trained with local noisy or clean samples. On the basis of a federated learning framework, recent advances primarily adopt label noise filtering to separate clean samples from noisy ones on each client, thereby mitigating the negative impact of label noise. However, these prior methods do not learn noise filters by exploiting knowledge ",
    "arxiv_url": "https://arxiv.org/abs/2312.12263v3",
    "pdf_url": "https://arxiv.org/pdf/2312.12263v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2312.12263",
    "arxiv_authors": [
      "Jichang Li",
      "Guanbin Li",
      "Hui Cheng",
      "Zicheng Liao",
      "Yizhou Yu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=FedDiv%3A+Collaborative+Noise+Filtering+for+Federated+Learning+with+Noisy+Labels+Jichang+Li+Guanbin+Li+Hui+Cheng+Zicheng+Liao+Yizhou+Yu",
    "gs_search_success": true,
    "gs_authors": [
      "RguLVJ8AAAAJ",
      "2A2Bx2UAAAAJ",
      "e38fTZQAAAAJ",
      "b8K5vcMAAAAJ"
    ],
    "citation_count": 31,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2411.02184",
    "title": "Double Descent Meets Out-of-Distribution Detection: Theoretical Insights and Empirical Analysis on the role of model complexity",
    "year": 2024,
    "published": "2024-11-04T15:39:12Z",
    "categories": [
      "stat.ML",
      "cs.AI",
      "cs.CV",
      "cs.LG",
      "math.ST"
    ],
    "abstract": "Out-of-distribution (OOD) detection is essential for ensuring the reliability and safety of machine learning systems. In recent years, it has received increasing attention, particularly through post-hoc detection and training-based methods. In this paper, we focus on post-hoc OOD detection, which enables identifying OOD samples without altering the model's training procedure or objective. Our primary goal is to investigate the relationship between model capacity and its OOD detection performance",
    "arxiv_url": "https://arxiv.org/abs/2411.02184v3",
    "pdf_url": "https://arxiv.org/pdf/2411.02184v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2411.02184",
    "arxiv_authors": [
      "Mouïn Ben Ammar",
      "David Brellmann",
      "Arturo Mendoza",
      "Antoine Manzanera",
      "Gianni Franchi"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Double+Descent+Meets+Out-of-Distribution+Detection%3A+Theoretical+Insights+and+Empirical+Analysis+on+the+role+of+model+complexity+Mou%C3%AFn+Ben+Ammar+David+Brellmann+Arturo+Mendoza+Antoine+Manzanera+Gianni+Franchi",
    "gs_search_success": true,
    "gs_authors": [
      "ZCW6-psAAAAJ",
      "YShgfpoAAAAJ",
      "xdA5-o4AAAAJ",
      "zBDRc_0AAAAJ",
      "YBJbAhoAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2505.08324",
    "title": "An incremental algorithm for non-convex AI-enhanced medical image processing",
    "year": 2025,
    "published": "2025-05-13T08:03:14Z",
    "categories": [
      "cs.CV",
      "math.NA"
    ],
    "abstract": "Solving non-convex regularized inverse problems is challenging due to their complex optimization landscapes and multiple local minima. However, these models remain widely studied as they often yield high-quality, task-oriented solutions, particularly in medical imaging, where the goal is to enhance clinically relevant features rather than merely minimizing global error. We propose incDG, a hybrid framework that integrates deep learning with incremental model-based optimization to efficiently app",
    "arxiv_url": "https://arxiv.org/abs/2505.08324v1",
    "pdf_url": "https://arxiv.org/pdf/2505.08324v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2505.08324",
    "arxiv_authors": [
      "Elena Morotti"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=An+incremental+algorithm+for+non-convex+AI-enhanced+medical+image+processing+Elena+Morotti",
    "gs_search_success": true,
    "gs_authors": [],
    "citation_count": null,
    "gs_author_count": 1
  },
  {
    "arxiv_id": "2501.15423",
    "title": "Stroke Lesion Segmentation using Multi-Stage Cross-Scale Attention",
    "year": 2025,
    "published": "2025-01-26T06:57:31Z",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "abstract": "Precise characterization of stroke lesions from MRI data has immense value in prognosticating clinical and cognitive outcomes following a stroke. Manual stroke lesion segmentation is time-consuming and requires the expertise of neurologists and neuroradiologists. Often, lesions are grossly characterized for their location and overall extent using bounding boxes without specific delineation of their boundaries. While such characterization provides some clinical value, to develop a precise mechani",
    "arxiv_url": "https://arxiv.org/abs/2501.15423v1",
    "pdf_url": "https://arxiv.org/pdf/2501.15423v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2501.15423",
    "arxiv_authors": [
      "Liang Shang",
      "William A. Sethares",
      "Anusha Adluru",
      "Andrew L. Alexander",
      "Vivek Prabhakaran",
      "Veena A. Nair",
      "Nagesh Adluru"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Stroke+Lesion+Segmentation+using+Multi-Stage+Cross-Scale+Attention+Liang+Shang+William+A.+Sethares+Anusha+Adluru+Andrew+L.+Alexander+Vivek+Prabhakaran",
    "gs_search_success": true,
    "gs_authors": [
      "aihMS60AAAAJ",
      "KVjj3ZwAAAAJ",
      "nTd0-lAAAAAJ",
      "vLOl6e0AAAAJ",
      "IuXLooMAAAAJ",
      "Z4IGgo4AAAAJ",
      "R_2UugQAAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 7
  },
  {
    "arxiv_id": "2401.00247",
    "title": "Probing the Limits and Capabilities of Diffusion Models for the Anatomic Editing of Digital Twins",
    "year": 2023,
    "published": "2023-12-30T14:21:30Z",
    "categories": [
      "cs.CV",
      "eess.IV"
    ],
    "abstract": "Numerical simulations can model the physical processes that govern cardiovascular device deployment. When such simulations incorporate digital twins; computational models of patient-specific anatomy, they can expedite and de-risk the device design process. Nonetheless, the exclusive use of patient-specific data constrains the anatomic variability which can be precisely or fully explored. In this study, we investigate the capacity of Latent Diffusion Models (LDMs) to edit digital twins to create ",
    "arxiv_url": "https://arxiv.org/abs/2401.00247v1",
    "pdf_url": "https://arxiv.org/pdf/2401.00247v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2401.00247",
    "arxiv_authors": [
      "Karim Kadry",
      "Shreya Gupta",
      "Farhad R. Nezami",
      "Elazer R. Edelman"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Probing+the+Limits+and+Capabilities+of+Diffusion+Models+for+the+Anatomic+Editing+of+Digital+Twins+Karim+Kadry+Shreya+Gupta+Farhad+R.+Nezami+Elazer+R.+Edelman",
    "gs_search_success": true,
    "gs_authors": [
      "rfePxA8AAAAJ",
      "ecxGghUAAAAJ",
      "n-1I8IIAAAAJ",
      "DIU4heEAAAAJ"
    ],
    "citation_count": 8,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2408.01471",
    "title": "Enhancing Online Road Network Perception and Reasoning with Standard Definition Maps",
    "year": 2024,
    "published": "2024-08-01T19:39:55Z",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "abstract": "Autonomous driving for urban and highway driving applications often requires High Definition (HD) maps to generate a navigation plan. Nevertheless, various challenges arise when generating and maintaining HD maps at scale. While recent online mapping methods have started to emerge, their performance especially for longer ranges is limited by heavy occlusion in dynamic environments. With these considerations in mind, our work focuses on leveraging lightweight and scalable priors-Standard Definiti",
    "arxiv_url": "https://arxiv.org/abs/2408.01471v1",
    "pdf_url": "https://arxiv.org/pdf/2408.01471v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2408.01471",
    "arxiv_authors": [
      "Hengyuan Zhang",
      "David Paz",
      "Yuliang Guo",
      "Arun Das",
      "Xinyu Huang",
      "Karsten Haug",
      "Henrik I. Christensen",
      "Liu Ren"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Enhancing+Online+Road+Network+Perception+and+Reasoning+with+Standard+Definition+Maps+Hengyuan+Zhang+David+Paz+Yuliang+Guo+Arun+Das+Xinyu+Huang",
    "gs_search_success": true,
    "gs_authors": [
      "6cM0AWEAAAAJ",
      "9zHvNqMAAAAJ",
      "MA8rI0MAAAAJ",
      "cL4bNBwAAAAJ",
      "JJhMod4AAAAJ",
      "CP-YkUwAAAAJ",
      "6NfC90UAAAAJ"
    ],
    "citation_count": 10,
    "gs_author_count": 8
  },
  {
    "arxiv_id": "2411.06347",
    "title": "Classification in Japanese Sign Language Based on Dynamic Facial Expressions",
    "year": 2024,
    "published": "2024-11-10T03:34:34Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Sign language is a visual language expressed through hand movements and non-manual markers. Non-manual markers include facial expressions and head movements. These expressions vary across different nations. Therefore, specialized analysis methods for each sign language are necessary. However, research on Japanese Sign Language (JSL) recognition is limited due to a lack of datasets. The development of recognition models that consider both manual and non-manual features of JSL is crucial for preci",
    "arxiv_url": "https://arxiv.org/abs/2411.06347v2",
    "pdf_url": "https://arxiv.org/pdf/2411.06347v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2411.06347",
    "arxiv_authors": [
      "Yui Tatsumi",
      "Shoko Tanaka",
      "Shunsuke Akamatsu",
      "Takahiro Shindo",
      "Hiroshi Watanabe"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Classification+in+Japanese+Sign+Language+Based+on+Dynamic+Facial+Expressions+Yui+Tatsumi+Shoko+Tanaka+Shunsuke+Akamatsu+Takahiro+Shindo+Hiroshi+Watanabe",
    "gs_search_success": true,
    "gs_authors": [
      "u1nK1wgAAAAJ",
      "Qs5XyiYAAAAJ",
      "14XgxpcAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2502.01401",
    "title": "Language-to-Space Programming for Training-Free 3D Visual Grounding",
    "year": 2025,
    "published": "2025-02-03T14:32:36Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "3D visual grounding (3DVG) is challenging due to the need to understand 3D spatial relations. While supervised approaches have achieved superior performance, they are constrained by the scarcity and high annotation costs of 3D vision-language datasets. Training-free approaches based on LLMs/VLMs eliminate the need for large-scale training data, but they either incur prohibitive grounding time and token costs or have unsatisfactory accuracy. To address the challenges, we introduce a novel method ",
    "arxiv_url": "https://arxiv.org/abs/2502.01401v4",
    "pdf_url": "https://arxiv.org/pdf/2502.01401v4",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2502.01401",
    "arxiv_authors": [
      "Boyu Mi",
      "Hanqing Wang",
      "Tai Wang",
      "Yilun Chen",
      "Jiangmiao Pang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Language-to-Space+Programming+for+Training-Free+3D+Visual+Grounding+Boyu+Mi+Hanqing+Wang+Tai+Wang+Yilun+Chen+Jiangmiao+Pang",
    "gs_search_success": true,
    "gs_authors": [
      "61VVLPoAAAAJ",
      "ssSfKpAAAAAJ",
      "gKXC9Q8AAAAJ",
      "JmbbZWIAAAAJ",
      "eNaVn_UAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2505.03554",
    "title": "Read My Ears! Horse Ear Movement Detection for Equine Affective State Assessment",
    "year": 2025,
    "published": "2025-05-06T14:05:49Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "The Equine Facial Action Coding System (EquiFACS) enables the systematic annotation of facial movements through distinct Action Units (AUs). It serves as a crucial tool for assessing affective states in horses by identifying subtle facial expressions associated with discomfort. However, the field of horse affective state assessment is constrained by the scarcity of annotated data, as manually labelling facial AUs is both time-consuming and costly. To address this challenge, automated annotation ",
    "arxiv_url": "https://arxiv.org/abs/2505.03554v1",
    "pdf_url": "https://arxiv.org/pdf/2505.03554v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2505.03554",
    "arxiv_authors": [
      "João Alves",
      "Pia Haubro Andersen",
      "Rikke Gade"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Read+My+Ears%21+Horse+Ear+Movement+Detection+for+Equine+Affective+State+Assessment+Jo%C3%A3o+Alves+Pia+Haubro+Andersen+Rikke+Gade",
    "gs_search_success": true,
    "gs_authors": [
      "x1D2CWQAAAAJ",
      "WqLUZY8AAAAJ",
      "0yaz_P8AAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2405.20067",
    "title": "N-Dimensional Gaussians for Fitting of High Dimensional Functions",
    "year": 2024,
    "published": "2024-05-30T13:56:58Z",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "abstract": "In the wake of many new ML-inspired approaches for reconstructing and representing high-quality 3D content, recent hybrid and explicitly learned representations exhibit promising performance and quality characteristics. However, their scaling to higher dimensions is challenging, e.g. when accounting for dynamic content with respect to additional parameters such as material properties, illumination, or time. In this paper, we tackle these challenges for an explicit representations based on Gaussi",
    "arxiv_url": "https://arxiv.org/abs/2405.20067v2",
    "pdf_url": "https://arxiv.org/pdf/2405.20067v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2405.20067",
    "arxiv_authors": [
      "Stavros Diolatzis",
      "Tobias Zirr",
      "Alexandr Kuznetsov",
      "Georgios Kopanas",
      "Anton Kaplanyan"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=N-Dimensional+Gaussians+for+Fitting+of+High+Dimensional+Functions+Stavros+Diolatzis+Tobias+Zirr+Alexandr+Kuznetsov+Georgios+Kopanas+Anton+Kaplanyan",
    "gs_search_success": true,
    "gs_authors": [
      "APbRQFoAAAAJ",
      "94EIDLcAAAAJ",
      "QLWLLHMAAAAJ",
      "xXpY4ckAAAAJ"
    ],
    "citation_count": 12,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2503.16408",
    "title": "RoboFactory: Exploring Embodied Agent Collaboration with Compositional Constraints",
    "year": 2025,
    "published": "2025-03-20T17:58:38Z",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "Designing effective embodied multi-agent systems is critical for solving complex real-world tasks across domains. Due to the complexity of multi-agent embodied systems, existing methods fail to automatically generate safe and efficient training data for such systems. To this end, we propose the concept of compositional constraints for embodied multi-agent systems, addressing the challenges arising from collaboration among embodied agents. We design various interfaces tailored to different types ",
    "arxiv_url": "https://arxiv.org/abs/2503.16408v1",
    "pdf_url": "https://arxiv.org/pdf/2503.16408v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2503.16408",
    "arxiv_authors": [
      "Yiran Qin",
      "Li Kang",
      "Xiufeng Song",
      "Zhenfei Yin",
      "Xiaohong Liu",
      "Xihui Liu",
      "Ruimao Zhang",
      "Lei Bai"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=RoboFactory%3A+Exploring+Embodied+Agent+Collaboration+with+Compositional+Constraints+Yiran+Qin+Li+Kang+Xiufeng+Song+Zhenfei+Yin+Xiaohong+Liu",
    "gs_search_success": true,
    "gs_authors": [
      "ngPR1dIAAAAJ",
      "4YL23GMAAAAJ",
      "Tq2hoMQAAAAJ",
      "sakOO04AAAAJ",
      "ZJwZdtgAAAAJ",
      "24a-y-4AAAAJ",
      "qO93EgIAAAAJ"
    ],
    "citation_count": 14,
    "gs_author_count": 8
  },
  {
    "arxiv_id": "2504.20466",
    "title": "LMME3DHF: Benchmarking and Evaluating Multimodal 3D Human Face Generation with LMMs",
    "year": 2025,
    "published": "2025-04-29T07:00:06Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "The rapid advancement in generative artificial intelligence have enabled the creation of 3D human faces (HFs) for applications including media production, virtual reality, security, healthcare, and game development, etc. However, assessing the quality and realism of these AI-generated 3D human faces remains a significant challenge due to the subjective nature of human perception and innate perceptual sensitivity to facial features. To this end, we conduct a comprehensive study on the quality ass",
    "arxiv_url": "https://arxiv.org/abs/2504.20466v3",
    "pdf_url": "https://arxiv.org/pdf/2504.20466v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2504.20466",
    "arxiv_authors": [
      "Woo Yi Yang",
      "Jiarui Wang",
      "Sijing Wu",
      "Huiyu Duan",
      "Yuxin Zhu",
      "Liu Yang",
      "Kang Fu",
      "Guangtao Zhai",
      "Xiongkuo Min"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=LMME3DHF%3A+Benchmarking+and+Evaluating+Multimodal+3D+Human+Face+Generation+with+LMMs+Woo+Yi+Yang+Jiarui+Wang+Sijing+Wu+Huiyu+Duan+Yuxin+Zhu",
    "gs_search_success": true,
    "gs_authors": [
      "-pD_qMYAAAAJ",
      "91sjuWIAAAAJ",
      "E6zbSYgAAAAJ",
      "lGt-KkwAAAAJ",
      "r0bRaCMAAAAJ"
    ],
    "citation_count": 8,
    "gs_author_count": 9
  },
  {
    "arxiv_id": "2310.07021",
    "title": "Pre-Trained Masked Image Model for Mobile Robot Navigation",
    "year": 2023,
    "published": "2023-10-10T21:16:29Z",
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "abstract": "2D top-down maps are commonly used for the navigation and exploration of mobile robots through unknown areas. Typically, the robot builds the navigation maps incrementally from local observations using onboard sensors. Recent works have shown that predicting the structural patterns in the environment through learning-based approaches can greatly enhance task efficiency. While many such works build task-specific networks using limited datasets, we show that the existing foundational vision networ",
    "arxiv_url": "https://arxiv.org/abs/2310.07021v2",
    "pdf_url": "https://arxiv.org/pdf/2310.07021v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2310.07021",
    "arxiv_authors": [
      "Vishnu Dutt Sharma",
      "Anukriti Singh",
      "Pratap Tokekar"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Pre-Trained+Masked+Image+Model+for+Mobile+Robot+Navigation+Vishnu+Dutt+Sharma+Anukriti+Singh+Pratap+Tokekar",
    "gs_search_success": true,
    "gs_authors": [
      "G4gza-gAAAAJ",
      "FKAovywAAAAJ",
      "SeRpU4YAAAAJ"
    ],
    "citation_count": 7,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2312.09553",
    "title": "Prompt-based Distribution Alignment for Unsupervised Domain Adaptation",
    "year": 2023,
    "published": "2023-12-15T06:15:04Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Recently, despite the unprecedented success of large pre-trained visual-language models (VLMs) on a wide range of downstream tasks, the real-world unsupervised domain adaptation (UDA) problem is still not well explored. Therefore, in this paper, we first experimentally demonstrate that the unsupervised-trained VLMs can significantly reduce the distribution discrepancy between source and target domains, thereby improving the performance of UDA. However, a major challenge for directly deploying su",
    "arxiv_url": "https://arxiv.org/abs/2312.09553v2",
    "pdf_url": "https://arxiv.org/pdf/2312.09553v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2312.09553",
    "arxiv_authors": [
      "Shuanghao Bai",
      "Min Zhang",
      "Wanqi Zhou",
      "Siteng Huang",
      "Zhirong Luan",
      "Donglin Wang",
      "Badong Chen"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Prompt-based+Distribution+Alignment+for+Unsupervised+Domain+Adaptation+Shuanghao+Bai+Min+Zhang+Wanqi+Zhou+Siteng+Huang+Zhirong+Luan",
    "gs_search_success": true,
    "gs_authors": [
      "xhd94DIAAAAJ",
      "mJNCeucAAAAJ",
      "CncXH-YAAAAJ",
      "mq6tPX4AAAAJ",
      "3Q_3PR8AAAAJ",
      "-fo6wdwAAAAJ",
      "mhpkWSYAAAAJ"
    ],
    "citation_count": 90,
    "gs_author_count": 7
  },
  {
    "arxiv_id": "2308.14650",
    "title": "Comparison of automated crater catalogs for Mars from Benedix et al. (2020) and Lee and Hogan (2021)",
    "year": 2023,
    "published": "2023-08-28T15:22:15Z",
    "categories": [
      "astro-ph.EP",
      "astro-ph.IM",
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "Crater mapping using neural networks and other automated methods has increased recently with automated Crater Detection Algorithms (CDAs) applied to planetary bodies throughout the solar system. A recent publication by Benedix et al. (2020) showed high performance at small scales compared to similar automated CDAs but with a net positive diameter bias in many crater candidates. I compare the publicly available catalogs from Benedix et al. (2020) and Lee & Hogan (2021) and show that the reported ",
    "arxiv_url": "https://arxiv.org/abs/2308.14650v1",
    "pdf_url": "https://arxiv.org/pdf/2308.14650v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2308.14650",
    "arxiv_authors": [
      "Christopher Lee"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Comparison+of+automated+crater+catalogs+for+Mars+from+Benedix+et+al.+%282020%29+and+Lee+and+Hogan+%282021%29+Christopher+Lee",
    "gs_search_success": true,
    "gs_authors": [
      "sBhyt4MAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 1
  },
  {
    "arxiv_id": "2501.03225",
    "title": "Automated Generation of Challenging Multiple-Choice Questions for Vision Language Model Evaluation",
    "year": 2025,
    "published": "2025-01-06T18:57:31Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.CY",
      "cs.LG"
    ],
    "abstract": "The rapid development of vision language models (VLMs) demands rigorous and reliable evaluation. However, current visual question answering (VQA) benchmarks often depend on open-ended questions, making accurate evaluation difficult due to the variability in natural language responses. To address this, we introduce AutoConverter, an agentic framework that automatically converts these open-ended questions into multiple-choice format, enabling objective evaluation while reducing the costly multiple",
    "arxiv_url": "https://arxiv.org/abs/2501.03225v2",
    "pdf_url": "https://arxiv.org/pdf/2501.03225v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2501.03225",
    "arxiv_authors": [
      "Yuhui Zhang",
      "Yuchang Su",
      "Yiming Liu",
      "Xiaohan Wang",
      "James Burgess",
      "Elaine Sui",
      "Chenyu Wang",
      "Josiah Aklilu",
      "Alejandro Lozano",
      "Anjiang Wei",
      "Ludwig Schmidt",
      "Serena Yeung-Levy"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Automated+Generation+of+Challenging+Multiple-Choice+Questions+for+Vision+Language+Model+Evaluation+Yuhui+Zhang+Yuchang+Su+Yiming+Liu+Xiaohan+Wang+James+Burgess",
    "gs_search_success": true,
    "gs_authors": [
      "iGA10XoAAAAJ",
      "W-Q0gJ8AAAAJ",
      "Kq0dhLAAAAAJ",
      "gqtV0bYAAAAJ",
      "JBqOK08AAAAJ",
      "X-Agfu8AAAAJ",
      "HoG_nZAAAAAJ",
      "tOEF7V8AAAAJ",
      "EBPnTFMAAAAJ",
      "R6GmdPIAAAAJ"
    ],
    "citation_count": 13,
    "gs_author_count": 9
  },
  {
    "arxiv_id": "2401.03870",
    "title": "Gramformer: Learning Crowd Counting via Graph-Modulated Transformer",
    "year": 2024,
    "published": "2024-01-08T13:01:54Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Transformer has been popular in recent crowd counting work since it breaks the limited receptive field of traditional CNNs. However, since crowd images always contain a large number of similar patches, the self-attention mechanism in Transformer tends to find a homogenized solution where the attention maps of almost all patches are identical. In this paper, we address this problem by proposing Gramformer: a graph-modulated transformer to enhance the network by adjusting the attention and input n",
    "arxiv_url": "https://arxiv.org/abs/2401.03870v1",
    "pdf_url": "https://arxiv.org/pdf/2401.03870v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2401.03870",
    "arxiv_authors": [
      "Hui Lin",
      "Zhiheng Ma",
      "Xiaopeng Hong",
      "Qinnan Shangguan",
      "Deyu Meng"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Gramformer%3A+Learning+Crowd+Counting+via+Graph-Modulated+Transformer+Hui+Lin+Zhiheng+Ma+Xiaopeng+Hong+Qinnan+Shangguan+Deyu+Meng",
    "gs_search_success": false,
    "gs_authors": [],
    "citation_count": null,
    "error_type": "fetch_failed",
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2411.17660",
    "title": "DROID-Splat: Combining end-to-end SLAM with 3D Gaussian Splatting",
    "year": 2024,
    "published": "2024-11-26T18:25:51Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Recent progress in scene synthesis makes standalone SLAM systems purely based on optimizing hyperprimitives with a Rendering objective possible. However, the tracking performance still lacks behind traditional and end-to-end SLAM systems. An optimal trade-off between robustness, speed and accuracy has not yet been reached, especially for monocular video. In this paper, we introduce a SLAM system based on an end-to-end Tracker and extend it with a Renderer based on recent 3D Gaussian Splatting te",
    "arxiv_url": "https://arxiv.org/abs/2411.17660v2",
    "pdf_url": "https://arxiv.org/pdf/2411.17660v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2411.17660",
    "arxiv_authors": [
      "Christian Homeyer",
      "Leon Begiristain",
      "Christoph Schnörr"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=DROID-Splat%3A+Combining+end-to-end+SLAM+with+3D+Gaussian+Splatting+Christian+Homeyer+Leon+Begiristain+Christoph+Schn%C3%B6rr",
    "gs_search_success": true,
    "gs_authors": [
      "6OIOuo4AAAAJ",
      "C-5j7CQAAAAJ"
    ],
    "citation_count": 4,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2308.05404",
    "title": "Enhancing Low-light Light Field Images with A Deep Compensation Unfolding Network",
    "year": 2023,
    "published": "2023-08-10T07:53:06Z",
    "categories": [
      "cs.CV",
      "eess.IV"
    ],
    "abstract": "This paper presents a novel and interpretable end-to-end learning framework, called the deep compensation unfolding network (DCUNet), for restoring light field (LF) images captured under low-light conditions. DCUNet is designed with a multi-stage architecture that mimics the optimization process of solving an inverse imaging problem in a data-driven fashion. The framework uses the intermediate enhanced result to estimate the illumination map, which is then employed in the unfolding process to pr",
    "arxiv_url": "https://arxiv.org/abs/2308.05404v3",
    "pdf_url": "https://arxiv.org/pdf/2308.05404v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2308.05404",
    "arxiv_authors": [
      "Xianqiang Lyu",
      "Junhui Hou"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Enhancing+Low-light+Light+Field+Images+with+A+Deep+Compensation+Unfolding+Network+Xianqiang+Lyu+Junhui+Hou",
    "gs_search_success": true,
    "gs_authors": [
      "fgWrxaIAAAAJ",
      "j6eefhwAAAAJ"
    ],
    "citation_count": 17,
    "gs_author_count": 2
  },
  {
    "arxiv_id": "2310.00132",
    "title": "QDFormer: Towards Robust Audiovisual Segmentation in Complex Environments with Quantization-based Semantic Decomposition",
    "year": 2023,
    "published": "2023-09-29T20:48:44Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Audiovisual segmentation (AVS) is a challenging task that aims to segment visual objects in videos according to their associated acoustic cues. With multiple sound sources and background disturbances involved, establishing robust correspondences between audio and visual contents poses unique challenges due to (1) complex entanglement across sound sources and (2) frequent changes in the occurrence of distinct sound events. Assuming sound events occur independently, the multi-source semantic space",
    "arxiv_url": "https://arxiv.org/abs/2310.00132v3",
    "pdf_url": "https://arxiv.org/pdf/2310.00132v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2310.00132",
    "arxiv_authors": [
      "Xiang Li",
      "Jinglu Wang",
      "Xiaohao Xu",
      "Xiulian Peng",
      "Rita Singh",
      "Yan Lu",
      "Bhiksha Raj"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=QDFormer%3A+Towards+Robust+Audiovisual+Segmentation+in+Complex+Environments+with+Quantization-based+Semantic+Decomposition+Xiang+Li+Jinglu+Wang+Xiaohao+Xu+Xiulian+Peng+Rita+Singh",
    "gs_search_success": true,
    "gs_authors": [
      "hGPBdf4AAAAJ",
      "A9o6duAAAAAJ",
      "3Ifn2DoAAAAJ"
    ],
    "citation_count": 17,
    "gs_author_count": 7
  },
  {
    "arxiv_id": "2406.06847",
    "title": "Generalized W-Net: Arbitrary-style Chinese Character Synthesization",
    "year": 2024,
    "published": "2024-06-10T23:31:36Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Synthesizing Chinese characters with consistent style using few stylized examples is challenging. Existing models struggle to generate arbitrary style characters with limited examples. In this paper, we propose the Generalized W-Net, a novel class of W-shaped architectures that addresses this. By incorporating Adaptive Instance Normalization and introducing multi-content, our approach can synthesize Chinese characters in any desired style, even with limited examples. It handles seen and unseen s",
    "arxiv_url": "https://arxiv.org/abs/2406.06847v1",
    "pdf_url": "https://arxiv.org/pdf/2406.06847v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2406.06847",
    "arxiv_authors": [
      "Haochuan Jiang",
      "Guanyu Yang",
      "Fei Cheng",
      "Kaizhu Huang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Generalized+W-Net%3A+Arbitrary-style+Chinese+Character+Synthesization+Haochuan+Jiang+Guanyu+Yang+Fei+Cheng+Kaizhu+Huang",
    "gs_search_success": true,
    "gs_authors": [
      "Cy6OU7gAAAAJ",
      "WLsxxIsAAAAJ",
      "3l5B0joAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2306.13337",
    "title": "Patch-Level Contrasting without Patch Correspondence for Accurate and Dense Contrastive Representation Learning",
    "year": 2023,
    "published": "2023-06-23T07:38:09Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "We propose ADCLR: A ccurate and D ense Contrastive Representation Learning, a novel self-supervised learning framework for learning accurate and dense vision representation. To extract spatial-sensitive information, ADCLR introduces query patches for contrasting in addition with global contrasting. Compared with previous dense contrasting methods, ADCLR mainly enjoys three merits: i) achieving both global-discriminative and spatial-sensitive representation, ii) model-efficient (no extra paramete",
    "arxiv_url": "https://arxiv.org/abs/2306.13337v1",
    "pdf_url": "https://arxiv.org/pdf/2306.13337v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2306.13337",
    "arxiv_authors": [
      "Shaofeng Zhang",
      "Feng Zhu",
      "Rui Zhao",
      "Junchi Yan"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Patch-Level+Contrasting+without+Patch+Correspondence+for+Accurate+and+Dense+Contrastive+Representation+Learning+Shaofeng+Zhang+Feng+Zhu+Rui+Zhao+Junchi+Yan",
    "gs_search_success": true,
    "gs_authors": [
      "oO53gjEAAAAJ",
      "VoVVJIgAAAAJ",
      "ga230VoAAAAJ",
      "1c9oQNMAAAAJ"
    ],
    "citation_count": 25,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2312.08785",
    "title": "Managing the unknown: a survey on Open Set Recognition and tangential areas",
    "year": 2023,
    "published": "2023-12-14T10:08:12Z",
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "abstract": "In real-world scenarios classification models are often required to perform robustly when predicting samples belonging to classes that have not appeared during its training stage. Open Set Recognition addresses this issue by devising models capable of detecting unknown classes from samples arriving during the testing phase, while maintaining a good level of performance in the classification of samples belonging to known classes. This review comprehensively overviews the recent literature related",
    "arxiv_url": "https://arxiv.org/abs/2312.08785v2",
    "pdf_url": "https://arxiv.org/pdf/2312.08785v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2312.08785",
    "arxiv_authors": [
      "Marcos Barcina-Blanco",
      "Jesus L. Lobo",
      "Pablo Garcia-Bringas",
      "Javier Del Ser"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Managing+the+unknown%3A+a+survey+on+Open+Set+Recognition+and+tangential+areas+Marcos+Barcina-Blanco+Jesus+L.+Lobo+Pablo+Garcia-Bringas+Javier+Del+Ser",
    "gs_search_success": true,
    "gs_authors": [
      "kedXhGwAAAAJ",
      "bUh5x3gAAAAJ",
      "p_wY0zUAAAAJ"
    ],
    "citation_count": 6,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2310.08784",
    "title": "Implicit Shape and Appearance Priors for Few-Shot Full Head Reconstruction",
    "year": 2023,
    "published": "2023-10-12T07:35:30Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Recent advancements in learning techniques that employ coordinate-based neural representations have yielded remarkable results in multi-view 3D reconstruction tasks. However, these approaches often require a substantial number of input views (typically several tens) and computationally intensive optimization procedures to achieve their effectiveness. In this paper, we address these limitations specifically for the problem of few-shot full 3D head reconstruction. We accomplish this by incorporati",
    "arxiv_url": "https://arxiv.org/abs/2310.08784v2",
    "pdf_url": "https://arxiv.org/pdf/2310.08784v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2310.08784",
    "arxiv_authors": [
      "Pol Caselles",
      "Eduard Ramon",
      "Jaime Garcia",
      "Gil Triginer",
      "Francesc Moreno-Noguer"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Implicit+Shape+and+Appearance+Priors+for+Few-Shot+Full+Head+Reconstruction+Pol+Caselles+Eduard+Ramon+Jaime+Garcia+Gil+Triginer+Francesc+Moreno-Noguer",
    "gs_search_success": false,
    "gs_authors": [],
    "citation_count": null,
    "error_type": "fetch_failed",
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2412.00784",
    "title": "EDTformer: An Efficient Decoder Transformer for Visual Place Recognition",
    "year": 2024,
    "published": "2024-12-01T12:14:36Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Visual place recognition (VPR) aims to determine the general geographical location of a query image by retrieving visually similar images from a large geo-tagged database. To obtain a global representation for each place image, most approaches typically focus on the aggregation of deep features extracted from a backbone through using current prominent architectures (e.g., CNNs, MLPs, pooling layer, and transformer encoder), giving little attention to the transformer decoder. However, we argue th",
    "arxiv_url": "https://arxiv.org/abs/2412.00784v2",
    "pdf_url": "https://arxiv.org/pdf/2412.00784v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2412.00784",
    "arxiv_authors": [
      "Tong Jin",
      "Feng Lu",
      "Shuyu Hu",
      "Chun Yuan",
      "Yunpeng Liu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=EDTformer%3A+An+Efficient+Decoder+Transformer+for+Visual+Place+Recognition+Tong+Jin+Feng+Lu+Shuyu+Hu+Chun+Yuan+Yunpeng+Liu",
    "gs_search_success": true,
    "gs_authors": [
      "ok_y2PMAAAAJ",
      "Gws7FKMAAAAJ",
      "fYdxi2sAAAAJ"
    ],
    "citation_count": 4,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2409.07291",
    "title": "Exploring User-level Gradient Inversion with a Diffusion Prior",
    "year": 2024,
    "published": "2024-09-11T14:20:47Z",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR",
      "cs.CV",
      "stat.ML"
    ],
    "abstract": "We explore user-level gradient inversion as a new attack surface in distributed learning. We first investigate existing attacks on their ability to make inferences about private information beyond training data reconstruction. Motivated by the low reconstruction quality of existing methods, we propose a novel gradient inversion attack that applies a denoising diffusion model as a strong image prior in order to enhance recovery in the large batch setting. Unlike traditional attacks, which aim to ",
    "arxiv_url": "https://arxiv.org/abs/2409.07291v1",
    "pdf_url": "https://arxiv.org/pdf/2409.07291v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2409.07291",
    "arxiv_authors": [
      "Zhuohang Li",
      "Andrew Lowy",
      "Jing Liu",
      "Toshiaki Koike-Akino",
      "Bradley Malin",
      "Kieran Parsons",
      "Ye Wang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Exploring+User-level+Gradient+Inversion+with+a+Diffusion+Prior+Zhuohang+Li+Andrew+Lowy+Jing+Liu+Toshiaki+Koike-Akino+Bradley+Malin",
    "gs_search_success": true,
    "gs_authors": [
      "fEC2LVMAAAAJ",
      "Kf97-EkAAAAJ",
      "ZMo2ZM0AAAAJ",
      "psRFdtUAAAAJ",
      "_FgPQ50AAAAJ",
      "kNHoiR4AAAAJ"
    ],
    "citation_count": 2,
    "gs_author_count": 7
  },
  {
    "arxiv_id": "2502.01445",
    "title": "SPFFNet: Strip Perception and Feature Fusion Spatial Pyramid Pooling for Fabric Defect Detection",
    "year": 2025,
    "published": "2025-02-03T15:33:11Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Defect detection in fabrics is critical for quality control, yet existing methods often struggle with complex backgrounds and shape-specific defects. In this paper, we propose an improved fabric defect detection model based on YOLOv11. To enhance the detection of strip defects, we introduce a Strip Perception Module (SPM) that improves feature capture through multi-scale convolution. We further enhance the spatial pyramid pooling fast (SPPF) by integrating a squeeze-and-excitation mechanism, res",
    "arxiv_url": "https://arxiv.org/abs/2502.01445v2",
    "pdf_url": "https://arxiv.org/pdf/2502.01445v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2502.01445",
    "arxiv_authors": [
      "Peizhe Zhao"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=SPFFNet%3A+Strip+Perception+and+Feature+Fusion+Spatial+Pyramid+Pooling+for+Fabric+Defect+Detection+Peizhe+Zhao",
    "gs_search_success": true,
    "gs_authors": [
      "gGhv_kQAAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 1
  },
  {
    "arxiv_id": "2311.05437",
    "title": "LLaVA-Plus: Learning to Use Tools for Creating Multimodal Agents",
    "year": 2023,
    "published": "2023-11-09T15:22:26Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "cs.MM"
    ],
    "abstract": "LLaVA-Plus is a general-purpose multimodal assistant that expands the capabilities of large multimodal models. It maintains a skill repository of pre-trained vision and vision-language models and can activate relevant tools based on users' inputs to fulfill real-world tasks. LLaVA-Plus is trained on multimodal instruction-following data to acquire the ability to use tools, covering visual understanding, generation, external knowledge retrieval, and compositions. Empirical results show that LLaVA",
    "arxiv_url": "https://arxiv.org/abs/2311.05437v1",
    "pdf_url": "https://arxiv.org/pdf/2311.05437v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2311.05437",
    "arxiv_authors": [
      "Shilong Liu",
      "Hao Cheng",
      "Haotian Liu",
      "Hao Zhang",
      "Feng Li",
      "Tianhe Ren",
      "Xueyan Zou",
      "Jianwei Yang",
      "Hang Su",
      "Jun Zhu",
      "Lei Zhang",
      "Jianfeng Gao",
      "Chunyuan Li"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=LLaVA-Plus%3A+Learning+to+Use+Tools+for+Creating+Multimodal+Agents+Shilong+Liu+Hao+Cheng+Haotian+Liu+Hao+Zhang+Feng+Li",
    "gs_search_success": true,
    "gs_authors": [
      "ybRe9GcAAAAJ",
      "dxN1_X0AAAAJ",
      "Xo6wfnQAAAAJ",
      "axsP38wAAAAJ",
      "CQ1cqKkAAAAJ",
      "d9s3sbQAAAAJ",
      "B8hPxMQAAAAJ",
      "nkSVY3MAAAAJ",
      "fIlGZToAAAAJ",
      "cW4ILs0AAAAJ",
      "eslbQqoAAAAJ",
      "Cl9byD8AAAAJ"
    ],
    "citation_count": 222,
    "gs_author_count": 9
  },
  {
    "arxiv_id": "2401.00151",
    "title": "CamPro: Camera-based Anti-Facial Recognition",
    "year": 2023,
    "published": "2023-12-30T06:20:36Z",
    "categories": [
      "cs.CV",
      "cs.CR"
    ],
    "abstract": "The proliferation of images captured from millions of cameras and the advancement of facial recognition (FR) technology have made the abuse of FR a severe privacy threat. Existing works typically rely on obfuscation, synthesis, or adversarial examples to modify faces in images to achieve anti-facial recognition (AFR). However, the unmodified images captured by camera modules that contain sensitive personally identifiable information (PII) could still be leaked. In this paper, we propose a novel ",
    "arxiv_url": "https://arxiv.org/abs/2401.00151v1",
    "pdf_url": "https://arxiv.org/pdf/2401.00151v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2401.00151",
    "arxiv_authors": [
      "Wenjun Zhu",
      "Yuan Sun",
      "Jiani Liu",
      "Yushi Cheng",
      "Xiaoyu Ji",
      "Wenyuan Xu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=CamPro%3A+Camera-based+Anti-Facial+Recognition+Wenjun+Zhu+Yuan+Sun+Jiani+Liu+Yushi+Cheng+Xiaoyu+Ji",
    "gs_search_success": true,
    "gs_authors": [
      "z-eDJ1L7FTMC",
      "9D4UYBoAAAAJ",
      "E59dgV4AAAAJ",
      "FCsdj0YAAAAJ"
    ],
    "citation_count": 5,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2306.16714",
    "title": "SimPLe: Similarity-Aware Propagation Learning for Weakly-Supervised Breast Cancer Segmentation in DCE-MRI",
    "year": 2023,
    "published": "2023-06-29T06:22:50Z",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "abstract": "Breast dynamic contrast-enhanced magnetic resonance imaging (DCE-MRI) plays an important role in the screening and prognosis assessment of high-risk breast cancer. The segmentation of cancerous regions is essential useful for the subsequent analysis of breast MRI. To alleviate the annotation effort to train the segmentation networks, we propose a weakly-supervised strategy using extreme points as annotations for breast cancer segmentation. Without using any bells and whistles, our strategy focus",
    "arxiv_url": "https://arxiv.org/abs/2306.16714v1",
    "pdf_url": "https://arxiv.org/pdf/2306.16714v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2306.16714",
    "arxiv_authors": [
      "Yuming Zhong",
      "Yi Wang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=SimPLe%3A+Similarity-Aware+Propagation+Learning+for+Weakly-Supervised+Breast+Cancer+Segmentation+in+DCE-MRI+Yuming+Zhong+Yi+Wang",
    "gs_search_success": true,
    "gs_authors": [
      "k2-vv-MAAAAJ"
    ],
    "citation_count": 13,
    "gs_author_count": 2
  },
  {
    "arxiv_id": "2309.16351",
    "title": "Dark Side Augmentation: Generating Diverse Night Examples for Metric Learning",
    "year": 2023,
    "published": "2023-09-28T11:20:24Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Image retrieval methods based on CNN descriptors rely on metric learning from a large number of diverse examples of positive and negative image pairs. Domains, such as night-time images, with limited availability and variability of training data suffer from poor retrieval performance even with methods performing well on standard benchmarks. We propose to train a GAN-based synthetic-image generator, translating available day-time image examples into night images. Such a generator is used in metri",
    "arxiv_url": "https://arxiv.org/abs/2309.16351v2",
    "pdf_url": "https://arxiv.org/pdf/2309.16351v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2309.16351",
    "arxiv_authors": [
      "Albert Mohwald",
      "Tomas Jenicek",
      "Ondřej Chum"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Dark+Side+Augmentation%3A+Generating+Diverse+Night+Examples+for+Metric+Learning+Albert+Mohwald+Tomas+Jenicek+Ond%C5%99ej+Chum",
    "gs_search_success": true,
    "gs_authors": [
      "O522zrUAAAAJ",
      "4T42Ke0AAAAJ",
      "Wmakwb8AAAAJ"
    ],
    "citation_count": 8,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2308.10111",
    "title": "Controllable Multi-domain Semantic Artwork Synthesis",
    "year": 2023,
    "published": "2023-08-19T21:16:28Z",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "abstract": "We present a novel framework for multi-domain synthesis of artwork from semantic layouts. One of the main limitations of this challenging task is the lack of publicly available segmentation datasets for art synthesis. To address this problem, we propose a dataset, which we call ArtSem, that contains 40,000 images of artwork from 4 different domains with their corresponding semantic label maps. We generate the dataset by first extracting semantic maps from landscape photography and then propose a",
    "arxiv_url": "https://arxiv.org/abs/2308.10111v1",
    "pdf_url": "https://arxiv.org/pdf/2308.10111v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2308.10111",
    "arxiv_authors": [
      "Yuantian Huang",
      "Satoshi Iizuka",
      "Edgar Simo-Serra",
      "Kazuhiro Fukui"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Controllable+Multi-domain+Semantic+Artwork+Synthesis+Yuantian+Huang+Satoshi+Iizuka+Edgar+Simo-Serra+Kazuhiro+Fukui",
    "gs_search_success": true,
    "gs_authors": [
      "Exg4wa0AAAAJ",
      "sHcEkmAAAAAJ",
      "fncZHTUAAAAJ",
      "2bDu3ecAAAAJ"
    ],
    "citation_count": 2,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2503.22087",
    "title": "Stream and Query-guided Feature Aggregation for Efficient and Effective 3D Occupancy Prediction",
    "year": 2025,
    "published": "2025-03-28T02:05:53Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "3D occupancy prediction has become a key perception task in autonomous driving, as it enables comprehensive scene understanding. Recent methods enhance this understanding by incorporating spatiotemporal information through multi-frame fusion, but they suffer from a trade-off: dense voxel-based representations provide high accuracy at significant computational cost, whereas sparse representations improve efficiency but lose spatial detail. To mitigate this trade-off, we introduce DuOcc, which emp",
    "arxiv_url": "https://arxiv.org/abs/2503.22087v2",
    "pdf_url": "https://arxiv.org/pdf/2503.22087v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2503.22087",
    "arxiv_authors": [
      "Seokha Moon",
      "Janghyun Baek",
      "Giseop Kim",
      "Jinkyu Kim",
      "Sunwook Choi"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Stream+and+Query-guided+Feature+Aggregation+for+Efficient+and+Effective+3D+Occupancy+Prediction+Seokha+Moon+Janghyun+Baek+Giseop+Kim+Jinkyu+Kim+Sunwook+Choi",
    "gs_search_success": true,
    "gs_authors": [
      "9mKOLX8AAAAJ",
      "HhvS9d4AAAAJ",
      "UJR1YYQAAAAJ"
    ],
    "citation_count": 2,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2303.10172",
    "title": "Hematoxylin and eosin stained oral squamous cell carcinoma histological images dataset",
    "year": 2023,
    "published": "2023-01-13T19:31:03Z",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "abstract": "Computer-aided diagnosis (CAD) can be used as an important tool to aid and enhance pathologists' diagnostic decision-making. Deep learning techniques, such as convolutional neural networks (CNN) and fully convolutional networks (FCN), have been successfully applied in medical and biological research. Unfortunately, histological image segmentation is often constrained by the availability of labeled training data once labeling histological images for segmentation purposes is a highly-skilled, comp",
    "arxiv_url": "https://arxiv.org/abs/2303.10172v1",
    "pdf_url": "https://arxiv.org/pdf/2303.10172v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2303.10172",
    "arxiv_authors": [
      "Dalí F. D. dos Santos",
      "Paulo R. de Faria",
      "Adriano M. Loyola",
      "Sérgio V. Cardoso",
      "Bruno A. N. Travençolo",
      "Marcelo Z. do Nascimento"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Hematoxylin+and+eosin+stained+oral+squamous+cell+carcinoma+histological+images+dataset+Dal%C3%AD+F.+D.+dos+Santos+Paulo+R.+de+Faria+Adriano+M.+Loyola+S%C3%A9rgio+V.+Cardoso+Bruno+A.+N.+Traven%C3%A7olo",
    "gs_search_success": true,
    "gs_authors": [
      "xVhnI00AAAAJ",
      "qCLjRUFiICAC",
      "SMjQjqAAAAAJ",
      "EK0RFxgAAAAJ",
      "45u5zOcAAAAJ"
    ],
    "citation_count": 6,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2304.09322",
    "title": "Multi-Modality Multi-Scale Cardiovascular Disease Subtypes Classification Using Raman Image and Medical History",
    "year": 2023,
    "published": "2023-04-18T22:09:16Z",
    "categories": [
      "eess.IV",
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "Raman spectroscopy (RS) has been widely used for disease diagnosis, e.g., cardiovascular disease (CVD), owing to its efficiency and component-specific testing capabilities. A series of popular deep learning methods have recently been introduced to learn nuance features from RS for binary classifications and achieved outstanding performance than conventional machine learning methods. However, these existing deep learning methods still confront some challenges in classifying subtypes of CVD. For e",
    "arxiv_url": "https://arxiv.org/abs/2304.09322v1",
    "pdf_url": "https://arxiv.org/pdf/2304.09322v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2304.09322",
    "arxiv_authors": [
      "Bo Yu",
      "Hechang Chen",
      "Chengyou Jia",
      "Hongren Zhou",
      "Lele Cong",
      "Xiankai Li",
      "Jianhui Zhuang",
      "Xianling Cong"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Multi-Modality+Multi-Scale+Cardiovascular+Disease+Subtypes+Classification+Using+Raman+Image+and+Medical+History+Bo+Yu+Hechang+Chen+Chengyou+Jia+Hongren+Zhou+Lele+Cong",
    "gs_search_success": true,
    "gs_authors": [
      "EezEcbgAAAAJ",
      "DMSevhgAAAAJ"
    ],
    "citation_count": 19,
    "gs_author_count": 8
  },
  {
    "arxiv_id": "2402.02574",
    "title": "Spatio-temporal Prompting Network for Robust Video Feature Extraction",
    "year": 2024,
    "published": "2024-02-04T17:52:04Z",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "Frame quality deterioration is one of the main challenges in the field of video understanding. To compensate for the information loss caused by deteriorated frames, recent approaches exploit transformer-based integration modules to obtain spatio-temporal information. However, these integration modules are heavy and complex. Furthermore, each integration module is specifically tailored for its target task, making it difficult to generalise to multiple tasks. In this paper, we present a neat and u",
    "arxiv_url": "https://arxiv.org/abs/2402.02574v1",
    "pdf_url": "https://arxiv.org/pdf/2402.02574v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2402.02574",
    "arxiv_authors": [
      "Guanxiong Sun",
      "Chi Wang",
      "Zhaoyu Zhang",
      "Jiankang Deng",
      "Stefanos Zafeiriou",
      "Yang Hua"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Spatio-temporal+Prompting+Network+for+Robust+Video+Feature+Extraction+Guanxiong+Sun+Chi+Wang+Zhaoyu+Zhang+Jiankang+Deng+Stefanos+Zafeiriou",
    "gs_search_success": true,
    "gs_authors": [
      "vLk3IzoAAAAJ",
      "Z_UoQFsAAAAJ",
      "N0tFi8MAAAAJ",
      "SeNxNjIAAAAJ",
      "QKOH5iYAAAAJ"
    ],
    "citation_count": 12,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2302.13540",
    "title": "OccDepth: A Depth-Aware Method for 3D Semantic Scene Completion",
    "year": 2023,
    "published": "2023-02-27T06:35:03Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "3D Semantic Scene Completion (SSC) can provide dense geometric and semantic scene representations, which can be applied in the field of autonomous driving and robotic systems. It is challenging to estimate the complete geometry and semantics of a scene solely from visual images, and accurate depth information is crucial for restoring 3D geometry. In this paper, we propose the first stereo SSC method named OccDepth, which fully exploits implicit depth information from stereo images (or RGBD image",
    "arxiv_url": "https://arxiv.org/abs/2302.13540v1",
    "pdf_url": "https://arxiv.org/pdf/2302.13540v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2302.13540",
    "arxiv_authors": [
      "Ruihang Miao",
      "Weizhou Liu",
      "Mingrui Chen",
      "Zheng Gong",
      "Weixin Xu",
      "Chen Hu",
      "Shuchang Zhou"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=OccDepth%3A+A+Depth-Aware+Method+for+3D+Semantic+Scene+Completion+Ruihang+Miao+Weizhou+Liu+Mingrui+Chen+Zheng+Gong+Weixin+Xu",
    "gs_search_success": false,
    "gs_authors": [],
    "citation_count": null,
    "error_type": "fetch_failed",
    "gs_author_count": 7
  },
  {
    "arxiv_id": "2305.16025",
    "title": "NVTC: Nonlinear Vector Transform Coding",
    "year": 2023,
    "published": "2023-05-25T13:06:38Z",
    "categories": [
      "cs.CV",
      "eess.IV"
    ],
    "abstract": "In theory, vector quantization (VQ) is always better than scalar quantization (SQ) in terms of rate-distortion (R-D) performance. Recent state-of-the-art methods for neural image compression are mainly based on nonlinear transform coding (NTC) with uniform scalar quantization, overlooking the benefits of VQ due to its exponentially increased complexity. In this paper, we first investigate on some toy sources, demonstrating that even if modern neural networks considerably enhance the compression ",
    "arxiv_url": "https://arxiv.org/abs/2305.16025v1",
    "pdf_url": "https://arxiv.org/pdf/2305.16025v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2305.16025",
    "arxiv_authors": [
      "Runsen Feng",
      "Zongyu Guo",
      "Weiping Li",
      "Zhibo Chen"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=NVTC%3A+Nonlinear+Vector+Transform+Coding+Runsen+Feng+Zongyu+Guo+Weiping+Li+Zhibo+Chen",
    "gs_search_success": true,
    "gs_authors": [
      "1ayDJfsAAAAJ",
      "paus9RMAAAAJ",
      "HOP5jkQAAAAJ"
    ],
    "citation_count": 32,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2305.14093",
    "title": "Weakly Supervised 3D Open-vocabulary Segmentation",
    "year": 2023,
    "published": "2023-05-23T14:16:49Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Open-vocabulary segmentation of 3D scenes is a fundamental function of human perception and thus a crucial objective in computer vision research. However, this task is heavily impeded by the lack of large-scale and diverse 3D open-vocabulary segmentation datasets for training robust and generalizable models. Distilling knowledge from pre-trained 2D open-vocabulary segmentation models helps but it compromises the open-vocabulary feature as the 2D models are mostly finetuned with close-vocabulary ",
    "arxiv_url": "https://arxiv.org/abs/2305.14093v4",
    "pdf_url": "https://arxiv.org/pdf/2305.14093v4",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2305.14093",
    "arxiv_authors": [
      "Kunhao Liu",
      "Fangneng Zhan",
      "Jiahui Zhang",
      "Muyu Xu",
      "Yingchen Yu",
      "Abdulmotaleb El Saddik",
      "Christian Theobalt",
      "Eric Xing",
      "Shijian Lu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Weakly+Supervised+3D+Open-vocabulary+Segmentation+Kunhao+Liu+Fangneng+Zhan+Jiahui+Zhang+Muyu+Xu+Yingchen+Yu",
    "gs_search_success": true,
    "gs_authors": [
      "0cet0X8AAAAJ",
      "eIWg8NMAAAAJ",
      "DXpYbWkAAAAJ",
      "7gDzUDEAAAAJ",
      "8zbcfzAAAAAJ",
      "VcOjgngAAAAJ",
      "fAc8WqwAAAAJ",
      "uYmK-A0AAAAJ",
      "5pKTRxEAAAAJ"
    ],
    "citation_count": 117,
    "gs_author_count": 9
  },
  {
    "arxiv_id": "2412.11779",
    "title": "Impact of Face Alignment on Face Image Quality",
    "year": 2024,
    "published": "2024-12-16T13:49:57Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Face alignment is a crucial step in preparing face images for feature extraction in facial analysis tasks. For applications such as face recognition, facial expression recognition, and facial attribute classification, alignment is widely utilized during both training and inference to standardize the positions of key landmarks in the face. It is well known that the application and method of face alignment significantly affect the performance of facial analysis models. However, the impact of align",
    "arxiv_url": "https://arxiv.org/abs/2412.11779v1",
    "pdf_url": "https://arxiv.org/pdf/2412.11779v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2412.11779",
    "arxiv_authors": [
      "Eren Onaran",
      "Erdi Sarıtaş",
      "Hazım Kemal Ekenel"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Impact+of+Face+Alignment+on+Face+Image+Quality+Eren+Onaran+Erdi+Sar%C4%B1ta%C5%9F+Haz%C4%B1m+Kemal+Ekenel",
    "gs_search_success": true,
    "gs_authors": [
      "lMe9XmoAAAAJ",
      "LfIDj68AAAAJ"
    ],
    "citation_count": 3,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2502.12545",
    "title": "IM360: Large-scale Indoor Mapping with 360 Cameras",
    "year": 2025,
    "published": "2025-02-18T05:15:19Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "We present a novel 3D mapping pipeline for large-scale indoor environments. To address the significant challenges in large-scale indoor scenes, such as prevalent occlusions and textureless regions, we propose IM360, a novel approach that leverages the wide field of view of omnidirectional images and integrates the spherical camera model into the Structure-from-Motion (SfM) pipeline. Our SfM utilizes dense matching features specifically designed for 360 images, demonstrating superior capability i",
    "arxiv_url": "https://arxiv.org/abs/2502.12545v3",
    "pdf_url": "https://arxiv.org/pdf/2502.12545v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2502.12545",
    "arxiv_authors": [
      "Dongki Jung",
      "Jaehoon Choi",
      "Yonghan Lee",
      "Dinesh Manocha"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=IM360%3A+Large-scale+Indoor+Mapping+with+360+Cameras+Dongki+Jung+Jaehoon+Choi+Yonghan+Lee+Dinesh+Manocha",
    "gs_search_success": true,
    "gs_authors": [
      "kila11YAAAAJ",
      "8mpfeP0AAAAJ",
      "X08l_4IAAAAJ",
      "MEwO0QwAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2312.15881",
    "title": "Attention-aware Social Graph Transformer Networks for Stochastic Trajectory Prediction",
    "year": 2023,
    "published": "2023-12-26T04:24:01Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Trajectory prediction is fundamental to various intelligent technologies, such as autonomous driving and robotics. The motion prediction of pedestrians and vehicles helps emergency braking, reduces collisions, and improves traffic safety. Current trajectory prediction research faces problems of complex social interactions, high dynamics and multi-modality. Especially, it still has limitations in long-time prediction. We propose Attention-aware Social Graph Transformer Networks for multi-modal tr",
    "arxiv_url": "https://arxiv.org/abs/2312.15881v2",
    "pdf_url": "https://arxiv.org/pdf/2312.15881v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2312.15881",
    "arxiv_authors": [
      "Yao Liu",
      "Binghao Li",
      "Xianzhi Wang",
      "Claude Sammut",
      "Lina Yao"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Attention-aware+Social+Graph+Transformer+Networks+for+Stochastic+Trajectory+Prediction+Yao+Liu+Binghao+Li+Xianzhi+Wang+Claude+Sammut+Lina+Yao",
    "gs_search_success": false,
    "gs_authors": [],
    "citation_count": null,
    "error_type": "fetch_failed",
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2504.16763",
    "title": "Noise-Tolerant Coreset-Based Class Incremental Continual Learning",
    "year": 2025,
    "published": "2025-04-23T14:34:20Z",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "cs.NE"
    ],
    "abstract": "Many applications of computer vision require the ability to adapt to novel data distributions after deployment. Adaptation requires algorithms capable of continual learning (CL). Continual learners must be plastic to adapt to novel tasks while minimizing forgetting of previous tasks.However, CL opens up avenues for noise to enter the training pipeline and disrupt the CL. This work focuses on label noise and instance noise in the context of class-incremental learning (CIL), where new classes are ",
    "arxiv_url": "https://arxiv.org/abs/2504.16763v1",
    "pdf_url": "https://arxiv.org/pdf/2504.16763v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2504.16763",
    "arxiv_authors": [
      "Edison Mucllari",
      "Aswin Raghavan",
      "Zachary Alan Daniels"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Noise-Tolerant+Coreset-Based+Class+Incremental+Continual+Learning+Edison+Mucllari+Aswin+Raghavan+Zachary+Alan+Daniels",
    "gs_search_success": true,
    "gs_authors": [
      "YavdTtgAAAAJ",
      "AYPSp00AAAAJ",
      "Ss2KBccAAAAJ"
    ],
    "citation_count": 2,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2305.13031",
    "title": "HGFormer: Hierarchical Grouping Transformer for Domain Generalized Semantic Segmentation",
    "year": 2023,
    "published": "2023-05-22T13:33:41Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Current semantic segmentation models have achieved great success under the independent and identically distributed (i.i.d.) condition. However, in real-world applications, test data might come from a different domain than training data. Therefore, it is important to improve model robustness against domain differences. This work studies semantic segmentation under the domain generalization setting, where a model is trained only on the source domain and tested on the unseen target domain. Existing",
    "arxiv_url": "https://arxiv.org/abs/2305.13031v1",
    "pdf_url": "https://arxiv.org/pdf/2305.13031v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2305.13031",
    "arxiv_authors": [
      "Jian Ding",
      "Nan Xue",
      "Gui-Song Xia",
      "Bernt Schiele",
      "Dengxin Dai"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=HGFormer%3A+Hierarchical+Grouping+Transformer+for+Domain+Generalized+Semantic+Segmentation+Jian+Ding+Nan+Xue+Gui-Song+Xia+Bernt+Schiele+Dengxin+Dai",
    "gs_search_success": false,
    "gs_authors": [],
    "citation_count": null,
    "error_type": "fetch_failed",
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2406.13105",
    "title": "A transformer boosted UNet for smoke segmentation in complex backgrounds in multispectral LandSat imagery",
    "year": 2024,
    "published": "2024-06-18T23:38:24Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Many studies have been done to detect smokes from satellite imagery. However, these prior methods are not still effective in detecting various smokes in complex backgrounds. Smokes present challenges in detection due to variations in density, color, lighting, and backgrounds such as clouds, haze, and/or mist, as well as the contextual nature of thin smoke. This paper addresses these challenges by proposing a new segmentation model called VTrUNet which consists of a virtual band construction modu",
    "arxiv_url": "https://arxiv.org/abs/2406.13105v1",
    "pdf_url": "https://arxiv.org/pdf/2406.13105v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2406.13105",
    "arxiv_authors": [
      "Jixue Liu",
      "Jiuyong Li",
      "Stefan Peters",
      "Liang Zhao"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=A+transformer+boosted+UNet+for+smoke+segmentation+in+complex+backgrounds+in+multispectral+LandSat+imagery+Jixue+Liu+Jiuyong+Li+Stefan+Peters+Liang+Zhao",
    "gs_search_success": true,
    "gs_authors": [
      "Tac3gAoAAAAJ",
      "HWz8_ZUAAAAJ",
      "Ztoa054AAAAJ"
    ],
    "citation_count": 4,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2501.12433",
    "title": "Owls are wise and foxes are unfaithful: Uncovering animal stereotypes in vision-language models",
    "year": 2025,
    "published": "2025-01-21T18:41:28Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "abstract": "Animal stereotypes are deeply embedded in human culture and language. They often shape our perceptions and expectations of various species. Our study investigates how animal stereotypes manifest in vision-language models during the task of image generation. Through targeted prompts, we explore whether DALL-E perpetuates stereotypical representations of animals, such as \"owls as wise,\" \"foxes as unfaithful,\" etc. Our findings reveal significant stereotyped instances where the model consistently g",
    "arxiv_url": "https://arxiv.org/abs/2501.12433v2",
    "pdf_url": "https://arxiv.org/pdf/2501.12433v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2501.12433",
    "arxiv_authors": [
      "Tabinda Aman",
      "Mohammad Nadeem",
      "Shahab Saquib Sohail",
      "Mohammad Anas",
      "Erik Cambria"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Owls+are+wise+and+foxes+are+unfaithful%3A+Uncovering+animal+stereotypes+in+vision-language+models+Tabinda+Aman+Mohammad+Nadeem+Shahab+Saquib+Sohail+Mohammad+Anas+Erik+Cambria",
    "gs_search_success": true,
    "gs_authors": [
      "XxBoh-MAAAAJ",
      "AYv5sqUAAAAJ",
      "u5UGqUMAAAAJ",
      "ilSYpW0AAAAJ"
    ],
    "citation_count": 3,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2504.17815",
    "title": "Visibility-Uncertainty-guided 3D Gaussian Inpainting via Scene Conceptional Learning",
    "year": 2025,
    "published": "2025-04-23T06:21:11Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has emerged as a powerful and efficient 3D representation for novel view synthesis. This paper extends 3DGS capabilities to inpainting, where masked objects in a scene are replaced with new contents that blend seamlessly with the surroundings. Unlike 2D image inpainting, 3D Gaussian inpainting (3DGI) is challenging in effectively leveraging complementary visual and semantic cues from multiple input views, as occluded areas in one view may be visible in others. To add",
    "arxiv_url": "https://arxiv.org/abs/2504.17815v1",
    "pdf_url": "https://arxiv.org/pdf/2504.17815v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2504.17815",
    "arxiv_authors": [
      "Mingxuan Cui",
      "Qing Guo",
      "Yuyi Wang",
      "Hongkai Yu",
      "Di Lin",
      "Qin Zou",
      "Ming-Ming Cheng",
      "Xi Li"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Visibility-Uncertainty-guided+3D+Gaussian+Inpainting+via+Scene+Conceptional+Learning+Mingxuan+Cui+Qing+Guo+Yuyi+Wang+Hongkai+Yu+Di+Lin",
    "gs_search_success": true,
    "gs_authors": [
      "huWpVyEAAAAJ",
      "dJ8izFAAAAAJ",
      "rW0r-hMAAAAJ",
      "JnQts0kAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 8
  },
  {
    "arxiv_id": "2505.09498",
    "title": "Flash-VL 2B: Optimizing Vision-Language Model Performance for Ultra-Low Latency and High Throughput",
    "year": 2025,
    "published": "2025-05-14T15:45:17Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "In this paper, we introduce Flash-VL 2B, a novel approach to optimizing Vision-Language Models (VLMs) for real-time applications, targeting ultra-low latency and high throughput without sacrificing accuracy. Leveraging advanced architectural enhancements and efficient computational strategies, Flash-VL 2B is designed to maximize throughput by reducing processing time while maintaining competitive performance across multiple vision-language benchmarks. Our approach includes tailored architectural",
    "arxiv_url": "https://arxiv.org/abs/2505.09498v1",
    "pdf_url": "https://arxiv.org/pdf/2505.09498v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2505.09498",
    "arxiv_authors": [
      "Bo Zhang",
      "Shuo Li",
      "Runhe Tian",
      "Yang Yang",
      "Jixin Tang",
      "Jinhao Zhou",
      "Lin Ma"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Flash-VL+2B%3A+Optimizing+Vision-Language+Model+Performance+for+Ultra-Low+Latency+and+High+Throughput+Bo+Zhang+Shuo+Li+Runhe+Tian+Yang+Yang+Jixin+Tang",
    "gs_search_success": false,
    "gs_authors": [],
    "citation_count": null,
    "error_type": "fetch_failed",
    "gs_author_count": 7
  },
  {
    "arxiv_id": "2403.10097",
    "title": "Adaptive Random Feature Regularization on Fine-tuning Deep Neural Networks",
    "year": 2024,
    "published": "2024-03-15T08:26:59Z",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "abstract": "While fine-tuning is a de facto standard method for training deep neural networks, it still suffers from overfitting when using small target datasets. Previous methods improve fine-tuning performance by maintaining knowledge of the source datasets or introducing regularization terms such as contrastive loss. However, these methods require auxiliary source information (e.g., source labels or datasets) or heavy additional computations. In this paper, we propose a simple method called adaptive rand",
    "arxiv_url": "https://arxiv.org/abs/2403.10097v1",
    "pdf_url": "https://arxiv.org/pdf/2403.10097v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2403.10097",
    "arxiv_authors": [
      "Shin'ya Yamaguchi",
      "Sekitoshi Kanai",
      "Kazuki Adachi",
      "Daiki Chijiwa"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Adaptive+Random+Feature+Regularization+on+Fine-tuning+Deep+Neural+Networks+Shin%27ya+Yamaguchi+Sekitoshi+Kanai+Kazuki+Adachi+Daiki+Chijiwa",
    "gs_search_success": true,
    "gs_authors": [
      "qa2i5_IAAAAJ",
      "hZC9Yy8AAAAJ",
      "CfD4JBEAAAAJ",
      "_xJYVD0AAAAJ"
    ],
    "citation_count": 4,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2404.09067",
    "title": "Exploring Explainability in Video Action Recognition",
    "year": 2024,
    "published": "2024-04-13T19:34:14Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Image Classification and Video Action Recognition are perhaps the two most foundational tasks in computer vision. Consequently, explaining the inner workings of trained deep neural networks is of prime importance. While numerous efforts focus on explaining the decisions of trained deep neural networks in image classification, exploration in the domain of its temporal version, video action recognition, has been scant. In this work, we take a deeper look at this problem. We begin by revisiting Gra",
    "arxiv_url": "https://arxiv.org/abs/2404.09067v1",
    "pdf_url": "https://arxiv.org/pdf/2404.09067v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2404.09067",
    "arxiv_authors": [
      "Avinab Saha",
      "Shashank Gupta",
      "Sravan Kumar Ankireddy",
      "Karl Chahine",
      "Joydeep Ghosh"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Exploring+Explainability+in+Video+Action+Recognition+Avinab+Saha+Shashank+Gupta+Sravan+Kumar+Ankireddy+Karl+Chahine+Joydeep+Ghosh",
    "gs_search_success": true,
    "gs_authors": [
      "j34sU94AAAAJ",
      "Xnk4W5cAAAAJ",
      "UYT2VwkAAAAJ",
      "KbyIL3MAAAAJ",
      "tlp7uQEAAAAJ"
    ],
    "citation_count": 16,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2301.04218",
    "title": "Leveraging Diffusion For Strong and High Quality Face Morphing Attacks",
    "year": 2023,
    "published": "2023-01-10T21:50:26Z",
    "categories": [
      "cs.CV",
      "cs.CR",
      "cs.LG"
    ],
    "abstract": "Face morphing attacks seek to deceive a Face Recognition (FR) system by presenting a morphed image consisting of the biometric qualities from two different identities with the aim of triggering a false acceptance with one of the two identities, thereby presenting a significant threat to biometric systems. The success of a morphing attack is dependent on the ability of the morphed image to represent the biometric characteristics of both identities that were used to create the image. We present a ",
    "arxiv_url": "https://arxiv.org/abs/2301.04218v4",
    "pdf_url": "https://arxiv.org/pdf/2301.04218v4",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2301.04218",
    "arxiv_authors": [
      "Zander W. Blasingame",
      "Chen Liu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Leveraging+Diffusion+For+Strong+and+High+Quality+Face+Morphing+Attacks+Zander+W.+Blasingame+Chen+Liu",
    "gs_search_success": true,
    "gs_authors": [
      "dpKWP8oAAAAJ",
      "gBBtH3AAAAAJ"
    ],
    "citation_count": 25,
    "gs_author_count": 2
  },
  {
    "arxiv_id": "2401.15362",
    "title": "Transformer-based Clipped Contrastive Quantization Learning for Unsupervised Image Retrieval",
    "year": 2024,
    "published": "2024-01-27T09:39:11Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Unsupervised image retrieval aims to learn the important visual characteristics without any given level to retrieve the similar images for a given query image. The Convolutional Neural Network (CNN)-based approaches have been extensively exploited with self-supervised contrastive learning for image hashing. However, the existing approaches suffer due to lack of effective utilization of global features by CNNs and biased-ness created by false negative pairs in the contrastive learning. In this pa",
    "arxiv_url": "https://arxiv.org/abs/2401.15362v1",
    "pdf_url": "https://arxiv.org/pdf/2401.15362v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2401.15362",
    "arxiv_authors": [
      "Ayush Dubey",
      "Shiv Ram Dubey",
      "Satish Kumar Singh",
      "Wei-Ta Chu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Transformer-based+Clipped+Contrastive+Quantization+Learning+for+Unsupervised+Image+Retrieval+Ayush+Dubey+Shiv+Ram+Dubey+Satish+Kumar+Singh+Wei-Ta+Chu",
    "gs_search_success": true,
    "gs_authors": [
      "TioEtyEAAAAJ",
      "RtmwqYEAAAAJ",
      "DcltNjQAAAAJ"
    ],
    "citation_count": 6,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2409.17728",
    "title": "AlterMOMA: Fusion Redundancy Pruning for Camera-LiDAR Fusion Models with Alternative Modality Masking",
    "year": 2024,
    "published": "2024-09-26T10:57:02Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Camera-LiDAR fusion models significantly enhance perception performance in autonomous driving. The fusion mechanism leverages the strengths of each modality while minimizing their weaknesses. Moreover, in practice, camera-LiDAR fusion models utilize pre-trained backbones for efficient training. However, we argue that directly loading single-modal pre-trained camera and LiDAR backbones into camera-LiDAR fusion models introduces similar feature redundancy across modalities due to the nature of the",
    "arxiv_url": "https://arxiv.org/abs/2409.17728v1",
    "pdf_url": "https://arxiv.org/pdf/2409.17728v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2409.17728",
    "arxiv_authors": [
      "Shiqi Sun",
      "Yantao Lu",
      "Ning Liu",
      "Bo Jiang",
      "JinChao Chen",
      "Ying Zhang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=AlterMOMA%3A+Fusion+Redundancy+Pruning+for+Camera-LiDAR+Fusion+Models+with+Alternative+Modality+Masking+Shiqi+Sun+Yantao+Lu+Ning+Liu+Bo+Jiang+JinChao+Chen",
    "gs_search_success": true,
    "gs_authors": [
      "jj1aip8AAAAJ",
      "CnF9Q-kAAAAJ",
      "kBUsho4AAAAJ",
      "OFOJM5MAAAAJ",
      "xkAyW3kAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2504.13297",
    "title": "Weak Cube R-CNN: Weakly Supervised 3D Detection using only 2D Bounding Boxes",
    "year": 2025,
    "published": "2025-04-17T19:13:42Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Monocular 3D object detection is an essential task in computer vision, and it has several applications in robotics and virtual reality. However, 3D object detectors are typically trained in a fully supervised way, relying extensively on 3D labeled data, which is labor-intensive and costly to annotate. This work focuses on weakly-supervised 3D detection to reduce data needs using a monocular method that leverages a singlecamera system over expensive LiDAR sensors or multi-camera setups. We propos",
    "arxiv_url": "https://arxiv.org/abs/2504.13297v1",
    "pdf_url": "https://arxiv.org/pdf/2504.13297v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2504.13297",
    "arxiv_authors": [
      "Andreas Lau Hansen",
      "Lukas Wanzeck",
      "Dim P. Papadopoulos"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Weak+Cube+R-CNN%3A+Weakly+Supervised+3D+Detection+using+only+2D+Bounding+Boxes+Andreas+Lau+Hansen+Lukas+Wanzeck+Dim+P.+Papadopoulos",
    "gs_search_success": false,
    "gs_authors": [],
    "citation_count": null,
    "error_type": "fetch_failed",
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2403.09124",
    "title": "Single Domain Generalization for Crowd Counting",
    "year": 2024,
    "published": "2024-03-14T06:16:21Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Due to its promising results, density map regression has been widely employed for image-based crowd counting. The approach, however, often suffers from severe performance degradation when tested on data from unseen scenarios, the so-called \"domain shift\" problem. To address the problem, we investigate in this work single domain generalization (SDG) for crowd counting. The existing SDG approaches are mainly for image classification and segmentation, and can hardly be extended to our case due to i",
    "arxiv_url": "https://arxiv.org/abs/2403.09124v2",
    "pdf_url": "https://arxiv.org/pdf/2403.09124v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2403.09124",
    "arxiv_authors": [
      "Zhuoxuan Peng",
      "S. -H. Gary Chan"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Single+Domain+Generalization+for+Crowd+Counting+Zhuoxuan+Peng+S.+-H.+Gary+Chan",
    "gs_search_success": true,
    "gs_authors": [
      "j9jhYqQAAAAJ",
      "ArIg7-0AAAAJ",
      "5kEpH1oAAAAJ",
      "DXuAYKwAAAAJ",
      "Shy1gnMAAAAJ",
      "loLLnQoAAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 2
  },
  {
    "arxiv_id": "2407.06611",
    "title": "CEIA: CLIP-Based Event-Image Alignment for Open-World Event-Based Understanding",
    "year": 2024,
    "published": "2024-07-09T07:26:15Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "We present CEIA, an effective framework for open-world event-based understanding. Currently training a large event-text model still poses a huge challenge due to the shortage of paired event-text data. In response to this challenge, CEIA learns to align event and image data as an alternative instead of directly aligning event and text data. Specifically, we leverage the rich event-image datasets to learn an event embedding space aligned with the image space of CLIP through contrastive learning. ",
    "arxiv_url": "https://arxiv.org/abs/2407.06611v1",
    "pdf_url": "https://arxiv.org/pdf/2407.06611v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2407.06611",
    "arxiv_authors": [
      "Wenhao Xu",
      "Wenming Weng",
      "Yueyi Zhang",
      "Zhiwei Xiong"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=CEIA%3A+CLIP-Based+Event-Image+Alignment+for+Open-World+Event-Based+Understanding+Wenhao+Xu+Wenming+Weng+Yueyi+Zhang+Zhiwei+Xiong",
    "gs_search_success": false,
    "gs_authors": [],
    "citation_count": null,
    "error_type": "fetch_failed",
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2306.09344",
    "title": "DreamSim: Learning New Dimensions of Human Visual Similarity using Synthetic Data",
    "year": 2023,
    "published": "2023-06-15T17:59:50Z",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "Current perceptual similarity metrics operate at the level of pixels and patches. These metrics compare images in terms of their low-level colors and textures, but fail to capture mid-level similarities and differences in image layout, object pose, and semantic content. In this paper, we develop a perceptual metric that assesses images holistically. Our first step is to collect a new dataset of human similarity judgments over image pairs that are alike in diverse ways. Critical to this dataset i",
    "arxiv_url": "https://arxiv.org/abs/2306.09344v3",
    "pdf_url": "https://arxiv.org/pdf/2306.09344v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2306.09344",
    "arxiv_authors": [
      "Stephanie Fu",
      "Netanel Tamir",
      "Shobhita Sundaram",
      "Lucy Chai",
      "Richard Zhang",
      "Tali Dekel",
      "Phillip Isola"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=DreamSim%3A+Learning+New+Dimensions+of+Human+Visual+Similarity+using+Synthetic+Data+Stephanie+Fu+Netanel+Tamir+Shobhita+Sundaram+Lucy+Chai+Richard+Zhang",
    "gs_search_success": true,
    "gs_authors": [
      "easC2gEAAAAJ",
      "LW8ze_UAAAAJ",
      "Rx-h05AAAAAJ",
      "tlTVtZ4AAAAJ",
      "bunnQWQAAAAJ"
    ],
    "citation_count": 357,
    "gs_author_count": 7
  },
  {
    "arxiv_id": "2306.15832",
    "title": "Easing Color Shifts in Score-Based Diffusion Models",
    "year": 2023,
    "published": "2023-06-27T23:33:30Z",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "abstract": "Generated images of score-based models can suffer from errors in their spatial means, an effect, referred to as a color shift, which grows for larger images. This paper investigates a previously-introduced approach to mitigate color shifts in score-based diffusion models. We quantify the performance of a nonlinear bypass connection in the score network, designed to process the spatial mean of the input and to predict the mean of the score function. We show that this network architecture substant",
    "arxiv_url": "https://arxiv.org/abs/2306.15832v2",
    "pdf_url": "https://arxiv.org/pdf/2306.15832v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2306.15832",
    "arxiv_authors": [
      "Katherine Deck",
      "Tobias Bischoff"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Easing+Color+Shifts+in+Score-Based+Diffusion+Models+Katherine+Deck+Tobias+Bischoff",
    "gs_search_success": true,
    "gs_authors": [],
    "citation_count": 8,
    "gs_author_count": 2
  },
  {
    "arxiv_id": "2310.04551",
    "title": "MeSa: Masked, Geometric, and Supervised Pre-training for Monocular Depth Estimation",
    "year": 2023,
    "published": "2023-10-06T19:24:54Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Pre-training has been an important ingredient in developing strong monocular depth estimation models in recent years. For instance, self-supervised learning (SSL) is particularly effective by alleviating the need for large datasets with dense ground-truth depth maps. However, despite these improvements, our study reveals that the later layers of the SOTA SSL method are actually suboptimal. By examining the layer-wise representations, we demonstrate significant changes in these later layers durin",
    "arxiv_url": "https://arxiv.org/abs/2310.04551v1",
    "pdf_url": "https://arxiv.org/pdf/2310.04551v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2310.04551",
    "arxiv_authors": [
      "Muhammad Osama Khan",
      "Junbang Liang",
      "Chun-Kai Wang",
      "Shan Yang",
      "Yu Lou"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=MeSa%3A+Masked%2C+Geometric%2C+and+Supervised+Pre-training+for+Monocular+Depth+Estimation+Muhammad+Osama+Khan+Junbang+Liang+Chun-Kai+Wang+Shan+Yang+Yu+Lou",
    "gs_search_success": true,
    "gs_authors": [
      "WLk8ZikAAAAJ",
      "MiN5SNQAAAAJ",
      "TVWgFWkAAAAJ",
      "PKUbHusAAAAJ"
    ],
    "citation_count": 4,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2301.07002",
    "title": "Opti-CAM: Optimizing saliency maps for interpretability",
    "year": 2023,
    "published": "2023-01-17T16:44:48Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Methods based on class activation maps (CAM) provide a simple mechanism to interpret predictions of convolutional neural networks by using linear combinations of feature maps as saliency maps. By contrast, masking-based methods optimize a saliency map directly in the image space or learn it by training another network on additional data.   In this work we introduce Opti-CAM, combining ideas from CAM-based and masking-based approaches. Our saliency map is a linear combination of feature maps, whe",
    "arxiv_url": "https://arxiv.org/abs/2301.07002v3",
    "pdf_url": "https://arxiv.org/pdf/2301.07002v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2301.07002",
    "arxiv_authors": [
      "Hanwei Zhang",
      "Felipe Torres",
      "Ronan Sicre",
      "Yannis Avrithis",
      "Stephane Ayache"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Opti-CAM%3A+Optimizing+saliency+maps+for+interpretability+Hanwei+Zhang+Felipe+Torres+Ronan+Sicre+Yannis+Avrithis+Stephane+Ayache",
    "gs_search_success": true,
    "gs_authors": [
      "AF2SxG0AAAAJ",
      "7QjjN2wAAAAJ",
      "g4qCnx8AAAAJ",
      "RZ7c4EQAAAAJ"
    ],
    "citation_count": 49,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2311.16042",
    "title": "Weakly-Supervised 3D Reconstruction of Clothed Humans via Normal Maps",
    "year": 2023,
    "published": "2023-11-27T18:06:35Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "We present a novel deep learning-based approach to the 3D reconstruction of clothed humans using weak supervision via 2D normal maps. Given a single RGB image or multiview images, our network infers a signed distance function (SDF) discretized on a tetrahedral mesh surrounding the body in a rest pose. Subsequently, inferred pose and camera parameters are used to generate a normal map from the SDF. A key aspect of our approach is the use of Marching Tetrahedra to (uniquely) compute a triangulated",
    "arxiv_url": "https://arxiv.org/abs/2311.16042v1",
    "pdf_url": "https://arxiv.org/pdf/2311.16042v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2311.16042",
    "arxiv_authors": [
      "Jane Wu",
      "Diego Thomas",
      "Ronald Fedkiw"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Weakly-Supervised+3D+Reconstruction+of+Clothed+Humans+via+Normal+Maps+Jane+Wu+Diego+Thomas+Ronald+Fedkiw",
    "gs_search_success": true,
    "gs_authors": [
      "PYvmkT0AAAAJ",
      "WvE2HhEAAAAJ",
      "OoItXTkAAAAJ"
    ],
    "citation_count": 2,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2502.06428",
    "title": "CoS: Chain-of-Shot Prompting for Long Video Understanding",
    "year": 2025,
    "published": "2025-02-10T13:03:05Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Multi-modal Large Language Models (MLLMs) struggle with long videos due to the need for excessive visual tokens. These tokens exceed massively the context length of MLLMs, resulting in filled by redundant task-irrelevant shots. How to select shots is an unsolved critical problem: sparse sampling risks missing key details, while exhaustive sampling overwhelms the model with irrelevant content, leading to video misunderstanding. To solve this problem, we propose Chain-of-Shot prompting (CoS). The ",
    "arxiv_url": "https://arxiv.org/abs/2502.06428v2",
    "pdf_url": "https://arxiv.org/pdf/2502.06428v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2502.06428",
    "arxiv_authors": [
      "Jian Hu",
      "Zixu Cheng",
      "Chenyang Si",
      "Wei Li",
      "Shaogang Gong"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=CoS%3A+Chain-of-Shot+Prompting+for+Long+Video+Understanding+Jian+Hu+Zixu+Cheng+Chenyang+Si+Wei+Li+Shaogang+Gong",
    "gs_search_success": true,
    "gs_authors": [
      "XdahAuoAAAAJ",
      "Ex1Dgz0AAAAJ",
      "unJmXtoAAAAJ",
      "Nhi0I8cAAAAJ",
      "41KAd6AAAAAJ"
    ],
    "citation_count": 15,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2403.12770",
    "title": "Multispectral Image Restoration by Generalized Opponent Transformation Total Variation",
    "year": 2024,
    "published": "2024-03-19T14:34:44Z",
    "categories": [
      "cs.CV",
      "math.NA"
    ],
    "abstract": "Multispectral images (MSI) contain light information in different wavelengths of objects, which convey spectral-spatial information and help improve the performance of various image processing tasks. Numerous techniques have been created to extend the application of total variation regularization in restoring multispectral images, for example, based on channel coupling and adaptive total variation regularization. The primary contribution of this paper is to propose and develop a new multispectra",
    "arxiv_url": "https://arxiv.org/abs/2403.12770v1",
    "pdf_url": "https://arxiv.org/pdf/2403.12770v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2403.12770",
    "arxiv_authors": [
      "Zhantao Ma",
      "Michael K. Ng"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Multispectral+Image+Restoration+by+Generalized+Opponent+Transformation+Total+Variation+Zhantao+Ma+Michael+K.+Ng",
    "gs_search_success": true,
    "gs_authors": [
      "BBpjLiIAAAAJ",
      "Df0zjV8AAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 2
  },
  {
    "arxiv_id": "2301.10492",
    "title": "Flow-guided Semi-supervised Video Object Segmentation",
    "year": 2023,
    "published": "2023-01-25T10:02:31Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "We propose an optical flow-guided approach for semi-supervised video object segmentation. Optical flow is usually exploited as additional guidance information in unsupervised video object segmentation. However, its relevance in semi-supervised video object segmentation has not been fully explored. In this work, we follow an encoder-decoder approach to address the segmentation task. A model to extract the combined information from optical flow and the image is proposed, which is then used as inpu",
    "arxiv_url": "https://arxiv.org/abs/2301.10492v1",
    "pdf_url": "https://arxiv.org/pdf/2301.10492v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2301.10492",
    "arxiv_authors": [
      "Yushan Zhang",
      "Andreas Robinson",
      "Maria Magnusson",
      "Michael Felsberg"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Flow-guided+Semi-supervised+Video+Object+Segmentation+Yushan+Zhang+Andreas+Robinson+Maria+Magnusson+Michael+Felsberg",
    "gs_search_success": true,
    "gs_authors": [
      "_4Mg38AAAAAJ",
      "lkWfR08AAAAJ",
      "mvY4rdIAAAAJ"
    ],
    "citation_count": 2,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2310.06440",
    "title": "Solution for SMART-101 Challenge of ICCV Multi-modal Algorithmic Reasoning Task 2023",
    "year": 2023,
    "published": "2023-10-10T09:12:27Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "In this paper, we present our solution to a Multi-modal Algorithmic Reasoning Task: SMART-101 Challenge. Different from the traditional visual question-answering datasets, this challenge evaluates the abstraction, deduction, and generalization abilities of neural networks in solving visuolinguistic puzzles designed specifically for children in the 6-8 age group. We employed a divide-and-conquer approach. At the data level, inspired by the challenge paper, we categorized the whole questions into ",
    "arxiv_url": "https://arxiv.org/abs/2310.06440v1",
    "pdf_url": "https://arxiv.org/pdf/2310.06440v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2310.06440",
    "arxiv_authors": [
      "Xiangyu Wu",
      "Yang Yang",
      "Shengdong Xu",
      "Yifeng Wu",
      "Qingguo Chen",
      "Jianfeng Lu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Solution+for+SMART-101+Challenge+of+ICCV+Multi-modal+Algorithmic+Reasoning+Task+2023+Xiangyu+Wu+Yang+Yang+Shengdong+Xu+Yifeng+Wu+Qingguo+Chen",
    "gs_search_success": true,
    "gs_authors": [
      "_6NJip0AAAAJ",
      "GlqRHLcAAAAJ",
      "R0GjVWIAAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2305.01111",
    "title": "Local and Global Contextual Features Fusion for Pedestrian Intention Prediction",
    "year": 2023,
    "published": "2023-05-01T22:37:31Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "abstract": "Autonomous vehicles (AVs) are becoming an indispensable part of future transportation. However, safety challenges and lack of reliability limit their real-world deployment. Towards boosting the appearance of AVs on the roads, the interaction of AVs with pedestrians including \"prediction of the pedestrian crossing intention\" deserves extensive research. This is a highly challenging task as involves multiple non-linear parameters. In this direction, we extract and analyse spatio-temporal visual fe",
    "arxiv_url": "https://arxiv.org/abs/2305.01111v1",
    "pdf_url": "https://arxiv.org/pdf/2305.01111v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2305.01111",
    "arxiv_authors": [
      "Mohsen Azarmi",
      "Mahdi Rezaei",
      "Tanveer Hussain",
      "Chenghao Qian"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Local+and+Global+Contextual+Features+Fusion+for+Pedestrian+Intention+Prediction+Mohsen+Azarmi+Mahdi+Rezaei+Tanveer+Hussain+Chenghao+Qian",
    "gs_search_success": true,
    "gs_authors": [
      "9Z7J8ecAAAAJ",
      "gJmVV6kAAAAJ",
      "EFV2q-4AAAAJ"
    ],
    "citation_count": 15,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2304.12760",
    "title": "Parallel Spiking Neurons with High Efficiency and Ability to Learn Long-term Dependencies",
    "year": 2023,
    "published": "2023-04-25T12:19:18Z",
    "categories": [
      "cs.NE",
      "cs.CV"
    ],
    "abstract": "Vanilla spiking neurons in Spiking Neural Networks (SNNs) use charge-fire-reset neuronal dynamics, which can only be simulated serially and can hardly learn long-time dependencies. We find that when removing reset, the neuronal dynamics can be reformulated in a non-iterative form and parallelized. By rewriting neuronal dynamics without reset to a general formulation, we propose the Parallel Spiking Neuron (PSN), which generates hidden states that are independent of their predecessors, resulting ",
    "arxiv_url": "https://arxiv.org/abs/2304.12760v4",
    "pdf_url": "https://arxiv.org/pdf/2304.12760v4",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2304.12760",
    "arxiv_authors": [
      "Wei Fang",
      "Zhaofei Yu",
      "Zhaokun Zhou",
      "Ding Chen",
      "Yanqi Chen",
      "Zhengyu Ma",
      "Timothée Masquelier",
      "Yonghong Tian"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Parallel+Spiking+Neurons+with+High+Efficiency+and+Ability+to+Learn+Long-term+Dependencies+Wei+Fang+Zhaofei+Yu+Zhaokun+Zhou+Ding+Chen+Yanqi+Chen",
    "gs_search_success": true,
    "gs_authors": [
      "fn6hJx0AAAAJ",
      "qaUgD50AAAAJ",
      "e2lED2gAAAAJ",
      "fkzUZ-oAAAAJ",
      "QzFrppAAAAAJ",
      "21SR930AAAAJ",
      "4nz-h1QAAAAJ",
      "SZuNhvQAAAAJ"
    ],
    "citation_count": 77,
    "gs_author_count": 8
  },
  {
    "arxiv_id": "2404.17670",
    "title": "Federated Learning for Blind Image Super-Resolution",
    "year": 2024,
    "published": "2024-04-26T19:27:07Z",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV",
      "cs.ET",
      "cs.LG"
    ],
    "abstract": "Traditional blind image SR methods need to model real-world degradations precisely. Consequently, current research struggles with this dilemma by assuming idealized degradations, which leads to limited applicability to actual user data. Moreover, the ideal scenario - training models on data from the targeted user base - presents significant privacy concerns. To address both challenges, we propose to fuse image SR with federated learning, allowing real-world degradations to be directly learned fr",
    "arxiv_url": "https://arxiv.org/abs/2404.17670v1",
    "pdf_url": "https://arxiv.org/pdf/2404.17670v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2404.17670",
    "arxiv_authors": [
      "Brian B. Moser",
      "Ahmed Anwar",
      "Federico Raue",
      "Stanislav Frolov",
      "Andreas Dengel"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Federated+Learning+for+Blind+Image+Super-Resolution+Brian+B.+Moser+Ahmed+Anwar+Federico+Raue+Stanislav+Frolov+Andreas+Dengel",
    "gs_search_success": true,
    "gs_authors": [
      "v6hF0VEAAAAJ",
      "TL8npH8AAAAJ",
      "p3YP0DMAAAAJ",
      "Jx9-sU0AAAAJ",
      "ZQbmL4wAAAAJ"
    ],
    "citation_count": 5,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2311.13321",
    "title": "Revisiting Supervision for Continual Representation Learning",
    "year": 2023,
    "published": "2023-11-22T11:24:04Z",
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "abstract": "In the field of continual learning, models are designed to learn tasks one after the other. While most research has centered on supervised continual learning, there is a growing interest in unsupervised continual learning, which makes use of the vast amounts of unlabeled data. Recent studies have highlighted the strengths of unsupervised methods, particularly self-supervised learning, in providing robust representations. The improved transferability of those representations built with self-super",
    "arxiv_url": "https://arxiv.org/abs/2311.13321v2",
    "pdf_url": "https://arxiv.org/pdf/2311.13321v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2311.13321",
    "arxiv_authors": [
      "Daniel Marczak",
      "Sebastian Cygert",
      "Tomasz Trzciński",
      "Bartłomiej Twardowski"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Revisiting+Supervision+for+Continual+Representation+Learning+Daniel+Marczak+Sebastian+Cygert+Tomasz+Trzci%C5%84ski+Bart%C5%82omiej+Twardowski",
    "gs_search_success": true,
    "gs_authors": [
      "wLH9PP8AAAAJ",
      "8yywECgAAAAJ",
      "Vs4kBzQAAAAJ",
      "bJMRBFoAAAAJ"
    ],
    "citation_count": 9,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2312.08887",
    "title": "SpeedUpNet: A Plug-and-Play Adapter Network for Accelerating Text-to-Image Diffusion Models",
    "year": 2023,
    "published": "2023-12-13T09:42:04Z",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "Text-to-image diffusion models (SD) exhibit significant advancements while requiring extensive computational resources. Existing acceleration methods usually require extensive training and are not universally applicable. LCM-LoRA, trainable once for diverse models, offers universality but rarely considers ensuring the consistency of generated content before and after acceleration. This paper proposes SpeedUpNet (SUN), an innovative acceleration module, to address the challenges of universality a",
    "arxiv_url": "https://arxiv.org/abs/2312.08887v4",
    "pdf_url": "https://arxiv.org/pdf/2312.08887v4",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2312.08887",
    "arxiv_authors": [
      "Weilong Chai",
      "DanDan Zheng",
      "Jiajiong Cao",
      "Zhiquan Chen",
      "Changbao Wang",
      "Chenguang Ma"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=SpeedUpNet%3A+A+Plug-and-Play+Adapter+Network+for+Accelerating+Text-to-Image+Diffusion+Models+Weilong+Chai+DanDan+Zheng+Jiajiong+Cao+Zhiquan+Chen+Changbao+Wang",
    "gs_search_success": true,
    "gs_authors": [
      "Pj7t0wQAAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2502.17608",
    "title": "Data-Driven Pseudo-spectral Full Waveform Inversion via Deep Neural Networks",
    "year": 2025,
    "published": "2025-02-24T19:50:36Z",
    "categories": [
      "physics.geo-ph",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "FWI seeks to achieve a high-resolution model of the subsurface through the application of multi-variate optimization to the seismic inverse problem. Although now a mature technology, FWI has limitations related to the choice of the appropriate solver for the forward problem in challenging environments requiring complex assumptions, and very wide angle and multi-azimuth data necessary for full reconstruction are often not available.   Deep Learning techniques have emerged as excellent optimizatio",
    "arxiv_url": "https://arxiv.org/abs/2502.17608v1",
    "pdf_url": "https://arxiv.org/pdf/2502.17608v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2502.17608",
    "arxiv_authors": [
      "Christopher Zerafa",
      "Pauline Galea",
      "Cristiana Sebu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Data-Driven+Pseudo-spectral+Full+Waveform+Inversion+via+Deep+Neural+Networks+Christopher+Zerafa+Pauline+Galea+Cristiana+Sebu",
    "gs_search_success": true,
    "gs_authors": [
      "HhlhrC0AAAAJ",
      "-bdKqmUAAAAJ",
      "KQsEKnQAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2410.12332",
    "title": "MC-Bench: A Benchmark for Multi-Context Visual Grounding in the Era of MLLMs",
    "year": 2024,
    "published": "2024-10-16T07:52:57Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "While multimodal large language models (MLLMs) have demonstrated extraordinary vision-language understanding capabilities, their abilities to solve instance-level visual-language problems beyond a single image warrant further exploration. To assess these unproven abilities of MLLMs, this paper proposes a new visual grounding task called multi-context visual grounding, which aims to localize instances of interest across multiple images based on open-ended text prompts. In order to facilitate this",
    "arxiv_url": "https://arxiv.org/abs/2410.12332v2",
    "pdf_url": "https://arxiv.org/pdf/2410.12332v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2410.12332",
    "arxiv_authors": [
      "Yunqiu Xu",
      "Linchao Zhu",
      "Yi Yang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=MC-Bench%3A+A+Benchmark+for+Multi-Context+Visual+Grounding+in+the+Era+of+MLLMs+Yunqiu+Xu+Linchao+Zhu+Yi+Yang",
    "gs_search_success": true,
    "gs_authors": [
      "RMSuNFwAAAAJ",
      "SdJX4nAAAAAJ",
      "9ZukE28AAAAJ"
    ],
    "citation_count": 11,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2411.04692",
    "title": "Personalized Federated Learning for Cross-view Geo-localization",
    "year": 2024,
    "published": "2024-11-07T13:25:52Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "In this paper we propose a methodology combining Federated Learning (FL) with Cross-view Image Geo-localization (CVGL) techniques. We address the challenges of data privacy and heterogeneity in autonomous vehicle environments by proposing a personalized Federated Learning scenario that allows selective sharing of model parameters. Our method implements a coarse-to-fine approach, where clients share only the coarse feature extractors while keeping fine-grained features specific to local environme",
    "arxiv_url": "https://arxiv.org/abs/2411.04692v1",
    "pdf_url": "https://arxiv.org/pdf/2411.04692v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2411.04692",
    "arxiv_authors": [
      "Christos Anagnostopoulos",
      "Alexandros Gkillas",
      "Nikos Piperigkos",
      "Aris S. Lalos"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Personalized+Federated+Learning+for+Cross-view+Geo-localization+Christos+Anagnostopoulos+Alexandros+Gkillas+Nikos+Piperigkos+Aris+S.+Lalos",
    "gs_search_success": true,
    "gs_authors": [
      "WdZRoPQAAAAJ",
      "Sz5RSbQAAAAJ",
      "k2etYWUAAAAJ",
      "Tk1EffEAAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2310.02997",
    "title": "Optimizing Key-Selection for Face-based One-Time Biometrics via Morphing",
    "year": 2023,
    "published": "2023-10-04T17:32:32Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Nowadays, facial recognition systems are still vulnerable to adversarial attacks. These attacks vary from simple perturbations of the input image to modifying the parameters of the recognition model to impersonate an authorised subject. So-called privacy-enhancing facial recognition systems have been mostly developed to provide protection of stored biometric reference data, i.e. templates. In the literature, privacy-enhancing facial recognition approaches have focused solely on conventional secu",
    "arxiv_url": "https://arxiv.org/abs/2310.02997v1",
    "pdf_url": "https://arxiv.org/pdf/2310.02997v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2310.02997",
    "arxiv_authors": [
      "Daile Osorio-Roig",
      "Mahdi Ghafourian",
      "Christian Rathgeb",
      "Ruben Vera-Rodriguez",
      "Christoph Busch",
      "Julian Fierrez"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Optimizing+Key-Selection+for+Face-based+One-Time+Biometrics+via+Morphing+Daile+Osorio-Roig+Mahdi+Ghafourian+Christian+Rathgeb+Ruben+Vera-Rodriguez+Christoph+Busch",
    "gs_search_success": true,
    "gs_authors": [
      "_itMaUcAAAAJ",
      "_GH2PrYAAAAJ",
      "AWj5Z3sAAAAJ",
      "HbG_NOoAAAAJ",
      "KYMQ0tsAAAAJ",
      "qsopcXIAAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2501.11992",
    "title": "Survey on Hand Gesture Recognition from Visual Input",
    "year": 2025,
    "published": "2025-01-21T09:23:22Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Hand gesture recognition has become an important research area, driven by the growing demand for human-computer interaction in fields such as sign language recognition, virtual and augmented reality, and robotics. Despite the rapid growth of the field, there are few surveys that comprehensively cover recent research developments, available solutions, and benchmark datasets. This survey addresses this gap by examining the latest advancements in hand gesture and 3D hand pose recognition from vario",
    "arxiv_url": "https://arxiv.org/abs/2501.11992v3",
    "pdf_url": "https://arxiv.org/pdf/2501.11992v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2501.11992",
    "arxiv_authors": [
      "Manousos Linardakis",
      "Iraklis Varlamis",
      "Georgios Th. Papadopoulos"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Survey+on+Hand+Gesture+Recognition+from+Visual+Input+Manousos+Linardakis+Iraklis+Varlamis+Georgios+Th.+Papadopoulos",
    "gs_search_success": true,
    "gs_authors": [
      "pJA7rO8AAAAJ",
      "DgpNAI0AAAAJ",
      "SUyTkTAAAAAJ"
    ],
    "citation_count": 17,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2408.00372",
    "title": "Few-shot Defect Image Generation based on Consistency Modeling",
    "year": 2024,
    "published": "2024-08-01T08:29:42Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Image generation can solve insufficient labeled data issues in defect detection. Most defect generation methods are only trained on a single product without considering the consistencies among multiple products, leading to poor quality and diversity of generated results. To address these issues, we propose DefectDiffu, a novel text-guided diffusion method to model both intra-product background consistency and inter-product defect consistency across multiple products and modulate the consistency ",
    "arxiv_url": "https://arxiv.org/abs/2408.00372v1",
    "pdf_url": "https://arxiv.org/pdf/2408.00372v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2408.00372",
    "arxiv_authors": [
      "Qingfeng Shi",
      "Jing Wei",
      "Fei Shen",
      "Zhengtao Zhang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Few-shot+Defect+Image+Generation+based+on+Consistency+Modeling+Qingfeng+Shi+Jing+Wei+Fei+Shen+Zhengtao+Zhang",
    "gs_search_success": true,
    "gs_authors": [
      "k1IruYkAAAAJ"
    ],
    "citation_count": 10,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2308.06341",
    "title": "Surrogate Model for Geological CO2 Storage and Its Use in Hierarchical MCMC History Matching",
    "year": 2023,
    "published": "2023-08-11T18:29:28Z",
    "categories": [
      "cs.CV",
      "physics.geo-ph"
    ],
    "abstract": "Deep-learning-based surrogate models show great promise for use in geological carbon storage operations. In this work we target an important application - the history matching of storage systems characterized by a high degree of (prior) geological uncertainty. Toward this goal, we extend the recently introduced recurrent R-U-Net surrogate model to treat geomodel realizations drawn from a wide range of geological scenarios. These scenarios are defined by a set of metaparameters, which include the",
    "arxiv_url": "https://arxiv.org/abs/2308.06341v2",
    "pdf_url": "https://arxiv.org/pdf/2308.06341v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2308.06341",
    "arxiv_authors": [
      "Yifu Han",
      "Francois P. Hamon",
      "Su Jiang",
      "Louis J. Durlofsky"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Surrogate+Model+for+Geological+CO2+Storage+and+Its+Use+in+Hierarchical+MCMC+History+Matching+Yifu+Han+Francois+P.+Hamon+Su+Jiang+Louis+J.+Durlofsky",
    "gs_search_success": true,
    "gs_authors": [
      "Rgy3AGoAAAAJ",
      "lb7BrecAAAAJ",
      "M9oFDycAAAAJ",
      "kC6vxm4AAAAJ"
    ],
    "citation_count": 27,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2311.18448",
    "title": "HOLD: Category-agnostic 3D Reconstruction of Interacting Hands and Objects from Video",
    "year": 2023,
    "published": "2023-11-30T10:50:35Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Since humans interact with diverse objects every day, the holistic 3D capture of these interactions is important to understand and model human behaviour. However, most existing methods for hand-object reconstruction from RGB either assume pre-scanned object templates or heavily rely on limited 3D hand-object data, restricting their ability to scale and generalize to more unconstrained interaction settings. To this end, we introduce HOLD -- the first category-agnostic method that reconstructs an ",
    "arxiv_url": "https://arxiv.org/abs/2311.18448v1",
    "pdf_url": "https://arxiv.org/pdf/2311.18448v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2311.18448",
    "arxiv_authors": [
      "Zicong Fan",
      "Maria Parelli",
      "Maria Eleni Kadoglou",
      "Muhammed Kocabas",
      "Xu Chen",
      "Michael J. Black",
      "Otmar Hilliges"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=HOLD%3A+Category-agnostic+3D+Reconstruction+of+Interacting+Hands+and+Objects+from+Video+Zicong+Fan+Maria+Parelli+Maria+Eleni+Kadoglou+Muhammed+Kocabas+Xu+Chen",
    "gs_search_success": true,
    "gs_authors": [
      "-epU9OsAAAAJ",
      "zkUFoAIAAAAJ",
      "jZXvcOsAAAAJ",
      "6NjbexEAAAAJ",
      "H4ioz9YAAAAJ",
      "ipSS2ToAAAAJ"
    ],
    "citation_count": 60,
    "gs_author_count": 7
  },
  {
    "arxiv_id": "2310.13772",
    "title": "TexFusion: Synthesizing 3D Textures with Text-Guided Image Diffusion Models",
    "year": 2023,
    "published": "2023-10-20T19:15:29Z",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "We present TexFusion (Texture Diffusion), a new method to synthesize textures for given 3D geometries, using large-scale text-guided image diffusion models. In contrast to recent works that leverage 2D text-to-image diffusion models to distill 3D objects using a slow and fragile optimization process, TexFusion introduces a new 3D-consistent generation technique specifically designed for texture synthesis that employs regular diffusion model sampling on different 2D rendered views. Specifically, ",
    "arxiv_url": "https://arxiv.org/abs/2310.13772v1",
    "pdf_url": "https://arxiv.org/pdf/2310.13772v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2310.13772",
    "arxiv_authors": [
      "Tianshi Cao",
      "Karsten Kreis",
      "Sanja Fidler",
      "Nicholas Sharp",
      "Kangxue Yin"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=TexFusion%3A+Synthesizing+3D+Textures+with+Text-Guided+Image+Diffusion+Models+Tianshi+Cao+Karsten+Kreis+Sanja+Fidler+Nicholas+Sharp+Kangxue+Yin",
    "gs_search_success": true,
    "gs_authors": [
      "CUlqK5EAAAAJ",
      "K7CRPucAAAAJ",
      "1YasCXcAAAAJ",
      "rFd-DiAAAAAJ",
      "CZ9wBBoAAAAJ"
    ],
    "citation_count": 144,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2406.15104",
    "title": "Deciphering the Definition of Adversarial Robustness for post-hoc OOD Detectors",
    "year": 2024,
    "published": "2024-06-21T12:45:07Z",
    "categories": [
      "cs.CR",
      "cs.CV"
    ],
    "abstract": "Detecting out-of-distribution (OOD) inputs is critical for safely deploying deep learning models in real-world scenarios. In recent years, many OOD detectors have been developed, and even the benchmarking has been standardized, i.e. OpenOOD. The number of post-hoc detectors is growing fast. They are showing an option to protect a pre-trained classifier against natural distribution shifts and claim to be ready for real-world scenarios. However, its effectiveness in dealing with adversarial exampl",
    "arxiv_url": "https://arxiv.org/abs/2406.15104v5",
    "pdf_url": "https://arxiv.org/pdf/2406.15104v5",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2406.15104",
    "arxiv_authors": [
      "Peter Lorenz",
      "Mario Fernandez",
      "Jens Müller",
      "Ullrich Köthe"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Deciphering+the+Definition+of+Adversarial+Robustness+for+post-hoc+OOD+Detectors+Peter+Lorenz+Mario+Fernandez+Jens+M%C3%BCller+Ullrich+K%C3%B6the",
    "gs_search_success": true,
    "gs_authors": [
      "gt-yaNMAAAAJ",
      "ayN8HoQAAAAJ",
      "sb4hPQMAAAAJ"
    ],
    "citation_count": 3,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2408.01942",
    "title": "Visual Grounding for Object-Level Generalization in Reinforcement Learning",
    "year": 2024,
    "published": "2024-08-04T06:34:24Z",
    "categories": [
      "cs.AI",
      "cs.CV"
    ],
    "abstract": "Generalization is a pivotal challenge for agents following natural language instructions. To approach this goal, we leverage a vision-language model (VLM) for visual grounding and transfer its vision-language knowledge into reinforcement learning (RL) for object-centric tasks, which makes the agent capable of zero-shot generalization to unseen objects and instructions. By visual grounding, we obtain an object-grounded confidence map for the target object indicated in the instruction. Based on th",
    "arxiv_url": "https://arxiv.org/abs/2408.01942v1",
    "pdf_url": "https://arxiv.org/pdf/2408.01942v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2408.01942",
    "arxiv_authors": [
      "Haobin Jiang",
      "Zongqing Lu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Visual+Grounding+for+Object-Level+Generalization+in+Reinforcement+Learning+Haobin+Jiang+Zongqing+Lu",
    "gs_search_success": true,
    "gs_authors": [
      "5Oc2LAEAAAAJ",
      "k3IFtTYAAAAJ"
    ],
    "citation_count": 7,
    "gs_author_count": 2
  },
  {
    "arxiv_id": "2505.24878",
    "title": "Open CaptchaWorld: A Comprehensive Web-based Platform for Testing and Benchmarking Multimodal LLM Agents",
    "year": 2025,
    "published": "2025-05-30T17:59:55Z",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "CAPTCHAs have been a critical bottleneck for deploying web agents in real-world applications, often blocking them from completing end-to-end automation tasks. While modern multimodal LLM agents have demonstrated impressive performance in static perception tasks, their ability to handle interactive, multi-step reasoning challenges like CAPTCHAs is largely untested. To address this gap, we introduce Open CaptchaWorld, the first web-based benchmark and platform specifically designed to evaluate the",
    "arxiv_url": "https://arxiv.org/abs/2505.24878v1",
    "pdf_url": "https://arxiv.org/pdf/2505.24878v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2505.24878",
    "arxiv_authors": [
      "Yaxin Luo",
      "Zhaoyi Li",
      "Jiacheng Liu",
      "Jiacheng Cui",
      "Xiaohan Zhao",
      "Zhiqiang Shen"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Open+CaptchaWorld%3A+A+Comprehensive+Web-based+Platform+for+Testing+and+Benchmarking+Multimodal+LLM+Agents+Yaxin+Luo+Zhaoyi+Li+Jiacheng+Liu+Jiacheng+Cui+Xiaohan+Zhao",
    "gs_search_success": false,
    "gs_authors": [],
    "citation_count": null,
    "error_type": "fetch_failed",
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2310.02792",
    "title": "Continuous 3D Myocardial Motion Tracking via Echocardiography",
    "year": 2023,
    "published": "2023-10-04T13:11:20Z",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "abstract": "Myocardial motion tracking stands as an essential clinical tool in the prevention and detection of cardiovascular diseases (CVDs), the foremost cause of death globally. However, current techniques suffer from incomplete and inaccurate motion estimation of the myocardium in both spatial and temporal dimensions, hindering the early identification of myocardial dysfunction. To address these challenges, this paper introduces the Neural Cardiac Motion Field (NeuralCMF). NeuralCMF leverages implicit n",
    "arxiv_url": "https://arxiv.org/abs/2310.02792v2",
    "pdf_url": "https://arxiv.org/pdf/2310.02792v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2310.02792",
    "arxiv_authors": [
      "Chengkang Shen",
      "Hao Zhu",
      "You Zhou",
      "Yu Liu",
      "Si Yi",
      "Lili Dong",
      "Weipeng Zhao",
      "David J. Brady",
      "Xun Cao",
      "Zhan Ma",
      "Yi Lin"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Continuous+3D+Myocardial+Motion+Tracking+via+Echocardiography+Chengkang+Shen+Hao+Zhu+You+Zhou+Yu+Liu+Si+Yi",
    "gs_search_success": true,
    "gs_authors": [
      "78KxtRMAAAAJ",
      "UC57uVQAAAAJ",
      "97GexEYAAAAJ",
      "CcSZwTsAAAAJ",
      "8hZIngIAAAAJ"
    ],
    "citation_count": 6,
    "gs_author_count": 9
  },
  {
    "arxiv_id": "2503.11750",
    "title": "Making Every Step Effective: Jailbreaking Large Vision-Language Models Through Hierarchical KV Equalization",
    "year": 2025,
    "published": "2025-03-14T17:57:42Z",
    "categories": [
      "cs.CV",
      "cs.CR"
    ],
    "abstract": "In the realm of large vision-language models (LVLMs), adversarial jailbreak attacks serve as a red-teaming approach to identify safety vulnerabilities of these models and their associated defense mechanisms. However, we identify a critical limitation: not every adversarial optimization step leads to a positive outcome, and indiscriminately accepting optimization results at each step may reduce the overall attack success rate. To address this challenge, we introduce HKVE (Hierarchical Key-Value E",
    "arxiv_url": "https://arxiv.org/abs/2503.11750v1",
    "pdf_url": "https://arxiv.org/pdf/2503.11750v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2503.11750",
    "arxiv_authors": [
      "Shuyang Hao",
      "Yiwei Wang",
      "Bryan Hooi",
      "Jun Liu",
      "Muhao Chen",
      "Zi Huang",
      "Yujun Cai"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Making+Every+Step+Effective%3A+Jailbreaking+Large+Vision-Language+Models+Through+Hierarchical+KV+Equalization+Shuyang+Hao+Yiwei+Wang+Bryan+Hooi+Jun+Liu+Muhao+Chen",
    "gs_search_success": true,
    "gs_authors": [
      "Q5Ild8UAAAAJ",
      "TE7lbQwAAAAJ",
      "ErEL3bgAAAAJ",
      "iAWMsgEAAAAJ",
      "k79yEZkAAAAJ",
      "Sh9QvBkAAAAJ",
      "7_wHcGoAAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 7
  },
  {
    "arxiv_id": "2310.05773",
    "title": "Towards Lossless Dataset Distillation via Difficulty-Aligned Trajectory Matching",
    "year": 2023,
    "published": "2023-10-09T14:57:41Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "The ultimate goal of Dataset Distillation is to synthesize a small synthetic dataset such that a model trained on this synthetic set will perform equally well as a model trained on the full, real dataset. Until now, no method of Dataset Distillation has reached this completely lossless goal, in part due to the fact that previous methods only remain effective when the total number of synthetic samples is extremely small. Since only so much information can be contained in such a small number of sa",
    "arxiv_url": "https://arxiv.org/abs/2310.05773v2",
    "pdf_url": "https://arxiv.org/pdf/2310.05773v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2310.05773",
    "arxiv_authors": [
      "Ziyao Guo",
      "Kai Wang",
      "George Cazenavette",
      "Hui Li",
      "Kaipeng Zhang",
      "Yang You"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Towards+Lossless+Dataset+Distillation+via+Difficulty-Aligned+Trajectory+Matching+Ziyao+Guo+Kai+Wang+George+Cazenavette+Hui+Li+Kaipeng+Zhang",
    "gs_search_success": false,
    "gs_authors": [],
    "citation_count": null,
    "error_type": "fetch_failed",
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2311.09355",
    "title": "Privacy Threats in Stable Diffusion Models",
    "year": 2023,
    "published": "2023-11-15T20:31:40Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "abstract": "This paper introduces a novel approach to membership inference attacks (MIA) targeting stable diffusion computer vision models, specifically focusing on the highly sophisticated Stable Diffusion V2 by StabilityAI. MIAs aim to extract sensitive information about a model's training data, posing significant privacy concerns. Despite its advancements in image synthesis, our research reveals privacy vulnerabilities in the stable diffusion models' outputs. Exploiting this information, we devise a blac",
    "arxiv_url": "https://arxiv.org/abs/2311.09355v1",
    "pdf_url": "https://arxiv.org/pdf/2311.09355v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2311.09355",
    "arxiv_authors": [
      "Thomas Cilloni",
      "Charles Fleming",
      "Charles Walter"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Privacy+Threats+in+Stable+Diffusion+Models+Thomas+Cilloni+Charles+Fleming+Charles+Walter",
    "gs_search_success": true,
    "gs_authors": [
      "DUcwAAsAAAAJ",
      "QS_ijiwAAAAJ"
    ],
    "citation_count": 4,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2504.07960",
    "title": "VisualCloze: A Universal Image Generation Framework via Visual In-Context Learning",
    "year": 2025,
    "published": "2025-04-10T17:59:42Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Recent progress in diffusion models significantly advances various image generation tasks. However, the current mainstream approach remains focused on building task-specific models, which have limited efficiency when supporting a wide range of different needs. While universal models attempt to address this limitation, they face critical challenges, including generalizable task instruction, appropriate task distributions, and unified architectural design. To tackle these challenges, we propose Vi",
    "arxiv_url": "https://arxiv.org/abs/2504.07960v2",
    "pdf_url": "https://arxiv.org/pdf/2504.07960v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2504.07960",
    "arxiv_authors": [
      "Zhong-Yu Li",
      "Ruoyi Du",
      "Juncheng Yan",
      "Le Zhuo",
      "Zhen Li",
      "Peng Gao",
      "Zhanyu Ma",
      "Ming-Ming Cheng"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=VisualCloze%3A+A+Universal+Image+Generation+Framework+via+Visual+In-Context+Learning+Zhong-Yu+Li+Ruoyi+Du+Juncheng+Yan+Le+Zhuo+Zhen+Li",
    "gs_search_success": true,
    "gs_authors": [
      "EhaGFnMAAAAJ",
      "utI5YFEAAAAJ",
      "g6WHXrgAAAAJ",
      "NxNC8qgAAAAJ",
      "5GAAs7IAAAAJ",
      "DbRZSaoAAAAJ",
      "huWpVyEAAAAJ"
    ],
    "citation_count": 3,
    "gs_author_count": 8
  },
  {
    "arxiv_id": "2306.15244",
    "title": "Cutting-Edge Techniques for Depth Map Super-Resolution",
    "year": 2023,
    "published": "2023-06-27T06:57:08Z",
    "categories": [
      "cs.CV",
      "eess.IV"
    ],
    "abstract": "To overcome hardware limitations in commercially available depth sensors which result in low-resolution depth maps, depth map super-resolution (DMSR) is a practical and valuable computer vision task. DMSR requires upscaling a low-resolution (LR) depth map into a high-resolution (HR) space. Joint image filtering for DMSR has been applied using spatially-invariant and spatially-variant convolutional neural network (CNN) approaches. In this project, we propose a novel joint image filtering DMSR alg",
    "arxiv_url": "https://arxiv.org/abs/2306.15244v1",
    "pdf_url": "https://arxiv.org/pdf/2306.15244v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2306.15244",
    "arxiv_authors": [
      "Ryan Peterson",
      "Josiah Smith"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Cutting-Edge+Techniques+for+Depth+Map+Super-Resolution+Ryan+Peterson+Josiah+Smith",
    "gs_search_success": true,
    "gs_authors": [
      "tZAFU0cAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 2
  },
  {
    "arxiv_id": "2311.02538",
    "title": "Dense Video Captioning: A Survey of Techniques, Datasets and Evaluation Protocols",
    "year": 2023,
    "published": "2023-11-05T01:45:31Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Untrimmed videos have interrelated events, dependencies, context, overlapping events, object-object interactions, domain specificity, and other semantics that are worth highlighting while describing a video in natural language. Owing to such a vast diversity, a single sentence can only correctly describe a portion of the video. Dense Video Captioning (DVC) aims at detecting and describing different events in a given video. The term DVC originated in the 2017 ActivityNet challenge, after which co",
    "arxiv_url": "https://arxiv.org/abs/2311.02538v1",
    "pdf_url": "https://arxiv.org/pdf/2311.02538v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2311.02538",
    "arxiv_authors": [
      "Iqra Qasim",
      "Alexander Horsch",
      "Dilip K. Prasad"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Dense+Video+Captioning%3A+A+Survey+of+Techniques%2C+Datasets+and+Evaluation+Protocols+Iqra+Qasim+Alexander+Horsch+Dilip+K.+Prasad",
    "gs_search_success": true,
    "gs_authors": [
      "a6ZNU7EAAAAJ",
      "kbjwuEIAAAAJ",
      "dLlhclQAAAAJ"
    ],
    "citation_count": 13,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2306.06354",
    "title": "EventCLIP: Adapting CLIP for Event-based Object Recognition",
    "year": 2023,
    "published": "2023-06-10T06:05:35Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Recent advances in zero-shot and few-shot classification heavily rely on the success of pre-trained vision-language models (VLMs) such as CLIP. Due to a shortage of large-scale datasets, training such models for event camera data remains infeasible. Thus, adapting existing VLMs across modalities to event vision is an important research challenge. In this work, we introduce EventCLIP, a novel approach that utilizes CLIP for zero-shot and few-shot event-based object recognition. We first generaliz",
    "arxiv_url": "https://arxiv.org/abs/2306.06354v3",
    "pdf_url": "https://arxiv.org/pdf/2306.06354v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2306.06354",
    "arxiv_authors": [
      "Ziyi Wu",
      "Xudong Liu",
      "Igor Gilitschenski"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=EventCLIP%3A+Adapting+CLIP+for+Event-based+Object+Recognition+Ziyi+Wu+Xudong+Liu+Igor+Gilitschenski",
    "gs_search_success": true,
    "gs_authors": [
      "VK2CEbgAAAAJ",
      "iopH6wIAAAAJ",
      "Nuw1Y4oAAAAJ"
    ],
    "citation_count": 33,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2505.04281",
    "title": "TS-Diff: Two-Stage Diffusion Model for Low-Light RAW Image Enhancement",
    "year": 2025,
    "published": "2025-05-07T09:35:05Z",
    "categories": [
      "cs.CV",
      "eess.IV"
    ],
    "abstract": "This paper presents a novel Two-Stage Diffusion Model (TS-Diff) for enhancing extremely low-light RAW images. In the pre-training stage, TS-Diff synthesizes noisy images by constructing multiple virtual cameras based on a noise space. Camera Feature Integration (CFI) modules are then designed to enable the model to learn generalizable features across diverse virtual cameras. During the aligning stage, CFIs are averaged to create a target-specific CFI$^T$, which is fine-tuned using a small amount",
    "arxiv_url": "https://arxiv.org/abs/2505.04281v1",
    "pdf_url": "https://arxiv.org/pdf/2505.04281v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2505.04281",
    "arxiv_authors": [
      "Yi Li",
      "Zhiyuan Zhang",
      "Jiangnan Xia",
      "Jianghan Cheng",
      "Qilong Wu",
      "Junwei Li",
      "Yibin Tian",
      "Hui Kong"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=TS-Diff%3A+Two-Stage+Diffusion+Model+for+Low-Light+RAW+Image+Enhancement+Yi+Li+Zhiyuan+Zhang+Jiangnan+Xia+Jianghan+Cheng+Qilong+Wu",
    "gs_search_success": true,
    "gs_authors": [
      "ziSJdRYAAAAJ",
      "Glqv2ooAAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 8
  },
  {
    "arxiv_id": "2311.10492",
    "title": "A Relay System for Semantic Image Transmission based on Shared Feature Extraction and Hyperprior Entropy Compression",
    "year": 2023,
    "published": "2023-11-17T12:45:30Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Nowadays, the need for high-quality image reconstruction and restoration is more and more urgent. However, most image transmission systems may suffer from image quality degradation or transmission interruption in the face of interference such as channel noise and link fading. To solve this problem, a relay communication network for semantic image transmission based on shared feature extraction and hyperprior entropy compression (HEC) is proposed, where the shared feature extraction technology ba",
    "arxiv_url": "https://arxiv.org/abs/2311.10492v1",
    "pdf_url": "https://arxiv.org/pdf/2311.10492v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2311.10492",
    "arxiv_authors": [
      "Wannian An",
      "Zhicheng Bao",
      "Haotai Liang",
      "Chen Dong",
      "Xiaodong"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=A+Relay+System+for+Semantic+Image+Transmission+based+on+Shared+Feature+Extraction+and+Hyperprior+Entropy+Compression+Wannian+An+Zhicheng+Bao+Haotai+Liang+Chen+Dong+Xiaodong",
    "gs_search_success": true,
    "gs_authors": [
      "GONSjCkAAAAJ",
      "PvSS6Z0AAAAJ"
    ],
    "citation_count": 12,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2504.15384",
    "title": "ICGM-FRAX: Iterative Cross Graph Matching for Hip Fracture Risk Assessment using Dual-energy X-ray Absorptiometry Images",
    "year": 2025,
    "published": "2025-04-21T18:52:15Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Hip fractures represent a major health concern, particularly among the elderly, often leading decreased mobility and increased mortality. Early and accurate detection of at risk individuals is crucial for effective intervention. In this study, we propose Iterative Cross Graph Matching for Hip Fracture Risk Assessment (ICGM-FRAX), a novel approach for predicting hip fractures using Dual-energy X-ray Absorptiometry (DXA) images. ICGM-FRAX involves iteratively comparing a test (subject) graph with ",
    "arxiv_url": "https://arxiv.org/abs/2504.15384v1",
    "pdf_url": "https://arxiv.org/pdf/2504.15384v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2504.15384",
    "arxiv_authors": [
      "Chen Zhao",
      "Anjum Shaik",
      "Joyce H. Keyak",
      "Nancy E. Lane",
      "Jeffrey D. Deng",
      "Kuan-Jui Su",
      "Qiuying Sha",
      "Hui Shen",
      "Hong-Wen Deng",
      "Weihua Zhou"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=ICGM-FRAX%3A+Iterative+Cross+Graph+Matching+for+Hip+Fracture+Risk+Assessment+using+Dual-energy+X-ray+Absorptiometry+Images+Chen+Zhao+Anjum+Shaik+Joyce+H.+Keyak+Nancy+E.+Lane+Jeffrey+D.+Deng",
    "gs_search_success": true,
    "gs_authors": [
      "YC76bh8AAAAJ",
      "quejwe0AAAAJ",
      "c-yfuI8AAAAJ",
      "m2lVp1AAAAAJ",
      "EsMeLP8AAAAJ",
      "imnS0CMAAAAJ",
      "xC77lA8AAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 9
  },
  {
    "arxiv_id": "2403.03431",
    "title": "Towards Understanding Cross and Self-Attention in Stable Diffusion for Text-Guided Image Editing",
    "year": 2024,
    "published": "2024-03-06T03:32:56Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Deep Text-to-Image Synthesis (TIS) models such as Stable Diffusion have recently gained significant popularity for creative Text-to-image generation. Yet, for domain-specific scenarios, tuning-free Text-guided Image Editing (TIE) is of greater importance for application developers, which modify objects or object properties in images by manipulating feature components in attention layers during the generation process. However, little is known about what semantic meanings these attention layers ha",
    "arxiv_url": "https://arxiv.org/abs/2403.03431v1",
    "pdf_url": "https://arxiv.org/pdf/2403.03431v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2403.03431",
    "arxiv_authors": [
      "Bingyan Liu",
      "Chengyu Wang",
      "Tingfeng Cao",
      "Kui Jia",
      "Jun Huang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Towards+Understanding+Cross+and+Self-Attention+in+Stable+Diffusion+for+Text-Guided+Image+Editing+Bingyan+Liu+Chengyu+Wang+Tingfeng+Cao+Kui+Jia+Jun+Huang",
    "gs_search_success": true,
    "gs_authors": [
      "2-scyfwAAAAJ",
      "Mf9VHRcAAAAJ",
      "M2IxAaQAAAAJ",
      "_AVfRnQAAAAJ"
    ],
    "citation_count": 127,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2307.07313",
    "title": "HEAL-SWIN: A Vision Transformer On The Sphere",
    "year": 2023,
    "published": "2023-07-14T12:46:59Z",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "High-resolution wide-angle fisheye images are becoming more and more important for robotics applications such as autonomous driving. However, using ordinary convolutional neural networks or vision transformers on this data is problematic due to projection and distortion losses introduced when projecting to a rectangular grid on the plane. We introduce the HEAL-SWIN transformer, which combines the highly uniform Hierarchical Equal Area iso-Latitude Pixelation (HEALPix) grid used in astrophysics a",
    "arxiv_url": "https://arxiv.org/abs/2307.07313v2",
    "pdf_url": "https://arxiv.org/pdf/2307.07313v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2307.07313",
    "arxiv_authors": [
      "Oscar Carlsson",
      "Jan E. Gerken",
      "Hampus Linander",
      "Heiner Spieß",
      "Fredrik Ohlsson",
      "Christoffer Petersson",
      "Daniel Persson"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=HEAL-SWIN%3A+A+Vision+Transformer+On+The+Sphere+Oscar+Carlsson+Jan+E.+Gerken+Hampus+Linander+Heiner+Spie%C3%9F+Fredrik+Ohlsson",
    "gs_search_success": true,
    "gs_authors": [
      "NIdlVIEAAAAJ",
      "jWZnXE4AAAAJ",
      "7q0gS3oAAAAJ",
      "n6SQ8doAAAAJ",
      "aMu5YbUAAAAJ",
      "SeRMUJwAAAAJ"
    ],
    "citation_count": 15,
    "gs_author_count": 7
  },
  {
    "arxiv_id": "2403.09948",
    "title": "RadCLIP: Enhancing Radiologic Image Analysis through Contrastive Language-Image Pre-training",
    "year": 2024,
    "published": "2024-03-15T01:18:08Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "The integration of artificial intelligence (AI) with radiology marks a transformative era in medicine. Vision foundation models have been adopted to enhance radiologic imaging analysis. However, the distinct complexities of radiologic 2D and 3D radiologic data pose unique challenges that existing models, pre-trained on general non-medical images, fail to address adequately. To bridge this gap and capitalize on the diagnostic precision required in radiologic imaging, we introduce Radiologic Contr",
    "arxiv_url": "https://arxiv.org/abs/2403.09948v3",
    "pdf_url": "https://arxiv.org/pdf/2403.09948v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2403.09948",
    "arxiv_authors": [
      "Zhixiu Lu",
      "Hailong Li",
      "Nehal A. Parikh",
      "Jonathan R. Dillman",
      "Lili He"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=RadCLIP%3A+Enhancing+Radiologic+Image+Analysis+through+Contrastive+Language-Image+Pre-training+Zhixiu+Lu+Hailong+Li+Nehal+A.+Parikh+Jonathan+R.+Dillman+Lili+He",
    "gs_search_success": true,
    "gs_authors": [
      "N-sWlRkAAAAJ",
      "2UIwStsAAAAJ",
      "Oi5zZksAAAAJ",
      "mXXToLAAAAAJ"
    ],
    "citation_count": 11,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2311.13335",
    "title": "Quantum learning and essential cognition under the traction of meta-characteristics in an open world",
    "year": 2023,
    "published": "2023-11-22T11:55:41Z",
    "categories": [
      "cs.AI",
      "cs.CV"
    ],
    "abstract": "Artificial intelligence has made significant progress in the Close World problem, being able to accurately recognize old knowledge through training and classification. However, AI faces significant challenges in the Open World problem, as it involves a new and unknown exploration journey. AI is not inherently proactive in exploration, and its challenge lies in not knowing how to approach and adapt to the unknown world. How do humans acquire knowledge of the unknown world. Humans identify new kno",
    "arxiv_url": "https://arxiv.org/abs/2311.13335v1",
    "pdf_url": "https://arxiv.org/pdf/2311.13335v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2311.13335",
    "arxiv_authors": [
      "Jin Wang",
      "Changlin Song"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Quantum+learning+and+essential+cognition+under+the+traction+of+meta-characteristics+in+an+open+world+Jin+Wang+Changlin+Song",
    "gs_search_success": true,
    "gs_authors": [],
    "citation_count": 1,
    "gs_author_count": 2
  },
  {
    "arxiv_id": "2303.17189",
    "title": "LayoutDiffusion: Controllable Diffusion Model for Layout-to-image Generation",
    "year": 2023,
    "published": "2023-03-30T06:56:12Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Recently, diffusion models have achieved great success in image synthesis. However, when it comes to the layout-to-image generation where an image often has a complex scene of multiple objects, how to make strong control over both the global layout map and each detailed object remains a challenging task. In this paper, we propose a diffusion model named LayoutDiffusion that can obtain higher generation quality and greater controllability than the previous works. To overcome the difficult multimo",
    "arxiv_url": "https://arxiv.org/abs/2303.17189v2",
    "pdf_url": "https://arxiv.org/pdf/2303.17189v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2303.17189",
    "arxiv_authors": [
      "Guangcong Zheng",
      "Xianpan Zhou",
      "Xuewei Li",
      "Zhongang Qi",
      "Ying Shan",
      "Xi Li"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=LayoutDiffusion%3A+Controllable+Diffusion+Model+for+Layout-to-image+Generation+Guangcong+Zheng+Xianpan+Zhou+Xuewei+Li+Zhongang+Qi+Ying+Shan",
    "gs_search_success": true,
    "gs_authors": [
      "TYNPJQMAAAAJ",
      "N8mK__gAAAAJ",
      "zJvrrusAAAAJ",
      "JvX6gPEAAAAJ",
      "i1PqK7kAAAAJ",
      "4oXBp9UAAAAJ"
    ],
    "citation_count": 303,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2411.02074",
    "title": "GraphVL: Graph-Enhanced Semantic Modeling via Vision-Language Models for Generalized Class Discovery",
    "year": 2024,
    "published": "2024-11-04T13:26:15Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Generalized Category Discovery (GCD) aims to cluster unlabeled images into known and novel categories using labeled images from known classes. To address the challenge of transferring features from known to unknown classes while mitigating model bias, we introduce GraphVL, a novel approach for vision-language modeling in GCD, leveraging CLIP. Our method integrates a graph convolutional network (GCN) with CLIP's text encoder to preserve class neighborhood structure. We also employ a lightweight v",
    "arxiv_url": "https://arxiv.org/abs/2411.02074v2",
    "pdf_url": "https://arxiv.org/pdf/2411.02074v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2411.02074",
    "arxiv_authors": [
      "Bhupendra Solanki",
      "Ashwin Nair",
      "Mainak Singha",
      "Souradeep Mukhopadhyay",
      "Ankit Jha",
      "Biplab Banerjee"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=GraphVL%3A+Graph-Enhanced+Semantic+Modeling+via+Vision-Language+Models+for+Generalized+Class+Discovery+Bhupendra+Solanki+Ashwin+Nair+Mainak+Singha+Souradeep+Mukhopadhyay+Ankit+Jha",
    "gs_search_success": true,
    "gs_authors": [
      "IEcsMPAAAAAJ",
      "DvIe72QAAAAJ",
      "CNt-vi0AAAAJ",
      "QGUtFRIAAAAJ",
      "Oz_A-coAAAAJ"
    ],
    "citation_count": 2,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2504.13231",
    "title": "WildFireCan-MMD: A Multimodal Dataset for Classification of User-Generated Content During Wildfires in Canada",
    "year": 2025,
    "published": "2025-04-17T14:43:56Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Rapid information access is vital during wildfires, yet traditional data sources are slow and costly. Social media offers real-time updates, but extracting relevant insights remains a challenge. In this work, we focus on multimodal wildfire social media data, which, although existing in current datasets, is currently underrepresented in Canadian contexts. We present WildFireCan-MMD, a new multimodal dataset of X posts from recent Canadian wildfires, annotated across twelve key themes. We evaluat",
    "arxiv_url": "https://arxiv.org/abs/2504.13231v4",
    "pdf_url": "https://arxiv.org/pdf/2504.13231v4",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2504.13231",
    "arxiv_authors": [
      "Braeden Sherritt",
      "Isar Nejadgholi",
      "Efstratios Aivaliotis",
      "Khaled Mslmani",
      "Marzieh Amini"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=WildFireCan-MMD%3A+A+Multimodal+Dataset+for+Classification+of+User-Generated+Content+During+Wildfires+in+Canada+Braeden+Sherritt+Isar+Nejadgholi+Efstratios+Aivaliotis+Khaled+Mslmani+Marzieh+Amini",
    "gs_search_success": true,
    "gs_authors": [
      "In99zA8AAAAJ",
      "wTEyJYwAAAAJ"
    ],
    "citation_count": 3,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2303.01497",
    "title": "Teach a Robot to FISH: Versatile Imitation from One Minute of Demonstrations",
    "year": 2023,
    "published": "2023-03-02T18:57:38Z",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "While imitation learning provides us with an efficient toolkit to train robots, learning skills that are robust to environment variations remains a significant challenge. Current approaches address this challenge by relying either on large amounts of demonstrations that span environment variations or on handcrafted reward functions that require state estimates. Both directions are not scalable to fast imitation. In this work, we present Fast Imitation of Skills from Humans (FISH), a new imitatio",
    "arxiv_url": "https://arxiv.org/abs/2303.01497v1",
    "pdf_url": "https://arxiv.org/pdf/2303.01497v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2303.01497",
    "arxiv_authors": [
      "Siddhant Haldar",
      "Jyothish Pari",
      "Anant Rai",
      "Lerrel Pinto"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Teach+a+Robot+to+FISH%3A+Versatile+Imitation+from+One+Minute+of+Demonstrations+Siddhant+Haldar+Jyothish+Pari+Anant+Rai+Lerrel+Pinto",
    "gs_search_success": true,
    "gs_authors": [
      "i0SlY6EAAAAJ",
      "-h_bkRgAAAAJ",
      "WyIW46YAAAAJ",
      "pmVPj94AAAAJ"
    ],
    "citation_count": 102,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2412.18653",
    "title": "1.58-bit FLUX",
    "year": 2024,
    "published": "2024-12-24T19:00:02Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "abstract": "We present 1.58-bit FLUX, the first successful approach to quantizing the state-of-the-art text-to-image generation model, FLUX.1-dev, using 1.58-bit weights (i.e., values in {-1, 0, +1}) while maintaining comparable performance for generating 1024 x 1024 images. Notably, our quantization method operates without access to image data, relying solely on self-supervision from the FLUX.1-dev model. Additionally, we develop a custom kernel optimized for 1.58-bit operations, achieving a 7.7x reduction",
    "arxiv_url": "https://arxiv.org/abs/2412.18653v1",
    "pdf_url": "https://arxiv.org/pdf/2412.18653v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2412.18653",
    "arxiv_authors": [
      "Chenglin Yang",
      "Celong Liu",
      "Xueqing Deng",
      "Dongwon Kim",
      "Xing Mei",
      "Xiaohui Shen",
      "Liang-Chieh Chen"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=1.58-bit+FLUX+Chenglin+Yang+Celong+Liu+Xueqing+Deng+Dongwon+Kim+Xing+Mei",
    "gs_search_success": true,
    "gs_authors": [
      "WJSj6FUAAAAJ",
      "UGhyv2UAAAAJ",
      "DsumNkgAAAAJ",
      "abXotYsAAAAJ",
      "ACjYGPUAAAAJ",
      "pViZYwIAAAAJ",
      "Utqz3uYAAAAJ"
    ],
    "citation_count": 24,
    "gs_author_count": 7
  },
  {
    "arxiv_id": "2501.06250",
    "title": "Generative AI for Cel-Animation: A Survey",
    "year": 2025,
    "published": "2025-01-08T20:57:39Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.HC"
    ],
    "abstract": "Traditional Celluloid (Cel) Animation production pipeline encompasses multiple essential steps, including storyboarding, layout design, keyframe animation, inbetweening, and colorization, which demand substantial manual effort, technical expertise, and significant time investment. These challenges have historically impeded the efficiency and scalability of Cel-Animation production. The rise of generative artificial intelligence (GenAI), encompassing large language models, multimodal models, and ",
    "arxiv_url": "https://arxiv.org/abs/2501.06250v5",
    "pdf_url": "https://arxiv.org/pdf/2501.06250v5",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2501.06250",
    "arxiv_authors": [
      "Yolo Y. Tang",
      "Junjia Guo",
      "Pinxin Liu",
      "Zhiyuan Wang",
      "Hang Hua",
      "Jia-Xing Zhong",
      "Yunzhong Xiao",
      "Chao Huang",
      "Luchuan Song",
      "Susan Liang",
      "Yizhi Song",
      "Liu He",
      "Jing Bi",
      "Mingqian Feng",
      "Xinyang Li",
      "Zeliang Zhang",
      "Chenliang Xu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Generative+AI+for+Cel-Animation%3A+A+Survey+Yolo+Y.+Tang+Junjia+Guo+Pinxin+Liu+Zhiyuan+Wang+Hang+Hua",
    "gs_search_success": true,
    "gs_authors": [
      "dIckm98AAAAJ",
      "ZJQldrQAAAAJ",
      "x3HBE2gAAAAJ",
      "xf1rCgoAAAAJ",
      "b9uTwEgAAAAJ",
      "4TdiRMYAAAAJ",
      "UVsyxOoAAAAJ",
      "IUj5R3EAAAAJ",
      "K9aLTwUAAAAJ",
      "5yYP5RIAAAAJ"
    ],
    "citation_count": 22,
    "gs_author_count": 9
  },
  {
    "arxiv_id": "2312.10588",
    "title": "Post-Training Quantization for Re-parameterization via Coarse & Fine Weight Splitting",
    "year": 2023,
    "published": "2023-12-17T02:31:20Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Although neural networks have made remarkable advancements in various applications, they require substantial computational and memory resources. Network quantization is a powerful technique to compress neural networks, allowing for more efficient and scalable AI deployments. Recently, Re-parameterization has emerged as a promising technique to enhance model performance while simultaneously alleviating the computational burden in various computer vision tasks. However, the accuracy drops signific",
    "arxiv_url": "https://arxiv.org/abs/2312.10588v1",
    "pdf_url": "https://arxiv.org/pdf/2312.10588v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2312.10588",
    "arxiv_authors": [
      "Dawei Yang",
      "Ning He",
      "Xing Hu",
      "Zhihang Yuan",
      "Jiangyong Yu",
      "Chen Xu",
      "Zhe Jiang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Post-Training+Quantization+for+Re-parameterization+via+Coarse+%26+Fine+Weight+Splitting+Dawei+Yang+Ning+He+Xing+Hu+Zhihang+Yuan+Jiangyong+Yu",
    "gs_search_success": true,
    "gs_authors": [
      "j2ANma0AAAAJ",
      "NirQb6sAAAAJ",
      "43VmtLIAAAAJ",
      "iipYHLoAAAAJ"
    ],
    "citation_count": 15,
    "gs_author_count": 7
  },
  {
    "arxiv_id": "2311.11289",
    "title": "Pair-wise Layer Attention with Spatial Masking for Video Prediction",
    "year": 2023,
    "published": "2023-11-19T10:29:05Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Video prediction yields future frames by employing the historical frames and has exhibited its great potential in many applications, e.g., meteorological prediction, and autonomous driving. Previous works often decode the ultimate high-level semantic features to future frames without texture details, which deteriorates the prediction quality. Motivated by this, we develop a Pair-wise Layer Attention (PLA) module to enhance the layer-wise semantic dependency of the feature maps derived from the U",
    "arxiv_url": "https://arxiv.org/abs/2311.11289v1",
    "pdf_url": "https://arxiv.org/pdf/2311.11289v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2311.11289",
    "arxiv_authors": [
      "Ping Li",
      "Chenhan Zhang",
      "Zheng Yang",
      "Xianghua Xu",
      "Mingli Song"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Pair-wise+Layer+Attention+with+Spatial+Masking+for+Video+Prediction+Ping+Li+Chenhan+Zhang+Zheng+Yang+Xianghua+Xu+Mingli+Song",
    "gs_search_success": true,
    "gs_authors": [
      "7oLbhAwAAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2502.13095",
    "title": "Understanding and Rectifying Safety Perception Distortion in VLMs",
    "year": 2025,
    "published": "2025-02-18T18:06:48Z",
    "categories": [
      "cs.CV",
      "cs.CL",
      "cs.LG"
    ],
    "abstract": "Recent studies reveal that vision-language models (VLMs) become more susceptible to harmful requests and jailbreak attacks after integrating the vision modality, exhibiting greater vulnerability than their text-only LLM backbones. To uncover the root cause of this phenomenon, we conduct an in-depth analysis and identify a key issue: multimodal inputs introduce an modality-induced activation shift toward a \"safer\" direction compared to their text-only counterparts, leading VLMs to systematically ",
    "arxiv_url": "https://arxiv.org/abs/2502.13095v1",
    "pdf_url": "https://arxiv.org/pdf/2502.13095v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2502.13095",
    "arxiv_authors": [
      "Xiaohan Zou",
      "Jian Kang",
      "George Kesidis",
      "Lu Lin"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Understanding+and+Rectifying+Safety+Perception+Distortion+in+VLMs+Xiaohan+Zou+Jian+Kang+George+Kesidis+Lu+Lin",
    "gs_search_success": true,
    "gs_authors": [
      "RuW6xgMAAAAJ",
      "8N04pBgAAAAJ",
      "U_jFlOQAAAAJ"
    ],
    "citation_count": 3,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2502.20388",
    "title": "Beyond Next-Token: Next-X Prediction for Autoregressive Visual Generation",
    "year": 2025,
    "published": "2025-02-27T18:59:08Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Autoregressive (AR) modeling, known for its next-token prediction paradigm, underpins state-of-the-art language and visual generative models. Traditionally, a ``token'' is treated as the smallest prediction unit, often a discrete symbol in language or a quantized patch in vision. However, the optimal token definition for 2D image structures remains an open question. Moreover, AR models suffer from exposure bias, where teacher forcing during training leads to error accumulation at inference. In t",
    "arxiv_url": "https://arxiv.org/abs/2502.20388v2",
    "pdf_url": "https://arxiv.org/pdf/2502.20388v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2502.20388",
    "arxiv_authors": [
      "Sucheng Ren",
      "Qihang Yu",
      "Ju He",
      "Xiaohui Shen",
      "Alan Yuille",
      "Liang-Chieh Chen"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Beyond+Next-Token%3A+Next-X+Prediction+for+Autoregressive+Visual+Generation+Sucheng+Ren+Qihang+Yu+Ju+He+Xiaohui+Shen+Alan+Yuille",
    "gs_search_success": true,
    "gs_authors": [
      "7zZdZxsAAAAJ",
      "FJ-huxgAAAAJ",
      "NyTPm_zUV_kC",
      "ACjYGPUAAAAJ",
      "pViZYwIAAAAJ",
      "Hbf-SoAAAAAJ"
    ],
    "citation_count": 30,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2408.08412",
    "title": "Penny-Wise and Pound-Foolish in Deepfake Detection",
    "year": 2024,
    "published": "2024-08-15T20:38:31Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "The diffusion of deepfake technologies has sparked serious concerns about its potential misuse across various domains, prompting the urgent need for robust detection methods. Despite advancement, many current approaches prioritize short-term gains at expense of long-term effectiveness. This paper critiques the overly specialized approach of fine-tuning pre-trained models solely with a penny-wise objective on a single deepfake dataset, while disregarding the pound-wise balance for generalization ",
    "arxiv_url": "https://arxiv.org/abs/2408.08412v1",
    "pdf_url": "https://arxiv.org/pdf/2408.08412v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2408.08412",
    "arxiv_authors": [
      "Yabin Wang",
      "Zhiwu Huang",
      "Su Zhou",
      "Adam Prugel-Bennett",
      "Xiaopeng Hong"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Penny-Wise+and+Pound-Foolish+in+Deepfake+Detection+Yabin+Wang+Zhiwu+Huang+Su+Zhou+Adam+Prugel-Bennett+Xiaopeng+Hong",
    "gs_search_success": true,
    "gs_authors": [
      "DSMOnTIAAAAJ",
      "yh6t92AAAAAJ",
      "oQgxYjkAAAAJ",
      "x3X-qysAAAAJ"
    ],
    "citation_count": 2,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2503.18225",
    "title": "DeLoRA: Decoupling Angles and Strength in Low-rank Adaptation",
    "year": 2025,
    "published": "2025-03-23T22:00:56Z",
    "categories": [
      "cs.LG",
      "cs.CL",
      "cs.CV"
    ],
    "abstract": "Parameter-Efficient FineTuning (PEFT) methods have recently gained significant popularity thanks to the widespread availability of large-scale pretrained models. These methods allow for quick adaptation to downstream tasks with minimal computational cost. However, popular finetuning methods such as LoRA exhibit limited robustness when it comes to hyperparameter choices or extended training regimes, preventing optimal out-of-the-box performance. In contrast, bounded approaches, such as ETHER, pro",
    "arxiv_url": "https://arxiv.org/abs/2503.18225v2",
    "pdf_url": "https://arxiv.org/pdf/2503.18225v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2503.18225",
    "arxiv_authors": [
      "Massimo Bini",
      "Leander Girrbach",
      "Zeynep Akata"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=DeLoRA%3A+Decoupling+Angles+and+Strength+in+Low-rank+Adaptation+Massimo+Bini+Leander+Girrbach+Zeynep+Akata",
    "gs_search_success": true,
    "gs_authors": [
      "xk6HHiAAAAAJ",
      "qNYKJoIAAAAJ",
      "jQl9RtkAAAAJ"
    ],
    "citation_count": 3,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2404.08580",
    "title": "Lossy Image Compression with Foundation Diffusion Models",
    "year": 2024,
    "published": "2024-04-12T16:23:42Z",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "abstract": "Incorporating diffusion models in the image compression domain has the potential to produce realistic and detailed reconstructions, especially at extremely low bitrates. Previous methods focus on using diffusion models as expressive decoders robust to quantization errors in the conditioning signals, yet achieving competitive results in this manner requires costly training of the diffusion model and long inference times due to the iterative generative process. In this work we formulate the remova",
    "arxiv_url": "https://arxiv.org/abs/2404.08580v2",
    "pdf_url": "https://arxiv.org/pdf/2404.08580v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2404.08580",
    "arxiv_authors": [
      "Lucas Relic",
      "Roberto Azevedo",
      "Markus Gross",
      "Christopher Schroers"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Lossy+Image+Compression+with+Foundation+Diffusion+Models+Lucas+Relic+Roberto+Azevedo+Markus+Gross+Christopher+Schroers",
    "gs_search_success": true,
    "gs_authors": [
      "uxk0GmUAAAAJ",
      "bHcLEbgAAAAJ",
      "ARrpwnQAAAAJ",
      "j0nCulgAAAAJ"
    ],
    "citation_count": 34,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2312.12223",
    "title": "Self-Supervised Detection of Perfect and Partial Input-Dependent Symmetries",
    "year": 2023,
    "published": "2023-12-19T15:11:46Z",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "Group equivariance can overly constrain models if the symmetries in the group differ from those observed in data. While common methods address this by determining the appropriate level of symmetry at the dataset level, they are limited to supervised settings and ignore scenarios in which multiple levels of symmetry co-exist in the same dataset. In this paper, we propose a method able to detect the level of symmetry of each input without the need for labels. Our framework is general enough to acc",
    "arxiv_url": "https://arxiv.org/abs/2312.12223v4",
    "pdf_url": "https://arxiv.org/pdf/2312.12223v4",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2312.12223",
    "arxiv_authors": [
      "Alonso Urbano",
      "David W. Romero"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Self-Supervised+Detection+of+Perfect+and+Partial+Input-Dependent+Symmetries+Alonso+Urbano+David+W.+Romero",
    "gs_search_success": true,
    "gs_authors": [
      "7tdzmVoAAAAJ",
      "PdMMvPAAAAAJ"
    ],
    "citation_count": 3,
    "gs_author_count": 2
  },
  {
    "arxiv_id": "2311.08909",
    "title": "DLAS: An Exploration and Assessment of the Deep Learning Acceleration Stack",
    "year": 2023,
    "published": "2023-11-15T12:26:31Z",
    "categories": [
      "cs.LG",
      "cs.CV",
      "cs.PF"
    ],
    "abstract": "Deep Neural Networks (DNNs) are extremely computationally demanding, which presents a large barrier to their deployment on resource-constrained devices. Since such devices are where many emerging deep learning applications lie (e.g., drones, vision-based medical technology), significant bodies of work from both the machine learning and systems communities have attempted to provide optimizations to accelerate DNNs. To help unify these two perspectives, in this paper we combine machine learning an",
    "arxiv_url": "https://arxiv.org/abs/2311.08909v1",
    "pdf_url": "https://arxiv.org/pdf/2311.08909v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2311.08909",
    "arxiv_authors": [
      "Perry Gibson",
      "José Cano",
      "Elliot J. Crowley",
      "Amos Storkey",
      "Michael O'Boyle"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=DLAS%3A+An+Exploration+and+Assessment+of+the+Deep+Learning+Acceleration+Stack+Perry+Gibson+Jos%C3%A9+Cano+Elliot+J.+Crowley+Amos+Storkey+Michael+O%27Boyle",
    "gs_search_success": true,
    "gs_authors": [
      "3Rlc8EAAAAAJ",
      "RyKtqiQAAAAJ",
      "T-JW3vwAAAAJ",
      "1Wmc01AAAAAJ"
    ],
    "citation_count": 3,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2312.17030",
    "title": "Learning Multi-axis Representation in Frequency Domain for Medical Image Segmentation",
    "year": 2023,
    "published": "2023-12-28T14:12:31Z",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "abstract": "Recently, Visual Transformer (ViT) has been extensively used in medical image segmentation (MIS) due to applying self-attention mechanism in the spatial domain to modeling global knowledge. However, many studies have focused on improving models in the spatial domain while neglecting the importance of frequency domain information. Therefore, we propose Multi-axis External Weights UNet (MEW-UNet) based on the U-shape architecture by replacing self-attention in ViT with our Multi-axis External Weig",
    "arxiv_url": "https://arxiv.org/abs/2312.17030v2",
    "pdf_url": "https://arxiv.org/pdf/2312.17030v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2312.17030",
    "arxiv_authors": [
      "Jiacheng Ruan",
      "Jingsheng Gao",
      "Mingye Xie",
      "Suncheng Xiang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Learning+Multi-axis+Representation+in+Frequency+Domain+for+Medical+Image+Segmentation+Jiacheng+Ruan+Jingsheng+Gao+Mingye+Xie+Suncheng+Xiang",
    "gs_search_success": true,
    "gs_authors": [
      "O4o2aQcAAAAJ",
      "5c9ctO4AAAAJ",
      "QDlaRJkAAAAJ",
      "IyrSeG8AAAAJ"
    ],
    "citation_count": 19,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2306.06211",
    "title": "A Survey on Segment Anything Model (SAM): Vision Foundation Model Meets Prompt Engineering",
    "year": 2023,
    "published": "2023-05-12T07:21:59Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "The Segment Anything Model (SAM), developed by Meta AI Research, represents a significant breakthrough in computer vision, offering a robust framework for image and video segmentation. This survey provides a comprehensive exploration of the SAM family, including SAM and SAM 2, highlighting their advancements in granularity and contextual understanding. Our study demonstrates SAM's versatility across a wide range of applications while identifying areas where improvements are needed, particularly ",
    "arxiv_url": "https://arxiv.org/abs/2306.06211v4",
    "pdf_url": "https://arxiv.org/pdf/2306.06211v4",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2306.06211",
    "arxiv_authors": [
      "Chaoning Zhang",
      "Joseph Cho",
      "Fachrina Dewi Puspitasari",
      "Sheng Zheng",
      "Chenghao Li",
      "Yu Qiao",
      "Taegoo Kang",
      "Xinru Shan",
      "Chenshuang Zhang",
      "Caiyan Qin",
      "Francois Rameau",
      "Lik-Hang Lee",
      "Sung-Ho Bae",
      "Choong Seon Hong"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=A+Survey+on+Segment+Anything+Model+%28SAM%29%3A+Vision+Foundation+Model+Meets+Prompt+Engineering+Chaoning+Zhang+Joseph+Cho+Fachrina+Dewi+Puspitasari+Sheng+Zheng+Chenghao+Li",
    "gs_search_success": true,
    "gs_authors": [
      "HbqjLHYAAAAJ",
      "TgrJOjoAAAAJ",
      "lvhxhyQAAAAJ",
      "2I_yzrUAAAAJ",
      "oPuMjXMAAAAJ",
      "RjS1qacAAAAJ",
      "q4n7oe0AAAAJ"
    ],
    "citation_count": 107,
    "gs_author_count": 9
  },
  {
    "arxiv_id": "2402.10404",
    "title": "Explaining generative diffusion models via visual analysis for interpretable decision-making process",
    "year": 2024,
    "published": "2024-02-16T02:12:20Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Diffusion models have demonstrated remarkable performance in generation tasks. Nevertheless, explaining the diffusion process remains challenging due to it being a sequence of denoising noisy images that are difficult for experts to interpret. To address this issue, we propose the three research questions to interpret the diffusion process from the perspective of the visual concepts generated by the model and the region where the model attends in each time step. We devise tools for visualizing t",
    "arxiv_url": "https://arxiv.org/abs/2402.10404v1",
    "pdf_url": "https://arxiv.org/pdf/2402.10404v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2402.10404",
    "arxiv_authors": [
      "Ji-Hoon Park",
      "Yeong-Joon Ju",
      "Seong-Whan Lee"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Explaining+generative+diffusion+models+via+visual+analysis+for+interpretable+decision-making+process+Ji-Hoon+Park+Yeong-Joon+Ju+Seong-Whan+Lee",
    "gs_search_success": true,
    "gs_authors": [
      "6Se6QUQAAAAJ",
      "tp1MLL8AAAAJ"
    ],
    "citation_count": 27,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2501.07783",
    "title": "Parameter-Inverted Image Pyramid Networks for Visual Perception and Multimodal Understanding",
    "year": 2025,
    "published": "2025-01-14T01:57:41Z",
    "categories": [
      "cs.CV",
      "cs.CL"
    ],
    "abstract": "Image pyramids are widely adopted in top-performing methods to obtain multi-scale features for precise visual perception and understanding. However, current image pyramids use the same large-scale model to process multiple resolutions of images, leading to significant computational cost. To address this challenge, we propose a novel network architecture, called Parameter-Inverted Image Pyramid Networks (PIIP). Specifically, PIIP uses pretrained models (ViTs or CNNs) as branches to process multi-",
    "arxiv_url": "https://arxiv.org/abs/2501.07783v1",
    "pdf_url": "https://arxiv.org/pdf/2501.07783v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2501.07783",
    "arxiv_authors": [
      "Zhaokai Wang",
      "Xizhou Zhu",
      "Xue Yang",
      "Gen Luo",
      "Hao Li",
      "Changyao Tian",
      "Wenhan Dou",
      "Junqi Ge",
      "Lewei Lu",
      "Yu Qiao",
      "Jifeng Dai"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Parameter-Inverted+Image+Pyramid+Networks+for+Visual+Perception+and+Multimodal+Understanding+Zhaokai+Wang+Xizhou+Zhu+Xue+Yang+Gen+Luo+Hao+Li",
    "gs_search_success": true,
    "gs_authors": [
      "2xTlvV0AAAAJ",
      "W0zVf-oAAAAJ",
      "02RXI00AAAAJ",
      "zdgKJXIAAAAJ",
      "_LHpf9oAAAAJ",
      "kQ3AisQAAAAJ",
      "SH_-B_AAAAAJ",
      "qHqQsY4AAAAJ",
      "EyZqU9gAAAAJ",
      "gFtI-8QAAAAJ"
    ],
    "citation_count": 8,
    "gs_author_count": 9
  },
  {
    "arxiv_id": "2310.13263",
    "title": "UE4-NeRF:Neural Radiance Field for Real-Time Rendering of Large-Scale Scene",
    "year": 2023,
    "published": "2023-10-20T04:01:35Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Neural Radiance Fields (NeRF) is a novel implicit 3D reconstruction method that shows immense potential and has been gaining increasing attention. It enables the reconstruction of 3D scenes solely from a set of photographs. However, its real-time rendering capability, especially for interactive real-time rendering of large-scale scenes, still has significant limitations. To address these challenges, in this paper, we propose a novel neural rendering system called UE4-NeRF, specifically designed ",
    "arxiv_url": "https://arxiv.org/abs/2310.13263v1",
    "pdf_url": "https://arxiv.org/pdf/2310.13263v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2310.13263",
    "arxiv_authors": [
      "Jiaming Gu",
      "Minchao Jiang",
      "Hongsheng Li",
      "Xiaoyuan Lu",
      "Guangming Zhu",
      "Syed Afaq Ali Shah",
      "Liang Zhang",
      "Mohammed Bennamoun"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=UE4-NeRF%3ANeural+Radiance+Field+for+Real-Time+Rendering+of+Large-Scale+Scene+Jiaming+Gu+Minchao+Jiang+Hongsheng+Li+Xiaoyuan+Lu+Guangming+Zhu",
    "gs_search_success": true,
    "gs_authors": [
      "jt8asuwAAAAJ",
      "jO8lwTYAAAAJ",
      "TcjxvFkAAAAJ",
      "hfNxbowAAAAJ",
      "ylX5MEAAAAAJ"
    ],
    "citation_count": 26,
    "gs_author_count": 8
  },
  {
    "arxiv_id": "2312.17192",
    "title": "HISR: Hybrid Implicit Surface Representation for Photorealistic 3D Human Reconstruction",
    "year": 2023,
    "published": "2023-12-28T18:24:57Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Neural reconstruction and rendering strategies have demonstrated state-of-the-art performances due, in part, to their ability to preserve high level shape details. Existing approaches, however, either represent objects as implicit surface functions or neural volumes and still struggle to recover shapes with heterogeneous materials, in particular human skin, hair or clothes. To this aim, we present a new hybrid implicit surface representation to model human shapes. This representation is composed",
    "arxiv_url": "https://arxiv.org/abs/2312.17192v1",
    "pdf_url": "https://arxiv.org/pdf/2312.17192v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2312.17192",
    "arxiv_authors": [
      "Angtian Wang",
      "Yuanlu Xu",
      "Nikolaos Sarafianos",
      "Robert Maier",
      "Edmond Boyer",
      "Alan Yuille",
      "Tony Tung"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=HISR%3A+Hybrid+Implicit+Surface+Representation+for+Photorealistic+3D+Human+Reconstruction+Angtian+Wang+Yuanlu+Xu+Nikolaos+Sarafianos+Robert+Maier+Edmond+Boyer",
    "gs_search_success": true,
    "gs_authors": [
      "fvr-J3sAAAAJ",
      "ni3EbYgAAAAJ",
      "JoLgWjkAAAAJ",
      "bxcnXn8AAAAJ",
      "FJ-huxgAAAAJ",
      "O_TOBmAAAAAJ",
      "YR7re-cAAAAJ"
    ],
    "citation_count": 4,
    "gs_author_count": 7
  },
  {
    "arxiv_id": "2404.00741",
    "title": "Rethinking Interactive Image Segmentation with Low Latency, High Quality, and Diverse Prompts",
    "year": 2024,
    "published": "2024-03-31T17:02:24Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "The goal of interactive image segmentation is to delineate specific regions within an image via visual or language prompts. Low-latency and high-quality interactive segmentation with diverse prompts remain challenging for existing specialist and generalist models. Specialist models, with their limited prompts and task-specific designs, experience high latency because the image must be recomputed every time the prompt is updated, due to the joint encoding of image and visual prompts. Generalist m",
    "arxiv_url": "https://arxiv.org/abs/2404.00741v1",
    "pdf_url": "https://arxiv.org/pdf/2404.00741v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2404.00741",
    "arxiv_authors": [
      "Qin Liu",
      "Jaemin Cho",
      "Mohit Bansal",
      "Marc Niethammer"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Rethinking+Interactive+Image+Segmentation+with+Low+Latency%2C+High+Quality%2C+and+Diverse+Prompts+Qin+Liu+Jaemin+Cho+Mohit+Bansal+Marc+Niethammer",
    "gs_search_success": true,
    "gs_authors": [
      "KqtBi6MAAAAJ",
      "IbQZoHQAAAAJ",
      "DN8QtscAAAAJ",
      "o209D9kAAAAJ"
    ],
    "citation_count": 36,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2408.02297",
    "title": "Perception Matters: Enhancing Embodied AI with Uncertainty-Aware Semantic Segmentation",
    "year": 2024,
    "published": "2024-08-05T08:14:28Z",
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "abstract": "Embodied AI has made significant progress acting in unexplored environments. However, tasks such as object search have largely focused on efficient policy learning. In this work, we identify several gaps in current search methods: They largely focus on dated perception models, neglect temporal aggregation, and transfer from ground truth directly to noisy perception at test time, without accounting for the resulting overconfidence in the perceived state. We address the identified problems through",
    "arxiv_url": "https://arxiv.org/abs/2408.02297v2",
    "pdf_url": "https://arxiv.org/pdf/2408.02297v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2408.02297",
    "arxiv_authors": [
      "Sai Prasanna",
      "Daniel Honerkamp",
      "Kshitij Sirohi",
      "Tim Welschehold",
      "Wolfram Burgard",
      "Abhinav Valada"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Perception+Matters%3A+Enhancing+Embodied+AI+with+Uncertainty-Aware+Semantic+Segmentation+Sai+Prasanna+Daniel+Honerkamp+Kshitij+Sirohi+Tim+Welschehold+Wolfram+Burgard",
    "gs_search_success": true,
    "gs_authors": [
      "zj6FavAAAAAJ",
      "0g0cuJsAAAAJ",
      "ZiB7SdEAAAAJ",
      "LcARjz0AAAAJ",
      "SBdrJEgAAAAJ",
      "Ian_c5AAAAAJ"
    ],
    "citation_count": 2,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2404.04624",
    "title": "Bridging the Gap Between End-to-End and Two-Step Text Spotting",
    "year": 2024,
    "published": "2024-04-06T13:14:04Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Modularity plays a crucial role in the development and maintenance of complex systems. While end-to-end text spotting efficiently mitigates the issues of error accumulation and sub-optimal performance seen in traditional two-step methodologies, the two-step methods continue to be favored in many competitions and practical settings due to their superior modularity. In this paper, we introduce Bridging Text Spotting, a novel approach that resolves the error accumulation and suboptimal performance ",
    "arxiv_url": "https://arxiv.org/abs/2404.04624v1",
    "pdf_url": "https://arxiv.org/pdf/2404.04624v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2404.04624",
    "arxiv_authors": [
      "Mingxin Huang",
      "Hongliang Li",
      "Yuliang Liu",
      "Xiang Bai",
      "Lianwen Jin"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Bridging+the+Gap+Between+End-to-End+and+Two-Step+Text+Spotting+Mingxin+Huang+Hongliang+Li+Yuliang+Liu+Xiang+Bai+Lianwen+Jin",
    "gs_search_success": true,
    "gs_authors": [
      "UeltiQ4AAAAJ",
      "WMUStEUAAAAJ",
      "0pZKNz8AAAAJ"
    ],
    "citation_count": 11,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2405.12736",
    "title": "Predicting the Influence of Adverse Weather on Pedestrian Detection with Automotive Radar and Lidar Sensors",
    "year": 2024,
    "published": "2024-05-21T12:44:43Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Pedestrians are among the most endangered traffic participants in road traffic. While pedestrian detection in nominal conditions is well established, the sensor and, therefore, the pedestrian detection performance degrades under adverse weather conditions. Understanding the influences of rain and fog on a specific radar and lidar sensor requires extensive testing, and if the sensors' specifications are altered, a retesting effort is required. These challenges are addressed in this paper, firstly",
    "arxiv_url": "https://arxiv.org/abs/2405.12736v1",
    "pdf_url": "https://arxiv.org/pdf/2405.12736v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2405.12736",
    "arxiv_authors": [
      "Daniel Weihmayr",
      "Fatih Sezgin",
      "Leon Tolksdorf",
      "Christian Birkner",
      "Reza N. Jazar"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Predicting+the+Influence+of+Adverse+Weather+on+Pedestrian+Detection+with+Automotive+Radar+and+Lidar+Sensors+Daniel+Weihmayr+Fatih+Sezgin+Leon+Tolksdorf+Christian+Birkner+Reza+N.+Jazar",
    "gs_search_success": true,
    "gs_authors": [
      "lBvQOV4AAAAJ",
      "qMHV-_AAAAAJ",
      "fj9Ki7MAAAAJ",
      "lb5abKwAAAAJ"
    ],
    "citation_count": 4,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2311.11714",
    "title": "On the Importance of Large Objects in CNN Based Object Detection Algorithms",
    "year": 2023,
    "published": "2023-11-20T12:32:32Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Object detection models, a prominent class of machine learning algorithms, aim to identify and precisely locate objects in images or videos. However, this task might yield uneven performances sometimes caused by the objects sizes and the quality of the images and labels used for training. In this paper, we highlight the importance of large objects in learning features that are critical for all sizes. Given these findings, we propose to introduce a weighting term into the training loss. This term",
    "arxiv_url": "https://arxiv.org/abs/2311.11714v1",
    "pdf_url": "https://arxiv.org/pdf/2311.11714v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2311.11714",
    "arxiv_authors": [
      "Ahmed Ben Saad",
      "Gabriele Facciolo",
      "Axel Davy"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=On+the+Importance+of+Large+Objects+in+CNN+Based+Object+Detection+Algorithms+Ahmed+Ben+Saad+Gabriele+Facciolo+Axel+Davy",
    "gs_search_success": true,
    "gs_authors": [
      "z7jm3uEAAAAJ",
      "UWt0MioAAAAJ",
      "gAGVgXsAAAAJ"
    ],
    "citation_count": 8,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2503.01144",
    "title": "One-shot In-context Part Segmentation",
    "year": 2025,
    "published": "2025-03-03T03:50:54Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "In this paper, we present the One-shot In-context Part Segmentation (OIParts) framework, designed to tackle the challenges of part segmentation by leveraging visual foundation models (VFMs). Existing training-based one-shot part segmentation methods that utilize VFMs encounter difficulties when faced with scenarios where the one-shot image and test image exhibit significant variance in appearance and perspective, or when the object in the test image is partially visible. We argue that training o",
    "arxiv_url": "https://arxiv.org/abs/2503.01144v1",
    "pdf_url": "https://arxiv.org/pdf/2503.01144v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2503.01144",
    "arxiv_authors": [
      "Zhenqi Dai",
      "Ting Liu",
      "Xingxing Zhang",
      "Yunchao Wei",
      "Yanning Zhang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=One-shot+In-context+Part+Segmentation+Zhenqi+Dai+Ting+Liu+Xingxing+Zhang+Yunchao+Wei+Yanning+Zhang",
    "gs_search_success": false,
    "gs_authors": [],
    "citation_count": null,
    "error_type": "fetch_failed",
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2404.16300",
    "title": "Reinforcement Learning with Generative Models for Compact Support Sets",
    "year": 2024,
    "published": "2024-04-25T02:48:16Z",
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "abstract": "Foundation models contain a wealth of information from their vast number of training samples. However, most prior arts fail to extract this information in a precise and efficient way for small sample sizes. In this work, we propose a framework utilizing reinforcement learning as a control for foundation models, allowing for the granular generation of small, focused synthetic support sets to augment the performance of neural network models on real data classification tasks. We first allow a reinf",
    "arxiv_url": "https://arxiv.org/abs/2404.16300v1",
    "pdf_url": "https://arxiv.org/pdf/2404.16300v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2404.16300",
    "arxiv_authors": [
      "Nico Schiavone",
      "Xingyu Li"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Reinforcement+Learning+with+Generative+Models+for+Compact+Support+Sets+Nico+Schiavone+Xingyu+Li",
    "gs_search_success": true,
    "gs_authors": [
      "PU7X0QIAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 2
  },
  {
    "arxiv_id": "2505.20665",
    "title": "DriveRX: A Vision-Language Reasoning Model for Cross-Task Autonomous Driving",
    "year": 2025,
    "published": "2025-05-27T03:21:04Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Autonomous driving requires real-time, robust reasoning across perception, prediction, planning, and behavior. However, conventional end-to-end models fail to generalize in complex scenarios due to the lack of structured reasoning. Recent vision-language models (VLMs) have been applied to driving tasks, but they typically rely on isolated modules and static supervision, limiting their ability to support multi-stage decision-making. We present AutoDriveRL, a unified training framework that formul",
    "arxiv_url": "https://arxiv.org/abs/2505.20665v1",
    "pdf_url": "https://arxiv.org/pdf/2505.20665v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2505.20665",
    "arxiv_authors": [
      "Muxi Diao",
      "Lele Yang",
      "Hongbo Yin",
      "Zhexu Wang",
      "Yejie Wang",
      "Daxin Tian",
      "Kongming Liang",
      "Zhanyu Ma"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=DriveRX%3A+A+Vision-Language+Reasoning+Model+for+Cross-Task+Autonomous+Driving+Muxi+Diao+Lele+Yang+Hongbo+Yin+Zhexu+Wang+Yejie+Wang",
    "gs_search_success": true,
    "gs_authors": [
      "dmlkJR4AAAAJ",
      "5GAAs7IAAAAJ",
      "8AcrmfsAAAAJ",
      "uHTwATMAAAAJ"
    ],
    "citation_count": 2,
    "gs_author_count": 8
  },
  {
    "arxiv_id": "2503.08581",
    "title": "MsaMIL-Net: An End-to-End Multi-Scale Aware Multiple Instance Learning Network for Efficient Whole Slide Image Classification",
    "year": 2025,
    "published": "2025-03-11T16:16:44Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Bag-based Multiple Instance Learning (MIL) approaches have emerged as the mainstream methodology for Whole Slide Image (WSI) classification. However, most existing methods adopt a segmented training strategy, which first extracts features using a pre-trained feature extractor and then aggregates these features through MIL. This segmented training approach leads to insufficient collaborative optimization between the feature extraction network and the MIL network, preventing end-to-end joint optim",
    "arxiv_url": "https://arxiv.org/abs/2503.08581v2",
    "pdf_url": "https://arxiv.org/pdf/2503.08581v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2503.08581",
    "arxiv_authors": [
      "Jiangping Wen",
      "Jinyu Wen",
      "Meie Fang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=MsaMIL-Net%3A+An+End-to-End+Multi-Scale+Aware+Multiple+Instance+Learning+Network+for+Efficient+Whole+Slide+Image+Classification+Jiangping+Wen+Jinyu+Wen+Meie+Fang",
    "gs_search_success": true,
    "gs_authors": [],
    "citation_count": 3,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2310.20389",
    "title": "High-Resolution Reference Image Assisted Volumetric Super-Resolution of Cardiac Diffusion Weighted Imaging",
    "year": 2023,
    "published": "2023-10-31T12:05:27Z",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "abstract": "Diffusion Tensor Cardiac Magnetic Resonance (DT-CMR) is the only in vivo method to non-invasively examine the microstructure of the human heart. Current research in DT-CMR aims to improve the understanding of how the cardiac microstructure relates to the macroscopic function of the healthy heart as well as how microstructural dysfunction contributes to disease. To get the final DT-CMR metrics, we need to acquire diffusion weighted images of at least 6 directions. However, due to DWI's low signal",
    "arxiv_url": "https://arxiv.org/abs/2310.20389v1",
    "pdf_url": "https://arxiv.org/pdf/2310.20389v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2310.20389",
    "arxiv_authors": [
      "Yinzhe Wu",
      "Jiahao Huang",
      "Fanwen Wang",
      "Pedro Ferreira",
      "Andrew Scott",
      "Sonia Nielles-Vallespin",
      "Guang Yang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=High-Resolution+Reference+Image+Assisted+Volumetric+Super-Resolution+of+Cardiac+Diffusion+Weighted+Imaging+Yinzhe+Wu+Jiahao+Huang+Fanwen+Wang+Pedro+Ferreira+Andrew+Scott",
    "gs_search_success": true,
    "gs_authors": [
      "-lT1KRoAAAAJ",
      "otR3e5QAAAAJ",
      "fk9ywHUAAAAJ",
      "ZfzEFpsAAAAJ",
      "t2z0h-sAAAAJ",
      "1EsA7vcAAAAJ",
      "ap-tq8cAAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 7
  },
  {
    "arxiv_id": "2303.14933",
    "title": "MD-VQA: Multi-Dimensional Quality Assessment for UGC Live Videos",
    "year": 2023,
    "published": "2023-03-27T06:17:10Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "User-generated content (UGC) live videos are often bothered by various distortions during capture procedures and thus exhibit diverse visual qualities. Such source videos are further compressed and transcoded by media server providers before being distributed to end-users. Because of the flourishing of UGC live videos, effective video quality assessment (VQA) tools are needed to monitor and perceptually optimize live streaming videos in the distributing process. In this paper, we address \\textbf",
    "arxiv_url": "https://arxiv.org/abs/2303.14933v2",
    "pdf_url": "https://arxiv.org/pdf/2303.14933v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2303.14933",
    "arxiv_authors": [
      "Zicheng Zhang",
      "Wei Wu",
      "Wei Sun",
      "Dangyang Tu",
      "Wei Lu",
      "Xiongkuo Min",
      "Ying Chen",
      "Guangtao Zhai"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=MD-VQA%3A+Multi-Dimensional+Quality+Assessment+for+UGC+Live+Videos+Zicheng+Zhang+Wei+Wu+Wei+Sun+Dangyang+Tu+Wei+Lu",
    "gs_search_success": true,
    "gs_authors": [
      "91sjuWIAAAAJ",
      "NpTmcKEAAAAJ",
      "E6zbSYgAAAAJ",
      "QICTEckAAAAJ",
      "segsUCEAAAAJ",
      "nDlEBJ8AAAAJ",
      "dfljfe0AAAAJ"
    ],
    "citation_count": 85,
    "gs_author_count": 8
  },
  {
    "arxiv_id": "2412.07114",
    "title": "TT-MPD: Test Time Model Pruning and Distillation",
    "year": 2024,
    "published": "2024-12-10T02:05:13Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Pruning can be an effective method of compressing large pre-trained models for inference speed acceleration. Previous pruning approaches rely on access to the original training dataset for both pruning and subsequent fine-tuning. However, access to the training data can be limited due to concerns such as data privacy and commercial confidentiality. Furthermore, with covariate shift (disparities between test and training data distributions), pruning and finetuning with training datasets can hinde",
    "arxiv_url": "https://arxiv.org/abs/2412.07114v1",
    "pdf_url": "https://arxiv.org/pdf/2412.07114v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2412.07114",
    "arxiv_authors": [
      "Haihang Wu",
      "Wei Wang",
      "Tamasha Malepathirana",
      "Sachith Seneviratne",
      "Denny Oetomo",
      "Saman Halgamuge"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=TT-MPD%3A+Test+Time+Model+Pruning+and+Distillation+Haihang+Wu+Wei+Wang+Tamasha+Malepathirana+Sachith+Seneviratne+Denny+Oetomo",
    "gs_search_success": true,
    "gs_authors": [
      "nvv8iZEAAAAJ",
      "9PaZyiIAAAAJ",
      "9cafqywAAAAJ",
      "oZNxrKcAAAAJ",
      "3YDh014AAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2410.21758",
    "title": "DOFS: A Real-world 3D Deformable Object Dataset with Full Spatial Information for Dynamics Model Learning",
    "year": 2024,
    "published": "2024-10-29T05:46:16Z",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "abstract": "This work proposes DOFS, a pilot dataset of 3D deformable objects (DOs) (e.g., elasto-plastic objects) with full spatial information (i.e., top, side, and bottom information) using a novel and low-cost data collection platform with a transparent operating plane. The dataset consists of active manipulation action, multi-view RGB-D images, well-registered point clouds, 3D deformed mesh, and 3D occupancy with semantics, using a pinching strategy with a two-parallel-finger gripper. In addition, we t",
    "arxiv_url": "https://arxiv.org/abs/2410.21758v1",
    "pdf_url": "https://arxiv.org/pdf/2410.21758v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2410.21758",
    "arxiv_authors": [
      "Zhen Zhang",
      "Xiangyu Chu",
      "Yunxi Tang",
      "K. W. Samuel Au"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=DOFS%3A+A+Real-world+3D+Deformable+Object+Dataset+with+Full+Spatial+Information+for+Dynamics+Model+Learning+Zhen+Zhang+Xiangyu+Chu+Yunxi+Tang+K.+W.+Samuel+Au",
    "gs_search_success": true,
    "gs_authors": [
      "sSDwMnsAAAAJ",
      "1v8bLDUAAAAJ",
      "HlvVxEYAAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2310.13859",
    "title": "Not all Fake News is Written: A Dataset and Analysis of Misleading Video Headlines",
    "year": 2023,
    "published": "2023-10-20T23:47:01Z",
    "categories": [
      "cs.CL",
      "cs.CV"
    ],
    "abstract": "Polarization and the marketplace for impressions have conspired to make navigating information online difficult for users, and while there has been a significant effort to detect false or misleading text, multimodal datasets have received considerably less attention. To complement existing resources, we present multimodal Video Misleading Headline (VMH), a dataset that consists of videos and whether annotators believe the headline is representative of the video's contents. After collecting and a",
    "arxiv_url": "https://arxiv.org/abs/2310.13859v2",
    "pdf_url": "https://arxiv.org/pdf/2310.13859v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2310.13859",
    "arxiv_authors": [
      "Yoo Yeon Sung",
      "Jordan Boyd-Graber",
      "Naeemul Hassan"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Not+all+Fake+News+is+Written%3A+A+Dataset+and+Analysis+of+Misleading+Video+Headlines+Yoo+Yeon+Sung+Jordan+Boyd-Graber+Naeemul+Hassan",
    "gs_search_success": true,
    "gs_authors": [
      "BT4XTP4AAAAJ",
      "utHgwGEAAAAJ",
      "-DeoD4EAAAAJ"
    ],
    "citation_count": 6,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2310.01142",
    "title": "[Re] CLRNet: Cross Layer Refinement Network for Lane Detection",
    "year": 2023,
    "published": "2023-10-02T12:31:10Z",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "abstract": "The following work is a reproducibility report for CLRNet: Cross Layer Refinement Network for Lane Detection. The basic code was made available by the author. The paper proposes a novel Cross Layer Refinement Network to utilize both high and low level features for lane detection. The authors assert that the proposed technique sets the new state-of-the-art on three lane-detection benchmarks",
    "arxiv_url": "https://arxiv.org/abs/2310.01142v1",
    "pdf_url": "https://arxiv.org/pdf/2310.01142v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2310.01142",
    "arxiv_authors": [
      "Viswesh N",
      "Kaushal Jadhav",
      "Avi Amalanshu",
      "Bratin Mondal",
      "Sabaris Waran",
      "Om Sadhwani",
      "Apoorv Kumar",
      "Debashish Chakravarty"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=%5BRe%5D+CLRNet%3A+Cross+Layer+Refinement+Network+for+Lane+Detection+Viswesh+N+Kaushal+Jadhav+Avi+Amalanshu+Bratin+Mondal+Sabaris+Waran",
    "gs_search_success": true,
    "gs_authors": [
      "9FYGbOgAAAAJ",
      "uxaD7gUAAAAJ",
      "SHZ3TJgAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 8
  },
  {
    "arxiv_id": "2409.09915",
    "title": "Forearm Ultrasound based Gesture Recognition on Edge",
    "year": 2024,
    "published": "2024-09-16T01:07:16Z",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "abstract": "Ultrasound imaging of the forearm has demonstrated significant potential for accurate hand gesture classification. Despite this progress, there has been limited focus on developing a stand-alone end- to-end gesture recognition system which makes it mobile, real-time and more user friendly. To bridge this gap, this paper explores the deployment of deep neural networks for forearm ultrasound-based hand gesture recognition on edge devices. Utilizing quantization techniques, we achieve substantial r",
    "arxiv_url": "https://arxiv.org/abs/2409.09915v1",
    "pdf_url": "https://arxiv.org/pdf/2409.09915v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2409.09915",
    "arxiv_authors": [
      "Keshav Bimbraw",
      "Haichong K. Zhang",
      "Bashima Islam"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Forearm+Ultrasound+based+Gesture+Recognition+on+Edge+Keshav+Bimbraw+Haichong+K.+Zhang+Bashima+Islam",
    "gs_search_success": true,
    "gs_authors": [
      "8HTrAP4AAAAJ",
      "jCeibEwAAAAJ",
      "Ft_8Ju8AAAAJ"
    ],
    "citation_count": 4,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2406.19899",
    "title": "On the Value of PHH3 for Mitotic Figure Detection on H&E-stained Images",
    "year": 2024,
    "published": "2024-06-28T13:07:43Z",
    "categories": [
      "cs.CV",
      "q-bio.QM"
    ],
    "abstract": "The count of mitotic figures (MFs) observed in hematoxylin and eosin (H&E)-stained slides is an important prognostic marker as it is a measure for tumor cell proliferation. However, the identification of MFs has a known low inter-rater agreement. Deep learning algorithms can standardize this task, but they require large amounts of annotated data for training and validation. Furthermore, label noise introduced during the annotation process may impede the algorithm's performance. Unlike H&E, the m",
    "arxiv_url": "https://arxiv.org/abs/2406.19899v1",
    "pdf_url": "https://arxiv.org/pdf/2406.19899v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2406.19899",
    "arxiv_authors": [
      "Jonathan Ganz",
      "Christian Marzahl",
      "Jonas Ammeling",
      "Barbara Richter",
      "Chloé Puget",
      "Daniela Denk",
      "Elena A. Demeter",
      "Flaviu A. Tabaran",
      "Gabriel Wasinger",
      "Karoline Lipnik",
      "Marco Tecilla",
      "Matthew J. Valentine",
      "Michael J. Dark",
      "Niklas Abele",
      "Pompei Bolfa",
      "Ramona Erber",
      "Robert Klopfleisch",
      "Sophie Merz",
      "Taryn A. Donovan",
      "Samir Jabari",
      "Christof A. Bertram",
      "Katharina Breininger",
      "Marc Aubreville"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=On+the+Value+of+PHH3+for+Mitotic+Figure+Detection+on+H%26E-stained+Images+Jonathan+Ganz+Christian+Marzahl+Jonas+Ammeling+Barbara+Richter+Chlo%C3%A9+Puget",
    "gs_search_success": true,
    "gs_authors": [
      "CS0pb9gAAAAJ",
      "pjR8SvsAAAAJ",
      "yiUIgesAAAAJ",
      "BehA6akAAAAJ",
      "gdVTIP4AAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 9
  },
  {
    "arxiv_id": "2308.06444",
    "title": "TongueSAM: An Universal Tongue Segmentation Model Based on SAM with Zero-Shot",
    "year": 2023,
    "published": "2023-08-12T02:38:43Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Tongue segmentation serves as the primary step in automated TCM tongue diagnosis, which plays a significant role in the diagnostic results. Currently, numerous deep learning based methods have achieved promising results. However, when confronted with tongue images that differ from the training set or possess challenging backgrounds, these methods demonstrate limited performance. To address this issue, this paper proposes a universal tongue segmentation model named TongueSAM based on SAM (Segment",
    "arxiv_url": "https://arxiv.org/abs/2308.06444v3",
    "pdf_url": "https://arxiv.org/pdf/2308.06444v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2308.06444",
    "arxiv_authors": [
      "Shan Cao",
      "Qunsheng Ruan",
      "Linjian Ma"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=TongueSAM%3A+An+Universal+Tongue+Segmentation+Model+Based+on+SAM+with+Zero-Shot+Shan+Cao+Qunsheng+Ruan+Linjian+Ma",
    "gs_search_success": true,
    "gs_authors": [],
    "citation_count": 20,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2303.03041",
    "title": "Automatic detection of aerial survey ground control points based on Yolov5-OBB",
    "year": 2023,
    "published": "2023-03-06T11:13:23Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "The use of ground control points (GCPs) for georeferencing is the most common strategy in unmanned aerial vehicle (UAV) photogrammetry, but at the same time their collection represents the most time-consuming and expensive part of UAV campaigns. Recently, deep learning has been rapidly developed in the field of small object detection. In this letter, to automatically extract coordinates information of ground control points (GCPs) by detecting GCP-markers in UAV images, we propose a solution that",
    "arxiv_url": "https://arxiv.org/abs/2303.03041v1",
    "pdf_url": "https://arxiv.org/pdf/2303.03041v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2303.03041",
    "arxiv_authors": [
      "Cheng Chuanxiang",
      "Yang Jia",
      "Wang Chao",
      "Zheng Zhi",
      "Li Xiaopeng",
      "Dong Di",
      "Chang Mengxia",
      "Zhuang Zhiheng"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Automatic+detection+of+aerial+survey+ground+control+points+based+on+Yolov5-OBB+Cheng+Chuanxiang+Yang+Jia+Wang+Chao+Zheng+Zhi+Li+Xiaopeng",
    "gs_search_success": true,
    "gs_authors": [
      "Q0IKlZwAAAAJ"
    ],
    "citation_count": 8,
    "gs_author_count": 8
  },
  {
    "arxiv_id": "2501.09203",
    "title": "Unified Few-shot Crack Segmentation and its Precise 3D Automatic Measurement in Concrete Structures",
    "year": 2025,
    "published": "2025-01-15T23:36:05Z",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "abstract": "Visual-Spatial Systems has become increasingly essential in concrete crack inspection. However, existing methods often lacks adaptability to diverse scenarios, exhibits limited robustness in image-based approaches, and struggles with curved or complex geometries. To address these limitations, an innovative framework for two-dimensional (2D) crack detection, three-dimensional (3D) reconstruction, and 3D automatic crack measurement was proposed by integrating computer vision technologies and multi",
    "arxiv_url": "https://arxiv.org/abs/2501.09203v1",
    "pdf_url": "https://arxiv.org/pdf/2501.09203v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2501.09203",
    "arxiv_authors": [
      "Pengru Deng",
      "Jiapeng Yao",
      "Chun Li",
      "Su Wang",
      "Xinrun Li",
      "Varun Ojha",
      "Xuhui He",
      "Takashi Matsumoto"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Unified+Few-shot+Crack+Segmentation+and+its+Precise+3D+Automatic+Measurement+in+Concrete+Structures+Pengru+Deng+Jiapeng+Yao+Chun+Li+Su+Wang+Xinrun+Li",
    "gs_search_success": true,
    "gs_authors": [
      "24k-M0EAAAAJ",
      "t7eDb1MAAAAJ",
      "bNLfWwgl4J4C",
      "I51t6lEAAAAJ"
    ],
    "citation_count": 4,
    "gs_author_count": 8
  },
  {
    "arxiv_id": "2308.04224",
    "title": "Will your Doorbell Camera still recognize you as you grow old",
    "year": 2023,
    "published": "2023-08-08T12:43:26Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Robust authentication for low-power consumer devices such as doorbell cameras poses a valuable and unique challenge. This work explores the effect of age and aging on the performance of facial authentication methods. Two public age datasets, AgeDB and Morph-II have been used as baselines in this work. A photo-realistic age transformation method has been employed to augment a set of high-quality facial images with various age effects. Then the effect of these synthetic aging data on the high-perf",
    "arxiv_url": "https://arxiv.org/abs/2308.04224v1",
    "pdf_url": "https://arxiv.org/pdf/2308.04224v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2308.04224",
    "arxiv_authors": [
      "Wang Yao",
      "Muhammad Ali Farooq",
      "Joseph Lemley",
      "Peter Corcoran"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Will+your+Doorbell+Camera+still+recognize+you+as+you+grow+old+Wang+Yao+Muhammad+Ali+Farooq+Joseph+Lemley+Peter+Corcoran",
    "gs_search_success": true,
    "gs_authors": [
      "cJjiDu8AAAAJ",
      "J6YWBB4AAAAJ",
      "Km-BcFoAAAAJ",
      "ciZE8sIAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2308.08220",
    "title": "Low-Light Image Enhancement with Illumination-Aware Gamma Correction and Complete Image Modelling Network",
    "year": 2023,
    "published": "2023-08-16T08:46:51Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "This paper presents a novel network structure with illumination-aware gamma correction and complete image modelling to solve the low-light image enhancement problem. Low-light environments usually lead to less informative large-scale dark areas, directly learning deep representations from low-light images is insensitive to recovering normal illumination. We propose to integrate the effectiveness of gamma correction with the strong modelling capacities of deep networks, which enables the correcti",
    "arxiv_url": "https://arxiv.org/abs/2308.08220v1",
    "pdf_url": "https://arxiv.org/pdf/2308.08220v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2308.08220",
    "arxiv_authors": [
      "Yinglong Wang",
      "Zhen Liu",
      "Jianzhuang Liu",
      "Songcen Xu",
      "Shuaicheng Liu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Low-Light+Image+Enhancement+with+Illumination-Aware+Gamma+Correction+and+Complete+Image+Modelling+Network+Yinglong+Wang+Zhen+Liu+Jianzhuang+Liu+Songcen+Xu+Shuaicheng+Liu",
    "gs_search_success": true,
    "gs_authors": [
      "sKauaAwAAAAJ",
      "QRP6qRQAAAAJ",
      "GVbeKBcAAAAJ",
      "1DP9DAUAAAAJ",
      "_xVW9SgAAAAJ"
    ],
    "citation_count": 66,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2310.16435",
    "title": "On Pixel-level Performance Assessment in Anomaly Detection",
    "year": 2023,
    "published": "2023-10-25T08:02:27Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Anomaly detection methods have demonstrated remarkable success across various applications. However, assessing their performance, particularly at the pixel-level, presents a complex challenge due to the severe imbalance that is most commonly present between normal and abnormal samples. Commonly adopted evaluation metrics designed for pixel-level detection may not effectively capture the nuanced performance variations arising from this class imbalance. In this paper, we dissect the intricacies of",
    "arxiv_url": "https://arxiv.org/abs/2310.16435v1",
    "pdf_url": "https://arxiv.org/pdf/2310.16435v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2310.16435",
    "arxiv_authors": [
      "Mehdi Rafiei",
      "Toby P. Breckon",
      "Alexandros Iosifidis"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=On+Pixel-level+Performance+Assessment+in+Anomaly+Detection+Mehdi+Rafiei+Toby+P.+Breckon+Alexandros+Iosifidis",
    "gs_search_success": true,
    "gs_authors": [
      "KjsL0KEAAAAJ",
      "9hpHb44AAAAJ",
      "5Xw7ap4AAAAJ"
    ],
    "citation_count": 5,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2312.11972",
    "title": "Expressive Forecasting of 3D Whole-body Human Motions",
    "year": 2023,
    "published": "2023-12-19T09:09:46Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Human motion forecasting, with the goal of estimating future human behavior over a period of time, is a fundamental task in many real-world applications. However, existing works typically concentrate on predicting the major joints of the human body without considering the delicate movements of the human hands. In practical applications, hand gesture plays an important role in human communication with the real world, and expresses the primary intention of human beings. In this work, we are the fi",
    "arxiv_url": "https://arxiv.org/abs/2312.11972v2",
    "pdf_url": "https://arxiv.org/pdf/2312.11972v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2312.11972",
    "arxiv_authors": [
      "Pengxiang Ding",
      "Qiongjie Cui",
      "Min Zhang",
      "Mengyuan Liu",
      "Haofan Wang",
      "Donglin Wang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Expressive+Forecasting+of+3D+Whole-body+Human+Motions+Pengxiang+Ding+Qiongjie+Cui+Min+Zhang+Mengyuan+Liu+Haofan+Wang",
    "gs_search_success": true,
    "gs_authors": [
      "woX_4AcAAAAJ",
      "QyBSTzEAAAAJ",
      "VoPGwJQAAAAJ",
      "EaMsuB0AAAAJ",
      "-fo6wdwAAAAJ",
      "5dlWxdUAAAAJ"
    ],
    "citation_count": 7,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2406.19316",
    "title": "Enhanced Data Transfer Cooperating with Artificial Triplets for Scene Graph Generation",
    "year": 2024,
    "published": "2024-06-27T16:52:01Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "This work focuses on training dataset enhancement of informative relational triplets for Scene Graph Generation (SGG). Due to the lack of effective supervision, the current SGG model predictions perform poorly for informative relational triplets with inadequate training samples. Therefore, we propose two novel training dataset enhancement modules: Feature Space Triplet Augmentation (FSTA) and Soft Transfer. FSTA leverages a feature generator trained to generate representations of an object in re",
    "arxiv_url": "https://arxiv.org/abs/2406.19316v2",
    "pdf_url": "https://arxiv.org/pdf/2406.19316v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2406.19316",
    "arxiv_authors": [
      "KuanChao Chu",
      "Satoshi Yamazaki",
      "Hideki Nakayama"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Enhanced+Data+Transfer+Cooperating+with+Artificial+Triplets+for+Scene+Graph+Generation+KuanChao+Chu+Satoshi+Yamazaki+Hideki+Nakayama",
    "gs_search_success": true,
    "gs_authors": [
      "QcM1xFsAAAAJ",
      "lZAYGJoAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2411.06702",
    "title": "Track Any Peppers: Weakly Supervised Sweet Pepper Tracking Using VLMs",
    "year": 2024,
    "published": "2024-11-11T04:07:25Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "In the Detection and Multi-Object Tracking of Sweet Peppers Challenge, we present Track Any Peppers (TAP) - a weakly supervised ensemble technique for sweet peppers tracking. TAP leverages the zero-shot detection capabilities of vision-language foundation models like Grounding DINO to automatically generate pseudo-labels for sweet peppers in video sequences with minimal human intervention. These pseudo-labels, refined when necessary, are used to train a YOLOv8 segmentation network. To enhance de",
    "arxiv_url": "https://arxiv.org/abs/2411.06702v1",
    "pdf_url": "https://arxiv.org/pdf/2411.06702v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2411.06702",
    "arxiv_authors": [
      "Jia Syuen Lim",
      "Yadan Luo",
      "Zhi Chen",
      "Tianqi Wei",
      "Scott Chapman",
      "Zi Huang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Track+Any+Peppers%3A+Weakly+Supervised+Sweet+Pepper+Tracking+Using+VLMs+Jia+Syuen+Lim+Yadan+Luo+Zhi+Chen+Tianqi+Wei+Scott+Chapman",
    "gs_search_success": true,
    "gs_authors": [
      "B9zn51sAAAAJ",
      "v-giX5UAAAAJ",
      "Dc0WuhoAAAAJ",
      "9ZypKEYAAAAJ",
      "3IfL11AAAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2409.01341",
    "title": "Enhancing Test Time Adaptation with Few-shot Guidance",
    "year": 2024,
    "published": "2024-09-02T15:50:48Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Deep neural networks often encounter significant performance drops while facing with domain shifts between training (source) and test (target) data. To address this issue, Test Time Adaptation (TTA) methods have been proposed to adapt pre-trained source model to handle out-of-distribution streaming target data. Although these methods offer some relief, they lack a reliable mechanism for domain shift correction, which can often be erratic in real-world applications. In response, we develop Few-Sh",
    "arxiv_url": "https://arxiv.org/abs/2409.01341v3",
    "pdf_url": "https://arxiv.org/pdf/2409.01341v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2409.01341",
    "arxiv_authors": [
      "Siqi Luo",
      "Yi Xin",
      "Yuntao Du",
      "Zhongwei Wan",
      "Tao Tan",
      "Guangtao Zhai",
      "Xiaohong Liu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Enhancing+Test+Time+Adaptation+with+Few-shot+Guidance+Siqi+Luo+Yi+Xin+Yuntao+Du+Zhongwei+Wan+Tao+Tan",
    "gs_search_success": true,
    "gs_authors": [
      "Tq2hoMQAAAAJ",
      "EVj1cNoAAAAJ",
      "1ogwTXkAAAAJ",
      "E6zbSYgAAAAJ",
      "lLg3WRkAAAAJ",
      "FbJyUYoAAAAJ",
      "_Cwn43wAAAAJ"
    ],
    "citation_count": 8,
    "gs_author_count": 7
  },
  {
    "arxiv_id": "2306.06712",
    "title": "Neural Architecture Design and Robustness: A Dataset",
    "year": 2023,
    "published": "2023-06-11T16:02:14Z",
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "abstract": "Deep learning models have proven to be successful in a wide range of machine learning tasks. Yet, they are often highly sensitive to perturbations on the input data which can lead to incorrect decisions with high confidence, hampering their deployment for practical use-cases. Thus, finding architectures that are (more) robust against perturbations has received much attention in recent years. Just like the search for well-performing architectures in terms of clean accuracy, this usually involves ",
    "arxiv_url": "https://arxiv.org/abs/2306.06712v1",
    "pdf_url": "https://arxiv.org/pdf/2306.06712v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2306.06712",
    "arxiv_authors": [
      "Steffen Jung",
      "Jovita Lukasik",
      "Margret Keuper"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Neural+Architecture+Design+and+Robustness%3A+A+Dataset+Steffen+Jung+Jovita+Lukasik+Margret+Keuper",
    "gs_search_success": true,
    "gs_authors": [
      "TpsZenwAAAAJ",
      "x5ovaJcAAAAJ",
      "KMqMQAcAAAAJ"
    ],
    "citation_count": 27,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2307.02978",
    "title": "Multi-modal multi-class Parkinson disease classification using CNN and decision level fusion",
    "year": 2023,
    "published": "2023-07-06T13:25:31Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Parkinson disease is the second most common neurodegenerative disorder, as reported by the World Health Organization. In this paper, we propose a direct three-Class PD classification using two different modalities, namely, MRI and DTI. The three classes used for classification are PD, Scans Without Evidence of Dopamine Deficit and Healthy Control. We use white matter and gray matter from the MRI and fractional anisotropy and mean diffusivity from the DTI to achieve our goal. We train four separa",
    "arxiv_url": "https://arxiv.org/abs/2307.02978v1",
    "pdf_url": "https://arxiv.org/pdf/2307.02978v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2307.02978",
    "arxiv_authors": [
      "Sushanta Kumar Sahu",
      "Ananda S. Chowdhury"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Multi-modal+multi-class+Parkinson+disease+classification+using+CNN+and+decision+level+fusion+Sushanta+Kumar+Sahu+Ananda+S.+Chowdhury",
    "gs_search_success": false,
    "gs_authors": [],
    "citation_count": null,
    "error_type": "fetch_failed",
    "gs_author_count": 2
  },
  {
    "arxiv_id": "2402.04541",
    "title": "BRI3L: A Brightness Illusion Image Dataset for Identification and Localization of Regions of Illusory Perception",
    "year": 2024,
    "published": "2024-02-07T02:57:40Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Visual illusions play a significant role in understanding visual perception. Current methods in understanding and evaluating visual illusions are mostly deterministic filtering based approach and they evaluate on a handful of visual illusions, and the conclusions therefore, are not generic. To this end, we generate a large-scale dataset of 22,366 images (BRI3L: BRightness Illusion Image dataset for Identification and Localization of illusory perception) of the five types of brightness illusions ",
    "arxiv_url": "https://arxiv.org/abs/2402.04541v1",
    "pdf_url": "https://arxiv.org/pdf/2402.04541v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2402.04541",
    "arxiv_authors": [
      "Aniket Roy",
      "Anirban Roy",
      "Soma Mitra",
      "Kuntal Ghosh"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=BRI3L%3A+A+Brightness+Illusion+Image+Dataset+for+Identification+and+Localization+of+Regions+of+Illusory+Perception+Aniket+Roy+Anirban+Roy+Soma+Mitra+Kuntal+Ghosh",
    "gs_search_success": false,
    "gs_authors": [],
    "citation_count": null,
    "error_type": "fetch_failed",
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2505.16740",
    "title": "Robust Vision-Based Runway Detection through Conformal Prediction and Conformal mAP",
    "year": 2025,
    "published": "2025-05-22T14:52:59Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "We explore the use of conformal prediction to provide statistical uncertainty guarantees for runway detection in vision-based landing systems (VLS). Using fine-tuned YOLOv5 and YOLOv6 models on aerial imagery, we apply conformal prediction to quantify localization reliability under user-defined risk levels. We also introduce Conformal mean Average Precision (C-mAP), a novel metric aligning object detection performance with conformal guarantees. Our results show that conformal prediction can impr",
    "arxiv_url": "https://arxiv.org/abs/2505.16740v1",
    "pdf_url": "https://arxiv.org/pdf/2505.16740v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2505.16740",
    "arxiv_authors": [
      "Alya Zouzou",
      "Léo andéol",
      "Mélanie Ducoffe",
      "Ryma Boumazouza"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Robust+Vision-Based+Runway+Detection+through+Conformal+Prediction+and+Conformal+mAP+Alya+Zouzou+L%C3%A9o+and%C3%A9ol+M%C3%A9lanie+Ducoffe+Ryma+Boumazouza",
    "gs_search_success": true,
    "gs_authors": [
      "jqG6J-AAAAAJ",
      "EmmaVDMAAAAJ",
      "3HtBvs0AAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2305.04374",
    "title": "Spatiotemporally Consistent HDR Indoor Lighting Estimation",
    "year": 2023,
    "published": "2023-05-07T20:36:29Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "We propose a physically-motivated deep learning framework to solve a general version of the challenging indoor lighting estimation problem. Given a single LDR image with a depth map, our method predicts spatially consistent lighting at any given image position. Particularly, when the input is an LDR video sequence, our framework not only progressively refines the lighting prediction as it sees more regions, but also preserves temporal consistency by keeping the refinement smooth. Our framework r",
    "arxiv_url": "https://arxiv.org/abs/2305.04374v1",
    "pdf_url": "https://arxiv.org/pdf/2305.04374v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2305.04374",
    "arxiv_authors": [
      "Zhengqin Li",
      "Li Yu",
      "Mikhail Okunev",
      "Manmohan Chandraker",
      "Zhao Dong"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Spatiotemporally+Consistent+HDR+Indoor+Lighting+Estimation+Zhengqin+Li+Li+Yu+Mikhail+Okunev+Manmohan+Chandraker+Zhao+Dong",
    "gs_search_success": true,
    "gs_authors": [
      "51ZIHGIAAAAJ",
      "_owXoG4AAAAJ",
      "Nxc2RbQAAAAJ",
      "Ja-8AFsAAAAJ"
    ],
    "citation_count": 17,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2303.04016",
    "title": "Decoupling Skill Learning from Robotic Control for Generalizable Object Manipulation",
    "year": 2023,
    "published": "2023-03-07T16:31:13Z",
    "categories": [
      "cs.RO",
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "Recent works in robotic manipulation through reinforcement learning (RL) or imitation learning (IL) have shown potential for tackling a range of tasks e.g., opening a drawer or a cupboard. However, these techniques generalize poorly to unseen objects. We conjecture that this is due to the high-dimensional action space for joint control. In this paper, we take an alternative approach and separate the task of learning 'what to do' from 'how to do it' i.e., whole-body control. We pose the RL proble",
    "arxiv_url": "https://arxiv.org/abs/2303.04016v2",
    "pdf_url": "https://arxiv.org/pdf/2303.04016v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2303.04016",
    "arxiv_authors": [
      "Kai Lu",
      "Bo Yang",
      "Bing Wang",
      "Andrew Markham"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Decoupling+Skill+Learning+from+Robotic+Control+for+Generalizable+Object+Manipulation+Kai+Lu+Bo+Yang+Bing+Wang+Andrew+Markham",
    "gs_search_success": true,
    "gs_authors": [
      "Gb2L268AAAAJ",
      "W7QhPeUAAAAJ",
      "g3JTO9EAAAAJ",
      "VqUAqz8AAAAJ"
    ],
    "citation_count": 7,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2409.14577",
    "title": "AR Overlay: Training Image Pose Estimation on Curved Surface in a Synthetic Way",
    "year": 2024,
    "published": "2024-09-22T19:44:46Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "In the field of spatial computing, one of the most essential tasks is the pose estimation of 3D objects. While rigid transformations of arbitrary 3D objects are relatively hard to detect due to varying environment introducing factors like insufficient lighting or even occlusion, objects with pre-defined shapes are often easy to track, leveraging geometric constraints. Curved images, with flexible dimensions but a confined shape, are essential shapes often targeted in 3D tracking. Traditionally, ",
    "arxiv_url": "https://arxiv.org/abs/2409.14577v1",
    "pdf_url": "https://arxiv.org/pdf/2409.14577v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2409.14577",
    "arxiv_authors": [
      "Sining Huang",
      "Yukun Song",
      "Yixiao Kang",
      "Chang Yu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=AR+Overlay%3A+Training+Image+Pose+Estimation+on+Curved+Surface+in+a+Synthetic+Way+Sining+Huang+Yukun+Song+Yixiao+Kang+Chang+Yu",
    "gs_search_success": true,
    "gs_authors": [
      "E_3JWjgAAAAJ",
      "RiniTd0AAAAJ"
    ],
    "citation_count": 15,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2402.13816",
    "title": "A unified framework of non-local parametric methods for image denoising",
    "year": 2024,
    "published": "2024-02-21T13:55:48Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "We propose a unified view of non-local methods for single-image denoising, for which BM3D is the most popular representative, that operate by gathering noisy patches together according to their similarities in order to process them collaboratively. Our general estimation framework is based on the minimization of the quadratic risk, which is approximated in two steps, and adapts to photon and electronic noises. Relying on unbiased risk estimation (URE) for the first step and on ``internal adaptat",
    "arxiv_url": "https://arxiv.org/abs/2402.13816v1",
    "pdf_url": "https://arxiv.org/pdf/2402.13816v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2402.13816",
    "arxiv_authors": [
      "Sébastien Herbreteau",
      "Charles Kervrann"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=A+unified+framework+of+non-local+parametric+methods+for+image+denoising+S%C3%A9bastien+Herbreteau+Charles+Kervrann",
    "gs_search_success": true,
    "gs_authors": [
      "aOHoWTAAAAAJ",
      "49pGY58AAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 2
  },
  {
    "arxiv_id": "2307.13654",
    "title": "Personal Protective Equipment Detection in Extreme Construction Conditions",
    "year": 2023,
    "published": "2023-07-25T17:01:10Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Object detection has been widely applied for construction safety management, especially personal protective equipment (PPE) detection. Though the existing PPE detection models trained on conventional datasets have achieved excellent results, their performance dramatically declines in extreme construction conditions. A robust detection model NST-YOLOv5 is developed by combining the neural style transfer (NST) and YOLOv5 technologies. Five extreme conditions are considered and simulated via the NS",
    "arxiv_url": "https://arxiv.org/abs/2307.13654v1",
    "pdf_url": "https://arxiv.org/pdf/2307.13654v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2307.13654",
    "arxiv_authors": [
      "Yuexiong Ding",
      "Xiaowei Luo"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Personal+Protective+Equipment+Detection+in+Extreme+Construction+Conditions+Yuexiong+Ding+Xiaowei+Luo",
    "gs_search_success": true,
    "gs_authors": [
      "ah-VgnEAAAAJ",
      "vzARdCcAAAAJ"
    ],
    "citation_count": 5,
    "gs_author_count": 2
  },
  {
    "arxiv_id": "2503.00665",
    "title": "Development of an Unpaired Deep Neural Network for Synthesizing X-ray Fluoroscopic Images from Digitally Reconstructed Tomography in Image Guided Radiotherapy",
    "year": 2025,
    "published": "2025-03-01T23:34:43Z",
    "categories": [
      "cs.CV",
      "physics.med-ph"
    ],
    "abstract": "Purpose The purpose of this study was to develop and evaluate a deep neural network (DNN) capable of generating flat-panel detector (FPD) images from digitally reconstructed radiography (DRR) images in lung cancer treatment, with the aim of improving clinical workflows in image-guided radiotherapy.   Methods A modified CycleGAN architecture was trained on paired DRR-FPD image data obtained from patients with lung tumors. The training dataset consisted of over 400 DRR-FPD image pairs, and the fin",
    "arxiv_url": "https://arxiv.org/abs/2503.00665v1",
    "pdf_url": "https://arxiv.org/pdf/2503.00665v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2503.00665",
    "arxiv_authors": [
      "Chisako Hayashi",
      "Shinichiro Mori",
      "Yasukuni Mori",
      "Lim Taehyeung",
      "Hiroki Suyari",
      "Hitoshi Ishikawa"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Development+of+an+Unpaired+Deep+Neural+Network+for+Synthesizing+X-ray+Fluoroscopic+Images+from+Digitally+Reconstructed+Tomography+in+Image+Guided+Radiotherapy+Chisako+Hayashi+Shinichiro+Mori+Yasukuni+Mori+Lim+Taehyeung+Hiroki+Suyari",
    "gs_search_success": false,
    "gs_authors": [],
    "citation_count": null,
    "error_type": "fetch_failed",
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2501.07297",
    "title": "Toward Realistic Camouflaged Object Detection: Benchmarks and Method",
    "year": 2025,
    "published": "2025-01-13T13:04:00Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Camouflaged object detection (COD) primarily relies on semantic or instance segmentation methods. While these methods have made significant advancements in identifying the contours of camouflaged objects, they may be inefficient or cost-effective for tasks that only require the specific location of the object. Object detection algorithms offer an optimized solution for Realistic Camouflaged Object Detection (RCOD) in such cases. However, detecting camouflaged objects remains a formidable challen",
    "arxiv_url": "https://arxiv.org/abs/2501.07297v1",
    "pdf_url": "https://arxiv.org/pdf/2501.07297v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2501.07297",
    "arxiv_authors": [
      "Zhimeng Xin",
      "Tianxu Wu",
      "Shiming Chen",
      "Shuo Ye",
      "Zijing Xie",
      "Yixiong Zou",
      "Xinge You",
      "Yufei Guo"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Toward+Realistic+Camouflaged+Object+Detection%3A+Benchmarks+and+Method+Zhimeng+Xin+Tianxu+Wu+Shiming+Chen+Shuo+Ye+Zijing+Xie",
    "gs_search_success": true,
    "gs_authors": [
      "E86_3t0AAAAJ",
      "v7bRZX8AAAAJ",
      "VXxF0mcAAAAJ",
      "ONJV2N4AAAAJ",
      "EWN-IogAAAAJ",
      "T8ThxCwAAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 8
  },
  {
    "arxiv_id": "2307.09416",
    "title": "Let's ViCE! Mimicking Human Cognitive Behavior in Image Generation Evaluation",
    "year": 2023,
    "published": "2023-07-18T16:33:30Z",
    "categories": [
      "cs.CV",
      "cs.CL"
    ],
    "abstract": "Research in Image Generation has recently made significant progress, particularly boosted by the introduction of Vision-Language models which are able to produce high-quality visual content based on textual inputs. Despite ongoing advancements in terms of generation quality and realism, no methodical frameworks have been defined yet to quantitatively measure the quality of the generated content and the adherence with the prompted requests: so far, only human-based evaluations have been adopted f",
    "arxiv_url": "https://arxiv.org/abs/2307.09416v2",
    "pdf_url": "https://arxiv.org/pdf/2307.09416v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2307.09416",
    "arxiv_authors": [
      "Federico Betti",
      "Jacopo Staiano",
      "Lorenzo Baraldi",
      "Lorenzo Baraldi",
      "Rita Cucchiara",
      "Nicu Sebe"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Let%27s+ViCE%21+Mimicking+Human+Cognitive+Behavior+in+Image+Generation+Evaluation+Federico+Betti+Jacopo+Staiano+Lorenzo+Baraldi+Lorenzo+Baraldi+Rita+Cucchiara",
    "gs_search_success": true,
    "gs_authors": [
      "OM3sZEoAAAAJ",
      "Ms5ctkUAAAAJ",
      "nw7mrA0AAAAJ",
      "stFCYOAAAAAJ",
      "hI5X8UYAAAAJ",
      "V4RuMvsAAAAJ"
    ],
    "citation_count": 10,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2407.17786",
    "title": "Topology-Preserving Downsampling of Binary Images",
    "year": 2024,
    "published": "2024-07-25T05:30:09Z",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "abstract": "We present a novel discrete optimization-based approach to generate downsampled versions of binary images that are guaranteed to have the same topology as the original, measured by the zeroth and first Betti numbers of the black regions, while having good similarity to the original image as measured by IoU and Dice scores. To our best knowledge, all existing binary image downsampling methods do not have such topology-preserving guarantees. We also implemented a baseline morphological operation (",
    "arxiv_url": "https://arxiv.org/abs/2407.17786v1",
    "pdf_url": "https://arxiv.org/pdf/2407.17786v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2407.17786",
    "arxiv_authors": [
      "Chia-Chia Chen",
      "Chi-Han Peng"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Topology-Preserving+Downsampling+of+Binary+Images+Chia-Chia+Chen+Chi-Han+Peng",
    "gs_search_success": true,
    "gs_authors": [
      "jp3YHjcAAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 2
  },
  {
    "arxiv_id": "2404.11981",
    "title": "Tendency-driven Mutual Exclusivity for Weakly Supervised Incremental Semantic Segmentation",
    "year": 2024,
    "published": "2024-04-18T08:23:24Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Weakly Incremental Learning for Semantic Segmentation (WILSS) leverages a pre-trained segmentation model to segment new classes using cost-effective and readily available image-level labels. A prevailing way to solve WILSS is the generation of seed areas for each new class, serving as a form of pixel-level supervision. However, a scenario usually arises where a pixel is concurrently predicted as an old class by the pre-trained segmentation model and a new class by the seed areas. Such a scenario",
    "arxiv_url": "https://arxiv.org/abs/2404.11981v2",
    "pdf_url": "https://arxiv.org/pdf/2404.11981v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2404.11981",
    "arxiv_authors": [
      "Chongjie Si",
      "Xuehui Wang",
      "Xiaokang Yang",
      "Wei Shen"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Tendency-driven+Mutual+Exclusivity+for+Weakly+Supervised+Incremental+Semantic+Segmentation+Chongjie+Si+Xuehui+Wang+Xiaokang+Yang+Wei+Shen",
    "gs_search_success": true,
    "gs_authors": [
      "JuRztWYAAAAJ",
      "yDEavdMAAAAJ",
      "wXc2EtsAAAAJ",
      "Ae2kRCEAAAAJ"
    ],
    "citation_count": 3,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2311.06772",
    "title": "ChatAnything: Facetime Chat with LLM-Enhanced Personas",
    "year": 2023,
    "published": "2023-11-12T08:29:41Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "In this technical report, we target generating anthropomorphized personas for LLM-based characters in an online manner, including visual appearance, personality and tones, with only text descriptions. To achieve this, we first leverage the in-context learning capability of LLMs for personality generation by carefully designing a set of system prompts. We then propose two novel concepts: the mixture of voices (MoV) and the mixture of diffusers (MoD) for diverse voice and appearance generation. Fo",
    "arxiv_url": "https://arxiv.org/abs/2311.06772v1",
    "pdf_url": "https://arxiv.org/pdf/2311.06772v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2311.06772",
    "arxiv_authors": [
      "Yilin Zhao",
      "Xinbin Yuan",
      "Shanghua Gao",
      "Zhijie Lin",
      "Qibin Hou",
      "Jiashi Feng",
      "Daquan Zhou"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=ChatAnything%3A+Facetime+Chat+with+LLM-Enhanced+Personas+Yilin+Zhao+Xinbin+Yuan+Shanghua+Gao+Zhijie+Lin+Qibin+Hou",
    "gs_search_success": true,
    "gs_authors": [
      "zW32dXsAAAAJ",
      "xXMj6_EAAAAJ",
      "DdCAbWwAAAAJ",
      "fF8OFV8AAAAJ",
      "Be26Qu8AAAAJ",
      "Q8iay0gAAAAJ"
    ],
    "citation_count": 6,
    "gs_author_count": 7
  },
  {
    "arxiv_id": "2310.01946",
    "title": "CoralVOS: Dataset and Benchmark for Coral Video Segmentation",
    "year": 2023,
    "published": "2023-10-03T10:45:37Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Coral reefs formulate the most valuable and productive marine ecosystems, providing habitat for many marine species. Coral reef surveying and analysis are currently confined to coral experts who invest substantial effort in generating comprehensive and dependable reports (\\emph{e.g.}, coral coverage, population, spatial distribution, \\textit{etc}), from the collected survey data. However, performing dense coral analysis based on manual efforts is significantly time-consuming, the existing coral ",
    "arxiv_url": "https://arxiv.org/abs/2310.01946v1",
    "pdf_url": "https://arxiv.org/pdf/2310.01946v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2310.01946",
    "arxiv_authors": [
      "Zheng Ziqiang",
      "Xie Yaofeng",
      "Liang Haixin",
      "Yu Zhibin",
      "Sai-Kit Yeung"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=CoralVOS%3A+Dataset+and+Benchmark+for+Coral+Video+Segmentation+Zheng+Ziqiang+Xie+Yaofeng+Liang+Haixin+Yu+Zhibin+Sai-Kit+Yeung",
    "gs_search_success": false,
    "gs_authors": [],
    "citation_count": null,
    "error_type": "fetch_failed",
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2501.07525",
    "title": "RadAlign: Advancing Radiology Report Generation with Vision-Language Concept Alignment",
    "year": 2025,
    "published": "2025-01-13T17:55:32Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "abstract": "Automated chest radiographs interpretation requires both accurate disease classification and detailed radiology report generation, presenting a significant challenge in the clinical workflow. Current approaches either focus on classification accuracy at the expense of interpretability or generate detailed but potentially unreliable reports through image captioning techniques. In this study, we present RadAlign, a novel framework that combines the predictive accuracy of vision-language models (VL",
    "arxiv_url": "https://arxiv.org/abs/2501.07525v2",
    "pdf_url": "https://arxiv.org/pdf/2501.07525v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2501.07525",
    "arxiv_authors": [
      "Difei Gu",
      "Yunhe Gao",
      "Yang Zhou",
      "Mu Zhou",
      "Dimitris Metaxas"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=RadAlign%3A+Advancing+Radiology+Report+Generation+with+Vision-Language+Concept+Alignment+Difei+Gu+Yunhe+Gao+Yang+Zhou+Mu+Zhou+Dimitris+Metaxas",
    "gs_search_success": true,
    "gs_authors": [
      "a7VNhCIAAAAJ",
      "TOsFPu4AAAAJ",
      "wYvr48gAAAAJ",
      "nKuyKx0AAAAJ",
      "BsQ8IUcAAAAJ"
    ],
    "citation_count": 9,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2405.14312",
    "title": "Improving Gloss-free Sign Language Translation by Reducing Representation Density",
    "year": 2024,
    "published": "2024-05-23T08:32:58Z",
    "categories": [
      "cs.CV",
      "cs.CL",
      "cs.MM"
    ],
    "abstract": "Gloss-free sign language translation (SLT) aims to develop well-performing SLT systems with no requirement for the costly gloss annotations, but currently still lags behind gloss-based approaches significantly. In this paper, we identify a representation density problem that could be a bottleneck in restricting the performance of gloss-free SLT. Specifically, the representation density problem describes that the visual representations of semantically distinct sign gestures tend to be closely pac",
    "arxiv_url": "https://arxiv.org/abs/2405.14312v2",
    "pdf_url": "https://arxiv.org/pdf/2405.14312v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2405.14312",
    "arxiv_authors": [
      "Jinhui Ye",
      "Xing Wang",
      "Wenxiang Jiao",
      "Junwei Liang",
      "Hui Xiong"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Improving+Gloss-free+Sign+Language+Translation+by+Reducing+Representation+Density+Jinhui+Ye+Xing+Wang+Wenxiang+Jiao+Junwei+Liang+Hui+Xiong",
    "gs_search_success": true,
    "gs_authors": [
      "6AqRKa0AAAAJ",
      "cVDF1tkAAAAJ",
      "bMedjfUAAAAJ",
      "CvtODukAAAAJ"
    ],
    "citation_count": 25,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2410.02830",
    "title": "YouTube Video Analytics for Patient Engagement: Evidence from Colonoscopy Preparation Videos",
    "year": 2024,
    "published": "2024-10-01T19:38:46Z",
    "categories": [
      "eess.IV",
      "cs.CV",
      "cs.IR",
      "cs.MM"
    ],
    "abstract": "Videos can be an effective way to deliver contextualized, just-in-time medical information for patient education. However, video analysis, from topic identification and retrieval to extraction and analysis of medical information and understandability from a patient perspective are extremely challenging tasks. This study demonstrates a data analysis pipeline that utilizes methods to retrieve medical information from YouTube videos on preparing for a colonoscopy exam, a much maligned and disliked ",
    "arxiv_url": "https://arxiv.org/abs/2410.02830v1",
    "pdf_url": "https://arxiv.org/pdf/2410.02830v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2410.02830",
    "arxiv_authors": [
      "Yawen Guo",
      "Xiao Liu",
      "Anjana Susarla",
      "Padman Rema"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=YouTube+Video+Analytics+for+Patient+Engagement%3A+Evidence+from+Colonoscopy+Preparation+Videos+Yawen+Guo+Xiao+Liu+Anjana+Susarla+Padman+Rema",
    "gs_search_success": true,
    "gs_authors": [
      "JpFHYKcAAAAJ",
      "4Vi6MzIAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2408.12141",
    "title": "TRRG: Towards Truthful Radiology Report Generation With Cross-modal Disease Clue Enhanced Large Language Model",
    "year": 2024,
    "published": "2024-08-22T05:52:27Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "The vision-language modeling capability of multi-modal large language models has attracted wide attention from the community. However, in medical domain, radiology report generation using vision-language models still faces significant challenges due to the imbalanced data distribution caused by numerous negated descriptions in radiology reports and issues such as rough alignment between radiology reports and radiography. In this paper, we propose a truthful radiology report generation framework,",
    "arxiv_url": "https://arxiv.org/abs/2408.12141v1",
    "pdf_url": "https://arxiv.org/pdf/2408.12141v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2408.12141",
    "arxiv_authors": [
      "Yuhao Wang",
      "Chao Hao",
      "Yawen Cui",
      "Xinqi Su",
      "Weicheng Xie",
      "Tao Tan",
      "Zitong Yu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=TRRG%3A+Towards+Truthful+Radiology+Report+Generation+With+Cross-modal+Disease+Clue+Enhanced+Large+Language+Model+Yuhao+Wang+Chao+Hao+Yawen+Cui+Xinqi+Su+Weicheng+Xie",
    "gs_search_success": true,
    "gs_authors": [
      "S2uh8OIAAAAJ",
      "ziHejLwAAAAJ",
      "lxxn3CcAAAAJ",
      "lLg3WRkAAAAJ",
      "9TQ_Pq8AAAAJ",
      "AZ_y9HgAAAAJ",
      "Er0gOskAAAAJ",
      "c4yDMc8AAAAJ",
      "wi016FcAAAAJ"
    ],
    "citation_count": 4,
    "gs_author_count": 7
  },
  {
    "arxiv_id": "2312.15273",
    "title": "Benefit from public unlabeled data: A Frangi filtering-based pretraining network for 3D cerebrovascular segmentation",
    "year": 2023,
    "published": "2023-12-23T14:47:21Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "The precise cerebrovascular segmentation in time-of-flight magnetic resonance angiography (TOF-MRA) data is crucial for clinically computer-aided diagnosis. However, the sparse distribution of cerebrovascular structures in TOF-MRA results in an exceedingly high cost for manual data labeling. The use of unlabeled TOF-MRA data holds the potential to enhance model performance significantly. In this study, we construct the largest preprocessed unlabeled TOF-MRA datasets (1510 subjects) to date. We a",
    "arxiv_url": "https://arxiv.org/abs/2312.15273v1",
    "pdf_url": "https://arxiv.org/pdf/2312.15273v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2312.15273",
    "arxiv_authors": [
      "Gen Shi",
      "Hao Lu",
      "Hui Hui",
      "Jie Tian"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Benefit+from+public+unlabeled+data%3A+A+Frangi+filtering-based+pretraining+network+for+3D+cerebrovascular+segmentation+Gen+Shi+Hao+Lu+Hui+Hui+Jie+Tian",
    "gs_search_success": true,
    "gs_authors": [
      "L_AsNpcAAAAJ",
      "xVgKQg0AAAAJ",
      "K6awU5kAAAAJ"
    ],
    "citation_count": 7,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2504.13540",
    "title": "EG-Gaussian: Epipolar Geometry and Graph Network Enhanced 3D Gaussian Splatting",
    "year": 2025,
    "published": "2025-04-18T08:10:39Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "In this paper, we explore an open research problem concerning the reconstruction of 3D scenes from images. Recent methods have adopt 3D Gaussian Splatting (3DGS) to produce 3D scenes due to its efficient training process. However, these methodologies may generate incomplete 3D scenes or blurred multiviews. This is because of (1) inaccurate 3DGS point initialization and (2) the tendency of 3DGS to flatten 3D Gaussians with the sparse-view input. To address these issues, we propose a novel framewo",
    "arxiv_url": "https://arxiv.org/abs/2504.13540v1",
    "pdf_url": "https://arxiv.org/pdf/2504.13540v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2504.13540",
    "arxiv_authors": [
      "Beizhen Zhao",
      "Yifan Zhou",
      "Zijian Wang",
      "Hao Wang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=EG-Gaussian%3A+Epipolar+Geometry+and+Graph+Network+Enhanced+3D+Gaussian+Splatting+Beizhen+Zhao+Yifan+Zhou+Zijian+Wang+Hao+Wang",
    "gs_search_success": true,
    "gs_authors": [
      "856zi9EAAAAJ",
      "qX801o4AAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2501.04440",
    "title": "RSAR: Restricted State Angle Resolver and Rotated SAR Benchmark",
    "year": 2025,
    "published": "2025-01-08T11:41:47Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Rotated object detection has made significant progress in the optical remote sensing. However, advancements in the Synthetic Aperture Radar (SAR) field are laggard behind, primarily due to the absence of a large-scale dataset. Annotating such a dataset is inefficient and costly. A promising solution is to employ a weakly supervised model (e.g., trained with available horizontal boxes only) to generate pseudo-rotated boxes for reference before manual calibration. Unfortunately, the existing weakl",
    "arxiv_url": "https://arxiv.org/abs/2501.04440v1",
    "pdf_url": "https://arxiv.org/pdf/2501.04440v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2501.04440",
    "arxiv_authors": [
      "Xin Zhang",
      "Xue Yang",
      "Yuxuan Li",
      "Jian Yang",
      "Ming-Ming Cheng",
      "Xiang Li"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=RSAR%3A+Restricted+State+Angle+Resolver+and+Rotated+SAR+Benchmark+Xin+Zhang+Xue+Yang+Yuxuan+Li+Jian+Yang+Ming-Ming+Cheng",
    "gs_search_success": true,
    "gs_authors": [
      "2xTlvV0AAAAJ",
      "vKnUqmMAAAAJ",
      "huWpVyEAAAAJ",
      "i9opWEgAAAAJ",
      "oamjJdYAAAAJ"
    ],
    "citation_count": 16,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2307.14187",
    "title": "ADAPT: Efficient Multi-Agent Trajectory Prediction with Adaptation",
    "year": 2023,
    "published": "2023-07-26T13:41:51Z",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "abstract": "Forecasting future trajectories of agents in complex traffic scenes requires reliable and efficient predictions for all agents in the scene. However, existing methods for trajectory prediction are either inefficient or sacrifice accuracy. To address this challenge, we propose ADAPT, a novel approach for jointly predicting the trajectories of all agents in the scene with dynamic weight learning. Our approach outperforms state-of-the-art methods in both single-agent and multi-agent settings on the",
    "arxiv_url": "https://arxiv.org/abs/2307.14187v1",
    "pdf_url": "https://arxiv.org/pdf/2307.14187v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2307.14187",
    "arxiv_authors": [
      "Görkay Aydemir",
      "Adil Kaan Akan",
      "Fatma Güney"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=ADAPT%3A+Efficient+Multi-Agent+Trajectory+Prediction+with+Adaptation+G%C3%B6rkay+Aydemir+Adil+Kaan+Akan+Fatma+G%C3%BCney",
    "gs_search_success": true,
    "gs_authors": [
      "g3UitywAAAAJ",
      "AtT2D54AAAAJ",
      "0nONNn0AAAAJ"
    ],
    "citation_count": 101,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2409.09766",
    "title": "Automated Lesion Segmentation in Whole-Body PET/CT in a multitracer setting",
    "year": 2024,
    "published": "2024-09-15T15:32:29Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "This study explores a workflow for automated segmentation of lesions in FDG and PSMA PET/CT images. Due to the substantial differences in image characteristics between FDG and PSMA, specialized preprocessing steps are required. Utilizing YOLOv8 for data classification, the FDG and PSMA images are preprocessed separately before feeding them into the segmentation models, aiming to improve lesion segmentation accuracy. The study focuses on evaluating the performance of automated segmentation workfl",
    "arxiv_url": "https://arxiv.org/abs/2409.09766v1",
    "pdf_url": "https://arxiv.org/pdf/2409.09766v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2409.09766",
    "arxiv_authors": [
      "Qiaoyi Xue",
      "Youdan Feng",
      "Jiayi Liu",
      "Tianming Xu",
      "Kaixin Shen",
      "Chuyun Shen",
      "Yuhang Shi"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Automated+Lesion+Segmentation+in+Whole-Body+PET%2FCT+in+a+multitracer+setting+Qiaoyi+Xue+Youdan+Feng+Jiayi+Liu+Tianming+Xu+Kaixin+Shen",
    "gs_search_success": true,
    "gs_authors": [
      "-6y5T3YAAAAJ",
      "sB4jZ54AAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 7
  },
  {
    "arxiv_id": "2407.14249",
    "title": "An Attention-based Representation Distillation Baseline for Multi-Label Continual Learning",
    "year": 2024,
    "published": "2024-07-19T12:30:03Z",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "The field of Continual Learning (CL) has inspired numerous researchers over the years, leading to increasingly advanced countermeasures to the issue of catastrophic forgetting. Most studies have focused on the single-class scenario, where each example comes with a single label. The recent literature has successfully tackled such a setting, with impressive results. Differently, we shift our attention to the multi-label scenario, as we feel it to be more representative of real-world open problems.",
    "arxiv_url": "https://arxiv.org/abs/2407.14249v1",
    "pdf_url": "https://arxiv.org/pdf/2407.14249v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2407.14249",
    "arxiv_authors": [
      "Martin Menabue",
      "Emanuele Frascaroli",
      "Matteo Boschini",
      "Lorenzo Bonicelli",
      "Angelo Porrello",
      "Simone Calderara"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=An+Attention-based+Representation+Distillation+Baseline+for+Multi-Label+Continual+Learning+Martin+Menabue+Emanuele+Frascaroli+Matteo+Boschini+Lorenzo+Bonicelli+Angelo+Porrello",
    "gs_search_success": true,
    "gs_authors": [
      "YaRuDkcAAAAJ",
      "BQnnO7QAAAAJ",
      "znwe03IAAAAJ",
      "4GTV0XoAAAAJ",
      "pGNOOk0AAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2405.15232",
    "title": "DEEM: Diffusion Models Serve as the Eyes of Large Language Models for Image Perception",
    "year": 2024,
    "published": "2024-05-24T05:46:04Z",
    "categories": [
      "cs.CV",
      "cs.CL"
    ],
    "abstract": "The development of large language models (LLMs) has significantly advanced the emergence of large multimodal models (LMMs). While LMMs have achieved tremendous success by promoting the synergy between multimodal comprehension and creation, they often face challenges when confronted with out-of-distribution data, such as which can hardly distinguish orientation, quantity, color, structure, etc. This is primarily due to their reliance on image encoders trained to encode images into task-relevant f",
    "arxiv_url": "https://arxiv.org/abs/2405.15232v4",
    "pdf_url": "https://arxiv.org/pdf/2405.15232v4",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2405.15232",
    "arxiv_authors": [
      "Run Luo",
      "Yunshui Li",
      "Longze Chen",
      "Wanwei He",
      "Ting-En Lin",
      "Ziqiang Liu",
      "Lei Zhang",
      "Zikai Song",
      "Xiaobo Xia",
      "Tongliang Liu",
      "Min Yang",
      "Binyuan Hui"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=DEEM%3A+Diffusion+Models+Serve+as+the+Eyes+of+Large+Language+Models+for+Image+Perception+Run+Luo+Yunshui+Li+Longze+Chen+Wanwei+He+Ting-En+Lin",
    "gs_search_success": true,
    "gs_authors": [
      "juR8ZS4AAAAJ",
      "jRsugY0AAAAJ",
      "x1EPTHAAAAAJ",
      "_wop6KgAAAAJ",
      "JT40MWMAAAAJ",
      "RBb3ItMAAAAJ",
      "XNdFVMAAAAAJ",
      "EiLdZ_YAAAAJ",
      "phg8yxoAAAAJ",
      "NNfxnUYAAAAJ",
      "K8di4JwAAAAJ",
      "1qnuOZsAAAAJ"
    ],
    "citation_count": 31,
    "gs_author_count": 9
  },
  {
    "arxiv_id": "2409.03555",
    "title": "Unified Framework for Pre-trained Neural Network Compression via Decomposition and Optimized Rank Selection",
    "year": 2024,
    "published": "2024-09-05T14:15:54Z",
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "abstract": "Despite their high accuracy, complex neural networks demand significant computational resources, posing challenges for deployment on resource constrained devices such as mobile phones and embedded systems. Compression algorithms have been developed to address these challenges by reducing model size and computational demands while maintaining accuracy. Among these approaches, factorization methods based on tensor decomposition are theoretically sound and effective. However, they face difficulties",
    "arxiv_url": "https://arxiv.org/abs/2409.03555v2",
    "pdf_url": "https://arxiv.org/pdf/2409.03555v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2409.03555",
    "arxiv_authors": [
      "Ali Aghababaei-Harandi",
      "Massih-Reza Amini"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Unified+Framework+for+Pre-trained+Neural+Network+Compression+via+Decomposition+and+Optimized+Rank+Selection+Ali+Aghababaei-Harandi+Massih-Reza+Amini",
    "gs_search_success": true,
    "gs_authors": [
      "e6f7gBoAAAAJ",
      "TAfVg4wAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 2
  },
  {
    "arxiv_id": "2407.08801",
    "title": "DG-PIC: Domain Generalized Point-In-Context Learning for Point Cloud Understanding",
    "year": 2024,
    "published": "2024-07-11T18:21:40Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Recent point cloud understanding research suffers from performance drops on unseen data, due to the distribution shifts across different domains. While recent studies use Domain Generalization (DG) techniques to mitigate this by learning domain-invariant features, most are designed for a single task and neglect the potential of testing data. Despite In-Context Learning (ICL) showcasing multi-task learning capability, it usually relies on high-quality context-rich data and considers a single data",
    "arxiv_url": "https://arxiv.org/abs/2407.08801v1",
    "pdf_url": "https://arxiv.org/pdf/2407.08801v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2407.08801",
    "arxiv_authors": [
      "Jincen Jiang",
      "Qianyu Zhou",
      "Yuhang Li",
      "Xuequan Lu",
      "Meili Wang",
      "Lizhuang Ma",
      "Jian Chang",
      "Jian Jun Zhang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=DG-PIC%3A+Domain+Generalized+Point-In-Context+Learning+for+Point+Cloud+Understanding+Jincen+Jiang+Qianyu+Zhou+Yuhang+Li+Xuequan+Lu+Meili+Wang",
    "gs_search_success": true,
    "gs_authors": [
      "zb8bH8gAAAAJ",
      "KHg04fkAAAAJ",
      "yNb6-d4AAAAJ",
      "yd58y_0AAAAJ",
      "VSeDxyYAAAAJ",
      "cG_WXywAAAAJ",
      "J4x2-d0AAAAJ"
    ],
    "citation_count": 16,
    "gs_author_count": 8
  },
  {
    "arxiv_id": "2505.18956",
    "title": "How Do Images Align and Complement LiDAR? Towards a Harmonized Multi-modal 3D Panoptic Segmentation",
    "year": 2025,
    "published": "2025-05-25T03:01:28Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "cs.MM"
    ],
    "abstract": "LiDAR-based 3D panoptic segmentation often struggles with the inherent sparsity of data from LiDAR sensors, which makes it challenging to accurately recognize distant or small objects. Recently, a few studies have sought to overcome this challenge by integrating LiDAR inputs with camera images, leveraging the rich and dense texture information provided by the latter. While these approaches have shown promising results, they still face challenges, such as misalignment during data augmentation and",
    "arxiv_url": "https://arxiv.org/abs/2505.18956v2",
    "pdf_url": "https://arxiv.org/pdf/2505.18956v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2505.18956",
    "arxiv_authors": [
      "Yining Pan",
      "Qiongjie Cui",
      "Xulei Yang",
      "Na Zhao"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=How+Do+Images+Align+and+Complement+LiDAR%3F+Towards+a+Harmonized+Multi-modal+3D+Panoptic+Segmentation+Yining+Pan+Qiongjie+Cui+Xulei+Yang+Na+Zhao",
    "gs_search_success": true,
    "gs_authors": [
      "l_6n20kAAAAJ",
      "tXkwIK8AAAAJ",
      "KOL2dMwAAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2310.18237",
    "title": "Generative AI Model for Artistic Style Transfer Using Convolutional Neural Networks",
    "year": 2023,
    "published": "2023-10-27T16:21:17Z",
    "categories": [
      "cs.CV",
      "cs.HC"
    ],
    "abstract": "Artistic style transfer, a captivating application of generative artificial intelligence, involves fusing the content of one image with the artistic style of another to create unique visual compositions. This paper presents a comprehensive overview of a novel technique for style transfer using Convolutional Neural Networks (CNNs). By leveraging deep image representations learned by CNNs, we demonstrate how to separate and manipulate image content and style, enabling the synthesis of high-quality",
    "arxiv_url": "https://arxiv.org/abs/2310.18237v2",
    "pdf_url": "https://arxiv.org/pdf/2310.18237v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2310.18237",
    "arxiv_authors": [
      "Jonayet Miah",
      "Duc M Cao",
      "Md Abu Sayed",
      "Md. Sabbirul Haque"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Generative+AI+Model+for+Artistic+Style+Transfer+Using+Convolutional+Neural+Networks+Jonayet+Miah+Duc+M+Cao+Md+Abu+Sayed+Md.+Sabbirul+Haque",
    "gs_search_success": true,
    "gs_authors": [
      "Y7AuzjcAAAAJ",
      "M76ivpgAAAAJ",
      "_40m5ugAAAAJ",
      "bMzkbP8AAAAJ"
    ],
    "citation_count": 24,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2412.19225",
    "title": "Completion as Enhancement: A Degradation-Aware Selective Image Guided Network for Depth Completion",
    "year": 2024,
    "published": "2024-12-26T14:05:01Z",
    "categories": [
      "cs.CV",
      "eess.IV"
    ],
    "abstract": "In this paper, we introduce the Selective Image Guided Network (SigNet), a novel degradation-aware framework that transforms depth completion into depth enhancement for the first time. Moving beyond direct completion using convolutional neural networks (CNNs), SigNet initially densifies sparse depth data through non-CNN densification tools to obtain coarse yet dense depth. This approach eliminates the mismatch and ambiguity caused by direct convolution over irregularly sampled sparse data. Subse",
    "arxiv_url": "https://arxiv.org/abs/2412.19225v2",
    "pdf_url": "https://arxiv.org/pdf/2412.19225v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2412.19225",
    "arxiv_authors": [
      "Zhiqiang Yan",
      "Zhengxue Wang",
      "Kun Wang",
      "Jun Li",
      "Jian Yang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Completion+as+Enhancement%3A+A+Degradation-Aware+Selective+Image+Guided+Network+for+Depth+Completion+Zhiqiang+Yan+Zhengxue+Wang+Kun+Wang+Jun+Li+Jian+Yang",
    "gs_search_success": true,
    "gs_authors": [
      "hnrkzIEAAAAJ",
      "iGPEwQsAAAAJ",
      "ORn7aZcAAAAJ",
      "VogTuQkAAAAJ",
      "i9opWEgAAAAJ"
    ],
    "citation_count": 9,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2409.02562",
    "title": "One Homography is All You Need: IMM-based Joint Homography and Multiple Object State Estimation",
    "year": 2024,
    "published": "2024-09-04T09:29:24Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "A novel online MOT algorithm, IMM Joint Homography State Estimation (IMM-JHSE), is proposed. IMM-JHSE uses an initial homography estimate as the only additional 3D information, whereas other 3D MOT methods use regular 3D measurements. By jointly modelling the homography matrix and its dynamics as part of track state vectors, IMM-JHSE removes the explicit influence of camera motion compensation techniques on predicted track position states, which was prevalent in previous approaches. Expanding up",
    "arxiv_url": "https://arxiv.org/abs/2409.02562v4",
    "pdf_url": "https://arxiv.org/pdf/2409.02562v4",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2409.02562",
    "arxiv_authors": [
      "Paul Johannes Claasen",
      "Johan Pieter de Villiers"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=One+Homography+is+All+You+Need%3A+IMM-based+Joint+Homography+and+Multiple+Object+State+Estimation+Paul+Johannes+Claasen+Johan+Pieter+de+Villiers",
    "gs_search_success": true,
    "gs_authors": [
      "2bUOLf0AAAAJ",
      "MLssl98AAAAJ"
    ],
    "citation_count": 2,
    "gs_author_count": 2
  },
  {
    "arxiv_id": "2504.20032",
    "title": "More Clear, More Flexible, More Precise: A Comprehensive Oriented Object Detection benchmark for UAV",
    "year": 2025,
    "published": "2025-04-28T17:56:02Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Applications of unmanned aerial vehicle (UAV) in logistics, agricultural automation, urban management, and emergency response are highly dependent on oriented object detection (OOD) to enhance visual perception. Although existing datasets for OOD in UAV provide valuable resources, they are often designed for specific downstream tasks.Consequently, they exhibit limited generalization performance in real flight scenarios and fail to thoroughly demonstrate algorithm effectiveness in practical envir",
    "arxiv_url": "https://arxiv.org/abs/2504.20032v1",
    "pdf_url": "https://arxiv.org/pdf/2504.20032v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2504.20032",
    "arxiv_authors": [
      "Kai Ye",
      "Haidi Tang",
      "Bowen Liu",
      "Pingyang Dai",
      "Liujuan Cao",
      "Rongrong Ji"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=More+Clear%2C+More+Flexible%2C+More+Precise%3A+A+Comprehensive+Oriented+Object+Detection+benchmark+for+UAV+Kai+Ye+Haidi+Tang+Bowen+Liu+Pingyang+Dai+Liujuan+Cao",
    "gs_search_success": true,
    "gs_authors": [
      "fEw3__QAAAAJ",
      "lRSD7PQAAAAJ",
      "HyFR80sAAAAJ",
      "R9MgE0cAAAAJ"
    ],
    "citation_count": 4,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2503.21829",
    "title": "Learning from spatially inhomogenous data: resolution-adaptive convolutions for multiple sclerosis lesion segmentation",
    "year": 2025,
    "published": "2025-03-26T14:07:52Z",
    "categories": [
      "eess.IV",
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "In the setting of clinical imaging, differences in between vendors, hospitals and sequences can yield highly inhomogeneous imaging data. In MRI in particular, voxel dimension, slice spacing and acquisition plane can vary substantially. For clinical applications, therefore, algorithms must be trained to handle data with various voxel resolutions. The usual strategy to deal with heterogeneity of resolution is harmonization: resampling imaging data to a common (usually isovoxel) resolution. This ca",
    "arxiv_url": "https://arxiv.org/abs/2503.21829v1",
    "pdf_url": "https://arxiv.org/pdf/2503.21829v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2503.21829",
    "arxiv_authors": [
      "Ivan Diaz",
      "Florin Scherer",
      "Yanik Berli",
      "Roland Wiest",
      "Helly Hammer",
      "Robert Hoepner",
      "Alejandro Leon Betancourt",
      "Piotr Radojewski",
      "Richard McKinley"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Learning+from+spatially+inhomogenous+data%3A+resolution-adaptive+convolutions+for+multiple+sclerosis+lesion+segmentation+Ivan+Diaz+Florin+Scherer+Yanik+Berli+Roland+Wiest+Helly+Hammer",
    "gs_search_success": false,
    "gs_authors": [],
    "citation_count": null,
    "error_type": "fetch_failed",
    "gs_author_count": 9
  },
  {
    "arxiv_id": "2502.03103",
    "title": "Edge Attention Module for Object Classification",
    "year": 2025,
    "published": "2025-02-05T11:59:47Z",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "A novel ``edge attention-based Convolutional Neural Network (CNN)'' is proposed in this research for object classification task. With the advent of advanced computing technology, CNN models have achieved to remarkable success, particularly in computer vision applications. Nevertheless, the efficacy of the conventional CNN is often hindered due to class imbalance and inter-class similarity problems, which are particularly prominent in the computer vision field. In this research, we introduce for ",
    "arxiv_url": "https://arxiv.org/abs/2502.03103v1",
    "pdf_url": "https://arxiv.org/pdf/2502.03103v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2502.03103",
    "arxiv_authors": [
      "Santanu Roy",
      "Ashvath Suresh",
      "Archit Gupta"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Edge+Attention+Module+for+Object+Classification+Santanu+Roy+Ashvath+Suresh+Archit+Gupta",
    "gs_search_success": true,
    "gs_authors": [],
    "citation_count": 2,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2503.18408",
    "title": "Fast and Physically-based Neural Explicit Surface for Relightable Human Avatars",
    "year": 2025,
    "published": "2025-03-24T07:31:15Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Efficiently modeling relightable human avatars from sparse-view videos is crucial for AR/VR applications. Current methods use neural implicit representations to capture dynamic geometry and reflectance, which incur high costs due to the need for dense sampling in volume rendering. To overcome these challenges, we introduce Physically-based Neural Explicit Surface (PhyNES), which employs compact neural material maps based on the Neural Explicit Surface (NES) representation. PhyNES organizes human",
    "arxiv_url": "https://arxiv.org/abs/2503.18408v1",
    "pdf_url": "https://arxiv.org/pdf/2503.18408v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2503.18408",
    "arxiv_authors": [
      "Jiacheng Wu",
      "Ruiqi Zhang",
      "Jie Chen",
      "Hui Zhang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Fast+and+Physically-based+Neural+Explicit+Surface+for+Relightable+Human+Avatars+Jiacheng+Wu+Ruiqi+Zhang+Jie+Chen+Hui+Zhang",
    "gs_search_success": true,
    "gs_authors": [
      "3f--1fEAAAAJ",
      "qrWi1RYAAAAJ",
      "w3mzCiwAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2303.09152",
    "title": "Learning a Room with the Occ-SDF Hybrid: Signed Distance Function Mingled with Occupancy Aids Scene Representation",
    "year": 2023,
    "published": "2023-03-16T08:34:02Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Implicit neural rendering, which uses signed distance function (SDF) representation with geometric priors (such as depth or surface normal), has led to impressive progress in the surface reconstruction of large-scale scenes. However, applying this method to reconstruct a room-level scene from images may miss structures in low-intensity areas or small and thin objects. We conducted experiments on three datasets to identify limitations of the original color rendering loss and priors-embedded SDF s",
    "arxiv_url": "https://arxiv.org/abs/2303.09152v1",
    "pdf_url": "https://arxiv.org/pdf/2303.09152v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2303.09152",
    "arxiv_authors": [
      "Xiaoyang Lyu",
      "Peng Dai",
      "Zizhang Li",
      "Dongyu Yan",
      "Yi Lin",
      "Yifan Peng",
      "Xiaojuan Qi"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Learning+a+Room+with+the+Occ-SDF+Hybrid%3A+Signed+Distance+Function+Mingled+with+Occupancy+Aids+Scene+Representation+Xiaoyang+Lyu+Peng+Dai+Zizhang+Li+Dongyu+Yan+Yi+Lin",
    "gs_search_success": true,
    "gs_authors": [
      "2fGIJBsAAAAJ",
      "UMveGGwAAAAJ",
      "bGn0uacAAAAJ",
      "29otYD8AAAAJ",
      "LCr0d_cAAAAJ",
      "SF7bq48AAAAJ",
      "uVtphHMAAAAJ"
    ],
    "citation_count": 20,
    "gs_author_count": 7
  },
  {
    "arxiv_id": "2501.16778",
    "title": "FlexMotion: Lightweight, Physics-Aware, and Controllable Human Motion Generation",
    "year": 2025,
    "published": "2025-01-28T08:02:21Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR",
      "cs.LG"
    ],
    "abstract": "Lightweight, controllable, and physically plausible human motion synthesis is crucial for animation, virtual reality, robotics, and human-computer interaction applications. Existing methods often compromise between computational efficiency, physical realism, or spatial controllability. We propose FlexMotion, a novel framework that leverages a computationally lightweight diffusion model operating in the latent space, eliminating the need for physics simulators and enabling fast and efficient trai",
    "arxiv_url": "https://arxiv.org/abs/2501.16778v1",
    "pdf_url": "https://arxiv.org/pdf/2501.16778v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2501.16778",
    "arxiv_authors": [
      "Arvin Tashakori",
      "Arash Tashakori",
      "Gongbo Yang",
      "Z. Jane Wang",
      "Peyman Servati"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=FlexMotion%3A+Lightweight%2C+Physics-Aware%2C+and+Controllable+Human+Motion+Generation+Arvin+Tashakori+Arash+Tashakori+Gongbo+Yang+Z.+Jane+Wang+Peyman+Servati",
    "gs_search_success": true,
    "gs_authors": [
      "W75uTm8AAAAJ",
      "bl8U3JoAAAAJ",
      "8pFUPnQAAAAJ",
      "UDnrOGUAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2401.02473",
    "title": "VASE: Object-Centric Appearance and Shape Manipulation of Real Videos",
    "year": 2024,
    "published": "2024-01-04T18:59:24Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Recently, several works tackled the video editing task fostered by the success of large-scale text-to-image generative models. However, most of these methods holistically edit the frame using the text, exploiting the prior given by foundation diffusion models and focusing on improving the temporal consistency across frames. In this work, we introduce a framework that is object-centric and is designed to control both the object's appearance and, notably, to execute precise and explicit structural",
    "arxiv_url": "https://arxiv.org/abs/2401.02473v1",
    "pdf_url": "https://arxiv.org/pdf/2401.02473v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2401.02473",
    "arxiv_authors": [
      "Elia Peruzzo",
      "Vidit Goel",
      "Dejia Xu",
      "Xingqian Xu",
      "Yifan Jiang",
      "Zhangyang Wang",
      "Humphrey Shi",
      "Nicu Sebe"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=VASE%3A+Object-Centric+Appearance+and+Shape+Manipulation+of+Real+Videos+Elia+Peruzzo+Vidit+Goel+Dejia+Xu+Xingqian+Xu+Yifan+Jiang",
    "gs_search_success": false,
    "gs_authors": [],
    "citation_count": null,
    "error_type": "fetch_failed",
    "gs_author_count": 8
  },
  {
    "arxiv_id": "2409.01421",
    "title": "DiffCSG: Differentiable CSG via Rasterization",
    "year": 2024,
    "published": "2024-09-02T18:57:07Z",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "abstract": "Differentiable rendering is a key ingredient for inverse rendering and machine learning, as it allows to optimize scene parameters (shape, materials, lighting) to best fit target images. Differentiable rendering requires that each scene parameter relates to pixel values through differentiable operations. While 3D mesh rendering algorithms have been implemented in a differentiable way, these algorithms do not directly extend to Constructive-Solid-Geometry (CSG), a popular parametric representatio",
    "arxiv_url": "https://arxiv.org/abs/2409.01421v2",
    "pdf_url": "https://arxiv.org/pdf/2409.01421v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2409.01421",
    "arxiv_authors": [
      "Haocheng Yuan",
      "Adrien Bousseau",
      "Hao Pan",
      "Chengquan Zhang",
      "Niloy J. Mitra",
      "Changjian Li"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=DiffCSG%3A+Differentiable+CSG+via+Rasterization+Haocheng+Yuan+Adrien+Bousseau+Hao+Pan+Chengquan+Zhang+Niloy+J.+Mitra",
    "gs_search_success": true,
    "gs_authors": [
      "FIjRkRoAAAAJ",
      "aCgyQsQAAAAJ",
      "A50bS8EAAAAJ",
      "KdX5MN4AAAAJ",
      "dPrZJWMAAAAJ",
      "iczxr4UAAAAJ"
    ],
    "citation_count": 4,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2307.05707",
    "title": "MoP-CLIP: A Mixture of Prompt-Tuned CLIP Models for Domain Incremental Learning",
    "year": 2023,
    "published": "2023-07-11T18:17:50Z",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "Despite the recent progress in incremental learning, addressing catastrophic forgetting under distributional drift is still an open and important problem. Indeed, while state-of-the-art domain incremental learning (DIL) methods perform satisfactorily within known domains, their performance largely degrades in the presence of novel domains. This limitation hampers their generalizability, and restricts their scalability to more realistic settings where train and test data are drawn from different ",
    "arxiv_url": "https://arxiv.org/abs/2307.05707v1",
    "pdf_url": "https://arxiv.org/pdf/2307.05707v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2307.05707",
    "arxiv_authors": [
      "Julien Nicolas",
      "Florent Chiaroni",
      "Imtiaz Ziko",
      "Ola Ahmad",
      "Christian Desrosiers",
      "Jose Dolz"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=MoP-CLIP%3A+A+Mixture+of+Prompt-Tuned+CLIP+Models+for+Domain+Incremental+Learning+Julien+Nicolas+Florent+Chiaroni+Imtiaz+Ziko+Ola+Ahmad+Christian+Desrosiers",
    "gs_search_success": true,
    "gs_authors": [
      "-yTa02cAAAAJ",
      "yHQIFFMAAAAJ",
      "XJ0jF3gAAAAJ",
      "d4b8eOgAAAAJ"
    ],
    "citation_count": 13,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2405.16534",
    "title": "Pruning for Robust Concept Erasing in Diffusion Models",
    "year": 2024,
    "published": "2024-05-26T11:42:20Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Despite the impressive capabilities of generating images, text-to-image diffusion models are susceptible to producing undesirable outputs such as NSFW content and copyrighted artworks. To address this issue, recent studies have focused on fine-tuning model parameters to erase problematic concepts. However, existing methods exhibit a major flaw in robustness, as fine-tuned models often reproduce the undesirable outputs when faced with cleverly crafted prompts. This reveals a fundamental limitatio",
    "arxiv_url": "https://arxiv.org/abs/2405.16534v1",
    "pdf_url": "https://arxiv.org/pdf/2405.16534v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2405.16534",
    "arxiv_authors": [
      "Tianyun Yang",
      "Juan Cao",
      "Chang Xu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Pruning+for+Robust+Concept+Erasing+in+Diffusion+Models+Tianyun+Yang+Juan+Cao+Chang+Xu",
    "gs_search_success": true,
    "gs_authors": [
      "N4F_3eoAAAAJ",
      "fSBdNg0AAAAJ"
    ],
    "citation_count": 24,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2308.01086",
    "title": "Homography Estimation in Complex Topological Scenes",
    "year": 2023,
    "published": "2023-08-02T11:31:43Z",
    "categories": [
      "cs.CV",
      "cs.LG",
      "eess.IV"
    ],
    "abstract": "Surveillance videos and images are used for a broad set of applications, ranging from traffic analysis to crime detection. Extrinsic camera calibration data is important for most analysis applications. However, security cameras are susceptible to environmental conditions and small camera movements, resulting in a need for an automated re-calibration method that can account for these varying conditions. In this paper, we present an automated camera-calibration process leveraging a dictionary-base",
    "arxiv_url": "https://arxiv.org/abs/2308.01086v1",
    "pdf_url": "https://arxiv.org/pdf/2308.01086v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2308.01086",
    "arxiv_authors": [
      "Giacomo D'Amicantonio",
      "Egor Bondarau",
      "Peter H. N. De With"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Homography+Estimation+in+Complex+Topological+Scenes+Giacomo+D%27Amicantonio+Egor+Bondarau+Peter+H.+N.+De+With",
    "gs_search_success": true,
    "gs_authors": [
      "i2ZGcx4AAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2308.13488",
    "title": "Temporal Uncertainty Localization to Enable Human-in-the-loop Analysis of Dynamic Contrast-enhanced Cardiac MRI Datasets",
    "year": 2023,
    "published": "2023-08-25T16:55:30Z",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV",
      "physics.med-ph"
    ],
    "abstract": "Dynamic contrast-enhanced (DCE) cardiac magnetic resonance imaging (CMRI) is a widely used modality for diagnosing myocardial blood flow (perfusion) abnormalities. During a typical free-breathing DCE-CMRI scan, close to 300 time-resolved images of myocardial perfusion are acquired at various contrast \"wash in/out\" phases. Manual segmentation of myocardial contours in each time-frame of a DCE image series can be tedious and time-consuming, particularly when non-rigid motion correction has failed ",
    "arxiv_url": "https://arxiv.org/abs/2308.13488v2",
    "pdf_url": "https://arxiv.org/pdf/2308.13488v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2308.13488",
    "arxiv_authors": [
      "Dilek M. Yalcinkaya",
      "Khalid Youssef",
      "Bobak Heydari",
      "Orlando Simonetti",
      "Rohan Dharmakumar",
      "Subha Raman",
      "Behzad Sharif"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Temporal+Uncertainty+Localization+to+Enable+Human-in-the-loop+Analysis+of+Dynamic+Contrast-enhanced+Cardiac+MRI+Datasets+Dilek+M.+Yalcinkaya+Khalid+Youssef+Bobak+Heydari+Orlando+Simonetti+Rohan+Dharmakumar",
    "gs_search_success": true,
    "gs_authors": [
      "V_-NfxMAAAAJ",
      "HS5QmRQAAAAJ",
      "e6PLJBIAAAAJ",
      "h9oTYlYAAAAJ",
      "MhIluHsAAAAJ",
      "TOCmtyYAAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 7
  },
  {
    "arxiv_id": "2503.15910",
    "title": "No Thing, Nothing: Highlighting Safety-Critical Classes for Robust LiDAR Semantic Segmentation in Adverse Weather",
    "year": 2025,
    "published": "2025-03-20T07:40:24Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Existing domain generalization methods for LiDAR semantic segmentation under adverse weather struggle to accurately predict \"things\" categories compared to \"stuff\" categories. In typical driving scenes, \"things\" categories can be dynamic and associated with higher collision risks, making them crucial for safe navigation and planning. Recognizing the importance of \"things\" categories, we identify their performance drop as a serious bottleneck in existing approaches. We observed that adverse weath",
    "arxiv_url": "https://arxiv.org/abs/2503.15910v2",
    "pdf_url": "https://arxiv.org/pdf/2503.15910v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2503.15910",
    "arxiv_authors": [
      "Junsung Park",
      "Hwijeong Lee",
      "Inha Kang",
      "Hyunjung Shim"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=No+Thing%2C+Nothing%3A+Highlighting+Safety-Critical+Classes+for+Robust+LiDAR+Semantic+Segmentation+in+Adverse+Weather+Junsung+Park+Hwijeong+Lee+Inha+Kang+Hyunjung+Shim",
    "gs_search_success": true,
    "gs_authors": [
      "KB5XZGIAAAAJ",
      "sAYvnb8AAAAJ",
      "WjKLCQ4AAAAJ"
    ],
    "citation_count": 2,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2301.09299",
    "title": "Self-Supervised Image Representation Learning: Transcending Masking with Paired Image Overlay",
    "year": 2023,
    "published": "2023-01-23T07:00:04Z",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "Self-supervised learning has become a popular approach in recent years for its ability to learn meaningful representations without the need for data annotation. This paper proposes a novel image augmentation technique, overlaying images, which has not been widely applied in self-supervised learning. This method is designed to provide better guidance for the model to understand underlying information, resulting in more useful representations. The proposed method is evaluated using contrastive lea",
    "arxiv_url": "https://arxiv.org/abs/2301.09299v1",
    "pdf_url": "https://arxiv.org/pdf/2301.09299v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2301.09299",
    "arxiv_authors": [
      "Yinheng Li",
      "Han Ding",
      "Shaofei Wang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Self-Supervised+Image+Representation+Learning%3A+Transcending+Masking+with+Paired+Image+Overlay+Yinheng+Li+Han+Ding+Shaofei+Wang",
    "gs_search_success": true,
    "gs_authors": [
      "ZHd4hcQAAAAJ",
      "wpmYIp0AAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2409.18592",
    "title": "From One to the Power of Many: Invariance to Multi-LiDAR Perception from Single-Sensor Datasets",
    "year": 2024,
    "published": "2024-09-27T09:51:45Z",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "abstract": "Recently, LiDAR segmentation methods for autonomous vehicles, powered by deep neural networks, have experienced steep growth in performance on classic benchmarks, such as nuScenes and SemanticKITTI. However, there are still large gaps in performance when deploying models trained on such single-sensor setups to modern vehicles with multiple high-resolution LiDAR sensors. In this work, we introduce a new metric for feature-level invariance which can serve as a proxy to measure cross-domain general",
    "arxiv_url": "https://arxiv.org/abs/2409.18592v2",
    "pdf_url": "https://arxiv.org/pdf/2409.18592v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2409.18592",
    "arxiv_authors": [
      "Marc Uecker",
      "J. Marius Zöllner"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=From+One+to+the+Power+of+Many%3A+Invariance+to+Multi-LiDAR+Perception+from+Single-Sensor+Datasets+Marc+Uecker+J.+Marius+Z%C3%B6llner",
    "gs_search_success": true,
    "gs_authors": [
      "ad5UxSAAAAAJ",
      "4PRFXzwAAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 2
  },
  {
    "arxiv_id": "2312.16551",
    "title": "Blind Image Quality Assessment: A Brief Survey",
    "year": 2023,
    "published": "2023-12-27T12:28:13Z",
    "categories": [
      "cs.CV",
      "cs.MM"
    ],
    "abstract": "Blind Image Quality Assessment (BIQA) is essential for automatically evaluating the perceptual quality of visual signals without access to the references. In this survey, we provide a comprehensive analysis and discussion of recent developments in the field of BIQA. We have covered various aspects, including hand-crafted BIQAs that focus on distortion-specific and general-purpose methods, as well as deep-learned BIQAs that employ supervised and unsupervised learning techniques. Additionally, we ",
    "arxiv_url": "https://arxiv.org/abs/2312.16551v1",
    "pdf_url": "https://arxiv.org/pdf/2312.16551v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2312.16551",
    "arxiv_authors": [
      "Miaohui Wang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Blind+Image+Quality+Assessment%3A+A+Brief+Survey+Miaohui+Wang",
    "gs_search_success": true,
    "gs_authors": [],
    "citation_count": 4,
    "gs_author_count": 1
  },
  {
    "arxiv_id": "2409.10725",
    "title": "Depth from Coupled Optical Differentiation",
    "year": 2024,
    "published": "2024-09-16T20:58:11Z",
    "categories": [
      "cs.CV",
      "eess.IV"
    ],
    "abstract": "We propose depth from coupled optical differentiation, a low-computation passive-lighting 3D sensing mechanism. It is based on our discovery that per-pixel object distance can be rigorously determined by a coupled pair of optical derivatives of a defocused image using a simple, closed-form relationship. Unlike previous depth-from-defocus (DfD) methods that leverage spatial derivatives of the image to estimate scene depths, the proposed mechanism's use of only optical derivatives makes it signifi",
    "arxiv_url": "https://arxiv.org/abs/2409.10725v1",
    "pdf_url": "https://arxiv.org/pdf/2409.10725v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2409.10725",
    "arxiv_authors": [
      "Junjie Luo",
      "Yuxuan Liu",
      "Emma Alexander",
      "Qi Guo"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Depth+from+Coupled+Optical+Differentiation+Junjie+Luo+Yuxuan+Liu+Emma+Alexander+Qi+Guo",
    "gs_search_success": true,
    "gs_authors": [
      "HR5v9cUAAAAJ",
      "IeL0th8AAAAJ",
      "ru5JWAEAAAAJ",
      "9xN3AFkAAAAJ"
    ],
    "citation_count": 3,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2309.05013",
    "title": "Geometrically Consistent Partial Shape Matching",
    "year": 2023,
    "published": "2023-09-10T12:21:42Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Finding correspondences between 3D shapes is a crucial problem in computer vision and graphics, which is for example relevant for tasks like shape interpolation, pose transfer, or texture transfer. An often neglected but essential property of matchings is geometric consistency, which means that neighboring triangles in one shape are consistently matched to neighboring triangles in the other shape. Moreover, while in practice one often has only access to partial observations of a 3D shape (e.g. d",
    "arxiv_url": "https://arxiv.org/abs/2309.05013v1",
    "pdf_url": "https://arxiv.org/pdf/2309.05013v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2309.05013",
    "arxiv_authors": [
      "Viktoria Ehm",
      "Paul Roetzer",
      "Marvin Eisenberger",
      "Maolin Gao",
      "Florian Bernard",
      "Daniel Cremers"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Geometrically+Consistent+Partial+Shape+Matching+Viktoria+Ehm+Paul+Roetzer+Marvin+Eisenberger+Maolin+Gao+Florian+Bernard",
    "gs_search_success": true,
    "gs_authors": [
      "UtPc6sYAAAAJ",
      "6ZX5D5QAAAAJ",
      "4dFBNP4AAAAJ",
      "BupaOMMAAAAJ"
    ],
    "citation_count": 9,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2410.15334",
    "title": "Modality-Fair Preference Optimization for Trustworthy MLLM Alignment",
    "year": 2024,
    "published": "2024-10-20T08:56:52Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Multimodal large language models (MLLMs) have achieved remarkable success across various tasks. However, separate training of visual and textual encoders often results in a misalignment of the modality. Such misalignment may lead models to generate content that is absent from the input image, a phenomenon referred to as hallucination. These inaccuracies severely undermine the trustworthiness of MLLMs in real-world applications. Despite attempts to optimize text preferences to mitigate this issue",
    "arxiv_url": "https://arxiv.org/abs/2410.15334v2",
    "pdf_url": "https://arxiv.org/pdf/2410.15334v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2410.15334",
    "arxiv_authors": [
      "Songtao Jiang",
      "Yan Zhang",
      "Ruizhe Chen",
      "Tianxiang Hu",
      "Yeying Jin",
      "Qinglin He",
      "Yang Feng",
      "Jian Wu",
      "Zuozhu Liu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Modality-Fair+Preference+Optimization+for+Trustworthy+MLLM+Alignment+Songtao+Jiang+Yan+Zhang+Ruizhe+Chen+Tianxiang+Hu+Yeying+Jin",
    "gs_search_success": true,
    "gs_authors": [
      "FeRO_G4AAAAJ",
      "VO9XIXYAAAAJ",
      "v5ulZ3oAAAAJ",
      "Wr2K2sMAAAAJ",
      "Z8PYhA4AAAAJ",
      "h602wLIAAAAJ"
    ],
    "citation_count": 16,
    "gs_author_count": 9
  },
  {
    "arxiv_id": "2411.00281",
    "title": "Detection and tracking of gas plumes in LWIR hyperspectral video sequence data",
    "year": 2024,
    "published": "2024-11-01T00:33:29Z",
    "categories": [
      "cs.CV",
      "eess.IV"
    ],
    "abstract": "Automated detection of chemical plumes presents a segmentation challenge. The segmentation problem for gas plumes is difficult due to the diffusive nature of the cloud. The advantage of considering hyperspectral images in the gas plume detection problem over the conventional RGB imagery is the presence of non-visual data, allowing for a richer representation of information. In this paper we present an effective method of visualizing hyperspectral video sequences containing chemical plumes and in",
    "arxiv_url": "https://arxiv.org/abs/2411.00281v1",
    "pdf_url": "https://arxiv.org/pdf/2411.00281v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2411.00281",
    "arxiv_authors": [
      "Torin Gerhart",
      "Justin Sunu",
      "Ekaterina Merkurjev",
      "Jen-Mei Chang",
      "Jerome Gilles",
      "Andrea L. Bertozzi"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Detection+and+tracking+of+gas+plumes+in+LWIR+hyperspectral+video+sequence+data+Torin+Gerhart+Justin+Sunu+Ekaterina+Merkurjev+Jen-Mei+Chang+Jerome+Gilles",
    "gs_search_success": true,
    "gs_authors": [
      "VJPRn1oAAAAJ",
      "m3vHJnAAAAAJ",
      "d7HwNkMAAAAJ",
      "t4E9ft4AAAAJ"
    ],
    "citation_count": 39,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2412.18090",
    "title": "Multi-Point Positional Insertion Tuning for Small Object Detection",
    "year": 2024,
    "published": "2024-12-24T02:04:47Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Small object detection aims to localize and classify small objects within images. With recent advances in large-scale vision-language pretraining, finetuning pretrained object detection models has emerged as a promising approach. However, finetuning large models is computationally and memory expensive. To address this issue, this paper introduces multi-point positional insertion (MPI) tuning, a parameter-efficient finetuning (PEFT) method for small object detection. Specifically, MPI incorporate",
    "arxiv_url": "https://arxiv.org/abs/2412.18090v1",
    "pdf_url": "https://arxiv.org/pdf/2412.18090v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2412.18090",
    "arxiv_authors": [
      "Kanoko Goto",
      "Takumi Karasawa",
      "Takumi Hirose",
      "Rei Kawakami",
      "Nakamasa Inoue"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Multi-Point+Positional+Insertion+Tuning+for+Small+Object+Detection+Kanoko+Goto+Takumi+Karasawa+Takumi+Hirose+Rei+Kawakami+Nakamasa+Inoue",
    "gs_search_success": true,
    "gs_authors": [
      "C6JFJjcAAAAJ",
      "QVOdCi8AAAAJ",
      "vz_Z31sAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2310.12007",
    "title": "KI-PMF: Knowledge Integrated Plausible Motion Forecasting",
    "year": 2023,
    "published": "2023-10-18T14:40:52Z",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV"
    ],
    "abstract": "Accurately forecasting the motion of traffic actors is crucial for the deployment of autonomous vehicles at a large scale. Current trajectory forecasting approaches primarily concentrate on optimizing a loss function with a specific metric, which can result in predictions that do not adhere to physical laws or violate external constraints. Our objective is to incorporate explicit knowledge priors that allow a network to forecast future trajectories in compliance with both the kinematic constrain",
    "arxiv_url": "https://arxiv.org/abs/2310.12007v3",
    "pdf_url": "https://arxiv.org/pdf/2310.12007v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2310.12007",
    "arxiv_authors": [
      "Abhishek Vivekanandan",
      "Ahmed Abouelazm",
      "Philip Schörner",
      "J. Marius Zöllner"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=KI-PMF%3A+Knowledge+Integrated+Plausible+Motion+Forecasting+Abhishek+Vivekanandan+Ahmed+Abouelazm+Philip+Sch%C3%B6rner+J.+Marius+Z%C3%B6llner",
    "gs_search_success": true,
    "gs_authors": [
      "EcKQPtUAAAAJ",
      "xqptEhUAAAAJ",
      "4PRFXzwAAAAJ",
      "FEf_0GMAAAAJ"
    ],
    "citation_count": 4,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2406.04031",
    "title": "Jailbreak Vision Language Models via Bi-Modal Adversarial Prompt",
    "year": 2024,
    "published": "2024-06-06T13:00:42Z",
    "categories": [
      "cs.CV",
      "cs.CR"
    ],
    "abstract": "In the realm of large vision language models (LVLMs), jailbreak attacks serve as a red-teaming approach to bypass guardrails and uncover safety implications. Existing jailbreaks predominantly focus on the visual modality, perturbing solely visual inputs in the prompt for attacks. However, they fall short when confronted with aligned models that fuse visual and textual features simultaneously for generation. To address this limitation, this paper introduces the Bi-Modal Adversarial Prompt Attack ",
    "arxiv_url": "https://arxiv.org/abs/2406.04031v2",
    "pdf_url": "https://arxiv.org/pdf/2406.04031v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2406.04031",
    "arxiv_authors": [
      "Zonghao Ying",
      "Aishan Liu",
      "Tianyuan Zhang",
      "Zhengmin Yu",
      "Siyuan Liang",
      "Xianglong Liu",
      "Dacheng Tao"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Jailbreak+Vision+Language+Models+via+Bi-Modal+Adversarial+Prompt+Zonghao+Ying+Aishan+Liu+Tianyuan+Zhang+Zhengmin+Yu+Siyuan+Liang",
    "gs_search_success": true,
    "gs_authors": [
      "88tzr_sAAAAJ",
      "w0mmwl4AAAAJ",
      "g0-11coAAAAJ",
      "MLE3GekAAAAJ",
      "RwlJNLcAAAAJ",
      "FhPWhckAAAAJ",
      "8VY7ZDcAAAAJ"
    ],
    "citation_count": 66,
    "gs_author_count": 7
  },
  {
    "arxiv_id": "2407.00742",
    "title": "PolygonGNN: Representation Learning for Polygonal Geometries with Heterogeneous Visibility Graph",
    "year": 2024,
    "published": "2024-06-30T16:07:49Z",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "Polygon representation learning is essential for diverse applications, encompassing tasks such as shape coding, building pattern classification, and geographic question answering. While recent years have seen considerable advancements in this field, much of the focus has been on single polygons, overlooking the intricate inner- and inter-polygonal relationships inherent in multipolygons. To address this gap, our study introduces a comprehensive framework specifically designed for learning repres",
    "arxiv_url": "https://arxiv.org/abs/2407.00742v2",
    "pdf_url": "https://arxiv.org/pdf/2407.00742v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2407.00742",
    "arxiv_authors": [
      "Dazhou Yu",
      "Yuntong Hu",
      "Yun Li",
      "Liang Zhao"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=PolygonGNN%3A+Representation+Learning+for+Polygonal+Geometries+with+Heterogeneous+Visibility+Graph+Dazhou+Yu+Yuntong+Hu+Yun+Li+Liang+Zhao",
    "gs_search_success": true,
    "gs_authors": [
      "45jOSjQAAAAJ",
      "qnvyqtwAAAAJ",
      "QeljXxUAAAAJ",
      "qFLHrXwAAAAJ"
    ],
    "citation_count": 22,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2505.06670",
    "title": "Video Dataset Condensation with Diffusion Models",
    "year": 2025,
    "published": "2025-05-10T15:12:19Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "In recent years, the rapid expansion of dataset sizes and the increasing complexity of deep learning models have significantly escalated the demand for computational resources, both for data storage and model training. Dataset distillation has emerged as a promising solution to address this challenge by generating a compact synthetic dataset that retains the essential information from a large real dataset. However, existing methods often suffer from limited performance and poor data quality, par",
    "arxiv_url": "https://arxiv.org/abs/2505.06670v1",
    "pdf_url": "https://arxiv.org/pdf/2505.06670v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2505.06670",
    "arxiv_authors": [
      "Zhe Li",
      "Hadrien Reynaud",
      "Mischa Dombrowski",
      "Sarah Cechnicka",
      "Franciskus Xaverius Erick",
      "Bernhard Kainz"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Video+Dataset+Condensation+with+Diffusion+Models+Zhe+Li+Hadrien+Reynaud+Mischa+Dombrowski+Sarah+Cechnicka+Franciskus+Xaverius+Erick",
    "gs_search_success": true,
    "gs_authors": [
      "dxw23KcAAAAJ",
      "5etzH5oAAAAJ",
      "h1-zIkYAAAAJ",
      "Igxq-YEAAAAJ",
      "jD0_c64AAAAJ",
      "7j-_wgIAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2308.04990",
    "title": "Foreground Object Search by Distilling Composite Image Feature",
    "year": 2023,
    "published": "2023-08-09T14:43:10Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Foreground object search (FOS) aims to find compatible foreground objects for a given background image, producing realistic composite image. We observe that competitive retrieval performance could be achieved by using a discriminator to predict the compatibility of composite image, but this approach has unaffordable time cost. To this end, we propose a novel FOS method via distilling composite feature (DiscoFOS). Specifically, the abovementioned discriminator serves as teacher network. The stude",
    "arxiv_url": "https://arxiv.org/abs/2308.04990v1",
    "pdf_url": "https://arxiv.org/pdf/2308.04990v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2308.04990",
    "arxiv_authors": [
      "Bo Zhang",
      "Jiacheng Sui",
      "Li Niu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Foreground+Object+Search+by+Distilling+Composite+Image+Feature+Bo+Zhang+Jiacheng+Sui+Li+Niu",
    "gs_search_success": true,
    "gs_authors": [
      "OhT3AWMAAAAJ",
      "Yb9AS0cAAAAJ",
      "fXLL3G0AAAAJ"
    ],
    "citation_count": 7,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2505.17684",
    "title": "5G-DIL: Domain Incremental Learning with Similarity-Aware Sampling for Dynamic 5G Indoor Localization",
    "year": 2025,
    "published": "2025-05-23T09:54:58Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Indoor positioning based on 5G data has achieved high accuracy through the adoption of recent machine learning (ML) techniques. However, the performance of learning-based methods degrades significantly when environmental conditions change, thereby hindering their applicability to new scenarios. Acquiring new training data for each environmental change and fine-tuning ML models is both time-consuming and resource-intensive. This paper introduces a domain incremental learning (DIL) approach for dy",
    "arxiv_url": "https://arxiv.org/abs/2505.17684v1",
    "pdf_url": "https://arxiv.org/pdf/2505.17684v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2505.17684",
    "arxiv_authors": [
      "Nisha Lakshmana Raichur",
      "Lucas Heublein",
      "Christopher Mutschler",
      "Felix Ott"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=5G-DIL%3A+Domain+Incremental+Learning+with+Similarity-Aware+Sampling+for+Dynamic+5G+Indoor+Localization+Nisha+Lakshmana+Raichur+Lucas+Heublein+Christopher+Mutschler+Felix+Ott",
    "gs_search_success": true,
    "gs_authors": [
      "8OU4_x8AAAAJ",
      "gKDSp8YAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2408.08184",
    "title": "Not Every Image is Worth a Thousand Words: Quantifying Originality in Stable Diffusion",
    "year": 2024,
    "published": "2024-08-15T14:42:02Z",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "This work addresses the challenge of quantifying originality in text-to-image (T2I) generative diffusion models, with a focus on copyright originality. We begin by evaluating T2I models' ability to innovate and generalize through controlled experiments, revealing that stable diffusion models can effectively recreate unseen elements with sufficiently diverse training data. Then, our key insight is that concepts and combinations of image elements the model is familiar with, and saw more during tra",
    "arxiv_url": "https://arxiv.org/abs/2408.08184v1",
    "pdf_url": "https://arxiv.org/pdf/2408.08184v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2408.08184",
    "arxiv_authors": [
      "Adi Haviv",
      "Shahar Sarfaty",
      "Uri Hacohen",
      "Niva Elkin-Koren",
      "Roi Livni",
      "Amit H Bermano"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Not+Every+Image+is+Worth+a+Thousand+Words%3A+Quantifying+Originality+in+Stable+Diffusion+Adi+Haviv+Shahar+Sarfaty+Uri+Hacohen+Niva+Elkin-Koren+Roi+Livni",
    "gs_search_success": true,
    "gs_authors": [
      "EPO5_f4AAAAJ",
      "tEOvuMMAAAAJ",
      "xhU85M4AAAAJ",
      "J4Kj7EIAAAAJ",
      "nSsDs-4AAAAJ",
      "AaiD8DcAAAAJ"
    ],
    "citation_count": 2,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2504.11195",
    "title": "R-TPT: Improving Adversarial Robustness of Vision-Language Models through Test-Time Prompt Tuning",
    "year": 2025,
    "published": "2025-04-15T13:49:31Z",
    "categories": [
      "cs.LG",
      "cs.CR",
      "cs.CV"
    ],
    "abstract": "Vision-language models (VLMs), such as CLIP, have gained significant popularity as foundation models, with numerous fine-tuning methods developed to enhance performance on downstream tasks. However, due to their inherent vulnerability and the common practice of selecting from a limited set of open-source models, VLMs suffer from a higher risk of adversarial attacks than traditional vision models. Existing defense techniques typically rely on adversarial fine-tuning during training, which require",
    "arxiv_url": "https://arxiv.org/abs/2504.11195v2",
    "pdf_url": "https://arxiv.org/pdf/2504.11195v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2504.11195",
    "arxiv_authors": [
      "Lijun Sheng",
      "Jian Liang",
      "Zilei Wang",
      "Ran He"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=R-TPT%3A+Improving+Adversarial+Robustness+of+Vision-Language+Models+through+Test-Time+Prompt+Tuning+Lijun+Sheng+Jian+Liang+Zilei+Wang+Ran+He",
    "gs_search_success": true,
    "gs_authors": [
      "ayrg9AUAAAAJ",
      "eY8i-mQAAAAJ",
      "1sM6ZrcAAAAJ"
    ],
    "citation_count": 11,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2312.09154",
    "title": "CMG-Net: Robust Normal Estimation for Point Clouds via Chamfer Normal Distance and Multi-scale Geometry",
    "year": 2023,
    "published": "2023-12-14T17:23:16Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "This work presents an accurate and robust method for estimating normals from point clouds. In contrast to predecessor approaches that minimize the deviations between the annotated and the predicted normals directly, leading to direction inconsistency, we first propose a new metric termed Chamfer Normal Distance to address this issue. This not only mitigates the challenge but also facilitates network training and substantially enhances the network robustness against noise. Subsequently, we devise",
    "arxiv_url": "https://arxiv.org/abs/2312.09154v1",
    "pdf_url": "https://arxiv.org/pdf/2312.09154v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2312.09154",
    "arxiv_authors": [
      "Yingrui Wu",
      "Mingyang Zhao",
      "Keqiang Li",
      "Weize Quan",
      "Tianqi Yu",
      "Jianfeng Yang",
      "Xiaohong Jia",
      "Dong-Ming Yan"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=CMG-Net%3A+Robust+Normal+Estimation+for+Point+Clouds+via+Chamfer+Normal+Distance+and+Multi-scale+Geometry+Yingrui+Wu+Mingyang+Zhao+Keqiang+Li+Weize+Quan+Tianqi+Yu",
    "gs_search_success": true,
    "gs_authors": [
      "FS2cmyoAAAAJ",
      "PMR9cooAAAAJ",
      "xAFSO3AAAAAJ",
      "9jrkWzUAAAAJ",
      "SiTCkU4AAAAJ",
      "15kZLwwAAAAJ"
    ],
    "citation_count": 11,
    "gs_author_count": 8
  },
  {
    "arxiv_id": "2407.08711",
    "title": "OmniNOCS: A unified NOCS dataset and model for 3D lifting of 2D objects",
    "year": 2024,
    "published": "2024-07-11T17:49:05Z",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "abstract": "We propose OmniNOCS, a large-scale monocular dataset with 3D Normalized Object Coordinate Space (NOCS) maps, object masks, and 3D bounding box annotations for indoor and outdoor scenes. OmniNOCS has 20 times more object classes and 200 times more instances than existing NOCS datasets (NOCS-Real275, Wild6D). We use OmniNOCS to train a novel, transformer-based monocular NOCS prediction model (NOCSformer) that can predict accurate NOCS, instance masks and poses from 2D object detections across dive",
    "arxiv_url": "https://arxiv.org/abs/2407.08711v1",
    "pdf_url": "https://arxiv.org/pdf/2407.08711v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2407.08711",
    "arxiv_authors": [
      "Akshay Krishnan",
      "Abhijit Kundu",
      "Kevis-Kokitsi Maninis",
      "James Hays",
      "Matthew Brown"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=OmniNOCS%3A+A+unified+NOCS+dataset+and+model+for+3D+lifting+of+2D+objects+Akshay+Krishnan+Abhijit+Kundu+Kevis-Kokitsi+Maninis+James+Hays+Matthew+Brown",
    "gs_search_success": true,
    "gs_authors": [
      "WZfM0qsAAAAJ",
      "vHqNiLkAAAAJ",
      "Lw_-pYsAAAAJ",
      "vjZrDKQAAAAJ",
      "GGcjtCsAAAAJ"
    ],
    "citation_count": 16,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2412.16662",
    "title": "Adversarial Attack Against Images Classification based on Generative Adversarial Networks",
    "year": 2024,
    "published": "2024-12-21T15:23:34Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Adversarial attacks on image classification systems have always been an important problem in the field of machine learning, and generative adversarial networks (GANs), as popular models in the field of image generation, have been widely used in various novel scenarios due to their powerful generative capabilities. However, with the popularity of generative adversarial networks, the misuse of fake image technology has raised a series of security problems, such as malicious tampering with other pe",
    "arxiv_url": "https://arxiv.org/abs/2412.16662v2",
    "pdf_url": "https://arxiv.org/pdf/2412.16662v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2412.16662",
    "arxiv_authors": [
      "Yahe Yang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Adversarial+Attack+Against+Images+Classification+based+on+Generative+Adversarial+Networks+Yahe+Yang",
    "gs_search_success": true,
    "gs_authors": [
      "sA-gK0oAAAAJ"
    ],
    "citation_count": 10,
    "gs_author_count": 1
  },
  {
    "arxiv_id": "2308.11070",
    "title": "Temporal-Distributed Backdoor Attack Against Video Based Action Recognition",
    "year": 2023,
    "published": "2023-08-21T22:31:54Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CR"
    ],
    "abstract": "Deep neural networks (DNNs) have achieved tremendous success in various applications including video action recognition, yet remain vulnerable to backdoor attacks (Trojans). The backdoor-compromised model will mis-classify to the target class chosen by the attacker when a test instance (from a non-target class) is embedded with a specific trigger, while maintaining high accuracy on attack-free instances. Although there are extensive studies on backdoor attacks against image data, the susceptibil",
    "arxiv_url": "https://arxiv.org/abs/2308.11070v3",
    "pdf_url": "https://arxiv.org/pdf/2308.11070v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2308.11070",
    "arxiv_authors": [
      "Xi Li",
      "Songhe Wang",
      "Ruiquan Huang",
      "Mahanth Gowda",
      "George Kesidis"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Temporal-Distributed+Backdoor+Attack+Against+Video+Based+Action+Recognition+Xi+Li+Songhe+Wang+Ruiquan+Huang+Mahanth+Gowda+George+Kesidis",
    "gs_search_success": true,
    "gs_authors": [
      "g-WedwYAAAAJ",
      "n9bDceAAAAAJ",
      "0eo3JGgAAAAJ",
      "R-fOpm4AAAAJ",
      "KbJERYoAAAAJ"
    ],
    "citation_count": 11,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2405.06383",
    "title": "How to Augment for Atmospheric Turbulence Effects on Thermal Adapted Object Detection Models?",
    "year": 2024,
    "published": "2024-05-10T10:44:29Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Atmospheric turbulence poses a significant challenge to the performance of object detection models. Turbulence causes distortions, blurring, and noise in images by bending and scattering light rays due to variations in the refractive index of air. This results in non-rigid geometric distortions and temporal fluctuations in the electromagnetic radiation received by optical systems. This paper explores the effectiveness of turbulence image augmentation techniques in improving the accuracy and robu",
    "arxiv_url": "https://arxiv.org/abs/2405.06383v1",
    "pdf_url": "https://arxiv.org/pdf/2405.06383v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2405.06383",
    "arxiv_authors": [
      "Engin Uzun",
      "Erdem Akagunduz"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=How+to+Augment+for+Atmospheric+Turbulence+Effects+on+Thermal+Adapted+Object+Detection+Models%3F+Engin+Uzun+Erdem+Akagunduz",
    "gs_search_success": true,
    "gs_authors": [
      "kbhNCBkAAAAJ",
      "Xnp8itEAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 2
  },
  {
    "arxiv_id": "2312.08193",
    "title": "Universal Adversarial Framework to Improve Adversarial Robustness for Diabetic Retinopathy Detection",
    "year": 2023,
    "published": "2023-12-13T14:58:17Z",
    "categories": [
      "eess.IV",
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "Diabetic Retinopathy (DR) is a prevalent illness associated with Diabetes which, if left untreated, can result in irreversible blindness. Deep Learning based systems are gradually being introduced as automated support for clinical diagnosis. Since healthcare has always been an extremely important domain demanding error-free performance, any adversaries could pose a big threat to the applicability of such systems. In this work, we use Universal Adversarial Perturbations (UAPs) to quantify the vul",
    "arxiv_url": "https://arxiv.org/abs/2312.08193v1",
    "pdf_url": "https://arxiv.org/pdf/2312.08193v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2312.08193",
    "arxiv_authors": [
      "Samrat Mukherjee",
      "Dibyanayan Bandyopadhyay",
      "Baban Gain",
      "Asif Ekbal"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Universal+Adversarial+Framework+to+Improve+Adversarial+Robustness+for+Diabetic+Retinopathy+Detection+Samrat+Mukherjee+Dibyanayan+Bandyopadhyay+Baban+Gain+Asif+Ekbal",
    "gs_search_success": true,
    "gs_authors": [
      "vI5Y7koAAAAJ",
      "zHF9tloAAAAJ",
      "t_kZ1qsAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2412.19184",
    "title": "Multi-Head Attention Driven Dynamic Visual-Semantic Embedding for Enhanced Image-Text Matching",
    "year": 2024,
    "published": "2024-12-26T11:46:22Z",
    "categories": [
      "cs.CV",
      "cs.CL"
    ],
    "abstract": "With the rapid development of multimodal learning, the image-text matching task, as a bridge connecting vision and language, has become increasingly important. Based on existing research, this study proposes an innovative visual semantic embedding model, Multi-Headed Consensus-Aware Visual-Semantic Embedding (MH-CVSE). This model introduces a multi-head self-attention mechanism based on the consensus-aware visual semantic embedding model (CVSE) to capture information in multiple subspaces in par",
    "arxiv_url": "https://arxiv.org/abs/2412.19184v1",
    "pdf_url": "https://arxiv.org/pdf/2412.19184v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2412.19184",
    "arxiv_authors": [
      "Wenjing Chen"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Multi-Head+Attention+Driven+Dynamic+Visual-Semantic+Embedding+for+Enhanced+Image-Text+Matching+Wenjing+Chen",
    "gs_search_success": true,
    "gs_authors": [],
    "citation_count": 1,
    "gs_author_count": 1
  },
  {
    "arxiv_id": "2401.16329",
    "title": "Synthesis of 3D on-air signatures with the Sigma-Lognormal model",
    "year": 2024,
    "published": "2024-01-29T17:35:19Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Signature synthesis is a computation technique that generates artificial specimens which can support decision making in automatic signature verification. A lot of work has been dedicated to this subject, which centres on synthesizing dynamic and static two-dimensional handwriting on canvas. This paper proposes a framework to generate synthetic 3D on-air signatures exploiting the lognormality principle, which mimics the complex neuromotor control processes at play as the fingertip moves. Addressi",
    "arxiv_url": "https://arxiv.org/abs/2401.16329v1",
    "pdf_url": "https://arxiv.org/pdf/2401.16329v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2401.16329",
    "arxiv_authors": [
      "Miguel A. Ferrer",
      "Moises Diaz",
      "Cristina Carmona-Duarte",
      "Jose J. Quintana Hernandez",
      "Rejean Plamondon"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Synthesis+of+3D+on-air+signatures+with+the+Sigma-Lognormal+model+Miguel+A.+Ferrer+Moises+Diaz+Cristina+Carmona-Duarte+Jose+J.+Quintana+Hernandez+Rejean+Plamondon",
    "gs_search_success": true,
    "gs_authors": [
      "XMsVDiQnzQMC",
      "m8NprB0AAAAJ",
      "486uCVgAAAAJ",
      "Me378dAAAAAJ",
      "-3NUmvIAAAAJ"
    ],
    "citation_count": 8,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2406.12258",
    "title": "Advancing Cross-Domain Generalizability in Face Anti-Spoofing: Insights, Design, and Metrics",
    "year": 2024,
    "published": "2024-06-18T04:15:22Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "This paper presents a novel perspective for enhancing anti-spoofing performance in zero-shot data domain generalization. Unlike traditional image classification tasks, face anti-spoofing datasets display unique generalization characteristics, necessitating novel zero-shot data domain generalization. One step forward to the previous frame-wise spoofing prediction, we introduce a nuanced metric calculation that aggregates frame-level probabilities for a video-wise prediction, to tackle the gap bet",
    "arxiv_url": "https://arxiv.org/abs/2406.12258v1",
    "pdf_url": "https://arxiv.org/pdf/2406.12258v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2406.12258",
    "arxiv_authors": [
      "Hyojin Kim",
      "Jiyoon Lee",
      "Yonghyun Jeong",
      "Haneol Jang",
      "YoungJoon Yoo"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Advancing+Cross-Domain+Generalizability+in+Face+Anti-Spoofing%3A+Insights%2C+Design%2C+and+Metrics+Hyojin+Kim+Jiyoon+Lee+Yonghyun+Jeong+Haneol+Jang+YoungJoon+Yoo",
    "gs_search_success": true,
    "gs_authors": [
      "e9pHCjUAAAAJ",
      "YGVqRuIAAAAJ",
      "3uQQYPEAAAAJ",
      "IbdJ1ekAAAAJ"
    ],
    "citation_count": 4,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2408.15355",
    "title": "Optimizing Lung Cancer Detection in CT Imaging: A Wavelet Multi-Layer Perceptron (WMLP) Approach Enhanced by Dragonfly Algorithm (DA)",
    "year": 2024,
    "published": "2024-08-27T18:27:47Z",
    "categories": [
      "eess.IV",
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "Lung cancer stands as the preeminent cause of cancer-related mortality globally. Prompt and precise diagnosis, coupled with effective treatment, is imperative to reduce the fatality rates associated with this formidable disease. This study introduces a cutting-edge deep learning framework for the classification of lung cancer from CT scan imagery. The research encompasses a suite of image pre-processing strategies, notably Canny edge detection, and wavelet transformations, which precede the extr",
    "arxiv_url": "https://arxiv.org/abs/2408.15355v1",
    "pdf_url": "https://arxiv.org/pdf/2408.15355v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2408.15355",
    "arxiv_authors": [
      "Bitasadat Jamshidi",
      "Nastaran Ghorbani",
      "Mohsen Rostamy-Malkhalifeh"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Optimizing+Lung+Cancer+Detection+in+CT+Imaging%3A+A+Wavelet+Multi-Layer+Perceptron+%28WMLP%29+Approach+Enhanced+by+Dragonfly+Algorithm+%28DA%29+Bitasadat+Jamshidi+Nastaran+Ghorbani+Mohsen+Rostamy-Malkhalifeh",
    "gs_search_success": true,
    "gs_authors": [
      "96zNa3oAAAAJ",
      "T_5sNDsAAAAJ",
      "4-yJ9ZsAAAAJ"
    ],
    "citation_count": 10,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2409.10921",
    "title": "KALE: An Artwork Image Captioning System Augmented with Heterogeneous Graph",
    "year": 2024,
    "published": "2024-09-17T06:39:18Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Exploring the narratives conveyed by fine-art paintings is a challenge in image captioning, where the goal is to generate descriptions that not only precisely represent the visual content but also offer a in-depth interpretation of the artwork's meaning. The task is particularly complex for artwork images due to their diverse interpretations and varied aesthetic principles across different artistic schools and styles. In response to this, we present KALE Knowledge-Augmented vision-Language model",
    "arxiv_url": "https://arxiv.org/abs/2409.10921v1",
    "pdf_url": "https://arxiv.org/pdf/2409.10921v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2409.10921",
    "arxiv_authors": [
      "Yanbei Jiang",
      "Krista A. Ehinger",
      "Jey Han Lau"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=KALE%3A+An+Artwork+Image+Captioning+System+Augmented+with+Heterogeneous+Graph+Yanbei+Jiang+Krista+A.+Ehinger+Jey+Han+Lau",
    "gs_search_success": true,
    "gs_authors": [
      "-JHMVhQAAAAJ",
      "MFi65f4AAAAJ",
      "EdGfpdcAAAAJ"
    ],
    "citation_count": 8,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2310.16640",
    "title": "EmoCLIP: A Vision-Language Method for Zero-Shot Video Facial Expression Recognition",
    "year": 2023,
    "published": "2023-10-25T13:43:36Z",
    "categories": [
      "cs.CV",
      "cs.HC"
    ],
    "abstract": "Facial Expression Recognition (FER) is a crucial task in affective computing, but its conventional focus on the seven basic emotions limits its applicability to the complex and expanding emotional spectrum. To address the issue of new and unseen emotions present in dynamic in-the-wild FER, we propose a novel vision-language model that utilises sample-level text descriptions (i.e. captions of the context, expressions or emotional cues) as natural language supervision, aiming to enhance the learni",
    "arxiv_url": "https://arxiv.org/abs/2310.16640v2",
    "pdf_url": "https://arxiv.org/pdf/2310.16640v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2310.16640",
    "arxiv_authors": [
      "Niki Maria Foteinopoulou",
      "Ioannis Patras"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=EmoCLIP%3A+A+Vision-Language+Method+for+Zero-Shot+Video+Facial+Expression+Recognition+Niki+Maria+Foteinopoulou+Ioannis+Patras",
    "gs_search_success": true,
    "gs_authors": [
      "R9DTsJEAAAAJ",
      "OBYLxRkAAAAJ"
    ],
    "citation_count": 47,
    "gs_author_count": 2
  },
  {
    "arxiv_id": "2302.04308",
    "title": "Enhancing Modality-Agnostic Representations via Meta-Learning for Brain Tumor Segmentation",
    "year": 2023,
    "published": "2023-02-08T19:53:07Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "In medical vision, different imaging modalities provide complementary information. However, in practice, not all modalities may be available during inference or even training. Previous approaches, e.g., knowledge distillation or image synthesis, often assume the availability of full modalities for all patients during training; this is unrealistic and impractical due to the variability in data collection across sites. We propose a novel approach to learn enhanced modality-agnostic representations",
    "arxiv_url": "https://arxiv.org/abs/2302.04308v2",
    "pdf_url": "https://arxiv.org/pdf/2302.04308v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2302.04308",
    "arxiv_authors": [
      "Aishik Konwer",
      "Xiaoling Hu",
      "Joseph Bae",
      "Xuan Xu",
      "Chao Chen",
      "Prateek Prasanna"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Enhancing+Modality-Agnostic+Representations+via+Meta-Learning+for+Brain+Tumor+Segmentation+Aishik+Konwer+Xiaoling+Hu+Joseph+Bae+Xuan+Xu+Chao+Chen",
    "gs_search_success": true,
    "gs_authors": [
      "ByXvf-YAAAAJ",
      "2yVlgVcAAAAJ",
      "Vtq1xfgAAAAJ",
      "6MfwhCAAAAAJ",
      "J-iIIFAAAAAJ",
      "uyA1Q18AAAAJ"
    ],
    "citation_count": 29,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2409.05279",
    "title": "BrainDecoder: Style-Based Visual Decoding of EEG Signals",
    "year": 2024,
    "published": "2024-09-09T02:14:23Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Decoding neural representations of visual stimuli from electroencephalography (EEG) offers valuable insights into brain activity and cognition. Recent advancements in deep learning have significantly enhanced the field of visual decoding of EEG, primarily focusing on reconstructing the semantic content of visual stimuli. In this paper, we present a novel visual decoding pipeline that, in addition to recovering the content, emphasizes the reconstruction of the style, such as color and texture, of",
    "arxiv_url": "https://arxiv.org/abs/2409.05279v1",
    "pdf_url": "https://arxiv.org/pdf/2409.05279v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2409.05279",
    "arxiv_authors": [
      "Minsuk Choi",
      "Hiroshi Ishikawa"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=BrainDecoder%3A+Style-Based+Visual+Decoding+of+EEG+Signals+Minsuk+Choi+Hiroshi+Ishikawa",
    "gs_search_success": true,
    "gs_authors": [],
    "citation_count": 6,
    "gs_author_count": 2
  },
  {
    "arxiv_id": "2309.06199",
    "title": "SCP: Scene Completion Pre-training for 3D Object Detection",
    "year": 2023,
    "published": "2023-09-12T13:08:46Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "3D object detection using LiDAR point clouds is a fundamental task in the fields of computer vision, robotics, and autonomous driving. However, existing 3D detectors heavily rely on annotated datasets, which are both time-consuming and prone to errors during the process of labeling 3D bounding boxes. In this paper, we propose a Scene Completion Pre-training (SCP) method to enhance the performance of 3D object detectors with less labeled data. SCP offers three key advantages: (1) Improved initial",
    "arxiv_url": "https://arxiv.org/abs/2309.06199v1",
    "pdf_url": "https://arxiv.org/pdf/2309.06199v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2309.06199",
    "arxiv_authors": [
      "Yiming Shan",
      "Yan Xia",
      "Yuhong Chen",
      "Daniel Cremers"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=SCP%3A+Scene+Completion+Pre-training+for+3D+Object+Detection+Yiming+Shan+Yan+Xia+Yuhong+Chen+Daniel+Cremers",
    "gs_search_success": true,
    "gs_authors": [
      "xkBn4mMAAAAJ",
      "9x_u3T0AAAAJ",
      "cXQciMEAAAAJ"
    ],
    "citation_count": 4,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2505.19659",
    "title": "LangDAug: Langevin Data Augmentation for Multi-Source Domain Generalization in Medical Image Segmentation",
    "year": 2025,
    "published": "2025-05-26T08:18:32Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Medical image segmentation models often struggle to generalize across different domains due to various reasons. Domain Generalization (DG) methods overcome this either through representation learning or data augmentation (DAug). While representation learning methods seek domain-invariant features, they often rely on ad-hoc techniques and lack formal guarantees. DAug methods, which enrich model representations through synthetic samples, have shown comparable or superior performance to representat",
    "arxiv_url": "https://arxiv.org/abs/2505.19659v1",
    "pdf_url": "https://arxiv.org/pdf/2505.19659v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2505.19659",
    "arxiv_authors": [
      "Piyush Tiwary",
      "Kinjawl Bhattacharyya",
      "Prathosh A. P"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=LangDAug%3A+Langevin+Data+Augmentation+for+Multi-Source+Domain+Generalization+in+Medical+Image+Segmentation+Piyush+Tiwary+Kinjawl+Bhattacharyya+Prathosh+A.+P",
    "gs_search_success": true,
    "gs_authors": [
      "Uc__Y2cAAAAJ",
      "tUdHYloAAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2307.02010",
    "title": "ZJU ReLER Submission for EPIC-KITCHEN Challenge 2023: Semi-Supervised Video Object Segmentation",
    "year": 2023,
    "published": "2023-07-05T03:43:15Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "The Associating Objects with Transformers (AOT) framework has exhibited exceptional performance in a wide range of complex scenarios for video object segmentation. In this study, we introduce MSDeAOT, a variant of the AOT series that incorporates transformers at multiple feature scales. Leveraging the hierarchical Gated Propagation Module (GPM), MSDeAOT efficiently propagates object masks from previous frames to the current frame using a feature scale with a stride of 16. Additionally, we employ",
    "arxiv_url": "https://arxiv.org/abs/2307.02010v2",
    "pdf_url": "https://arxiv.org/pdf/2307.02010v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2307.02010",
    "arxiv_authors": [
      "Jiahao Li",
      "Yuanyou Xu",
      "Zongxin Yang",
      "Yi Yang",
      "Yueting Zhuang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=ZJU+ReLER+Submission+for+EPIC-KITCHEN+Challenge+2023%3A+Semi-Supervised+Video+Object+Segmentation+Jiahao+Li+Yuanyou+Xu+Zongxin+Yang+Yi+Yang+Yueting+Zhuang",
    "gs_search_success": true,
    "gs_authors": [
      "RMSuNFwAAAAJ",
      "8IE0CfwAAAAJ",
      "KYpxwREAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2405.17677",
    "title": "Understanding differences in applying DETR to natural and medical images",
    "year": 2024,
    "published": "2024-05-27T22:06:42Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Transformer-based detectors have shown success in computer vision tasks with natural images. These models, exemplified by the Deformable DETR, are optimized through complex engineering strategies tailored to the typical characteristics of natural scenes. However, medical imaging data presents unique challenges such as extremely large image sizes, fewer and smaller regions of interest, and object classes which can be differentiated only through subtle differences. This study evaluates the applica",
    "arxiv_url": "https://arxiv.org/abs/2405.17677v2",
    "pdf_url": "https://arxiv.org/pdf/2405.17677v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2405.17677",
    "arxiv_authors": [
      "Yanqi Xu",
      "Yiqiu Shen",
      "Carlos Fernandez-Granda",
      "Laura Heacock",
      "Krzysztof J. Geras"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Understanding+differences+in+applying+DETR+to+natural+and+medical+images+Yanqi+Xu+Yiqiu+Shen+Carlos+Fernandez-Granda+Laura+Heacock+Krzysztof+J.+Geras",
    "gs_search_success": true,
    "gs_authors": [
      "tYYM5IkAAAAJ",
      "ZMCGp48AAAAJ",
      "XaeN2zgAAAAJ",
      "GX-PtukAAAAJ",
      "41Rlk4IAAAAJ"
    ],
    "citation_count": 8,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2307.03449",
    "title": "Universal Semi-supervised Model Adaptation via Collaborative Consistency Training",
    "year": 2023,
    "published": "2023-07-07T08:19:40Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "In this paper, we introduce a realistic and challenging domain adaptation problem called Universal Semi-supervised Model Adaptation (USMA), which i) requires only a pre-trained source model, ii) allows the source and target domain to have different label sets, i.e., they share a common label set and hold their own private label set, and iii) requires only a few labeled samples in each class of the target domain. To address USMA, we propose a collaborative consistency training framework that regu",
    "arxiv_url": "https://arxiv.org/abs/2307.03449v2",
    "pdf_url": "https://arxiv.org/pdf/2307.03449v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2307.03449",
    "arxiv_authors": [
      "Zizheng Yan",
      "Yushuang Wu",
      "Yipeng Qin",
      "Xiaoguang Han",
      "Shuguang Cui",
      "Guanbin Li"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Universal+Semi-supervised+Model+Adaptation+via+Collaborative+Consistency+Training+Zizheng+Yan+Yushuang+Wu+Yipeng+Qin+Xiaoguang+Han+Shuguang+Cui",
    "gs_search_success": true,
    "gs_authors": [
      "Dt5LAqcAAAAJ",
      "ojgWPpgAAAAJ",
      "z-rqsR4AAAAJ",
      "1o_qvR0AAAAJ",
      "2A2Bx2UAAAAJ",
      "x5gpN0sAAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2504.08269",
    "title": "VLMT: Vision-Language Multimodal Transformer for Multimodal Multi-hop Question Answering",
    "year": 2025,
    "published": "2025-04-11T05:51:44Z",
    "categories": [
      "cs.CV",
      "cs.CL"
    ],
    "abstract": "The increasing availability of multimodal data across text, tables, and images presents new challenges for developing models capable of complex cross-modal reasoning. Existing methods for Multimodal Multi-hop Question Answering (MMQA) often suffer from limited reasoning capabilities, reliance on modality conversion, and inadequate alignment between visual and textual representations. To address these limitations, this paper introduces Vision-Language Multimodal Transformer (VLMT), a unified arch",
    "arxiv_url": "https://arxiv.org/abs/2504.08269v1",
    "pdf_url": "https://arxiv.org/pdf/2504.08269v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2504.08269",
    "arxiv_authors": [
      "Qi Zhi Lim",
      "Chin Poo Lee",
      "Kian Ming Lim",
      "Kalaiarasi Sonai Muthu Anbananthen"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=VLMT%3A+Vision-Language+Multimodal+Transformer+for+Multimodal+Multi-hop+Question+Answering+Qi+Zhi+Lim+Chin+Poo+Lee+Kian+Ming+Lim+Kalaiarasi+Sonai+Muthu+Anbananthen",
    "gs_search_success": true,
    "gs_authors": [
      "J0v1SskAAAAJ",
      "-ANzf1gAAAAJ"
    ],
    "citation_count": 3,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2305.14298",
    "title": "MOTRv3: Release-Fetch Supervision for End-to-End Multi-Object Tracking",
    "year": 2023,
    "published": "2023-05-23T17:40:13Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Although end-to-end multi-object trackers like MOTR enjoy the merits of simplicity, they suffer from the conflict between detection and association seriously, resulting in unsatisfactory convergence dynamics. While MOTRv2 partly addresses this problem, it demands an additional detection network for assistance. In this work, we serve as the first to reveal that this conflict arises from the unfair label assignment between detect queries and track queries during training, where these detect querie",
    "arxiv_url": "https://arxiv.org/abs/2305.14298v1",
    "pdf_url": "https://arxiv.org/pdf/2305.14298v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2305.14298",
    "arxiv_authors": [
      "En Yu",
      "Tiancai Wang",
      "Zhuoling Li",
      "Yuang Zhang",
      "Xiangyu Zhang",
      "Wenbing Tao"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=MOTRv3%3A+Release-Fetch+Supervision+for+End-to-End+Multi-Object+Tracking+En+Yu+Tiancai+Wang+Zhuoling+Li+Yuang+Zhang+Xiangyu+Zhang",
    "gs_search_success": true,
    "gs_authors": [
      "rWCQMNgAAAAJ",
      "yuB-cfoAAAAJ",
      "2r6ejykAAAAJ",
      "jRDPE2AAAAAJ",
      "YI0sRroAAAAJ",
      "pP5WG9wAAAAJ"
    ],
    "citation_count": 61,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2310.19405",
    "title": "Radar-Lidar Fusion for Object Detection by Designing Effective Convolution Networks",
    "year": 2023,
    "published": "2023-10-30T10:18:40Z",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "abstract": "Object detection is a core component of perception systems, providing the ego vehicle with information about its surroundings to ensure safe route planning. While cameras and Lidar have significantly advanced perception systems, their performance can be limited in adverse weather conditions. In contrast, millimeter-wave technology enables radars to function effectively in such conditions. However, relying solely on radar for building a perception system doesn't fully capture the environment due ",
    "arxiv_url": "https://arxiv.org/abs/2310.19405v1",
    "pdf_url": "https://arxiv.org/pdf/2310.19405v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2310.19405",
    "arxiv_authors": [
      "Farzeen Munir",
      "Shoaib Azam",
      "Tomasz Kucner",
      "Ville Kyrki",
      "Moongu Jeon"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Radar-Lidar+Fusion+for+Object+Detection+by+Designing+Effective+Convolution+Networks+Farzeen+Munir+Shoaib+Azam+Tomasz+Kucner+Ville+Kyrki+Moongu+Jeon",
    "gs_search_success": true,
    "gs_authors": [
      "xl0DHsMAAAAJ",
      "8OBnyXQAAAAJ",
      "zfngGSkAAAAJ",
      "7twUsBMAAAAJ",
      "ByjVNHoAAAAJ"
    ],
    "citation_count": 5,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2310.19119",
    "title": "Out-of-distribution Object Detection through Bayesian Uncertainty Estimation",
    "year": 2023,
    "published": "2023-10-29T19:10:52Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "The superior performance of object detectors is often established under the condition that the test samples are in the same distribution as the training data. However, in many practical applications, out-of-distribution (OOD) instances are inevitable and usually lead to uncertainty in the results. In this paper, we propose a novel, intuitive, and scalable probabilistic object detection method for OOD detection. Unlike other uncertainty-modeling methods that either require huge computational cost",
    "arxiv_url": "https://arxiv.org/abs/2310.19119v1",
    "pdf_url": "https://arxiv.org/pdf/2310.19119v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2310.19119",
    "arxiv_authors": [
      "Tianhao Zhang",
      "Shenglin Wang",
      "Nidhal Bouaynaya",
      "Radu Calinescu",
      "Lyudmila Mihaylova"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Out-of-distribution+Object+Detection+through+Bayesian+Uncertainty+Estimation+Tianhao+Zhang+Shenglin+Wang+Nidhal+Bouaynaya+Radu+Calinescu+Lyudmila+Mihaylova",
    "gs_search_success": true,
    "gs_authors": [
      "hYUmZEoAAAAJ",
      "NLiNqPwAAAAJ",
      "Cja4jlYAAAAJ",
      "-tI15hMAAAAJ",
      "mDBtQggAAAAJ"
    ],
    "citation_count": 3,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2311.02559",
    "title": "Rotation Invariant Transformer for Recognizing Object in UAVs",
    "year": 2023,
    "published": "2023-11-05T03:55:08Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Recognizing a target of interest from the UAVs is much more challenging than the existing object re-identification tasks across multiple city cameras. The images taken by the UAVs usually suffer from significant size difference when generating the object bounding boxes and uncertain rotation variations. Existing methods are usually designed for city cameras, incapable of handing the rotation issue in UAV scenarios. A straightforward solution is to perform the image-level rotation augmentation, b",
    "arxiv_url": "https://arxiv.org/abs/2311.02559v1",
    "pdf_url": "https://arxiv.org/pdf/2311.02559v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2311.02559",
    "arxiv_authors": [
      "Shuoyi Chen",
      "Mang Ye",
      "Bo Du"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Rotation+Invariant+Transformer+for+Recognizing+Object+in+UAVs+Shuoyi+Chen+Mang+Ye+Bo+Du",
    "gs_search_success": true,
    "gs_authors": [
      "B4C804oAAAAJ",
      "Shy1gnMAAAAJ",
      "j-HxRy0AAAAJ"
    ],
    "citation_count": 36,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2407.12511",
    "title": "Fast Context-Based Low-Light Image Enhancement via Neural Implicit Representations",
    "year": 2024,
    "published": "2024-07-17T11:51:52Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Current deep learning-based low-light image enhancement methods often struggle with high-resolution images, and fail to meet the practical demands of visual perception across diverse and unseen scenarios. In this paper, we introduce a novel approach termed CoLIE, which redefines the enhancement process through mapping the 2D coordinates of an underexposed image to its illumination component, conditioned on local context. We propose a reconstruction of enhanced-light images within the HSV space u",
    "arxiv_url": "https://arxiv.org/abs/2407.12511v1",
    "pdf_url": "https://arxiv.org/pdf/2407.12511v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2407.12511",
    "arxiv_authors": [
      "Tomáš Chobola",
      "Yu Liu",
      "Hanyi Zhang",
      "Julia A. Schnabel",
      "Tingying Peng"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Fast+Context-Based+Low-Light+Image+Enhancement+via+Neural+Implicit+Representations+Tom%C3%A1%C5%A1+Chobola+Yu+Liu+Hanyi+Zhang+Julia+A.+Schnabel+Tingying+Peng",
    "gs_search_success": true,
    "gs_authors": [
      "jUiKc6QAAAAJ",
      "FPykfZ0AAAAJ",
      "7j2-eIIAAAAJ",
      "KoL2wdQAAAAJ",
      "ZE_mde0AAAAJ"
    ],
    "citation_count": 30,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2306.09615",
    "title": "EVOPOSE: A Recursive Transformer For 3D Human Pose Estimation With Kinematic Structure Priors",
    "year": 2023,
    "published": "2023-06-16T04:09:16Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Transformer is popular in recent 3D human pose estimation, which utilizes long-term modeling to lift 2D keypoints into the 3D space. However, current transformer-based methods do not fully exploit the prior knowledge of the human skeleton provided by the kinematic structure. In this paper, we propose a novel transformer-based model EvoPose to introduce the human body prior knowledge for 3D human pose estimation effectively. Specifically, a Structural Priors Representation (SPR) module represents",
    "arxiv_url": "https://arxiv.org/abs/2306.09615v1",
    "pdf_url": "https://arxiv.org/pdf/2306.09615v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2306.09615",
    "arxiv_authors": [
      "Yaqi Zhang",
      "Yan Lu",
      "Bin Liu",
      "Zhiwei Zhao",
      "Qi Chu",
      "Nenghai Yu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=EVOPOSE%3A+A+Recursive+Transformer+For+3D+Human+Pose+Estimation+With+Kinematic+Structure+Priors+Yaqi+Zhang+Yan+Lu+Bin+Liu+Zhiwei+Zhao+Qi+Chu",
    "gs_search_success": true,
    "gs_authors": [
      "7620QAMAAAAJ",
      "V4lj1MkAAAAJ",
      "kReWULQAAAAJ",
      "JZjOMdsAAAAJ",
      "YbbHudwAAAAJ"
    ],
    "citation_count": 5,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2503.00746",
    "title": "DoF-Gaussian: Controllable Depth-of-Field for 3D Gaussian Splatting",
    "year": 2025,
    "published": "2025-03-02T05:57:57Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Recent advances in 3D Gaussian Splatting (3D-GS) have shown remarkable success in representing 3D scenes and generating high-quality, novel views in real-time. However, 3D-GS and its variants assume that input images are captured based on pinhole imaging and are fully in focus. This assumption limits their applicability, as real-world images often feature shallow depth-of-field (DoF). In this paper, we introduce DoF-Gaussian, a controllable depth-of-field method for 3D-GS. We develop a lens-base",
    "arxiv_url": "https://arxiv.org/abs/2503.00746v3",
    "pdf_url": "https://arxiv.org/pdf/2503.00746v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2503.00746",
    "arxiv_authors": [
      "Liao Shen",
      "Tianqi Liu",
      "Huiqiang Sun",
      "Jiaqi Li",
      "Zhiguo Cao",
      "Wei Li",
      "Chen Change Loy"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=DoF-Gaussian%3A+Controllable+Depth-of-Field+for+3D+Gaussian+Splatting+Liao+Shen+Tianqi+Liu+Huiqiang+Sun+Jiaqi+Li+Zhiguo+Cao",
    "gs_search_success": true,
    "gs_authors": [
      "mY2Qc7YAAAAJ",
      "396o2BAAAAAJ",
      "i-2ghuYAAAAJ",
      "CafUdpEAAAAJ",
      "bEi3j64AAAAJ",
      "41KAd6AAAAAJ",
      "559LF80AAAAJ"
    ],
    "citation_count": 4,
    "gs_author_count": 7
  },
  {
    "arxiv_id": "2506.00568",
    "title": "CReFT-CAD: Boosting Orthographic Projection Reasoning for CAD via Reinforcement Fine-Tuning",
    "year": 2025,
    "published": "2025-05-31T13:52:56Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Computer-Aided Design (CAD) plays a pivotal role in industrial manufacturing. Orthographic projection reasoning underpins the entire CAD workflow, encompassing design, manufacturing, and simulation. However, prevailing deep-learning approaches employ standard 3D reconstruction pipelines as an alternative, which often introduce imprecise dimensions and limit the parametric editability required for CAD workflows. Recently, some researchers adopt vision-language models (VLMs), particularly supervis",
    "arxiv_url": "https://arxiv.org/abs/2506.00568v2",
    "pdf_url": "https://arxiv.org/pdf/2506.00568v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2506.00568",
    "arxiv_authors": [
      "Ke Niu",
      "Zhuofan Chen",
      "Haiyang Yu",
      "Yuwen Chen",
      "Teng Fu",
      "Mengyang Zhao",
      "Bin Li",
      "Xiangyang Xue"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=CReFT-CAD%3A+Boosting+Orthographic+Projection+Reasoning+for+CAD+via+Reinforcement+Fine-Tuning+Ke+Niu+Zhuofan+Chen+Haiyang+Yu+Yuwen+Chen+Teng+Fu",
    "gs_search_success": true,
    "gs_authors": [
      "8t97oL8AAAAJ",
      "UdARxSMAAAAJ",
      "pmM1n58AAAAJ",
      "n9VHywgAAAAJ",
      "DTbhX6oAAAAJ",
      "SdWMyyEAAAAJ",
      "T_xCF4YAAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 8
  },
  {
    "arxiv_id": "2505.06991",
    "title": "Technical Report for ICRA 2025 GOOSE 2D Semantic Segmentation Challenge: Leveraging Color Shift Correction, RoPE-Swin Backbone, and Quantile-based Label Denoising Strategy for Robust Outdoor Scene Understanding",
    "year": 2025,
    "published": "2025-05-11T14:35:06Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "This report presents our semantic segmentation framework developed by team ACVLAB for the ICRA 2025 GOOSE 2D Semantic Segmentation Challenge, which focuses on parsing outdoor scenes into nine semantic categories under real-world conditions. Our method integrates a Swin Transformer backbone enhanced with Rotary Position Embedding (RoPE) for improved spatial generalization, alongside a Color Shift Estimation-and-Correction module designed to compensate for illumination inconsistencies in natural e",
    "arxiv_url": "https://arxiv.org/abs/2505.06991v1",
    "pdf_url": "https://arxiv.org/pdf/2505.06991v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2505.06991",
    "arxiv_authors": [
      "Chih-Chung Hsu",
      "I-Hsuan Wu",
      "Wen-Hai Tseng",
      "Ching-Heng Cheng",
      "Ming-Hsuan Wu",
      "Jin-Hui Jiang",
      "Yu-Jou Hsiao"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Technical+Report+for+ICRA+2025+GOOSE+2D+Semantic+Segmentation+Challenge%3A+Leveraging+Color+Shift+Correction%2C+RoPE-Swin+Backbone%2C+and+Quantile-based+Label+Denoising+Strategy+for+Robust+Outdoor+Scene+Understanding+Chih-Chung+Hsu+I-Hsuan+Wu+Wen-Hai+Tseng+Ching-Heng+Cheng+Ming-Hsuan+Wu",
    "gs_search_success": true,
    "gs_authors": [
      "mIWRYc4AAAAJ",
      "2UmoEfcAAAAJ",
      "P5UkX0sAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 7
  },
  {
    "arxiv_id": "2403.14966",
    "title": "DreamFlow: High-Quality Text-to-3D Generation by Approximating Probability Flow",
    "year": 2024,
    "published": "2024-03-22T05:38:15Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Recent progress in text-to-3D generation has been achieved through the utilization of score distillation methods: they make use of the pre-trained text-to-image (T2I) diffusion models by distilling via the diffusion model training objective. However, such an approach inevitably results in the use of random timesteps at each update, which increases the variance of the gradient and ultimately prolongs the optimization process. In this paper, we propose to enhance the text-to-3D optimization by lev",
    "arxiv_url": "https://arxiv.org/abs/2403.14966v1",
    "pdf_url": "https://arxiv.org/pdf/2403.14966v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2403.14966",
    "arxiv_authors": [
      "Kyungmin Lee",
      "Kihyuk Sohn",
      "Jinwoo Shin"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=DreamFlow%3A+High-Quality+Text-to-3D+Generation+by+Approximating+Probability+Flow+Kyungmin+Lee+Kihyuk+Sohn+Jinwoo+Shin",
    "gs_search_success": true,
    "gs_authors": [
      "VxpypngAAAAJ",
      "m3eDp7kAAAAJ",
      "6dpime0AAAAJ"
    ],
    "citation_count": 25,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2406.03150",
    "title": "Sample-specific Masks for Visual Reprogramming-based Prompting",
    "year": 2024,
    "published": "2024-06-05T11:15:43Z",
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "abstract": "Visual reprogramming (VR) is a prompting technique that aims to re-purpose a pre-trained model (e.g., a classifier on ImageNet) to target tasks (e.g., medical data prediction) by learning a small-scale pattern added into input images instead of tuning considerable parameters within the model. The location of the pattern within input samples is usually determined by a pre-defined mask shared across all samples. In this paper, we show that the shared mask potentially limits VR's generalization and",
    "arxiv_url": "https://arxiv.org/abs/2406.03150v1",
    "pdf_url": "https://arxiv.org/pdf/2406.03150v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2406.03150",
    "arxiv_authors": [
      "Chengyi Cai",
      "Zesheng Ye",
      "Lei Feng",
      "Jianzhong Qi",
      "Feng Liu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Sample-specific+Masks+for+Visual+Reprogramming-based+Prompting+Chengyi+Cai+Zesheng+Ye+Lei+Feng+Jianzhong+Qi+Feng+Liu",
    "gs_search_success": true,
    "gs_authors": [
      "KomQOFkAAAAJ",
      "wNTF8zkAAAAJ",
      "mxS6eHYAAAAJ",
      "ogCTxwczFJYC",
      "eqe3JS8AAAAJ"
    ],
    "citation_count": 15,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2407.03036",
    "title": "SAFT: Towards Out-of-Distribution Generalization in Fine-Tuning",
    "year": 2024,
    "published": "2024-07-03T11:56:55Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Handling distribution shifts from training data, known as out-of-distribution (OOD) generalization, poses a significant challenge in the field of machine learning. While a pre-trained vision-language model like CLIP has demonstrated remarkable zero-shot performance, further adaptation of the model to downstream tasks leads to undesirable degradation for OOD data. In this work, we introduce Sparse Adaptation for Fine-Tuning (SAFT), a method that prevents fine-tuning from forgetting the general kn",
    "arxiv_url": "https://arxiv.org/abs/2407.03036v1",
    "pdf_url": "https://arxiv.org/pdf/2407.03036v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2407.03036",
    "arxiv_authors": [
      "Bac Nguyen",
      "Stefan Uhlich",
      "Fabien Cardinaux",
      "Lukas Mauch",
      "Marzieh Edraki",
      "Aaron Courville"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=SAFT%3A+Towards+Out-of-Distribution+Generalization+in+Fine-Tuning+Bac+Nguyen+Stefan+Uhlich+Fabien+Cardinaux+Lukas+Mauch+Marzieh+Edraki",
    "gs_search_success": true,
    "gs_authors": [
      "hja8ejYAAAAJ",
      "B6wophkAAAAJ",
      "ivJ6Tf8AAAAJ",
      "BRGBjOwAAAAJ",
      "km6CP8cAAAAJ",
      "UFl8n4gAAAAJ"
    ],
    "citation_count": 4,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2309.06751",
    "title": "Remote Sensing Object Detection Meets Deep Learning: A Meta-review of Challenges and Advances",
    "year": 2023,
    "published": "2023-09-13T06:48:32Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Remote sensing object detection (RSOD), one of the most fundamental and challenging tasks in the remote sensing field, has received longstanding attention. In recent years, deep learning techniques have demonstrated robust feature representation capabilities and led to a big leap in the development of RSOD techniques. In this era of rapid technical evolution, this review aims to present a comprehensive review of the recent achievements in deep learning based RSOD methods. More than 300 papers ar",
    "arxiv_url": "https://arxiv.org/abs/2309.06751v1",
    "pdf_url": "https://arxiv.org/pdf/2309.06751v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2309.06751",
    "arxiv_authors": [
      "Xiangrong Zhang",
      "Tianyang Zhang",
      "Guanchun Wang",
      "Peng Zhu",
      "Xu Tang",
      "Xiuping Jia",
      "Licheng Jiao"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Remote+Sensing+Object+Detection+Meets+Deep+Learning%3A+A+Meta-review+of+Challenges+and+Advances+Xiangrong+Zhang+Tianyang+Zhang+Guanchun+Wang+Peng+Zhu+Xu+Tang",
    "gs_search_success": true,
    "gs_authors": [
      "FZbrL2YAAAAJ",
      "4pHdDZ0AAAAJ",
      "G6AdRfwAAAAJ",
      "-vl0ZSEAAAAJ",
      "lmskeBMAAAAJ",
      "fD5SuyoAAAAJ",
      "udrTSCkAAAAJ"
    ],
    "citation_count": 118,
    "gs_author_count": 7
  },
  {
    "arxiv_id": "2403.04200",
    "title": "ACC-ViT : Atrous Convolution's Comeback in Vision Transformers",
    "year": 2024,
    "published": "2024-03-07T04:05:16Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Transformers have elevated to the state-of-the-art vision architectures through innovations in attention mechanism inspired from visual perception. At present two classes of attentions prevail in vision transformers, regional and sparse attention. The former bounds the pixel interactions within a region; the latter spreads them across sparse grids. The opposing natures of them have resulted in a dilemma between either preserving hierarchical relation or attaining a global context. In this work, ",
    "arxiv_url": "https://arxiv.org/abs/2403.04200v1",
    "pdf_url": "https://arxiv.org/pdf/2403.04200v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2403.04200",
    "arxiv_authors": [
      "Nabil Ibtehaz",
      "Ning Yan",
      "Masood Mortazavi",
      "Daisuke Kihara"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=ACC-ViT+%3A+Atrous+Convolution%27s+Comeback+in+Vision+Transformers+Nabil+Ibtehaz+Ning+Yan+Masood+Mortazavi+Daisuke+Kihara",
    "gs_search_success": true,
    "gs_authors": [
      "VIF17fgAAAAJ",
      "16zCV9EAAAAJ",
      "WfGLzyAAAAAJ"
    ],
    "citation_count": 9,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2310.07572",
    "title": "Impact of Label Types on Training SWIN Models with Overhead Imagery",
    "year": 2023,
    "published": "2023-10-11T15:14:54Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Understanding the impact of data set design on model training and performance can help alleviate the costs associated with generating remote sensing and overhead labeled data. This work examined the impact of training shifted window transformers using bounding boxes and segmentation labels, where the latter are more expensive to produce. We examined classification tasks by comparing models trained with both target and backgrounds against models trained with only target pixels, extracted by segme",
    "arxiv_url": "https://arxiv.org/abs/2310.07572v1",
    "pdf_url": "https://arxiv.org/pdf/2310.07572v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2310.07572",
    "arxiv_authors": [
      "Ryan Ford",
      "Kenneth Hutchison",
      "Nicholas Felts",
      "Benjamin Cheng",
      "Jesse Lew",
      "Kyle Jackson"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Impact+of+Label+Types+on+Training+SWIN+Models+with+Overhead+Imagery+Ryan+Ford+Kenneth+Hutchison+Nicholas+Felts+Benjamin+Cheng+Jesse+Lew",
    "gs_search_success": true,
    "gs_authors": [
      "gCvYvzAAAAAJ",
      "aLzydlwAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2410.23991",
    "title": "Localization, balance and affinity: a stronger multifaceted collaborative salient object detector in remote sensing images",
    "year": 2024,
    "published": "2024-10-31T14:50:48Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Despite significant advancements in salient object detection(SOD) in optical remote sensing images(ORSI), challenges persist due to the intricate edge structures of ORSIs and the complexity of their contextual relationships. Current deep learning approaches encounter difficulties in accurately identifying boundary features and lack efficiency in collaboratively modeling the foreground and background by leveraging contextual features. To address these challenges, we propose a stronger multifacete",
    "arxiv_url": "https://arxiv.org/abs/2410.23991v1",
    "pdf_url": "https://arxiv.org/pdf/2410.23991v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2410.23991",
    "arxiv_authors": [
      "Yakun Xie",
      "Suning Liu",
      "Hongyu Chen",
      "Shaohan Cao",
      "Huixin Zhang",
      "Dejun Feng",
      "Qian Wan",
      "Jun Zhu",
      "Qing Zhu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Localization%2C+balance+and+affinity%3A+a+stronger+multifaceted+collaborative+salient+object+detector+in+remote+sensing+images+Yakun+Xie+Suning+Liu+Hongyu+Chen+Shaohan+Cao+Huixin+Zhang",
    "gs_search_success": true,
    "gs_authors": [
      "vsLbCY4AAAAJ",
      "xmf_X1EAAAAJ",
      "dZaAPNQAAAAJ"
    ],
    "citation_count": 24,
    "gs_author_count": 9
  },
  {
    "arxiv_id": "2304.11968",
    "title": "Track Anything: Segment Anything Meets Videos",
    "year": 2023,
    "published": "2023-04-24T10:04:06Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Recently, the Segment Anything Model (SAM) gains lots of attention rapidly due to its impressive segmentation performance on images. Regarding its strong ability on image segmentation and high interactivity with different prompts, we found that it performs poorly on consistent segmentation in videos. Therefore, in this report, we propose Track Anything Model (TAM), which achieves high-performance interactive tracking and segmentation in videos. To be detailed, given a video sequence, only with v",
    "arxiv_url": "https://arxiv.org/abs/2304.11968v2",
    "pdf_url": "https://arxiv.org/pdf/2304.11968v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2304.11968",
    "arxiv_authors": [
      "Jinyu Yang",
      "Mingqi Gao",
      "Zhe Li",
      "Shang Gao",
      "Fangjing Wang",
      "Feng Zheng"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Track+Anything%3A+Segment+Anything+Meets+Videos+Jinyu+Yang+Mingqi+Gao+Zhe+Li+Shang+Gao+Fangjing+Wang",
    "gs_search_success": true,
    "gs_authors": [
      "PcmyXHMAAAAJ",
      "of0KrQQAAAAJ",
      "XZqhEdgAAAAJ",
      "ECCd0hwAAAAJ"
    ],
    "citation_count": 362,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2310.17281",
    "title": "BEVContrast: Self-Supervision in BEV Space for Automotive Lidar Point Clouds",
    "year": 2023,
    "published": "2023-10-26T10:02:33Z",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "We present a surprisingly simple and efficient method for self-supervision of 3D backbone on automotive Lidar point clouds. We design a contrastive loss between features of Lidar scans captured in the same scene. Several such approaches have been proposed in the literature from PointConstrast, which uses a contrast at the level of points, to the state-of-the-art TARL, which uses a contrast at the level of segments, roughly corresponding to objects. While the former enjoys a great simplicity of i",
    "arxiv_url": "https://arxiv.org/abs/2310.17281v1",
    "pdf_url": "https://arxiv.org/pdf/2310.17281v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2310.17281",
    "arxiv_authors": [
      "Corentin Sautier",
      "Gilles Puy",
      "Alexandre Boulch",
      "Renaud Marlet",
      "Vincent Lepetit"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=BEVContrast%3A+Self-Supervision+in+BEV+Space+for+Automotive+Lidar+Point+Clouds+Corentin+Sautier+Gilles+Puy+Alexandre+Boulch+Renaud+Marlet+Vincent+Lepetit",
    "gs_search_success": true,
    "gs_authors": [
      "2rclwh4AAAAJ",
      "enaORE8AAAAJ",
      "xYDkHEsAAAAJ",
      "iJ3qFGAAAAAJ",
      "h0a5q3QAAAAJ"
    ],
    "citation_count": 32,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2505.18342",
    "title": "Pose Splatter: A 3D Gaussian Splatting Model for Quantifying Animal Pose and Appearance",
    "year": 2025,
    "published": "2025-05-23T19:57:31Z",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "Accurate and scalable quantification of animal pose and appearance is crucial for studying behavior. Current 3D pose estimation techniques, such as keypoint- and mesh-based techniques, often face challenges including limited representational detail, labor-intensive annotation requirements, and expensive per-frame optimization. These limitations hinder the study of subtle movements and can make large-scale analyses impractical. We propose Pose Splatter, a novel framework leveraging shape carving ",
    "arxiv_url": "https://arxiv.org/abs/2505.18342v1",
    "pdf_url": "https://arxiv.org/pdf/2505.18342v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2505.18342",
    "arxiv_authors": [
      "Jack Goffinet",
      "Youngjo Min",
      "Carlo Tomasi",
      "David E. Carlson"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Pose+Splatter%3A+A+3D+Gaussian+Splatting+Model+for+Quantifying+Animal+Pose+and+Appearance+Jack+Goffinet+Youngjo+Min+Carlo+Tomasi+David+E.+Carlson",
    "gs_search_success": true,
    "gs_authors": [
      "R9FqK9cAAAAJ",
      "xViwZ0QAAAAJ",
      "-oXW2RYAAAAJ",
      "-Iv1pQUAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2503.05322",
    "title": "Attenuation artifact detection and severity classification in intracoronary OCT using mixed image representations",
    "year": 2025,
    "published": "2025-03-07T11:01:00Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "eess.IV"
    ],
    "abstract": "In intracoronary optical coherence tomography (OCT), blood residues and gas bubbles cause attenuation artifacts that can obscure critical vessel structures. The presence and severity of these artifacts may warrant re-acquisition, prolonging procedure time and increasing use of contrast agent. Accurate detection of these artifacts can guide targeted re-acquisition, reducing the amount of repeated scans needed to achieve diagnostically viable images. However, the highly heterogeneous appearance of",
    "arxiv_url": "https://arxiv.org/abs/2503.05322v1",
    "pdf_url": "https://arxiv.org/pdf/2503.05322v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2503.05322",
    "arxiv_authors": [
      "Pierandrea Cancian",
      "Simone Saitta",
      "Xiaojin Gu",
      "Rudolf L. M. van Herten",
      "Thijs J. Luttikholt",
      "Jos Thannhauser",
      "Rick H. J. A. Volleberg",
      "Ruben G. A. van der Waerden",
      "Joske L. van der Zande",
      "Clarisa I. Sánchez",
      "Bram van Ginneken",
      "Niels van Royen",
      "Ivana Išgum"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Attenuation+artifact+detection+and+severity+classification+in+intracoronary+OCT+using+mixed+image+representations+Pierandrea+Cancian+Simone+Saitta+Xiaojin+Gu+Rudolf+L.+M.+van+Herten+Thijs+J.+Luttikholt",
    "gs_search_success": true,
    "gs_authors": [
      "RnJGH34AAAAJ",
      "vNbTaXsAAAAJ",
      "CFNgI7QAAAAJ",
      "mNvt5A4AAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 9
  },
  {
    "arxiv_id": "2305.04142",
    "title": "Transformer-Based Hierarchical Clustering for Brain Network Analysis",
    "year": 2023,
    "published": "2023-05-06T22:14:13Z",
    "categories": [
      "cs.LG",
      "cs.CV",
      "cs.NE",
      "q-bio.NC"
    ],
    "abstract": "Brain networks, graphical models such as those constructed from MRI, have been widely used in pathological prediction and analysis of brain functions. Within the complex brain system, differences in neuronal connection strengths parcellate the brain into various functional modules (network communities), which are critical for brain analysis. However, identifying such communities within the brain has been a nontrivial issue due to the complexity of neuronal interactions. In this work, we propose ",
    "arxiv_url": "https://arxiv.org/abs/2305.04142v1",
    "pdf_url": "https://arxiv.org/pdf/2305.04142v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2305.04142",
    "arxiv_authors": [
      "Wei Dai",
      "Hejie Cui",
      "Xuan Kan",
      "Ying Guo",
      "Sanne van Rooij",
      "Carl Yang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Transformer-Based+Hierarchical+Clustering+for+Brain+Network+Analysis+Wei+Dai+Hejie+Cui+Xuan+Kan+Ying+Guo+Sanne+van+Rooij",
    "gs_search_success": true,
    "gs_authors": [
      "N1x7v90AAAAJ",
      "mamsPeoAAAAJ",
      "r0Vh6GEAAAAJ",
      "mOINlwcAAAAJ",
      "SiD9eLgAAAAJ"
    ],
    "citation_count": 23,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2401.09939",
    "title": "ICGNet: A Unified Approach for Instance-Centric Grasping",
    "year": 2024,
    "published": "2024-01-18T12:41:41Z",
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "abstract": "Accurate grasping is the key to several robotic tasks including assembly and household robotics. Executing a successful grasp in a cluttered environment requires multiple levels of scene understanding: First, the robot needs to analyze the geometric properties of individual objects to find feasible grasps. These grasps need to be compliant with the local object geometry. Second, for each proposed grasp, the robot needs to reason about the interactions with other objects in the scene. Finally, th",
    "arxiv_url": "https://arxiv.org/abs/2401.09939v2",
    "pdf_url": "https://arxiv.org/pdf/2401.09939v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2401.09939",
    "arxiv_authors": [
      "René Zurbrügg",
      "Yifan Liu",
      "Francis Engelmann",
      "Suryansh Kumar",
      "Marco Hutter",
      "Vaishakh Patil",
      "Fisher Yu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=ICGNet%3A+A+Unified+Approach+for+Instance-Centric+Grasping+Ren%C3%A9+Zurbr%C3%BCgg+Yifan+Liu+Francis+Engelmann+Suryansh+Kumar+Marco+Hutter",
    "gs_search_success": true,
    "gs_authors": [
      "DO3quJYAAAAJ",
      "-XCiamcAAAAJ",
      "wbk0QAcAAAAJ",
      "-xOsXi8AAAAJ",
      "aB04078AAAAJ",
      "feJr7REAAAAJ"
    ],
    "citation_count": 15,
    "gs_author_count": 7
  },
  {
    "arxiv_id": "2308.00307",
    "title": "Domain Adaptation based on Human Feedback for Enhancing Generative Model Denoising Abilities",
    "year": 2023,
    "published": "2023-08-01T05:59:02Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "How can we apply human feedback into generative model? As answer of this question, in this paper, we show the method applied on denoising problem and domain adaptation using human feedback. Deep generative models have demonstrated impressive results in image denoising. However, current image denoising models often produce inappropriate results when applied to domains different from the ones they were trained on. If there are `Good' and `Bad' result for unseen data, how to raise up quality of `Ba",
    "arxiv_url": "https://arxiv.org/abs/2308.00307v1",
    "pdf_url": "https://arxiv.org/pdf/2308.00307v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2308.00307",
    "arxiv_authors": [
      "Hyun-Cheol Park",
      "Sung Ho Kang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Domain+Adaptation+based+on+Human+Feedback+for+Enhancing+Generative+Model+Denoising+Abilities+Hyun-Cheol+Park+Sung+Ho+Kang",
    "gs_search_success": true,
    "gs_authors": [
      "pHwPqc4AAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 2
  },
  {
    "arxiv_id": "2303.06859",
    "title": "Learning Distortion Invariant Representation for Image Restoration from A Causality Perspective",
    "year": 2023,
    "published": "2023-03-13T05:04:18Z",
    "categories": [
      "cs.CV",
      "cs.MM",
      "eess.IV"
    ],
    "abstract": "In recent years, we have witnessed the great advancement of Deep neural networks (DNNs) in image restoration. However, a critical limitation is that they cannot generalize well to real-world degradations with different degrees or types. In this paper, we are the first to propose a novel training strategy for image restoration from the causality perspective, to improve the generalization ability of DNNs for unknown degradations. Our method, termed Distortion Invariant representation Learning (DIL",
    "arxiv_url": "https://arxiv.org/abs/2303.06859v2",
    "pdf_url": "https://arxiv.org/pdf/2303.06859v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2303.06859",
    "arxiv_authors": [
      "Xin Li",
      "Bingchen Li",
      "Xin Jin",
      "Cuiling Lan",
      "Zhibo Chen"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Learning+Distortion+Invariant+Representation+for+Image+Restoration+from+A+Causality+Perspective+Xin+Li+Bingchen+Li+Xin+Jin+Cuiling+Lan+Zhibo+Chen",
    "gs_search_success": true,
    "gs_authors": [
      "sbiY97gAAAAJ",
      "XZugqiwAAAAJ",
      "qHeWjNwAAAAJ",
      "1ayDJfsAAAAJ",
      "byaSC-kAAAAJ"
    ],
    "citation_count": 36,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2311.13621",
    "title": "EA-KD: Entropy-based Adaptive Knowledge Distillation",
    "year": 2023,
    "published": "2023-11-22T08:34:33Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Knowledge distillation (KD) enables a smaller \"student\" model to mimic a larger \"teacher\" model by transferring knowledge from the teacher's output or features. However, most KD methods treat all samples uniformly, overlooking the varying learning value of each sample and thereby limiting their effectiveness. In this paper, we propose Entropy-based Adaptive Knowledge Distillation (EA-KD), a simple yet effective plug-and-play KD method that prioritizes learning from valuable samples. EA-KD quanti",
    "arxiv_url": "https://arxiv.org/abs/2311.13621v3",
    "pdf_url": "https://arxiv.org/pdf/2311.13621v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2311.13621",
    "arxiv_authors": [
      "Chi-Ping Su",
      "Ching-Hsun Tseng",
      "Bin Pu",
      "Lei Zhao",
      "Jiewen Yang",
      "Zhuangzhuang Chen",
      "Shin-Jye Lee"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=EA-KD%3A+Entropy-based+Adaptive+Knowledge+Distillation+Chi-Ping+Su+Ching-Hsun+Tseng+Bin+Pu+Lei+Zhao+Jiewen+Yang",
    "gs_search_success": true,
    "gs_authors": [
      "dBGvXLEAAAAJ",
      "V75QQ9AAAAAJ",
      "Y0MYdh8AAAAJ",
      "MHb9pTUAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 7
  },
  {
    "arxiv_id": "2504.13026",
    "title": "TTRD3: Texture Transfer Residual Denoising Dual Diffusion Model for Remote Sensing Image Super-Resolution",
    "year": 2025,
    "published": "2025-04-17T15:37:13Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Remote Sensing Image Super-Resolution (RSISR) reconstructs high-resolution (HR) remote sensing images from low-resolution inputs to support fine-grained ground object interpretation. Existing methods face three key challenges: (1) Difficulty in extracting multi-scale features from spatially heterogeneous RS scenes, (2) Limited prior information causing semantic inconsistency in reconstructions, and (3) Trade-off imbalance between geometric accuracy and visual quality. To address these issues, we",
    "arxiv_url": "https://arxiv.org/abs/2504.13026v1",
    "pdf_url": "https://arxiv.org/pdf/2504.13026v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2504.13026",
    "arxiv_authors": [
      "Yide Liu",
      "Haijiang Sun",
      "Xiaowen Zhang",
      "Qiaoyuan Liu",
      "Zhouchang Chen",
      "Chongzhuo Xiao"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=TTRD3%3A+Texture+Transfer+Residual+Denoising+Dual+Diffusion+Model+for+Remote+Sensing+Image+Super-Resolution+Yide+Liu+Haijiang+Sun+Xiaowen+Zhang+Qiaoyuan+Liu+Zhouchang+Chen",
    "gs_search_success": true,
    "gs_authors": [],
    "citation_count": null,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2309.01907",
    "title": "SyntheWorld: A Large-Scale Synthetic Dataset for Land Cover Mapping and Building Change Detection",
    "year": 2023,
    "published": "2023-09-05T02:42:41Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.HC"
    ],
    "abstract": "Synthetic datasets, recognized for their cost effectiveness, play a pivotal role in advancing computer vision tasks and techniques. However, when it comes to remote sensing image processing, the creation of synthetic datasets becomes challenging due to the demand for larger-scale and more diverse 3D models. This complexity is compounded by the difficulties associated with real remote sensing datasets, including limited data acquisition and high annotation costs, which amplifies the need for high",
    "arxiv_url": "https://arxiv.org/abs/2309.01907v1",
    "pdf_url": "https://arxiv.org/pdf/2309.01907v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2309.01907",
    "arxiv_authors": [
      "Jian Song",
      "Hongruixuan Chen",
      "Naoto Yokoya"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=SyntheWorld%3A+A+Large-Scale+Synthetic+Dataset+for+Land+Cover+Mapping+and+Building+Change+Detection+Jian+Song+Hongruixuan+Chen+Naoto+Yokoya",
    "gs_search_success": true,
    "gs_authors": [
      "DJ2KOn8AAAAJ",
      "I-Mr4x0AAAAJ",
      "XOk4Cf0AAAAJ"
    ],
    "citation_count": 30,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2305.15701",
    "title": "Action Sensitivity Learning for Temporal Action Localization",
    "year": 2023,
    "published": "2023-05-25T04:19:14Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Temporal action localization (TAL), which involves recognizing and locating action instances, is a challenging task in video understanding. Most existing approaches directly predict action classes and regress offsets to boundaries, while overlooking the discrepant importance of each frame. In this paper, we propose an Action Sensitivity Learning framework (ASL) to tackle this task, which aims to assess the value of each frame and then leverage the generated action sensitivity to recalibrate the ",
    "arxiv_url": "https://arxiv.org/abs/2305.15701v2",
    "pdf_url": "https://arxiv.org/pdf/2305.15701v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2305.15701",
    "arxiv_authors": [
      "Jiayi Shao",
      "Xiaohan Wang",
      "Ruijie Quan",
      "Junjun Zheng",
      "Jiang Yang",
      "Yi Yang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Action+Sensitivity+Learning+for+Temporal+Action+Localization+Jiayi+Shao+Xiaohan+Wang+Ruijie+Quan+Junjun+Zheng+Jiang+Yang",
    "gs_search_success": false,
    "gs_authors": [],
    "citation_count": null,
    "error_type": "fetch_failed",
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2407.09088",
    "title": "FD-SOS: Vision-Language Open-Set Detectors for Bone Fenestration and Dehiscence Detection from Intraoral Images",
    "year": 2024,
    "published": "2024-07-12T08:29:25Z",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "abstract": "Accurate detection of bone fenestration and dehiscence (FD) is crucial for effective treatment planning in dentistry. While cone-beam computed tomography (CBCT) is the gold standard for evaluating FD, it comes with limitations such as radiation exposure, limited accessibility, and higher cost compared to intraoral images. In intraoral images, dentists face challenges in the differential diagnosis of FD. This paper presents a novel and clinically significant application of FD detection solely fro",
    "arxiv_url": "https://arxiv.org/abs/2407.09088v1",
    "pdf_url": "https://arxiv.org/pdf/2407.09088v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2407.09088",
    "arxiv_authors": [
      "Marawan Elbatel",
      "Keyuan Liu",
      "Yanqi Yang",
      "Xiaomeng Li"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=FD-SOS%3A+Vision-Language+Open-Set+Detectors+for+Bone+Fenestration+and+Dehiscence+Detection+from+Intraoral+Images+Marawan+Elbatel+Keyuan+Liu+Yanqi+Yang+Xiaomeng+Li",
    "gs_search_success": true,
    "gs_authors": [
      "uVTzPpoAAAAJ",
      "f6yps4gAAAAJ"
    ],
    "citation_count": 2,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2403.11340",
    "title": "StainDiffuser: MultiTask Dual Diffusion Model for Virtual Staining",
    "year": 2024,
    "published": "2024-03-17T20:47:52Z",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "abstract": "Hematoxylin and Eosin (H&E) staining is widely regarded as the standard in pathology for diagnosing diseases and tracking tumor recurrence. While H&E staining shows tissue structures, it lacks the ability to reveal specific proteins that are associated with disease severity and treatment response. Immunohistochemical (IHC) stains use antibodies to highlight the expression of these proteins on their respective cell types, improving diagnostic accuracy, and assisting with drug selection for treatm",
    "arxiv_url": "https://arxiv.org/abs/2403.11340v2",
    "pdf_url": "https://arxiv.org/pdf/2403.11340v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2403.11340",
    "arxiv_authors": [
      "Tushar Kataria",
      "Beatrice Knudsen",
      "Shireen Y. Elhabian"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=StainDiffuser%3A+MultiTask+Dual+Diffusion+Model+for+Virtual+Staining+Tushar+Kataria+Beatrice+Knudsen+Shireen+Y.+Elhabian",
    "gs_search_success": true,
    "gs_authors": [
      "IABaOcUAAAAJ",
      "Qrs30aYAAAAJ",
      "Jyr7HhwAAAAJ"
    ],
    "citation_count": 21,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2308.06925",
    "title": "CBA: Improving Online Continual Learning via Continual Bias Adaptor",
    "year": 2023,
    "published": "2023-08-14T04:03:51Z",
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "abstract": "Online continual learning (CL) aims to learn new knowledge and consolidate previously learned knowledge from non-stationary data streams. Due to the time-varying training setting, the model learned from a changing distribution easily forgets the previously learned knowledge and biases toward the newly received task. To address this problem, we propose a Continual Bias Adaptor (CBA) module to augment the classifier network to adapt to catastrophic distribution change during training, such that th",
    "arxiv_url": "https://arxiv.org/abs/2308.06925v1",
    "pdf_url": "https://arxiv.org/pdf/2308.06925v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2308.06925",
    "arxiv_authors": [
      "Quanziang Wang",
      "Renzhen Wang",
      "Yichen Wu",
      "Xixi Jia",
      "Deyu Meng"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=CBA%3A+Improving+Online+Continual+Learning+via+Continual+Bias+Adaptor+Quanziang+Wang+Renzhen+Wang+Yichen+Wu+Xixi+Jia+Deyu+Meng",
    "gs_search_success": true,
    "gs_authors": [
      "K4hTFFwAAAAJ",
      "QdBFdk4AAAAJ",
      "an6w-64AAAAJ",
      "p53r6j0AAAAJ",
      "QZ1-nnwAAAAJ"
    ],
    "citation_count": 36,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2410.07571",
    "title": "How Does Vision-Language Adaptation Impact the Safety of Vision Language Models?",
    "year": 2024,
    "published": "2024-10-10T03:12:03Z",
    "categories": [
      "cs.CL",
      "cs.CV"
    ],
    "abstract": "Vision-Language adaptation (VL adaptation) transforms Large Language Models (LLMs) into Large Vision-Language Models (LVLMs) for multimodal tasks, but this process often compromises the inherent safety capabilities embedded in the original LLMs. Despite potential harmfulness due to weakened safety measures, in-depth analysis on the effects of VL adaptation on safety remains under-explored. This study examines how VL adaptation influences safety and evaluates the impact of safety fine-tuning meth",
    "arxiv_url": "https://arxiv.org/abs/2410.07571v2",
    "pdf_url": "https://arxiv.org/pdf/2410.07571v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2410.07571",
    "arxiv_authors": [
      "Seongyun Lee",
      "Geewook Kim",
      "Jiyeon Kim",
      "Hyunji Lee",
      "Hoyeon Chang",
      "Sue Hyun Park",
      "Minjoon Seo"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=How+Does+Vision-Language+Adaptation+Impact+the+Safety+of+Vision+Language+Models%3F+Seongyun+Lee+Geewook+Kim+Jiyeon+Kim+Hyunji+Lee+Hoyeon+Chang",
    "gs_search_success": true,
    "gs_authors": [
      "jD4MD7sAAAAJ",
      "jAzbKqEAAAAJ",
      "T8Zid6YAAAAJ",
      "LQ-52vsAAAAJ",
      "zYze5fIAAAAJ",
      "1a2QbgEAAAAJ"
    ],
    "citation_count": 8,
    "gs_author_count": 7
  },
  {
    "arxiv_id": "2407.21498",
    "title": "MaskUno: Switch-Split Block For Enhancing Instance Segmentation",
    "year": 2024,
    "published": "2024-07-31T10:12:14Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Instance segmentation is an advanced form of image segmentation which, beyond traditional segmentation, requires identifying individual instances of repeating objects in a scene. Mask R-CNN is the most common architecture for instance segmentation, and improvements to this architecture include steps such as benefiting from bounding box refinements, adding semantics, or backbone enhancements. In all the proposed variations to date, the problem of competing kernels (each class aims to maximize its",
    "arxiv_url": "https://arxiv.org/abs/2407.21498v1",
    "pdf_url": "https://arxiv.org/pdf/2407.21498v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2407.21498",
    "arxiv_authors": [
      "Jawad Haidar",
      "Marc Mouawad",
      "Imad Elhajj",
      "Daniel Asmar"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=MaskUno%3A+Switch-Split+Block+For+Enhancing+Instance+Segmentation+Jawad+Haidar+Marc+Mouawad+Imad+Elhajj+Daniel+Asmar",
    "gs_search_success": true,
    "gs_authors": [
      "wJJeWOEAAAAJ",
      "56N3Qm4AAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2307.03505",
    "title": "RCDN -- Robust X-Corner Detection Algorithm based on Advanced CNN Model",
    "year": 2023,
    "published": "2023-07-07T10:40:41Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Accurate detection and localization of X-corner on both planar and non-planar patterns is a core step in robotics and machine vision. However, previous works could not make a good balance between accuracy and robustness, which are both crucial criteria to evaluate the detectors performance. To address this problem, in this paper we present a novel detection algorithm which can maintain high sub-pixel precision on inputs under multiple interference, such as lens distortion, extreme poses and nois",
    "arxiv_url": "https://arxiv.org/abs/2307.03505v1",
    "pdf_url": "https://arxiv.org/pdf/2307.03505v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2307.03505",
    "arxiv_authors": [
      "Ben Chen",
      "Caihua Xiong",
      "Quanlin Li",
      "Zhonghua Wan"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=RCDN+--+Robust+X-Corner+Detection+Algorithm+based+on+Advanced+CNN+Model+Ben+Chen+Caihua+Xiong+Quanlin+Li+Zhonghua+Wan",
    "gs_search_success": true,
    "gs_authors": [
      "qa3VWnQAAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2305.05344",
    "title": "Trustworthy Multi-phase Liver Tumor Segmentation via Evidence-based Uncertainty",
    "year": 2023,
    "published": "2023-05-09T11:10:51Z",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "abstract": "Multi-phase liver contrast-enhanced computed tomography (CECT) images convey the complementary multi-phase information for liver tumor segmentation (LiTS), which are crucial to assist the diagnosis of liver cancer clinically. However, the performances of existing multi-phase liver tumor segmentation (MPLiTS)-based methods suffer from redundancy and weak interpretability, % of the fused result, resulting in the implicit unreliability of clinical applications. In this paper, we propose a novel tru",
    "arxiv_url": "https://arxiv.org/abs/2305.05344v2",
    "pdf_url": "https://arxiv.org/pdf/2305.05344v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2305.05344",
    "arxiv_authors": [
      "Chuanfei Hu",
      "Tianyi Xia",
      "Ying Cui",
      "Quchen Zou",
      "Yuancheng Wang",
      "Wenbo Xiao",
      "Shenghong Ju",
      "Xinde Li"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Trustworthy+Multi-phase+Liver+Tumor+Segmentation+via+Evidence-based+Uncertainty+Chuanfei+Hu+Tianyi+Xia+Ying+Cui+Quchen+Zou+Yuancheng+Wang",
    "gs_search_success": false,
    "gs_authors": [],
    "citation_count": null,
    "error_type": "fetch_failed",
    "gs_author_count": 8
  },
  {
    "arxiv_id": "2411.03551",
    "title": "Enhancing Weakly Supervised Semantic Segmentation for Fibrosis via Controllable Image Generation",
    "year": 2024,
    "published": "2024-11-05T23:11:26Z",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "abstract": "Fibrotic Lung Disease (FLD) is a severe condition marked by lung stiffening and scarring, leading to respiratory decline. High-resolution computed tomography (HRCT) is critical for diagnosing and monitoring FLD; however, fibrosis appears as irregular, diffuse patterns with unclear boundaries, leading to high inter-observer variability and time-intensive manual annotation. To tackle this challenge, we propose DiffSeg, a novel weakly supervised semantic segmentation (WSSS) method that uses image-l",
    "arxiv_url": "https://arxiv.org/abs/2411.03551v1",
    "pdf_url": "https://arxiv.org/pdf/2411.03551v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2411.03551",
    "arxiv_authors": [
      "Zhiling Yue",
      "Yingying Fang",
      "Liutao Yang",
      "Nikhil Baid",
      "Simon Walsh",
      "Guang Yang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Enhancing+Weakly+Supervised+Semantic+Segmentation+for+Fibrosis+via+Controllable+Image+Generation+Zhiling+Yue+Yingying+Fang+Liutao+Yang+Nikhil+Baid+Simon+Walsh",
    "gs_search_success": false,
    "gs_authors": [],
    "citation_count": null,
    "error_type": "fetch_failed",
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2409.12635",
    "title": "EFA-YOLO: An Efficient Feature Attention Model for Fire and Flame Detection",
    "year": 2024,
    "published": "2024-09-19T10:20:07Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "As a natural disaster with high suddenness and great destructiveness, fire has long posed a major threat to human society and ecological environment. In recent years, with the rapid development of smart city and Internet of Things (IoT) technologies, fire detection systems based on deep learning have gradually become a key means to cope with fire hazards. However, existing fire detection models still have many challenges in terms of detection accuracy and real-time performance in complex context",
    "arxiv_url": "https://arxiv.org/abs/2409.12635v1",
    "pdf_url": "https://arxiv.org/pdf/2409.12635v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2409.12635",
    "arxiv_authors": [
      "Weichao Pan",
      "Xu Wang",
      "Wenqing Huan"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=EFA-YOLO%3A+An+Efficient+Feature+Attention+Model+for+Fire+and+Flame+Detection+Weichao+Pan+Xu+Wang+Wenqing+Huan",
    "gs_search_success": false,
    "gs_authors": [],
    "citation_count": null,
    "error_type": "fetch_failed",
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2305.06492",
    "title": "Treasure What You Have: Exploiting Similarity in Deep Neural Networks for Efficient Video Processing",
    "year": 2023,
    "published": "2023-05-10T23:18:47Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Deep learning has enabled various Internet of Things (IoT) applications. Still, designing models with high accuracy and computational efficiency remains a significant challenge, especially in real-time video processing applications. Such applications exhibit high inter- and intra-frame redundancy, allowing further improvement. This paper proposes a similarity-aware training methodology that exploits data redundancy in video frames for efficient processing. Our approach introduces a per-layer reg",
    "arxiv_url": "https://arxiv.org/abs/2305.06492v1",
    "pdf_url": "https://arxiv.org/pdf/2305.06492v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2305.06492",
    "arxiv_authors": [
      "Hadjer Benmeziane",
      "Halima Bouzidi",
      "Hamza Ouarnoughi",
      "Ozcan Ozturk",
      "Smail Niar"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Treasure+What+You+Have%3A+Exploiting+Similarity+in+Deep+Neural+Networks+for+Efficient+Video+Processing+Hadjer+Benmeziane+Halima+Bouzidi+Hamza+Ouarnoughi+Ozcan+Ozturk+Smail+Niar",
    "gs_search_success": true,
    "gs_authors": [
      "G81DPWYAAAAJ",
      "_FL7xDYAAAAJ",
      "sI0ity4AAAAJ"
    ],
    "citation_count": 3,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2301.02560",
    "title": "GeoDE: a Geographically Diverse Evaluation Dataset for Object Recognition",
    "year": 2023,
    "published": "2023-01-05T18:21:50Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Current dataset collection methods typically scrape large amounts of data from the web. While this technique is extremely scalable, data collected in this way tends to reinforce stereotypical biases, can contain personally identifiable information, and typically originates from Europe and North America. In this work, we rethink the dataset collection paradigm and introduce GeoDE, a geographically diverse dataset with 61,940 images from 40 classes and 6 world regions, with no personally identifia",
    "arxiv_url": "https://arxiv.org/abs/2301.02560v4",
    "pdf_url": "https://arxiv.org/pdf/2301.02560v4",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2301.02560",
    "arxiv_authors": [
      "Vikram V. Ramaswamy",
      "Sing Yu Lin",
      "Dora Zhao",
      "Aaron B. Adcock",
      "Laurens van der Maaten",
      "Deepti Ghadiyaram",
      "Olga Russakovsky"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=GeoDE%3A+a+Geographically+Diverse+Evaluation+Dataset+for+Object+Recognition+Vikram+V.+Ramaswamy+Sing+Yu+Lin+Dora+Zhao+Aaron+B.+Adcock+Laurens+van+der+Maaten",
    "gs_search_success": true,
    "gs_authors": [
      "oa78zHUAAAAJ",
      "QgBn1BgAAAAJ",
      "Iq8AdKQAAAAJ",
      "OoHs7BgAAAAJ",
      "6GDfcqEAAAAJ"
    ],
    "citation_count": 53,
    "gs_author_count": 7
  },
  {
    "arxiv_id": "2412.00878",
    "title": "Beyond Pixels: Text Enhances Generalization in Real-World Image Restoration",
    "year": 2024,
    "published": "2024-12-01T16:36:22Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Generalization has long been a central challenge in real-world image restoration. While recent diffusion-based restoration methods, which leverage generative priors from text-to-image models, have made progress in recovering more realistic details, they still encounter \"generative capability deactivation\" when applied to out-of-distribution real-world data. To address this, we propose using text as an auxiliary invariant representation to reactivate the generative capabilities of these models. W",
    "arxiv_url": "https://arxiv.org/abs/2412.00878v2",
    "pdf_url": "https://arxiv.org/pdf/2412.00878v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2412.00878",
    "arxiv_authors": [
      "Haoze Sun",
      "Wenbo Li",
      "Jiayue Liu",
      "Kaiwen Zhou",
      "Yongqiang Chen",
      "Yong Guo",
      "Yanwei Li",
      "Renjing Pei",
      "Long Peng",
      "Yujiu Yang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Beyond+Pixels%3A+Text+Enhances+Generalization+in+Real-World+Image+Restoration+Haoze+Sun+Wenbo+Li+Jiayue+Liu+Kaiwen+Zhou+Yongqiang+Chen",
    "gs_search_success": true,
    "gs_authors": [
      "4gH3sxsAAAAJ",
      "VmLZOc0AAAAJ",
      "foGn_TIAAAAJ",
      "nHmlZ5QAAAAJ",
      "aB2KirIAAAAJ",
      "huQ_Ig8AAAAJ"
    ],
    "citation_count": 9,
    "gs_author_count": 9
  },
  {
    "arxiv_id": "2304.03950",
    "title": "GANHead: Towards Generative Animatable Neural Head Avatars",
    "year": 2023,
    "published": "2023-04-08T07:56:21Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "To bring digital avatars into people's lives, it is highly demanded to efficiently generate complete, realistic, and animatable head avatars. This task is challenging, and it is difficult for existing methods to satisfy all the requirements at once. To achieve these goals, we propose GANHead (Generative Animatable Neural Head Avatar), a novel generative head model that takes advantages of both the fine-grained control over the explicit expression parameters and the realistic rendering results of",
    "arxiv_url": "https://arxiv.org/abs/2304.03950v1",
    "pdf_url": "https://arxiv.org/pdf/2304.03950v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2304.03950",
    "arxiv_authors": [
      "Sijing Wu",
      "Yichao Yan",
      "Yunhao Li",
      "Yuhao Cheng",
      "Wenhan Zhu",
      "Ke Gao",
      "Xiaobo Li",
      "Guangtao Zhai"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=GANHead%3A+Towards+Generative+Animatable+Neural+Head+Avatars+Sijing+Wu+Yichao+Yan+Yunhao+Li+Yuhao+Cheng+Wenhan+Zhu",
    "gs_search_success": true,
    "gs_authors": [
      "GNk1yVkAAAAJ",
      "wYEh5DUAAAAJ",
      "ZPHMMRkAAAAJ",
      "E6zbSYgAAAAJ"
    ],
    "citation_count": 29,
    "gs_author_count": 8
  },
  {
    "arxiv_id": "2403.13261",
    "title": "Self-Supervised Class-Agnostic Motion Prediction with Spatial and Temporal Consistency Regularizations",
    "year": 2024,
    "published": "2024-03-20T02:58:45Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "The perception of motion behavior in a dynamic environment holds significant importance for autonomous driving systems, wherein class-agnostic motion prediction methods directly predict the motion of the entire point cloud. While most existing methods rely on fully-supervised learning, the manual labeling of point cloud data is laborious and time-consuming. Therefore, several annotation-efficient methods have been proposed to address this challenge. Although effective, these methods rely on weak",
    "arxiv_url": "https://arxiv.org/abs/2403.13261v2",
    "pdf_url": "https://arxiv.org/pdf/2403.13261v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2403.13261",
    "arxiv_authors": [
      "Kewei Wang",
      "Yizheng Wu",
      "Jun Cen",
      "Zhiyu Pan",
      "Xingyi Li",
      "Zhe Wang",
      "Zhiguo Cao",
      "Guosheng Lin"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Self-Supervised+Class-Agnostic+Motion+Prediction+with+Spatial+and+Temporal+Consistency+Regularizations+Kewei+Wang+Yizheng+Wu+Jun+Cen+Zhiyu+Pan+Xingyi+Li",
    "gs_search_success": true,
    "gs_authors": [
      "396o2BAAAAAJ",
      "XDKQsvUAAAAJ",
      "X1AP9ZEAAAAJ",
      "0_iF4jMAAAAJ",
      "7SKAhBwAAAAJ",
      "ZudEhvcAAAAJ",
      "fW7pUGMAAAAJ"
    ],
    "citation_count": 6,
    "gs_author_count": 8
  },
  {
    "arxiv_id": "2310.06845",
    "title": "RobustEdge: Low Power Adversarial Detection for Cloud-Edge Systems",
    "year": 2023,
    "published": "2023-09-05T13:51:28Z",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "In practical cloud-edge scenarios, where a resource constrained edge performs data acquisition and a cloud system (having sufficient resources) performs inference tasks with a deep neural network (DNN), adversarial robustness is critical for reliability and ubiquitous deployment. Adversarial detection is a prime adversarial defence technique used in prior literature. However, in prior detection works, the detector is attached to the classifier model and both detector and classifier work in tande",
    "arxiv_url": "https://arxiv.org/abs/2310.06845v1",
    "pdf_url": "https://arxiv.org/pdf/2310.06845v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2310.06845",
    "arxiv_authors": [
      "Abhishek Moitra",
      "Abhiroop Bhattacharjee",
      "Youngeun Kim",
      "Priyadarshini Panda"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=RobustEdge%3A+Low+Power+Adversarial+Detection+for+Cloud-Edge+Systems+Abhishek+Moitra+Abhiroop+Bhattacharjee+Youngeun+Kim+Priyadarshini+Panda",
    "gs_search_success": true,
    "gs_authors": [
      "qA5WsYUAAAAJ",
      "Mv7T17QAAAAJ",
      "7KYFOe8AAAAJ",
      "bh5Ve0EAAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2407.20062",
    "title": "SalNAS: Efficient Saliency-prediction Neural Architecture Search with self-knowledge distillation",
    "year": 2024,
    "published": "2024-07-29T14:48:34Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "abstract": "Recent advancements in deep convolutional neural networks have significantly improved the performance of saliency prediction. However, the manual configuration of the neural network architectures requires domain knowledge expertise and can still be time-consuming and error-prone. To solve this, we propose a new Neural Architecture Search (NAS) framework for saliency prediction with two contributions. Firstly, a supernet for saliency prediction is built with a weight-sharing network containing al",
    "arxiv_url": "https://arxiv.org/abs/2407.20062v1",
    "pdf_url": "https://arxiv.org/pdf/2407.20062v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2407.20062",
    "arxiv_authors": [
      "Chakkrit Termritthikun",
      "Ayaz Umer",
      "Suwichaya Suwanwimolkul",
      "Feng Xia",
      "Ivan Lee"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=SalNAS%3A+Efficient+Saliency-prediction+Neural+Architecture+Search+with+self-knowledge+distillation+Chakkrit+Termritthikun+Ayaz+Umer+Suwichaya+Suwanwimolkul+Feng+Xia+Ivan+Lee",
    "gs_search_success": false,
    "gs_authors": [],
    "citation_count": null,
    "error_type": "fetch_failed",
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2503.22204",
    "title": "Segment then Splat: Unified 3D Open-Vocabulary Segmentation via Gaussian Splatting",
    "year": 2025,
    "published": "2025-03-28T07:36:51Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Open-vocabulary querying in 3D space is crucial for enabling more intelligent perception in applications such as robotics, autonomous systems, and augmented reality. However, most existing methods rely on 2D pixel-level parsing, leading to multi-view inconsistencies and poor 3D object retrieval. Moreover, they are limited to static scenes and struggle with dynamic scenes due to the complexities of motion modeling. In this paper, we propose Segment then Splat, a 3D-aware open vocabulary segmentat",
    "arxiv_url": "https://arxiv.org/abs/2503.22204v2",
    "pdf_url": "https://arxiv.org/pdf/2503.22204v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2503.22204",
    "arxiv_authors": [
      "Yiren Lu",
      "Yunlai Zhou",
      "Yiran Qiao",
      "Chaoda Song",
      "Tuo Liang",
      "Jing Ma",
      "Huan Wang",
      "Yu Yin"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Segment+then+Splat%3A+Unified+3D+Open-Vocabulary+Segmentation+via+Gaussian+Splatting+Yiren+Lu+Yunlai+Zhou+Yiran+Qiao+Chaoda+Song+Tuo+Liang",
    "gs_search_success": true,
    "gs_authors": [
      "8euSVtcAAAAJ",
      "pY0_YNcAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 8
  },
  {
    "arxiv_id": "2409.12576",
    "title": "StoryMaker: Towards Holistic Consistent Characters in Text-to-image Generation",
    "year": 2024,
    "published": "2024-09-19T08:53:06Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Tuning-free personalized image generation methods have achieved significant success in maintaining facial consistency, i.e., identities, even with multiple characters. However, the lack of holistic consistency in scenes with multiple characters hampers these methods' ability to create a cohesive narrative. In this paper, we introduce StoryMaker, a personalization solution that preserves not only facial consistency but also clothing, hairstyles, and body consistency, thus facilitating the creatio",
    "arxiv_url": "https://arxiv.org/abs/2409.12576v1",
    "pdf_url": "https://arxiv.org/pdf/2409.12576v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2409.12576",
    "arxiv_authors": [
      "Zhengguang Zhou",
      "Jing Li",
      "Huaxia Li",
      "Nemo Chen",
      "Xu Tang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=StoryMaker%3A+Towards+Holistic+Consistent+Characters+in+Text-to-image+Generation+Zhengguang+Zhou+Jing+Li+Huaxia+Li+Nemo+Chen+Xu+Tang",
    "gs_search_success": true,
    "gs_authors": [
      "BCjT17sAAAAJ",
      "xLv7GQEAAAAJ",
      "grP24aAAAAAJ"
    ],
    "citation_count": 28,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2503.13163",
    "title": "Beyond RGB: Adaptive Parallel Processing for RAW Object Detection",
    "year": 2025,
    "published": "2025-03-17T13:36:49Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Object detection models are typically applied to standard RGB images processed through Image Signal Processing (ISP) pipelines, which are designed to enhance sensor-captured RAW images for human vision. However, these ISP functions can lead to a loss of critical information that may be essential in optimizing for computer vision tasks, such as object detection. In this work, we introduce Raw Adaptation Module (RAM), a module designed to replace the traditional ISP, with parameters optimized spec",
    "arxiv_url": "https://arxiv.org/abs/2503.13163v2",
    "pdf_url": "https://arxiv.org/pdf/2503.13163v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2503.13163",
    "arxiv_authors": [
      "Shani Gamrian",
      "Hila Barel",
      "Feiran Li",
      "Masakazu Yoshimura",
      "Daisuke Iso"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Beyond+RGB%3A+Adaptive+Parallel+Processing+for+RAW+Object+Detection+Shani+Gamrian+Hila+Barel+Feiran+Li+Masakazu+Yoshimura+Daisuke+Iso",
    "gs_search_success": true,
    "gs_authors": [
      "w9_TuxIAAAAJ",
      "kSzUzJUAAAAJ",
      "-nNU6V4AAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2302.11544",
    "title": "One-Pot Multi-Frame Denoising",
    "year": 2023,
    "published": "2023-02-18T09:32:59Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "The performance of learning-based denoising largely depends on clean supervision. However, it is difficult to obtain clean images in many scenes. On the contrary, the capture of multiple noisy frames for the same field of view is available and often natural in real life. Therefore, it is necessary to avoid the restriction of clean labels and make full use of noisy data for model training. So we propose an unsupervised learning strategy named one-pot denoising (OPD) for multi-frame images. OPD is",
    "arxiv_url": "https://arxiv.org/abs/2302.11544v1",
    "pdf_url": "https://arxiv.org/pdf/2302.11544v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2302.11544",
    "arxiv_authors": [
      "Lujia Jin",
      "Shi Zhao",
      "Lei Zhu",
      "Qian Chen",
      "Yanye Lu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=One-Pot+Multi-Frame+Denoising+Lujia+Jin+Shi+Zhao+Lei+Zhu+Qian+Chen+Yanye+Lu",
    "gs_search_success": true,
    "gs_authors": [
      "Wz0lfcwAAAAJ",
      "lF5-gp0AAAAJ",
      "7cOdUFgAAAAJ",
      "WSFToOMAAAAJ",
      "d4dHv9oAAAAJ",
      "-BWasB8AAAAJ"
    ],
    "citation_count": 4,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2307.01024",
    "title": "SAM-DA: UAV Tracks Anything at Night with SAM-Powered Domain Adaptation",
    "year": 2023,
    "published": "2023-07-03T13:55:44Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Domain adaptation (DA) has demonstrated significant promise for real-time nighttime unmanned aerial vehicle (UAV) tracking. However, the state-of-the-art (SOTA) DA still lacks the potential object with accurate pixel-level location and boundary to generate the high-quality target domain training sample. This key issue constrains the transfer learning of the real-time daytime SOTA trackers for challenging nighttime UAV tracking. Recently, the notable Segment Anything Model (SAM) has achieved a re",
    "arxiv_url": "https://arxiv.org/abs/2307.01024v2",
    "pdf_url": "https://arxiv.org/pdf/2307.01024v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2307.01024",
    "arxiv_authors": [
      "Changhong Fu",
      "Liangliang Yao",
      "Haobo Zuo",
      "Guangze Zheng",
      "Jia Pan"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=SAM-DA%3A+UAV+Tracks+Anything+at+Night+with+SAM-Powered+Domain+Adaptation+Changhong+Fu+Liangliang+Yao+Haobo+Zuo+Guangze+Zheng+Jia+Pan",
    "gs_search_success": true,
    "gs_authors": [
      "-kcZWRQAAAAJ",
      "zmbMZ4kAAAAJ",
      "YYT8-7kAAAAJ",
      "DipDDOwAAAAJ",
      "5RhJGKgAAAAJ"
    ],
    "citation_count": 31,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2304.10535",
    "title": "Farm3D: Learning Articulated 3D Animals by Distilling 2D Diffusion",
    "year": 2023,
    "published": "2023-04-20T17:59:34Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "We present Farm3D, a method for learning category-specific 3D reconstructors for articulated objects, relying solely on \"free\" virtual supervision from a pre-trained 2D diffusion-based image generator. Recent approaches can learn a monocular network that predicts the 3D shape, albedo, illumination, and viewpoint of any object occurrence, given a collection of single-view images of an object category. However, these approaches heavily rely on manually curated clean training data, which are expens",
    "arxiv_url": "https://arxiv.org/abs/2304.10535v3",
    "pdf_url": "https://arxiv.org/pdf/2304.10535v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2304.10535",
    "arxiv_authors": [
      "Tomas Jakab",
      "Ruining Li",
      "Shangzhe Wu",
      "Christian Rupprecht",
      "Andrea Vedaldi"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Farm3D%3A+Learning+Articulated+3D+Animals+by+Distilling+2D+Diffusion+Tomas+Jakab+Ruining+Li+Shangzhe+Wu+Christian+Rupprecht+Andrea+Vedaldi",
    "gs_search_success": true,
    "gs_authors": [
      "JmQkXLUAAAAJ",
      "bRT7t28AAAAJ",
      "-gFCCLMAAAAJ",
      "IrYlproAAAAJ",
      "36NmvrMAAAAJ"
    ],
    "citation_count": 24,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2408.15641",
    "title": "MMDRFuse: Distilled Mini-Model with Dynamic Refresh for Multi-Modality Image Fusion",
    "year": 2024,
    "published": "2024-08-28T08:52:33Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "In recent years, Multi-Modality Image Fusion (MMIF) has been applied to many fields, which has attracted many scholars to endeavour to improve the fusion performance. However, the prevailing focus has predominantly been on the architecture design, rather than the training strategies. As a low-level vision task, image fusion is supposed to quickly deliver output images for observation and supporting downstream tasks. Thus, superfluous computational and storage overheads should be avoided. In this",
    "arxiv_url": "https://arxiv.org/abs/2408.15641v1",
    "pdf_url": "https://arxiv.org/pdf/2408.15641v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2408.15641",
    "arxiv_authors": [
      "Yanglin Deng",
      "Tianyang Xu",
      "Chunyang Cheng",
      "Xiao-Jun Wu",
      "Josef Kittler"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=MMDRFuse%3A+Distilled+Mini-Model+with+Dynamic+Refresh+for+Multi-Modality+Image+Fusion+Yanglin+Deng+Tianyang+Xu+Chunyang+Cheng+Xiao-Jun+Wu+Josef+Kittler",
    "gs_search_success": true,
    "gs_authors": [
      "tkTfP6wAAAAJ",
      "pk-yb_kAAAAJ",
      "5IST34sAAAAJ",
      "bbSNrAgAAAAJ"
    ],
    "citation_count": 10,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2503.06361",
    "title": "Adversarial Robustness of Discriminative Self-Supervised Learning in Vision",
    "year": 2025,
    "published": "2025-03-08T23:50:36Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Self-supervised learning (SSL) has advanced significantly in visual representation learning, yet comprehensive evaluations of its adversarial robustness remain limited. In this study, we evaluate the adversarial robustness of seven discriminative self-supervised models and one supervised model across diverse tasks, including ImageNet classification, transfer learning, segmentation, and detection. Our findings suggest that discriminative SSL models generally exhibit better robustness to adversari",
    "arxiv_url": "https://arxiv.org/abs/2503.06361v2",
    "pdf_url": "https://arxiv.org/pdf/2503.06361v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2503.06361",
    "arxiv_authors": [
      "Ömer Veysel Çağatan",
      "Ömer Faruk Tal",
      "M. Emre Gürsoy"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Adversarial+Robustness+of+Discriminative+Self-Supervised+Learning+in+Vision+%C3%96mer+Veysel+%C3%87a%C4%9Fatan+%C3%96mer+Faruk+Tal+M.+Emre+G%C3%BCrsoy",
    "gs_search_success": true,
    "gs_authors": [
      "mkszHGsAAAAJ",
      "FpSkrNMAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2306.16979",
    "title": "Post-train Black-box Defense via Bayesian Boundary Correction",
    "year": 2023,
    "published": "2023-06-29T14:33:20Z",
    "categories": [
      "cs.CV",
      "cs.CR"
    ],
    "abstract": "Classifiers based on deep neural networks are susceptible to adversarial attack, where the widely existing vulnerability has invoked the research in defending them from potential threats. Given a vulnerable classifier, existing defense methods are mostly white-box and often require re-training the victim under modified loss functions/training regimes. While the model/data/training specifics of the victim are usually unavailable to the user, re-training is unappealing, if not impossible for reaso",
    "arxiv_url": "https://arxiv.org/abs/2306.16979v3",
    "pdf_url": "https://arxiv.org/pdf/2306.16979v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2306.16979",
    "arxiv_authors": [
      "He Wang",
      "Yunfeng Diao"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Post-train+Black-box+Defense+via+Bayesian+Boundary+Correction+He+Wang+Yunfeng+Diao",
    "gs_search_success": true,
    "gs_authors": [
      "apZHSYwAAAAJ",
      "BaaPAVYAAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 2
  },
  {
    "arxiv_id": "2404.07032",
    "title": "An Evidential-enhanced Tri-Branch Consistency Learning Method for Semi-supervised Medical Image Segmentation",
    "year": 2024,
    "published": "2024-04-10T14:25:23Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Semi-supervised segmentation presents a promising approach for large-scale medical image analysis, effectively reducing annotation burdens while achieving comparable performance. This methodology holds substantial potential for streamlining the segmentation process and enhancing its feasibility within clinical settings for translational investigations. While cross-supervised training, based on distinct co-training sub-networks, has become a prevalent paradigm for this task, addressing critical i",
    "arxiv_url": "https://arxiv.org/abs/2404.07032v1",
    "pdf_url": "https://arxiv.org/pdf/2404.07032v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2404.07032",
    "arxiv_authors": [
      "Zhenxi Zhang",
      "Heng Zhou",
      "Xiaoran Shi",
      "Ran Ran",
      "Chunna Tian",
      "Feng Zhou"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=An+Evidential-enhanced+Tri-Branch+Consistency+Learning+Method+for+Semi-supervised+Medical+Image+Segmentation+Zhenxi+Zhang+Heng+Zhou+Xiaoran+Shi+Ran+Ran+Chunna+Tian",
    "gs_search_success": true,
    "gs_authors": [
      "ZzPllmQAAAAJ",
      "sfcq7BcAAAAJ",
      "WFFSbNQAAAAJ"
    ],
    "citation_count": 29,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2408.09676",
    "title": "Image-based Freeform Handwriting Authentication with Energy-oriented Self-Supervised Learning",
    "year": 2024,
    "published": "2024-08-19T03:33:39Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Freeform handwriting authentication verifies a person's identity from their writing style and habits in messy handwriting data. This technique has gained widespread attention in recent years as a valuable tool for various fields, e.g., fraud prevention and cultural heritage protection. However, it still remains a challenging task in reality due to three reasons: (i) severe damage, (ii) complex high-dimensional features, and (iii) lack of supervision. To address these issues, we propose SherlockN",
    "arxiv_url": "https://arxiv.org/abs/2408.09676v1",
    "pdf_url": "https://arxiv.org/pdf/2408.09676v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2408.09676",
    "arxiv_authors": [
      "Jingyao Wang",
      "Luntian Mou",
      "Changwen Zheng",
      "Wen Gao"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Image-based+Freeform+Handwriting+Authentication+with+Energy-oriented+Self-Supervised+Learning+Jingyao+Wang+Luntian+Mou+Changwen+Zheng+Wen+Gao",
    "gs_search_success": true,
    "gs_authors": [
      "JciDCW0AAAAJ",
      "u9aw5o8AAAAJ",
      "-lErK1QAAAAJ",
      "btThEsYAAAAJ"
    ],
    "citation_count": 8,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2301.07306",
    "title": "Improve Noise Tolerance of Robust Loss via Noise-Awareness",
    "year": 2023,
    "published": "2023-01-18T04:54:58Z",
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "abstract": "Robust loss minimization is an important strategy for handling robust learning issue on noisy labels. Current approaches for designing robust losses involve the introduction of noise-robust factors, i.e., hyperparameters, to control the trade-off between noise robustness and learnability. However, finding suitable hyperparameters for different datasets with noisy labels is a challenging and time-consuming task. Moreover, existing robust loss methods usually assume that all training samples share",
    "arxiv_url": "https://arxiv.org/abs/2301.07306v2",
    "pdf_url": "https://arxiv.org/pdf/2301.07306v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2301.07306",
    "arxiv_authors": [
      "Kehui Ding",
      "Jun Shu",
      "Deyu Meng",
      "Zongben Xu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Improve+Noise+Tolerance+of+Robust+Loss+via+Noise-Awareness+Kehui+Ding+Jun+Shu+Deyu+Meng+Zongben+Xu",
    "gs_search_success": true,
    "gs_authors": [
      "an6w-64AAAAJ",
      "qnDOEV4AAAAJ"
    ],
    "citation_count": 13,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2412.16227",
    "title": "GALOT: Generative Active Learning via Optimizable Zero-shot Text-to-image Generation",
    "year": 2024,
    "published": "2024-12-18T18:40:21Z",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "Active Learning (AL) represents a crucial methodology within machine learning, emphasizing the identification and utilization of the most informative samples for efficient model training. However, a significant challenge of AL is its dependence on the limited labeled data samples and data distribution, resulting in limited performance. To address this limitation, this paper integrates the zero-shot text-to-image (T2I) synthesis and active learning by designing a novel framework that can efficien",
    "arxiv_url": "https://arxiv.org/abs/2412.16227v1",
    "pdf_url": "https://arxiv.org/pdf/2412.16227v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2412.16227",
    "arxiv_authors": [
      "Hanbin Hong",
      "Shenao Yan",
      "Shuya Feng",
      "Yan Yan",
      "Yuan Hong"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=GALOT%3A+Generative+Active+Learning+via+Optimizable+Zero-shot+Text-to-image+Generation+Hanbin+Hong+Shenao+Yan+Shuya+Feng+Yan+Yan+Yuan+Hong",
    "gs_search_success": true,
    "gs_authors": [
      "NW1XWCEAAAAJ",
      "KJuZW2wAAAAJ",
      "HpDG3SEAAAAJ",
      "sC8KKnsAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2307.08414",
    "title": "Active Learning for Object Detection with Non-Redundant Informative Sampling",
    "year": 2023,
    "published": "2023-07-17T11:55:20Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Curating an informative and representative dataset is essential for enhancing the performance of 2D object detectors. We present a novel active learning sampling strategy that addresses both the informativeness and diversity of the selections. Our strategy integrates uncertainty and diversity-based selection principles into a joint selection objective by measuring the collective information score of the selected samples. Specifically, our proposed NORIS algorithm quantifies the impact of trainin",
    "arxiv_url": "https://arxiv.org/abs/2307.08414v1",
    "pdf_url": "https://arxiv.org/pdf/2307.08414v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2307.08414",
    "arxiv_authors": [
      "Aral Hekimoglu",
      "Adrian Brucker",
      "Alper Kagan Kayali",
      "Michael Schmidt",
      "Alvaro Marcos-Ramiro"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Active+Learning+for+Object+Detection+with+Non-Redundant+Informative+Sampling+Aral+Hekimoglu+Adrian+Brucker+Alper+Kagan+Kayali+Michael+Schmidt+Alvaro+Marcos-Ramiro",
    "gs_search_success": true,
    "gs_authors": [
      "yH3ISGYAAAAJ",
      "5g18zasAAAAJ"
    ],
    "citation_count": 7,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2503.19851",
    "title": "Towards Online Multi-Modal Social Interaction Understanding",
    "year": 2025,
    "published": "2025-03-25T17:17:19Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Multimodal social interaction understanding (MMSI) is critical in human-robot interaction systems. In real-world scenarios, AI agents are required to provide real-time feedback. However, existing models often depend on both past and future contexts, which hinders them from applying to real-world problems. To bridge this gap, we propose an online MMSI setting, where the model must resolve MMSI tasks using only historical information, such as recorded dialogues and video streams. To address the ch",
    "arxiv_url": "https://arxiv.org/abs/2503.19851v1",
    "pdf_url": "https://arxiv.org/pdf/2503.19851v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2503.19851",
    "arxiv_authors": [
      "Xinpeng Li",
      "Shijian Deng",
      "Bolin Lai",
      "Weiguo Pian",
      "James M. Rehg",
      "Yapeng Tian"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Towards+Online+Multi-Modal+Social+Interaction+Understanding+Xinpeng+Li+Shijian+Deng+Bolin+Lai+Weiguo+Pian+James+M.+Rehg",
    "gs_search_success": true,
    "gs_authors": [
      "K-ObTwoAAAAJ",
      "7LBj70IAAAAJ",
      "lWrljmQAAAAJ",
      "8kA3eDwAAAAJ",
      "lxCqdpoAAAAJ",
      "59fdU3wAAAAJ"
    ],
    "citation_count": 5,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2505.05659",
    "title": "V-EfficientNets: Vector-Valued Efficiently Scaled Convolutional Neural Network Models",
    "year": 2025,
    "published": "2025-05-08T21:35:35Z",
    "categories": [
      "eess.IV",
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "EfficientNet models are convolutional neural networks optimized for parameter allocation by jointly balancing network width, depth, and resolution. Renowned for their exceptional accuracy, these models have become a standard for image classification tasks across diverse computer vision benchmarks. While traditional neural networks learn correlations between feature channels during training, vector-valued neural networks inherently treat multidimensional data as coherent entities, taking for gran",
    "arxiv_url": "https://arxiv.org/abs/2505.05659v1",
    "pdf_url": "https://arxiv.org/pdf/2505.05659v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2505.05659",
    "arxiv_authors": [
      "Guilherme Vieira Neto",
      "Marcos Eduardo Valle"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=V-EfficientNets%3A+Vector-Valued+Efficiently+Scaled+Convolutional+Neural+Network+Models+Guilherme+Vieira+Neto+Marcos+Eduardo+Valle",
    "gs_search_success": true,
    "gs_authors": [
      "uBM-MIkAAAAJ",
      "7tJYwNIAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 2
  },
  {
    "arxiv_id": "2302.01334",
    "title": "STEPS: Joint Self-supervised Nighttime Image Enhancement and Depth Estimation",
    "year": 2023,
    "published": "2023-02-02T18:59:47Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Self-supervised depth estimation draws a lot of attention recently as it can promote the 3D sensing capabilities of self-driving vehicles. However, it intrinsically relies upon the photometric consistency assumption, which hardly holds during nighttime. Although various supervised nighttime image enhancement methods have been proposed, their generalization performance in challenging driving scenarios is not satisfactory. To this end, we propose the first method that jointly learns a nighttime im",
    "arxiv_url": "https://arxiv.org/abs/2302.01334v1",
    "pdf_url": "https://arxiv.org/pdf/2302.01334v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2302.01334",
    "arxiv_authors": [
      "Yupeng Zheng",
      "Chengliang Zhong",
      "Pengfei Li",
      "Huan-ang Gao",
      "Yuhang Zheng",
      "Bu Jin",
      "Ling Wang",
      "Hao Zhao",
      "Guyue Zhou",
      "Qichao Zhang",
      "Dongbin Zhao"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=STEPS%3A+Joint+Self-supervised+Nighttime+Image+Enhancement+and+Depth+Estimation+Yupeng+Zheng+Chengliang+Zhong+Pengfei+Li+Huan-ang+Gao+Yuhang+Zheng",
    "gs_search_success": true,
    "gs_authors": [
      "snkECPAAAAAJ",
      "ygQznUQAAAAJ",
      "anGhGdYAAAAJ",
      "RxvYlNQAAAAJ",
      "WvbKfLgAAAAJ",
      "q-YZG78AAAAJ",
      "Wn2Aic0AAAAJ",
      "uUd5v2cAAAAJ"
    ],
    "citation_count": 64,
    "gs_author_count": 9
  },
  {
    "arxiv_id": "2403.09108",
    "title": "CardioCaps: Attention-based Capsule Network for Class-Imbalanced Echocardiogram Classification",
    "year": 2024,
    "published": "2024-03-14T05:01:31Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Capsule Neural Networks (CapsNets) is a novel architecture that utilizes vector-wise representations formed by multiple neurons. Specifically, the Dynamic Routing CapsNets (DR-CapsNets) employ an affine matrix and dynamic routing mechanism to train capsules and acquire translation-equivariance properties, enhancing its robustness compared to traditional Convolutional Neural Networks (CNNs). Echocardiograms, which capture moving images of the heart, present unique challenges for traditional image",
    "arxiv_url": "https://arxiv.org/abs/2403.09108v2",
    "pdf_url": "https://arxiv.org/pdf/2403.09108v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2403.09108",
    "arxiv_authors": [
      "Hyunkyung Han",
      "Jihyeon Seong",
      "Jaesik Choi"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=CardioCaps%3A+Attention-based+Capsule+Network+for+Class-Imbalanced+Echocardiogram+Classification+Hyunkyung+Han+Jihyeon+Seong+Jaesik+Choi",
    "gs_search_success": true,
    "gs_authors": [
      "DdSo9q4AAAAJ",
      "nQFtd5MAAAAJ",
      "RqMLVzUAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2408.12934",
    "title": "WildFusion: Individual Animal Identification with Calibrated Similarity Fusion",
    "year": 2024,
    "published": "2024-08-23T09:30:11Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "We propose a new method - WildFusion - for individual identification of a broad range of animal species. The method fuses deep scores (e.g., MegaDescriptor or DINOv2) and local matching similarity (e.g., LoFTR and LightGlue) to identify individual animals. The global and local information fusion is facilitated by similarity score calibration. In a zero-shot setting, relying on local similarity score only, WildFusion achieved mean accuracy, measured on 17 datasets, of 76.2%. This is better than t",
    "arxiv_url": "https://arxiv.org/abs/2408.12934v1",
    "pdf_url": "https://arxiv.org/pdf/2408.12934v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2408.12934",
    "arxiv_authors": [
      "Vojtěch Cermak",
      "Lukas Picek",
      "Lukáš Adam",
      "Lukáš Neumann",
      "Jiří Matas"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=WildFusion%3A+Individual+Animal+Identification+with+Calibrated+Similarity+Fusion+Vojt%C4%9Bch+Cermak+Lukas+Picek+Luk%C3%A1%C5%A1+Adam+Luk%C3%A1%C5%A1+Neumann+Ji%C5%99%C3%AD+Matas",
    "gs_search_success": true,
    "gs_authors": [
      "EJCNY6QAAAAJ",
      "SBifPtYAAAAJ",
      "9sNvmo4AAAAJ",
      "xhxJGGcAAAAJ",
      "PwPyV_0AAAAJ"
    ],
    "citation_count": 14,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2304.09373",
    "title": "Multi-scale Adaptive Fusion Network for Hyperspectral Image Denoising",
    "year": 2023,
    "published": "2023-04-19T02:00:21Z",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "abstract": "Removing the noise and improving the visual quality of hyperspectral images (HSIs) is challenging in academia and industry. Great efforts have been made to leverage local, global or spectral context information for HSI denoising. However, existing methods still have limitations in feature interaction exploitation among multiple scales and rich spectral structure preservation. In view of this, we propose a novel solution to investigate the HSI denoising using a Multi-scale Adaptive Fusion Network",
    "arxiv_url": "https://arxiv.org/abs/2304.09373v1",
    "pdf_url": "https://arxiv.org/pdf/2304.09373v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2304.09373",
    "arxiv_authors": [
      "Haodong Pan",
      "Feng Gao",
      "Junyu Dong",
      "Qian Du"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Multi-scale+Adaptive+Fusion+Network+for+Hyperspectral+Image+Denoising+Haodong+Pan+Feng+Gao+Junyu+Dong+Qian+Du",
    "gs_search_success": true,
    "gs_authors": [
      "iPYdUpAAAAAJ",
      "k91CLXQAAAAJ",
      "0OdKQoQAAAAJ"
    ],
    "citation_count": 29,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2310.16148",
    "title": "Yin Yang Convolutional Nets: Image Manifold Extraction by the Analysis of Opposites",
    "year": 2023,
    "published": "2023-10-24T19:48:07Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Computer vision in general presented several advances such as training optimizations, new architectures (pure attention, efficient block, vision language models, generative models, among others). This have improved performance in several tasks such as classification, and others. However, the majority of these models focus on modifications that are taking distance from realistic neuroscientific approaches related to the brain. In this work, we adopt a more bio-inspired approach and present the Yi",
    "arxiv_url": "https://arxiv.org/abs/2310.16148v1",
    "pdf_url": "https://arxiv.org/pdf/2310.16148v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2310.16148",
    "arxiv_authors": [
      "Augusto Seben da Rosa",
      "Frederico Santos de Oliveira",
      "Anderson da Silva Soares",
      "Arnaldo Candido Junior"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Yin+Yang+Convolutional+Nets%3A+Image+Manifold+Extraction+by+the+Analysis+of+Opposites+Augusto+Seben+da+Rosa+Frederico+Santos+de+Oliveira+Anderson+da+Silva+Soares+Arnaldo+Candido+Junior",
    "gs_search_success": true,
    "gs_authors": [],
    "citation_count": null,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2312.04671",
    "title": "The automatic detection of lumber anatomy in epidural injections for ultrasound guidance",
    "year": 2023,
    "published": "2023-12-07T20:11:36Z",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "abstract": "The purpose of this paper is to help the anesthesiologist to find the epidural depth automatically to make the first attempt to enter the path of the needle into the patient's body while it is clogged with bone and avoid causing a puncture in the surrounding areas of the patient`s back. In this regard, a morphology-based bone enhancement and detection followed by a Ramer-Douglas-Peucker algorithm and Hough transform is proposed. The proposed algorithm is tested on synthetic and real ultrasound i",
    "arxiv_url": "https://arxiv.org/abs/2312.04671v1",
    "pdf_url": "https://arxiv.org/pdf/2312.04671v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2312.04671",
    "arxiv_authors": [
      "Farhad Piri",
      "Sima Sobhiyeh",
      "Amir H. Rezaie",
      "Faramarz Mosaffa"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=The+automatic+detection+of+lumber+anatomy+in+epidural+injections+for+ultrasound+guidance+Farhad+Piri+Sima+Sobhiyeh+Amir+H.+Rezaie+Faramarz+Mosaffa",
    "gs_search_success": true,
    "gs_authors": [
      "AuwNzRAAAAAJ",
      "UgAwshMAAAAJ",
      "lpQc0HgAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2503.17069",
    "title": "PVChat: Personalized Video Chat with One-Shot Learning",
    "year": 2025,
    "published": "2025-03-21T11:50:06Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Video large language models (ViLLMs) excel in general video understanding, e.g., recognizing activities like talking and eating, but struggle with identity-aware comprehension, such as \"Wilson is receiving chemotherapy\" or \"Tom is discussing with Sarah\", limiting their applicability in smart healthcare and smart home environments. To address this limitation, we propose a one-shot learning framework PVChat, the first personalized ViLLM that enables subject-aware question answering (QA) from a sin",
    "arxiv_url": "https://arxiv.org/abs/2503.17069v4",
    "pdf_url": "https://arxiv.org/pdf/2503.17069v4",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2503.17069",
    "arxiv_authors": [
      "Yufei Shi",
      "Weilong Yan",
      "Gang Xu",
      "Yumeng Li",
      "Yucheng Chen",
      "Zhenxi Li",
      "Fei Richard Yu",
      "Ming Li",
      "Si Yong Yeo"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=PVChat%3A+Personalized+Video+Chat+with+One-Shot+Learning+Yufei+Shi+Weilong+Yan+Gang+Xu+Yumeng+Li+Yucheng+Chen",
    "gs_search_success": true,
    "gs_authors": [
      "iVLZexoAAAAJ",
      "zuGMGBoAAAAJ",
      "2wySPkcAAAAJ",
      "etAbMbAAAAAJ",
      "rpnlkwEAAAAJ",
      "t5wecEMAAAAJ",
      "Pnj4k5MAAAAJ",
      "c__9eLUAAAAJ"
    ],
    "citation_count": 2,
    "gs_author_count": 9
  },
  {
    "arxiv_id": "2408.11576",
    "title": "RaNDT SLAM: Radar SLAM Based on Intensity-Augmented Normal Distributions Transform",
    "year": 2024,
    "published": "2024-08-21T12:32:11Z",
    "categories": [
      "cs.RO",
      "cs.CV",
      "eess.SP"
    ],
    "abstract": "Rescue robotics sets high requirements to perception algorithms due to the unstructured and potentially vision-denied environments. Pivoting Frequency-Modulated Continuous Wave radars are an emerging sensing modality for SLAM in this kind of environment. However, the complex noise characteristics of radar SLAM makes, particularly indoor, applications computationally demanding and slow. In this work, we introduce a novel radar SLAM framework, RaNDT SLAM, that operates fast and generates accurate ",
    "arxiv_url": "https://arxiv.org/abs/2408.11576v1",
    "pdf_url": "https://arxiv.org/pdf/2408.11576v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2408.11576",
    "arxiv_authors": [
      "Maximilian Hilger",
      "Nils Mandischer",
      "Burkhard Corves"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=RaNDT+SLAM%3A+Radar+SLAM+Based+on+Intensity-Augmented+Normal+Distributions+Transform+Maximilian+Hilger+Nils+Mandischer+Burkhard+Corves",
    "gs_search_success": true,
    "gs_authors": [
      "oQEw8bsAAAAJ",
      "svvsOWwAAAAJ",
      "mFvcwSIAAAAJ"
    ],
    "citation_count": 2,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2405.11978",
    "title": "SM-DTW: Stability Modulated Dynamic Time Warping for signature verification",
    "year": 2024,
    "published": "2024-05-20T12:18:15Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Building upon findings in computational model of handwriting learning and execution, we introduce the concept of stability to explain the difference between the actual movements performed during multiple execution of the subject's signature, and conjecture that the most stable parts of the signature should play a paramount role in evaluating the similarity between a questioned signature and the reference ones during signature verification. We then introduce the Stability Modulated Dynamic Time W",
    "arxiv_url": "https://arxiv.org/abs/2405.11978v1",
    "pdf_url": "https://arxiv.org/pdf/2405.11978v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2405.11978",
    "arxiv_authors": [
      "Antonio Parziale",
      "Moises Diaz",
      "Miguel A. Ferrer",
      "Angelo Marcelli"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=SM-DTW%3A+Stability+Modulated+Dynamic+Time+Warping+for+signature+verification+Antonio+Parziale+Moises+Diaz+Miguel+A.+Ferrer+Angelo+Marcelli",
    "gs_search_success": true,
    "gs_authors": [
      "Me378dAAAAAJ",
      "IFINq2QAAAAJ",
      "XMsVDiQnzQMC"
    ],
    "citation_count": 77,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2405.12328",
    "title": "Multi-dimension Transformer with Attention-based Filtering for Medical Image Segmentation",
    "year": 2024,
    "published": "2024-05-20T18:52:41Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "The accurate segmentation of medical images is crucial for diagnosing and treating diseases. Recent studies demonstrate that vision transformer-based methods have significantly improved performance in medical image segmentation, primarily due to their superior ability to establish global relationships among features and adaptability to various inputs. However, these methods struggle with the low signal-to-noise ratio inherent to medical images. Additionally, the effective utilization of channel ",
    "arxiv_url": "https://arxiv.org/abs/2405.12328v1",
    "pdf_url": "https://arxiv.org/pdf/2405.12328v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2405.12328",
    "arxiv_authors": [
      "Wentao Wang",
      "Xi Xiao",
      "Mingjie Liu",
      "Qing Tian",
      "Xuanyao Huang",
      "Qizhen Lan",
      "Swalpa Kumar Roy",
      "Tianyang Wang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Multi-dimension+Transformer+with+Attention-based+Filtering+for+Medical+Image+Segmentation+Wentao+Wang+Xi+Xiao+Mingjie+Liu+Qing+Tian+Xuanyao+Huang",
    "gs_search_success": true,
    "gs_authors": [
      "Q4DnRVgAAAAJ",
      "QbTV0r0AAAAJ",
      "hz8cb2AAAAAJ",
      "1WVrFGwAAAAJ",
      "gHuVioYAAAAJ"
    ],
    "citation_count": 8,
    "gs_author_count": 8
  },
  {
    "arxiv_id": "2311.15171",
    "title": "HumanRecon: Neural Reconstruction of Dynamic Human Using Geometric Cues and Physical Priors",
    "year": 2023,
    "published": "2023-11-26T03:06:59Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Recent methods for dynamic human reconstruction have attained promising reconstruction results. Most of these methods rely only on RGB color supervision without considering explicit geometric constraints. This leads to existing human reconstruction techniques being more prone to overfitting to color and causes geometrically inherent ambiguities, especially in the sparse multi-view setup.   Motivated by recent advances in the field of monocular geometry prediction, we consider the geometric const",
    "arxiv_url": "https://arxiv.org/abs/2311.15171v1",
    "pdf_url": "https://arxiv.org/pdf/2311.15171v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2311.15171",
    "arxiv_authors": [
      "Junhui Yin",
      "Wei Yin",
      "Hao Chen",
      "Xuqian Ren",
      "Zhanyu Ma",
      "Jun Guo",
      "Yifan Liu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=HumanRecon%3A+Neural+Reconstruction+of+Dynamic+Human+Using+Geometric+Cues+and+Physical+Priors+Junhui+Yin+Wei+Yin+Hao+Chen+Xuqian+Ren+Zhanyu+Ma",
    "gs_search_success": true,
    "gs_authors": [
      "5GAAs7IAAAAJ",
      "1j8cfOQAAAAJ",
      "ZIf_rtcAAAAJ"
    ],
    "citation_count": 2,
    "gs_author_count": 7
  },
  {
    "arxiv_id": "2308.08812",
    "title": "A Fusion of Variational Distribution Priors and Saliency Map Replay for Continual 3D Reconstruction",
    "year": 2023,
    "published": "2023-08-17T06:48:55Z",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "Single-image 3D reconstruction is a research challenge focused on predicting 3D object shapes from single-view images. This task requires significant data acquisition to predict both visible and occluded portions of the shape. Furthermore, learning-based methods face the difficulty of creating a comprehensive training dataset for all possible classes. To this end, we propose a continual learning-based 3D reconstruction method where our goal is to design a model using Variational Priors that can ",
    "arxiv_url": "https://arxiv.org/abs/2308.08812v2",
    "pdf_url": "https://arxiv.org/pdf/2308.08812v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2308.08812",
    "arxiv_authors": [
      "Sanchar Palit",
      "Sandika Biswas"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=A+Fusion+of+Variational+Distribution+Priors+and+Saliency+Map+Replay+for+Continual+3D+Reconstruction+Sanchar+Palit+Sandika+Biswas",
    "gs_search_success": true,
    "gs_authors": [
      "nme4ze8AAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 2
  },
  {
    "arxiv_id": "2404.05490",
    "title": "Two-Person Interaction Augmentation with Skeleton Priors",
    "year": 2024,
    "published": "2024-04-08T13:11:57Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Close and continuous interaction with rich contacts is a crucial aspect of human activities (e.g. hugging, dancing) and of interest in many domains like activity recognition, motion prediction, character animation, etc. However, acquiring such skeletal motion is challenging. While direct motion capture is expensive and slow, motion editing/generation is also non-trivial, as complex contact patterns with topological and geometric constraints have to be retained. To this end, we propose a new deep",
    "arxiv_url": "https://arxiv.org/abs/2404.05490v2",
    "pdf_url": "https://arxiv.org/pdf/2404.05490v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2404.05490",
    "arxiv_authors": [
      "Baiyi Li",
      "Edmond S. L. Ho",
      "Hubert P. H. Shum",
      "He Wang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Two-Person+Interaction+Augmentation+with+Skeleton+Priors+Baiyi+Li+Edmond+S.+L.+Ho+Hubert+P.+H.+Shum+He+Wang",
    "gs_search_success": true,
    "gs_authors": [
      "BaaPAVYAAAAJ",
      "Py1-xgMAAAAJ",
      "pkPLCEYAAAAJ"
    ],
    "citation_count": 8,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2504.14933",
    "title": "TWIG: Two-Step Image Generation using Segmentation Masks in Diffusion Models",
    "year": 2025,
    "published": "2025-04-21T07:53:58Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "In today's age of social media and marketing, copyright issues can be a major roadblock to the free sharing of images. Generative AI models have made it possible to create high-quality images, but concerns about copyright infringement are a hindrance to their abundant use. As these models use data from training images to generate new ones, it is often a daunting task to ensure they do not violate intellectual property rights. Some AI models have even been noted to directly copy copyrighted image",
    "arxiv_url": "https://arxiv.org/abs/2504.14933v2",
    "pdf_url": "https://arxiv.org/pdf/2504.14933v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2504.14933",
    "arxiv_authors": [
      "Mazharul Islam Rakib",
      "Showrin Rahman",
      "Joyanta Jyoti Mondal",
      "Xi Xiao",
      "David Lewis",
      "Alessandra Mileo",
      "Meem Arafat Manab"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=TWIG%3A+Two-Step+Image+Generation+using+Segmentation+Masks+in+Diffusion+Models+Mazharul+Islam+Rakib+Showrin+Rahman+Joyanta+Jyoti+Mondal+Xi+Xiao+David+Lewis",
    "gs_search_success": true,
    "gs_authors": [
      "lIwU1F0AAAAJ",
      "Uc5np_EAAAAJ",
      "Q4DnRVgAAAAJ",
      "4nlxG2IAAAAJ",
      "ZkQuEgMAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 7
  },
  {
    "arxiv_id": "2405.18449",
    "title": "Adaptive Multiscale Retinal Diagnosis: A Hybrid Trio-Model Approach for Comprehensive Fundus Multi-Disease Detection Leveraging Transfer Learning and Siamese Networks",
    "year": 2024,
    "published": "2024-05-28T03:06:10Z",
    "categories": [
      "eess.IV",
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "WHO has declared that more than 2.2 billion people worldwide are suffering from visual disorders, such as media haze, glaucoma, and drusen. At least 1 billion of these cases could have been either prevented or successfully treated, yet they remain unaddressed due to poverty, a lack of specialists, inaccurate ocular fundus diagnoses by ophthalmologists, or the presence of a rare disease. To address this, the research has developed the Hybrid Trio-Network Model Algorithm for accurately diagnosing ",
    "arxiv_url": "https://arxiv.org/abs/2405.18449v1",
    "pdf_url": "https://arxiv.org/pdf/2405.18449v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2405.18449",
    "arxiv_authors": [
      "Yavuz Selim Inan"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Adaptive+Multiscale+Retinal+Diagnosis%3A+A+Hybrid+Trio-Model+Approach+for+Comprehensive+Fundus+Multi-Disease+Detection+Leveraging+Transfer+Learning+and+Siamese+Networks+Yavuz+Selim+Inan",
    "gs_search_success": true,
    "gs_authors": [],
    "citation_count": 7,
    "gs_author_count": 1
  },
  {
    "arxiv_id": "2302.14402",
    "title": "Neural Video Compression with Diverse Contexts",
    "year": 2023,
    "published": "2023-02-28T08:35:50Z",
    "categories": [
      "eess.IV",
      "cs.CV",
      "cs.MM"
    ],
    "abstract": "For any video codecs, the coding efficiency highly relies on whether the current signal to be encoded can find the relevant contexts from the previous reconstructed signals. Traditional codec has verified more contexts bring substantial coding gain, but in a time-consuming manner. However, for the emerging neural video codec (NVC), its contexts are still limited, leading to low compression ratio. To boost NVC, this paper proposes increasing the context diversity in both temporal and spatial dime",
    "arxiv_url": "https://arxiv.org/abs/2302.14402v3",
    "pdf_url": "https://arxiv.org/pdf/2302.14402v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2302.14402",
    "arxiv_authors": [
      "Jiahao Li",
      "Bin Li",
      "Yan Lu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Neural+Video+Compression+with+Diverse+Contexts+Jiahao+Li+Bin+Li+Yan+Lu",
    "gs_search_success": false,
    "gs_authors": [],
    "citation_count": null,
    "error_type": "fetch_failed",
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2407.00592",
    "title": "Unveiling Glitches: A Deep Dive into Image Encoding Bugs within CLIP",
    "year": 2024,
    "published": "2024-06-30T05:23:11Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Understanding the limitations and weaknesses of state-of-the-art models in artificial intelligence is crucial for their improvement and responsible application. In this research, we focus on CLIP, a model renowned for its integration of vision and language processing. Our objective is to uncover recurring problems and blind spots in CLIP's image comprehension. By delving into both the commonalities and disparities between CLIP and human image understanding, we augment our comprehension of these ",
    "arxiv_url": "https://arxiv.org/abs/2407.00592v1",
    "pdf_url": "https://arxiv.org/pdf/2407.00592v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2407.00592",
    "arxiv_authors": [
      "Ayush Ranjan",
      "Daniel Wen",
      "Karthik Bhat"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Unveiling+Glitches%3A+A+Deep+Dive+into+Image+Encoding+Bugs+within+CLIP+Ayush+Ranjan+Daniel+Wen+Karthik+Bhat",
    "gs_search_success": false,
    "gs_authors": [],
    "citation_count": null,
    "error_type": "fetch_failed",
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2402.18181",
    "title": "CFDNet: A Generalizable Foggy Stereo Matching Network with Contrastive Feature Distillation",
    "year": 2024,
    "published": "2024-02-28T09:12:01Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Stereo matching under foggy scenes remains a challenging task since the scattering effect degrades the visibility and results in less distinctive features for dense correspondence matching. While some previous learning-based methods integrated a physical scattering function for simultaneous stereo-matching and dehazing, simply removing fog might not aid depth estimation because the fog itself can provide crucial depth cues. In this work, we introduce a framework based on contrastive feature dist",
    "arxiv_url": "https://arxiv.org/abs/2402.18181v2",
    "pdf_url": "https://arxiv.org/pdf/2402.18181v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2402.18181",
    "arxiv_authors": [
      "Zihua Liu",
      "Yizhou Li",
      "Masatoshi Okutomi"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=CFDNet%3A+A+Generalizable+Foggy+Stereo+Matching+Network+with+Contrastive+Feature+Distillation+Zihua+Liu+Yizhou+Li+Masatoshi+Okutomi",
    "gs_search_success": true,
    "gs_authors": [
      "5GVxjvIAAAAJ",
      "zcKv1JQAAAAJ",
      "bri3qqcAAAAJ"
    ],
    "citation_count": 2,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2409.02676",
    "title": "Improved Single Camera BEV Perception Using Multi-Camera Training",
    "year": 2024,
    "published": "2024-09-04T13:06:40Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Bird's Eye View (BEV) map prediction is essential for downstream autonomous driving tasks like trajectory prediction. In the past, this was accomplished through the use of a sophisticated sensor configuration that captured a surround view from multiple cameras. However, in large-scale production, cost efficiency is an optimization goal, so that using fewer cameras becomes more relevant. But the consequence of fewer input images correlates with a performance drop. This raises the problem of devel",
    "arxiv_url": "https://arxiv.org/abs/2409.02676v1",
    "pdf_url": "https://arxiv.org/pdf/2409.02676v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2409.02676",
    "arxiv_authors": [
      "Daniel Busch",
      "Ido Freeman",
      "Richard Meyes",
      "Tobias Meisen"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Improved+Single+Camera+BEV+Perception+Using+Multi-Camera+Training+Daniel+Busch+Ido+Freeman+Richard+Meyes+Tobias+Meisen",
    "gs_search_success": true,
    "gs_authors": [
      "fSmbntoAAAAJ",
      "hi1dy_4AAAAJ",
      "HtzoWrIAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2309.02067",
    "title": "Histograms of Points, Orientations, and Dynamics of Orientations Features for Hindi Online Handwritten Character Recognition",
    "year": 2023,
    "published": "2023-09-05T09:11:18Z",
    "categories": [
      "cs.CV",
      "eess.SP"
    ],
    "abstract": "A set of features independent of character stroke direction and order variations is proposed for online handwritten character recognition. A method is developed that maps features like co-ordinates of points, orientations of strokes at points, and dynamics of orientations of strokes at points spatially as a function of co-ordinate values of the points and computes histograms of these features from different regions in the spatial map.   Different features like spatio-temporal, discrete Fourier t",
    "arxiv_url": "https://arxiv.org/abs/2309.02067v1",
    "pdf_url": "https://arxiv.org/pdf/2309.02067v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2309.02067",
    "arxiv_authors": [
      "Anand Sharma",
      "A. G. Ramakrishnan"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Histograms+of+Points%2C+Orientations%2C+and+Dynamics+of+Orientations+Features+for+Hindi+Online+Handwritten+Character+Recognition+Anand+Sharma+A.+G.+Ramakrishnan",
    "gs_search_success": true,
    "gs_authors": [
      "IPjx_EIAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 2
  },
  {
    "arxiv_id": "2402.12292",
    "title": "Regularization by denoising: Bayesian model and Langevin-within-split Gibbs sampling",
    "year": 2024,
    "published": "2024-02-19T17:12:16Z",
    "categories": [
      "stat.ML",
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "This paper introduces a Bayesian framework for image inversion by deriving a probabilistic counterpart to the regularization-by-denoising (RED) paradigm. It additionally implements a Monte Carlo algorithm specifically tailored for sampling from the resulting posterior distribution, based on an asymptotically exact data augmentation (AXDA). The proposed algorithm is an approximate instance of split Gibbs sampling (SGS) which embeds one Langevin Monte Carlo step. The proposed method is applied to ",
    "arxiv_url": "https://arxiv.org/abs/2402.12292v1",
    "pdf_url": "https://arxiv.org/pdf/2402.12292v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2402.12292",
    "arxiv_authors": [
      "Elhadji C. Faye",
      "Mame Diarra Fall",
      "Nicolas Dobigeon"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Regularization+by+denoising%3A+Bayesian+model+and+Langevin-within-split+Gibbs+sampling+Elhadji+C.+Faye+Mame+Diarra+Fall+Nicolas+Dobigeon",
    "gs_search_success": true,
    "gs_authors": [
      "m-y3JawAAAAJ",
      "8XiqilYAAAAJ",
      "YPQFwDYAAAAJ"
    ],
    "citation_count": 8,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2409.13626",
    "title": "Improved Unet brain tumor image segmentation based on GSConv module and ECA attention mechanism",
    "year": 2024,
    "published": "2024-09-20T16:35:19Z",
    "categories": [
      "eess.IV",
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "An improved model of medical image segmentation for brain tumor is discussed, which is a deep learning algorithm based on U-Net architecture. Based on the traditional U-Net, we introduce GSConv module and ECA attention mechanism to improve the performance of the model in medical image segmentation tasks. With these improvements, the new U-Net model is able to extract and utilize multi-scale features more efficiently while flexibly focusing on important channels, resulting in significantly improv",
    "arxiv_url": "https://arxiv.org/abs/2409.13626v1",
    "pdf_url": "https://arxiv.org/pdf/2409.13626v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2409.13626",
    "arxiv_authors": [
      "Qiyuan Tian",
      "Zhuoyue Wang",
      "Xiaoling Cui"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Improved+Unet+brain+tumor+image+segmentation+based+on+GSConv+module+and+ECA+attention+mechanism+Qiyuan+Tian+Zhuoyue+Wang+Xiaoling+Cui",
    "gs_search_success": true,
    "gs_authors": [],
    "citation_count": 48,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2406.02534",
    "title": "Enhancing predictive imaging biomarker discovery through treatment effect analysis",
    "year": 2024,
    "published": "2024-06-04T17:54:44Z",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "Identifying predictive covariates, which forecast individual treatment effectiveness, is crucial for decision-making across different disciplines such as personalized medicine. These covariates, referred to as biomarkers, are extracted from pre-treatment data, often within randomized controlled trials, and should be distinguished from prognostic biomarkers, which are independent of treatment assignment. Our study focuses on discovering predictive imaging biomarkers, specific image features, by l",
    "arxiv_url": "https://arxiv.org/abs/2406.02534v2",
    "pdf_url": "https://arxiv.org/pdf/2406.02534v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2406.02534",
    "arxiv_authors": [
      "Shuhan Xiao",
      "Lukas Klein",
      "Jens Petersen",
      "Philipp Vollmuth",
      "Paul F. Jaeger",
      "Klaus H. Maier-Hein"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Enhancing+predictive+imaging+biomarker+discovery+through+treatment+effect+analysis+Shuhan+Xiao+Lukas+Klein+Jens+Petersen+Philipp+Vollmuth+Paul+F.+Jaeger",
    "gs_search_success": true,
    "gs_authors": [
      "sxpuZg4AAAAJ",
      "iz99XU4AAAAJ",
      "9B9-8h0AAAAJ",
      "oCrBpVMAAAAJ",
      "z0ENYEQAAAAJ",
      "1jelkc4AAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2311.15963",
    "title": "From Pixels to Titles: Video Game Identification by Screenshots using Convolutional Neural Networks",
    "year": 2023,
    "published": "2023-11-27T16:07:34Z",
    "categories": [
      "cs.CV",
      "cs.NE"
    ],
    "abstract": "This paper investigates video game identification through single screenshots, utilizing ten convolutional neural network (CNN) architectures (VGG16, ResNet50, ResNet152, MobileNet, DenseNet169, DenseNet201, EfficientNetB0, EfficientNetB2, EfficientNetB3, and EfficientNetV2S) and three transformers architectures (ViT-B16, ViT-L32, and SwinT) across 22 home console systems, spanning from Atari 2600 to PlayStation 5, totalling 8,796 games and 170,881 screenshots. Except for VGG16, all CNNs outperfo",
    "arxiv_url": "https://arxiv.org/abs/2311.15963v3",
    "pdf_url": "https://arxiv.org/pdf/2311.15963v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2311.15963",
    "arxiv_authors": [
      "Fabricio Breve"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=From+Pixels+to+Titles%3A+Video+Game+Identification+by+Screenshots+using+Convolutional+Neural+Networks+Fabricio+Breve",
    "gs_search_success": true,
    "gs_authors": [
      "_Ec-wvsAAAAJ"
    ],
    "citation_count": 3,
    "gs_author_count": 1
  },
  {
    "arxiv_id": "2407.09797",
    "title": "ScaleFlow++: Robust and Accurate Estimation of 3D Motion from Video",
    "year": 2024,
    "published": "2024-07-13T07:58:48Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Perceiving and understanding 3D motion is a core technology in fields such as autonomous driving, robots, and motion prediction. This paper proposes a 3D motion perception method called ScaleFlow++ that is easy to generalize. With just a pair of RGB images, ScaleFlow++ can robustly estimate optical flow and motion-in-depth (MID). Most existing methods directly regress MID from two RGB frames or optical flow, resulting in inaccurate and unstable results. Our key insight is cross-scale matching, w",
    "arxiv_url": "https://arxiv.org/abs/2407.09797v2",
    "pdf_url": "https://arxiv.org/pdf/2407.09797v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2407.09797",
    "arxiv_authors": [
      "Han Ling",
      "Quansen Sun"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=ScaleFlow%2B%2B%3A+Robust+and+Accurate+Estimation+of+3D+Motion+from+Video+Han+Ling+Quansen+Sun",
    "gs_search_success": true,
    "gs_authors": [],
    "citation_count": 2,
    "gs_author_count": 2
  },
  {
    "arxiv_id": "2409.02543",
    "title": "StyleTokenizer: Defining Image Style by a Single Instance for Controlling Diffusion Models",
    "year": 2024,
    "published": "2024-09-04T09:01:21Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Despite the burst of innovative methods for controlling the diffusion process, effectively controlling image styles in text-to-image generation remains a challenging task. Many adapter-based methods impose image representation conditions on the denoising process to accomplish image control. However these conditions are not aligned with the word embedding space, leading to interference between image and text control conditions and the potential loss of semantic information from the text prompt. A",
    "arxiv_url": "https://arxiv.org/abs/2409.02543v1",
    "pdf_url": "https://arxiv.org/pdf/2409.02543v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2409.02543",
    "arxiv_authors": [
      "Wen Li",
      "Muyuan Fang",
      "Cheng Zou",
      "Biao Gong",
      "Ruobing Zheng",
      "Meng Wang",
      "Jingdong Chen",
      "Ming Yang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=StyleTokenizer%3A+Defining+Image+Style+by+a+Single+Instance+for+Controlling+Diffusion+Models+Wen+Li+Muyuan+Fang+Cheng+Zou+Biao+Gong+Ruobing+Zheng",
    "gs_search_success": true,
    "gs_authors": [
      "8SCEv-YAAAAJ",
      "S8FmqTUAAAAJ",
      "xjVTx4UAAAAJ",
      "BwdpTiQAAAAJ",
      "uBHJx08AAAAJ"
    ],
    "citation_count": 17,
    "gs_author_count": 8
  },
  {
    "arxiv_id": "2405.18368",
    "title": "The 2024 Brain Tumor Segmentation (BraTS) Challenge: Glioma Segmentation on Post-treatment MRI",
    "year": 2024,
    "published": "2024-05-28T17:07:55Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Gliomas are the most common malignant primary brain tumors in adults and one of the deadliest types of cancer. There are many challenges in treatment and monitoring due to the genetic diversity and high intrinsic heterogeneity in appearance, shape, histology, and treatment response. Treatments include surgery, radiation, and systemic therapies, with magnetic resonance imaging (MRI) playing a key role in treatment planning and post-treatment longitudinal assessment. The 2024 Brain Tumor Segmentat",
    "arxiv_url": "https://arxiv.org/abs/2405.18368v1",
    "pdf_url": "https://arxiv.org/pdf/2405.18368v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2405.18368",
    "arxiv_authors": [
      "Maria Correia de Verdier",
      "Rachit Saluja",
      "Louis Gagnon",
      "Dominic LaBella",
      "Ujjwall Baid",
      "Nourel Hoda Tahon",
      "Martha Foltyn-Dumitru",
      "Jikai Zhang",
      "Maram Alafif",
      "Saif Baig",
      "Ken Chang",
      "Gennaro D'Anna",
      "Lisa Deptula",
      "Diviya Gupta",
      "Muhammad Ammar Haider",
      "Ali Hussain",
      "Michael Iv",
      "Marinos Kontzialis",
      "Paul Manning",
      "Farzan Moodi",
      "Teresa Nunes",
      "Aaron Simon",
      "Nico Sollmann",
      "David Vu",
      "Maruf Adewole",
      "Jake Albrecht",
      "Udunna Anazodo",
      "Rongrong Chai",
      "Verena Chung",
      "Shahriar Faghani",
      "Keyvan Farahani",
      "Anahita Fathi Kazerooni",
      "Eugenio Iglesias",
      "Florian Kofler",
      "Hongwei Li",
      "Marius George Linguraru",
      "Bjoern Menze",
      "Ahmed W. Moawad",
      "Yury Velichko",
      "Benedikt Wiestler",
      "Talissa Altes",
      "Patil Basavasagar",
      "Martin Bendszus",
      "Gianluca Brugnara",
      "Jaeyoung Cho",
      "Yaseen Dhemesh",
      "Brandon K. K. Fields",
      "Filip Garrett",
      "Jaime Gass",
      "Lubomir Hadjiiski",
      "Jona Hattangadi-Gluth",
      "Christopher Hess",
      "Jessica L. Houk",
      "Edvin Isufi",
      "Lester J. Layfield",
      "George Mastorakos",
      "John Mongan",
      "Pierre Nedelec",
      "Uyen Nguyen",
      "Sebastian Oliva",
      "Matthew W. Pease",
      "Aditya Rastogi",
      "Jason Sinclair",
      "Robert X. Smith",
      "Leo P. Sugrue",
      "Jonathan Thacker",
      "Igor Vidic",
      "Javier Villanueva-Meyer",
      "Nathan S. White",
      "Mariam Aboian",
      "Gian Marco Conte",
      "Anders Dale",
      "Mert R. Sabuncu",
      "Tyler M. Seibert",
      "Brent Weinberg",
      "Aly Abayazeed",
      "Raymond Huang",
      "Sevcan Turk",
      "Andreas M. Rauschecker",
      "Nikdokht Farid",
      "Philipp Vollmuth",
      "Ayman Nada",
      "Spyridon Bakas",
      "Evan Calabrese",
      "Jeffrey D. Rudie"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=The+2024+Brain+Tumor+Segmentation+%28BraTS%29+Challenge%3A+Glioma+Segmentation+on+Post-treatment+MRI+Maria+Correia+de+Verdier+Rachit+Saluja+Louis+Gagnon+Dominic+LaBella+Ujjwall+Baid",
    "gs_search_success": true,
    "gs_authors": [
      "SONSGlUAAAAJ",
      "DE8dALgAAAAJ",
      "OjvjJQUAAAAJ",
      "xQLkqw0AAAAJ"
    ],
    "citation_count": 78,
    "gs_author_count": 9
  },
  {
    "arxiv_id": "2410.06527",
    "title": "The Sampling-Gaussian for stereo matching",
    "year": 2024,
    "published": "2024-10-09T03:57:13Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "The soft-argmax operation is widely adopted in neural network-based stereo matching methods to enable differentiable regression of disparity. However, network trained with soft-argmax is prone to being multimodal due to absence of explicit constraint to the shape of the probability distribution. Previous methods leverages Laplacian distribution and cross-entropy for training but failed to effectively improve the accuracy and even compromises the efficiency of the network. In this paper, we condu",
    "arxiv_url": "https://arxiv.org/abs/2410.06527v1",
    "pdf_url": "https://arxiv.org/pdf/2410.06527v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2410.06527",
    "arxiv_authors": [
      "Baiyu Pan",
      "jichao jiao",
      "Bowen Yao",
      "Jianxin Pang",
      "Jun Cheng"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=The+Sampling-Gaussian+for+stereo+matching+Baiyu+Pan+jichao+jiao+Bowen+Yao+Jianxin+Pang+Jun+Cheng",
    "gs_search_success": true,
    "gs_authors": [
      "42Oy5CYAAAAJ",
      "XNfG8uMAAAAJ",
      "qa0fH6UAAAAJ"
    ],
    "citation_count": 2,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2411.02871",
    "title": "Enhancing Adversarial Robustness via Uncertainty-Aware Distributional Adversarial Training",
    "year": 2024,
    "published": "2024-11-05T07:26:24Z",
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "abstract": "Despite remarkable achievements in deep learning across various domains, its inherent vulnerability to adversarial examples still remains a critical concern for practical deployment. Adversarial training has emerged as one of the most effective defensive techniques for improving model robustness against such malicious inputs. However, existing adversarial training schemes often lead to limited generalization ability against underlying adversaries with diversity due to their overreliance on a poi",
    "arxiv_url": "https://arxiv.org/abs/2411.02871v1",
    "pdf_url": "https://arxiv.org/pdf/2411.02871v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2411.02871",
    "arxiv_authors": [
      "Junhao Dong",
      "Xinghua Qu",
      "Z. Jane Wang",
      "Yew-Soon Ong"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Enhancing+Adversarial+Robustness+via+Uncertainty-Aware+Distributional+Adversarial+Training+Junhao+Dong+Xinghua+Qu+Z.+Jane+Wang+Yew-Soon+Ong",
    "gs_search_success": true,
    "gs_authors": [
      "h9oWOsEAAAAJ",
      "48A9z_UAAAAJ",
      "2PxlmU0AAAAJ"
    ],
    "citation_count": 3,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2312.00040",
    "title": "Presentation Attack detection using Wavelet Transform and Deep Residual Neural Net",
    "year": 2023,
    "published": "2023-11-23T20:21:49Z",
    "categories": [
      "cs.CR",
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "Biometric authentication is becoming more prevalent for secured authentication systems. However, the biometric substances can be deceived by the imposters in several ways. Among other imposter attacks, print attacks, mask attacks, and replay attacks fall under the presentation attack category. The bio-metric images, especially the iris and face, are vulnerable to different presentation attacks. This research applies deep learning approaches to mitigate presentation attacks in a biometric access ",
    "arxiv_url": "https://arxiv.org/abs/2312.00040v1",
    "pdf_url": "https://arxiv.org/pdf/2312.00040v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2312.00040",
    "arxiv_authors": [
      "Prosenjit Chatterjee",
      "Alex Yalchin",
      "Joseph Shelton",
      "Kaushik Roy",
      "Xiaohong Yuan",
      "Kossi D. Edoh"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Presentation+Attack+detection+using+Wavelet+Transform+and+Deep+Residual+Neural+Net+Prosenjit+Chatterjee+Alex+Yalchin+Joseph+Shelton+Kaushik+Roy+Xiaohong+Yuan",
    "gs_search_success": true,
    "gs_authors": [
      "K5OxH2YAAAAJ",
      "JOi3ND4AAAAJ",
      "ubPDmV0AAAAJ",
      "ZODfDcUAAAAJ"
    ],
    "citation_count": 15,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2411.05597",
    "title": "Predicting Stroke through Retinal Graphs and Multimodal Self-supervised Learning",
    "year": 2024,
    "published": "2024-11-08T14:40:56Z",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "Early identification of stroke is crucial for intervention, requiring reliable models. We proposed an efficient retinal image representation together with clinical information to capture a comprehensive overview of cardiovascular health, leveraging large multimodal datasets for new medical insights. Our approach is one of the first contrastive frameworks that integrates graph and tabular data, using vessel graphs derived from retinal images for efficient representation. This method, combined wit",
    "arxiv_url": "https://arxiv.org/abs/2411.05597v1",
    "pdf_url": "https://arxiv.org/pdf/2411.05597v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2411.05597",
    "arxiv_authors": [
      "Yuqing Huang",
      "Bastian Wittmann",
      "Olga Demler",
      "Bjoern Menze",
      "Neda Davoudi"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Predicting+Stroke+through+Retinal+Graphs+and+Multimodal+Self-supervised+Learning+Yuqing+Huang+Bastian+Wittmann+Olga+Demler+Bjoern+Menze+Neda+Davoudi",
    "gs_search_success": true,
    "gs_authors": [
      "44Xg-3YAAAAJ",
      "fZoCmiYAAAAJ",
      "0mxtU-cAAAAJ",
      "Kv2QrQgAAAAJ"
    ],
    "citation_count": 2,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2504.17515",
    "title": "Mamba-Sea: A Mamba-based Framework with Global-to-Local Sequence Augmentation for Generalizable Medical Image Segmentation",
    "year": 2025,
    "published": "2025-04-24T12:57:25Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "To segment medical images with distribution shifts, domain generalization (DG) has emerged as a promising setting to train models on source domains that can generalize to unseen target domains. Existing DG methods are mainly based on CNN or ViT architectures. Recently, advanced state space models, represented by Mamba, have shown promising results in various supervised medical image segmentation. The success of Mamba is primarily owing to its ability to capture long-range dependencies while keep",
    "arxiv_url": "https://arxiv.org/abs/2504.17515v1",
    "pdf_url": "https://arxiv.org/pdf/2504.17515v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2504.17515",
    "arxiv_authors": [
      "Zihan Cheng",
      "Jintao Guo",
      "Jian Zhang",
      "Lei Qi",
      "Luping Zhou",
      "Yinghuan Shi",
      "Yang Gao"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Mamba-Sea%3A+A+Mamba-based+Framework+with+Global-to-Local+Sequence+Augmentation+for+Generalizable+Medical+Image+Segmentation+Zihan+Cheng+Jintao+Guo+Jian+Zhang+Lei+Qi+Luping+Zhou",
    "gs_search_success": true,
    "gs_authors": [
      "m6BKDUMAAAAJ",
      "K4lrdKc_YLUC",
      "C5PIXOEAAAAJ",
      "SgofT2MAAAAJ",
      "7mm8iZwAAAAJ",
      "k0jjQo8AAAAJ"
    ],
    "citation_count": 15,
    "gs_author_count": 7
  },
  {
    "arxiv_id": "2505.13391",
    "title": "Advancing Generalization Across a Variety of Abstract Visual Reasoning Tasks",
    "year": 2025,
    "published": "2025-05-19T17:32:07Z",
    "categories": [
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "The abstract visual reasoning (AVR) domain presents a diverse suite of analogy-based tasks devoted to studying model generalization. Recent years have brought dynamic progress in the field, particularly in i.i.d. scenarios, in which models are trained and evaluated on the same data distributions. Nevertheless, o.o.d. setups that assess model generalization to new test distributions remain challenging even for the most recent models. To advance generalization in AVR tasks, we present the Pathways",
    "arxiv_url": "https://arxiv.org/abs/2505.13391v1",
    "pdf_url": "https://arxiv.org/pdf/2505.13391v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2505.13391",
    "arxiv_authors": [
      "Mikołaj Małkiński",
      "Jacek Mańdziuk"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Advancing+Generalization+Across+a+Variety+of+Abstract+Visual+Reasoning+Tasks+Miko%C5%82aj+Ma%C5%82ki%C5%84ski+Jacek+Ma%C5%84dziuk",
    "gs_search_success": true,
    "gs_authors": [
      "KHxnO24AAAAJ",
      "V7a-vNQAAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 2
  },
  {
    "arxiv_id": "2412.00832",
    "title": "EventGPT: Event Stream Understanding with Multimodal Large Language Models",
    "year": 2024,
    "published": "2024-12-01T14:38:40Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Event cameras record visual information as asynchronous pixel change streams, excelling at scene perception under unsatisfactory lighting or high-dynamic conditions. Existing multimodal large language models (MLLMs) concentrate on natural RGB images, failing in scenarios where event data fits better. In this paper, we introduce EventGPT, the first MLLM for event stream understanding, to the best of our knowledge, marking a pioneering attempt to integrate large language models (LLMs) with event s",
    "arxiv_url": "https://arxiv.org/abs/2412.00832v1",
    "pdf_url": "https://arxiv.org/pdf/2412.00832v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2412.00832",
    "arxiv_authors": [
      "Shaoyu Liu",
      "Jianing Li",
      "Guanghui Zhao",
      "Yunjian Zhang",
      "Xin Meng",
      "Fei Richard Yu",
      "Xiangyang Ji",
      "Ming Li"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=EventGPT%3A+Event+Stream+Understanding+with+Multimodal+Large+Language+Models+Shaoyu+Liu+Jianing+Li+Guanghui+Zhao+Yunjian+Zhang+Xin+Meng",
    "gs_search_success": true,
    "gs_authors": [
      "zuGMGBoAAAAJ",
      "2wySPkcAAAAJ",
      "NVs0AsMAAAAJ",
      "xrYnfwcAAAAJ",
      "L7OroPIAAAAJ"
    ],
    "citation_count": 17,
    "gs_author_count": 8
  },
  {
    "arxiv_id": "2301.05032",
    "title": "Online Hyperparameter Optimization for Class-Incremental Learning",
    "year": 2023,
    "published": "2023-01-11T17:58:51Z",
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "abstract": "Class-incremental learning (CIL) aims to train a classification model while the number of classes increases phase-by-phase. An inherent challenge of CIL is the stability-plasticity tradeoff, i.e., CIL models should keep stable to retain old knowledge and keep plastic to absorb new knowledge. However, none of the existing CIL models can achieve the optimal tradeoff in different data-receiving settings--where typically the training-from-half (TFH) setting needs more stability, but the training-fro",
    "arxiv_url": "https://arxiv.org/abs/2301.05032v2",
    "pdf_url": "https://arxiv.org/pdf/2301.05032v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2301.05032",
    "arxiv_authors": [
      "Yaoyao Liu",
      "Yingying Li",
      "Bernt Schiele",
      "Qianru Sun"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Online+Hyperparameter+Optimization+for+Class-Incremental+Learning+Yaoyao+Liu+Yingying+Li+Bernt+Schiele+Qianru+Sun",
    "gs_search_success": true,
    "gs_authors": [
      "z76PBfYAAAAJ",
      "fNfrGMIAAAAJ",
      "-b9PTaUAAAAJ",
      "Qi2PSmEAAAAJ"
    ],
    "citation_count": 52,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2302.03629",
    "title": "Ethical Considerations for Responsible Data Curation",
    "year": 2023,
    "published": "2023-02-07T17:33:00Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.DB",
      "cs.LG"
    ],
    "abstract": "Human-centric computer vision (HCCV) data curation practices often neglect privacy and bias concerns, leading to dataset retractions and unfair models. HCCV datasets constructed through nonconsensual web scraping lack crucial metadata for comprehensive fairness and robustness evaluations. Current remedies are post hoc, lack persuasive justification for adoption, or fail to provide proper contextualization for appropriate application. Our research focuses on proactive, domain-specific recommendat",
    "arxiv_url": "https://arxiv.org/abs/2302.03629v3",
    "pdf_url": "https://arxiv.org/pdf/2302.03629v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2302.03629",
    "arxiv_authors": [
      "Jerone T. A. Andrews",
      "Dora Zhao",
      "William Thong",
      "Apostolos Modas",
      "Orestis Papakyriakopoulos",
      "Alice Xiang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Ethical+Considerations+for+Responsible+Data+Curation+Jerone+T.+A.+Andrews+Dora+Zhao+William+Thong+Apostolos+Modas+Orestis+Papakyriakopoulos",
    "gs_search_success": true,
    "gs_authors": [
      "Iq8AdKQAAAAJ",
      "CPuz5p4AAAAJ",
      "Bq9Osr8AAAAJ",
      "9z-fD3sAAAAJ",
      "Cddl4_QAAAAJ",
      "cEr1ouIAAAAJ"
    ],
    "citation_count": 42,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2406.17162",
    "title": "Virtual Mines -- Component-level recycling of printed circuit boards using deep learning",
    "year": 2024,
    "published": "2024-06-24T22:29:30Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "abstract": "This contribution gives an overview of an ongoing project using machine learning and computer vision components for improving the electronic waste recycling process. In circular economy, the \"virtual mines\" concept refers to production cycles where interesting raw materials are reclaimed in an efficient and cost-effective manner from end-of-life items. In particular, the growth of e-waste, due to the increasingly shorter life cycle of hi-tech goods, is a global problem. In this paper, we describ",
    "arxiv_url": "https://arxiv.org/abs/2406.17162v1",
    "pdf_url": "https://arxiv.org/pdf/2406.17162v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2406.17162",
    "arxiv_authors": [
      "Muhammad Mohsin",
      "Stefano Rovetta",
      "Francesco Masulli",
      "Alberto Cabri"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Virtual+Mines+--+Component-level+recycling+of+printed+circuit+boards+using+deep+learning+Muhammad+Mohsin+Stefano+Rovetta+Francesco+Masulli+Alberto+Cabri",
    "gs_search_success": true,
    "gs_authors": [
      "DxovdUEAAAAJ",
      "7nlRez0AAAAJ",
      "zMisBHkAAAAJ",
      "Ht3LV2kAAAAJ"
    ],
    "citation_count": 13,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2505.12644",
    "title": "Use as Many Surrogates as You Want: Selective Ensemble Attack to Unleash Transferability without Sacrificing Resource Efficiency",
    "year": 2025,
    "published": "2025-05-19T02:56:41Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "In surrogate ensemble attacks, using more surrogate models yields higher transferability but lower resource efficiency. This practical trade-off between transferability and efficiency has largely limited existing attacks despite many pre-trained models are easily accessible online. In this paper, we argue that such a trade-off is caused by an unnecessary common assumption, i.e., all models should be \\textit{identical} across iterations. By lifting this assumption, we can use as many surrogates a",
    "arxiv_url": "https://arxiv.org/abs/2505.12644v2",
    "pdf_url": "https://arxiv.org/pdf/2505.12644v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2505.12644",
    "arxiv_authors": [
      "Bo Yang",
      "Hengwei Zhang",
      "Jindong Wang",
      "Yuchen Ren",
      "Chenhao Lin",
      "Chao Shen",
      "Zhengyu Zhao"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Use+as+Many+Surrogates+as+You+Want%3A+Selective+Ensemble+Attack+to+Unleash+Transferability+without+Sacrificing+Resource+Efficiency+Bo+Yang+Hengwei+Zhang+Jindong+Wang+Yuchen+Ren+Chenhao+Lin",
    "gs_search_success": true,
    "gs_authors": [
      "pC8KpPMAAAAJ",
      "YK0G990AAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 7
  },
  {
    "arxiv_id": "2410.18208",
    "title": "Automated Defect Detection and Grading of Piarom Dates Using Deep Learning",
    "year": 2024,
    "published": "2024-10-23T18:25:20Z",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "Grading and quality control of Piarom dates, a premium and high-value variety cultivated predominantly in Iran, present significant challenges due to the complexity and variability of defects, as well as the absence of specialized automated systems tailored to this fruit. Traditional manual inspection methods are labor intensive, time consuming, and prone to human error, while existing AI-based sorting solutions are insufficient for addressing the nuanced characteristics of Piarom dates. In this",
    "arxiv_url": "https://arxiv.org/abs/2410.18208v1",
    "pdf_url": "https://arxiv.org/pdf/2410.18208v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2410.18208",
    "arxiv_authors": [
      "Nasrin Azimi",
      "Danial Mohammad Rezaei"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Automated+Defect+Detection+and+Grading+of+Piarom+Dates+Using+Deep+Learning+Nasrin+Azimi+Danial+Mohammad+Rezaei",
    "gs_search_success": true,
    "gs_authors": [],
    "citation_count": 2,
    "gs_author_count": 2
  },
  {
    "arxiv_id": "2404.08526",
    "title": "Masked Image Modeling as a Framework for Self-Supervised Learning across Eye Movements",
    "year": 2024,
    "published": "2024-04-12T15:15:39Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "To make sense of their surroundings, intelligent systems must transform complex sensory inputs to structured codes that are reduced to task-relevant information such as object category. Biological agents achieve this in a largely autonomous manner, presumably via self-supervised learning. Whereas previous attempts to model the underlying mechanisms were largely discriminative in nature, there is ample evidence that the brain employs a generative model of the world. Here, we propose that eye move",
    "arxiv_url": "https://arxiv.org/abs/2404.08526v2",
    "pdf_url": "https://arxiv.org/pdf/2404.08526v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2404.08526",
    "arxiv_authors": [
      "Robin Weiler",
      "Matthias Brucklacher",
      "Cyriel M. A. Pennartz",
      "Sander M. Bohté"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Masked+Image+Modeling+as+a+Framework+for+Self-Supervised+Learning+across+Eye+Movements+Robin+Weiler+Matthias+Brucklacher+Cyriel+M.+A.+Pennartz+Sander+M.+Boht%C3%A9",
    "gs_search_success": true,
    "gs_authors": [
      "SKQaJesAAAAJ",
      "_MBLSIoAAAAJ",
      "9YosOMoAAAAJ",
      "zHlebkUAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2503.07298",
    "title": "ALLVB: All-in-One Long Video Understanding Benchmark",
    "year": 2025,
    "published": "2025-03-10T13:18:05Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "From image to video understanding, the capabilities of Multi-modal LLMs (MLLMs) are increasingly powerful. However, most existing video understanding benchmarks are relatively short, which makes them inadequate for effectively evaluating the long-sequence modeling capabilities of MLLMs. This highlights the urgent need for a comprehensive and integrated long video understanding benchmark to assess the ability of MLLMs thoroughly. To this end, we propose ALLVB (ALL-in-One Long Video Understanding ",
    "arxiv_url": "https://arxiv.org/abs/2503.07298v2",
    "pdf_url": "https://arxiv.org/pdf/2503.07298v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2503.07298",
    "arxiv_authors": [
      "Xichen Tan",
      "Yuanjing Luo",
      "Yunfan Ye",
      "Fang Liu",
      "Zhiping Cai"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=ALLVB%3A+All-in-One+Long+Video+Understanding+Benchmark+Xichen+Tan+Yuanjing+Luo+Yunfan+Ye+Fang+Liu+Zhiping+Cai",
    "gs_search_success": true,
    "gs_authors": [
      "YlHIglMAAAAJ",
      "iTGg6eQAAAAJ",
      "iqMBqZgAAAAJ",
      "uVwk4XIAAAAJ"
    ],
    "citation_count": 3,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2305.09147",
    "title": "Self-Aware Trajectory Prediction for Safe Autonomous Driving",
    "year": 2023,
    "published": "2023-05-16T03:53:23Z",
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "abstract": "Trajectory prediction is one of the key components of the autonomous driving software stack. Accurate prediction for the future movement of surrounding traffic participants is an important prerequisite for ensuring the driving efficiency and safety of intelligent vehicles. Trajectory prediction algorithms based on artificial intelligence have been widely studied and applied in recent years and have achieved remarkable results. However, complex artificial intelligence models are uncertain and dif",
    "arxiv_url": "https://arxiv.org/abs/2305.09147v1",
    "pdf_url": "https://arxiv.org/pdf/2305.09147v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2305.09147",
    "arxiv_authors": [
      "Wenbo Shao",
      "Jun Li",
      "Hong Wang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Self-Aware+Trajectory+Prediction+for+Safe+Autonomous+Driving+Wenbo+Shao+Jun+Li+Hong+Wang",
    "gs_search_success": true,
    "gs_authors": [
      "648k23UAAAAJ",
      "nJgFCn0AAAAJ"
    ],
    "citation_count": 13,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2301.04634",
    "title": "Street-View Image Generation from a Bird's-Eye View Layout",
    "year": 2023,
    "published": "2023-01-11T18:39:34Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Bird's-Eye View (BEV) Perception has received increasing attention in recent years as it provides a concise and unified spatial representation across views and benefits a diverse set of downstream driving applications. At the same time, data-driven simulation for autonomous driving has been a focal point of recent research but with few approaches that are both fully data-driven and controllable. Instead of using perception data from real-life scenarios, an ideal model for simulation would genera",
    "arxiv_url": "https://arxiv.org/abs/2301.04634v4",
    "pdf_url": "https://arxiv.org/pdf/2301.04634v4",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2301.04634",
    "arxiv_authors": [
      "Alexander Swerdlow",
      "Runsheng Xu",
      "Bolei Zhou"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Street-View+Image+Generation+from+a+Bird%27s-Eye+View+Layout+Alexander+Swerdlow+Runsheng+Xu+Bolei+Zhou",
    "gs_search_success": true,
    "gs_authors": [
      "--NjJhoAAAAJ",
      "QW6Ro8IAAAAJ",
      "9D4aG8AAAAAJ"
    ],
    "citation_count": 102,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2308.07650",
    "title": "EQ-Net: Elastic Quantization Neural Networks",
    "year": 2023,
    "published": "2023-08-15T08:57:03Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Current model quantization methods have shown their promising capability in reducing storage space and computation complexity. However, due to the diversity of quantization forms supported by different hardware, one limitation of existing solutions is that usually require repeated optimization for different scenarios. How to construct a model with flexible quantization forms has been less studied. In this paper, we explore a one-shot network quantization regime, named Elastic Quantization Neural",
    "arxiv_url": "https://arxiv.org/abs/2308.07650v1",
    "pdf_url": "https://arxiv.org/pdf/2308.07650v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2308.07650",
    "arxiv_authors": [
      "Ke Xu",
      "Lei Han",
      "Ye Tian",
      "Shangshang Yang",
      "Xingyi Zhang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=EQ-Net%3A+Elastic+Quantization+Neural+Networks+Ke+Xu+Lei+Han+Ye+Tian+Shangshang+Yang+Xingyi+Zhang",
    "gs_search_success": true,
    "gs_authors": [
      "arzbiNUAAAAJ",
      "e46B-tEAAAAJ",
      "FEjf5HgAAAAJ",
      "7lmPe2kAAAAJ"
    ],
    "citation_count": 22,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2405.12139",
    "title": "DTLLM-VLT: Diverse Text Generation for Visual Language Tracking Based on LLM",
    "year": 2024,
    "published": "2024-05-20T16:01:01Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Visual Language Tracking (VLT) enhances single object tracking (SOT) by integrating natural language descriptions from a video, for the precise tracking of a specified object. By leveraging high-level semantic information, VLT guides object tracking, alleviating the constraints associated with relying on a visual modality. Nevertheless, most VLT benchmarks are annotated in a single granularity and lack a coherent semantic framework to provide scientific guidance. Moreover, coordinating human ann",
    "arxiv_url": "https://arxiv.org/abs/2405.12139v2",
    "pdf_url": "https://arxiv.org/pdf/2405.12139v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2405.12139",
    "arxiv_authors": [
      "Xuchen Li",
      "Xiaokun Feng",
      "Shiyu Hu",
      "Meiqi Wu",
      "Dailing Zhang",
      "Jing Zhang",
      "Kaiqi Huang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=DTLLM-VLT%3A+Diverse+Text+Generation+for+Visual+Language+Tracking+Based+on+LLM+Xuchen+Li+Xiaokun+Feng+Shiyu+Hu+Meiqi+Wu+Dailing+Zhang",
    "gs_search_success": true,
    "gs_authors": [
      "fGc7NVAAAAAJ",
      "ApH4wOcAAAAJ",
      "49W-Rx4AAAAJ",
      "NqXtIPIAAAAJ",
      "caQ-OmYAAAAJ"
    ],
    "citation_count": 42,
    "gs_author_count": 7
  },
  {
    "arxiv_id": "2303.11771",
    "title": "Self-Sufficient Framework for Continuous Sign Language Recognition",
    "year": 2023,
    "published": "2023-03-21T11:42:57Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "The goal of this work is to develop self-sufficient framework for Continuous Sign Language Recognition (CSLR) that addresses key issues of sign language recognition. These include the need for complex multi-scale features such as hands, face, and mouth for understanding, and absence of frame-level annotations. To this end, we propose (1) Divide and Focus Convolution (DFConv) which extracts both manual and non-manual features without the need for additional networks or annotations, and (2) Dense ",
    "arxiv_url": "https://arxiv.org/abs/2303.11771v1",
    "pdf_url": "https://arxiv.org/pdf/2303.11771v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2303.11771",
    "arxiv_authors": [
      "Youngjoon Jang",
      "Youngtaek Oh",
      "Jae Won Cho",
      "Myungchul Kim",
      "Dong-Jin Kim",
      "In So Kweon",
      "Joon Son Chung"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Self-Sufficient+Framework+for+Continuous+Sign+Language+Recognition+Youngjoon+Jang+Youngtaek+Oh+Jae+Won+Cho+Myungchul+Kim+Dong-Jin+Kim",
    "gs_search_success": true,
    "gs_authors": [
      "3o79fuQAAAAJ",
      "oB5AOQQAAAAJ",
      "th4fvfIAAAAJ",
      "iMLuuj4AAAAJ",
      "JSUafnAAAAAJ",
      "XA8EOlEAAAAJ",
      "JJ_LQ0YAAAAJ"
    ],
    "citation_count": 23,
    "gs_author_count": 7
  },
  {
    "arxiv_id": "2407.11890",
    "title": "DepGAN: Leveraging Depth Maps for Handling Occlusions and Transparency in Image Composition",
    "year": 2024,
    "published": "2024-07-16T16:18:40Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Image composition is a complex task which requires a lot of information about the scene for an accurate and realistic composition, such as perspective, lighting, shadows, occlusions, and object interactions. Previous methods have predominantly used 2D information for image composition, neglecting the potentials of 3D spatial information. In this work, we propose DepGAN, a Generative Adversarial Network that utilizes depth maps and alpha channels to rectify inaccurate occlusions and enhance trans",
    "arxiv_url": "https://arxiv.org/abs/2407.11890v1",
    "pdf_url": "https://arxiv.org/pdf/2407.11890v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2407.11890",
    "arxiv_authors": [
      "Amr Ghoneim",
      "Jiju Poovvancheri",
      "Yasushi Akiyama",
      "Dong Chen"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=DepGAN%3A+Leveraging+Depth+Maps+for+Handling+Occlusions+and+Transparency+in+Image+Composition+Amr+Ghoneim+Jiju+Poovvancheri+Yasushi+Akiyama+Dong+Chen",
    "gs_search_success": true,
    "gs_authors": [
      "8vCk3iMAAAAJ",
      "bHa33I0AAAAJ",
      "g3BShQwAAAAJ",
      "kj5O6NwAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2407.04119",
    "title": "An Autoencoder Architecture for L-band Passive Microwave Retrieval of Landscape Freeze-Thaw Cycle",
    "year": 2024,
    "published": "2024-07-04T18:40:50Z",
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "abstract": "Estimating the landscape and soil freeze-thaw (FT) dynamics in the Northern Hemisphere is crucial for understanding permafrost response to global warming and changes in regional and global carbon budgets. A new framework is presented for surface FT-cycle retrievals using L-band microwave radiometry based on a deep convolutional autoencoder neural network. This framework defines the landscape FT-cycle retrieval as a time series anomaly detection problem considering the frozen states as normal and",
    "arxiv_url": "https://arxiv.org/abs/2407.04119v1",
    "pdf_url": "https://arxiv.org/pdf/2407.04119v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2407.04119",
    "arxiv_authors": [
      "Divya Kumawat",
      "Ardeshir Ebtehaj",
      "Xiaolan Xu",
      "Andreas Colliander",
      "Vipin Kumar"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=An+Autoencoder+Architecture+for+L-band+Passive+Microwave+Retrieval+of+Landscape+Freeze-Thaw+Cycle+Divya+Kumawat+Ardeshir+Ebtehaj+Xiaolan+Xu+Andreas+Colliander+Vipin+Kumar",
    "gs_search_success": true,
    "gs_authors": [
      "v--BLIAAAAAJ",
      "t6rmKg0AAAAJ",
      "TGNYXa8AAAAJ",
      "Owwu1P4AAAAJ",
      "BnxU9TEAAAAJ"
    ],
    "citation_count": 3,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2312.04948",
    "title": "Scientific Preparation for CSST: Classification of Galaxy and Nebula/Star Cluster Based on Deep Learning",
    "year": 2023,
    "published": "2023-12-08T10:27:40Z",
    "categories": [
      "cs.CV",
      "astro-ph.GA",
      "cs.LG"
    ],
    "abstract": "The Chinese Space Station Telescope (abbreviated as CSST) is a future advanced space telescope. Real-time identification of galaxy and nebula/star cluster (abbreviated as NSC) images is of great value during CSST survey. While recent research on celestial object recognition has progressed, the rapid and efficient identification of high-resolution local celestial images remains challenging. In this study, we conducted galaxy and NSC image classification research using deep learning methods based ",
    "arxiv_url": "https://arxiv.org/abs/2312.04948v1",
    "pdf_url": "https://arxiv.org/pdf/2312.04948v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2312.04948",
    "arxiv_authors": [
      "Yuquan Zhang",
      "Zhong Cao",
      "Feng Wang",
      "Lam",
      "Man I",
      "Hui Deng",
      "Ying Mei",
      "Lei Tan"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Scientific+Preparation+for+CSST%3A+Classification+of+Galaxy+and+Nebula%2FStar+Cluster+Based+on+Deep+Learning+Yuquan+Zhang+Zhong+Cao+Feng+Wang+Lam+Man+I",
    "gs_search_success": true,
    "gs_authors": [
      "w3LWdjYAAAAJ",
      "Aw09DYIAAAAJ"
    ],
    "citation_count": 3,
    "gs_author_count": 8
  },
  {
    "arxiv_id": "2306.14104",
    "title": "A Novel Dual-pooling Attention Module for UAV Vehicle Re-identification",
    "year": 2023,
    "published": "2023-06-25T02:46:12Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Vehicle re-identification (Re-ID) involves identifying the same vehicle captured by other cameras, given a vehicle image. It plays a crucial role in the development of safe cities and smart cities. With the rapid growth and implementation of unmanned aerial vehicles (UAVs) technology, vehicle Re-ID in UAV aerial photography scenes has garnered significant attention from researchers. However, due to the high altitude of UAVs, the shooting angle of vehicle images sometimes approximates vertical, r",
    "arxiv_url": "https://arxiv.org/abs/2306.14104v1",
    "pdf_url": "https://arxiv.org/pdf/2306.14104v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2306.14104",
    "arxiv_authors": [
      "Xiaoyan Guo",
      "Jie Yang",
      "Xinyu Jia",
      "Chuanyan Zang",
      "Yan Xu",
      "Zhaoyang Chen"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=A+Novel+Dual-pooling+Attention+Module+for+UAV+Vehicle+Re-identification+Xiaoyan+Guo+Jie+Yang+Xinyu+Jia+Chuanyan+Zang+Yan+Xu",
    "gs_search_success": true,
    "gs_authors": [],
    "citation_count": 8,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2410.10058",
    "title": "Learning to Customize Text-to-Image Diffusion In Diverse Context",
    "year": 2024,
    "published": "2024-10-14T00:53:59Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Most text-to-image customization techniques fine-tune models on a small set of \\emph{personal concept} images captured in minimal contexts. This often results in the model becoming overfitted to these training images and unable to generalize to new contexts in future text prompts. Existing customization methods are built on the success of effectively representing personal concepts as textual embeddings. Thus, in this work, we resort to diversifying the context of these personal concepts \\emph{so",
    "arxiv_url": "https://arxiv.org/abs/2410.10058v1",
    "pdf_url": "https://arxiv.org/pdf/2410.10058v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2410.10058",
    "arxiv_authors": [
      "Taewook Kim",
      "Wei Chen",
      "Qiang Qiu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Learning+to+Customize+Text-to-Image+Diffusion+In+Diverse+Context+Taewook+Kim+Wei+Chen+Qiang+Qiu",
    "gs_search_success": true,
    "gs_authors": [
      "jdLtt_YAAAAJ",
      "ZV_crdAAAAAJ",
      "jVT7rQgAAAAJ"
    ],
    "citation_count": 5,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2404.00179",
    "title": "Multi-Region Transfer Learning for Segmentation of Crop Field Boundaries in Satellite Images with Limited Labels",
    "year": 2024,
    "published": "2024-03-29T22:24:12Z",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "The goal of field boundary delineation is to predict the polygonal boundaries and interiors of individual crop fields in overhead remotely sensed images (e.g., from satellites or drones). Automatic delineation of field boundaries is a necessary task for many real-world use cases in agriculture, such as estimating cultivated area in a region or predicting end-of-season yield in a field. Field boundary delineation can be framed as an instance segmentation problem, but presents unique research chal",
    "arxiv_url": "https://arxiv.org/abs/2404.00179v1",
    "pdf_url": "https://arxiv.org/pdf/2404.00179v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2404.00179",
    "arxiv_authors": [
      "Hannah Kerner",
      "Saketh Sundar",
      "Mathan Satish"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Multi-Region+Transfer+Learning+for+Segmentation+of+Crop+Field+Boundaries+in+Satellite+Images+with+Limited+Labels+Hannah+Kerner+Saketh+Sundar+Mathan+Satish",
    "gs_search_success": true,
    "gs_authors": [
      "g5CD7dQAAAAJ",
      "_umLD84AAAAJ",
      "JEky9g0AAAAJ"
    ],
    "citation_count": 13,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2304.05060",
    "title": "SPIRiT-Diffusion: Self-Consistency Driven Diffusion Model for Accelerated MRI",
    "year": 2023,
    "published": "2023-04-11T08:43:52Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Diffusion models have emerged as a leading methodology for image generation and have proven successful in the realm of magnetic resonance imaging (MRI) reconstruction. However, existing reconstruction methods based on diffusion models are primarily formulated in the image domain, making the reconstruction quality susceptible to inaccuracies in coil sensitivity maps (CSMs). k-space interpolation methods can effectively address this issue but conventional diffusion models are not readily applicabl",
    "arxiv_url": "https://arxiv.org/abs/2304.05060v2",
    "pdf_url": "https://arxiv.org/pdf/2304.05060v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2304.05060",
    "arxiv_authors": [
      "Zhuo-Xu Cui",
      "Chentao Cao",
      "Yue Wang",
      "Sen Jia",
      "Jing Cheng",
      "Xin Liu",
      "Hairong Zheng",
      "Dong Liang",
      "Yanjie Zhu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=SPIRiT-Diffusion%3A+Self-Consistency+Driven+Diffusion+Model+for+Accelerated+MRI+Zhuo-Xu+Cui+Chentao+Cao+Yue+Wang+Sen+Jia+Jing+Cheng",
    "gs_search_success": true,
    "gs_authors": [
      "QZx0xdgAAAAJ",
      "3cAJWoIAAAAJ",
      "voDu8Y4AAAAJ",
      "-pqgxXsAAAAJ",
      "X2mIoQ4AAAAJ",
      "vZPl_oQAAAAJ",
      "aCcLh1oAAAAJ"
    ],
    "citation_count": 16,
    "gs_author_count": 9
  },
  {
    "arxiv_id": "2305.17271",
    "title": "Robust Lane Detection through Self Pre-training with Masked Sequential Autoencoders and Fine-tuning with Customized PolyLoss",
    "year": 2023,
    "published": "2023-05-26T21:36:08Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "eess.IV"
    ],
    "abstract": "Lane detection is crucial for vehicle localization which makes it the foundation for automated driving and many intelligent and advanced driving assistant systems. Available vision-based lane detection methods do not make full use of the valuable features and aggregate contextual information, especially the interrelationships between lane lines and other regions of the images in continuous frames. To fill this research gap and upgrade lane detection performance, this paper proposes a pipeline co",
    "arxiv_url": "https://arxiv.org/abs/2305.17271v2",
    "pdf_url": "https://arxiv.org/pdf/2305.17271v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2305.17271",
    "arxiv_authors": [
      "Ruohan Li",
      "Yongqi Dong"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Robust+Lane+Detection+through+Self+Pre-training+with+Masked+Sequential+Autoencoders+and+Fine-tuning+with+Customized+PolyLoss+Ruohan+Li+Yongqi+Dong",
    "gs_search_success": true,
    "gs_authors": [
      "aORBRHkAAAAJ",
      "L2kD-DwAAAAJ"
    ],
    "citation_count": 12,
    "gs_author_count": 2
  },
  {
    "arxiv_id": "2502.14420",
    "title": "ChatVLA: Unified Multimodal Understanding and Robot Control with Vision-Language-Action Model",
    "year": 2025,
    "published": "2025-02-20T10:16:18Z",
    "categories": [
      "cs.RO",
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "Humans possess a unified cognitive ability to perceive, comprehend, and interact with the physical world. Why can't large language models replicate this holistic understanding? Through a systematic analysis of existing training paradigms in vision-language-action models (VLA), we identify two key challenges: spurious forgetting, where robot training overwrites crucial visual-text alignments, and task interference, where competing control and understanding tasks degrade performance when trained j",
    "arxiv_url": "https://arxiv.org/abs/2502.14420v2",
    "pdf_url": "https://arxiv.org/pdf/2502.14420v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2502.14420",
    "arxiv_authors": [
      "Zhongyi Zhou",
      "Yichen Zhu",
      "Minjie Zhu",
      "Junjie Wen",
      "Ning Liu",
      "Zhiyuan Xu",
      "Weibin Meng",
      "Ran Cheng",
      "Yaxin Peng",
      "Chaomin Shen",
      "Feifei Feng"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=ChatVLA%3A+Unified+Multimodal+Understanding+and+Robot+Control+with+Vision-Language-Action+Model+Zhongyi+Zhou+Yichen+Zhu+Minjie+Zhu+Junjie+Wen+Ning+Liu",
    "gs_search_success": true,
    "gs_authors": [
      "UtUyfnEAAAAJ",
      "4cRt3XoAAAAJ",
      "OFOJM5MAAAAJ",
      "78OCZ8AAAAAJ",
      "fZfNcs8AAAAJ",
      "axtR_4QAAAAJ",
      "NQXcIqcAAAAJ",
      "eyKyrbsAAAAJ",
      "jKHMVnYAAAAJ"
    ],
    "citation_count": 52,
    "gs_author_count": 9
  },
  {
    "arxiv_id": "2410.09501",
    "title": "Fine-grained subjective visual quality assessment for high-fidelity compressed images",
    "year": 2024,
    "published": "2024-10-12T11:37:19Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Advances in image compression, storage, and display technologies have made high-quality images and videos widely accessible. At this level of quality, distinguishing between compressed and original content becomes difficult, highlighting the need for assessment methodologies that are sensitive to even the smallest visual quality differences. Conventional subjective visual quality assessments often use absolute category rating scales, ranging from ``excellent'' to ``bad''. While suitable for eval",
    "arxiv_url": "https://arxiv.org/abs/2410.09501v1",
    "pdf_url": "https://arxiv.org/pdf/2410.09501v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2410.09501",
    "arxiv_authors": [
      "Michela Testolina",
      "Mohsen Jenadeleh",
      "Shima Mohammadi",
      "Shaolin Su",
      "Joao Ascenso",
      "Touradj Ebrahimi",
      "Jon Sneyers",
      "Dietmar Saupe"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Fine-grained+subjective+visual+quality+assessment+for+high-fidelity+compressed+images+Michela+Testolina+Mohsen+Jenadeleh+Shima+Mohammadi+Shaolin+Su+Joao+Ascenso",
    "gs_search_success": true,
    "gs_authors": [
      "jt-UsrcAAAAJ",
      "hvzOCpAAAAAJ",
      "aYoq6CsAAAAJ",
      "9CK8xEgAAAAJ",
      "azZf-ewAAAAJ",
      "cwkHAnYAAAAJ",
      "AkDunrwAAAAJ",
      "sjpFHQQAAAAJ"
    ],
    "citation_count": 6,
    "gs_author_count": 8
  },
  {
    "arxiv_id": "2501.07496",
    "title": "Aligning First, Then Fusing: A Novel Weakly Supervised Multimodal Violence Detection Method",
    "year": 2025,
    "published": "2025-01-13T17:14:25Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Weakly supervised violence detection refers to the technique of training models to identify violent segments in videos using only video-level labels. Among these approaches, multimodal violence detection, which integrates modalities such as audio and optical flow, holds great potential. Existing methods in this domain primarily focus on designing multimodal fusion models to address modality discrepancies. In contrast, we take a different approach; leveraging the inherent discrepancies across mod",
    "arxiv_url": "https://arxiv.org/abs/2501.07496v2",
    "pdf_url": "https://arxiv.org/pdf/2501.07496v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2501.07496",
    "arxiv_authors": [
      "Wenping Jin",
      "Li Zhu",
      "Jing Sun"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Aligning+First%2C+Then+Fusing%3A+A+Novel+Weakly+Supervised+Multimodal+Violence+Detection+Method+Wenping+Jin+Li+Zhu+Jing+Sun",
    "gs_search_success": true,
    "gs_authors": [
      "AD6GzxsAAAAJ"
    ],
    "citation_count": 2,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2404.02877",
    "title": "FlightScope: An Experimental Comparative Review of Aircraft Detection Algorithms in Satellite Imagery",
    "year": 2024,
    "published": "2024-04-03T17:24:27Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Object detection in remotely sensed satellite pictures is fundamental in many fields such as biophysical, and environmental monitoring. While deep learning algorithms are constantly evolving, they have been mostly implemented and tested on popular ground-based taken photos. This paper critically evaluates and compares a suite of advanced object detection algorithms customized for the task of identifying aircraft within satellite imagery. Using the large HRPlanesV2 dataset, together with a rigoro",
    "arxiv_url": "https://arxiv.org/abs/2404.02877v4",
    "pdf_url": "https://arxiv.org/pdf/2404.02877v4",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2404.02877",
    "arxiv_authors": [
      "Safouane El Ghazouali",
      "Arnaud Gucciardi",
      "Francesca Venturini",
      "Nicola Venturi",
      "Michael Rueegsegger",
      "Umberto Michelucci"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=FlightScope%3A+An+Experimental+Comparative+Review+of+Aircraft+Detection+Algorithms+in+Satellite+Imagery+Safouane+El+Ghazouali+Arnaud+Gucciardi+Francesca+Venturini+Nicola+Venturi+Michael+Rueegsegger",
    "gs_search_success": true,
    "gs_authors": [
      "G9Gwnh8AAAAJ",
      "Pg8YQ3UAAAAJ",
      "7cY2IUwAAAAJ",
      "iyuXTTUAAAAJ"
    ],
    "citation_count": 7,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2309.00018",
    "title": "Unsupervised discovery of Interpretable Visual Concepts",
    "year": 2023,
    "published": "2023-08-31T07:53:02Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "eess.IV"
    ],
    "abstract": "Providing interpretability of deep-learning models to non-experts, while fundamental for a responsible real-world usage, is challenging. Attribution maps from xAI techniques, such as Integrated Gradients, are a typical example of a visualization technique containing a high level of information, but with difficult interpretation. In this paper, we propose two methods, Maximum Activation Groups Extraction (MAGE) and Multiscale Interpretable Visualization (Ms-IV), to explain the model's decision, e",
    "arxiv_url": "https://arxiv.org/abs/2309.00018v2",
    "pdf_url": "https://arxiv.org/pdf/2309.00018v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2309.00018",
    "arxiv_authors": [
      "Caroline Mazini Rodrigues",
      "Nicolas Boutry",
      "Laurent Najman"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Unsupervised+discovery+of+Interpretable+Visual+Concepts+Caroline+Mazini+Rodrigues+Nicolas+Boutry+Laurent+Najman",
    "gs_search_success": true,
    "gs_authors": [
      "j-2_cT0AAAAJ",
      "hMFyhaUAAAAJ",
      "hU-3BxkAAAAJ"
    ],
    "citation_count": 3,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2306.05704",
    "title": "Exploring Effective Mask Sampling Modeling for Neural Image Compression",
    "year": 2023,
    "published": "2023-06-09T06:50:20Z",
    "categories": [
      "cs.CV",
      "cs.MM",
      "eess.IV"
    ],
    "abstract": "Image compression aims to reduce the information redundancy in images. Most existing neural image compression methods rely on side information from hyperprior or context models to eliminate spatial redundancy, but rarely address the channel redundancy. Inspired by the mask sampling modeling in recent self-supervised learning methods for natural language processing and high-level vision, we propose a novel pretraining strategy for neural image compression. Specifically, Cube Mask Sampling Module ",
    "arxiv_url": "https://arxiv.org/abs/2306.05704v1",
    "pdf_url": "https://arxiv.org/pdf/2306.05704v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2306.05704",
    "arxiv_authors": [
      "Lin Liu",
      "Mingming Zhao",
      "Shanxin Yuan",
      "Wenlong Lyu",
      "Wengang Zhou",
      "Houqiang Li",
      "Yanfeng Wang",
      "Qi Tian"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Exploring+Effective+Mask+Sampling+Modeling+for+Neural+Image+Compression+Lin+Liu+Mingming+Zhao+Shanxin+Yuan+Wenlong+Lyu+Wengang+Zhou",
    "gs_search_success": true,
    "gs_authors": [
      "61b6eYkAAAAJ",
      "aliP2WYAAAAJ",
      "htlcuX4AAAAJ",
      "8s1JF8YAAAAJ"
    ],
    "citation_count": 3,
    "gs_author_count": 8
  },
  {
    "arxiv_id": "2309.06286",
    "title": "Transferability analysis of data-driven additive manufacturing knowledge: a case study between powder bed fusion and directed energy deposition",
    "year": 2023,
    "published": "2023-09-12T14:46:56Z",
    "categories": [
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "Data-driven research in Additive Manufacturing (AM) has gained significant success in recent years. This has led to a plethora of scientific literature to emerge. The knowledge in these works consists of AM and Artificial Intelligence (AI) contexts that have not been mined and formalized in an integrated way. Moreover, no tools or guidelines exist to support data-driven knowledge transfer from one context to another. As a result, data-driven solutions using specific AI techniques are being devel",
    "arxiv_url": "https://arxiv.org/abs/2309.06286v1",
    "pdf_url": "https://arxiv.org/pdf/2309.06286v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2309.06286",
    "arxiv_authors": [
      "Mutahar Safdar",
      "Jiarui Xie",
      "Hyunwoong Ko",
      "Yan Lu",
      "Guy Lamouche",
      "Yaoyao Fiona Zhao"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Transferability+analysis+of+data-driven+additive+manufacturing+knowledge%3A+a+case+study+between+powder+bed+fusion+and+directed+energy+deposition+Mutahar+Safdar+Jiarui+Xie+Hyunwoong+Ko+Yan+Lu+Guy+Lamouche",
    "gs_search_success": true,
    "gs_authors": [
      "n4iYxCIAAAAJ",
      "RysDa3cAAAAJ",
      "4v2covcAAAAJ",
      "H-C9MGcAAAAJ"
    ],
    "citation_count": 10,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2505.11724",
    "title": "Semantically-Aware Game Image Quality Assessment",
    "year": 2025,
    "published": "2025-05-16T22:12:19Z",
    "categories": [
      "cs.CV",
      "eess.IV"
    ],
    "abstract": "Assessing the visual quality of video game graphics presents unique challenges due to the absence of reference images and the distinct types of distortions, such as aliasing, texture blur, and geometry level of detail (LOD) issues, which differ from those in natural images or user-generated content. Existing no-reference image and video quality assessment (NR-IQA/VQA) methods fail to generalize to gaming environments as they are primarily designed for distortions like compression artifacts. This",
    "arxiv_url": "https://arxiv.org/abs/2505.11724v1",
    "pdf_url": "https://arxiv.org/pdf/2505.11724v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2505.11724",
    "arxiv_authors": [
      "Kai Zhu",
      "Vignesh Edithal",
      "Le Zhang",
      "Ilia Blank",
      "Imran Junejo"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Semantically-Aware+Game+Image+Quality+Assessment+Kai+Zhu+Vignesh+Edithal+Le+Zhang+Ilia+Blank+Imran+Junejo",
    "gs_search_success": true,
    "gs_authors": [],
    "citation_count": 1,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2504.11379",
    "title": "Omni$^2$: Unifying Omnidirectional Image Generation and Editing in an Omni Model",
    "year": 2025,
    "published": "2025-04-15T16:53:11Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "$360^{\\circ}$ omnidirectional images (ODIs) have gained considerable attention recently, and are widely used in various virtual reality (VR) and augmented reality (AR) applications. However, capturing such images is expensive and requires specialized equipment, making ODI synthesis increasingly important. While common 2D image generation and editing methods are rapidly advancing, these models struggle to deliver satisfactory results when generating or editing ODIs due to the unique format and br",
    "arxiv_url": "https://arxiv.org/abs/2504.11379v2",
    "pdf_url": "https://arxiv.org/pdf/2504.11379v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2504.11379",
    "arxiv_authors": [
      "Liu Yang",
      "Huiyu Duan",
      "Yucheng Zhu",
      "Xiaohong Liu",
      "Lu Liu",
      "Zitong Xu",
      "Guangji Ma",
      "Xiongkuo Min",
      "Guangtao Zhai",
      "Patrick Le Callet"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Omni%24%5E2%24%3A+Unifying+Omnidirectional+Image+Generation+and+Editing+in+an+Omni+Model+Liu+Yang+Huiyu+Duan+Yucheng+Zhu+Xiaohong+Liu+Lu+Liu",
    "gs_search_success": true,
    "gs_authors": [
      "-pD_qMYAAAAJ",
      "91sjuWIAAAAJ",
      "Tq2hoMQAAAAJ",
      "lkEkF-kAAAAJ",
      "82Z_AfgAAAAJ",
      "E6zbSYgAAAAJ",
      "llgwlUgAAAAJ",
      "r0bRaCMAAAAJ"
    ],
    "citation_count": 4,
    "gs_author_count": 9
  },
  {
    "arxiv_id": "2312.04563",
    "title": "Visual Geometry Grounded Deep Structure From Motion",
    "year": 2023,
    "published": "2023-12-07T18:59:52Z",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "abstract": "Structure-from-motion (SfM) is a long-standing problem in the computer vision community, which aims to reconstruct the camera poses and 3D structure of a scene from a set of unconstrained 2D images. Classical frameworks solve this problem in an incremental manner by detecting and matching keypoints, registering images, triangulating 3D points, and conducting bundle adjustment. Recent research efforts have predominantly revolved around harnessing the power of deep learning techniques to enhance s",
    "arxiv_url": "https://arxiv.org/abs/2312.04563v1",
    "pdf_url": "https://arxiv.org/pdf/2312.04563v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2312.04563",
    "arxiv_authors": [
      "Jianyuan Wang",
      "Nikita Karaev",
      "Christian Rupprecht",
      "David Novotny"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Visual+Geometry+Grounded+Deep+Structure+From+Motion+Jianyuan+Wang+Nikita+Karaev+Christian+Rupprecht+David+Novotny",
    "gs_search_success": true,
    "gs_authors": [
      "DW-X-Y8AAAAJ",
      "2wk2RdgAAAAJ",
      "IrYlproAAAAJ",
      "2glXz7cAAAAJ"
    ],
    "citation_count": 164,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2405.19331",
    "title": "NPGA: Neural Parametric Gaussian Avatars",
    "year": 2024,
    "published": "2024-05-29T17:58:09Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR"
    ],
    "abstract": "The creation of high-fidelity, digital versions of human heads is an important stepping stone in the process of further integrating virtual components into our everyday lives. Constructing such avatars is a challenging research problem, due to a high demand for photo-realism and real-time rendering performance. In this work, we propose Neural Parametric Gaussian Avatars (NPGA), a data-driven approach to create high-fidelity, controllable avatars from multi-view video recordings. We build our met",
    "arxiv_url": "https://arxiv.org/abs/2405.19331v2",
    "pdf_url": "https://arxiv.org/pdf/2405.19331v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2405.19331",
    "arxiv_authors": [
      "Simon Giebenhain",
      "Tobias Kirschstein",
      "Martin Rünz",
      "Lourdes Agapito",
      "Matthias Nießner"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=NPGA%3A+Neural+Parametric+Gaussian+Avatars+Simon+Giebenhain+Tobias+Kirschstein+Martin+R%C3%BCnz+Lourdes+Agapito+Matthias+Nie%C3%9Fner",
    "gs_search_success": true,
    "gs_authors": [
      "rri0OT0AAAAJ",
      "nhRzTtEAAAAJ",
      "eUtEs6YAAAAJ",
      "IRMX4-4AAAAJ"
    ],
    "citation_count": 33,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2402.09567",
    "title": "TAI-GAN: A Temporally and Anatomically Informed Generative Adversarial Network for early-to-late frame conversion in dynamic cardiac PET inter-frame motion correction",
    "year": 2024,
    "published": "2024-02-14T20:39:07Z",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "abstract": "Inter-frame motion in dynamic cardiac positron emission tomography (PET) using rubidium-82 (82-Rb) myocardial perfusion imaging impacts myocardial blood flow (MBF) quantification and the diagnosis accuracy of coronary artery diseases. However, the high cross-frame distribution variation due to rapid tracer kinetics poses a considerable challenge for inter-frame motion correction, especially for early frames where intensity-based image registration techniques often fail. To address this issue, we",
    "arxiv_url": "https://arxiv.org/abs/2402.09567v1",
    "pdf_url": "https://arxiv.org/pdf/2402.09567v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2402.09567",
    "arxiv_authors": [
      "Xueqi Guo",
      "Luyao Shi",
      "Xiongchao Chen",
      "Qiong Liu",
      "Bo Zhou",
      "Huidong Xie",
      "Yi-Hwa Liu",
      "Richard Palyo",
      "Edward J. Miller",
      "Albert J. Sinusas",
      "Lawrence H. Staib",
      "Bruce Spottiswoode",
      "Chi Liu",
      "Nicha C. Dvornek"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=TAI-GAN%3A+A+Temporally+and+Anatomically+Informed+Generative+Adversarial+Network+for+early-to-late+frame+conversion+in+dynamic+cardiac+PET+inter-frame+motion+correction+Xueqi+Guo+Luyao+Shi+Xiongchao+Chen+Qiong+Liu+Bo+Zhou",
    "gs_search_success": true,
    "gs_authors": [
      "2D1eyc0AAAAJ",
      "sZw4ndYAAAAJ",
      "k71vsAIAAAAJ",
      "BXIq39UAAAAJ",
      "Ijnh8E0AAAAJ",
      "94Rsf5wAAAAJ",
      "tROPwjAAAAAJ",
      "SvnpHCkAAAAJ",
      "F1i4oscAAAAJ"
    ],
    "citation_count": 7,
    "gs_author_count": 9
  },
  {
    "arxiv_id": "2310.05202",
    "title": "Enhancing Cross-Dataset Performance of Distracted Driving Detection With Score Softmax Classifier And Dynamic Gaussian Smoothing Supervision",
    "year": 2023,
    "published": "2023-10-08T15:28:01Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Deep neural networks enable real-time monitoring of in-vehicle drivers, facilitating the timely prediction of distractions, fatigue, and potential hazards. This technology is now integral to intelligent transportation systems. Recent research has exposed unreliable cross-dataset driver behavior recognition due to a limited number of data samples and background noise. In this paper, we propose a Score-Softmax classifier, which reduces the model overconfidence by enhancing category independence. I",
    "arxiv_url": "https://arxiv.org/abs/2310.05202v4",
    "pdf_url": "https://arxiv.org/pdf/2310.05202v4",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2310.05202",
    "arxiv_authors": [
      "Cong Duan",
      "Zixuan Liu",
      "Jiahao Xia",
      "Minghai Zhang",
      "Jiacai Liao",
      "Libo Cao"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Enhancing+Cross-Dataset+Performance+of+Distracted+Driving+Detection+With+Score+Softmax+Classifier+And+Dynamic+Gaussian+Smoothing+Supervision+Cong+Duan+Zixuan+Liu+Jiahao+Xia+Minghai+Zhang+Jiacai+Liao",
    "gs_search_success": true,
    "gs_authors": [
      "uAkVEucAAAAJ",
      "8frVIOAAAAAJ"
    ],
    "citation_count": 4,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2406.13362",
    "title": "VisualRWKV: Exploring Recurrent Neural Networks for Visual Language Models",
    "year": 2024,
    "published": "2024-06-19T09:07:31Z",
    "categories": [
      "cs.CV",
      "cs.CL",
      "cs.LG"
    ],
    "abstract": "Visual Language Models (VLMs) have rapidly progressed with the recent success of large language models. However, there have been few attempts to incorporate efficient linear Recurrent Neural Networks (RNNs) architectures into VLMs. In this study, we introduce VisualRWKV, the first application of a linear RNN model to multimodal learning tasks, leveraging the pre-trained RWKV language model. We propose a data-dependent recurrence and sandwich prompts to enhance our modeling capabilities, along wi",
    "arxiv_url": "https://arxiv.org/abs/2406.13362v3",
    "pdf_url": "https://arxiv.org/pdf/2406.13362v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2406.13362",
    "arxiv_authors": [
      "Haowen Hou",
      "Peigen Zeng",
      "Fei Ma",
      "Fei Richard Yu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=VisualRWKV%3A+Exploring+Recurrent+Neural+Networks+for+Visual+Language+Models+Haowen+Hou+Peigen+Zeng+Fei+Ma+Fei+Richard+Yu",
    "gs_search_success": true,
    "gs_authors": [
      "P6pDyoYAAAAJ",
      "RJOEAMYAAAAJ",
      "zuGMGBoAAAAJ"
    ],
    "citation_count": 12,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2503.22557",
    "title": "MO-CTranS: A unified multi-organ segmentation model learning from multiple heterogeneously labelled datasets",
    "year": 2025,
    "published": "2025-03-28T16:00:59Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Multi-organ segmentation holds paramount significance in many clinical tasks. In practice, compared to large fully annotated datasets, multiple small datasets are often more accessible and organs are not labelled consistently. Normally, an individual model is trained for each of these datasets, which is not an effective way of using data for model learning. It remains challenging to train a single model that can robustly learn from several partially labelled datasets due to label conflict and da",
    "arxiv_url": "https://arxiv.org/abs/2503.22557v1",
    "pdf_url": "https://arxiv.org/pdf/2503.22557v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2503.22557",
    "arxiv_authors": [
      "Zhendi Gong",
      "Susan Francis",
      "Eleanor Cox",
      "Stamatios N. Sotiropoulos",
      "Dorothee P. Auer",
      "Guoping Qiu",
      "Andrew P. French",
      "Xin Chen"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=MO-CTranS%3A+A+unified+multi-organ+segmentation+model+learning+from+multiple+heterogeneously+labelled+datasets+Zhendi+Gong+Susan+Francis+Eleanor+Cox+Stamatios+N.+Sotiropoulos+Dorothee+P.+Auer",
    "gs_search_success": true,
    "gs_authors": [
      "sqhatnIAAAAJ",
      "9p2f6vUAAAAJ",
      "SESz4-UAAAAJ",
      "yqpHF90AAAAJ",
      "g2IuFugAAAAJ",
      "FQHFqroAAAAJ",
      "nOY5fjcAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 8
  },
  {
    "arxiv_id": "2502.19316",
    "title": "Model Adaptation: Unsupervised Domain Adaptation without Source Data",
    "year": 2025,
    "published": "2025-02-26T17:10:52Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "In this paper, we investigate a challenging unsupervised domain adaptation setting -- unsupervised model adaptation. We aim to explore how to rely only on unlabeled target data to improve performance of an existing source prediction model on the target domain, since labeled source data may not be available in some real-world scenarios due to data privacy issues. For this purpose, we propose a new framework, which is referred to as collaborative class conditional generative adversarial net to byp",
    "arxiv_url": "https://arxiv.org/abs/2502.19316v1",
    "pdf_url": "https://arxiv.org/pdf/2502.19316v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2502.19316",
    "arxiv_authors": [
      "Rui Li",
      "Qianfen Jiao",
      "Wenming Cao",
      "Hau-San Wong",
      "Si Wu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Model+Adaptation%3A+Unsupervised+Domain+Adaptation+without+Source+Data+Rui+Li+Qianfen+Jiao+Wenming+Cao+Hau-San+Wong+Si+Wu",
    "gs_search_success": true,
    "gs_authors": [
      "i9Dh1OkAAAAJ",
      "Nt9es7kAAAAJ",
      "p0GLwtoAAAAJ",
      "uYmK-A0AAAAJ",
      "uPxjSDIAAAAJ",
      "6CsB8k0AAAAJ",
      "3Vj3SGYAAAAJ",
      "DsLYx7YAAAAJ",
      "ZbA-z1cAAAAJ",
      "8uhkD9QAAAAJ",
      "UUbEzBYAAAAJ",
      "Mf9VHRcAAAAJ",
      "9nxadX4AAAAJ",
      "dGiwzcIAAAAJ",
      "Q7VKnRYAAAAJ",
      "yGKsEpAAAAAJ",
      "V9oO4oMAAAAJ",
      "czirNcwAAAAJ",
      "BPQ9bSwAAAAJ",
      "SE-GpAIAAAAJ",
      "tAK5l1IAAAAJ",
      "RtkXrnwAAAAJ",
      "F2La0hoAAAAJ",
      "p27Iqt4AAAAJ",
      "Gsw2iUEAAAAJ"
    ],
    "citation_count": 33,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2504.05979",
    "title": "An Empirical Study of GPT-4o Image Generation Capabilities",
    "year": 2025,
    "published": "2025-04-08T12:34:36Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "The landscape of image generation has rapidly evolved, from early GAN-based approaches to diffusion models and, most recently, to unified generative architectures that seek to bridge understanding and generation tasks. Recent advances, especially the GPT-4o, have demonstrated the feasibility of high-fidelity multimodal generation, their architectural design remains mysterious and unpublished. This prompts the question of whether image and text generation have already been successfully integrated",
    "arxiv_url": "https://arxiv.org/abs/2504.05979v2",
    "pdf_url": "https://arxiv.org/pdf/2504.05979v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2504.05979",
    "arxiv_authors": [
      "Sixiang Chen",
      "Jinbin Bai",
      "Zhuoran Zhao",
      "Tian Ye",
      "Qingyu Shi",
      "Donghao Zhou",
      "Wenhao Chai",
      "Xin Lin",
      "Jianzong Wu",
      "Chao Tang",
      "Shilin Xu",
      "Tao Zhang",
      "Haobo Yuan",
      "Yikang Zhou",
      "Wei Chow",
      "Linfeng Li",
      "Xiangtai Li",
      "Lei Zhu",
      "Lu Qi"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=An+Empirical+Study+of+GPT-4o+Image+Generation+Capabilities+Sixiang+Chen+Jinbin+Bai+Zhuoran+Zhao+Tian+Ye+Qingyu+Shi",
    "gs_search_success": true,
    "gs_authors": [
      "3xu4a5oAAAAJ",
      "Td_KJgIAAAAJ",
      "PAfNNrYAAAAJ",
      "KpVFMqYAAAAJ",
      "8bBcL9sAAAAJ",
      "9j7dtGkAAAAJ",
      "SL--7UMAAAAJ",
      "EtljKSgAAAAJ",
      "fLZ_r4oAAAAJ",
      "VpSqhJAAAAAJ"
    ],
    "citation_count": 14,
    "gs_author_count": 9
  },
  {
    "arxiv_id": "2303.15062",
    "title": "The Devil is in the Points: Weakly Semi-Supervised Instance Segmentation via Point-Guided Mask Representation",
    "year": 2023,
    "published": "2023-03-27T10:11:22Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "In this paper, we introduce a novel learning scheme named weakly semi-supervised instance segmentation (WSSIS) with point labels for budget-efficient and high-performance instance segmentation. Namely, we consider a dataset setting consisting of a few fully-labeled images and a lot of point-labeled images. Motivated by the main challenge of semi-supervised approaches mainly derives from the trade-off between false-negative and false-positive instance proposals, we propose a method for WSSIS that",
    "arxiv_url": "https://arxiv.org/abs/2303.15062v1",
    "pdf_url": "https://arxiv.org/pdf/2303.15062v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2303.15062",
    "arxiv_authors": [
      "Beomyoung Kim",
      "Joonhyun Jeong",
      "Dongyoon Han",
      "Sung Ju Hwang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=The+Devil+is+in+the+Points%3A+Weakly+Semi-Supervised+Instance+Segmentation+via+Point-Guided+Mask+Representation+Beomyoung+Kim+Joonhyun+Jeong+Dongyoon+Han+Sung+Ju+Hwang",
    "gs_search_success": true,
    "gs_authors": [
      "jcP7m1QAAAAJ",
      "RP4Qx3QAAAAJ",
      "n_TR1LcAAAAJ",
      "M3JKVIcAAAAJ"
    ],
    "citation_count": 40,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2501.07983",
    "title": "V-Trans4Style: Visual Transition Recommendation for Video Production Style Adaptation",
    "year": 2025,
    "published": "2025-01-14T10:06:02Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "We introduce V-Trans4Style, an innovative algorithm tailored for dynamic video content editing needs. It is designed to adapt videos to different production styles like documentaries, dramas, feature films, or a specific YouTube channel's video-making technique. Our algorithm recommends optimal visual transitions to help achieve this flexibility using a more bottom-up approach. We first employ a transformer-based encoder-decoder network to learn recommending temporally consistent and visually se",
    "arxiv_url": "https://arxiv.org/abs/2501.07983v1",
    "pdf_url": "https://arxiv.org/pdf/2501.07983v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2501.07983",
    "arxiv_authors": [
      "Pooja Guhan",
      "Tsung-Wei Huang",
      "Guan-Ming Su",
      "Subhadra Gopalakrishnan",
      "Dinesh Manocha"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=V-Trans4Style%3A+Visual+Transition+Recommendation+for+Video+Production+Style+Adaptation+Pooja+Guhan+Tsung-Wei+Huang+Guan-Ming+Su+Subhadra+Gopalakrishnan+Dinesh+Manocha",
    "gs_search_success": true,
    "gs_authors": [
      "hsSUHasAAAAJ",
      "PnMZVsgAAAAJ",
      "X08l_4IAAAAJ",
      "oe-qvf4AAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2501.16221",
    "title": "Automatic Calibration of a Multi-Camera System with Limited Overlapping Fields of View for 3D Surgical Scene Reconstruction",
    "year": 2025,
    "published": "2025-01-27T17:10:33Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "The purpose of this study is to develop an automated and accurate external camera calibration method for multi-camera systems used in 3D surgical scene reconstruction (3D-SSR), eliminating the need for operator intervention or specialized expertise. The method specifically addresses the problem of limited overlapping fields of view caused by significant variations in optical zoom levels and camera locations. We contribute a novel, fast, and fully automatic calibration method based on the project",
    "arxiv_url": "https://arxiv.org/abs/2501.16221v2",
    "pdf_url": "https://arxiv.org/pdf/2501.16221v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2501.16221",
    "arxiv_authors": [
      "Tim Flückiger",
      "Jonas Hein",
      "Valery Fischer",
      "Philipp Fürnstahl",
      "Lilian Calvet"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Automatic+Calibration+of+a+Multi-Camera+System+with+Limited+Overlapping+Fields+of+View+for+3D+Surgical+Scene+Reconstruction+Tim+Fl%C3%BCckiger+Jonas+Hein+Valery+Fischer+Philipp+F%C3%BCrnstahl+Lilian+Calvet",
    "gs_search_success": true,
    "gs_authors": [
      "Kk_o9AYAAAAJ",
      "6JewdrMAAAAJ",
      "nQ4B3BgAAAAJ"
    ],
    "citation_count": 2,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2402.13778",
    "title": "Weakly supervised localisation of prostate cancer using reinforcement learning for bi-parametric MR images",
    "year": 2024,
    "published": "2024-02-21T12:57:43Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "In this paper we propose a reinforcement learning based weakly supervised system for localisation. We train a controller function to localise regions of interest within an image by introducing a novel reward definition that utilises non-binarised classification probability, generated by a pre-trained binary classifier which classifies object presence in images or image crops. The object-presence classifier may then inform the controller of its localisation quality by quantifying the likelihood o",
    "arxiv_url": "https://arxiv.org/abs/2402.13778v1",
    "pdf_url": "https://arxiv.org/pdf/2402.13778v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2402.13778",
    "arxiv_authors": [
      "Martynas Pocius",
      "Wen Yan",
      "Dean C. Barratt",
      "Mark Emberton",
      "Matthew J. Clarkson",
      "Yipeng Hu",
      "Shaheer U. Saeed"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Weakly+supervised+localisation+of+prostate+cancer+using+reinforcement+learning+for+bi-parametric+MR+images+Martynas+Pocius+Wen+Yan+Dean+C.+Barratt+Mark+Emberton+Matthew+J.+Clarkson",
    "gs_search_success": true,
    "gs_authors": [
      "_jYXK0IAAAAJ",
      "mnABWkIAAAAJ",
      "bE6rIJEAAAAJ",
      "hgqgNbsAAAAJ",
      "dfHLpIYAAAAJ",
      "-nqARLAAAAAJ"
    ],
    "citation_count": 5,
    "gs_author_count": 7
  },
  {
    "arxiv_id": "2310.07223",
    "title": "Bidirectional recurrent imputation and abundance estimation of LULC classes with MODIS multispectral time series and geo-topographic and climatic data",
    "year": 2023,
    "published": "2023-10-11T06:13:50Z",
    "categories": [
      "cs.CV",
      "cs.LG",
      "eess.IV"
    ],
    "abstract": "Remotely sensed data are dominated by mixed Land Use and Land Cover (LULC) types. Spectral unmixing (SU) is a key technique that disentangles mixed pixels into constituent LULC types and their abundance fractions. While existing studies on Deep Learning (DL) for SU typically focus on single time-step hyperspectral (HS) or multispectral (MS) data, our work pioneers SU using MODIS MS time series, addressing missing data with end-to-end DL models. Our approach enhances a Long-Short Term Memory (LST",
    "arxiv_url": "https://arxiv.org/abs/2310.07223v3",
    "pdf_url": "https://arxiv.org/pdf/2310.07223v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2310.07223",
    "arxiv_authors": [
      "José Rodríguez-Ortega",
      "Rohaifa Khaldi",
      "Domingo Alcaraz-Segura",
      "Siham Tabik"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Bidirectional+recurrent+imputation+and+abundance+estimation+of+LULC+classes+with+MODIS+multispectral+time+series+and+geo-topographic+and+climatic+data+Jos%C3%A9+Rodr%C3%ADguez-Ortega+Rohaifa+Khaldi+Domingo+Alcaraz-Segura+Siham+Tabik",
    "gs_search_success": true,
    "gs_authors": [
      "_IaQxa8AAAAJ",
      "jsPSrRQAAAAJ",
      "2nE7TNgAAAAJ",
      "3HGXp-IAAAAJ"
    ],
    "citation_count": 3,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2410.23092",
    "title": "First Place Solution to the ECCV 2024 ROAD++ Challenge @ ROAD++ Atomic Activity Recognition 2024",
    "year": 2024,
    "published": "2024-10-30T15:06:58Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "This report presents our team's technical solution for participating in Track 3 of the 2024 ECCV ROAD++ Challenge. The task of Track 3 is atomic activity recognition, which aims to identify 64 types of atomic activities in road scenes based on video content. Our approach primarily addresses the challenges of small objects, discriminating between single object and a group of objects, as well as model overfitting in this task. Firstly, we construct a multi-branch activity recognition framework tha",
    "arxiv_url": "https://arxiv.org/abs/2410.23092v1",
    "pdf_url": "https://arxiv.org/pdf/2410.23092v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2410.23092",
    "arxiv_authors": [
      "Ruyang Li",
      "Tengfei Zhang",
      "Heng Zhang",
      "Tiejun Liu",
      "Yanwei Wang",
      "Xuelei Li"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=First+Place+Solution+to+the+ECCV+2024+ROAD%2B%2B+Challenge+%40+ROAD%2B%2B+Atomic+Activity+Recognition+2024+Ruyang+Li+Tengfei+Zhang+Heng+Zhang+Tiejun+Liu+Yanwei+Wang",
    "gs_search_success": true,
    "gs_authors": [],
    "citation_count": null,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2409.06485",
    "title": "Mitigating Hallucination in Visual-Language Models via Re-Balancing Contrastive Decoding",
    "year": 2024,
    "published": "2024-09-10T13:13:14Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Although Visual-Language Models (VLMs) have shown impressive capabilities in tasks like visual question answering and image captioning, they still struggle with hallucinations. Analysis of attention distribution in these models shows that VLMs tend to processing textual tokens rather than visual tokens. This imbalance of attention distribution causes VLMs to favor textual knowledge in the case of multimodal knowledge conflicts, resulting in differences from the image information. In this paper, ",
    "arxiv_url": "https://arxiv.org/abs/2409.06485v1",
    "pdf_url": "https://arxiv.org/pdf/2409.06485v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2409.06485",
    "arxiv_authors": [
      "Xiaoyu Liang",
      "Jiayuan Yu",
      "Lianrui Mu",
      "Jiedong Zhuang",
      "Jiaqi Hu",
      "Yuchen Yang",
      "Jiangnan Ye",
      "Lu Lu",
      "Jian Chen",
      "Haoji Hu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Mitigating+Hallucination+in+Visual-Language+Models+via+Re-Balancing+Contrastive+Decoding+Xiaoyu+Liang+Jiayuan+Yu+Lianrui+Mu+Jiedong+Zhuang+Jiaqi+Hu",
    "gs_search_success": true,
    "gs_authors": [
      "cAitmk0AAAAJ",
      "aiy5G0oAAAAJ",
      "dCik-2YAAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 9
  },
  {
    "arxiv_id": "2304.04595",
    "title": "Scale-Equivariant UNet for Histopathology Image Segmentation",
    "year": 2023,
    "published": "2023-04-10T14:03:08Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Digital histopathology slides are scanned and viewed under different magnifications and stored as images at different resolutions. Convolutional Neural Networks (CNNs) trained on such images at a given scale fail to generalise to those at different scales. This inability is often addressed by augmenting training data with re-scaled images, allowing a model with sufficient capacity to learn the requisite patterns. Alternatively, designing CNN filters to be scale-equivariant frees up model capacit",
    "arxiv_url": "https://arxiv.org/abs/2304.04595v1",
    "pdf_url": "https://arxiv.org/pdf/2304.04595v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2304.04595",
    "arxiv_authors": [
      "Yilong Yang",
      "Srinandan Dasmahapatra",
      "Sasan Mahmoodi"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Scale-Equivariant+UNet+for+Histopathology+Image+Segmentation+Yilong+Yang+Srinandan+Dasmahapatra+Sasan+Mahmoodi",
    "gs_search_success": true,
    "gs_authors": [
      "O784PiEAAAAJ",
      "4FVYygkAAAAJ"
    ],
    "citation_count": 18,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2402.14309",
    "title": "YOLO-TLA: An Efficient and Lightweight Small Object Detection Model based on YOLOv5",
    "year": 2024,
    "published": "2024-02-22T05:55:17Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Object detection, a crucial aspect of computer vision, has seen significant advancements in accuracy and robustness. Despite these advancements, practical applications still face notable challenges, primarily the inaccurate detection or missed detection of small objects. In this paper, we propose YOLO-TLA, an advanced object detection model building on YOLOv5. We first introduce an additional detection layer for small objects in the neck network pyramid architecture, thereby producing a feature ",
    "arxiv_url": "https://arxiv.org/abs/2402.14309v2",
    "pdf_url": "https://arxiv.org/pdf/2402.14309v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2402.14309",
    "arxiv_authors": [
      "Chun-Lin Ji",
      "Tao Yu",
      "Peng Gao",
      "Fei Wang",
      "Ru-Yue Yuan"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=YOLO-TLA%3A+An+Efficient+and+Lightweight+Small+Object+Detection+Model+based+on+YOLOv5+Chun-Lin+Ji+Tao+Yu+Peng+Gao+Fei+Wang+Ru-Yue+Yuan",
    "gs_search_success": true,
    "gs_authors": [
      "qSxzuocAAAAJ"
    ],
    "citation_count": 77,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2503.07952",
    "title": "NeRF-VIO: Map-Based Visual-Inertial Odometry with Initialization Leveraging Neural Radiance Fields",
    "year": 2025,
    "published": "2025-03-11T01:23:22Z",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "abstract": "A prior map serves as a foundational reference for localization in context-aware applications such as augmented reality (AR). Providing valuable contextual information about the environment, the prior map is a vital tool for mitigating drift. In this paper, we propose a map-based visual-inertial localization algorithm (NeRF-VIO) with initialization using neural radiance fields (NeRF). Our algorithm utilizes a multilayer perceptron model and redefines the loss function as the geodesic distance on",
    "arxiv_url": "https://arxiv.org/abs/2503.07952v1",
    "pdf_url": "https://arxiv.org/pdf/2503.07952v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2503.07952",
    "arxiv_authors": [
      "Yanyu Zhang",
      "Dongming Wang",
      "Jie Xu",
      "Mengyuan Liu",
      "Pengxiang Zhu",
      "Wei Ren"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=NeRF-VIO%3A+Map-Based+Visual-Inertial+Odometry+with+Initialization+Leveraging+Neural+Radiance+Fields+Yanyu+Zhang+Dongming+Wang+Jie+Xu+Mengyuan+Liu+Pengxiang+Zhu",
    "gs_search_success": true,
    "gs_authors": [
      "K8riPnMAAAAJ",
      "caDPgZkAAAAJ",
      "-tAyfhgAAAAJ",
      "WX3oPGIAAAAJ"
    ],
    "citation_count": 4,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2306.05888",
    "title": "TrajectoryFormer: 3D Object Tracking Transformer with Predictive Trajectory Hypotheses",
    "year": 2023,
    "published": "2023-06-09T13:31:50Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "3D multi-object tracking (MOT) is vital for many applications including autonomous driving vehicles and service robots. With the commonly used tracking-by-detection paradigm, 3D MOT has made important progress in recent years. However, these methods only use the detection boxes of the current frame to obtain trajectory-box association results, which makes it impossible for the tracker to recover objects missed by the detector. In this paper, we present TrajectoryFormer, a novel point-cloud-based",
    "arxiv_url": "https://arxiv.org/abs/2306.05888v2",
    "pdf_url": "https://arxiv.org/pdf/2306.05888v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2306.05888",
    "arxiv_authors": [
      "Xuesong Chen",
      "Shaoshuai Shi",
      "Chao Zhang",
      "Benjin Zhu",
      "Qiang Wang",
      "Ka Chun Cheung",
      "Simon See",
      "Hongsheng Li"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=TrajectoryFormer%3A+3D+Object+Tracking+Transformer+with+Predictive+Trajectory+Hypotheses+Xuesong+Chen+Shaoshuai+Shi+Chao+Zhang+Benjin+Zhu+Qiang+Wang",
    "gs_search_success": true,
    "gs_authors": [
      "DC9wzBgAAAAJ",
      "Gpu-_58AAAAJ",
      "mo6St_YAAAAJ",
      "BN2Ze-QAAAAJ",
      "NvbCXToAAAAJ",
      "ebIHTEoAAAAJ",
      "_XZonAsAAAAJ",
      "rhuzgdgAAAAJ"
    ],
    "citation_count": 24,
    "gs_author_count": 8
  },
  {
    "arxiv_id": "2504.09506",
    "title": "Pillar-Voxel Fusion Network for 3D Object Detection in Airborne Hyperspectral Point Clouds",
    "year": 2025,
    "published": "2025-04-13T10:13:48Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Hyperspectral point clouds (HPCs) can simultaneously characterize 3D spatial and spectral information of ground objects, offering excellent 3D perception and target recognition capabilities. Current approaches for generating HPCs often involve fusion techniques with hyperspectral images and LiDAR point clouds, which inevitably lead to geometric-spectral distortions due to fusion errors and obstacle occlusions. These adverse effects limit their performance in downstream fine-grained tasks across ",
    "arxiv_url": "https://arxiv.org/abs/2504.09506v2",
    "pdf_url": "https://arxiv.org/pdf/2504.09506v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2504.09506",
    "arxiv_authors": [
      "Yanze Jiang",
      "Yanfeng Gu",
      "Xian Li"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Pillar-Voxel+Fusion+Network+for+3D+Object+Detection+in+Airborne+Hyperspectral+Point+Clouds+Yanze+Jiang+Yanfeng+Gu+Xian+Li",
    "gs_search_success": true,
    "gs_authors": [
      "k-9e_DYAAAAJ",
      "WHkRZscAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2405.04595",
    "title": "An Advanced Features Extraction Module for Remote Sensing Image Super-Resolution",
    "year": 2024,
    "published": "2024-05-07T18:15:51Z",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "abstract": "In recent years, convolutional neural networks (CNNs) have achieved remarkable advancement in the field of remote sensing image super-resolution due to the complexity and variability of textures and structures in remote sensing images (RSIs), which often repeat in the same images but differ across others. Current deep learning-based super-resolution models focus less on high-frequency features, which leads to suboptimal performance in capturing contours, textures, and spatial information. State-",
    "arxiv_url": "https://arxiv.org/abs/2405.04595v1",
    "pdf_url": "https://arxiv.org/pdf/2405.04595v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2405.04595",
    "arxiv_authors": [
      "Naveed Sultan",
      "Amir Hajian",
      "Supavadee Aramvith"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=An+Advanced+Features+Extraction+Module+for+Remote+Sensing+Image+Super-Resolution+Naveed+Sultan+Amir+Hajian+Supavadee+Aramvith",
    "gs_search_success": true,
    "gs_authors": [
      "Xm3sMKgAAAAJ",
      "hEnjgp4AAAAJ"
    ],
    "citation_count": 14,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2411.17221",
    "title": "AIGV-Assessor: Benchmarking and Evaluating the Perceptual Quality of Text-to-Video Generation with LMM",
    "year": 2024,
    "published": "2024-11-26T08:43:15Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "The rapid advancement of large multimodal models (LMMs) has led to the rapid expansion of artificial intelligence generated videos (AIGVs), which highlights the pressing need for effective video quality assessment (VQA) models designed specifically for AIGVs. Current VQA models generally fall short in accurately assessing the perceptual quality of AIGVs due to the presence of unique distortions, such as unrealistic objects, unnatural movements, or inconsistent visual elements. To address this ch",
    "arxiv_url": "https://arxiv.org/abs/2411.17221v1",
    "pdf_url": "https://arxiv.org/pdf/2411.17221v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2411.17221",
    "arxiv_authors": [
      "Jiarui Wang",
      "Huiyu Duan",
      "Guangtao Zhai",
      "Juntong Wang",
      "Xiongkuo Min"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=AIGV-Assessor%3A+Benchmarking+and+Evaluating+the+Perceptual+Quality+of+Text-to-Video+Generation+with+LMM+Jiarui+Wang+Huiyu+Duan+Guangtao+Zhai+Juntong+Wang+Xiongkuo+Min",
    "gs_search_success": true,
    "gs_authors": [
      "91sjuWIAAAAJ",
      "x0tDoYAAAAAJ",
      "E6zbSYgAAAAJ",
      "lGt-KkwAAAAJ",
      "r0bRaCMAAAAJ"
    ],
    "citation_count": 22,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2407.16503",
    "title": "HDRSplat: Gaussian Splatting for High Dynamic Range 3D Scene Reconstruction from Raw Images",
    "year": 2024,
    "published": "2024-07-23T14:21:00Z",
    "categories": [
      "cs.CV",
      "eess.IV"
    ],
    "abstract": "The recent advent of 3D Gaussian Splatting (3DGS) has revolutionized the 3D scene reconstruction space enabling high-fidelity novel view synthesis in real-time. However, with the exception of RawNeRF, all prior 3DGS and NeRF-based methods rely on 8-bit tone-mapped Low Dynamic Range (LDR) images for scene reconstruction. Such methods struggle to achieve accurate reconstructions in scenes that require a higher dynamic range. Examples include scenes captured in nighttime or poorly lit indoor spaces",
    "arxiv_url": "https://arxiv.org/abs/2407.16503v1",
    "pdf_url": "https://arxiv.org/pdf/2407.16503v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2407.16503",
    "arxiv_authors": [
      "Shreyas Singh",
      "Aryan Garg",
      "Kaushik Mitra"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=HDRSplat%3A+Gaussian+Splatting+for+High+Dynamic+Range+3D+Scene+Reconstruction+from+Raw+Images+Shreyas+Singh+Aryan+Garg+Kaushik+Mitra",
    "gs_search_success": true,
    "gs_authors": [
      "TeDLg10AAAAJ",
      "cZBq_9MAAAAJ",
      "z_VblYsAAAAJ"
    ],
    "citation_count": 15,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2504.10972",
    "title": "AFiRe: Anatomy-Driven Self-Supervised Learning for Fine-Grained Representation in Radiographic Images",
    "year": 2025,
    "published": "2025-04-15T08:29:54Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Current self-supervised methods, such as contrastive learning, predominantly focus on global discrimination, neglecting the critical fine-grained anatomical details required for accurate radiographic analysis. To address this challenge, we propose an Anatomy-driven self-supervised framework for enhancing Fine-grained Representation in radiographic image analysis (AFiRe). The core idea of AFiRe is to align the anatomical consistency with the unique token-processing characteristics of Vision Trans",
    "arxiv_url": "https://arxiv.org/abs/2504.10972v2",
    "pdf_url": "https://arxiv.org/pdf/2504.10972v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2504.10972",
    "arxiv_authors": [
      "Yihang Liu",
      "Lianghua He",
      "Ying Wen",
      "Longzhen Yang",
      "Hongzhou Chen"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=AFiRe%3A+Anatomy-Driven+Self-Supervised+Learning+for+Fine-Grained+Representation+in+Radiographic+Images+Yihang+Liu+Lianghua+He+Ying+Wen+Longzhen+Yang+Hongzhou+Chen",
    "gs_search_success": false,
    "gs_authors": [],
    "citation_count": null,
    "error_type": "fetch_failed",
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2308.02668",
    "title": "Guided Distillation for Semi-Supervised Instance Segmentation",
    "year": 2023,
    "published": "2023-08-03T13:25:04Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "abstract": "Although instance segmentation methods have improved considerably, the dominant paradigm is to rely on fully-annotated training images, which are tedious to obtain. To alleviate this reliance, and boost results, semi-supervised approaches leverage unlabeled data as an additional training signal that limits overfitting to the labeled samples. In this context, we present novel design choices to significantly improve teacher-student distillation models. In particular, we (i) improve the distillatio",
    "arxiv_url": "https://arxiv.org/abs/2308.02668v2",
    "pdf_url": "https://arxiv.org/pdf/2308.02668v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2308.02668",
    "arxiv_authors": [
      "Tariq Berrada",
      "Camille Couprie",
      "Karteek Alahari",
      "Jakob Verbeek"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Guided+Distillation+for+Semi-Supervised+Instance+Segmentation+Tariq+Berrada+Camille+Couprie+Karteek+Alahari+Jakob+Verbeek",
    "gs_search_success": true,
    "gs_authors": [
      "oZGA-rAAAAAJ",
      "qcyG7rwAAAAJ",
      "1T2Xh68AAAAJ",
      "StQIGq8AAAAJ"
    ],
    "citation_count": 23,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2306.17115",
    "title": "Michelangelo: Conditional 3D Shape Generation based on Shape-Image-Text Aligned Latent Representation",
    "year": 2023,
    "published": "2023-06-29T17:17:57Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "We present a novel alignment-before-generation approach to tackle the challenging task of generating general 3D shapes based on 2D images or texts. Directly learning a conditional generative model from images or texts to 3D shapes is prone to producing inconsistent results with the conditions because 3D shapes have an additional dimension whose distribution significantly differs from that of 2D images and texts. To bridge the domain gap among the three modalities and facilitate multi-modal-condi",
    "arxiv_url": "https://arxiv.org/abs/2306.17115v2",
    "pdf_url": "https://arxiv.org/pdf/2306.17115v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2306.17115",
    "arxiv_authors": [
      "Zibo Zhao",
      "Wen Liu",
      "Xin Chen",
      "Xianfang Zeng",
      "Rui Wang",
      "Pei Cheng",
      "Bin Fu",
      "Tao Chen",
      "Gang Yu",
      "Shenghua Gao"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Michelangelo%3A+Conditional+3D+Shape+Generation+based+on+Shape-Image-Text+Aligned+Latent+Representation+Zibo+Zhao+Wen+Liu+Xin+Chen+Xianfang+Zeng+Rui+Wang",
    "gs_search_success": true,
    "gs_authors": [
      "tgDc0fsAAAAJ",
      "nVR6p7cAAAAJ",
      "fe-1v0MAAAAJ",
      "7qeAJZ4AAAAJ",
      "A6K6bkoAAAAJ",
      "w3OoFL0AAAAJ",
      "BJdigYsAAAAJ"
    ],
    "citation_count": 155,
    "gs_author_count": 9
  },
  {
    "arxiv_id": "2502.14890",
    "title": "WeedVision: Multi-Stage Growth and Classification of Weeds using DETR and RetinaNet for Precision Agriculture",
    "year": 2025,
    "published": "2025-02-16T20:49:22Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Weed management remains a critical challenge in agriculture, where weeds compete with crops for essential resources, leading to significant yield losses. Accurate detection of weeds at various growth stages is crucial for effective management yet challenging for farmers, as it requires identifying different species at multiple growth phases. This research addresses these challenges by utilizing advanced object detection models, specifically, the Detection Transformer (DETR) with a ResNet50 backb",
    "arxiv_url": "https://arxiv.org/abs/2502.14890v1",
    "pdf_url": "https://arxiv.org/pdf/2502.14890v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2502.14890",
    "arxiv_authors": [
      "Taminul Islam",
      "Toqi Tahamid Sarker",
      "Khaled R Ahmed",
      "Cristiana Bernardi Rankrape",
      "Karla Gage"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=WeedVision%3A+Multi-Stage+Growth+and+Classification+of+Weeds+using+DETR+and+RetinaNet+for+Precision+Agriculture+Taminul+Islam+Toqi+Tahamid+Sarker+Khaled+R+Ahmed+Cristiana+Bernardi+Rankrape+Karla+Gage",
    "gs_search_success": true,
    "gs_authors": [
      "Kgo_S9sAAAAJ",
      "i1SmuwYAAAAJ",
      "6IqTa8AAAAAJ",
      "CNb5Z4UAAAAJ",
      "FYKqgh4AAAAJ"
    ],
    "citation_count": 4,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2308.10196",
    "title": "Blind Face Restoration for Under-Display Camera via Dictionary Guided Transformer",
    "year": 2023,
    "published": "2023-08-20T08:02:23Z",
    "categories": [
      "cs.CV",
      "eess.IV"
    ],
    "abstract": "By hiding the front-facing camera below the display panel, Under-Display Camera (UDC) provides users with a full-screen experience. However, due to the characteristics of the display, images taken by UDC suffer from significant quality degradation. Methods have been proposed to tackle UDC image restoration and advances have been achieved. There are still no specialized methods and datasets for restoring UDC face images, which may be the most common problem in the UDC scene. To this end, consider",
    "arxiv_url": "https://arxiv.org/abs/2308.10196v2",
    "pdf_url": "https://arxiv.org/pdf/2308.10196v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2308.10196",
    "arxiv_authors": [
      "Jingfan Tan",
      "Xiaoxu Chen",
      "Tao Wang",
      "Kaihao Zhang",
      "Wenhan Luo",
      "Xiaocun Cao"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Blind+Face+Restoration+for+Under-Display+Camera+via+Dictionary+Guided+Transformer+Jingfan+Tan+Xiaoxu+Chen+Tao+Wang+Kaihao+Zhang+Wenhan+Luo",
    "gs_search_success": true,
    "gs_authors": [
      "l9iowsYAAAAJ",
      "dvjRjHoAAAAJ",
      "eqwDXdMAAAAJ",
      "g20Q12MAAAAJ"
    ],
    "citation_count": 19,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2304.02407",
    "title": "Explaining Multimodal Data Fusion: Occlusion Analysis for Wilderness Mapping",
    "year": 2023,
    "published": "2023-04-05T12:35:02Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "abstract": "Jointly harnessing complementary features of multi-modal input data in a common latent space has been found to be beneficial long ago. However, the influence of each modality on the models decision remains a puzzle. This study proposes a deep learning framework for the modality-level interpretation of multimodal earth observation data in an end-to-end fashion. While leveraging an explainable machine learning method, namely Occlusion Sensitivity, the proposed framework investigates the influence ",
    "arxiv_url": "https://arxiv.org/abs/2304.02407v1",
    "pdf_url": "https://arxiv.org/pdf/2304.02407v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2304.02407",
    "arxiv_authors": [
      "Burak Ekim",
      "Michael Schmitt"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Explaining+Multimodal+Data+Fusion%3A+Occlusion+Analysis+for+Wilderness+Mapping+Burak+Ekim+Michael+Schmitt",
    "gs_search_success": true,
    "gs_authors": [
      "fv_5G_4AAAAJ",
      "CVnD4h4AAAAJ"
    ],
    "citation_count": 4,
    "gs_author_count": 2
  },
  {
    "arxiv_id": "2404.05814",
    "title": "Towards Explainable Automated Neuroanatomy",
    "year": 2024,
    "published": "2024-04-08T18:36:18Z",
    "categories": [
      "cs.CV",
      "q-bio.NC"
    ],
    "abstract": "We present a novel method for quantifying the microscopic structure of brain tissue. It is based on the automated recognition of interpretable features obtained by analyzing the shapes of cells. This contrasts with prevailing methods of brain anatomical analysis in two ways. First, contemporary methods use gray-scale values derived from smoothed version of the anatomical images, which dissipated valuable information from the texture of the images. Second, contemporary analysis uses the output of",
    "arxiv_url": "https://arxiv.org/abs/2404.05814v1",
    "pdf_url": "https://arxiv.org/pdf/2404.05814v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2404.05814",
    "arxiv_authors": [
      "Kui Qian",
      "Litao Qiao",
      "Beth Friedman",
      "Edward O'Donnell",
      "David Kleinfeld",
      "Yoav Freund"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Towards+Explainable+Automated+Neuroanatomy+Kui+Qian+Litao+Qiao+Beth+Friedman+Edward+O%27Donnell+David+Kleinfeld",
    "gs_search_success": true,
    "gs_authors": [
      "vRlgHrUAAAAJ",
      "51Q5DR8AAAAJ",
      "hYlrJuAAAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2409.10980",
    "title": "PSFHS Challenge Report: Pubic Symphysis and Fetal Head Segmentation from Intrapartum Ultrasound Images",
    "year": 2024,
    "published": "2024-09-17T08:24:34Z",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "abstract": "Segmentation of the fetal and maternal structures, particularly intrapartum ultrasound imaging as advocated by the International Society of Ultrasound in Obstetrics and Gynecology (ISUOG) for monitoring labor progression, is a crucial first step for quantitative diagnosis and clinical decision-making. This requires specialized analysis by obstetrics professionals, in a task that i) is highly time- and cost-consuming and ii) often yields inconsistent results. The utility of automatic segmentation",
    "arxiv_url": "https://arxiv.org/abs/2409.10980v1",
    "pdf_url": "https://arxiv.org/pdf/2409.10980v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2409.10980",
    "arxiv_authors": [
      "Jieyun Bai",
      "Zihao Zhou",
      "Zhanhong Ou",
      "Gregor Koehler",
      "Raphael Stock",
      "Klaus Maier-Hein",
      "Marawan Elbatel",
      "Robert Martí",
      "Xiaomeng Li",
      "Yaoyang Qiu",
      "Panjie Gou",
      "Gongping Chen",
      "Lei Zhao",
      "Jianxun Zhang",
      "Yu Dai",
      "Fangyijie Wang",
      "Guénolé Silvestre",
      "Kathleen Curran",
      "Hongkun Sun",
      "Jing Xu",
      "Pengzhou Cai",
      "Lu Jiang",
      "Libin Lan",
      "Dong Ni",
      "Mei Zhong",
      "Gaowen Chen",
      "Víctor M. Campello",
      "Yaosheng Lu",
      "Karim Lekadir"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=PSFHS+Challenge+Report%3A+Pubic+Symphysis+and+Fetal+Head+Segmentation+from+Intrapartum+Ultrasound+Images+Jieyun+Bai+Zihao+Zhou+Zhanhong+Ou+Gregor+Koehler+Raphael+Stock",
    "gs_search_success": true,
    "gs_authors": [
      "oCrBpVMAAAAJ",
      "f6yps4gAAAAJ",
      "9mLPQDwAAAAJ",
      "b8U4UTAAAAAJ",
      "uVTzPpoAAAAJ",
      "S-tEilMAAAAJ",
      "M_sM6x8AAAAJ"
    ],
    "citation_count": 16,
    "gs_author_count": 9
  },
  {
    "arxiv_id": "2410.19773",
    "title": "Developing Gridded Emission Inventory from High-Resolution Satellite Object Detection for Improved Air Quality Forecasts",
    "year": 2024,
    "published": "2024-10-14T01:32:45Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "This study presents an innovative approach to creating a dynamic, AI based emission inventory system for use with the Weather Research and Forecasting model coupled with Chemistry (WRF Chem), designed to simulate vehicular and other anthropogenic emissions at satellite detectable resolution. The methodology leverages state of the art deep learning based computer vision models, primarily employing YOLO (You Only Look Once) architectures (v8 to v10) and T Rex, for high precision object detection. ",
    "arxiv_url": "https://arxiv.org/abs/2410.19773v1",
    "pdf_url": "https://arxiv.org/pdf/2410.19773v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2410.19773",
    "arxiv_authors": [
      "Shubham Ghosal",
      "Manmeet Singh",
      "Sachin Ghude",
      "Harsh Kamath",
      "Vaisakh SB",
      "Subodh Wasekar",
      "Anoop Mahajan",
      "Hassan Dashtian",
      "Zong-Liang Yang",
      "Michael Young",
      "Dev Niyogi"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Developing+Gridded+Emission+Inventory+from+High-Resolution+Satellite+Object+Detection+for+Improved+Air+Quality+Forecasts+Shubham+Ghosal+Manmeet+Singh+Sachin+Ghude+Harsh+Kamath+Vaisakh+SB",
    "gs_search_success": true,
    "gs_authors": [
      "kAW3RogAAAAJ",
      "Ik5Xt6wAAAAJ",
      "_GJ1x44AAAAJ",
      "ZncpGB0AAAAJ",
      "n1US5zQAAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 9
  },
  {
    "arxiv_id": "2505.05591",
    "title": "QuickSplat: Fast 3D Surface Reconstruction via Learned Gaussian Initialization",
    "year": 2025,
    "published": "2025-05-08T18:43:26Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Surface reconstruction is fundamental to computer vision and graphics, enabling applications in 3D modeling, mixed reality, robotics, and more. Existing approaches based on volumetric rendering obtain promising results, but optimize on a per-scene basis, resulting in a slow optimization that can struggle to model under-observed or textureless regions. We introduce QuickSplat, which learns data-driven priors to generate dense initializations for 2D gaussian splatting optimization of large-scale i",
    "arxiv_url": "https://arxiv.org/abs/2505.05591v2",
    "pdf_url": "https://arxiv.org/pdf/2505.05591v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2505.05591",
    "arxiv_authors": [
      "Yueh-Cheng Liu",
      "Lukas Höllein",
      "Matthias Nießner",
      "Angela Dai"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=QuickSplat%3A+Fast+3D+Surface+Reconstruction+via+Learned+Gaussian+Initialization+Yueh-Cheng+Liu+Lukas+H%C3%B6llein+Matthias+Nie%C3%9Fner+Angela+Dai",
    "gs_search_success": true,
    "gs_authors": [
      "eUtEs6YAAAAJ",
      "qCecu60AAAAJ",
      "Te0MGV4AAAAJ",
      "g-tGztMAAAAJ"
    ],
    "citation_count": 4,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2405.20058",
    "title": "Enhancing Plant Disease Detection: A Novel CNN-Based Approach with Tensor Subspace Learning and HOWSVD-MD",
    "year": 2024,
    "published": "2024-05-30T13:46:56Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Machine learning has revolutionized the field of agricultural science, particularly in the early detection and management of plant diseases, which are crucial for maintaining crop health and productivity. Leveraging advanced algorithms and imaging technologies, researchers are now able to identify and classify plant diseases with unprecedented accuracy and speed. Effective management of tomato diseases is crucial for enhancing agricultural productivity. The development and application of tomato ",
    "arxiv_url": "https://arxiv.org/abs/2405.20058v1",
    "pdf_url": "https://arxiv.org/pdf/2405.20058v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2405.20058",
    "arxiv_authors": [
      "Abdelmalik Ouamane",
      "Ammar Chouchane",
      "Yassine Himeur",
      "Abderrazak Debilou",
      "Abbes Amira",
      "Shadi Atalla",
      "Wathiq Mansoor",
      "Hussain Al Ahmad"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Enhancing+Plant+Disease+Detection%3A+A+Novel+CNN-Based+Approach+with+Tensor+Subspace+Learning+and+HOWSVD-MD+Abdelmalik+Ouamane+Ammar+Chouchane+Yassine+Himeur+Abderrazak+Debilou+Abbes+Amira",
    "gs_search_success": true,
    "gs_authors": [
      "d-1rCsAAAAAJ",
      "yp_wQZMAAAAJ",
      "wh4QZPUAAAAJ",
      "QMjnQ4AAAAAJ"
    ],
    "citation_count": 18,
    "gs_author_count": 8
  },
  {
    "arxiv_id": "2407.10427",
    "title": "Transformer for Multitemporal Hyperspectral Image Unmixing",
    "year": 2024,
    "published": "2024-07-15T04:02:01Z",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "abstract": "Multitemporal hyperspectral image unmixing (MTHU) holds significant importance in monitoring and analyzing the dynamic changes of surface. However, compared to single-temporal unmixing, the multitemporal approach demands comprehensive consideration of information across different phases, rendering it a greater challenge. To address this challenge, we propose the Multitemporal Hyperspectral Image Unmixing Transformer (MUFormer), an end-to-end unsupervised deep learning model. To effectively perfo",
    "arxiv_url": "https://arxiv.org/abs/2407.10427v1",
    "pdf_url": "https://arxiv.org/pdf/2407.10427v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2407.10427",
    "arxiv_authors": [
      "Hang Li",
      "Qiankun Dong",
      "Xueshuo Xie",
      "Xia Xu",
      "Tao Li",
      "Zhenwei Shi"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Transformer+for+Multitemporal+Hyperspectral+Image+Unmixing+Hang+Li+Qiankun+Dong+Xueshuo+Xie+Xia+Xu+Tao+Li",
    "gs_search_success": true,
    "gs_authors": [
      "F14qOygAAAAJ",
      "CAjk7n0AAAAJ",
      "FWamm4sAAAAJ",
      "kNhFWQIAAAAJ",
      "9dwOZYwAAAAJ"
    ],
    "citation_count": 4,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2504.18235",
    "title": "BiasBench: A reproducible benchmark for tuning the biases of event cameras",
    "year": 2025,
    "published": "2025-04-25T10:33:24Z",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "abstract": "Event-based cameras are bio-inspired sensors that detect light changes asynchronously for each pixel. They are increasingly used in fields like computer vision and robotics because of several advantages over traditional frame-based cameras, such as high temporal resolution, low latency, and high dynamic range. As with any camera, the output's quality depends on how well the camera's settings, called biases for event-based cameras, are configured. While frame-based cameras have advanced automatic",
    "arxiv_url": "https://arxiv.org/abs/2504.18235v1",
    "pdf_url": "https://arxiv.org/pdf/2504.18235v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2504.18235",
    "arxiv_authors": [
      "Andreas Ziegler",
      "David Joseph",
      "Thomas Gossard",
      "Emil Moldovan",
      "Andreas Zell"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=BiasBench%3A+A+reproducible+benchmark+for+tuning+the+biases+of+event+cameras+Andreas+Ziegler+David+Joseph+Thomas+Gossard+Emil+Moldovan+Andreas+Zell",
    "gs_search_success": true,
    "gs_authors": [
      "go72M2kAAAAJ",
      "cBo1NCwAAAAJ",
      "u7w1MYcAAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2403.06467",
    "title": "Point Mamba: A Novel Point Cloud Backbone Based on State Space Model with Octree-Based Ordering Strategy",
    "year": 2024,
    "published": "2024-03-11T07:07:39Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Recently, state space model (SSM) has gained great attention due to its promising performance, linear complexity, and long sequence modeling ability in both language and image domains. However, it is non-trivial to extend SSM to the point cloud field, because of the causality requirement of SSM and the disorder and irregularity nature of point clouds. In this paper, we propose a novel SSM-based point cloud processing backbone, named Point Mamba, with a causality-aware ordering mechanism. To cons",
    "arxiv_url": "https://arxiv.org/abs/2403.06467v2",
    "pdf_url": "https://arxiv.org/pdf/2403.06467v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2403.06467",
    "arxiv_authors": [
      "Jiuming Liu",
      "Ruiji Yu",
      "Yian Wang",
      "Yu Zheng",
      "Tianchen Deng",
      "Weicai Ye",
      "Hesheng Wang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Point+Mamba%3A+A+Novel+Point+Cloud+Backbone+Based+on+State+Space+Model+with+Octree-Based+Ordering+Strategy+Jiuming+Liu+Ruiji+Yu+Yian+Wang+Yu+Zheng+Tianchen+Deng",
    "gs_search_success": true,
    "gs_authors": [
      "q6AY9XsAAAAJ",
      "M4cXM9kAAAAJ",
      "j4YXCukAAAAJ",
      "Zw2dAssAAAAJ"
    ],
    "citation_count": 91,
    "gs_author_count": 7
  },
  {
    "arxiv_id": "2407.13304",
    "title": "A Dataset and Benchmark for Shape Completion of Fruits for Agricultural Robotics",
    "year": 2024,
    "published": "2024-07-18T09:07:23Z",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "abstract": "As the world population is expected to reach 10 billion by 2050, our agricultural production system needs to double its productivity despite a decline of human workforce in the agricultural sector. Autonomous robotic systems are one promising pathway to increase productivity by taking over labor-intensive manual tasks like fruit picking. To be effective, such systems need to monitor and interact with plants and fruits precisely, which is challenging due to the cluttered nature of agricultural en",
    "arxiv_url": "https://arxiv.org/abs/2407.13304v3",
    "pdf_url": "https://arxiv.org/pdf/2407.13304v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2407.13304",
    "arxiv_authors": [
      "Federico Magistri",
      "Thomas Läbe",
      "Elias Marks",
      "Sumanth Nagulavancha",
      "Yue Pan",
      "Claus Smitt",
      "Lasse Klingbeil",
      "Michael Halstead",
      "Heiner Kuhlmann",
      "Chris McCool",
      "Jens Behley",
      "Cyrill Stachniss"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=A+Dataset+and+Benchmark+for+Shape+Completion+of+Fruits+for+Agricultural+Robotics+Federico+Magistri+Thomas+L%C3%A4be+Elias+Marks+Sumanth+Nagulavancha+Yue+Pan",
    "gs_search_success": true,
    "gs_authors": [
      "GJiaCY4AAAAJ",
      "0qnFBi0AAAAJ",
      "NZuWVDgAAAAJ",
      "irT8pKcAAAAJ",
      "94-yNR0AAAAJ",
      "KPFLkbUAAAAJ"
    ],
    "citation_count": 3,
    "gs_author_count": 9
  },
  {
    "arxiv_id": "2408.00932",
    "title": "Towards Zero-Shot Annotation of the Built Environment with Vision-Language Models (Vision Paper)",
    "year": 2024,
    "published": "2024-08-01T21:50:23Z",
    "categories": [
      "cs.CV",
      "cs.CL"
    ],
    "abstract": "Equitable urban transportation applications require high-fidelity digital representations of the built environment: not just streets and sidewalks, but bike lanes, marked and unmarked crossings, curb ramps and cuts, obstructions, traffic signals, signage, street markings, potholes, and more. Direct inspections and manual annotations are prohibitively expensive at scale. Conventional machine learning methods require substantial annotated training data for adequate performance. In this paper, we c",
    "arxiv_url": "https://arxiv.org/abs/2408.00932v1",
    "pdf_url": "https://arxiv.org/pdf/2408.00932v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2408.00932",
    "arxiv_authors": [
      "Bin Han",
      "Yiwei Yang",
      "Anat Caspi",
      "Bill Howe"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Towards+Zero-Shot+Annotation+of+the+Built+Environment+with+Vision-Language+Models+%28Vision+Paper%29+Bin+Han+Yiwei+Yang+Anat+Caspi+Bill+Howe",
    "gs_search_success": true,
    "gs_authors": [
      "-ZAORukAAAAJ",
      "dQ-x9NQAAAAJ",
      "z0n3JnQAAAAJ",
      "hm0UQUwAAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2308.10362",
    "title": "Vehicle Cameras Guide mmWave Beams: Approach and Real-World V2V Demonstration",
    "year": 2023,
    "published": "2023-08-20T20:43:11Z",
    "categories": [
      "cs.IT",
      "cs.CV",
      "eess.SP"
    ],
    "abstract": "Accurately aligning millimeter-wave (mmWave) and terahertz (THz) narrow beams is essential to satisfy reliability and high data rates of 5G and beyond wireless communication systems. However, achieving this objective is difficult, especially in vehicle-to-vehicle (V2V) communication scenarios, where both transmitter and receiver are constantly mobile. Recently, additional sensing modalities, such as visual sensors, have attracted significant interest due to their capability to provide accurate i",
    "arxiv_url": "https://arxiv.org/abs/2308.10362v1",
    "pdf_url": "https://arxiv.org/pdf/2308.10362v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2308.10362",
    "arxiv_authors": [
      "Tawfik Osman",
      "Gouranga Charan",
      "Ahmed Alkhateeb"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Vehicle+Cameras+Guide+mmWave+Beams%3A+Approach+and+Real-World+V2V+Demonstration+Tawfik+Osman+Gouranga+Charan+Ahmed+Alkhateeb",
    "gs_search_success": true,
    "gs_authors": [
      "dLHw2qcAAAAJ",
      "MHKcvFMAAAAJ",
      "yT8Ge7IAAAAJ"
    ],
    "citation_count": 10,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2407.04507",
    "title": "Few-Shot Airway-Tree Modeling using Data-Driven Sparse Priors",
    "year": 2024,
    "published": "2024-07-05T13:46:11Z",
    "categories": [
      "eess.IV",
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "The lack of large annotated datasets in medical imaging is an intrinsic burden for supervised Deep Learning (DL) segmentation models. Few-shot learning approaches are cost-effective solutions to transfer pre-trained models using only limited annotated data. However, such methods can be prone to overfitting due to limited data diversity especially when segmenting complex, diverse, and sparse tubular structures like airways. Furthermore, crafting informative image representations has played a cruc",
    "arxiv_url": "https://arxiv.org/abs/2407.04507v1",
    "pdf_url": "https://arxiv.org/pdf/2407.04507v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2407.04507",
    "arxiv_authors": [
      "Ali Keshavarzi",
      "Elsa Angelini"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Few-Shot+Airway-Tree+Modeling+using+Data-Driven+Sparse+Priors+Ali+Keshavarzi+Elsa+Angelini",
    "gs_search_success": true,
    "gs_authors": [
      "Fvy_0I4AAAAJ",
      "GBaZhSQAAAAJ"
    ],
    "citation_count": 2,
    "gs_author_count": 2
  },
  {
    "arxiv_id": "2503.17672",
    "title": "A Temporal Modeling Framework for Video Pre-Training on Video Instance Segmentation",
    "year": 2025,
    "published": "2025-03-22T07:01:25Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Contemporary Video Instance Segmentation (VIS) methods typically adhere to a pre-train then fine-tune regime, where a segmentation model trained on images is fine-tuned on videos. However, the lack of temporal knowledge in the pre-trained model introduces a domain gap which may adversely affect the VIS performance. To effectively bridge this gap, we present a novel video pre-training approach to enhance VIS models, especially for videos with intricate instance relationships. Our crucial innovati",
    "arxiv_url": "https://arxiv.org/abs/2503.17672v1",
    "pdf_url": "https://arxiv.org/pdf/2503.17672v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2503.17672",
    "arxiv_authors": [
      "Qing Zhong",
      "Peng-Tao Jiang",
      "Wen Wang",
      "Guodong Ding",
      "Lin Wu",
      "Kaiqi Huang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=A+Temporal+Modeling+Framework+for+Video+Pre-Training+on+Video+Instance+Segmentation+Qing+Zhong+Peng-Tao+Jiang+Wen+Wang+Guodong+Ding+Lin+Wu",
    "gs_search_success": true,
    "gs_authors": [
      "t6-g5_wAAAAJ",
      "mMiJUegAAAAJ",
      "85QJ_i4AAAAJ"
    ],
    "citation_count": 2,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2406.10887",
    "title": "Imperceptible Face Forgery Attack via Adversarial Semantic Mask",
    "year": 2024,
    "published": "2024-06-16T10:38:11Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "With the great development of generative model techniques, face forgery detection draws more and more attention in the related field. Researchers find that existing face forgery models are still vulnerable to adversarial examples with generated pixel perturbations in the global image. These generated adversarial samples still can't achieve satisfactory performance because of the high detectability. To address these problems, we propose an Adversarial Semantic Mask Attack framework (ASMA) which c",
    "arxiv_url": "https://arxiv.org/abs/2406.10887v1",
    "pdf_url": "https://arxiv.org/pdf/2406.10887v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2406.10887",
    "arxiv_authors": [
      "Decheng Liu",
      "Qixuan Su",
      "Chunlei Peng",
      "Nannan Wang",
      "Xinbo Gao"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Imperceptible+Face+Forgery+Attack+via+Adversarial+Semantic+Mask+Decheng+Liu+Qixuan+Su+Chunlei+Peng+Nannan+Wang+Xinbo+Gao",
    "gs_search_success": true,
    "gs_authors": [
      "SRBn7oUAAAAJ",
      "VZVTOOIAAAAJ",
      "U9TnHJgAAAAJ",
      "c5TFU9sAAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2304.13014",
    "title": "Methods and datasets for segmentation of minimally invasive surgical instruments in endoscopic images and videos: A review of the state of the art",
    "year": 2023,
    "published": "2023-04-25T17:38:41Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "In the field of computer- and robot-assisted minimally invasive surgery, enormous progress has been made in recent years based on the recognition of surgical instruments in endoscopic images and videos. In particular, the determination of the position and type of instruments is of great interest. Current work involves both spatial and temporal information, with the idea that predicting the movement of surgical tools over time may improve the quality of the final segmentations. The provision of p",
    "arxiv_url": "https://arxiv.org/abs/2304.13014v4",
    "pdf_url": "https://arxiv.org/pdf/2304.13014v4",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2304.13014",
    "arxiv_authors": [
      "Tobias Rueckert",
      "Daniel Rueckert",
      "Christoph Palm"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Methods+and+datasets+for+segmentation+of+minimally+invasive+surgical+instruments+in+endoscopic+images+and+videos%3A+A+review+of+the+state+of+the+art+Tobias+Rueckert+Daniel+Rueckert+Christoph+Palm",
    "gs_search_success": true,
    "gs_authors": [
      "6alm5WsAAAAJ",
      "VblqqkcAAAAJ",
      "H0O0WnQAAAAJ"
    ],
    "citation_count": 41,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2307.11914",
    "title": "Building3D: An Urban-Scale Dataset and Benchmarks for Learning Roof Structures from Point Clouds",
    "year": 2023,
    "published": "2023-07-21T21:38:57Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Urban modeling from LiDAR point clouds is an important topic in computer vision, computer graphics, photogrammetry and remote sensing. 3D city models have found a wide range of applications in smart cities, autonomous navigation, urban planning and mapping etc. However, existing datasets for 3D modeling mainly focus on common objects such as furniture or cars. Lack of building datasets has become a major obstacle for applying deep learning technology to specific domains such as urban modeling. I",
    "arxiv_url": "https://arxiv.org/abs/2307.11914v1",
    "pdf_url": "https://arxiv.org/pdf/2307.11914v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2307.11914",
    "arxiv_authors": [
      "Ruisheng Wang",
      "Shangfeng Huang",
      "Hongxin Yang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Building3D%3A+An+Urban-Scale+Dataset+and+Benchmarks+for+Learning+Roof+Structures+from+Point+Clouds+Ruisheng+Wang+Shangfeng+Huang+Hongxin+Yang",
    "gs_search_success": true,
    "gs_authors": [
      "iCsHFo4AAAAJ",
      "UWVhokgAAAAJ"
    ],
    "citation_count": 50,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2505.13123",
    "title": "Just Dance with $π$! A Poly-modal Inductor for Weakly-supervised Video Anomaly Detection",
    "year": 2025,
    "published": "2025-05-19T13:51:57Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "abstract": "Weakly-supervised methods for video anomaly detection (VAD) are conventionally based merely on RGB spatio-temporal features, which continues to limit their reliability in real-world scenarios. This is due to the fact that RGB-features are not sufficiently distinctive in setting apart categories such as shoplifting from visually similar events. Therefore, towards robust complex real-world VAD, it is essential to augment RGB spatio-temporal features by additional modalities. Motivated by this, we ",
    "arxiv_url": "https://arxiv.org/abs/2505.13123v1",
    "pdf_url": "https://arxiv.org/pdf/2505.13123v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2505.13123",
    "arxiv_authors": [
      "Snehashis Majhi",
      "Giacomo D'Amicantonio",
      "Antitza Dantcheva",
      "Quan Kong",
      "Lorenzo Garattoni",
      "Gianpiero Francesca",
      "Egor Bondarev",
      "Francois Bremond"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Just+Dance+with+%24%CF%80%24%21+A+Poly-modal+Inductor+for+Weakly-supervised+Video+Anomaly+Detection+Snehashis+Majhi+Giacomo+D%27Amicantonio+Antitza+Dantcheva+Quan+Kong+Lorenzo+Garattoni",
    "gs_search_success": true,
    "gs_authors": [
      "i2ZGcx4AAAAJ",
      "9lqFOsEAAAAJ",
      "zfaaI_EAAAAJ",
      "giSjEBMAAAAJ",
      "8ZQZ_QUAAAAJ",
      "ycpLmvIAAAAJ",
      "ZMggPHMAAAAJ"
    ],
    "citation_count": 4,
    "gs_author_count": 8
  },
  {
    "arxiv_id": "2303.10601",
    "title": "Transfer learning method in the problem of binary classification of chest X-rays",
    "year": 2023,
    "published": "2023-03-19T08:35:47Z",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "abstract": "The possibility of high-precision and rapid detection of pathologies on chest X-rays makes it possible to detect the development of pneumonia at an early stage and begin immediate treatment. Artificial intelligence can speed up and qualitatively improve the procedure of X-ray analysis and give recommendations to the doctor for additional consideration of suspicious images. The purpose of this study is to determine the best models and implementations of the transfer learning method in the binary ",
    "arxiv_url": "https://arxiv.org/abs/2303.10601v1",
    "pdf_url": "https://arxiv.org/pdf/2303.10601v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2303.10601",
    "arxiv_authors": [
      "Kolesnikov Dmitry"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Transfer+learning+method+in+the+problem+of+binary+classification+of+chest+X-rays+Kolesnikov+Dmitry",
    "gs_search_success": true,
    "gs_authors": [],
    "citation_count": null,
    "gs_author_count": 1
  },
  {
    "arxiv_id": "2505.24873",
    "title": "MiniMax-Remover: Taming Bad Noise Helps Video Object Removal",
    "year": 2025,
    "published": "2025-05-30T17:59:45Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Recent advances in video diffusion models have driven rapid progress in video editing techniques. However, video object removal, a critical subtask of video editing, remains challenging due to issues such as hallucinated objects and visual artifacts. Furthermore, existing methods often rely on computationally expensive sampling procedures and classifier-free guidance (CFG), resulting in slow inference. To address these limitations, we propose MiniMax-Remover, a novel two-stage video object remov",
    "arxiv_url": "https://arxiv.org/abs/2505.24873v1",
    "pdf_url": "https://arxiv.org/pdf/2505.24873v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2505.24873",
    "arxiv_authors": [
      "Bojia Zi",
      "Weixuan Peng",
      "Xianbiao Qi",
      "Jianan Wang",
      "Shihao Zhao",
      "Rong Xiao",
      "Kam-Fai Wong"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=MiniMax-Remover%3A+Taming+Bad+Noise+Helps+Video+Object+Removal+Bojia+Zi+Weixuan+Peng+Xianbiao+Qi+Jianan+Wang+Shihao+Zhao",
    "gs_search_success": true,
    "gs_authors": [
      "mt5mvZ8AAAAJ",
      "fyMni2cAAAAJ",
      "Zb5wT08AAAAJ",
      "dNQiLDQAAAAJ",
      "QrMKIkEAAAAJ"
    ],
    "citation_count": 3,
    "gs_author_count": 7
  },
  {
    "arxiv_id": "2308.06868",
    "title": "Camera Based mmWave Beam Prediction: Towards Multi-Candidate Real-World Scenarios",
    "year": 2023,
    "published": "2023-08-14T00:15:01Z",
    "categories": [
      "cs.IT",
      "cs.CV",
      "eess.SP"
    ],
    "abstract": "Leveraging sensory information to aid the millimeter-wave (mmWave) and sub-terahertz (sub-THz) beam selection process is attracting increasing interest. This sensory data, captured for example by cameras at the basestations, has the potential of significantly reducing the beam sweeping overhead and enabling highly-mobile applications. The solutions developed so far, however, have mainly considered single-candidate scenarios, i.e., scenarios with a single candidate user in the visual scene, and w",
    "arxiv_url": "https://arxiv.org/abs/2308.06868v1",
    "pdf_url": "https://arxiv.org/pdf/2308.06868v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2308.06868",
    "arxiv_authors": [
      "Gouranga Charan",
      "Muhammad Alrabeiah",
      "Tawfik Osman",
      "Ahmed Alkhateeb"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Camera+Based+mmWave+Beam+Prediction%3A+Towards+Multi-Candidate+Real-World+Scenarios+Gouranga+Charan+Muhammad+Alrabeiah+Tawfik+Osman+Ahmed+Alkhateeb",
    "gs_search_success": true,
    "gs_authors": [
      "dLHw2qcAAAAJ",
      "MHKcvFMAAAAJ",
      "RtqVGBAAAAAJ",
      "yT8Ge7IAAAAJ"
    ],
    "citation_count": 24,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2408.06970",
    "title": "Prompt-Based Segmentation at Multiple Resolutions and Lighting Conditions using Segment Anything Model 2",
    "year": 2024,
    "published": "2024-08-13T15:27:43Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "This paper provides insights on the effectiveness of the zero shot, prompt-based Segment Anything Model (SAM) and its updated versions, SAM 2 and SAM 2.1, along with the non-promptable conventional neural network (CNN), for segmenting solar panels in RGB aerial remote sensing imagery. The study evaluates these models across diverse lighting conditions, spatial resolutions, and prompt strategies. SAM 2 showed slight improvements over SAM, while SAM 2.1 demonstrated notable improvements, particula",
    "arxiv_url": "https://arxiv.org/abs/2408.06970v4",
    "pdf_url": "https://arxiv.org/pdf/2408.06970v4",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2408.06970",
    "arxiv_authors": [
      "Osher Rafaeli",
      "Tal Svoray",
      "Roni Blushtein-Livnon",
      "Ariel Nahlieli"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Prompt-Based+Segmentation+at+Multiple+Resolutions+and+Lighting+Conditions+using+Segment+Anything+Model+2+Osher+Rafaeli+Tal+Svoray+Roni+Blushtein-Livnon+Ariel+Nahlieli",
    "gs_search_success": true,
    "gs_authors": [
      "4YJ0nyAAAAAJ",
      "rug2UvcAAAAJ",
      "A1n_jPkAAAAJ",
      "-mm6BWMAAAAJ"
    ],
    "citation_count": 18,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2301.13254",
    "title": "Deep Monocular Hazard Detection for Safe Small Body Landing",
    "year": 2023,
    "published": "2023-01-30T19:40:46Z",
    "categories": [
      "cs.CV",
      "cs.RO",
      "eess.IV"
    ],
    "abstract": "Hazard detection and avoidance is a key technology for future robotic small body sample return and lander missions. Current state-of-the-practice methods rely on high-fidelity, a priori terrain maps, which require extensive human-in-the-loop verification and expensive reconnaissance campaigns to resolve mapping uncertainties. We propose a novel safety mapping paradigm that leverages deep semantic segmentation techniques to predict landing safety directly from a single monocular image, thus reduc",
    "arxiv_url": "https://arxiv.org/abs/2301.13254v1",
    "pdf_url": "https://arxiv.org/pdf/2301.13254v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2301.13254",
    "arxiv_authors": [
      "Travis Driver",
      "Kento Tomita",
      "Koki Ho",
      "Panagiotis Tsiotras"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Deep+Monocular+Hazard+Detection+for+Safe+Small+Body+Landing+Travis+Driver+Kento+Tomita+Koki+Ho+Panagiotis+Tsiotras",
    "gs_search_success": true,
    "gs_authors": [
      "z6no7WsAAAAJ",
      "jojWYOoAAAAJ",
      "qmVayjgAAAAJ",
      "p8akSRAAAAAJ"
    ],
    "citation_count": 3,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2308.11484",
    "title": "Pose2Gait: Extracting Gait Features from Monocular Video of Individuals with Dementia",
    "year": 2023,
    "published": "2023-08-22T14:59:17Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Video-based ambient monitoring of gait for older adults with dementia has the potential to detect negative changes in health and allow clinicians and caregivers to intervene early to prevent falls or hospitalizations. Computer vision-based pose tracking models can process video data automatically and extract joint locations; however, publicly available models are not optimized for gait analysis on older adults or clinical populations. In this work we train a deep neural network to map from a two",
    "arxiv_url": "https://arxiv.org/abs/2308.11484v1",
    "pdf_url": "https://arxiv.org/pdf/2308.11484v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2308.11484",
    "arxiv_authors": [
      "Caroline Malin-Mayor",
      "Vida Adeli",
      "Andrea Sabo",
      "Sergey Noritsyn",
      "Carolina Gorodetsky",
      "Alfonso Fasano",
      "Andrea Iaboni",
      "Babak Taati"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Pose2Gait%3A+Extracting+Gait+Features+from+Monocular+Video+of+Individuals+with+Dementia+Caroline+Malin-Mayor+Vida+Adeli+Andrea+Sabo+Sergey+Noritsyn+Carolina+Gorodetsky",
    "gs_search_success": true,
    "gs_authors": [
      "j_mCCb0AAAAJ",
      "JoIj22gAAAAJ",
      "KxhRl2UAAAAJ",
      "7-X6qUUAAAAJ",
      "gqk38ygAAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 8
  },
  {
    "arxiv_id": "2402.18673",
    "title": "Trends, Applications, and Challenges in Human Attention Modelling",
    "year": 2024,
    "published": "2024-02-28T19:35:30Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Human attention modelling has proven, in recent years, to be particularly useful not only for understanding the cognitive processes underlying visual exploration, but also for providing support to artificial intelligence models that aim to solve problems in various domains, including image and video processing, vision-and-language applications, and language modelling. This survey offers a reasoned overview of recent efforts to integrate human attention mechanisms into contemporary deep learning ",
    "arxiv_url": "https://arxiv.org/abs/2402.18673v2",
    "pdf_url": "https://arxiv.org/pdf/2402.18673v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2402.18673",
    "arxiv_authors": [
      "Giuseppe Cartella",
      "Marcella Cornia",
      "Vittorio Cuculo",
      "Alessandro D'Amelio",
      "Dario Zanca",
      "Giuseppe Boccignone",
      "Rita Cucchiara"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Trends%2C+Applications%2C+and+Challenges+in+Human+Attention+Modelling+Giuseppe+Cartella+Marcella+Cornia+Vittorio+Cuculo+Alessandro+D%27Amelio+Dario+Zanca",
    "gs_search_success": true,
    "gs_authors": [
      "chkawtoAAAAJ",
      "0sJ4VCcAAAAJ",
      "OM3sZEoAAAAJ",
      "usEfqxoAAAAJ",
      "DzgmSJEAAAAJ",
      "LqM0uJwAAAAJ",
      "KjwaSXkAAAAJ"
    ],
    "citation_count": 22,
    "gs_author_count": 7
  },
  {
    "arxiv_id": "2411.09462",
    "title": "SINETRA: a Versatile Framework for Evaluating Single Neuron Tracking in Behaving Animals",
    "year": 2024,
    "published": "2024-11-14T14:12:16Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Accurately tracking neuronal activity in behaving animals presents significant challenges due to complex motions and background noise. The lack of annotated datasets limits the evaluation and improvement of such tracking algorithms. To address this, we developed SINETRA, a versatile simulator that generates synthetic tracking data for particles on a deformable background, closely mimicking live animal recordings. This simulator produces annotated 2D and 3D videos that reflect the intricate movem",
    "arxiv_url": "https://arxiv.org/abs/2411.09462v2",
    "pdf_url": "https://arxiv.org/pdf/2411.09462v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2411.09462",
    "arxiv_authors": [
      "Raphael Reme",
      "Alasdair Newson",
      "Elsa Angelini",
      "Jean-Christophe Olivo-Marin",
      "Thibault Lagache"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=SINETRA%3A+a+Versatile+Framework+for+Evaluating+Single+Neuron+Tracking+in+Behaving+Animals+Raphael+Reme+Alasdair+Newson+Elsa+Angelini+Jean-Christophe+Olivo-Marin+Thibault+Lagache",
    "gs_search_success": true,
    "gs_authors": [
      "hQ07GR8AAAAJ",
      "Fvy_0I4AAAAJ",
      "KudRaT8AAAAJ",
      "W5NIAnoAAAAJ",
      "UuEMMkIAAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2406.16346",
    "title": "Directed Domain Fine-Tuning: Tailoring Separate Modalities for Specific Training Tasks",
    "year": 2024,
    "published": "2024-06-24T06:39:02Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Large language models (LLMs) and large visual language models (LVLMs) have been at the forefront of the artificial intelligence field, particularly for tasks like text generation, video captioning, and question-answering. Typically, it is more applicable to train these models on broader knowledge bases or datasets to increase generalizability, learn relationships between topics, and recognize patterns. Instead, we propose to provide instructional datasets specific to the task of each modality wi",
    "arxiv_url": "https://arxiv.org/abs/2406.16346v1",
    "pdf_url": "https://arxiv.org/pdf/2406.16346v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2406.16346",
    "arxiv_authors": [
      "Daniel Wen",
      "Nafisa Hussain"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Directed+Domain+Fine-Tuning%3A+Tailoring+Separate+Modalities+for+Specific+Training+Tasks+Daniel+Wen+Nafisa+Hussain",
    "gs_search_success": true,
    "gs_authors": [
      "pV5EFXIAAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 2
  },
  {
    "arxiv_id": "2403.13570",
    "title": "Portrait4D-v2: Pseudo Multi-View Data Creates Better 4D Head Synthesizer",
    "year": 2024,
    "published": "2024-03-20T13:09:54Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "In this paper, we propose a novel learning approach for feed-forward one-shot 4D head avatar synthesis. Different from existing methods that often learn from reconstructing monocular videos guided by 3DMM, we employ pseudo multi-view videos to learn a 4D head synthesizer in a data-driven manner, avoiding reliance on inaccurate 3DMM reconstruction that could be detrimental to the synthesis performance. The key idea is to first learn a 3D head synthesizer using synthetic multi-view images to conve",
    "arxiv_url": "https://arxiv.org/abs/2403.13570v2",
    "pdf_url": "https://arxiv.org/pdf/2403.13570v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2403.13570",
    "arxiv_authors": [
      "Yu Deng",
      "Duomin Wang",
      "Baoyuan Wang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Portrait4D-v2%3A+Pseudo+Multi-View+Data+Creates+Better+4D+Head+Synthesizer+Yu+Deng+Duomin+Wang+Baoyuan+Wang",
    "gs_search_success": true,
    "gs_authors": [
      "OWa5rOEAAAAJ",
      "N3F3H0sAAAAJ",
      "a6GrP_EAAAAJ"
    ],
    "citation_count": 35,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2409.14444",
    "title": "Fake It till You Make It: Curricular Dynamic Forgery Augmentations towards General Deepfake Detection",
    "year": 2024,
    "published": "2024-09-22T13:51:22Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Previous studies in deepfake detection have shown promising results when testing face forgeries from the same dataset as the training.   However, the problem remains challenging when one tries to generalize the detector to forgeries from unseen datasets and created by unseen methods.   In this work, we present a novel general deepfake detection method, called \\textbf{C}urricular \\textbf{D}ynamic \\textbf{F}orgery \\textbf{A}ugmentation (CDFA), which jointly trains a deepfake detector with a forger",
    "arxiv_url": "https://arxiv.org/abs/2409.14444v1",
    "pdf_url": "https://arxiv.org/pdf/2409.14444v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2409.14444",
    "arxiv_authors": [
      "Yuzhen Lin",
      "Wentang Song",
      "Bin Li",
      "Yuezun Li",
      "Jiangqun Ni",
      "Han Chen",
      "Qiushi Li"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Fake+It+till+You+Make+It%3A+Curricular+Dynamic+Forgery+Augmentations+towards+General+Deepfake+Detection+Yuzhen+Lin+Wentang+Song+Bin+Li+Yuezun+Li+Jiangqun+Ni",
    "gs_search_success": true,
    "gs_authors": [
      "MnLrPLoAAAAJ",
      "SGvZrXgAAAAJ",
      "SthohFEAAAAJ",
      "g0iR9IkAAAAJ",
      "v0Qt7BAAAAAJ"
    ],
    "citation_count": 31,
    "gs_author_count": 7
  },
  {
    "arxiv_id": "2408.11814",
    "title": "SynPlay: Importing Real-world Diversity for a Synthetic Human Dataset",
    "year": 2024,
    "published": "2024-08-21T17:58:49Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "We introduce Synthetic Playground (SynPlay), a new synthetic human dataset that aims to bring out the diversity of human appearance in the real world. We focus on two factors to achieve a level of diversity that has not yet been seen in previous works: i) realistic human motions and poses and ii) multiple camera viewpoints towards human instances. We first use a game engine and its library-provided elementary motions to create games where virtual players can take less-constrained and natural mov",
    "arxiv_url": "https://arxiv.org/abs/2408.11814v1",
    "pdf_url": "https://arxiv.org/pdf/2408.11814v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2408.11814",
    "arxiv_authors": [
      "Jinsub Yim",
      "Hyungtae Lee",
      "Sungmin Eum",
      "Yi-Ting Shen",
      "Yan Zhang",
      "Heesung Kwon",
      "Shuvra S. Bhattacharyya"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=SynPlay%3A+Importing+Real-world+Diversity+for+a+Synthetic+Human+Dataset+Jinsub+Yim+Hyungtae+Lee+Sungmin+Eum+Yi-Ting+Shen+Yan+Zhang",
    "gs_search_success": true,
    "gs_authors": [],
    "citation_count": 1,
    "gs_author_count": 7
  },
  {
    "arxiv_id": "2307.04136",
    "title": "ECL: Class-Enhancement Contrastive Learning for Long-tailed Skin Lesion Classification",
    "year": 2023,
    "published": "2023-07-09T09:29:15Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Skin image datasets often suffer from imbalanced data distribution, exacerbating the difficulty of computer-aided skin disease diagnosis. Some recent works exploit supervised contrastive learning (SCL) for this long-tailed challenge. Despite achieving significant performance, these SCL-based methods focus more on head classes, yet ignoring the utilization of information in tail classes. In this paper, we propose class-Enhancement Contrastive Learning (ECL), which enriches the information of mino",
    "arxiv_url": "https://arxiv.org/abs/2307.04136v1",
    "pdf_url": "https://arxiv.org/pdf/2307.04136v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2307.04136",
    "arxiv_authors": [
      "Yilan Zhang",
      "Jianqi Chen",
      "Ke Wang",
      "Fengying Xie"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=ECL%3A+Class-Enhancement+Contrastive+Learning+for+Long-tailed+Skin+Lesion+Classification+Yilan+Zhang+Jianqi+Chen+Ke+Wang+Fengying+Xie",
    "gs_search_success": true,
    "gs_authors": [
      "7jZZKH0AAAAJ",
      "wZ4M4ecAAAAJ"
    ],
    "citation_count": 18,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2410.15312",
    "title": "Synergistic Dual Spatial-aware Generation of Image-to-Text and Text-to-Image",
    "year": 2024,
    "published": "2024-10-20T06:47:34Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "In the visual spatial understanding (VSU) area, spatial image-to-text (SI2T) and spatial text-to-image (ST2I) are two fundamental tasks that appear in dual form. Existing methods for standalone SI2T or ST2I perform imperfectly in spatial understanding, due to the difficulty of 3D-wise spatial feature modeling. In this work, we consider modeling the SI2T and ST2I together under a dual learning framework. During the dual framework, we then propose to represent the 3D spatial scene features with a ",
    "arxiv_url": "https://arxiv.org/abs/2410.15312v2",
    "pdf_url": "https://arxiv.org/pdf/2410.15312v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2410.15312",
    "arxiv_authors": [
      "Yu Zhao",
      "Hao Fei",
      "Xiangtai Li",
      "Libo Qin",
      "Jiayi Ji",
      "Hongyuan Zhu",
      "Meishan Zhang",
      "Min Zhang",
      "Jianguo Wei"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Synergistic+Dual+Spatial-aware+Generation+of+Image-to-Text+and+Text-to-Image+Yu+Zhao+Hao+Fei+Xiangtai+Li+Libo+Qin+Jiayi+Ji",
    "gs_search_success": false,
    "gs_authors": [],
    "citation_count": null,
    "error_type": "fetch_failed",
    "gs_author_count": 9
  },
  {
    "arxiv_id": "2410.02805",
    "title": "Beyond Uncertainty Quantification: Learning Uncertainty for Trust-Informed Neural Network Decisions - A Case Study in COVID-19 Classification",
    "year": 2024,
    "published": "2024-09-19T04:20:12Z",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "abstract": "Reliable uncertainty quantification is critical in high-stakes applications, such as medical diagnosis, where confidently incorrect predictions can erode trust in automated decision-making systems. Traditional uncertainty quantification methods rely on a predefined confidence threshold to classify predictions as confident or uncertain. However, this approach assumes that predictions exceeding the threshold are trustworthy, while those below it are uncertain, without explicitly assessing the corr",
    "arxiv_url": "https://arxiv.org/abs/2410.02805v2",
    "pdf_url": "https://arxiv.org/pdf/2410.02805v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2410.02805",
    "arxiv_authors": [
      "Hassan Gharoun",
      "Mohammad Sadegh Khorshidi",
      "Fang Chen",
      "Amir H. Gandomi"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Beyond+Uncertainty+Quantification%3A+Learning+Uncertainty+for+Trust-Informed+Neural+Network+Decisions+-+A+Case+Study+in+COVID-19+Classification+Hassan+Gharoun+Mohammad+Sadegh+Khorshidi+Fang+Chen+Amir+H.+Gandomi",
    "gs_search_success": true,
    "gs_authors": [
      "NAuded0AAAAJ",
      "VMf3wfMAAAAJ",
      "EMVGAKgAAAAJ",
      "W2qflDQAAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2412.01485",
    "title": "SerialGen: Personalized Image Generation by First Standardization Then Personalization",
    "year": 2024,
    "published": "2024-12-02T13:35:45Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "In this work, we are interested in achieving both high text controllability and whole-body appearance consistency in the generation of personalized human characters. We propose a novel framework, named SerialGen, which is a serial generation method consisting of two stages: first, a standardization stage that standardizes reference images, and then a personalized generation stage based on the standardized reference. Furthermore, we introduce two modules aimed at enhancing the standardization pro",
    "arxiv_url": "https://arxiv.org/abs/2412.01485v2",
    "pdf_url": "https://arxiv.org/pdf/2412.01485v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2412.01485",
    "arxiv_authors": [
      "Cong Xie",
      "Han Zou",
      "Ruiqi Yu",
      "Yan Zhang",
      "Zhenpeng Zhan"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=SerialGen%3A+Personalized+Image+Generation+by+First+Standardization+Then+Personalization+Cong+Xie+Han+Zou+Ruiqi+Yu+Yan+Zhang+Zhenpeng+Zhan",
    "gs_search_success": true,
    "gs_authors": [
      "U5Gh6lMAAAAJ",
      "dcakOP4AAAAJ",
      "J4nydPEAAAAJ",
      "6yL2BFgAAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2407.01726",
    "title": "Grouped Discrete Representation Guides Object-Centric Learning",
    "year": 2024,
    "published": "2024-07-01T19:00:40Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Similar to humans perceiving visual scenes as objects, Object-Centric Learning (OCL) can abstract dense images or videos into sparse object-level features. Transformer-based OCL handles complex textures well due to the decoding guidance of discrete representation, obtained by discretizing noisy features in image or video feature maps using template features from a codebook. However, treating features as minimal units overlooks their composing attributes, thus impeding model generalization; index",
    "arxiv_url": "https://arxiv.org/abs/2407.01726v3",
    "pdf_url": "https://arxiv.org/pdf/2407.01726v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2407.01726",
    "arxiv_authors": [
      "Rongzhen Zhao",
      "Vivienne Wang",
      "Juho Kannala",
      "Joni Pajarinen"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Grouped+Discrete+Representation+Guides+Object-Centric+Learning+Rongzhen+Zhao+Vivienne+Wang+Juho+Kannala+Joni+Pajarinen",
    "gs_search_success": true,
    "gs_authors": [
      "c4mWQPQAAAAJ",
      "-2fJStwAAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2505.00938",
    "title": "CDFormer: Cross-Domain Few-Shot Object Detection Transformer Against Feature Confusion",
    "year": 2025,
    "published": "2025-05-02T00:46:25Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Cross-domain few-shot object detection (CD-FSOD) aims to detect novel objects across different domains with limited class instances. Feature confusion, including object-background confusion and object-object confusion, presents significant challenges in both cross-domain and few-shot settings. In this work, we introduce CDFormer, a cross-domain few-shot object detection transformer against feature confusion, to address these challenges. The method specifically tackles feature confusion through t",
    "arxiv_url": "https://arxiv.org/abs/2505.00938v1",
    "pdf_url": "https://arxiv.org/pdf/2505.00938v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2505.00938",
    "arxiv_authors": [
      "Boyuan Meng",
      "Xiaohan Zhang",
      "Peilin Li",
      "Zhe Wu",
      "Yiming Li",
      "Wenkai Zhao",
      "Beinan Yu",
      "Hui-Liang Shen"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=CDFormer%3A+Cross-Domain+Few-Shot+Object+Detection+Transformer+Against+Feature+Confusion+Boyuan+Meng+Xiaohan+Zhang+Peilin+Li+Zhe+Wu+Yiming+Li",
    "gs_search_success": false,
    "gs_authors": [],
    "citation_count": null,
    "error_type": "fetch_failed",
    "gs_author_count": 8
  },
  {
    "arxiv_id": "2503.05911",
    "title": "Generalizable Image Repair for Robust Visual Control",
    "year": 2025,
    "published": "2025-03-07T20:16:40Z",
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "abstract": "Vision-based control relies on accurate perception to achieve robustness. However, image distribution changes caused by sensor noise, adverse weather, and dynamic lighting can degrade perception, leading to suboptimal control decisions. Existing approaches, including domain adaptation and adversarial training, improve robustness but struggle to generalize to unseen corruptions while introducing computational overhead. To address this challenge, we propose a real-time image repair module that res",
    "arxiv_url": "https://arxiv.org/abs/2503.05911v2",
    "pdf_url": "https://arxiv.org/pdf/2503.05911v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2503.05911",
    "arxiv_authors": [
      "Carson Sobolewski",
      "Zhenjiang Mao",
      "Kshitij Maruti Vejre",
      "Ivan Ruchkin"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Generalizable+Image+Repair+for+Robust+Visual+Control+Carson+Sobolewski+Zhenjiang+Mao+Kshitij+Maruti+Vejre+Ivan+Ruchkin",
    "gs_search_success": true,
    "gs_authors": [
      "yPEQ7kUAAAAJ",
      "7t0E27IAAAAJ",
      "2c00FlAAAAAJ",
      "6KujFWgAAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2401.15877",
    "title": "3DPFIX: Improving Remote Novices' 3D Printing Troubleshooting through Human-AI Collaboration",
    "year": 2024,
    "published": "2024-01-29T04:16:37Z",
    "categories": [
      "cs.HC",
      "cs.CV"
    ],
    "abstract": "The widespread consumer-grade 3D printers and learning resources online enable novices to self-train in remote settings. While troubleshooting plays an essential part of 3D printing, the process remains challenging for many remote novices even with the help of well-developed online sources, such as online troubleshooting archives and online community help. We conducted a formative study with 76 active 3D printing users to learn how remote novices leverage online resources in troubleshooting and ",
    "arxiv_url": "https://arxiv.org/abs/2401.15877v2",
    "pdf_url": "https://arxiv.org/pdf/2401.15877v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2401.15877",
    "arxiv_authors": [
      "Nahyun Kwon",
      "Tong Sun",
      "Yuyang Gao",
      "Liang Zhao",
      "Xu Wang",
      "Jeeeun Kim",
      "Sungsoo Ray Hong"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=3DPFIX%3A+Improving+Remote+Novices%27+3D+Printing+Troubleshooting+through+Human-AI+Collaboration+Nahyun+Kwon+Tong+Sun+Yuyang+Gao+Liang+Zhao+Xu+Wang",
    "gs_search_success": true,
    "gs_authors": [
      "zNjEAPwAAAAJ",
      "h0J-7rgAAAAJ",
      "Btg0rCAAAAAJ",
      "1v7E8mIAAAAJ",
      "qnvyqtwAAAAJ",
      "D5bxMHYAAAAJ",
      "Fa0scckAAAAJ"
    ],
    "citation_count": 6,
    "gs_author_count": 7
  },
  {
    "arxiv_id": "2409.01679",
    "title": "Adaptive Explicit Knowledge Transfer for Knowledge Distillation",
    "year": 2024,
    "published": "2024-09-03T07:42:59Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Logit-based knowledge distillation (KD) for classification is cost-efficient compared to feature-based KD but often subject to inferior performance. Recently, it was shown that the performance of logit-based KD can be improved by effectively delivering the probability distribution for the non-target classes from the teacher model, which is known as `implicit (dark) knowledge', to the student model. Through gradient analysis, we first show that this actually has an effect of adaptively controllin",
    "arxiv_url": "https://arxiv.org/abs/2409.01679v2",
    "pdf_url": "https://arxiv.org/pdf/2409.01679v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2409.01679",
    "arxiv_authors": [
      "Hyungkeun Park",
      "Jong-Seok Lee"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Adaptive+Explicit+Knowledge+Transfer+for+Knowledge+Distillation+Hyungkeun+Park+Jong-Seok+Lee",
    "gs_search_success": true,
    "gs_authors": [
      "K6HTMQ4AAAAJ"
    ],
    "citation_count": 2,
    "gs_author_count": 2
  },
  {
    "arxiv_id": "2405.17167",
    "title": "Partitioned Hankel-based Diffusion Models for Few-shot Low-dose CT Reconstruction",
    "year": 2024,
    "published": "2024-05-27T13:44:53Z",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "abstract": "Low-dose computed tomography (LDCT) plays a vital role in clinical applications by mitigating radiation risks. Nevertheless, reducing radiation doses significantly degrades image quality. Concurrently, common deep learning methods demand extensive data, posing concerns about privacy, cost, and time constraints. Consequently, we propose a few-shot low-dose CT reconstruction method using Partitioned Hankel-based Diffusion (PHD) models. During the prior learning stage, the projection data is first ",
    "arxiv_url": "https://arxiv.org/abs/2405.17167v1",
    "pdf_url": "https://arxiv.org/pdf/2405.17167v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2405.17167",
    "arxiv_authors": [
      "Wenhao Zhang",
      "Bin Huang",
      "Shuyue Chen",
      "Xiaoling Xu",
      "Weiwen Wu",
      "Qiegen Liu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Partitioned+Hankel-based+Diffusion+Models+for+Few-shot+Low-dose+CT+Reconstruction+Wenhao+Zhang+Bin+Huang+Shuyue+Chen+Xiaoling+Xu+Weiwen+Wu",
    "gs_search_success": true,
    "gs_authors": [
      "T00zMvIAAAAJ",
      "T4o77REAAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2403.03326",
    "title": "AnatoMix: Anatomy-aware Data Augmentation for Multi-organ Segmentation",
    "year": 2024,
    "published": "2024-03-05T21:07:50Z",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "abstract": "Multi-organ segmentation in medical images is a widely researched task and can save much manual efforts of clinicians in daily routines. Automating the organ segmentation process using deep learning (DL) is a promising solution and state-of-the-art segmentation models are achieving promising accuracy. In this work, We proposed a novel data augmentation strategy for increasing the generalizibility of multi-organ segmentation datasets, namely AnatoMix. By object-level matching and manipulation, ou",
    "arxiv_url": "https://arxiv.org/abs/2403.03326v1",
    "pdf_url": "https://arxiv.org/pdf/2403.03326v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2403.03326",
    "arxiv_authors": [
      "Chang Liu",
      "Fuxin Fan",
      "Annette Schwarz",
      "Andreas Maier"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=AnatoMix%3A+Anatomy-aware+Data+Augmentation+for+Multi-organ+Segmentation+Chang+Liu+Fuxin+Fan+Annette+Schwarz+Andreas+Maier",
    "gs_search_success": true,
    "gs_authors": [
      "MA6SDuEAAAAJ",
      "0SnnsAoAAAAJ"
    ],
    "citation_count": 3,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2404.00679",
    "title": "Weak-to-Strong 3D Object Detection with X-Ray Distillation",
    "year": 2024,
    "published": "2024-03-31T13:09:06Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "This paper addresses the critical challenges of sparsity and occlusion in LiDAR-based 3D object detection. Current methods often rely on supplementary modules or specific architectural designs, potentially limiting their applicability to new and evolving architectures. To our knowledge, we are the first to propose a versatile technique that seamlessly integrates into any existing framework for 3D Object Detection, marking the first instance of Weak-to-Strong generalization in 3D computer vision.",
    "arxiv_url": "https://arxiv.org/abs/2404.00679v1",
    "pdf_url": "https://arxiv.org/pdf/2404.00679v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2404.00679",
    "arxiv_authors": [
      "Alexander Gambashidze",
      "Aleksandr Dadukin",
      "Maksim Golyadkin",
      "Maria Razzhivina",
      "Ilya Makarov"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Weak-to-Strong+3D+Object+Detection+with+X-Ray+Distillation+Alexander+Gambashidze+Aleksandr+Dadukin+Maksim+Golyadkin+Maria+Razzhivina+Ilya+Makarov",
    "gs_search_success": true,
    "gs_authors": [
      "DhNH46gAAAAJ",
      "Y_kv_6sAAAAJ",
      "U7gopOoAAAAJ",
      "cFpDMzIAAAAJ"
    ],
    "citation_count": 8,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2505.12861",
    "title": "RMMSS: Towards Advanced Robust Multi-Modal Semantic Segmentation with Hybrid Prototype Distillation and Feature Selection",
    "year": 2025,
    "published": "2025-05-19T08:46:03Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Multi-modal semantic segmentation (MMSS) faces significant challenges in real-world applications due to incomplete, degraded, or missing sensor data. While current MMSS methods typically use self-distillation with modality dropout to improve robustness, they largely overlook inter-modal correlations and thus suffer significant performance degradation when no modalities are missing. To this end, we present RMMSS, a two-stage framework designed to progressively enhance model robustness under missi",
    "arxiv_url": "https://arxiv.org/abs/2505.12861v2",
    "pdf_url": "https://arxiv.org/pdf/2505.12861v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2505.12861",
    "arxiv_authors": [
      "Jiaqi Tan",
      "Xu Zheng",
      "Yang Liu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=RMMSS%3A+Towards+Advanced+Robust+Multi-Modal+Semantic+Segmentation+with+Hybrid+Prototype+Distillation+and+Feature+Selection+Jiaqi+Tan+Xu+Zheng+Yang+Liu",
    "gs_search_success": true,
    "gs_authors": [
      "Ii1c51QAAAAJ",
      "iNkMsdoAAAAJ",
      "E-loHKYAAAAJ",
      "VxQUr90AAAAJ"
    ],
    "citation_count": 2,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2405.09353",
    "title": "Large coordinate kernel attention network for lightweight image super-resolution",
    "year": 2024,
    "published": "2024-05-15T14:03:38Z",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "abstract": "The multi-scale receptive field and large kernel attention (LKA) module have been shown to significantly improve performance in the lightweight image super-resolution task. However, existing lightweight super-resolution (SR) methods seldom pay attention to designing efficient building block with multi-scale receptive field for local modeling, and their LKA modules face a quadratic increase in computational and memory footprints as the convolutional kernel size increases. To address the first iss",
    "arxiv_url": "https://arxiv.org/abs/2405.09353v2",
    "pdf_url": "https://arxiv.org/pdf/2405.09353v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2405.09353",
    "arxiv_authors": [
      "Fangwei Hao",
      "Jiesheng Wu",
      "Haotian Lu",
      "Ji Du",
      "Jing Xu",
      "Xiaoxuan Xu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Large+coordinate+kernel+attention+network+for+lightweight+image+super-resolution+Fangwei+Hao+Jiesheng+Wu+Haotian+Lu+Ji+Du+Jing+Xu",
    "gs_search_success": true,
    "gs_authors": [
      "fDgrxRcAAAAJ"
    ],
    "citation_count": 5,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2305.18060",
    "title": "Mining Negative Temporal Contexts For False Positive Suppression In Real-Time Ultrasound Lesion Detection",
    "year": 2023,
    "published": "2023-05-29T12:53:54Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "During ultrasonic scanning processes, real-time lesion detection can assist radiologists in accurate cancer diagnosis. However, this essential task remains challenging and underexplored. General-purpose real-time object detection models can mistakenly report obvious false positives (FPs) when applied to ultrasound videos, potentially misleading junior radiologists. One key issue is their failure to utilize negative symptoms in previous frames, denoted as negative temporal contexts (NTC). To addr",
    "arxiv_url": "https://arxiv.org/abs/2305.18060v2",
    "pdf_url": "https://arxiv.org/pdf/2305.18060v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2305.18060",
    "arxiv_authors": [
      "Haojun Yu",
      "Youcheng Li",
      "QuanLin Wu",
      "Ziwei Zhao",
      "Dengbo Chen",
      "Dong Wang",
      "Liwei Wang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Mining+Negative+Temporal+Contexts+For+False+Positive+Suppression+In+Real-Time+Ultrasound+Lesion+Detection+Haojun+Yu+Youcheng+Li+QuanLin+Wu+Ziwei+Zhao+Dengbo+Chen",
    "gs_search_success": true,
    "gs_authors": [
      "WmBj0jsAAAAJ",
      "CHMpZBIAAAAJ",
      "KpnMXvMAAAAJ",
      "oW1KZZQAAAAJ",
      "cRWgAzcAAAAJ"
    ],
    "citation_count": 5,
    "gs_author_count": 7
  },
  {
    "arxiv_id": "2412.09599",
    "title": "RatBodyFormer: Rat Body Surface from Keypoints",
    "year": 2024,
    "published": "2024-12-12T18:59:00Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Analyzing rat behavior lies at the heart of many scientific studies. Past methods for automated rodent modeling have focused on 3D pose estimation from keypoints, e.g., face and appendages. The pose, however, does not capture the rich body surface movement encoding the subtle rat behaviors like curling and stretching. The body surface lacks features that can be visually defined, evading these established keypoint-based methods. In this paper, we introduce the first method for reconstructing the ",
    "arxiv_url": "https://arxiv.org/abs/2412.09599v3",
    "pdf_url": "https://arxiv.org/pdf/2412.09599v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2412.09599",
    "arxiv_authors": [
      "Ayaka Higami",
      "Karin Oshima",
      "Tomoyo Isoguchi Shiramatsu",
      "Hirokazu Takahashi",
      "Shohei Nobuhara",
      "Ko Nishino"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=RatBodyFormer%3A+Rat+Body+Surface+from+Keypoints+Ayaka+Higami+Karin+Oshima+Tomoyo+Isoguchi+Shiramatsu+Hirokazu+Takahashi+Shohei+Nobuhara",
    "gs_search_success": true,
    "gs_authors": [
      "flI1-nYAAAAJ",
      "keXiLQ0AAAAJ",
      "5M-ewMAAAAAJ",
      "SXXEZhYAAAAJ",
      "hd2ZxdIAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2404.04880",
    "title": "GauU-Scene V2: Assessing the Reliability of Image-Based Metrics with Expansive Lidar Image Dataset Using 3DGS and NeRF",
    "year": 2024,
    "published": "2024-04-07T08:51:31Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "We introduce a novel, multimodal large-scale scene reconstruction benchmark that utilizes newly developed 3D representation approaches: Gaussian Splatting and Neural Radiance Fields (NeRF). Our expansive U-Scene dataset surpasses any previously existing real large-scale outdoor LiDAR and image dataset in both area and point count. GauU-Scene encompasses over 6.5 square kilometers and features a comprehensive RGB dataset coupled with LiDAR ground truth. Additionally, we are the first to propose a",
    "arxiv_url": "https://arxiv.org/abs/2404.04880v2",
    "pdf_url": "https://arxiv.org/pdf/2404.04880v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2404.04880",
    "arxiv_authors": [
      "Butian Xiong",
      "Nanjun Zheng",
      "Junhua Liu",
      "Zhen Li"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=GauU-Scene+V2%3A+Assessing+the+Reliability+of+Image-Based+Metrics+with+Expansive+Lidar+Image+Dataset+Using+3DGS+and+NeRF+Butian+Xiong+Nanjun+Zheng+Junhua+Liu+Zhen+Li",
    "gs_search_success": true,
    "gs_authors": [
      "I6OCMtUAAAAJ",
      "iuY1-IgAAAAJ"
    ],
    "citation_count": 11,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2409.05531",
    "title": "HMAFlow: Learning More Accurate Optical Flow via Hierarchical Motion Field Alignment",
    "year": 2024,
    "published": "2024-09-09T11:43:35Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Optical flow estimation is a fundamental and long-standing visual task. In this work, we present a novel method, dubbed HMAFlow, to improve optical flow estimation in challenging scenes, particularly those involving small objects. The proposed model mainly consists of two core components: a Hierarchical Motion Field Alignment (HMA) module and a Correlation Self-Attention (CSA) module. In addition, we rebuild 4D cost volumes by employing a Multi-Scale Correlation Search (MCS) layer and replacing ",
    "arxiv_url": "https://arxiv.org/abs/2409.05531v3",
    "pdf_url": "https://arxiv.org/pdf/2409.05531v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2409.05531",
    "arxiv_authors": [
      "Dianbo Ma",
      "Kousuke Imamura",
      "Ziyan Gao",
      "Xiangjie Wang",
      "Satoshi Yamane"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=HMAFlow%3A+Learning+More+Accurate+Optical+Flow+via+Hierarchical+Motion+Field+Alignment+Dianbo+Ma+Kousuke+Imamura+Ziyan+Gao+Xiangjie+Wang+Satoshi+Yamane",
    "gs_search_success": true,
    "gs_authors": [
      "F6lOZpsAAAAJ",
      "c_62MhUAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2312.11057",
    "title": "DataElixir: Purifying Poisoned Dataset to Mitigate Backdoor Attacks via Diffusion Models",
    "year": 2023,
    "published": "2023-12-18T09:40:38Z",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CV"
    ],
    "abstract": "Dataset sanitization is a widely adopted proactive defense against poisoning-based backdoor attacks, aimed at filtering out and removing poisoned samples from training datasets. However, existing methods have shown limited efficacy in countering the ever-evolving trigger functions, and often leading to considerable degradation of benign accuracy. In this paper, we propose DataElixir, a novel sanitization approach tailored to purify poisoned datasets. We leverage diffusion models to eliminate tri",
    "arxiv_url": "https://arxiv.org/abs/2312.11057v2",
    "pdf_url": "https://arxiv.org/pdf/2312.11057v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2312.11057",
    "arxiv_authors": [
      "Jiachen Zhou",
      "Peizhuo Lv",
      "Yibing Lan",
      "Guozhu Meng",
      "Kai Chen",
      "Hualong Ma"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=DataElixir%3A+Purifying+Poisoned+Dataset+to+Mitigate+Backdoor+Attacks+via+Diffusion+Models+Jiachen+Zhou+Peizhuo+Lv+Yibing+Lan+Guozhu+Meng+Kai+Chen",
    "gs_search_success": false,
    "gs_authors": [],
    "citation_count": null,
    "error_type": "fetch_failed",
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2404.18731",
    "title": "Real Time Multi Organ Classification on Computed Tomography Images",
    "year": 2024,
    "published": "2024-04-29T14:17:52Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "abstract": "Organ segmentation is a fundamental task in medical imaging since it is useful for many clinical automation pipelines. However, some tasks do not require full segmentation. Instead, a classifier can identify the selected organ without segmenting the entire volume. In this study, we demonstrate a classifier based method to obtain organ labels in real time by using a large context size with a sparse data sampling strategy. Although our method operates as an independent classifier at query location",
    "arxiv_url": "https://arxiv.org/abs/2404.18731v3",
    "pdf_url": "https://arxiv.org/pdf/2404.18731v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2404.18731",
    "arxiv_authors": [
      "Halid Ziya Yerebakan",
      "Yoshihisa Shinagawa",
      "Gerardo Hermosillo Valadez"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Real+Time+Multi+Organ+Classification+on+Computed+Tomography+Images+Halid+Ziya+Yerebakan+Yoshihisa+Shinagawa+Gerardo+Hermosillo+Valadez",
    "gs_search_success": true,
    "gs_authors": [
      "jgI0b8cAAAAJ",
      "QxmD4Q4AAAAJ"
    ],
    "citation_count": 4,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2504.04582",
    "title": "Your Image Generator Is Your New Private Dataset",
    "year": 2025,
    "published": "2025-04-06T18:46:08Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "abstract": "Generative diffusion models have emerged as powerful tools to synthetically produce training data, offering potential solutions to data scarcity and reducing labelling costs for downstream supervised deep learning applications. However, effectively leveraging text-conditioned image generation for building classifier training sets requires addressing key issues: constructing informative textual prompts, adapting generative models to specific domains, and ensuring robust performance. This paper pr",
    "arxiv_url": "https://arxiv.org/abs/2504.04582v2",
    "pdf_url": "https://arxiv.org/pdf/2504.04582v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2504.04582",
    "arxiv_authors": [
      "Nicolo Resmini",
      "Eugenio Lomurno",
      "Cristian Sbrolli",
      "Matteo Matteucci"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Your+Image+Generator+Is+Your+New+Private+Dataset+Nicolo+Resmini+Eugenio+Lomurno+Cristian+Sbrolli+Matteo+Matteucci",
    "gs_search_success": true,
    "gs_authors": [
      "yhvx_3AAAAAJ",
      "ecy3eIMAAAAJ",
      "7VpjbGoAAAAJ",
      "PdbEg5YAAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2505.08123",
    "title": "JSover: Joint Spectrum Estimation and Multi-Material Decomposition from Single-Energy CT Projections",
    "year": 2025,
    "published": "2025-05-12T23:32:21Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Multi-material decomposition (MMD) enables quantitative reconstruction of tissue compositions in the human body, supporting a wide range of clinical applications. However, traditional MMD typically requires spectral CT scanners and pre-measured X-ray energy spectra, significantly limiting clinical applicability. To this end, various methods have been developed to perform MMD using conventional (i.e., single-energy, SE) CT systems, commonly referred to as SEMMD. Despite promising progress, most S",
    "arxiv_url": "https://arxiv.org/abs/2505.08123v1",
    "pdf_url": "https://arxiv.org/pdf/2505.08123v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2505.08123",
    "arxiv_authors": [
      "Qing Wu",
      "Hongjiang Wei",
      "Jingyi Yu",
      "S. Kevin Zhou",
      "Yuyao Zhang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=JSover%3A+Joint+Spectrum+Estimation+and+Multi-Material+Decomposition+from+Single-Energy+CT+Projections+Qing+Wu+Hongjiang+Wei+Jingyi+Yu+S.+Kevin+Zhou+Yuyao+Zhang",
    "gs_search_success": true,
    "gs_authors": [
      "IORn-tEAAAAJ",
      "gIE0JTAAAAAJ",
      "8eNm2GMAAAAJ",
      "A1E80HUAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2408.12593",
    "title": "Automating Deformable Gasket Assembly",
    "year": 2024,
    "published": "2024-08-22T17:57:03Z",
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "abstract": "In Gasket Assembly, a deformable gasket must be aligned and pressed into a narrow channel. This task is common for sealing surfaces in the manufacturing of automobiles, appliances, electronics, and other products. Gasket Assembly is a long-horizon, high-precision task and the gasket must align with the channel and be fully pressed in to achieve a secure fit. To compare approaches, we present 4 methods for Gasket Assembly: one policy from deep imitation learning and three procedural algorithms. W",
    "arxiv_url": "https://arxiv.org/abs/2408.12593v1",
    "pdf_url": "https://arxiv.org/pdf/2408.12593v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2408.12593",
    "arxiv_authors": [
      "Simeon Adebola",
      "Tara Sadjadpour",
      "Karim El-Refai",
      "Will Panitch",
      "Zehan Ma",
      "Roy Lin",
      "Tianshuang Qiu",
      "Shreya Ganti",
      "Charlotte Le",
      "Jaimyn Drake",
      "Ken Goldberg"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Automating+Deformable+Gasket+Assembly+Simeon+Adebola+Tara+Sadjadpour+Karim+El-Refai+Will+Panitch+Zehan+Ma",
    "gs_search_success": false,
    "gs_authors": [],
    "citation_count": null,
    "error_type": "fetch_failed",
    "gs_author_count": 9
  },
  {
    "arxiv_id": "2305.18418",
    "title": "Just a Glimpse: Rethinking Temporal Information for Video Continual Learning",
    "year": 2023,
    "published": "2023-05-28T19:14:25Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "abstract": "Class-incremental learning is one of the most important settings for the study of Continual Learning, as it closely resembles real-world application scenarios. With constrained memory sizes, catastrophic forgetting arises as the number of classes/tasks increases. Studying continual learning in the video domain poses even more challenges, as video data contains a large number of frames, which places a higher burden on the replay memory. The current common practice is to sub-sample frames from the",
    "arxiv_url": "https://arxiv.org/abs/2305.18418v2",
    "pdf_url": "https://arxiv.org/pdf/2305.18418v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2305.18418",
    "arxiv_authors": [
      "Lama Alssum",
      "Juan Leon Alcazar",
      "Merey Ramazanova",
      "Chen Zhao",
      "Bernard Ghanem"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Just+a+Glimpse%3A+Rethinking+Temporal+Information+for+Video+Continual+Learning+Lama+Alssum+Juan+Leon+Alcazar+Merey+Ramazanova+Chen+Zhao+Bernard+Ghanem",
    "gs_search_success": false,
    "gs_authors": [],
    "citation_count": null,
    "error_type": "fetch_failed",
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2402.00375",
    "title": "Disentangled Multimodal Brain MR Image Translation via Transformer-based Modality Infuser",
    "year": 2024,
    "published": "2024-02-01T06:34:35Z",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "abstract": "Multimodal Magnetic Resonance (MR) Imaging plays a crucial role in disease diagnosis due to its ability to provide complementary information by analyzing a relationship between multimodal images on the same subject. Acquiring all MR modalities, however, can be expensive, and, during a scanning session, certain MR images may be missed depending on the study protocol. The typical solution would be to synthesize the missing modalities from the acquired images such as using generative adversarial ne",
    "arxiv_url": "https://arxiv.org/abs/2402.00375v1",
    "pdf_url": "https://arxiv.org/pdf/2402.00375v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2402.00375",
    "arxiv_authors": [
      "Jihoon Cho",
      "Xiaofeng Liu",
      "Fangxu Xing",
      "Jinsong Ouyang",
      "Georges El Fakhri",
      "Jinah Park",
      "Jonghye Woo"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Disentangled+Multimodal+Brain+MR+Image+Translation+via+Transformer-based+Modality+Infuser+Jihoon+Cho+Xiaofeng+Liu+Fangxu+Xing+Jinsong+Ouyang+Georges+El+Fakhri",
    "gs_search_success": true,
    "gs_authors": [
      "bp7V1bYAAAAJ",
      "_TtLrxIAAAAJ",
      "cys-jlsAAAAJ",
      "VighnTUAAAAJ",
      "lgLXQYkAAAAJ",
      "vXq6jPgAAAAJ",
      "QIablCYAAAAJ"
    ],
    "citation_count": 6,
    "gs_author_count": 7
  },
  {
    "arxiv_id": "2303.07868",
    "title": "DynaMask: Dynamic Mask Selection for Instance Segmentation",
    "year": 2023,
    "published": "2023-03-14T13:01:25Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "The representative instance segmentation methods mostly segment different object instances with a mask of the fixed resolution, e.g., 28*28 grid. However, a low-resolution mask loses rich details, while a high-resolution mask incurs quadratic computation overhead. It is a challenging task to predict the optimal binary mask for each instance. In this paper, we propose to dynamically select suitable masks for different object proposals. First, a dual-level Feature Pyramid Network (FPN) with adapti",
    "arxiv_url": "https://arxiv.org/abs/2303.07868v1",
    "pdf_url": "https://arxiv.org/pdf/2303.07868v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2303.07868",
    "arxiv_authors": [
      "Ruihuang Li",
      "Chenhang He",
      "Shuai Li",
      "Yabin Zhang",
      "Lei Zhang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=DynaMask%3A+Dynamic+Mask+Selection+for+Instance+Segmentation+Ruihuang+Li+Chenhang+He+Shuai+Li+Yabin+Zhang+Lei+Zhang",
    "gs_search_success": true,
    "gs_authors": [
      "Nt9es7kAAAAJ",
      "tAK5l1IAAAAJ",
      "8CfyOtQAAAAJ",
      "Bd73ldQAAAAJ",
      "dU6hpFUAAAAJ"
    ],
    "citation_count": 45,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2501.03616",
    "title": "BTMTrack: Robust RGB-T Tracking via Dual-template Bridging and Temporal-Modal Candidate Elimination",
    "year": 2025,
    "published": "2025-01-07T08:32:48Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "RGB-T tracking leverages the complementary strengths of RGB and thermal infrared (TIR) modalities to address challenging scenarios such as low illumination and adverse weather. However, existing methods often fail to effectively integrate temporal information and perform efficient cross-modal interactions, which constrain their adaptability to dynamic targets. In this paper, we propose BTMTrack, a novel framework for RGB-T tracking. The core of our approach lies in the dual-template backbone net",
    "arxiv_url": "https://arxiv.org/abs/2501.03616v3",
    "pdf_url": "https://arxiv.org/pdf/2501.03616v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2501.03616",
    "arxiv_authors": [
      "Zhongxuan Zhang",
      "Bi Zeng",
      "Xinyu Ni",
      "Yimin Du"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=BTMTrack%3A+Robust+RGB-T+Tracking+via+Dual-template+Bridging+and+Temporal-Modal+Candidate+Elimination+Zhongxuan+Zhang+Bi+Zeng+Xinyu+Ni+Yimin+Du",
    "gs_search_success": true,
    "gs_authors": [],
    "citation_count": 2,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2502.16423",
    "title": "Unified Prompt Attack Against Text-to-Image Generation Models",
    "year": 2025,
    "published": "2025-02-23T03:36:18Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Text-to-Image (T2I) models have advanced significantly, but their growing popularity raises security concerns due to their potential to generate harmful images. To address these issues, we propose UPAM, a novel framework to evaluate the robustness of T2I models from an attack perspective. Unlike prior methods that focus solely on textual defenses, UPAM unifies the attack on both textual and visual defenses. Additionally, it enables gradient-based optimization, overcoming reliance on enumeration ",
    "arxiv_url": "https://arxiv.org/abs/2502.16423v1",
    "pdf_url": "https://arxiv.org/pdf/2502.16423v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2502.16423",
    "arxiv_authors": [
      "Duo Peng",
      "Qiuhong Ke",
      "Mark He Huang",
      "Ping Hu",
      "Jun Liu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Unified+Prompt+Attack+Against+Text-to-Image+Generation+Models+Duo+Peng+Qiuhong+Ke+Mark+He+Huang+Ping+Hu+Jun+Liu",
    "gs_search_success": true,
    "gs_authors": [
      "Q5Ild8UAAAAJ",
      "lv1uAiMAAAAJ",
      "ddrD2TgAAAAJ",
      "DixuxWQAAAAJ",
      "84qxdhsAAAAJ"
    ],
    "citation_count": 4,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2404.10157",
    "title": "Salient Object-Aware Background Generation using Text-Guided Diffusion Models",
    "year": 2024,
    "published": "2024-04-15T22:13:35Z",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "Generating background scenes for salient objects plays a crucial role across various domains including creative design and e-commerce, as it enhances the presentation and context of subjects by integrating them into tailored environments. Background generation can be framed as a task of text-conditioned outpainting, where the goal is to extend image content beyond a salient object's boundaries on a blank background. Although popular diffusion models for text-guided inpainting can also be used fo",
    "arxiv_url": "https://arxiv.org/abs/2404.10157v1",
    "pdf_url": "https://arxiv.org/pdf/2404.10157v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2404.10157",
    "arxiv_authors": [
      "Amir Erfan Eshratifar",
      "Joao V. B. Soares",
      "Kapil Thadani",
      "Shaunak Mishra",
      "Mikhail Kuznetsov",
      "Yueh-Ning Ku",
      "Paloma de Juan"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Salient+Object-Aware+Background+Generation+using+Text-Guided+Diffusion+Models+Amir+Erfan+Eshratifar+Joao+V.+B.+Soares+Kapil+Thadani+Shaunak+Mishra+Mikhail+Kuznetsov",
    "gs_search_success": false,
    "gs_authors": [],
    "citation_count": null,
    "error_type": "fetch_failed",
    "gs_author_count": 7
  },
  {
    "arxiv_id": "2303.17941",
    "title": "Comparing Adversarial and Supervised Learning for Organs at Risk Segmentation in CT images",
    "year": 2023,
    "published": "2023-03-31T10:10:05Z",
    "categories": [
      "eess.IV",
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "Organ at Risk (OAR) segmentation from CT scans is a key component of the radiotherapy treatment workflow. In recent years, deep learning techniques have shown remarkable potential in automating this process. In this paper, we investigate the performance of Generative Adversarial Networks (GANs) compared to supervised learning approaches for segmenting OARs from CT images. We propose three GAN-based models with identical generator architectures but different discriminator networks. These models a",
    "arxiv_url": "https://arxiv.org/abs/2303.17941v1",
    "pdf_url": "https://arxiv.org/pdf/2303.17941v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2303.17941",
    "arxiv_authors": [
      "Leonardo Crespi",
      "Mattia Portanti",
      "Daniele Loiacono"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Comparing+Adversarial+and+Supervised+Learning+for+Organs+at+Risk+Segmentation+in+CT+images+Leonardo+Crespi+Mattia+Portanti+Daniele+Loiacono",
    "gs_search_success": true,
    "gs_authors": [
      "Vhh52kkAAAAJ",
      "zBT0nh0AAAAJ"
    ],
    "citation_count": 4,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2303.11183",
    "title": "Architecture, Dataset and Model-Scale Agnostic Data-free Meta-Learning",
    "year": 2023,
    "published": "2023-03-20T15:10:41Z",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "abstract": "The goal of data-free meta-learning is to learn useful prior knowledge from a collection of pre-trained models without accessing their training data. However, existing works only solve the problem in parameter space, which (i) ignore the fruitful data knowledge contained in the pre-trained models; (ii) can not scale to large-scale pre-trained models; (iii) can only meta-learn pre-trained models with the same network architecture. To address those issues, we propose a unified framework, dubbed PU",
    "arxiv_url": "https://arxiv.org/abs/2303.11183v3",
    "pdf_url": "https://arxiv.org/pdf/2303.11183v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2303.11183",
    "arxiv_authors": [
      "Zixuan Hu",
      "Li Shen",
      "Zhenyi Wang",
      "Tongliang Liu",
      "Chun Yuan",
      "Dacheng Tao"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Architecture%2C+Dataset+and+Model-Scale+Agnostic+Data-free+Meta-Learning+Zixuan+Hu+Li+Shen+Zhenyi+Wang+Tongliang+Liu+Chun+Yuan",
    "gs_search_success": true,
    "gs_authors": [
      "LA7hjlIAAAAJ",
      "yVhgENIAAAAJ",
      "EiLdZ_YAAAAJ",
      "RwlJNLcAAAAJ",
      "fYdxi2sAAAAJ",
      "F4uLsroAAAAJ"
    ],
    "citation_count": 18,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2410.15060",
    "title": "BYOCL: Build Your Own Consistent Latent with Hierarchical Representative Latent Clustering",
    "year": 2024,
    "published": "2024-10-19T10:48:19Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "To address the semantic inconsistency issue with SAM or other single-image segmentation models handling image sequences, we introduce BYOCL. This novel model outperforms SAM in extensive experiments, showcasing its Hierarchical prototype capabilities across CLIP and other representations. BYOCL significantly reduces time and space consumption by dividing inputs into smaller batches, achieving exponential time reduction compared to previous methods. Our approach leverages the SAM image encoder fo",
    "arxiv_url": "https://arxiv.org/abs/2410.15060v2",
    "pdf_url": "https://arxiv.org/pdf/2410.15060v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2410.15060",
    "arxiv_authors": [
      "Jiayue Dai",
      "Yunya Wang",
      "Yihan Fang",
      "Yuetong Chen",
      "Butian Xiong"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=BYOCL%3A+Build+Your+Own+Consistent+Latent+with+Hierarchical+Representative+Latent+Clustering+Jiayue+Dai+Yunya+Wang+Yihan+Fang+Yuetong+Chen+Butian+Xiong",
    "gs_search_success": true,
    "gs_authors": [
      "iuY1-IgAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2307.02814",
    "title": "Single Image LDR to HDR Conversion using Conditional Diffusion",
    "year": 2023,
    "published": "2023-07-06T07:19:47Z",
    "categories": [
      "cs.CV",
      "eess.IV"
    ],
    "abstract": "Digital imaging aims to replicate realistic scenes, but Low Dynamic Range (LDR) cameras cannot represent the wide dynamic range of real scenes, resulting in under-/overexposed images. This paper presents a deep learning-based approach for recovering intricate details from shadows and highlights while reconstructing High Dynamic Range (HDR) images. We formulate the problem as an image-to-image (I2I) translation task and propose a conditional Denoising Diffusion Probabilistic Model (DDPM) based fr",
    "arxiv_url": "https://arxiv.org/abs/2307.02814v1",
    "pdf_url": "https://arxiv.org/pdf/2307.02814v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2307.02814",
    "arxiv_authors": [
      "Dwip Dalal",
      "Gautam Vashishtha",
      "Prajwal Singh",
      "Shanmuganathan Raman"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Single+Image+LDR+to+HDR+Conversion+using+Conditional+Diffusion+Dwip+Dalal+Gautam+Vashishtha+Prajwal+Singh+Shanmuganathan+Raman",
    "gs_search_success": true,
    "gs_authors": [
      "w5GbFHIAAAAJ",
      "zrGsmv8AAAAJ",
      "IV17RukAAAAJ",
      "3YWptB8AAAAJ"
    ],
    "citation_count": 11,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2407.03893",
    "title": "Do Generalised Classifiers really work on Human Drawn Sketches?",
    "year": 2024,
    "published": "2024-07-04T12:37:08Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "This paper, for the first time, marries large foundation models with human sketch understanding. We demonstrate what this brings -- a paradigm shift in terms of generalised sketch representation learning (e.g., classification). This generalisation happens on two fronts: (i) generalisation across unknown categories (i.e., open-set), and (ii) generalisation traversing abstraction levels (i.e., good and bad sketches), both being timely challenges that remain unsolved in the sketch literature. Our d",
    "arxiv_url": "https://arxiv.org/abs/2407.03893v1",
    "pdf_url": "https://arxiv.org/pdf/2407.03893v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2407.03893",
    "arxiv_authors": [
      "Hmrishav Bandyopadhyay",
      "Pinaki Nath Chowdhury",
      "Aneeshan Sain",
      "Subhadeep Koley",
      "Tao Xiang",
      "Ayan Kumar Bhunia",
      "Yi-Zhe Song"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Do+Generalised+Classifiers+really+work+on+Human+Drawn+Sketches%3F+Hmrishav+Bandyopadhyay+Pinaki+Nath+Chowdhury+Aneeshan+Sain+Subhadeep+Koley+Tao+Xiang",
    "gs_search_success": true,
    "gs_authors": [
      "gjslbzsAAAAJ",
      "_QWFBvoAAAAJ",
      "irZFP_AAAAAJ",
      "MeS5d4gAAAAJ",
      "aBturOYAAAAJ",
      "HE2nfp0AAAAJ",
      "-mOrpz8AAAAJ"
    ],
    "citation_count": 3,
    "gs_author_count": 7
  },
  {
    "arxiv_id": "2503.09803",
    "title": "Evaluating the Impact of Synthetic Data on Object Detection Tasks in Autonomous Driving",
    "year": 2025,
    "published": "2025-03-12T20:13:33Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "The increasing applications of autonomous driving systems necessitates large-scale, high-quality datasets to ensure robust performance across diverse scenarios. Synthetic data has emerged as a viable solution to augment real-world datasets due to its cost-effectiveness, availability of precise ground-truth labels, and the ability to model specific edge cases. However, synthetic data may introduce distributional differences and biases that could impact model performance in real-world settings. To",
    "arxiv_url": "https://arxiv.org/abs/2503.09803v1",
    "pdf_url": "https://arxiv.org/pdf/2503.09803v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2503.09803",
    "arxiv_authors": [
      "Enes Özeren",
      "Arka Bhowmick"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Evaluating+the+Impact+of+Synthetic+Data+on+Object+Detection+Tasks+in+Autonomous+Driving+Enes+%C3%96zeren+Arka+Bhowmick",
    "gs_search_success": false,
    "gs_authors": [],
    "citation_count": null,
    "error_type": "fetch_failed",
    "gs_author_count": 2
  },
  {
    "arxiv_id": "2304.09179",
    "title": "Pretrained Language Models as Visual Planners for Human Assistance",
    "year": 2023,
    "published": "2023-04-17T18:07:36Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "In our pursuit of advancing multi-modal AI assistants capable of guiding users to achieve complex multi-step goals, we propose the task of \"Visual Planning for Assistance (VPA)\". Given a succinct natural language goal, e.g., \"make a shelf\", and a video of the user's progress so far, the aim of VPA is to devise a plan, i.e., a sequence of actions such as \"sand shelf\", \"paint shelf\", etc. to realize the specified goal. This requires assessing the user's progress from the (untrimmed) video, and rel",
    "arxiv_url": "https://arxiv.org/abs/2304.09179v3",
    "pdf_url": "https://arxiv.org/pdf/2304.09179v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2304.09179",
    "arxiv_authors": [
      "Dhruvesh Patel",
      "Hamid Eghbalzadeh",
      "Nitin Kamra",
      "Michael Louis Iuzzolino",
      "Unnat Jain",
      "Ruta Desai"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Pretrained+Language+Models+as+Visual+Planners+for+Human+Assistance+Dhruvesh+Patel+Hamid+Eghbalzadeh+Nitin+Kamra+Michael+Louis+Iuzzolino+Unnat+Jain",
    "gs_search_success": false,
    "gs_authors": [],
    "citation_count": null,
    "error_type": "fetch_failed",
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2501.05566",
    "title": "Vision-Language Models for Autonomous Driving: CLIP-Based Dynamic Scene Understanding",
    "year": 2025,
    "published": "2025-01-09T20:29:31Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CY"
    ],
    "abstract": "Scene understanding is essential for enhancing driver safety, generating human-centric explanations for Automated Vehicle (AV) decisions, and leveraging Artificial Intelligence (AI) for retrospective driving video analysis. This study developed a dynamic scene retrieval system using Contrastive Language-Image Pretraining (CLIP) models, which can be optimized for real-time deployment on edge devices. The proposed system outperforms state-of-the-art in-context learning methods, including the zero-",
    "arxiv_url": "https://arxiv.org/abs/2501.05566v1",
    "pdf_url": "https://arxiv.org/pdf/2501.05566v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2501.05566",
    "arxiv_authors": [
      "Mohammed Elhenawy",
      "Huthaifa I. Ashqar",
      "Andry Rakotonirainy",
      "Taqwa I. Alhadidi",
      "Ahmed Jaber",
      "Mohammad Abu Tami"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Vision-Language+Models+for+Autonomous+Driving%3A+CLIP-Based+Dynamic+Scene+Understanding+Mohammed+Elhenawy+Huthaifa+I.+Ashqar+Andry+Rakotonirainy+Taqwa+I.+Alhadidi+Ahmed+Jaber",
    "gs_search_success": true,
    "gs_authors": [
      "V9TfHlQAAAAJ",
      "Cpk47XoAAAAJ",
      "66t6m4UAAAAJ",
      "1mZfHEUAAAAJ",
      "NjSTE8QAAAAJ",
      "i-YpTO0AAAAJ"
    ],
    "citation_count": 14,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2409.10213",
    "title": "Neuromorphic Facial Analysis with Cross-Modal Supervision",
    "year": 2024,
    "published": "2024-09-16T12:04:26Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Traditional approaches for analyzing RGB frames are capable of providing a fine-grained understanding of a face from different angles by inferring emotions, poses, shapes, landmarks. However, when it comes to subtle movements standard RGB cameras might fall behind due to their latency, making it hard to detect micro-movements that carry highly informative cues to infer the true emotions of a subject. To address this issue, the usage of event cameras to analyze faces is gaining increasing interes",
    "arxiv_url": "https://arxiv.org/abs/2409.10213v1",
    "pdf_url": "https://arxiv.org/pdf/2409.10213v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2409.10213",
    "arxiv_authors": [
      "Federico Becattini",
      "Luca Cultrera",
      "Lorenzo Berlincioni",
      "Claudio Ferrari",
      "Andrea Leonardo",
      "Alberto Del Bimbo"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Neuromorphic+Facial+Analysis+with+Cross-Modal+Supervision+Federico+Becattini+Luca+Cultrera+Lorenzo+Berlincioni+Claudio+Ferrari+Andrea+Leonardo",
    "gs_search_success": true,
    "gs_authors": [
      "ty0aAwQAAAAJ",
      "t0sXvkAAAAAJ",
      "cQT8SvQAAAAJ",
      "aael17YAAAAJ",
      "bf2ZrFcAAAAJ"
    ],
    "citation_count": 8,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2305.05144",
    "title": "Adapt and Align to Improve Zero-Shot Sketch-Based Image Retrieval",
    "year": 2023,
    "published": "2023-05-09T03:10:15Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Zero-shot sketch-based image retrieval (ZS-SBIR) is challenging due to the cross-domain nature of sketches and photos, as well as the semantic gap between seen and unseen image distributions. Previous methods fine-tune pre-trained models with various side information and learning strategies to learn a compact feature space that is shared between the sketch and photo domains and bridges seen and unseen classes. However, these efforts are inadequate in adapting domains and transferring knowledge f",
    "arxiv_url": "https://arxiv.org/abs/2305.05144v3",
    "pdf_url": "https://arxiv.org/pdf/2305.05144v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2305.05144",
    "arxiv_authors": [
      "Shiyin Dong",
      "Mingrui Zhu",
      "Nannan Wang",
      "Xinbo Gao"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Adapt+and+Align+to+Improve+Zero-Shot+Sketch-Based+Image+Retrieval+Shiyin+Dong+Mingrui+Zhu+Nannan+Wang+Xinbo+Gao",
    "gs_search_success": true,
    "gs_authors": [
      "SRBn7oUAAAAJ",
      "VZVTOOIAAAAJ",
      "a8FXS1UAAAAJ"
    ],
    "citation_count": 4,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2503.00043",
    "title": "VOILA: Evaluation of MLLMs For Perceptual Understanding and Analogical Reasoning",
    "year": 2025,
    "published": "2025-02-25T23:36:19Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "abstract": "Multimodal Large Language Models (MLLMs) have become a powerful tool for integrating visual and textual information. Despite their exceptional performance on visual understanding benchmarks, measuring their ability to reason abstractly across multiple images remains a significant challenge. To address this, we introduce VOILA, a large-scale, open-ended, dynamic benchmark designed to evaluate MLLMs' perceptual understanding and abstract relational reasoning. VOILA employs an analogical mapping ap",
    "arxiv_url": "https://arxiv.org/abs/2503.00043v2",
    "pdf_url": "https://arxiv.org/pdf/2503.00043v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2503.00043",
    "arxiv_authors": [
      "Nilay Yilmaz",
      "Maitreya Patel",
      "Yiran Lawrence Luo",
      "Tejas Gokhale",
      "Chitta Baral",
      "Suren Jayasuriya",
      "Yezhou Yang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=VOILA%3A+Evaluation+of+MLLMs+For+Perceptual+Understanding+and+Analogical+Reasoning+Nilay+Yilmaz+Maitreya+Patel+Yiran+Lawrence+Luo+Tejas+Gokhale+Chitta+Baral",
    "gs_search_success": true,
    "gs_authors": [
      "_ILTlEwAAAAJ",
      "92awVdYAAAAJ",
      "DEfu2GoAAAAJ",
      "z--mlKgAAAAJ",
      "k2suuZgAAAAJ",
      "9Yd716IAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 7
  },
  {
    "arxiv_id": "2411.01173",
    "title": "Reasoning Limitations of Multimodal Large Language Models. A Case Study of Bongard Problems",
    "year": 2024,
    "published": "2024-11-02T08:06:30Z",
    "categories": [
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "Abstract visual reasoning (AVR) involves discovering shared concepts across images through analogy, akin to solving IQ test problems. Bongard Problems (BPs) remain a key challenge in AVR, requiring both visual reasoning and verbal description. We investigate whether multimodal large language models (MLLMs) can solve BPs by formulating a set of diverse MLLM-suited solution strategies and testing $4$ proprietary and $4$ open-access models on $3$ BP datasets featuring synthetic (classic BPs) and re",
    "arxiv_url": "https://arxiv.org/abs/2411.01173v2",
    "pdf_url": "https://arxiv.org/pdf/2411.01173v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2411.01173",
    "arxiv_authors": [
      "Mikołaj Małkiński",
      "Szymon Pawlonka",
      "Jacek Mańdziuk"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Reasoning+Limitations+of+Multimodal+Large+Language+Models.+A+Case+Study+of+Bongard+Problems+Miko%C5%82aj+Ma%C5%82ki%C5%84ski+Szymon+Pawlonka+Jacek+Ma%C5%84dziuk",
    "gs_search_success": true,
    "gs_authors": [
      "KHxnO24AAAAJ",
      "V7a-vNQAAAAJ",
      "XqunHn0AAAAJ"
    ],
    "citation_count": 17,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2303.14840",
    "title": "On the Importance of Accurate Geometry Data for Dense 3D Vision Tasks",
    "year": 2023,
    "published": "2023-03-26T22:32:44Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Learning-based methods to solve dense 3D vision problems typically train on 3D sensor data. The respectively used principle of measuring distances provides advantages and drawbacks. These are typically not compared nor discussed in the literature due to a lack of multi-modal datasets. Texture-less regions are problematic for structure from motion and stereo, reflective material poses issues for active sensing, and distances for translucent objects are intricate to measure with existing hardware.",
    "arxiv_url": "https://arxiv.org/abs/2303.14840v1",
    "pdf_url": "https://arxiv.org/pdf/2303.14840v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2303.14840",
    "arxiv_authors": [
      "HyunJun Jung",
      "Patrick Ruhkamp",
      "Guangyao Zhai",
      "Nikolas Brasch",
      "Yitong Li",
      "Yannick Verdie",
      "Jifei Song",
      "Yiren Zhou",
      "Anil Armagan",
      "Slobodan Ilic",
      "Ales Leonardis",
      "Nassir Navab",
      "Benjamin Busam"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=On+the+Importance+of+Accurate+Geometry+Data+for+Dense+3D+Vision+Tasks+HyunJun+Jung+Patrick+Ruhkamp+Guangyao+Zhai+Nikolas+Brasch+Yitong+Li",
    "gs_search_success": true,
    "gs_authors": [
      "sZU8ZsQAAAAJ",
      "X_djKCUAAAAJ",
      "9a1PjCIAAAAJ",
      "8rYJ5qAAAAAJ",
      "ELOVd8sAAAAJ",
      "biF5W7IAAAAJ",
      "Linn47YAAAAJ",
      "G7jv5RwAAAAJ"
    ],
    "citation_count": 26,
    "gs_author_count": 9
  },
  {
    "arxiv_id": "2308.10262",
    "title": "Learning Disentangled Representation with Mutual Information Maximization for Real-Time UAV Tracking",
    "year": 2023,
    "published": "2023-08-20T13:16:15Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Efficiency has been a critical problem in UAV tracking due to limitations in computation resources, battery capacity, and unmanned aerial vehicle maximum load. Although discriminative correlation filters (DCF)-based trackers prevail in this field for their favorable efficiency, some recently proposed lightweight deep learning (DL)-based trackers using model compression demonstrated quite remarkable CPU efficiency as well as precision. Unfortunately, the model compression methods utilized by thes",
    "arxiv_url": "https://arxiv.org/abs/2308.10262v1",
    "pdf_url": "https://arxiv.org/pdf/2308.10262v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2308.10262",
    "arxiv_authors": [
      "Xucheng Wang",
      "Xiangyang Yang",
      "Hengzhou Ye",
      "Shuiwang Li"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Learning+Disentangled+Representation+with+Mutual+Information+Maximization+for+Real-Time+UAV+Tracking+Xucheng+Wang+Xiangyang+Yang+Hengzhou+Ye+Shuiwang+Li",
    "gs_search_success": true,
    "gs_authors": [
      "jcgfhQEAAAAJ"
    ],
    "citation_count": 7,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2504.05623",
    "title": "Time-Aware Auto White Balance in Mobile Photography",
    "year": 2025,
    "published": "2025-04-08T02:45:37Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Cameras rely on auto white balance (AWB) to correct undesirable color casts caused by scene illumination and the camera's spectral sensitivity. This is typically achieved using an illuminant estimator that determines the global color cast solely from the color information in the camera's raw sensor image. Mobile devices provide valuable additional metadata-such as capture timestamp and geolocation-that offers strong contextual clues to help narrow down the possible illumination solutions. This p",
    "arxiv_url": "https://arxiv.org/abs/2504.05623v2",
    "pdf_url": "https://arxiv.org/pdf/2504.05623v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2504.05623",
    "arxiv_authors": [
      "Mahmoud Afifi",
      "Luxi Zhao",
      "Abhijith Punnappurath",
      "Mohammed A. Abdelsalam",
      "Ran Zhang",
      "Michael S. Brown"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Time-Aware+Auto+White+Balance+in+Mobile+Photography+Mahmoud+Afifi+Luxi+Zhao+Abhijith+Punnappurath+Mohammed+A.+Abdelsalam+Ran+Zhang",
    "gs_search_success": true,
    "gs_authors": [
      "Gv1QGSMAAAAJ",
      "IIZHWHoAAAAJ",
      "7sFo9KUAAAAJ",
      "1kpuF5EAAAAJ",
      "3sHjF1MAAAAJ"
    ],
    "citation_count": 2,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2405.11621",
    "title": "Computer Vision in the Food Industry: Accurate, Real-time, and Automatic Food Recognition with Pretrained MobileNetV2",
    "year": 2024,
    "published": "2024-05-19T17:20:20Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "In contemporary society, the application of artificial intelligence for automatic food recognition offers substantial potential for nutrition tracking, reducing food waste, and enhancing productivity in food production and consumption scenarios. Modern technologies such as Computer Vision and Deep Learning are highly beneficial, enabling machines to learn automatically, thereby facilitating automatic visual recognition. Despite some research in this field, the challenge of achieving accurate aut",
    "arxiv_url": "https://arxiv.org/abs/2405.11621v1",
    "pdf_url": "https://arxiv.org/pdf/2405.11621v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2405.11621",
    "arxiv_authors": [
      "Shayan Rokhva",
      "Babak Teimourpour",
      "Amir Hossein Soltani"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Computer+Vision+in+the+Food+Industry%3A+Accurate%2C+Real-time%2C+and+Automatic+Food+Recognition+with+Pretrained+MobileNetV2+Shayan+Rokhva+Babak+Teimourpour+Amir+Hossein+Soltani",
    "gs_search_success": true,
    "gs_authors": [
      "GECJM-4AAAAJ",
      "jbiQIcYAAAAJ",
      "Hb0DMrUAAAAJ"
    ],
    "citation_count": 27,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2505.00534",
    "title": "A Robust Deep Networks based Multi-Object MultiCamera Tracking System for City Scale Traffic",
    "year": 2025,
    "published": "2025-05-01T14:00:25Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Vision sensors are becoming more important in Intelligent Transportation Systems (ITS) for traffic monitoring, management, and optimization as the number of network cameras continues to rise. However, manual object tracking and matching across multiple non-overlapping cameras pose significant challenges in city-scale urban traffic scenarios. These challenges include handling diverse vehicle attributes, occlusions, illumination variations, shadows, and varying video resolutions. To address these ",
    "arxiv_url": "https://arxiv.org/abs/2505.00534v1",
    "pdf_url": "https://arxiv.org/pdf/2505.00534v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2505.00534",
    "arxiv_authors": [
      "Muhammad Imran Zaman",
      "Usama Ijaz Bajwa",
      "Gulshan Saleem",
      "Rana Hammad Raza"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=A+Robust+Deep+Networks+based+Multi-Object+MultiCamera+Tracking+System+for+City+Scale+Traffic+Muhammad+Imran+Zaman+Usama+Ijaz+Bajwa+Gulshan+Saleem+Rana+Hammad+Raza",
    "gs_search_success": true,
    "gs_authors": [
      "ulVFpy8AAAAJ",
      "_BkrMqQAAAAJ",
      "sg9MITkAAAAJ",
      "lGWptLcAAAAJ"
    ],
    "citation_count": 19,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2311.03082",
    "title": "A survey and classification of face alignment methods based on face models",
    "year": 2023,
    "published": "2023-11-06T13:09:04Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "A face model is a mathematical representation of the distinct features of a human face. Traditionally, face models were built using a set of fiducial points or landmarks, each point ideally located on a facial feature, i.e., corner of the eye, tip of the nose, etc. Face alignment is the process of fitting the landmarks in a face model to the respective ground truth positions in an input image containing a face. Despite significant research on face alignment in the past decades, no review analyse",
    "arxiv_url": "https://arxiv.org/abs/2311.03082v1",
    "pdf_url": "https://arxiv.org/pdf/2311.03082v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2311.03082",
    "arxiv_authors": [
      "Jagmohan Meher",
      "Hector Allende-Cid",
      "Torbjörn E. M. Nordling"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=A+survey+and+classification+of+face+alignment+methods+based+on+face+models+Jagmohan+Meher+Hector+Allende-Cid+Torbj%C3%B6rn+E.+M.+Nordling",
    "gs_search_success": true,
    "gs_authors": [
      "zxQjGLAAAAAJ",
      "ZlJxcM0AAAAJ",
      "R9VMvXwAAAAJ"
    ],
    "citation_count": 2,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2411.10200",
    "title": "Block based Adaptive Compressive Sensing with Sampling Rate Control",
    "year": 2024,
    "published": "2024-11-15T13:58:56Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Compressive sensing (CS), acquiring and reconstructing signals below the Nyquist rate, has great potential in image and video acquisition to exploit data redundancy and greatly reduce the amount of sampled data. To further reduce the sampled data while keeping the video quality, this paper explores the temporal redundancy in video CS and proposes a block based adaptive compressive sensing framework with a sampling rate (SR) control strategy. To avoid redundant compression of non-moving regions, ",
    "arxiv_url": "https://arxiv.org/abs/2411.10200v1",
    "pdf_url": "https://arxiv.org/pdf/2411.10200v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2411.10200",
    "arxiv_authors": [
      "Kosuke Iwama",
      "Ryugo Morita",
      "Jinjia Zhou"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Block+based+Adaptive+Compressive+Sensing+with+Sampling+Rate+Control+Kosuke+Iwama+Ryugo+Morita+Jinjia+Zhou",
    "gs_search_success": true,
    "gs_authors": [
      "84gWaLUAAAAJ",
      "MqJvzUsAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2406.12179",
    "title": "The Wisdom of a Crowd of Brains: A Universal Brain Encoder",
    "year": 2024,
    "published": "2024-06-18T01:17:07Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Image-to-fMRI encoding is important for both neuroscience research and practical applications. However, such \"Brain-Encoders\" have been typically trained per-subject and per fMRI-dataset, thus restricted to very limited training data. In this paper we propose a Universal Brain-Encoder, which can be trained jointly on data from many different subjects/datasets/machines. What makes this possible is our new voxel-centric Encoder architecture, which learns a unique \"voxel-embedding\" per brain-voxel.",
    "arxiv_url": "https://arxiv.org/abs/2406.12179v3",
    "pdf_url": "https://arxiv.org/pdf/2406.12179v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2406.12179",
    "arxiv_authors": [
      "Roman Beliy",
      "Navve Wasserman",
      "Amit Zalcher",
      "Michal Irani"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=The+Wisdom+of+a+Crowd+of+Brains%3A+A+Universal+Brain+Encoder+Roman+Beliy+Navve+Wasserman+Amit+Zalcher+Michal+Irani",
    "gs_search_success": true,
    "gs_authors": [
      "qXjCxQ0AAAAJ",
      "VfQmRM8AAAAJ",
      "5hJNWakAAAAJ",
      "zP9EODAAAAAJ"
    ],
    "citation_count": 2,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2307.04639",
    "title": "Multimodal brain age estimation using interpretable adaptive population-graph learning",
    "year": 2023,
    "published": "2023-07-10T15:35:31Z",
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "abstract": "Brain age estimation is clinically important as it can provide valuable information in the context of neurodegenerative diseases such as Alzheimer's. Population graphs, which include multimodal imaging information of the subjects along with the relationships among the population, have been used in literature along with Graph Convolutional Networks (GCNs) and have proved beneficial for a variety of medical imaging tasks. A population graph is usually static and constructed manually using non-imag",
    "arxiv_url": "https://arxiv.org/abs/2307.04639v2",
    "pdf_url": "https://arxiv.org/pdf/2307.04639v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2307.04639",
    "arxiv_authors": [
      "Kyriaki-Margarita Bintsi",
      "Vasileios Baltatzis",
      "Rolandos Alexandros Potamias",
      "Alexander Hammers",
      "Daniel Rueckert"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Multimodal+brain+age+estimation+using+interpretable+adaptive+population-graph+learning+Kyriaki-Margarita+Bintsi+Vasileios+Baltatzis+Rolandos+Alexandros+Potamias+Alexander+Hammers+Daniel+Rueckert",
    "gs_search_success": true,
    "gs_authors": [
      "LteFCZsAAAAJ",
      "QFkozOIAAAAJ",
      "J6BbEBQAAAAJ",
      "A4B8Wf4AAAAJ",
      "H0O0WnQAAAAJ"
    ],
    "citation_count": 15,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2312.01361",
    "title": "MoEC: Mixture of Experts Implicit Neural Compression",
    "year": 2023,
    "published": "2023-12-03T12:02:23Z",
    "categories": [
      "cs.CV",
      "cs.LG",
      "eess.IV"
    ],
    "abstract": "Emerging Implicit Neural Representation (INR) is a promising data compression technique, which represents the data using the parameters of a Deep Neural Network (DNN). Existing methods manually partition a complex scene into local regions and overfit the INRs into those regions. However, manually designing the partition scheme for a complex scene is very challenging and fails to jointly learn the partition and INRs. To solve the problem, we propose MoEC, a novel implicit neural compression metho",
    "arxiv_url": "https://arxiv.org/abs/2312.01361v1",
    "pdf_url": "https://arxiv.org/pdf/2312.01361v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2312.01361",
    "arxiv_authors": [
      "Jianchen Zhao",
      "Cheng-Ching Tseng",
      "Ming Lu",
      "Ruichuan An",
      "Xiaobao Wei",
      "He Sun",
      "Shanghang Zhang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=MoEC%3A+Mixture+of+Experts+Implicit+Neural+Compression+Jianchen+Zhao+Cheng-Ching+Tseng+Ming+Lu+Ruichuan+An+Xiaobao+Wei",
    "gs_search_success": true,
    "gs_authors": [
      "MGAz5PkAAAAJ",
      "R5iSLPQAAAAJ",
      "voqw10cAAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 7
  },
  {
    "arxiv_id": "2404.03584",
    "title": "Towards more realistic human motion prediction with attention to motion coordination",
    "year": 2024,
    "published": "2024-04-04T16:48:40Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Joint relation modeling is a curial component in human motion prediction. Most existing methods rely on skeletal-based graphs to build the joint relations, where local interactive relations between joint pairs are well learned. However, the motion coordination, a global joint relation reflecting the simultaneous cooperation of all joints, is usually weakened because it is learned from part to whole progressively and asynchronously. Thus, the final predicted motions usually appear unrealistic. To",
    "arxiv_url": "https://arxiv.org/abs/2404.03584v1",
    "pdf_url": "https://arxiv.org/pdf/2404.03584v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2404.03584",
    "arxiv_authors": [
      "Pengxiang Ding",
      "Jianqin Yin"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Towards+more+realistic+human+motion+prediction+with+attention+to+motion+coordination+Pengxiang+Ding+Jianqin+Yin",
    "gs_search_success": true,
    "gs_authors": [
      "QyBSTzEAAAAJ"
    ],
    "citation_count": 22,
    "gs_author_count": 2
  },
  {
    "arxiv_id": "2305.16494",
    "title": "Diffusion-Based Adversarial Sample Generation for Improved Stealthiness and Controllability",
    "year": 2023,
    "published": "2023-05-25T21:51:23Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Neural networks are known to be susceptible to adversarial samples: small variations of natural examples crafted to deliberately mislead the models. While they can be easily generated using gradient-based techniques in digital and physical scenarios, they often differ greatly from the actual data distribution of natural images, resulting in a trade-off between strength and stealthiness. In this paper, we propose a novel framework dubbed Diffusion-Based Projected Gradient Descent (Diff-PGD) for g",
    "arxiv_url": "https://arxiv.org/abs/2305.16494v3",
    "pdf_url": "https://arxiv.org/pdf/2305.16494v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2305.16494",
    "arxiv_authors": [
      "Haotian Xue",
      "Alexandre Araujo",
      "Bin Hu",
      "Yongxin Chen"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Diffusion-Based+Adversarial+Sample+Generation+for+Improved+Stealthiness+and+Controllability+Haotian+Xue+Alexandre+Araujo+Bin+Hu+Yongxin+Chen",
    "gs_search_success": true,
    "gs_authors": [
      "wsu61VYAAAAJ",
      "wnweG9UAAAAJ",
      "o3tVp68AAAAJ",
      "2z7iDDUAAAAJ"
    ],
    "citation_count": 93,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2504.13391",
    "title": "Cardiac MRI Semantic Segmentation for Ventricles and Myocardium using Deep Learning",
    "year": 2025,
    "published": "2025-04-18T00:54:30Z",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "Automated noninvasive cardiac diagnosis plays a critical role in the early detection of cardiac disorders and cost-effective clinical management. Automated diagnosis involves the automated segmentation and analysis of cardiac images. Precise delineation of cardiac substructures and extraction of their morphological attributes are essential for evaluating the cardiac function, and diagnosing cardiovascular disease such as cardiomyopathy, valvular diseases, abnormalities related to septum perforat",
    "arxiv_url": "https://arxiv.org/abs/2504.13391v1",
    "pdf_url": "https://arxiv.org/pdf/2504.13391v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2504.13391",
    "arxiv_authors": [
      "Racheal Mukisa",
      "Arvind K. Bansal"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Cardiac+MRI+Semantic+Segmentation+for+Ventricles+and+Myocardium+using+Deep+Learning+Racheal+Mukisa+Arvind+K.+Bansal",
    "gs_search_success": true,
    "gs_authors": [
      "uuGnBb4AAAAJ"
    ],
    "citation_count": 4,
    "gs_author_count": 2
  },
  {
    "arxiv_id": "2505.17973",
    "title": "To Glue or Not to Glue? Classical vs Learned Image Matching for Mobile Mapping Cameras to Textured Semantic 3D Building Models",
    "year": 2025,
    "published": "2025-05-23T14:41:41Z",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "Feature matching is a necessary step for many computer vision and photogrammetry applications such as image registration, structure-from-motion, and visual localization. Classical handcrafted methods such as SIFT feature detection and description combined with nearest neighbour matching and RANSAC outlier removal have been state-of-the-art for mobile mapping cameras. With recent advances in deep learning, learnable methods have been introduced and proven to have better robustness and performance",
    "arxiv_url": "https://arxiv.org/abs/2505.17973v1",
    "pdf_url": "https://arxiv.org/pdf/2505.17973v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2505.17973",
    "arxiv_authors": [
      "Simone Gaisbauer",
      "Prabin Gyawali",
      "Qilin Zhang",
      "Olaf Wysocki",
      "Boris Jutzi"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=To+Glue+or+Not+to+Glue%3F+Classical+vs+Learned+Image+Matching+for+Mobile+Mapping+Cameras+to+Textured+Semantic+3D+Building+Models+Simone+Gaisbauer+Prabin+Gyawali+Qilin+Zhang+Olaf+Wysocki+Boris+Jutzi",
    "gs_search_success": true,
    "gs_authors": [
      "ZpB02CwAAAAJ",
      "w524orEAAAAJ",
      "9xQhtFcAAAAJ",
      "Eh75_GcAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2304.00534",
    "title": "LG-BPN: Local and Global Blind-Patch Network for Self-Supervised Real-World Denoising",
    "year": 2023,
    "published": "2023-04-02T13:32:16Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Despite the significant results on synthetic noise under simplified assumptions, most self-supervised denoising methods fail under real noise due to the strong spatial noise correlation, including the advanced self-supervised blind-spot networks (BSNs). For recent methods targeting real-world denoising, they either suffer from ignoring this spatial correlation, or are limited by the destruction of fine textures for under-considering the correlation. In this paper, we present a novel method calle",
    "arxiv_url": "https://arxiv.org/abs/2304.00534v1",
    "pdf_url": "https://arxiv.org/pdf/2304.00534v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2304.00534",
    "arxiv_authors": [
      "Zichun Wang",
      "Ying Fu",
      "Ji Liu",
      "Yulun Zhang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=LG-BPN%3A+Local+and+Global+Blind-Patch+Network+for+Self-Supervised+Real-World+Denoising+Zichun+Wang+Ying+Fu+Ji+Liu+Yulun+Zhang",
    "gs_search_success": true,
    "gs_authors": [
      "gRkf-GkAAAAJ",
      "ORmLjWoAAAAJ",
      "PE4xMlkAAAAJ",
      "C16EBHUAAAAJ"
    ],
    "citation_count": 91,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2404.06353",
    "title": "High Noise Scheduling is a Must",
    "year": 2024,
    "published": "2024-04-09T14:44:12Z",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "abstract": "Consistency models possess high capabilities for image generation, advancing sampling steps to a single step through their advanced techniques. Current advancements move one step forward consistency training techniques and eliminates the limitation of distillation training. Even though the proposed curriculum and noise scheduling in improved training techniques yield better results than basic consistency models, it lacks well balanced noise distribution and its consistency between curriculum. In",
    "arxiv_url": "https://arxiv.org/abs/2404.06353v1",
    "pdf_url": "https://arxiv.org/pdf/2404.06353v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2404.06353",
    "arxiv_authors": [
      "Mahmut S. Gokmen",
      "Cody Bumgardner",
      "Jie Zhang",
      "Ge Wang",
      "Jin Chen"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=High+Noise+Scheduling+is+a+Must+Mahmut+S.+Gokmen+Cody+Bumgardner+Jie+Zhang+Ge+Wang+Jin+Chen",
    "gs_search_success": true,
    "gs_authors": [
      "-CkpZKIAAAAJ",
      "pjK2mQwAAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2503.07763",
    "title": "2D/3D Registration of Acetabular Hip Implants Under Perspective Projection and Fully Differentiable Ellipse Fitting",
    "year": 2025,
    "published": "2025-03-10T18:34:41Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "This paper presents a novel method for estimating the orientation and the position of acetabular hip implants in total hip arthroplasty using full anterior-posterior hip fluoroscopy images. Our method accounts for distortions induced in the fluoroscope geometry, estimating acetabular component pose by creating a forward model of the perspective projection and implementing differentiable ellipse fitting for the similarity of our estimation from the ground truth. This approach enables precise esti",
    "arxiv_url": "https://arxiv.org/abs/2503.07763v1",
    "pdf_url": "https://arxiv.org/pdf/2503.07763v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2503.07763",
    "arxiv_authors": [
      "Yehyun Suh",
      "J. Ryan Martin",
      "Daniel Moyer"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=2D%2F3D+Registration+of+Acetabular+Hip+Implants+Under+Perspective+Projection+and+Fully+Differentiable+Ellipse+Fitting+Yehyun+Suh+J.+Ryan+Martin+Daniel+Moyer",
    "gs_search_success": true,
    "gs_authors": [
      "sKmoxSMAAAAJ",
      "5GxHvrcAAAAJ",
      "djTUBB4AAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2308.08942",
    "title": "Auxiliary Tasks Benefit 3D Skeleton-based Human Motion Prediction",
    "year": 2023,
    "published": "2023-08-17T12:26:11Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Exploring spatial-temporal dependencies from observed motions is one of the core challenges of human motion prediction. Previous methods mainly focus on dedicated network structures to model the spatial and temporal dependencies. This paper considers a new direction by introducing a model learning framework with auxiliary tasks. In our auxiliary tasks, partial body joints' coordinates are corrupted by either masking or adding noise and the goal is to recover corrupted coordinates depending on th",
    "arxiv_url": "https://arxiv.org/abs/2308.08942v2",
    "pdf_url": "https://arxiv.org/pdf/2308.08942v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2308.08942",
    "arxiv_authors": [
      "Chenxin Xu",
      "Robby T. Tan",
      "Yuhong Tan",
      "Siheng Chen",
      "Xinchao Wang",
      "Yanfeng Wang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Auxiliary+Tasks+Benefit+3D+Skeleton-based+Human+Motion+Prediction+Chenxin+Xu+Robby+T.+Tan+Yuhong+Tan+Siheng+Chen+Xinchao+Wang",
    "gs_search_success": true,
    "gs_authors": [
      "t8ZCZSkAAAAJ",
      "W_Q33RMAAAAJ",
      "x_sgJskAAAAJ",
      "MOD0gv4AAAAJ",
      "UgmMqV0AAAAJ",
      "w69Buq0AAAAJ"
    ],
    "citation_count": 63,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2306.04451",
    "title": "Referring Expression Comprehension Using Language Adaptive Inference",
    "year": 2023,
    "published": "2023-06-06T07:58:59Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Different from universal object detection, referring expression comprehension (REC) aims to locate specific objects referred to by natural language expressions. The expression provides high-level concepts of relevant visual and contextual patterns, which vary significantly with different expressions and account for only a few of those encoded in the REC model. This leads us to a question: do we really need the entire network with a fixed structure for various referring expressions? Ideally, give",
    "arxiv_url": "https://arxiv.org/abs/2306.04451v1",
    "pdf_url": "https://arxiv.org/pdf/2306.04451v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2306.04451",
    "arxiv_authors": [
      "Wei Su",
      "Peihan Miao",
      "Huanzhang Dou",
      "Yongjian Fu",
      "Xi Li"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Referring+Expression+Comprehension+Using+Language+Adaptive+Inference+Wei+Su+Peihan+Miao+Huanzhang+Dou+Yongjian+Fu+Xi+Li",
    "gs_search_success": true,
    "gs_authors": [
      "kwEc1AgAAAAJ",
      "TYNPJQMAAAAJ",
      "N0kaKcwAAAAJ",
      "8Miq-agAAAAJ"
    ],
    "citation_count": 30,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2503.03446",
    "title": "Biased Heritage: How Datasets Shape Models in Facial Expression Recognition",
    "year": 2025,
    "published": "2025-03-05T12:25:22Z",
    "categories": [
      "cs.CV",
      "cs.CY"
    ],
    "abstract": "In recent years, the rapid development of artificial intelligence (AI) systems has raised concerns about our ability to ensure their fairness, that is, how to avoid discrimination based on protected characteristics such as gender, race, or age. While algorithmic fairness is well-studied in simple binary classification tasks on tabular data, its application to complex, real-world scenarios-such as Facial Expression Recognition (FER)-remains underexplored. FER presents unique challenges: it is inh",
    "arxiv_url": "https://arxiv.org/abs/2503.03446v1",
    "pdf_url": "https://arxiv.org/pdf/2503.03446v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2503.03446",
    "arxiv_authors": [
      "Iris Dominguez-Catena",
      "Daniel Paternain",
      "Mikel Galar",
      "MaryBeth Defrance",
      "Maarten Buyl",
      "Tijl De Bie"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Biased+Heritage%3A+How+Datasets+Shape+Models+in+Facial+Expression+Recognition+Iris+Dominguez-Catena+Daniel+Paternain+Mikel+Galar+MaryBeth+Defrance+Maarten+Buyl",
    "gs_search_success": true,
    "gs_authors": [
      "A5bU3BUAAAAJ",
      "qrGF8m0AAAAJ",
      "eH_c4R4AAAAJ",
      "V_MNUskAAAAJ",
      "I-yOYzMAAAAJ",
      "vva4E5gAAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2408.09117",
    "title": "LOID: Lane Occlusion Inpainting and Detection for Enhanced Autonomous Driving Systems",
    "year": 2024,
    "published": "2024-08-17T06:55:40Z",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "abstract": "Accurate lane detection is essential for effective path planning and lane following in autonomous driving, especially in scenarios with significant occlusion from vehicles and pedestrians. Existing models often struggle under such conditions, leading to unreliable navigation and safety risks. We propose two innovative approaches to enhance lane detection in these challenging environments, each showing notable improvements over current methods.   The first approach aug-Segment improves convention",
    "arxiv_url": "https://arxiv.org/abs/2408.09117v1",
    "pdf_url": "https://arxiv.org/pdf/2408.09117v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2408.09117",
    "arxiv_authors": [
      "Aayush Agrawal",
      "Ashmitha Jaysi Sivakumar",
      "Ibrahim Kaif",
      "Chayan Banerjee"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=LOID%3A+Lane+Occlusion+Inpainting+and+Detection+for+Enhanced+Autonomous+Driving+Systems+Aayush+Agrawal+Ashmitha+Jaysi+Sivakumar+Ibrahim+Kaif+Chayan+Banerjee",
    "gs_search_success": true,
    "gs_authors": [
      "65hqtGYAAAAJ",
      "HsoQwC4AAAAJ",
      "z8JYL5YAAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2308.09718",
    "title": "Towards Large-scale 3D Representation Learning with Multi-dataset Point Prompt Training",
    "year": 2023,
    "published": "2023-08-18T17:59:57Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "The rapid advancement of deep learning models often attributes to their ability to leverage massive training data. In contrast, such privilege has not yet fully benefited 3D deep learning, mainly due to the limited availability of large-scale 3D datasets. Merging multiple available data sources and letting them collaboratively train a single model is a potential solution. However, due to the large domain gap between 3D point cloud datasets, such mixed supervision could adversely affect the model",
    "arxiv_url": "https://arxiv.org/abs/2308.09718v2",
    "pdf_url": "https://arxiv.org/pdf/2308.09718v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2308.09718",
    "arxiv_authors": [
      "Xiaoyang Wu",
      "Zhuotao Tian",
      "Xin Wen",
      "Bohao Peng",
      "Xihui Liu",
      "Kaicheng Yu",
      "Hengshuang Zhao"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Towards+Large-scale+3D+Representation+Learning+with+Multi-dataset+Point+Prompt+Training+Xiaoyang+Wu+Zhuotao+Tian+Xin+Wen+Bohao+Peng+Xihui+Liu",
    "gs_search_success": true,
    "gs_authors": [
      "9xcCm1oAAAAJ",
      "4YL23GMAAAAJ",
      "Jtmq_m0AAAAJ",
      "byCeJl4AAAAJ",
      "Np1dTpQAAAAJ",
      "mEjhz-IAAAAJ",
      "4uE10I0AAAAJ"
    ],
    "citation_count": 86,
    "gs_author_count": 7
  },
  {
    "arxiv_id": "2501.12255",
    "title": "HAC++: Towards 100X Compression of 3D Gaussian Splatting",
    "year": 2025,
    "published": "2025-01-21T16:23:05Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has emerged as a promising framework for novel view synthesis, boasting rapid rendering speed with high fidelity. However, the substantial Gaussians and their associated attributes necessitate effective compression techniques. Nevertheless, the sparse and unorganized nature of the point cloud of Gaussians (or anchors in our paper) presents challenges for compression. To achieve a compact size, we propose HAC++, which leverages the relationships between unorganized an",
    "arxiv_url": "https://arxiv.org/abs/2501.12255v4",
    "pdf_url": "https://arxiv.org/pdf/2501.12255v4",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2501.12255",
    "arxiv_authors": [
      "Yihang Chen",
      "Qianyi Wu",
      "Weiyao Lin",
      "Mehrtash Harandi",
      "Jianfei Cai"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=HAC%2B%2B%3A+Towards+100X+Compression+of+3D+Gaussian+Splatting+Yihang+Chen+Qianyi+Wu+Weiyao+Lin+Mehrtash+Harandi+Jianfei+Cai",
    "gs_search_success": true,
    "gs_authors": [
      "S9g81n8AAAAJ",
      "05KWkUAAAAAJ",
      "Z9gvBegAAAAJ",
      "XI0RtesAAAAJ",
      "N6czCoUAAAAJ"
    ],
    "citation_count": 21,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2407.02793",
    "title": "Learning Positional Attention for Sequential Recommendation",
    "year": 2024,
    "published": "2024-07-03T03:42:13Z",
    "categories": [
      "cs.IR",
      "cs.CV"
    ],
    "abstract": "Self-attention-based networks have achieved remarkable performance in sequential recommendation tasks. A crucial component of these models is positional encoding. In this study, we delve into the learned positional embedding, demonstrating that it often captures the distance between tokens. Building on this insight, we introduce novel attention models that directly learn positional relations. Extensive experiments reveal that our proposed models, \\textbf{PARec} and \\textbf{FPARec} outperform pre",
    "arxiv_url": "https://arxiv.org/abs/2407.02793v3",
    "pdf_url": "https://arxiv.org/pdf/2407.02793v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2407.02793",
    "arxiv_authors": [
      "Fan Luo",
      "Haibo He",
      "Juan Zhang",
      "Shenghui Xu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Learning+Positional+Attention+for+Sequential+Recommendation+Fan+Luo+Haibo+He+Juan+Zhang+Shenghui+Xu",
    "gs_search_success": true,
    "gs_authors": [
      "IZVYn8IAAAAJ"
    ],
    "citation_count": 2,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2306.08859",
    "title": "SF-TMN: SlowFast Temporal Modeling Network for Surgical Phase Recognition",
    "year": 2023,
    "published": "2023-06-15T05:04:29Z",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "abstract": "Automatic surgical phase recognition is one of the key technologies to support Video-Based Assessment (VBA) systems for surgical education. Utilizing temporal information is crucial for surgical phase recognition, hence various recent approaches extract frame-level features to conduct full video temporal modeling. For better temporal modeling, we propose SlowFast Temporal Modeling Network (SF-TMN) for surgical phase recognition that can not only achieve frame-level full video temporal modeling b",
    "arxiv_url": "https://arxiv.org/abs/2306.08859v1",
    "pdf_url": "https://arxiv.org/pdf/2306.08859v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2306.08859",
    "arxiv_authors": [
      "Bokai Zhang",
      "Mohammad Hasan Sarhan",
      "Bharti Goel",
      "Svetlana Petculescu",
      "Amer Ghanem"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=SF-TMN%3A+SlowFast+Temporal+Modeling+Network+for+Surgical+Phase+Recognition+Bokai+Zhang+Mohammad+Hasan+Sarhan+Bharti+Goel+Svetlana+Petculescu+Amer+Ghanem",
    "gs_search_success": true,
    "gs_authors": [
      "yRWSRtsAAAAJ",
      "YtAA6ToAAAAJ",
      "6cDzhYMAAAAJ",
      "tjrf8EIAAAAJ"
    ],
    "citation_count": 15,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2409.16431",
    "title": "Hand Gesture Classification Based on Forearm Ultrasound Video Snippets Using 3D Convolutional Neural Networks",
    "year": 2024,
    "published": "2024-09-24T19:51:41Z",
    "categories": [
      "cs.CV",
      "cs.RO",
      "eess.IV"
    ],
    "abstract": "Ultrasound based hand movement estimation is a crucial area of research with applications in human-machine interaction. Forearm ultrasound offers detailed information about muscle morphology changes during hand movement which can be used to estimate hand gestures. Previous work has focused on analyzing 2-Dimensional (2D) ultrasound image frames using techniques such as convolutional neural networks (CNNs). However, such 2D techniques do not capture temporal features from segments of ultrasound d",
    "arxiv_url": "https://arxiv.org/abs/2409.16431v1",
    "pdf_url": "https://arxiv.org/pdf/2409.16431v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2409.16431",
    "arxiv_authors": [
      "Keshav Bimbraw",
      "Ankit Talele",
      "Haichong K. Zhang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Hand+Gesture+Classification+Based+on+Forearm+Ultrasound+Video+Snippets+Using+3D+Convolutional+Neural+Networks+Keshav+Bimbraw+Ankit+Talele+Haichong+K.+Zhang",
    "gs_search_success": true,
    "gs_authors": [
      "8HTrAP4AAAAJ",
      "jCeibEwAAAAJ",
      "66xtoqIAAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2307.06335",
    "title": "Neural Free-Viewpoint Relighting for Glossy Indirect Illumination",
    "year": 2023,
    "published": "2023-07-12T17:56:09Z",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "abstract": "Precomputed Radiance Transfer (PRT) remains an attractive solution for real-time rendering of complex light transport effects such as glossy global illumination. After precomputation, we can relight the scene with new environment maps while changing viewpoint in real-time. However, practical PRT methods are usually limited to low-frequency spherical harmonic lighting. All-frequency techniques using wavelets are promising but have so far had little practical impact. The curse of dimensionality an",
    "arxiv_url": "https://arxiv.org/abs/2307.06335v1",
    "pdf_url": "https://arxiv.org/pdf/2307.06335v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2307.06335",
    "arxiv_authors": [
      "Nithin Raghavan",
      "Yan Xiao",
      "Kai-En Lin",
      "Tiancheng Sun",
      "Sai Bi",
      "Zexiang Xu",
      "Tzu-Mao Li",
      "Ravi Ramamoorthi"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Neural+Free-Viewpoint+Relighting+for+Glossy+Indirect+Illumination+Nithin+Raghavan+Yan+Xiao+Kai-En+Lin+Tiancheng+Sun+Sai+Bi",
    "gs_search_success": true,
    "gs_authors": [
      "VpsjjcYAAAAJ",
      "_RRIYvEAAAAJ",
      "6oM-aXUAAAAJ",
      "Y7MCOdYAAAAJ",
      "kYGsix0AAAAJ",
      "-q4nE1kAAAAJ",
      "vCYpQV4AAAAJ",
      "q0MzO6cAAAAJ"
    ],
    "citation_count": 13,
    "gs_author_count": 8
  },
  {
    "arxiv_id": "2412.13611",
    "title": "Robust Tracking via Mamba-based Context-aware Token Learning",
    "year": 2024,
    "published": "2024-12-18T08:37:22Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "How to make a good trade-off between performance and computational cost is crucial for a tracker. However, current famous methods typically focus on complicated and time-consuming learning that combining temporal and appearance information by input more and more images (or features). Consequently, these methods not only increase the model's computational source and learning burden but also introduce much useless and potentially interfering information. To alleviate the above issues, we propose a",
    "arxiv_url": "https://arxiv.org/abs/2412.13611v1",
    "pdf_url": "https://arxiv.org/pdf/2412.13611v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2412.13611",
    "arxiv_authors": [
      "Jinxia Xie",
      "Bineng Zhong",
      "Qihua Liang",
      "Ning Li",
      "Zhiyi Mo",
      "Shuxiang Song"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Robust+Tracking+via+Mamba-based+Context-aware+Token+Learning+Jinxia+Xie+Bineng+Zhong+Qihua+Liang+Ning+Li+Zhiyi+Mo",
    "gs_search_success": true,
    "gs_authors": [
      "hvRBydsAAAAJ",
      "FdPRC04AAAAJ"
    ],
    "citation_count": 13,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2503.20540",
    "title": "Beyond Intermediate States: Explaining Visual Redundancy through Language",
    "year": 2025,
    "published": "2025-03-26T13:38:10Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Multi-modal Large Langue Models (MLLMs) often process thousands of visual tokens, which consume a significant portion of the context window and impose a substantial computational burden. Prior work has empirically explored visual token pruning methods based on MLLMs' intermediate states (e.g., attention scores). However, they have limitations in precisely defining visual redundancy due to their inability to capture the influence of visual tokens on MLLMs' visual understanding (i.e., the predicte",
    "arxiv_url": "https://arxiv.org/abs/2503.20540v1",
    "pdf_url": "https://arxiv.org/pdf/2503.20540v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2503.20540",
    "arxiv_authors": [
      "Dingchen Yang",
      "Bowen Cao",
      "Anran Zhang",
      "Weibo Gu",
      "Winston Hu",
      "Guang Chen"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Beyond+Intermediate+States%3A+Explaining+Visual+Redundancy+through+Language+Dingchen+Yang+Bowen+Cao+Anran+Zhang+Weibo+Gu+Winston+Hu",
    "gs_search_success": true,
    "gs_authors": [
      "u_7NMWwAAAAJ",
      "jaI8ym8AAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2402.12531",
    "title": "Improving Deep Generative Models on Many-To-One Image-to-Image Translation",
    "year": 2024,
    "published": "2024-02-19T20:41:03Z",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "Deep generative models have been applied to multiple applications in image-to-image translation. Generative Adversarial Networks and Diffusion Models have presented impressive results, setting new state-of-the-art results on these tasks. Most methods have symmetric setups across the different domains in a dataset. These methods assume that all domains have either multiple modalities or only one modality. However, there are many datasets that have a many-to-one relationship between two domains. I",
    "arxiv_url": "https://arxiv.org/abs/2402.12531v2",
    "pdf_url": "https://arxiv.org/pdf/2402.12531v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2402.12531",
    "arxiv_authors": [
      "Sagar Saxena",
      "Mohammad Nayeem Teli"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Improving+Deep+Generative+Models+on+Many-To-One+Image-to-Image+Translation+Sagar+Saxena+Mohammad+Nayeem+Teli",
    "gs_search_success": true,
    "gs_authors": [
      "-BdyRikAAAAJ",
      "3mSGT78AAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 2
  },
  {
    "arxiv_id": "2504.04935",
    "title": "RCCFormer: A Robust Crowd Counting Network Based on Transformer",
    "year": 2025,
    "published": "2025-04-07T11:19:05Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Crowd counting, which is a key computer vision task, has emerged as a fundamental technology in crowd analysis and public safety management. However, challenges such as scale variations and complex backgrounds significantly impact the accuracy of crowd counting. To mitigate these issues, this paper proposes a robust Transformer-based crowd counting network, termed RCCFormer, specifically designed for background suppression and scale awareness. The proposed method incorporates a Multi-level Featu",
    "arxiv_url": "https://arxiv.org/abs/2504.04935v1",
    "pdf_url": "https://arxiv.org/pdf/2504.04935v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2504.04935",
    "arxiv_authors": [
      "Peng Liu",
      "Heng-Chao Li",
      "Sen Lei",
      "Nanqing Liu",
      "Bin Feng",
      "Xiao Wu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=RCCFormer%3A+A+Robust+Crowd+Counting+Network+Based+on+Transformer+Peng+Liu+Heng-Chao+Li+Sen+Lei+Nanqing+Liu+Bin+Feng",
    "gs_search_success": true,
    "gs_authors": [
      "7k_DigcAAAAJ",
      "x3dCJrAAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2503.15342",
    "title": "TruthLens:A Training-Free Paradigm for DeepFake Detection",
    "year": 2025,
    "published": "2025-03-19T15:41:32Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "The proliferation of synthetic images generated by advanced AI models poses significant challenges in identifying and understanding manipulated visual content. Current fake image detection methods predominantly rely on binary classification models that focus on accuracy while often neglecting interpretability, leaving users without clear insights into why an image is deemed real or fake. To bridge this gap, we introduce TruthLens, a novel training-free framework that reimagines deepfake detectio",
    "arxiv_url": "https://arxiv.org/abs/2503.15342v1",
    "pdf_url": "https://arxiv.org/pdf/2503.15342v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2503.15342",
    "arxiv_authors": [
      "Ritabrata Chakraborty",
      "Rajatsubhra Chakraborty",
      "Ali Khaleghi Rahimian",
      "Thomas MacDougall"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=TruthLens%3AA+Training-Free+Paradigm+for+DeepFake+Detection+Ritabrata+Chakraborty+Rajatsubhra+Chakraborty+Ali+Khaleghi+Rahimian+Thomas+MacDougall",
    "gs_search_success": true,
    "gs_authors": [
      "1VMtl5gAAAAJ",
      "MIelP8kAAAAJ",
      "39r7ciQAAAAJ"
    ],
    "citation_count": 2,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2504.14516",
    "title": "Back on Track: Bundle Adjustment for Dynamic Scene Reconstruction",
    "year": 2025,
    "published": "2025-04-20T07:29:42Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Traditional SLAM systems, which rely on bundle adjustment, struggle with highly dynamic scenes commonly found in casual videos. Such videos entangle the motion of dynamic elements, undermining the assumption of static environments required by traditional systems. Existing techniques either filter out dynamic elements or model their motion independently. However, the former often results in incomplete reconstructions, whereas the latter can lead to inconsistent motion estimates. Taking a novel ap",
    "arxiv_url": "https://arxiv.org/abs/2504.14516v2",
    "pdf_url": "https://arxiv.org/pdf/2504.14516v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2504.14516",
    "arxiv_authors": [
      "Weirong Chen",
      "Ganlin Zhang",
      "Felix Wimbauer",
      "Rui Wang",
      "Nikita Araslanov",
      "Andrea Vedaldi",
      "Daniel Cremers"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Back+on+Track%3A+Bundle+Adjustment+for+Dynamic+Scene+Reconstruction+Weirong+Chen+Ganlin+Zhang+Felix+Wimbauer+Rui+Wang+Nikita+Araslanov",
    "gs_search_success": true,
    "gs_authors": [
      "SRPTcxAAAAAJ",
      "RdMFioAAAAAJ",
      "cXQciMEAAAAJ",
      "bRT7t28AAAAJ",
      "1B_T56IAAAAJ",
      "50yGm5wAAAAJ",
      "buN3yw8AAAAJ"
    ],
    "citation_count": 3,
    "gs_author_count": 7
  },
  {
    "arxiv_id": "2405.06389",
    "title": "Continual Novel Class Discovery via Feature Enhancement and Adaptation",
    "year": 2024,
    "published": "2024-05-10T10:52:22Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Continual Novel Class Discovery (CNCD) aims to continually discover novel classes without labels while maintaining the recognition capability for previously learned classes. The main challenges faced by CNCD include the feature-discrepancy problem, the inter-session confusion problem, etc. In this paper, we propose a novel Feature Enhancement and Adaptation method for the CNCD to tackle the above challenges, which consists of a guide-to-novel framework, a centroid-to-samples similarity constrain",
    "arxiv_url": "https://arxiv.org/abs/2405.06389v1",
    "pdf_url": "https://arxiv.org/pdf/2405.06389v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2405.06389",
    "arxiv_authors": [
      "Yifan Yu",
      "Shaokun Wang",
      "Yuhang He",
      "Junzhe Chen",
      "Yihong Gong"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Continual+Novel+Class+Discovery+via+Feature+Enhancement+and+Adaptation+Yifan+Yu+Shaokun+Wang+Yuhang+He+Junzhe+Chen+Yihong+Gong",
    "gs_search_success": true,
    "gs_authors": [
      "9VCIiVcAAAAJ",
      "1muAGtYAAAAJ",
      "x2xdU7gAAAAJ"
    ],
    "citation_count": 3,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2412.11284",
    "title": "Learning Normal Flow Directly From Event Neighborhoods",
    "year": 2024,
    "published": "2024-12-15T19:09:45Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Event-based motion field estimation is an important task. However, current optical flow methods face challenges: learning-based approaches, often frame-based and relying on CNNs, lack cross-domain transferability, while model-based methods, though more robust, are less accurate. To address the limitations of optical flow estimation, recent works have focused on normal flow, which can be more reliably measured in regions with limited texture or strong edges. However, existing normal flow estimato",
    "arxiv_url": "https://arxiv.org/abs/2412.11284v1",
    "pdf_url": "https://arxiv.org/pdf/2412.11284v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2412.11284",
    "arxiv_authors": [
      "Dehao Yuan",
      "Levi Burner",
      "Jiayi Wu",
      "Minghui Liu",
      "Jingxi Chen",
      "Yiannis Aloimonos",
      "Cornelia Fermüller"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Learning+Normal+Flow+Directly+From+Event+Neighborhoods+Dehao+Yuan+Levi+Burner+Jiayi+Wu+Minghui+Liu+Jingxi+Chen",
    "gs_search_success": true,
    "gs_authors": [
      "B3zkxloAAAAJ",
      "3HJahTsAAAAJ",
      "jcfUmocAAAAJ",
      "xoZE1GsAAAAJ",
      "BKY_uwoAAAAJ"
    ],
    "citation_count": 5,
    "gs_author_count": 7
  },
  {
    "arxiv_id": "2401.11492",
    "title": "Edge-Enabled Real-time Railway Track Segmentation",
    "year": 2024,
    "published": "2024-01-21T13:45:52Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Accurate and rapid railway track segmentation can assist automatic train driving and is a key step in early warning to fixed or moving obstacles on the railway track. However, certain existing algorithms tailored for track segmentation often struggle to meet the requirements of real-time and efficiency on resource-constrained edge devices. Considering this challenge, we propose an edge-enabled real-time railway track segmentation algorithm, which is optimized to be suitable for edge applications",
    "arxiv_url": "https://arxiv.org/abs/2401.11492v1",
    "pdf_url": "https://arxiv.org/pdf/2401.11492v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2401.11492",
    "arxiv_authors": [
      "Chen Chenglin",
      "Wang Fei",
      "Yang Min",
      "Qin Yong",
      "Bai Yun"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Edge-Enabled+Real-time+Railway+Track+Segmentation+Chen+Chenglin+Wang+Fei+Yang+Min+Qin+Yong+Bai+Yun",
    "gs_search_success": true,
    "gs_authors": [],
    "citation_count": 2,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2504.20091",
    "title": "VideoMultiAgents: A Multi-Agent Framework for Video Question Answering",
    "year": 2025,
    "published": "2025-04-25T22:08:09Z",
    "categories": [
      "cs.CV",
      "cs.MA"
    ],
    "abstract": "Video Question Answering (VQA) inherently relies on multimodal reasoning, integrating visual, temporal, and linguistic cues to achieve a deeper understanding of video content. However, many existing methods rely on feeding frame-level captions into a single model, making it difficult to adequately capture temporal and interactive contexts. To address this limitation, we introduce VideoMultiAgents, a framework that integrates specialized agents for vision, scene graph analysis, and text processin",
    "arxiv_url": "https://arxiv.org/abs/2504.20091v2",
    "pdf_url": "https://arxiv.org/pdf/2504.20091v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2504.20091",
    "arxiv_authors": [
      "Noriyuki Kugo",
      "Xiang Li",
      "Zixin Li",
      "Ashish Gupta",
      "Arpandeep Khatua",
      "Nidhish Jain",
      "Chaitanya Patel",
      "Yuta Kyuragi",
      "Yasunori Ishii",
      "Masamoto Tanabiki",
      "Kazuki Kozuka",
      "Ehsan Adeli"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=VideoMultiAgents%3A+A+Multi-Agent+Framework+for+Video+Question+Answering+Noriyuki+Kugo+Xiang+Li+Zixin+Li+Ashish+Gupta+Arpandeep+Khatua",
    "gs_search_success": true,
    "gs_authors": [
      "0zN_288AAAAJ",
      "hOo4TBcAAAAJ",
      "u_PUG9MAAAAJ"
    ],
    "citation_count": 7,
    "gs_author_count": 9
  },
  {
    "arxiv_id": "2303.01112",
    "title": "Visual Atoms: Pre-training Vision Transformers with Sinusoidal Waves",
    "year": 2023,
    "published": "2023-03-02T09:47:28Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "abstract": "Formula-driven supervised learning (FDSL) has been shown to be an effective method for pre-training vision transformers, where ExFractalDB-21k was shown to exceed the pre-training effect of ImageNet-21k. These studies also indicate that contours mattered more than textures when pre-training vision transformers. However, the lack of a systematic investigation as to why these contour-oriented synthetic datasets can achieve the same accuracy as real datasets leaves much room for skepticism. In the ",
    "arxiv_url": "https://arxiv.org/abs/2303.01112v1",
    "pdf_url": "https://arxiv.org/pdf/2303.01112v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2303.01112",
    "arxiv_authors": [
      "Sora Takashima",
      "Ryo Hayamizu",
      "Nakamasa Inoue",
      "Hirokatsu Kataoka",
      "Rio Yokota"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Visual+Atoms%3A+Pre-training+Vision+Transformers+with+Sinusoidal+Waves+Sora+Takashima+Ryo+Hayamizu+Nakamasa+Inoue+Hirokatsu+Kataoka+Rio+Yokota",
    "gs_search_success": true,
    "gs_authors": [
      "DJTdm1cAAAAJ",
      "MLaytfkAAAAJ",
      "f1CePVQAAAAJ"
    ],
    "citation_count": 31,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2501.01371",
    "title": "CLIP-UP: CLIP-Based Unanswerable Problem Detection for Visual Question Answering",
    "year": 2025,
    "published": "2025-01-02T17:30:53Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Recent Vision-Language Models (VLMs) have demonstrated remarkable capabilities in visual understanding and reasoning, and in particular on multiple-choice Visual Question Answering (VQA). Still, these models can make distinctly unnatural errors, for example, providing (wrong) answers to unanswerable VQA questions, such as questions asking about objects that do not appear in the image. To address this issue, we propose CLIP-UP: CLIP-based Unanswerable Problem detection, a novel lightweight method",
    "arxiv_url": "https://arxiv.org/abs/2501.01371v1",
    "pdf_url": "https://arxiv.org/pdf/2501.01371v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2501.01371",
    "arxiv_authors": [
      "Ben Vardi",
      "Oron Nir",
      "Ariel Shamir"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=CLIP-UP%3A+CLIP-Based+Unanswerable+Problem+Detection+for+Visual+Question+Answering+Ben+Vardi+Oron+Nir+Ariel+Shamir",
    "gs_search_success": true,
    "gs_authors": [
      "-q90a0EAAAAJ",
      "f4v5HMgAAAAJ",
      "qCsDSbEAAAAJ"
    ],
    "citation_count": 4,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2404.18801",
    "title": "A Partial Replication of MaskFormer in TensorFlow on TPUs for the TensorFlow Model Garden",
    "year": 2024,
    "published": "2024-04-29T15:40:40Z",
    "categories": [
      "cs.CV",
      "cs.LG",
      "cs.SE"
    ],
    "abstract": "This paper undertakes the task of replicating the MaskFormer model a universal image segmentation model originally developed using the PyTorch framework, within the TensorFlow ecosystem, specifically optimized for execution on Tensor Processing Units (TPUs). Our implementation exploits the modular constructs available within the TensorFlow Model Garden (TFMG), encompassing elements such as the data loader, training orchestrator, and various architectural components, tailored and adapted to meet ",
    "arxiv_url": "https://arxiv.org/abs/2404.18801v1",
    "pdf_url": "https://arxiv.org/pdf/2404.18801v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2404.18801",
    "arxiv_authors": [
      "Vishal Purohit",
      "Wenxin Jiang",
      "Akshath R. Ravikiran",
      "James C. Davis"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=A+Partial+Replication+of+MaskFormer+in+TensorFlow+on+TPUs+for+the+TensorFlow+Model+Garden+Vishal+Purohit+Wenxin+Jiang+Akshath+R.+Ravikiran+James+C.+Davis",
    "gs_search_success": true,
    "gs_authors": [
      "skZNwokAAAAJ",
      "VSAWPQ4AAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2401.16867",
    "title": "A Tournament of Transformation Models: B-Spline-based vs. Mesh-based Multi-Objective Deformable Image Registration",
    "year": 2024,
    "published": "2024-01-30T10:17:46Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.NE"
    ],
    "abstract": "The transformation model is an essential component of any deformable image registration approach. It provides a representation of physical deformations between images, thereby defining the range and realism of registrations that can be found. Two types of transformation models have emerged as popular choices: B-spline models and mesh models. Although both models have been investigated in detail, a direct comparison has not yet been made, since the models are optimized using very different optimi",
    "arxiv_url": "https://arxiv.org/abs/2401.16867v1",
    "pdf_url": "https://arxiv.org/pdf/2401.16867v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2401.16867",
    "arxiv_authors": [
      "Georgios Andreadis",
      "Joas I. Mulder",
      "Anton Bouter",
      "Peter A. N. Bosman",
      "Tanja Alderliesten"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=A+Tournament+of+Transformation+Models%3A+B-Spline-based+vs.+Mesh-based+Multi-Objective+Deformable+Image+Registration+Georgios+Andreadis+Joas+I.+Mulder+Anton+Bouter+Peter+A.+N.+Bosman+Tanja+Alderliesten",
    "gs_search_success": false,
    "gs_authors": [],
    "citation_count": null,
    "error_type": "fetch_failed",
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2404.05673",
    "title": "CoReS: Orchestrating the Dance of Reasoning and Segmentation",
    "year": 2024,
    "published": "2024-04-08T16:55:39Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "The reasoning segmentation task, which demands a nuanced comprehension of intricate queries to accurately pinpoint object regions, is attracting increasing attention. However, Multi-modal Large Language Models (MLLM) often find it difficult to accurately localize the objects described in complex reasoning contexts. We believe that the act of reasoning segmentation should mirror the cognitive stages of human visual search, where each step is a progressive refinement of thought toward the final ob",
    "arxiv_url": "https://arxiv.org/abs/2404.05673v3",
    "pdf_url": "https://arxiv.org/pdf/2404.05673v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2404.05673",
    "arxiv_authors": [
      "Xiaoyi Bao",
      "Siyang Sun",
      "Shuailei Ma",
      "Kecheng Zheng",
      "Yuxin Guo",
      "Guosheng Zhao",
      "Yun Zheng",
      "Xingang Wang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=CoReS%3A+Orchestrating+the+Dance+of+Reasoning+and+Segmentation+Xiaoyi+Bao+Siyang+Sun+Shuailei+Ma+Kecheng+Zheng+Yuxin+Guo",
    "gs_search_success": true,
    "gs_authors": [
      "IK9DwSIAAAAJ",
      "C3BbrU4AAAAJ",
      "dNhzCu4AAAAJ",
      "-hFpScAAAAAJ",
      "x_0spxgAAAAJ",
      "hMDQifQAAAAJ",
      "gSI_eiIAAAAJ"
    ],
    "citation_count": 16,
    "gs_author_count": 8
  },
  {
    "arxiv_id": "2306.11710",
    "title": "Data-Driven but Privacy-Conscious: Pedestrian Dataset De-identification via Full-Body Person Synthesis",
    "year": 2023,
    "published": "2023-06-20T17:39:24Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "The advent of data-driven technology solutions is accompanied by an increasing concern with data privacy. This is of particular importance for human-centered image recognition tasks, such as pedestrian detection, re-identification, and tracking. To highlight the importance of privacy issues and motivate future research, we motivate and introduce the Pedestrian Dataset De-Identification (PDI) task. PDI evaluates the degree of de-identification and downstream task training performance for a given ",
    "arxiv_url": "https://arxiv.org/abs/2306.11710v2",
    "pdf_url": "https://arxiv.org/pdf/2306.11710v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2306.11710",
    "arxiv_authors": [
      "Maxim Maximov",
      "Tim Meinhardt",
      "Ismail Elezi",
      "Zoe Papakipos",
      "Caner Hazirbas",
      "Cristian Canton Ferrer",
      "Laura Leal-Taixé"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Data-Driven+but+Privacy-Conscious%3A+Pedestrian+Dataset+De-identification+via+Full-Body+Person+Synthesis+Maxim+Maximov+Tim+Meinhardt+Ismail+Elezi+Zoe+Papakipos+Caner+Hazirbas",
    "gs_search_success": true,
    "gs_authors": [
      "akzoGjUAAAAJ",
      "RVfOKa0AAAAJ",
      "tT2TC-UAAAAJ",
      "70pPqNkAAAAJ",
      "u0g6zqsAAAAJ",
      "JEiXKpcAAAAJ"
    ],
    "citation_count": 2,
    "gs_author_count": 7
  },
  {
    "arxiv_id": "2307.10387",
    "title": "POV-Surgery: A Dataset for Egocentric Hand and Tool Pose Estimation During Surgical Activities",
    "year": 2023,
    "published": "2023-07-19T18:00:32Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "The surgical usage of Mixed Reality (MR) has received growing attention in areas such as surgical navigation systems, skill assessment, and robot-assisted surgeries. For such applications, pose estimation for hand and surgical instruments from an egocentric perspective is a fundamental task and has been studied extensively in the computer vision field in recent years. However, the development of this field has been impeded by a lack of datasets, especially in the surgical field, where bloody glo",
    "arxiv_url": "https://arxiv.org/abs/2307.10387v1",
    "pdf_url": "https://arxiv.org/pdf/2307.10387v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2307.10387",
    "arxiv_authors": [
      "Rui Wang",
      "Sophokles Ktistakis",
      "Siwei Zhang",
      "Mirko Meboldt",
      "Quentin Lohmeyer"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=POV-Surgery%3A+A+Dataset+for+Egocentric+Hand+and+Tool+Pose+Estimation+During+Surgical+Activities+Rui+Wang+Sophokles+Ktistakis+Siwei+Zhang+Mirko+Meboldt+Quentin+Lohmeyer",
    "gs_search_success": true,
    "gs_authors": [
      "b3lMQc8AAAAJ",
      "YiRW8PQAAAAJ",
      "q7JVOrQAAAAJ",
      "oxfOn3UAAAAJ"
    ],
    "citation_count": 22,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2502.08997",
    "title": "Hierarchical Vision Transformer with Prototypes for Interpretable Medical Image Classification",
    "year": 2025,
    "published": "2025-02-13T06:24:07Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Explainability is a highly demanded requirement for applications in high-risk areas such as medicine. Vision Transformers have mainly been limited to attention extraction to provide insight into the model's reasoning. Our approach combines the high performance of Vision Transformers with the introduction of new explainability capabilities. We present HierViT, a Vision Transformer that is inherently interpretable and adapts its reasoning to that of humans. A hierarchical structure is used to proc",
    "arxiv_url": "https://arxiv.org/abs/2502.08997v1",
    "pdf_url": "https://arxiv.org/pdf/2502.08997v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2502.08997",
    "arxiv_authors": [
      "Luisa Gallée",
      "Catharina Silvia Lisson",
      "Meinrad Beer",
      "Michael Götz"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Hierarchical+Vision+Transformer+with+Prototypes+for+Interpretable+Medical+Image+Classification+Luisa+Gall%C3%A9e+Catharina+Silvia+Lisson+Meinrad+Beer+Michael+G%C3%B6tz",
    "gs_search_success": true,
    "gs_authors": [
      "RotwTNsAAAAJ",
      "GSyIRg4AAAAJ"
    ],
    "citation_count": 3,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2505.15576",
    "title": "Visual Perturbation and Adaptive Hard Negative Contrastive Learning for Compositional Reasoning in Vision-Language Models",
    "year": 2025,
    "published": "2025-05-21T14:28:43Z",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "Vision-Language Models (VLMs) are essential for multimodal tasks, especially compositional reasoning (CR) tasks, which require distinguishing fine-grained semantic differences between visual and textual embeddings. However, existing methods primarily fine-tune the model by generating text-based hard negative samples, neglecting the importance of image-based negative samples, which results in insufficient training of the visual encoder and ultimately impacts the overall performance of the model. ",
    "arxiv_url": "https://arxiv.org/abs/2505.15576v2",
    "pdf_url": "https://arxiv.org/pdf/2505.15576v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2505.15576",
    "arxiv_authors": [
      "Xin Huang",
      "Ruibin Li",
      "Tong Jia",
      "Wei Zheng",
      "Ya Wang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Visual+Perturbation+and+Adaptive+Hard+Negative+Contrastive+Learning+for+Compositional+Reasoning+in+Vision-Language+Models+Xin+Huang+Ruibin+Li+Tong+Jia+Wei+Zheng+Ya+Wang",
    "gs_search_success": true,
    "gs_authors": [
      "TjrQo3EAAAAJ",
      "uGqZIQsAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2311.15497",
    "title": "Adaptive Image Registration: A Hybrid Approach Integrating Deep Learning and Optimization Functions for Enhanced Precision",
    "year": 2023,
    "published": "2023-11-27T02:48:06Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "abstract": "Image registration has traditionally been done using two distinct approaches: learning based methods, relying on robust deep neural networks, and optimization-based methods, applying complex mathematical transformations to warp images accordingly. Of course, both paradigms offer advantages and disadvantages, and, in this work, we seek to combine their respective strengths into a single streamlined framework, using the outputs of the learning based method as initial parameters for optimization wh",
    "arxiv_url": "https://arxiv.org/abs/2311.15497v3",
    "pdf_url": "https://arxiv.org/pdf/2311.15497v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2311.15497",
    "arxiv_authors": [
      "Gabriel De Araujo",
      "Shanlin Sun",
      "Xiaohui Xie"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Adaptive+Image+Registration%3A+A+Hybrid+Approach+Integrating+Deep+Learning+and+Optimization+Functions+for+Enhanced+Precision+Gabriel+De+Araujo+Shanlin+Sun+Xiaohui+Xie",
    "gs_search_success": false,
    "gs_authors": [],
    "citation_count": null,
    "error_type": "fetch_failed",
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2412.17595",
    "title": "V$^2$-SfMLearner: Learning Monocular Depth and Ego-motion for Multimodal Wireless Capsule Endoscopy",
    "year": 2024,
    "published": "2024-12-23T14:11:30Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.RO"
    ],
    "abstract": "Deep learning can predict depth maps and capsule ego-motion from capsule endoscopy videos, aiding in 3D scene reconstruction and lesion localization. However, the collisions of the capsule endoscopies within the gastrointestinal tract cause vibration perturbations in the training data. Existing solutions focus solely on vision-based processing, neglecting other auxiliary signals like vibrations that could reduce noise and improve performance. Therefore, we propose V$^2$-SfMLearner, a multimodal ",
    "arxiv_url": "https://arxiv.org/abs/2412.17595v1",
    "pdf_url": "https://arxiv.org/pdf/2412.17595v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2412.17595",
    "arxiv_authors": [
      "Long Bai",
      "Beilei Cui",
      "Liangyu Wang",
      "Yanheng Li",
      "Shilong Yao",
      "Sishen Yuan",
      "Yanan Wu",
      "Yang Zhang",
      "Max Q. -H. Meng",
      "Zhen Li",
      "Weiping Ding",
      "Hongliang Ren"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=V%24%5E2%24-SfMLearner%3A+Learning+Monocular+Depth+and+Ego-motion+for+Multimodal+Wireless+Capsule+Endoscopy+Long+Bai+Beilei+Cui+Liangyu+Wang+Yanheng+Li+Shilong+Yao",
    "gs_search_success": true,
    "gs_authors": [
      "RBTbqVQAAAAJ",
      "G-WO0oYAAAAJ",
      "DxDCU7AAAAAJ",
      "rcF7N44AAAAJ",
      "mGkM_WgAAAAJ",
      "v-fw-98AAAAJ",
      "phzPWXMAAAAJ",
      "lgXDV5cAAAAJ",
      "oq5omiMAAAAJ",
      "OJUbSAMAAAAJ"
    ],
    "citation_count": 6,
    "gs_author_count": 9
  },
  {
    "arxiv_id": "2411.14517",
    "title": "The Double-Ellipsoid Geometry of CLIP",
    "year": 2024,
    "published": "2024-11-21T16:27:22Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Contrastive Language-Image Pre-Training (CLIP) is highly instrumental in machine learning applications within a large variety of domains. We investigate the geometry of this embedding, which is still not well understood. We examine the raw unnormalized embedding and show that text and image reside on linearly separable ellipsoid shells, not centered at the origin. We explain the benefits of having this structure, allowing to better embed instances according to their uncertainty during contrastiv",
    "arxiv_url": "https://arxiv.org/abs/2411.14517v3",
    "pdf_url": "https://arxiv.org/pdf/2411.14517v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2411.14517",
    "arxiv_authors": [
      "Meir Yossef Levi",
      "Guy Gilboa"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=The+Double-Ellipsoid+Geometry+of+CLIP+Meir+Yossef+Levi+Guy+Gilboa",
    "gs_search_success": true,
    "gs_authors": [
      "7k8ZP6UAAAAJ",
      "tH2_FEoAAAAJ"
    ],
    "citation_count": 7,
    "gs_author_count": 2
  },
  {
    "arxiv_id": "2505.17732",
    "title": "RQR3D: Reparametrizing the regression targets for BEV-based 3D object detection",
    "year": 2025,
    "published": "2025-05-23T10:52:34Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "abstract": "Accurate, fast, and reliable 3D perception is essential for autonomous driving. Recently, bird's-eye view (BEV)-based perception approaches have emerged as superior alternatives to perspective-based solutions, offering enhanced spatial understanding and more natural outputs for planning. Existing BEV-based 3D object detection methods, typically adhering to angle-based representation, directly estimate the size and orientation of rotated bounding boxes. We observe that BEV-based 3D object detecti",
    "arxiv_url": "https://arxiv.org/abs/2505.17732v1",
    "pdf_url": "https://arxiv.org/pdf/2505.17732v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2505.17732",
    "arxiv_authors": [
      "Ozsel Kilinc",
      "Cem Tarhan"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=RQR3D%3A+Reparametrizing+the+regression+targets+for+BEV-based+3D+object+detection+Ozsel+Kilinc+Cem+Tarhan",
    "gs_search_success": true,
    "gs_authors": [
      "5xplFFsAAAAJ",
      "XHqdGk8AAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 2
  },
  {
    "arxiv_id": "2304.06857",
    "title": "Enhancing Self-Supervised Learning for Remote Sensing with Elevation Data: A Case Study with Scarce And High Level Semantic Labels",
    "year": 2023,
    "published": "2023-04-13T23:01:11Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "This work proposes a hybrid unsupervised and supervised learning method to pre-train models applied in Earth observation downstream tasks when only a handful of labels denoting very general semantic concepts are available. We combine a contrastive approach to pre-train models with a pixel-wise regression pre-text task to predict coarse elevation maps, which are commonly available worldwide. We hypothesize that this will allow the model to pre-learn useful representations, as there is generally s",
    "arxiv_url": "https://arxiv.org/abs/2304.06857v3",
    "pdf_url": "https://arxiv.org/pdf/2304.06857v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2304.06857",
    "arxiv_authors": [
      "Omar A. Castaño-Idarraga",
      "Raul Ramos-Pollán",
      "Freddie Kalaitzis"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Enhancing+Self-Supervised+Learning+for+Remote+Sensing+with+Elevation+Data%3A+A+Case+Study+with+Scarce+And+High+Level+Semantic+Labels+Omar+A.+Casta%C3%B1o-Idarraga+Raul+Ramos-Poll%C3%A1n+Freddie+Kalaitzis",
    "gs_search_success": true,
    "gs_authors": [
      "QObKt9IAAAAJ",
      "SywTqIIAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2310.13250",
    "title": "Diagnosis-oriented Medical Image Compression with Efficient Transfer Learning",
    "year": 2023,
    "published": "2023-10-20T03:15:13Z",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "abstract": "Remote medical diagnosis has emerged as a critical and indispensable technique in practical medical systems, where medical data are required to be efficiently compressed and transmitted for diagnosis by either professional doctors or intelligent diagnosis devices. In this process, a large amount of redundant content irrelevant to the diagnosis is subjected to high-fidelity coding, leading to unnecessary transmission costs. To mitigate this, we propose diagnosis-oriented medical image compression",
    "arxiv_url": "https://arxiv.org/abs/2310.13250v1",
    "pdf_url": "https://arxiv.org/pdf/2310.13250v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2310.13250",
    "arxiv_authors": [
      "Guangqi Xie",
      "Xin Li",
      "Xiaohan Pan",
      "Zhibo Chen"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Diagnosis-oriented+Medical+Image+Compression+with+Efficient+Transfer+Learning+Guangqi+Xie+Xin+Li+Xiaohan+Pan+Zhibo+Chen",
    "gs_search_success": true,
    "gs_authors": [
      "1ayDJfsAAAAJ",
      "pfEwP84AAAAJ",
      "sbiY97gAAAAJ"
    ],
    "citation_count": 3,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2410.22707",
    "title": "Robotic State Recognition with Image-to-Text Retrieval Task of Pre-Trained Vision-Language Model and Black-Box Optimization",
    "year": 2024,
    "published": "2024-10-30T05:34:52Z",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV"
    ],
    "abstract": "State recognition of the environment and objects, such as the open/closed state of doors and the on/off of lights, is indispensable for robots that perform daily life support and security tasks. Until now, state recognition methods have been based on training neural networks from manual annotations, preparing special sensors for the recognition, or manually programming to extract features from point clouds or raw images. In contrast, we propose a robotic state recognition method using a pre-trai",
    "arxiv_url": "https://arxiv.org/abs/2410.22707v1",
    "pdf_url": "https://arxiv.org/pdf/2410.22707v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2410.22707",
    "arxiv_authors": [
      "Kento Kawaharazuka",
      "Yoshiki Obinata",
      "Naoaki Kanazawa",
      "Kei Okada",
      "Masayuki Inaba"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Robotic+State+Recognition+with+Image-to-Text+Retrieval+Task+of+Pre-Trained+Vision-Language+Model+and+Black-Box+Optimization+Kento+Kawaharazuka+Yoshiki+Obinata+Naoaki+Kanazawa+Kei+Okada+Masayuki+Inaba",
    "gs_search_success": true,
    "gs_authors": [
      "iwOGCwwAAAAJ",
      "QG-dVw8AAAAJ",
      "WCjv9PcAAAAJ",
      "456Oe4YAAAAJ",
      "E75YHyUAAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2303.01237",
    "title": "FlowFormer++: Masked Cost Volume Autoencoding for Pretraining Optical Flow Estimation",
    "year": 2023,
    "published": "2023-03-02T13:28:07Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "FlowFormer introduces a transformer architecture into optical flow estimation and achieves state-of-the-art performance. The core component of FlowFormer is the transformer-based cost-volume encoder. Inspired by the recent success of masked autoencoding (MAE) pretraining in unleashing transformers' capacity of encoding visual representation, we propose Masked Cost Volume Autoencoding (MCVA) to enhance FlowFormer by pretraining the cost-volume encoder with a novel MAE scheme. Firstly, we introduc",
    "arxiv_url": "https://arxiv.org/abs/2303.01237v1",
    "pdf_url": "https://arxiv.org/pdf/2303.01237v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2303.01237",
    "arxiv_authors": [
      "Xiaoyu Shi",
      "Zhaoyang Huang",
      "Dasong Li",
      "Manyuan Zhang",
      "Ka Chun Cheung",
      "Simon See",
      "Hongwei Qin",
      "Jifeng Dai",
      "Hongsheng Li"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=FlowFormer%2B%2B%3A+Masked+Cost+Volume+Autoencoding+for+Pretraining+Optical+Flow+Estimation+Xiaoyu+Shi+Zhaoyang+Huang+Dasong+Li+Manyuan+Zhang+Ka+Chun+Cheung",
    "gs_search_success": true,
    "gs_authors": [
      "BN2Ze-QAAAAJ",
      "ZJLzmwgAAAAJ",
      "NvbCXToAAAAJ",
      "ZGM7HfgAAAAJ",
      "SH_-B_AAAAAJ",
      "ZYmcm0EAAAAJ",
      "fbEuTJUAAAAJ",
      "y2xos7IAAAAJ",
      "ebIHTEoAAAAJ"
    ],
    "citation_count": 167,
    "gs_author_count": 9
  },
  {
    "arxiv_id": "2404.05139",
    "title": "Better Monocular 3D Detectors with LiDAR from the Past",
    "year": 2024,
    "published": "2024-04-08T01:38:43Z",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "abstract": "Accurate 3D object detection is crucial to autonomous driving. Though LiDAR-based detectors have achieved impressive performance, the high cost of LiDAR sensors precludes their widespread adoption in affordable vehicles. Camera-based detectors are cheaper alternatives but often suffer inferior performance compared to their LiDAR-based counterparts due to inherent depth ambiguities in images. In this work, we seek to improve monocular 3D detectors by leveraging unlabeled historical LiDAR data. Sp",
    "arxiv_url": "https://arxiv.org/abs/2404.05139v2",
    "pdf_url": "https://arxiv.org/pdf/2404.05139v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2404.05139",
    "arxiv_authors": [
      "Yurong You",
      "Cheng Perng Phoo",
      "Carlos Andres Diaz-Ruiz",
      "Katie Z Luo",
      "Wei-Lun Chao",
      "Mark Campbell",
      "Bharath Hariharan",
      "Kilian Q Weinberger"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Better+Monocular+3D+Detectors+with+LiDAR+from+the+Past+Yurong+You+Cheng+Perng+Phoo+Carlos+Andres+Diaz-Ruiz+Katie+Z+Luo+Wei-Lun+Chao",
    "gs_search_success": true,
    "gs_authors": [
      "e1iAhHQAAAAJ",
      "kt9D2usAAAAJ",
      "rdwkreIAAAAJ",
      "PGKakWwAAAAJ",
      "ud0vmoMAAAAJ",
      "qlmK27YAAAAJ",
      "TpglobcAAAAJ"
    ],
    "citation_count": 3,
    "gs_author_count": 8
  },
  {
    "arxiv_id": "2305.13593",
    "title": "Neural Image Re-Exposure",
    "year": 2023,
    "published": "2023-05-23T01:55:37Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "The shutter strategy applied to the photo-shooting process has a significant influence on the quality of the captured photograph. An improper shutter may lead to a blurry image, video discontinuity, or rolling shutter artifact. Existing works try to provide an independent solution for each issue. In this work, we aim to re-expose the captured photo in post-processing to provide a more flexible way of addressing those issues within a unified framework. Specifically, we propose a neural network-ba",
    "arxiv_url": "https://arxiv.org/abs/2305.13593v1",
    "pdf_url": "https://arxiv.org/pdf/2305.13593v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2305.13593",
    "arxiv_authors": [
      "Xinyu Zhang",
      "Hefei Huang",
      "Xu Jia",
      "Dong Wang",
      "Huchuan Lu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Neural+Image+Re-Exposure+Xinyu+Zhang+Hefei+Huang+Xu+Jia+Dong+Wang+Huchuan+Lu",
    "gs_search_success": false,
    "gs_authors": [],
    "citation_count": null,
    "error_type": "fetch_failed",
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2303.08698",
    "title": "Bi-directional Distribution Alignment for Transductive Zero-Shot Learning",
    "year": 2023,
    "published": "2023-03-15T15:32:59Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "It is well-known that zero-shot learning (ZSL) can suffer severely from the problem of domain shift, where the true and learned data distributions for the unseen classes do not match. Although transductive ZSL (TZSL) attempts to improve this by allowing the use of unlabelled examples from the unseen classes, there is still a high level of distribution shift. We propose a novel TZSL model (named as Bi-VAEGAN), which largely improves the shift by a strengthened distribution alignment between the v",
    "arxiv_url": "https://arxiv.org/abs/2303.08698v2",
    "pdf_url": "https://arxiv.org/pdf/2303.08698v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2303.08698",
    "arxiv_authors": [
      "Zhicai Wang",
      "Yanbin Hao",
      "Tingting Mu",
      "Ouxiang Li",
      "Shuo Wang",
      "Xiangnan He"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Bi-directional+Distribution+Alignment+for+Transductive+Zero-Shot+Learning+Zhicai+Wang+Yanbin+Hao+Tingting+Mu+Ouxiang+Li+Shuo+Wang",
    "gs_search_success": true,
    "gs_authors": [
      "X45Go24AAAAJ",
      "21xyM8oAAAAJ",
      "vhPSOkEAAAAJ",
      "dOG10IUAAAAJ",
      "g2oUt1AAAAAJ",
      "qTE3BacAAAAJ"
    ],
    "citation_count": 36,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2308.01030",
    "title": "Three Factors to Improve Out-of-Distribution Detection",
    "year": 2023,
    "published": "2023-08-02T09:27:11Z",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "abstract": "In the problem of out-of-distribution (OOD) detection, the usage of auxiliary data as outlier data for fine-tuning has demonstrated encouraging performance. However, previous methods have suffered from a trade-off between classification accuracy (ACC) and OOD detection performance (AUROC, FPR, AUPR). To improve this trade-off, we make three contributions: (i) Incorporating a self-knowledge distillation loss can enhance the accuracy of the network; (ii) Sampling semi-hard outlier data for trainin",
    "arxiv_url": "https://arxiv.org/abs/2308.01030v1",
    "pdf_url": "https://arxiv.org/pdf/2308.01030v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2308.01030",
    "arxiv_authors": [
      "Hyunjun Choi",
      "JaeHo Chung",
      "Hawook Jeong",
      "Jin Young Choi"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Three+Factors+to+Improve+Out-of-Distribution+Detection+Hyunjun+Choi+JaeHo+Chung+Hawook+Jeong+Jin+Young+Choi",
    "gs_search_success": true,
    "gs_authors": [
      "PzfRs9sAAAAJ",
      "NoEVFWQAAAAJ",
      "rv1a9U4AAAAJ"
    ],
    "citation_count": 3,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2304.05015",
    "title": "Continual Semantic Segmentation with Automatic Memory Sample Selection",
    "year": 2023,
    "published": "2023-04-11T06:51:31Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Continual Semantic Segmentation (CSS) extends static semantic segmentation by incrementally introducing new classes for training. To alleviate the catastrophic forgetting issue in CSS, a memory buffer that stores a small number of samples from the previous classes is constructed for replay. However, existing methods select the memory samples either randomly or based on a single-factor-driven handcrafted strategy, which has no guarantee to be optimal. In this work, we propose a novel memory sampl",
    "arxiv_url": "https://arxiv.org/abs/2304.05015v1",
    "pdf_url": "https://arxiv.org/pdf/2304.05015v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2304.05015",
    "arxiv_authors": [
      "Lanyun Zhu",
      "Tianrun Chen",
      "Jianxiong Yin",
      "Simon See",
      "Jun Liu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Continual+Semantic+Segmentation+with+Automatic+Memory+Sample+Selection+Lanyun+Zhu+Tianrun+Chen+Jianxiong+Yin+Simon+See+Jun+Liu",
    "gs_search_success": true,
    "gs_authors": [
      "Q5Ild8UAAAAJ",
      "ebIHTEoAAAAJ",
      "ZuDfZR8AAAAJ",
      "urOSnlQAAAAJ"
    ],
    "citation_count": 82,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2506.01970",
    "title": "Johnny: Structuring Representation Space to Enhance Machine Abstract Reasoning Ability",
    "year": 2025,
    "published": "2025-05-13T18:57:47Z",
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "abstract": "This paper thoroughly investigates the challenges of enhancing AI's abstract reasoning capabilities, with a particular focus on Raven's Progressive Matrices (RPM) tasks involving complex human-like concepts. Firstly, it dissects the empirical reality that traditional end-to-end RPM-solving models heavily rely on option pool configurations, highlighting that this dependency constrains the model's reasoning capabilities. To address this limitation, the paper proposes the Johnny architecture - a no",
    "arxiv_url": "https://arxiv.org/abs/2506.01970v1",
    "pdf_url": "https://arxiv.org/pdf/2506.01970v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2506.01970",
    "arxiv_authors": [
      "Ruizhuo Song",
      "Beiming Yuan"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Johnny%3A+Structuring+Representation+Space+to+Enhance+Machine+Abstract+Reasoning+Ability+Ruizhuo+Song+Beiming+Yuan",
    "gs_search_success": true,
    "gs_authors": [],
    "citation_count": null,
    "gs_author_count": 2
  },
  {
    "arxiv_id": "2407.14982",
    "title": "GreenStableYolo: Optimizing Inference Time and Image Quality of Text-to-Image Generation",
    "year": 2024,
    "published": "2024-07-20T21:14:24Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Tuning the parameters and prompts for improving AI-based text-to-image generation has remained a substantial yet unaddressed challenge. Hence we introduce GreenStableYolo, which improves the parameters and prompts for Stable Diffusion to both reduce GPU inference time and increase image generation quality using NSGA-II and Yolo.   Our experiments show that despite a relatively slight trade-off (18%) in image quality compared to StableYolo (which only considers image quality), GreenStableYolo ach",
    "arxiv_url": "https://arxiv.org/abs/2407.14982v1",
    "pdf_url": "https://arxiv.org/pdf/2407.14982v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2407.14982",
    "arxiv_authors": [
      "Jingzhi Gong",
      "Sisi Li",
      "Giordano d'Aloisio",
      "Zishuo Ding",
      "Yulong Ye",
      "William B. Langdon",
      "Federica Sarro"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=GreenStableYolo%3A+Optimizing+Inference+Time+and+Image+Quality+of+Text-to-Image+Generation+Jingzhi+Gong+Sisi+Li+Giordano+d%27Aloisio+Zishuo+Ding+Yulong+Ye",
    "gs_search_success": true,
    "gs_authors": [
      "m63bkGEAAAAJ",
      "dRAn7W4AAAAJ",
      "O5cSyYMAAAAJ",
      "WVRSfB0AAAAJ",
      "fb_nsrkAAAAJ",
      "T9DXqhIAAAAJ",
      "nW9MDIQAAAAJ"
    ],
    "citation_count": 13,
    "gs_author_count": 7
  },
  {
    "arxiv_id": "2409.20083",
    "title": "SurgPETL: Parameter-Efficient Image-to-Surgical-Video Transfer Learning for Surgical Phase Recognition",
    "year": 2024,
    "published": "2024-09-30T08:33:50Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Capitalizing on image-level pre-trained models for various downstream tasks has recently emerged with promising performance. However, the paradigm of \"image pre-training followed by video fine-tuning\" for high-dimensional video data inevitably poses significant performance bottlenecks. Furthermore, in the medical domain, many surgical video tasks encounter additional challenges posed by the limited availability of video data and the necessity for comprehensive spatial-temporal modeling. Recently",
    "arxiv_url": "https://arxiv.org/abs/2409.20083v1",
    "pdf_url": "https://arxiv.org/pdf/2409.20083v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2409.20083",
    "arxiv_authors": [
      "Shu Yang",
      "Zhiyuan Cai",
      "Luyang Luo",
      "Ning Ma",
      "Shuchang Xu",
      "Hao Chen"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=SurgPETL%3A+Parameter-Efficient+Image-to-Surgical-Video+Transfer+Learning+for+Surgical+Phase+Recognition+Shu+Yang+Zhiyuan+Cai+Luyang+Luo+Ning+Ma+Shuchang+Xu",
    "gs_search_success": true,
    "gs_authors": [
      "J3BgJc4AAAAJ",
      "Z_t5DjwAAAAJ",
      "eD1O_vAAAAAJ",
      "6QlQF1oAAAAJ"
    ],
    "citation_count": 2,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2305.04275",
    "title": "RSC-VAE: Recoding Semantic Consistency Based VAE for One-Class Novelty Detection",
    "year": 2023,
    "published": "2023-05-07T13:36:54Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "In recent years, there is an increasing interests in reconstruction based generative models for image One-Class Novelty Detection, most of which only focus on image-level information. While in this paper, we further exploit the latent space of Variational Auto-encoder (VAE), a typical reconstruction based model, and we innovatively divide it into three regions: Normal/Anomalous/Unknown-semantic-region. Based on this hypothesis, we propose a new VAE architecture, Recoding Semantic Consistency Bas",
    "arxiv_url": "https://arxiv.org/abs/2305.04275v1",
    "pdf_url": "https://arxiv.org/pdf/2305.04275v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2305.04275",
    "arxiv_authors": [
      "Ge Zhang",
      "Wangzhe Du"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=RSC-VAE%3A+Recoding+Semantic+Consistency+Based+VAE+for+One-Class+Novelty+Detection+Ge+Zhang+Wangzhe+Du",
    "gs_search_success": true,
    "gs_authors": [
      "NkFC4gMAAAAJ",
      "AuwaI9YAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 2
  },
  {
    "arxiv_id": "2403.11032",
    "title": "FH-TabNet: Multi-Class Familial Hypercholesterolemia Detection via a Multi-Stage Tabular Deep Learning",
    "year": 2024,
    "published": "2024-03-16T22:35:21Z",
    "categories": [
      "cs.LG",
      "cs.CV",
      "eess.IV"
    ],
    "abstract": "Familial Hypercholesterolemia (FH) is a genetic disorder characterized by elevated levels of Low-Density Lipoprotein (LDL) cholesterol or its associated genes. Early-stage and accurate categorization of FH is of significance allowing for timely interventions to mitigate the risk of life-threatening conditions. Conventional diagnosis approach, however, is complex, costly, and a challenging interpretation task even for experienced clinicians resulting in high underdiagnosis rates. Although there h",
    "arxiv_url": "https://arxiv.org/abs/2403.11032v1",
    "pdf_url": "https://arxiv.org/pdf/2403.11032v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2403.11032",
    "arxiv_authors": [
      "Sadaf Khademi",
      "Zohreh Hajiakhondi",
      "Golnaz Vaseghi",
      "Nizal Sarrafzadegan",
      "Arash Mohammadi"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=FH-TabNet%3A+Multi-Class+Familial+Hypercholesterolemia+Detection+via+a+Multi-Stage+Tabular+Deep+Learning+Sadaf+Khademi+Zohreh+Hajiakhondi+Golnaz+Vaseghi+Nizal+Sarrafzadegan+Arash+Mohammadi",
    "gs_search_success": true,
    "gs_authors": [
      "bvU1CuEAAAAJ",
      "GNWl92YAAAAJ",
      "tCYlO6oAAAAJ"
    ],
    "citation_count": 2,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2309.16656",
    "title": "Visual In-Context Learning for Few-Shot Eczema Segmentation",
    "year": 2023,
    "published": "2023-09-28T17:55:24Z",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "Automated diagnosis of eczema from digital camera images is crucial for developing applications that allow patients to self-monitor their recovery. An important component of this is the segmentation of eczema region from such images. Current methods for eczema segmentation rely on deep neural networks such as convolutional (CNN)-based U-Net or transformer-based Swin U-Net. While effective, these methods require high volume of annotated data, which can be difficult to obtain. Here, we investigate",
    "arxiv_url": "https://arxiv.org/abs/2309.16656v1",
    "pdf_url": "https://arxiv.org/pdf/2309.16656v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2309.16656",
    "arxiv_authors": [
      "Neelesh Kumar",
      "Oya Aran",
      "Venugopal Vasudevan"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Visual+In-Context+Learning+for+Few-Shot+Eczema+Segmentation+Neelesh+Kumar+Oya+Aran+Venugopal+Vasudevan",
    "gs_search_success": true,
    "gs_authors": [
      "tnv8_FQAAAAJ",
      "dGXwrtEAAAAJ"
    ],
    "citation_count": 6,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2403.05796",
    "title": "Weakly Supervised Change Detection via Knowledge Distillation and Multiscale Sigmoid Inference",
    "year": 2024,
    "published": "2024-03-09T05:01:51Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Change detection, which aims to detect spatial changes from a pair of multi-temporal images due to natural or man-made causes, has been widely applied in remote sensing, disaster management, urban management, etc. Most existing change detection approaches, however, are fully supervised and require labor-intensive pixel-level labels. To address this, we develop a novel weakly supervised change detection technique via Knowledge Distillation and Multiscale Sigmoid Inference (KD-MSI) that leverages ",
    "arxiv_url": "https://arxiv.org/abs/2403.05796v1",
    "pdf_url": "https://arxiv.org/pdf/2403.05796v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2403.05796",
    "arxiv_authors": [
      "Binghao Lu",
      "Caiwen Ding",
      "Jinbo Bi",
      "Dongjin Song"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Weakly+Supervised+Change+Detection+via+Knowledge+Distillation+and+Multiscale+Sigmoid+Inference+Binghao+Lu+Caiwen+Ding+Jinbo+Bi+Dongjin+Song",
    "gs_search_success": true,
    "gs_authors": [
      "7hR0r_EAAAAJ",
      "BJdHw6AAAAAJ",
      "w4dK5UIAAAAJ"
    ],
    "citation_count": 7,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2411.06810",
    "title": "JPEG AI Image Compression Visual Artifacts: Detection Methods and Dataset",
    "year": 2024,
    "published": "2024-11-11T09:11:01Z",
    "categories": [
      "cs.AI",
      "cs.CV",
      "cs.MM"
    ],
    "abstract": "Learning-based image compression methods have improved in recent years and started to outperform traditional codecs. However, neural-network approaches can unexpectedly introduce visual artifacts in some images. We therefore propose methods to separately detect three types of artifacts (texture and boundary degradation, color change, and text corruption), to localize the affected regions, and to quantify the artifact strength. We consider only those regions that exhibit distortion due solely to ",
    "arxiv_url": "https://arxiv.org/abs/2411.06810v1",
    "pdf_url": "https://arxiv.org/pdf/2411.06810v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2411.06810",
    "arxiv_authors": [
      "Daria Tsereh",
      "Mark Mirgaleev",
      "Ivan Molodetskikh",
      "Roman Kazantsev",
      "Dmitriy Vatolin"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=JPEG+AI+Image+Compression+Visual+Artifacts%3A+Detection+Methods+and+Dataset+Daria+Tsereh+Mark+Mirgaleev+Ivan+Molodetskikh+Roman+Kazantsev+Dmitriy+Vatolin",
    "gs_search_success": true,
    "gs_authors": [
      "545J9E4AAAAJ",
      "T4pm3JUAAAAJ",
      "OPUQL90AAAAJ"
    ],
    "citation_count": 2,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2304.01999",
    "title": "Revisiting the Evaluation of Image Synthesis with GANs",
    "year": 2023,
    "published": "2023-04-04T17:54:32Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "A good metric, which promises a reliable comparison between solutions, is essential for any well-defined task. Unlike most vision tasks that have per-sample ground-truth, image synthesis tasks target generating unseen data and hence are usually evaluated through a distributional distance between one set of real samples and another set of generated samples. This study presents an empirical investigation into the evaluation of synthesis performance, with generative adversarial networks (GANs) as a",
    "arxiv_url": "https://arxiv.org/abs/2304.01999v2",
    "pdf_url": "https://arxiv.org/pdf/2304.01999v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2304.01999",
    "arxiv_authors": [
      "Mengping Yang",
      "Ceyuan Yang",
      "Yichi Zhang",
      "Qingyan Bai",
      "Yujun Shen",
      "Bo Dai"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Revisiting+the+Evaluation+of+Image+Synthesis+with+GANs+Mengping+Yang+Ceyuan+Yang+Yichi+Zhang+Qingyan+Bai+Yujun+Shen",
    "gs_search_success": true,
    "gs_authors": [
      "Rfj4jWoAAAAJ",
      "HOCyXzsAAAAJ",
      "xUMjxi4AAAAJ",
      "KNWTvgEAAAAJ",
      "u76xfogAAAAJ"
    ],
    "citation_count": 9,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2504.10143",
    "title": "On the Value of Cross-Modal Misalignment in Multimodal Representation Learning",
    "year": 2025,
    "published": "2025-04-14T11:54:19Z",
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "abstract": "Multimodal representation learning, exemplified by multimodal contrastive learning (MMCL) using image-text pairs, aims to learn powerful representations by aligning cues across modalities. This approach relies on the core assumption that the exemplar image-text pairs constitute two representations of an identical concept. However, recent research has revealed that real-world datasets often exhibit cross-modal misalignment. There are two distinct viewpoints on how to address this issue: one sugge",
    "arxiv_url": "https://arxiv.org/abs/2504.10143v7",
    "pdf_url": "https://arxiv.org/pdf/2504.10143v7",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2504.10143",
    "arxiv_authors": [
      "Yichao Cai",
      "Yuhang Liu",
      "Erdun Gao",
      "Tianjiao Jiang",
      "Zhen Zhang",
      "Anton van den Hengel",
      "Javen Qinfeng Shi"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=On+the+Value+of+Cross-Modal+Misalignment+in+Multimodal+Representation+Learning+Yichao+Cai+Yuhang+Liu+Erdun+Gao+Tianjiao+Jiang+Zhen+Zhang",
    "gs_search_success": true,
    "gs_authors": [
      "SilAzvwAAAAJ",
      "5xZspvQAAAAJ",
      "nMGZ2ZQAAAAJ",
      "h6O9vYkAAAAJ",
      "4X6Hqg0AAAAJ",
      "nNp0nL4AAAAJ"
    ],
    "citation_count": 2,
    "gs_author_count": 7
  },
  {
    "arxiv_id": "2407.14478",
    "title": "A review on vision-based motion estimation",
    "year": 2024,
    "published": "2024-07-19T17:28:49Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Compared to contact sensors-based motion measurement, vision-based motion measurement has advantages of low cost and high efficiency and have been under active development in the past decades. This paper provides a review on existing motion measurement methods. In addition to the development of each branch of vision-based motion measurement methods, this paper also discussed the advantages and disadvantages of existing methods. Based on this discussion, it was identified that existing methods ha",
    "arxiv_url": "https://arxiv.org/abs/2407.14478v1",
    "pdf_url": "https://arxiv.org/pdf/2407.14478v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2407.14478",
    "arxiv_authors": [
      "Hongyi Liu",
      "Haifeng Wang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=A+review+on+vision-based+motion+estimation+Hongyi+Liu+Haifeng+Wang",
    "gs_search_success": true,
    "gs_authors": [
      "RCFLYL0AAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 2
  },
  {
    "arxiv_id": "2311.00668",
    "title": "ProcSim: Proxy-based Confidence for Robust Similarity Learning",
    "year": 2023,
    "published": "2023-11-01T17:17:14Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Deep Metric Learning (DML) methods aim at learning an embedding space in which distances are closely related to the inherent semantic similarity of the inputs. Previous studies have shown that popular benchmark datasets often contain numerous wrong labels, and DML methods are susceptible to them. Intending to study the effect of realistic noise, we create an ontology of the classes in a dataset and use it to simulate semantically coherent labeling mistakes. To train robust DML models, we propose",
    "arxiv_url": "https://arxiv.org/abs/2311.00668v1",
    "pdf_url": "https://arxiv.org/pdf/2311.00668v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2311.00668",
    "arxiv_authors": [
      "Oriol Barbany",
      "Xiaofan Lin",
      "Muhammet Bastan",
      "Arnab Dhua"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=ProcSim%3A+Proxy-based+Confidence+for+Robust+Similarity+Learning+Oriol+Barbany+Xiaofan+Lin+Muhammet+Bastan+Arnab+Dhua",
    "gs_search_success": true,
    "gs_authors": [
      "emYye28AAAAJ",
      "REEqrZEAAAAJ",
      "5-ZdbeAAAAAJ",
      "5oa_2lgAAAAJ"
    ],
    "citation_count": 4,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2301.05180",
    "title": "Effective Decision Boundary Learning for Class Incremental Learning",
    "year": 2023,
    "published": "2023-01-12T18:04:51Z",
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "abstract": "Rehearsal approaches in class incremental learning (CIL) suffer from decision boundary overfitting to new classes, which is mainly caused by two factors: insufficiency of old classes data for knowledge distillation and imbalanced data learning between the learned and new classes because of the limited storage memory. In this work, we present a simple but effective approach to tackle these two factors. First, we employ a re-sampling strategy and Mixup K}nowledge D}istillation (Re-MKD) to improve ",
    "arxiv_url": "https://arxiv.org/abs/2301.05180v4",
    "pdf_url": "https://arxiv.org/pdf/2301.05180v4",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2301.05180",
    "arxiv_authors": [
      "Kunchi Li",
      "Jun Wan",
      "Shan Yu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Effective+Decision+Boundary+Learning+for+Class+Incremental+Learning+Kunchi+Li+Jun+Wan+Shan+Yu",
    "gs_search_success": true,
    "gs_authors": [
      "oI6AIkMAAAAJ",
      "cuJ3QG8AAAAJ",
      "YdaRHiIAAAAJ",
      "bSbc7FQAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2505.06191",
    "title": "Neuro-Symbolic Concepts",
    "year": 2025,
    "published": "2025-05-09T17:02:51Z",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.CV",
      "cs.LG",
      "cs.RO"
    ],
    "abstract": "This article presents a concept-centric paradigm for building agents that can learn continually and reason flexibly. The concept-centric agent utilizes a vocabulary of neuro-symbolic concepts. These concepts, such as object, relation, and action concepts, are grounded on sensory inputs and actuation outputs. They are also compositional, allowing for the creation of novel concepts through their structural combination. To facilitate learning and reasoning, the concepts are typed and represented us",
    "arxiv_url": "https://arxiv.org/abs/2505.06191v1",
    "pdf_url": "https://arxiv.org/pdf/2505.06191v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2505.06191",
    "arxiv_authors": [
      "Jiayuan Mao",
      "Joshua B. Tenenbaum",
      "Jiajun Wu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Neuro-Symbolic+Concepts+Jiayuan+Mao+Joshua+B.+Tenenbaum+Jiajun+Wu",
    "gs_search_success": true,
    "gs_authors": [
      "-xaOIZIAAAAJ",
      "2efgcS0AAAAJ",
      "rRJ9wTJMUB8C"
    ],
    "citation_count": 5,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2411.18335",
    "title": "Helvipad: A Real-World Dataset for Omnidirectional Stereo Depth Estimation",
    "year": 2024,
    "published": "2024-11-27T13:34:41Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.RO"
    ],
    "abstract": "Despite progress in stereo depth estimation, omnidirectional imaging remains underexplored, mainly due to the lack of appropriate data. We introduce Helvipad, a real-world dataset for omnidirectional stereo depth estimation, featuring 40K video frames from video sequences across diverse environments, including crowded indoor and outdoor scenes with various lighting conditions. Collected using two 360° cameras in a top-bottom setup and a LiDAR sensor, the dataset includes accurate depth and dispa",
    "arxiv_url": "https://arxiv.org/abs/2411.18335v2",
    "pdf_url": "https://arxiv.org/pdf/2411.18335v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2411.18335",
    "arxiv_authors": [
      "Mehdi Zayene",
      "Jannik Endres",
      "Albias Havolli",
      "Charles Corbière",
      "Salim Cherkaoui",
      "Alexandre Kontouli",
      "Alexandre Alahi"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Helvipad%3A+A+Real-World+Dataset+for+Omnidirectional+Stereo+Depth+Estimation+Mehdi+Zayene+Jannik+Endres+Albias+Havolli+Charles+Corbi%C3%A8re+Salim+Cherkaoui",
    "gs_search_success": true,
    "gs_authors": [
      "imQyM0QAAAAJ",
      "UIhXQ64AAAAJ",
      "UcnFUZ8AAAAJ",
      "DyTqVmMAAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 7
  },
  {
    "arxiv_id": "2309.12311",
    "title": "LLM-Grounder: Open-Vocabulary 3D Visual Grounding with Large Language Model as an Agent",
    "year": 2023,
    "published": "2023-09-21T17:59:45Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "cs.RO"
    ],
    "abstract": "3D visual grounding is a critical skill for household robots, enabling them to navigate, manipulate objects, and answer questions based on their environment. While existing approaches often rely on extensive labeled data or exhibit limitations in handling complex language queries, we propose LLM-Grounder, a novel zero-shot, open-vocabulary, Large Language Model (LLM)-based 3D visual grounding pipeline. LLM-Grounder utilizes an LLM to decompose complex natural language queries into semantic const",
    "arxiv_url": "https://arxiv.org/abs/2309.12311v1",
    "pdf_url": "https://arxiv.org/pdf/2309.12311v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2309.12311",
    "arxiv_authors": [
      "Jianing Yang",
      "Xuweiyi Chen",
      "Shengyi Qian",
      "Nikhil Madaan",
      "Madhavan Iyengar",
      "David F. Fouhey",
      "Joyce Chai"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=LLM-Grounder%3A+Open-Vocabulary+3D+Visual+Grounding+with+Large+Language+Model+as+an+Agent+Jianing+Yang+Xuweiyi+Chen+Shengyi+Qian+Nikhil+Madaan+Madhavan+Iyengar",
    "gs_search_success": true,
    "gs_authors": [
      "U4NhlRYAAAAJ",
      "-b3v9LsAAAAJ",
      "BouqoPwAAAAJ",
      "FLcpd34AAAAJ",
      "QgoY8GEAAAAJ",
      "PwKfQq0AAAAJ"
    ],
    "citation_count": 143,
    "gs_author_count": 7
  },
  {
    "arxiv_id": "2312.15895",
    "title": "Semantic-aware SAM for Point-Prompted Instance Segmentation",
    "year": 2023,
    "published": "2023-12-26T05:56:44Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Single-point annotation in visual tasks, with the goal of minimizing labelling costs, is becoming increasingly prominent in research. Recently, visual foundation models, such as Segment Anything (SAM), have gained widespread usage due to their robust zero-shot capabilities and exceptional annotation performance. However, SAM's class-agnostic output and high confidence in local segmentation introduce 'semantic ambiguity', posing a challenge for precise category-specific segmentation. In this pape",
    "arxiv_url": "https://arxiv.org/abs/2312.15895v2",
    "pdf_url": "https://arxiv.org/pdf/2312.15895v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2312.15895",
    "arxiv_authors": [
      "Zhaoyang Wei",
      "Pengfei Chen",
      "Xuehui Yu",
      "Guorong Li",
      "Jianbin Jiao",
      "Zhenjun Han"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Semantic-aware+SAM+for+Point-Prompted+Instance+Segmentation+Zhaoyang+Wei+Pengfei+Chen+Xuehui+Yu+Guorong+Li+Jianbin+Jiao",
    "gs_search_success": true,
    "gs_authors": [
      "XqdpqNcAAAAJ",
      "CkgaTVAAAAAJ",
      "0rK4yTcAAAAJ",
      "AiuGlVQAAAAJ",
      "WYrxoBEAAAAJ"
    ],
    "citation_count": 28,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2502.18101",
    "title": "Detecting Offensive Memes with Social Biases in Singapore Context Using Multimodal Large Language Models",
    "year": 2025,
    "published": "2025-02-25T11:15:49Z",
    "categories": [
      "cs.CV",
      "cs.CL"
    ],
    "abstract": "Traditional online content moderation systems struggle to classify modern multimodal means of communication, such as memes, a highly nuanced and information-dense medium. This task is especially hard in a culturally diverse society like Singapore, where low-resource languages are used and extensive knowledge on local context is needed to interpret online content. We curate a large collection of 112K memes labeled by GPT-4V for fine-tuning a VLM to classify offensive memes in Singapore context. W",
    "arxiv_url": "https://arxiv.org/abs/2502.18101v2",
    "pdf_url": "https://arxiv.org/pdf/2502.18101v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2502.18101",
    "arxiv_authors": [
      "Cao Yuxuan",
      "Wu Jiayang",
      "Alistair Cheong Liang Chuen",
      "Bryan Shan Guanrong",
      "Theodore Lee Chong Jen",
      "Sherman Chann Zhi Shen"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Detecting+Offensive+Memes+with+Social+Biases+in+Singapore+Context+Using+Multimodal+Large+Language+Models+Cao+Yuxuan+Wu+Jiayang+Alistair+Cheong+Liang+Chuen+Bryan+Shan+Guanrong+Theodore+Lee+Chong+Jen",
    "gs_search_success": true,
    "gs_authors": [
      "Ko7vtgwAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2408.01579",
    "title": "THOR2: Topological Analysis for 3D Shape and Color-Based Human-Inspired Object Recognition in Unseen Environments",
    "year": 2024,
    "published": "2024-08-02T21:24:14Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Visual object recognition in unseen and cluttered indoor environments is a challenging problem for mobile robots. This study presents a 3D shape and color-based descriptor, TOPS2, for point clouds generated from RGB-D images and an accompanying recognition framework, THOR2. The TOPS2 descriptor embodies object unity, a human cognition mechanism, by retaining the slicing-based topological representation of 3D shape from the TOPS descriptor while capturing object color information through slicing-",
    "arxiv_url": "https://arxiv.org/abs/2408.01579v2",
    "pdf_url": "https://arxiv.org/pdf/2408.01579v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2408.01579",
    "arxiv_authors": [
      "Ekta U. Samani",
      "Ashis G. Banerjee"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=THOR2%3A+Topological+Analysis+for+3D+Shape+and+Color-Based+Human-Inspired+Object+Recognition+in+Unseen+Environments+Ekta+U.+Samani+Ashis+G.+Banerjee",
    "gs_search_success": true,
    "gs_authors": [
      "_hfwT30AAAAJ",
      "ruNwEFYAAAAJ"
    ],
    "citation_count": 2,
    "gs_author_count": 2
  },
  {
    "arxiv_id": "2410.12692",
    "title": "Machine learning approach to brain tumor detection and classification",
    "year": 2024,
    "published": "2024-10-16T15:52:32Z",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "Brain tumor detection and classification are critical tasks in medical image analysis, particularly in early-stage diagnosis, where accurate and timely detection can significantly improve treatment outcomes. In this study, we apply various statistical and machine learning models to detect and classify brain tumors using brain MRI images. We explore a variety of statistical models including linear, logistic, and Bayesian regressions, and the machine learning models including decision tree, random",
    "arxiv_url": "https://arxiv.org/abs/2410.12692v2",
    "pdf_url": "https://arxiv.org/pdf/2410.12692v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2410.12692",
    "arxiv_authors": [
      "Alice Oh",
      "Inyoung Noh",
      "Jian Choo",
      "Jihoo Lee",
      "Justin Park",
      "Kate Hwang",
      "Sanghyeon Kim",
      "Soo Min Oh"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Machine+learning+approach+to+brain+tumor+detection+and+classification+Alice+Oh+Inyoung+Noh+Jian+Choo+Jihoo+Lee+Justin+Park",
    "gs_search_success": true,
    "gs_authors": [
      "61GsJl8AAAAJ"
    ],
    "citation_count": 8,
    "gs_author_count": 8
  },
  {
    "arxiv_id": "2304.13375",
    "title": "Streamlined Global and Local Features Combinator (SGLC) for High Resolution Image Dehazing",
    "year": 2023,
    "published": "2023-04-26T08:34:00Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Image Dehazing aims to remove atmospheric fog or haze from an image. Although the Dehazing models have evolved a lot in recent years, few have precisely tackled the problem of High-Resolution hazy images. For this kind of image, the model needs to work on a downscaled version of the image or on cropped patches from it. In both cases, the accuracy will drop. This is primarily due to the inherent failure to combine global and local features when the image size increases. The Dehazing model require",
    "arxiv_url": "https://arxiv.org/abs/2304.13375v1",
    "pdf_url": "https://arxiv.org/pdf/2304.13375v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2304.13375",
    "arxiv_authors": [
      "Bilel Benjdira",
      "Anas M. Ali",
      "Anis Koubaa"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Streamlined+Global+and+Local+Features+Combinator+%28SGLC%29+for+High+Resolution+Image+Dehazing+Bilel+Benjdira+Anas+M.+Ali+Anis+Koubaa",
    "gs_search_success": true,
    "gs_authors": [
      "yOJoZ84AAAAJ",
      "aEoY-0IAAAAJ",
      "eBVRL_gAAAAJ"
    ],
    "citation_count": 17,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2311.00436",
    "title": "Enhancing Traffic Object Detection in Variable Illumination with RGB-Event Fusion",
    "year": 2023,
    "published": "2023-11-01T10:59:57Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Traffic object detection under variable illumination is challenging due to the information loss caused by the limited dynamic range of conventional frame-based cameras. To address this issue, we introduce bio-inspired event cameras and propose a novel Structure-aware Fusion Network (SFNet) that extracts sharp and complete object structures from the event stream to compensate for the lost information in images through cross-modality fusion, enabling the network to obtain illumination-robust repre",
    "arxiv_url": "https://arxiv.org/abs/2311.00436v2",
    "pdf_url": "https://arxiv.org/pdf/2311.00436v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2311.00436",
    "arxiv_authors": [
      "Zhanwen Liu",
      "Nan Yang",
      "Yang Wang",
      "Yuke Li",
      "Xiangmo Zhao",
      "Fei-Yue Wang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Enhancing+Traffic+Object+Detection+in+Variable+Illumination+with+RGB-Event+Fusion+Zhanwen+Liu+Nan+Yang+Yang+Wang+Yuke+Li+Xiangmo+Zhao",
    "gs_search_success": true,
    "gs_authors": [
      "3TTXGAoAAAAJ",
      "7TFKCpQAAAAJ",
      "jwy6J38AAAAJ"
    ],
    "citation_count": 28,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2306.09483",
    "title": "R2-Diff: Denoising by diffusion as a refinement of retrieved motion for image-based motion prediction",
    "year": 2023,
    "published": "2023-06-15T20:27:06Z",
    "categories": [
      "cs.CV",
      "cs.LG",
      "cs.RO"
    ],
    "abstract": "Image-based motion prediction is one of the essential techniques for robot manipulation. Among the various prediction models, we focus on diffusion models because they have achieved state-of-the-art performance in various applications. In image-based motion prediction, diffusion models stochastically predict contextually appropriate motion by gradually denoising random Gaussian noise based on the image context. While diffusion models are able to predict various motions by changing the random noi",
    "arxiv_url": "https://arxiv.org/abs/2306.09483v1",
    "pdf_url": "https://arxiv.org/pdf/2306.09483v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2306.09483",
    "arxiv_authors": [
      "Takeru Oba",
      "Norimichi Ukita"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=R2-Diff%3A+Denoising+by+diffusion+as+a+refinement+of+retrieved+motion+for+image-based+motion+prediction+Takeru+Oba+Norimichi+Ukita",
    "gs_search_success": true,
    "gs_authors": [
      "Tgbsbs8AAAAJ",
      "Dwb-5UgAAAAJ"
    ],
    "citation_count": 3,
    "gs_author_count": 1
  },
  {
    "arxiv_id": "2403.18554",
    "title": "CosalPure: Learning Concept from Group Images for Robust Co-Saliency Detection",
    "year": 2024,
    "published": "2024-03-27T13:33:14Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Co-salient object detection (CoSOD) aims to identify the common and salient (usually in the foreground) regions across a given group of images. Although achieving significant progress, state-of-the-art CoSODs could be easily affected by some adversarial perturbations, leading to substantial accuracy reduction. The adversarial perturbations can mislead CoSODs but do not change the high-level semantic information (e.g., concept) of the co-salient objects. In this paper, we propose a novel robustne",
    "arxiv_url": "https://arxiv.org/abs/2403.18554v2",
    "pdf_url": "https://arxiv.org/pdf/2403.18554v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2403.18554",
    "arxiv_authors": [
      "Jiayi Zhu",
      "Qing Guo",
      "Felix Juefei-Xu",
      "Yihao Huang",
      "Yang Liu",
      "Geguang Pu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=CosalPure%3A+Learning+Concept+from+Group+Images+for+Robust+Co-Saliency+Detection+Jiayi+Zhu+Qing+Guo+Felix+Juefei-Xu+Yihao+Huang+Yang+Liu",
    "gs_search_success": true,
    "gs_authors": [
      "niQAGcQAAAAJ",
      "dgN8vtwAAAAJ",
      "_Pvgwd0AAAAJ",
      "jHkc3DsAAAAJ",
      "Rj2x4QUAAAAJ"
    ],
    "citation_count": 7,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2407.04180",
    "title": "Slice-100K: A Multimodal Dataset for Extrusion-based 3D Printing",
    "year": 2024,
    "published": "2024-07-04T22:52:25Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "G-code (Geometric code) or RS-274 is the most widely used computer numerical control (CNC) and 3D printing programming language. G-code provides machine instructions for the movement of the 3D printer, especially for the nozzle, stage, and extrusion of material for extrusion-based additive manufacturing. Currently, there does not exist a large repository of curated CAD models along with their corresponding G-code files for additive manufacturing. To address this issue, we present Slice-100K, a f",
    "arxiv_url": "https://arxiv.org/abs/2407.04180v3",
    "pdf_url": "https://arxiv.org/pdf/2407.04180v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2407.04180",
    "arxiv_authors": [
      "Anushrut Jignasu",
      "Kelly O. Marshall",
      "Ankush Kumar Mishra",
      "Lucas Nerone Rillo",
      "Baskar Ganapathysubramanian",
      "Aditya Balu",
      "Chinmay Hegde",
      "Adarsh Krishnamurthy"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Slice-100K%3A+A+Multimodal+Dataset+for+Extrusion-based+3D+Printing+Anushrut+Jignasu+Kelly+O.+Marshall+Ankush+Kumar+Mishra+Lucas+Nerone+Rillo+Baskar+Ganapathysubramanian",
    "gs_search_success": true,
    "gs_authors": [
      "GNuXi6oAAAAJ",
      "R1JIs4cAAAAJ",
      "LBAeMfQAAAAJ",
      "GudYxOIAAAAJ",
      "Gb_YM5oAAAAJ",
      "ZZAhatoAAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2402.02491",
    "title": "VM-UNet: Vision Mamba UNet for Medical Image Segmentation",
    "year": 2024,
    "published": "2024-02-04T13:37:21Z",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "abstract": "In the realm of medical image segmentation, both CNN-based and Transformer-based models have been extensively explored. However, CNNs exhibit limitations in long-range modeling capabilities, whereas Transformers are hampered by their quadratic computational complexity. Recently, State Space Models (SSMs), exemplified by Mamba, have emerged as a promising approach. They not only excel in modeling long-range interactions but also maintain a linear computational complexity. In this paper, leveragin",
    "arxiv_url": "https://arxiv.org/abs/2402.02491v2",
    "pdf_url": "https://arxiv.org/pdf/2402.02491v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2402.02491",
    "arxiv_authors": [
      "Jiacheng Ruan",
      "Jincheng Li",
      "Suncheng Xiang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=VM-UNet%3A+Vision+Mamba+UNet+for+Medical+Image+Segmentation+Jiacheng+Ruan+Jincheng+Li+Suncheng+Xiang",
    "gs_search_success": true,
    "gs_authors": [
      "IyrSeG8AAAAJ",
      "QDlaRJkAAAAJ"
    ],
    "citation_count": 795,
    "gs_author_count": 2
  },
  {
    "arxiv_id": "2412.05605",
    "title": "RefSAM3D: Adapting SAM with Cross-modal Reference for 3D Medical Image Segmentation",
    "year": 2024,
    "published": "2024-12-07T10:22:46Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "The Segment Anything Model (SAM), originally built on a 2D Vision Transformer (ViT), excels at capturing global patterns in 2D natural images but struggles with 3D medical imaging modalities like CT and MRI. These modalities require capturing spatial information in volumetric space for tasks such as organ segmentation and tumor quantification. To address this challenge, we introduce RefSAM3D, which adapts SAM for 3D medical imaging by incorporating a 3D image adapter and cross-modal reference pr",
    "arxiv_url": "https://arxiv.org/abs/2412.05605v1",
    "pdf_url": "https://arxiv.org/pdf/2412.05605v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2412.05605",
    "arxiv_authors": [
      "Xiang Gao",
      "Kai Lu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=RefSAM3D%3A+Adapting+SAM+with+Cross-modal+Reference+for+3D+Medical+Image+Segmentation+Xiang+Gao+Kai+Lu",
    "gs_search_success": true,
    "gs_authors": [],
    "citation_count": null,
    "gs_author_count": 2
  },
  {
    "arxiv_id": "2406.09201",
    "title": "Enhanced Object Detection: A Study on Vast Vocabulary Object Detection Track for V3Det Challenge 2024",
    "year": 2024,
    "published": "2024-06-13T14:59:45Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "In this technical report, we present our findings from the research conducted on the Vast Vocabulary Visual Detection (V3Det) dataset for Supervised Vast Vocabulary Visual Detection task. How to deal with complex categories and detection boxes has become a difficulty in this track. The original supervised detector is not suitable for this task. We have designed a series of improvements, including adjustments to the network structure, changes to the loss function, and design of training strategie",
    "arxiv_url": "https://arxiv.org/abs/2406.09201v3",
    "pdf_url": "https://arxiv.org/pdf/2406.09201v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2406.09201",
    "arxiv_authors": [
      "Peixi Wu",
      "Bosong Chai",
      "Xuan Nie",
      "Longquan Yan",
      "Zeyu Wang",
      "Qifan Zhou",
      "Boning Wang",
      "Yansong Peng",
      "Hebei Li"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Enhanced+Object+Detection%3A+A+Study+on+Vast+Vocabulary+Object+Detection+Track+for+V3Det+Challenge+2024+Peixi+Wu+Bosong+Chai+Xuan+Nie+Longquan+Yan+Zeyu+Wang",
    "gs_search_success": true,
    "gs_authors": [
      "7yQgeZ90MJUC",
      "rgeEZmsAAAAJ",
      "zb1Dgs8AAAAJ"
    ],
    "citation_count": 3,
    "gs_author_count": 9
  },
  {
    "arxiv_id": "2303.12965",
    "title": "Efficient Meshy Neural Fields for Animatable Human Avatars",
    "year": 2023,
    "published": "2023-03-23T00:15:34Z",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "abstract": "Efficiently digitizing high-fidelity animatable human avatars from videos is a challenging and active research topic. Recent volume rendering-based neural representations open a new way for human digitization with their friendly usability and photo-realistic reconstruction quality. However, they are inefficient for long optimization times and slow inference speed; their implicit nature results in entangled geometry, materials, and dynamics of humans, which are hard to edit afterward. Such drawba",
    "arxiv_url": "https://arxiv.org/abs/2303.12965v1",
    "pdf_url": "https://arxiv.org/pdf/2303.12965v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2303.12965",
    "arxiv_authors": [
      "Xiaoke Huang",
      "Yiji Cheng",
      "Yansong Tang",
      "Xiu Li",
      "Jie Zhou",
      "Jiwen Lu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Efficient+Meshy+Neural+Fields+for+Animatable+Human+Avatars+Xiaoke+Huang+Yiji+Cheng+Yansong+Tang+Xiu+Li+Jie+Zhou",
    "gs_search_success": true,
    "gs_authors": [
      "BD9AT04AAAAJ",
      "TIbistUAAAAJ",
      "Plo8ZSYAAAAJ",
      "Xrh1OIUAAAAJ"
    ],
    "citation_count": 2,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2403.18281",
    "title": "AIR-HLoc: Adaptive Retrieved Images Selection for Efficient Visual Localisation",
    "year": 2024,
    "published": "2024-03-27T06:17:21Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "State-of-the-art hierarchical localisation pipelines (HLoc) employ image retrieval (IR) to establish 2D-3D correspondences by selecting the top-$k$ most similar images from a reference database. While increasing $k$ improves localisation robustness, it also linearly increases computational cost and runtime, creating a significant bottleneck. This paper investigates the relationship between global and local descriptors, showing that greater similarity between the global descriptors of query and d",
    "arxiv_url": "https://arxiv.org/abs/2403.18281v3",
    "pdf_url": "https://arxiv.org/pdf/2403.18281v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2403.18281",
    "arxiv_authors": [
      "Changkun Liu",
      "Jianhao Jiao",
      "Huajian Huang",
      "Zhengyang Ma",
      "Dimitrios Kanoulas",
      "Tristan Braud"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=AIR-HLoc%3A+Adaptive+Retrieved+Images+Selection+for+Efficient+Visual+Localisation+Changkun+Liu+Jianhao+Jiao+Huajian+Huang+Zhengyang+Ma+Dimitrios+Kanoulas",
    "gs_search_success": true,
    "gs_authors": [
      "gLL_wO4AAAAJ",
      "psqleSQAAAAJ",
      "ZOZtoQUAAAAJ",
      "25npC_YAAAAJ",
      "cE8_5EsAAAAJ",
      "rOhG9NoAAAAJ"
    ],
    "citation_count": 2,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2303.14384",
    "title": "Reliability-Hierarchical Memory Network for Scribble-Supervised Video Object Segmentation",
    "year": 2023,
    "published": "2023-03-25T07:21:40Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "This paper aims to solve the video object segmentation (VOS) task in a scribble-supervised manner, in which VOS models are not only trained by the sparse scribble annotations but also initialized with the sparse target scribbles for inference. Thus, the annotation burdens for both training and initialization can be substantially lightened. The difficulties of scribble-supervised VOS lie in two aspects. On the one hand, it requires the powerful ability to learn from the sparse scribble annotation",
    "arxiv_url": "https://arxiv.org/abs/2303.14384v1",
    "pdf_url": "https://arxiv.org/pdf/2303.14384v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2303.14384",
    "arxiv_authors": [
      "Zikun Zhou",
      "Kaige Mao",
      "Wenjie Pei",
      "Hongpeng Wang",
      "Yaowei Wang",
      "Zhenyu He"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Reliability-Hierarchical+Memory+Network+for+Scribble-Supervised+Video+Object+Segmentation+Zikun+Zhou+Kaige+Mao+Wenjie+Pei+Hongpeng+Wang+Yaowei+Wang",
    "gs_search_success": true,
    "gs_authors": [
      "fTPfQugAAAAJ",
      "wX3avNkAAAAJ",
      "4A8SXMEAAAAJ",
      "7_VvbbAAAAAJ",
      "o_DllmIAAAAJ"
    ],
    "citation_count": 2,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2401.01107",
    "title": "CityPulse: Fine-Grained Assessment of Urban Change with Street View Time Series",
    "year": 2024,
    "published": "2024-01-02T08:57:09Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Urban transformations have profound societal impact on both individuals and communities at large. Accurately assessing these shifts is essential for understanding their underlying causes and ensuring sustainable urban planning. Traditional measurements often encounter constraints in spatial and temporal granularity, failing to capture real-time physical changes. While street view imagery, capturing the heartbeat of urban spaces from a pedestrian point of view, can add as a high-definition, up-to",
    "arxiv_url": "https://arxiv.org/abs/2401.01107v2",
    "pdf_url": "https://arxiv.org/pdf/2401.01107v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2401.01107",
    "arxiv_authors": [
      "Tianyuan Huang",
      "Zejia Wu",
      "Jiajun Wu",
      "Jackelyn Hwang",
      "Ram Rajagopal"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=CityPulse%3A+Fine-Grained+Assessment+of+Urban+Change+with+Street+View+Time+Series+Tianyuan+Huang+Zejia+Wu+Jiajun+Wu+Jackelyn+Hwang+Ram+Rajagopal",
    "gs_search_success": true,
    "gs_authors": [
      "v2cC_98AAAAJ",
      "2efgcS0AAAAJ",
      "ty2rpKcAAAAJ",
      "N-pVDkYAAAAJ",
      "1I0ff2cAAAAJ"
    ],
    "citation_count": 11,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2302.06857",
    "title": "Make Your Brief Stroke Real and Stereoscopic: 3D-Aware Simplified Sketch to Portrait Generation",
    "year": 2023,
    "published": "2023-02-14T06:28:42Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Creating the photo-realistic version of people sketched portraits is useful to various entertainment purposes. Existing studies only generate portraits in the 2D plane with fixed views, making the results less vivid. In this paper, we present Stereoscopic Simplified Sketch-to-Portrait (SSSP), which explores the possibility of creating Stereoscopic 3D-aware portraits from simple contour sketches by involving 3D generative models. Our key insight is to design sketch-aware constraints that can full",
    "arxiv_url": "https://arxiv.org/abs/2302.06857v2",
    "pdf_url": "https://arxiv.org/pdf/2302.06857v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2302.06857",
    "arxiv_authors": [
      "Yasheng Sun",
      "Qianyi Wu",
      "Hang Zhou",
      "Kaisiyuan Wang",
      "Tianshu Hu",
      "Chen-Chieh Liao",
      "Shio Miyafuji",
      "Ziwei Liu",
      "Hideki Koike"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Make+Your+Brief+Stroke+Real+and+Stereoscopic%3A+3D-Aware+Simplified+Sketch+to+Portrait+Generation+Yasheng+Sun+Qianyi+Wu+Hang+Zhou+Kaisiyuan+Wang+Tianshu+Hu",
    "gs_search_success": false,
    "gs_authors": [],
    "citation_count": null,
    "gs_author_count": 9,
    "error_type": "fetch_failed"
  },
  {
    "arxiv_id": "2301.10056",
    "title": "Side Eye: Characterizing the Limits of POV Acoustic Eavesdropping from Smartphone Cameras with Rolling Shutters and Movable Lenses",
    "year": 2023,
    "published": "2023-01-24T15:00:47Z",
    "categories": [
      "cs.CR",
      "cs.CV",
      "cs.MM",
      "cs.SD",
      "eess.AS"
    ],
    "abstract": "Our research discovers how the rolling shutter and movable lens structures widely found in smartphone cameras modulate structure-borne sounds onto camera images, creating a point-of-view (POV) optical-acoustic side channel for acoustic eavesdropping. The movement of smartphone camera hardware leaks acoustic information because images unwittingly modulate ambient sound as imperceptible distortions. Our experiments find that the side channel is further amplified by intrinsic behaviors of Complemen",
    "arxiv_url": "https://arxiv.org/abs/2301.10056v2",
    "pdf_url": "https://arxiv.org/pdf/2301.10056v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2301.10056",
    "arxiv_authors": [
      "Yan Long",
      "Pirouz Naghavi",
      "Blas Kojusner",
      "Kevin Butler",
      "Sara Rampazzi",
      "Kevin Fu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Side+Eye%3A+Characterizing+the+Limits+of+POV+Acoustic+Eavesdropping+from+Smartphone+Cameras+with+Rolling+Shutters+and+Movable+Lenses+Yan+Long+Pirouz+Naghavi+Blas+Kojusner+Kevin+Butler+Sara+Rampazzi",
    "gs_search_success": true,
    "gs_authors": [
      "pZQuSyUAAAAJ",
      "gvo0nMsAAAAJ",
      "cIx9hWcAAAAJ",
      "AgkNYuIAAAAJ",
      "I9d0CrAAAAAJ"
    ],
    "citation_count": 19,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2410.08675",
    "title": "Bukva: Russian Sign Language Alphabet",
    "year": 2024,
    "published": "2024-10-11T09:59:48Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "This paper investigates the recognition of the Russian fingerspelling alphabet, also known as the Russian Sign Language (RSL) dactyl. Dactyl is a component of sign languages where distinct hand movements represent individual letters of a written language. This method is used to spell words without specific signs, such as proper nouns or technical terms. The alphabet learning simulator is an essential isolated dactyl recognition application. There is a notable issue of data shortage in isolated d",
    "arxiv_url": "https://arxiv.org/abs/2410.08675v1",
    "pdf_url": "https://arxiv.org/pdf/2410.08675v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2410.08675",
    "arxiv_authors": [
      "Karina Kvanchiani",
      "Petr Surovtsev",
      "Alexander Nagaev",
      "Elizaveta Petrova",
      "Alexander Kapitanov"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Bukva%3A+Russian+Sign+Language+Alphabet+Karina+Kvanchiani+Petr+Surovtsev+Alexander+Nagaev+Elizaveta+Petrova+Alexander+Kapitanov",
    "gs_search_success": true,
    "gs_authors": [
      "TfgptlcAAAAJ",
      "IRcihhUAAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2407.13553",
    "title": "SAM-Driven Weakly Supervised Nodule Segmentation with Uncertainty-Aware Cross Teaching",
    "year": 2024,
    "published": "2024-07-18T14:27:54Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Automated nodule segmentation is essential for computer-assisted diagnosis in ultrasound images. Nevertheless, most existing methods depend on precise pixel-level annotations by medical professionals, a process that is both costly and labor-intensive. Recently, segmentation foundation models like SAM have shown impressive generalizability on natural images, suggesting their potential as pseudo-labelers. However, accurate prompts remain crucial for their success in medical images. In this work, w",
    "arxiv_url": "https://arxiv.org/abs/2407.13553v1",
    "pdf_url": "https://arxiv.org/pdf/2407.13553v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2407.13553",
    "arxiv_authors": [
      "Xingyue Zhao",
      "Peiqi Li",
      "Xiangde Luo",
      "Meng Yang",
      "Shi Chang",
      "Zhongyu Li"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=SAM-Driven+Weakly+Supervised+Nodule+Segmentation+with+Uncertainty-Aware+Cross+Teaching+Xingyue+Zhao+Peiqi+Li+Xiangde+Luo+Meng+Yang+Shi+Chang",
    "gs_search_success": true,
    "gs_authors": [
      "tYN83dIAAAAJ",
      "fZuqWe0AAAAJ",
      "dD4HLS4AAAAJ"
    ],
    "citation_count": 3,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2505.20629",
    "title": "Unified Text-Image-to-Video Generation: A Training-Free Approach to Flexible Visual Conditioning",
    "year": 2025,
    "published": "2025-05-27T02:16:06Z",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "Text-image-to-video (TI2V) generation is a critical problem for controllable video generation using both semantic and visual conditions. Most existing methods typically add visual conditions to text-to-video (T2V) foundation models by finetuning, which is costly in resources and only limited to a few pre-defined conditioning settings. To tackle these constraints, we introduce a unified formulation for TI2V generation with flexible visual conditioning. Furthermore, we propose an innovative traini",
    "arxiv_url": "https://arxiv.org/abs/2505.20629v2",
    "pdf_url": "https://arxiv.org/pdf/2505.20629v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2505.20629",
    "arxiv_authors": [
      "Bolin Lai",
      "Sangmin Lee",
      "Xu Cao",
      "Xiang Li",
      "James M. Rehg"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Unified+Text-Image-to-Video+Generation%3A+A+Training-Free+Approach+to+Flexible+Visual+Conditioning+Bolin+Lai+Sangmin+Lee+Xu+Cao+Xiang+Li+James+M.+Rehg",
    "gs_search_success": true,
    "gs_authors": [
      "lWrljmQAAAAJ",
      "3Ds7hOQAAAAJ",
      "oXWRBrwAAAAJ",
      "8kA3eDwAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2308.09538",
    "title": "Uncertainty-based quality assurance of carotid artery wall segmentation in black-blood MRI",
    "year": 2023,
    "published": "2023-08-18T13:16:00Z",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "abstract": "The application of deep learning models to large-scale data sets requires means for automatic quality assurance. We have previously developed a fully automatic algorithm for carotid artery wall segmentation in black-blood MRI that we aim to apply to large-scale data sets. This method identifies nested artery walls in 3D patches centered on the carotid artery. In this study, we investigate to what extent the uncertainty in the model predictions for the contour location can serve as a surrogate fo",
    "arxiv_url": "https://arxiv.org/abs/2308.09538v1",
    "pdf_url": "https://arxiv.org/pdf/2308.09538v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2308.09538",
    "arxiv_authors": [
      "Elina Thibeau-Sutre",
      "Dieuwertje Alblas",
      "Sophie Buurman",
      "Christoph Brune",
      "Jelmer M. Wolterink"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Uncertainty-based+quality+assurance+of+carotid+artery+wall+segmentation+in+black-blood+MRI+Elina+Thibeau-Sutre+Dieuwertje+Alblas+Sophie+Buurman+Christoph+Brune+Jelmer+M.+Wolterink",
    "gs_search_success": true,
    "gs_authors": [
      "QkD3WhsAAAAJ",
      "xzlXuWoAAAAJ",
      "xK_9i68AAAAJ",
      "Nx2gEPMAAAAJ"
    ],
    "citation_count": 3,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2406.15754",
    "title": "Multimodal Segmentation for Vocal Tract Modeling",
    "year": 2024,
    "published": "2024-06-22T06:44:38Z",
    "categories": [
      "cs.CV",
      "cs.CL",
      "cs.LG",
      "cs.SD",
      "eess.AS"
    ],
    "abstract": "Accurate modeling of the vocal tract is necessary to construct articulatory representations for interpretable speech processing and linguistics. However, vocal tract modeling is challenging because many internal articulators are occluded from external motion capture technologies. Real-time magnetic resonance imaging (RT-MRI) allows measuring precise movements of internal articulators during speech, but annotated datasets of MRI are limited in size due to time-consuming and computationally expens",
    "arxiv_url": "https://arxiv.org/abs/2406.15754v1",
    "pdf_url": "https://arxiv.org/pdf/2406.15754v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2406.15754",
    "arxiv_authors": [
      "Rishi Jain",
      "Bohan Yu",
      "Peter Wu",
      "Tejas Prabhune",
      "Gopala Anumanchipalli"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Multimodal+Segmentation+for+Vocal+Tract+Modeling+Rishi+Jain+Bohan+Yu+Peter+Wu+Tejas+Prabhune+Gopala+Anumanchipalli",
    "gs_search_success": true,
    "gs_authors": [
      "VecEj6kAAAAJ",
      "QI1zl84AAAAJ",
      "hsbhfl8AAAAJ",
      "j9ICsOEAAAAJ",
      "CiT4huMAAAAJ"
    ],
    "citation_count": 4,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2412.15570",
    "title": "DefFiller: Mask-Conditioned Diffusion for Salient Steel Surface Defect Generation",
    "year": 2024,
    "published": "2024-12-20T05:08:42Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Current saliency-based defect detection methods show promise in industrial settings, but the unpredictability of defects in steel production environments complicates dataset creation, hampering model performance. Existing data augmentation approaches using generative models often require pixel-level annotations, which are time-consuming and resource-intensive. To address this, we introduce DefFiller, a mask-conditioned defect generation method that leverages a layout-to-image diffusion model. De",
    "arxiv_url": "https://arxiv.org/abs/2412.15570v1",
    "pdf_url": "https://arxiv.org/pdf/2412.15570v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2412.15570",
    "arxiv_authors": [
      "Yichun Tai",
      "Zhenzhen Huang",
      "Tao Peng",
      "Zhijiang Zhang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=DefFiller%3A+Mask-Conditioned+Diffusion+for+Salient+Steel+Surface+Defect+Generation+Yichun+Tai+Zhenzhen+Huang+Tao+Peng+Zhijiang+Zhang",
    "gs_search_success": true,
    "gs_authors": [
      "-mS2F54AAAAJ"
    ],
    "citation_count": 2,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2409.10719",
    "title": "Benchmarking VLMs' Reasoning About Persuasive Atypical Images",
    "year": 2024,
    "published": "2024-09-16T20:47:00Z",
    "categories": [
      "cs.CV",
      "cs.MM"
    ],
    "abstract": "Vision language models (VLMs) have shown strong zero-shot generalization across various tasks, especially when integrated with large language models (LLMs). However, their ability to comprehend rhetorical and persuasive visual media, such as advertisements, remains understudied. Ads often employ atypical imagery, using surprising object juxtapositions to convey shared properties. For example, Fig. 1 (e) shows a beer with a feather-like texture. This requires advanced reasoning to deduce that thi",
    "arxiv_url": "https://arxiv.org/abs/2409.10719v3",
    "pdf_url": "https://arxiv.org/pdf/2409.10719v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2409.10719",
    "arxiv_authors": [
      "Sina Malakouti",
      "Aysan Aghazadeh",
      "Ashmit Khandelwal",
      "Adriana Kovashka"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Benchmarking+VLMs%27+Reasoning+About+Persuasive+Atypical+Images+Sina+Malakouti+Aysan+Aghazadeh+Ashmit+Khandelwal+Adriana+Kovashka",
    "gs_search_success": true,
    "gs_authors": [
      "Dl949GoAAAAJ",
      "nYiQ8-8AAAAJ",
      "T_MTBc8AAAAJ",
      "UDAxK0YAAAAJ"
    ],
    "citation_count": 3,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2312.11537",
    "title": "FastSR-NeRF: Improving NeRF Efficiency on Consumer Devices with A Simple Super-Resolution Pipeline",
    "year": 2023,
    "published": "2023-12-15T21:02:23Z",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "abstract": "Super-resolution (SR) techniques have recently been proposed to upscale the outputs of neural radiance fields (NeRF) and generate high-quality images with enhanced inference speeds. However, existing NeRF+SR methods increase training overhead by using extra input features, loss functions, and/or expensive training procedures such as knowledge distillation. In this paper, we aim to leverage SR for efficiency gains without costly training or architectural changes. Specifically, we build a simple N",
    "arxiv_url": "https://arxiv.org/abs/2312.11537v2",
    "pdf_url": "https://arxiv.org/pdf/2312.11537v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2312.11537",
    "arxiv_authors": [
      "Chien-Yu Lin",
      "Qichen Fu",
      "Thomas Merth",
      "Karren Yang",
      "Anurag Ranjan"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=FastSR-NeRF%3A+Improving+NeRF+Efficiency+on+Consumer+Devices+with+A+Simple+Super-Resolution+Pipeline+Chien-Yu+Lin+Qichen+Fu+Thomas+Merth+Karren+Yang+Anurag+Ranjan",
    "gs_search_success": true,
    "gs_authors": [
      "4wcQM3UAAAAJ",
      "dhQUwOAAAAAJ",
      "K-Xbnp8AAAAJ",
      "kCBh-r8AAAAJ",
      "7SQqHgcAAAAJ"
    ],
    "citation_count": 19,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2504.09033",
    "title": "Chest X-ray Classification using Deep Convolution Models on Low-resolution images with Uncertain Labels",
    "year": 2025,
    "published": "2025-04-12T01:13:00Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Deep Convolutional Neural Networks have consistently proven to achieve state-of-the-art results on a lot of imaging tasks over the past years' majority of which comprise of high-quality data. However, it is important to work on low-resolution images since it could be a cheaper alternative for remote healthcare access where the primary need of automated pathology identification models occurs. Medical diagnosis using low-resolution images is challenging since critical details may not be easily ide",
    "arxiv_url": "https://arxiv.org/abs/2504.09033v1",
    "pdf_url": "https://arxiv.org/pdf/2504.09033v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2504.09033",
    "arxiv_authors": [
      "Snigdha Agarwal",
      "Neelam Sinha"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Chest+X-ray+Classification+using+Deep+Convolution+Models+on+Low-resolution+images+with+Uncertain+Labels+Snigdha+Agarwal+Neelam+Sinha",
    "gs_search_success": true,
    "gs_authors": [
      "6A_OhfoAAAAJ",
      "uvoHB7oAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 2
  },
  {
    "arxiv_id": "2409.01459",
    "title": "3D-LSPTM: An Automatic Framework with 3D-Large-Scale Pretrained Model for Laryngeal Cancer Detection Using Laryngoscopic Videos",
    "year": 2024,
    "published": "2024-09-02T20:44:09Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Laryngeal cancer is a malignant disease with a high morality rate in otorhinolaryngology, posing an significant threat to human health. Traditionally larygologists manually visual-inspect laryngeal cancer in laryngoscopic videos, which is quite time-consuming and subjective. In this study, we propose a novel automatic framework via 3D-large-scale pretrained models termed 3D-LSPTM for laryngeal cancer detection. Firstly, we collect 1,109 laryngoscopic videos from the First Affiliated Hospital Sun",
    "arxiv_url": "https://arxiv.org/abs/2409.01459v1",
    "pdf_url": "https://arxiv.org/pdf/2409.01459v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2409.01459",
    "arxiv_authors": [
      "Meiyu Qiu",
      "Yun Li",
      "Wenjun Huang",
      "Haoyun Zhang",
      "Weiping Zheng",
      "Wenbin Lei",
      "Xiaomao Fan"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=3D-LSPTM%3A+An+Automatic+Framework+with+3D-Large-Scale+Pretrained+Model+for+Laryngeal+Cancer+Detection+Using+Laryngoscopic+Videos+Meiyu+Qiu+Yun+Li+Wenjun+Huang+Haoyun+Zhang+Weiping+Zheng",
    "gs_search_success": true,
    "gs_authors": [
      "zDwIj2wAAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 7
  },
  {
    "arxiv_id": "2402.17726",
    "title": "VRP-SAM: SAM with Visual Reference Prompt",
    "year": 2024,
    "published": "2024-02-27T17:58:09Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "In this paper, we propose a novel Visual Reference Prompt (VRP) encoder that empowers the Segment Anything Model (SAM) to utilize annotated reference images as prompts for segmentation, creating the VRP-SAM model. In essence, VRP-SAM can utilize annotated reference images to comprehend specific objects and perform segmentation of specific objects in target image. It is note that the VRP encoder can support a variety of annotation formats for reference images, including \\textbf{point}, \\textbf{bo",
    "arxiv_url": "https://arxiv.org/abs/2402.17726v4",
    "pdf_url": "https://arxiv.org/pdf/2402.17726v4",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2402.17726",
    "arxiv_authors": [
      "Yanpeng Sun",
      "Jiahui Chen",
      "Shan Zhang",
      "Xinyu Zhang",
      "Xiaofan Li",
      "Qiang Chen",
      "Gang Zhang",
      "Errui Ding",
      "Jingdong Wang",
      "Zechao Li"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=VRP-SAM%3A+SAM+with+Visual+Reference+Prompt+Yanpeng+Sun+Jiahui+Chen+Shan+Zhang+Xinyu+Zhang+Xiaofan+Li",
    "gs_search_success": true,
    "gs_authors": [
      "PSzJxD8AAAAJ",
      "z5SPCmgAAAAJ",
      "L6J2V3sAAAAJ",
      "a3FI8c4AAAAJ",
      "6CvMFKsAAAAJ",
      "cnVvh7AAAAAJ",
      "1wzEtxcAAAAJ"
    ],
    "citation_count": 88,
    "gs_author_count": 9
  },
  {
    "arxiv_id": "2505.14948",
    "title": "Programmatic Video Prediction Using Large Language Models",
    "year": 2025,
    "published": "2025-05-20T22:17:47Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "abstract": "The task of estimating the world model describing the dynamics of a real world process assumes immense importance for anticipating and preparing for future outcomes. For applications such as video surveillance, robotics applications, autonomous driving, etc. this objective entails synthesizing plausible visual futures, given a few frames of a video to set the visual context. Towards this end, we propose ProgGen, which undertakes the task of video frame prediction by representing the dynamics of ",
    "arxiv_url": "https://arxiv.org/abs/2505.14948v1",
    "pdf_url": "https://arxiv.org/pdf/2505.14948v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2505.14948",
    "arxiv_authors": [
      "Hao Tang",
      "Kevin Ellis",
      "Suhas Lohit",
      "Michael J. Jones",
      "Moitreya Chatterjee"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Programmatic+Video+Prediction+Using+Large+Language+Models+Hao+Tang+Kevin+Ellis+Suhas+Lohit+Michael+J.+Jones+Moitreya+Chatterjee",
    "gs_search_success": true,
    "gs_authors": [
      "h-V4QaMAAAAJ",
      "tVjxANMAAAAJ",
      "GMRYY5cAAAAJ",
      "CSxgi6AAAAAJ",
      "_cp4LFcAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2312.11461",
    "title": "GAvatar: Animatable 3D Gaussian Avatars with Implicit Mesh Learning",
    "year": 2023,
    "published": "2023-12-18T18:59:12Z",
    "categories": [
      "cs.CV",
      "cs.GR",
      "cs.LG"
    ],
    "abstract": "Gaussian splatting has emerged as a powerful 3D representation that harnesses the advantages of both explicit (mesh) and implicit (NeRF) 3D representations. In this paper, we seek to leverage Gaussian splatting to generate realistic animatable avatars from textual descriptions, addressing the limitations (e.g., flexibility and efficiency) imposed by mesh or NeRF-based representations. However, a naive application of Gaussian splatting cannot generate high-quality animatable avatars and suffers f",
    "arxiv_url": "https://arxiv.org/abs/2312.11461v2",
    "pdf_url": "https://arxiv.org/pdf/2312.11461v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2312.11461",
    "arxiv_authors": [
      "Ye Yuan",
      "Xueting Li",
      "Yangyi Huang",
      "Shalini De Mello",
      "Koki Nagano",
      "Jan Kautz",
      "Umar Iqbal"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=GAvatar%3A+Animatable+3D+Gaussian+Avatars+with+Implicit+Mesh+Learning+Ye+Yuan+Xueting+Li+Yangyi+Huang+Shalini+De+Mello+Koki+Nagano",
    "gs_search_success": true,
    "gs_authors": [
      "P9FclNEAAAAJ",
      "EEp82sIAAAAJ",
      "nfXdXswAAAAJ",
      "GB7NfS4AAAAJ",
      "xQM4BlMAAAAJ",
      "MzGqlqIAAAAJ",
      "QSKXFiYAAAAJ"
    ],
    "citation_count": 66,
    "gs_author_count": 7
  },
  {
    "arxiv_id": "2406.01797",
    "title": "The Empirical Impact of Forgetting and Transfer in Continual Visual Odometry",
    "year": 2024,
    "published": "2024-06-03T21:32:50Z",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "abstract": "As robotics continues to advance, the need for adaptive and continuously-learning embodied agents increases, particularly in the realm of assistance robotics. Quick adaptability and long-term information retention are essential to operate in dynamic environments typical of humans' everyday lives. A lifelong learning paradigm is thus required, but it is scarcely addressed by current robotics literature. This study empirically investigates the impact of catastrophic forgetting and the effectivenes",
    "arxiv_url": "https://arxiv.org/abs/2406.01797v1",
    "pdf_url": "https://arxiv.org/pdf/2406.01797v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2406.01797",
    "arxiv_authors": [
      "Paolo Cudrano",
      "Xiaoyu Luo",
      "Matteo Matteucci"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=The+Empirical+Impact+of+Forgetting+and+Transfer+in+Continual+Visual+Odometry+Paolo+Cudrano+Xiaoyu+Luo+Matteo+Matteucci",
    "gs_search_success": true,
    "gs_authors": [
      "zpHQDQgAAAAJ",
      "YjUypF4AAAAJ",
      "PdbEg5YAAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2307.05508",
    "title": "Human in the AI loop via xAI and Active Learning for Visual Inspection",
    "year": 2023,
    "published": "2023-07-03T17:23:23Z",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CV"
    ],
    "abstract": "Industrial revolutions have historically disrupted manufacturing by introducing automation into production. Increasing automation reshapes the role of the human worker. Advances in robotics and artificial intelligence open new frontiers of human-machine collaboration. Such collaboration can be realized considering two sub-fields of artificial intelligence: active learning and explainable artificial intelligence. Active learning aims to devise strategies that help obtain data that allows machine ",
    "arxiv_url": "https://arxiv.org/abs/2307.05508v2",
    "pdf_url": "https://arxiv.org/pdf/2307.05508v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2307.05508",
    "arxiv_authors": [
      "Jože M. Rožanec",
      "Elias Montini",
      "Vincenzo Cutrona",
      "Dimitrios Papamartzivanos",
      "Timotej Klemenčič",
      "Blaž Fortuna",
      "Dunja Mladenić",
      "Entso Veliou",
      "Thanassis Giannetsos",
      "Christos Emmanouilidis"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Human+in+the+AI+loop+via+xAI+and+Active+Learning+for+Visual+Inspection+Jo%C5%BEe+M.+Ro%C5%BEanec+Elias+Montini+Vincenzo+Cutrona+Dimitrios+Papamartzivanos+Timotej+Klemen%C4%8Di%C4%8D",
    "gs_search_success": true,
    "gs_authors": [
      "LCPDIKIAAAAJ",
      "3ugDRlsAAAAJ",
      "8_Kwz3IAAAAJ",
      "TdxG5D4AAAAJ"
    ],
    "citation_count": 19,
    "gs_author_count": 7
  },
  {
    "arxiv_id": "2503.02316",
    "title": "Unified Arbitrary-Time Video Frame Interpolation and Prediction",
    "year": 2025,
    "published": "2025-03-04T06:17:17Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Video frame interpolation and prediction aim to synthesize frames in-between and subsequent to existing frames, respectively. Despite being closely-related, these two tasks are traditionally studied with different model architectures, or same architecture but individually trained weights. Furthermore, while arbitrary-time interpolation has been extensively studied, the value of arbitrary-time prediction has been largely overlooked. In this work, we present uniVIP - unified arbitrary-time Video I",
    "arxiv_url": "https://arxiv.org/abs/2503.02316v1",
    "pdf_url": "https://arxiv.org/pdf/2503.02316v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2503.02316",
    "arxiv_authors": [
      "Xin Jin",
      "Longhai Wu",
      "Jie Chen",
      "Ilhyun Cho",
      "Cheul-Hee Hahm"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Unified+Arbitrary-Time+Video+Frame+Interpolation+and+Prediction+Xin+Jin+Longhai+Wu+Jie+Chen+Ilhyun+Cho+Cheul-Hee+Hahm",
    "gs_search_success": true,
    "gs_authors": [
      "ddsm5qYAAAAJ"
    ],
    "citation_count": 2,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2402.14354",
    "title": "GAM-Depth: Self-Supervised Indoor Depth Estimation Leveraging a Gradient-Aware Mask and Semantic Constraints",
    "year": 2024,
    "published": "2024-02-22T07:53:34Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Self-supervised depth estimation has evolved into an image reconstruction task that minimizes a photometric loss. While recent methods have made strides in indoor depth estimation, they often produce inconsistent depth estimation in textureless areas and unsatisfactory depth discrepancies at object boundaries. To address these issues, in this work, we propose GAM-Depth, developed upon two novel components: gradient-aware mask and semantic constraints. The gradient-aware mask enables adaptive and",
    "arxiv_url": "https://arxiv.org/abs/2402.14354v1",
    "pdf_url": "https://arxiv.org/pdf/2402.14354v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2402.14354",
    "arxiv_authors": [
      "Anqi Cheng",
      "Zhiyuan Yang",
      "Haiyue Zhu",
      "Kezhi Mao"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=GAM-Depth%3A+Self-Supervised+Indoor+Depth+Estimation+Leveraging+a+Gradient-Aware+Mask+and+Semantic+Constraints+Anqi+Cheng+Zhiyuan+Yang+Haiyue+Zhu+Kezhi+Mao",
    "gs_search_success": true,
    "gs_authors": [
      "IHMpsg0AAAAJ",
      "RZBaKLsAAAAJ",
      "jCsRJXUAAAAJ",
      "uO_R9wQAAAAJ"
    ],
    "citation_count": 8,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2501.03775",
    "title": "Strip R-CNN: Large Strip Convolution for Remote Sensing Object Detection",
    "year": 2025,
    "published": "2025-01-07T13:30:54Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "While witnessed with rapid development, remote sensing object detection remains challenging for detecting high aspect ratio objects. This paper shows that large strip convolutions are good feature representation learners for remote sensing object detection and can detect objects of various aspect ratios well. Based on large strip convolutions, we build a new network architecture called Strip R-CNN, which is simple, efficient, and powerful. Unlike recent remote sensing object detectors that lever",
    "arxiv_url": "https://arxiv.org/abs/2501.03775v4",
    "pdf_url": "https://arxiv.org/pdf/2501.03775v4",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2501.03775",
    "arxiv_authors": [
      "Xinbin Yuan",
      "Zhaohui Zheng",
      "Yuxuan Li",
      "Xialei Liu",
      "Li Liu",
      "Xiang Li",
      "Qibin Hou",
      "Ming-Ming Cheng"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Strip+R-CNN%3A+Large+Strip+Convolution+for+Remote+Sensing+Object+Detection+Xinbin+Yuan+Zhaohui+Zheng+Yuxuan+Li+Xialei+Liu+Li+Liu",
    "gs_search_success": true,
    "gs_authors": [
      "LxgdmqYAAAAJ",
      "T07OWMkAAAAJ",
      "_vNKp08AAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2411.16375",
    "title": "Ca2-VDM: Efficient Autoregressive Video Diffusion Model with Causal Generation and Cache Sharing",
    "year": 2024,
    "published": "2024-11-25T13:33:41Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "With the advance of diffusion models, today's video generation has achieved impressive quality. To extend the generation length and facilitate real-world applications, a majority of video diffusion models (VDMs) generate videos in an autoregressive manner, i.e., generating subsequent clips conditioned on the last frame(s) of the previous clip. However, existing autoregressive VDMs are highly inefficient and redundant: The model must re-compute all the conditional frames that are overlapped betwe",
    "arxiv_url": "https://arxiv.org/abs/2411.16375v2",
    "pdf_url": "https://arxiv.org/pdf/2411.16375v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2411.16375",
    "arxiv_authors": [
      "Kaifeng Gao",
      "Jiaxin Shi",
      "Hanwang Zhang",
      "Chunping Wang",
      "Jun Xiao",
      "Long Chen"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Ca2-VDM%3A+Efficient+Autoregressive+Video+Diffusion+Model+with+Causal+Generation+and+Cache+Sharing+Kaifeng+Gao+Jiaxin+Shi+Hanwang+Zhang+Chunping+Wang+Jun+Xiao",
    "gs_search_success": true,
    "gs_authors": [
      "fqOwFhQAAAAJ",
      "-gtmMpIAAAAJ",
      "gpk70j8AAAAJ",
      "8XcQHUEAAAAJ"
    ],
    "citation_count": 14,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2303.15919",
    "title": "Fully Hyperbolic Convolutional Neural Networks for Computer Vision",
    "year": 2023,
    "published": "2023-03-28T12:20:52Z",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "Real-world visual data exhibit intrinsic hierarchical structures that can be represented effectively in hyperbolic spaces. Hyperbolic neural networks (HNNs) are a promising approach for learning feature representations in such spaces. However, current HNNs in computer vision rely on Euclidean backbones and only project features to the hyperbolic space in the task heads, limiting their ability to fully leverage the benefits of hyperbolic geometry. To address this, we present HCNN, a fully hyperbo",
    "arxiv_url": "https://arxiv.org/abs/2303.15919v3",
    "pdf_url": "https://arxiv.org/pdf/2303.15919v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2303.15919",
    "arxiv_authors": [
      "Ahmad Bdeir",
      "Kristian Schwethelm",
      "Niels Landwehr"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Fully+Hyperbolic+Convolutional+Neural+Networks+for+Computer+Vision+Ahmad+Bdeir+Kristian+Schwethelm+Niels+Landwehr",
    "gs_search_success": true,
    "gs_authors": [
      "k6Ei14sAAAAJ",
      "Q5X7Kw8AAAAJ",
      "eeeO7kEAAAAJ"
    ],
    "citation_count": 43,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2306.01735",
    "title": "Multilingual Conceptual Coverage in Text-to-Image Models",
    "year": 2023,
    "published": "2023-06-02T17:59:09Z",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV",
      "eess.IV"
    ],
    "abstract": "We propose \"Conceptual Coverage Across Languages\" (CoCo-CroLa), a technique for benchmarking the degree to which any generative text-to-image system provides multilingual parity to its training language in terms of tangible nouns. For each model we can assess \"conceptual coverage\" of a given target language relative to a source language by comparing the population of images generated for a series of tangible nouns in the source language to the population of images generated for each noun under t",
    "arxiv_url": "https://arxiv.org/abs/2306.01735v1",
    "pdf_url": "https://arxiv.org/pdf/2306.01735v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2306.01735",
    "arxiv_authors": [
      "Michael Saxon",
      "William Yang Wang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Multilingual+Conceptual+Coverage+in+Text-to-Image+Models+Michael+Saxon+William+Yang+Wang",
    "gs_search_success": true,
    "gs_authors": [
      "gf8Ms_8AAAAJ",
      "pAlwjdgAAAAJ"
    ],
    "citation_count": 16,
    "gs_author_count": 2
  },
  {
    "arxiv_id": "2410.20502",
    "title": "ARLON: Boosting Diffusion Transformers with Autoregressive Models for Long Video Generation",
    "year": 2024,
    "published": "2024-10-27T16:28:28Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Text-to-video models have recently undergone rapid and substantial advancements. Nevertheless, due to limitations in data and computational resources, achieving efficient generation of long videos with rich motion dynamics remains a significant challenge. To generate high-quality, dynamic, and temporally consistent long videos, this paper presents ARLON, a novel framework that boosts diffusion Transformers with autoregressive models for long video generation, by integrating the coarse spatial an",
    "arxiv_url": "https://arxiv.org/abs/2410.20502v3",
    "pdf_url": "https://arxiv.org/pdf/2410.20502v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2410.20502",
    "arxiv_authors": [
      "Zongyi Li",
      "Shujie Hu",
      "Shujie Liu",
      "Long Zhou",
      "Jeongsoo Choi",
      "Lingwei Meng",
      "Xun Guo",
      "Jinyu Li",
      "Hefei Ling",
      "Furu Wei"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=ARLON%3A+Boosting+Diffusion+Transformers+with+Autoregressive+Models+for+Long+Video+Generation+Zongyi+Li+Shujie+Hu+Shujie+Liu+Long+Zhou+Jeongsoo+Choi",
    "gs_search_success": true,
    "gs_authors": [
      "ZnwgSXIAAAAJ",
      "Ow4R8-EAAAAJ",
      "Vtirkf4AAAAJ",
      "6mNya-wAAAAJ",
      "hPrKNUUAAAAJ",
      "WPcjsEkAAAAJ",
      "zyk30CYAAAAJ"
    ],
    "citation_count": 14,
    "gs_author_count": null
  },
  {
    "arxiv_id": "2312.16388",
    "title": "Gaussian Mixture Proposals with Pull-Push Learning Scheme to Capture Diverse Events for Weakly Supervised Temporal Video Grounding",
    "year": 2023,
    "published": "2023-12-27T03:29:01Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "In the weakly supervised temporal video grounding study, previous methods use predetermined single Gaussian proposals which lack the ability to express diverse events described by the sentence query. To enhance the expression ability of a proposal, we propose a Gaussian mixture proposal (GMP) that can depict arbitrary shapes by learning importance, centroid, and range of every Gaussian in the mixture. In learning GMP, each Gaussian is not trained in a feature space but is implemented over a temp",
    "arxiv_url": "https://arxiv.org/abs/2312.16388v1",
    "pdf_url": "https://arxiv.org/pdf/2312.16388v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2312.16388",
    "arxiv_authors": [
      "Sunoh Kim",
      "Jungchan Cho",
      "Joonsang Yu",
      "YoungJoon Yoo",
      "Jin Young Choi"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Gaussian+Mixture+Proposals+with+Pull-Push+Learning+Scheme+to+Capture+Diverse+Events+for+Weakly+Supervised+Temporal+Video+Grounding+Sunoh+Kim+Jungchan+Cho+Joonsang+Yu+YoungJoon+Yoo+Jin+Young+Choi",
    "gs_search_success": true,
    "gs_authors": [
      "NoEVFWQAAAAJ",
      "YGVqRuIAAAAJ",
      "wlhSowsAAAAJ",
      "IC6M7_IAAAAJ"
    ],
    "citation_count": 16,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2409.09804",
    "title": "Abnormal Event Detection In Videos Using Deep Embedding",
    "year": 2024,
    "published": "2024-09-15T17:44:51Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Abnormal event detection or anomaly detection in surveillance videos is currently a challenge because of the diversity of possible events. Due to the lack of anomalous events at training time, anomaly detection requires the design of learning methods without supervision. In this work we propose an unsupervised approach for video anomaly detection with the aim to jointly optimize the objectives of the deep neural network and the anomaly detection task using a hybrid architecture. Initially, a con",
    "arxiv_url": "https://arxiv.org/abs/2409.09804v1",
    "pdf_url": "https://arxiv.org/pdf/2409.09804v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2409.09804",
    "arxiv_authors": [
      "Darshan Venkatrayappa"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Abnormal+Event+Detection+In+Videos+Using+Deep+Embedding+Darshan+Venkatrayappa",
    "gs_search_success": true,
    "gs_authors": [],
    "citation_count": 2,
    "gs_author_count": 1
  },
  {
    "arxiv_id": "2405.19186",
    "title": "MetaToken: Detecting Hallucination in Image Descriptions by Meta Classification",
    "year": 2024,
    "published": "2024-05-29T15:28:42Z",
    "categories": [
      "cs.CV",
      "cs.CL",
      "cs.LG"
    ],
    "abstract": "Large Vision Language Models (LVLMs) have shown remarkable capabilities in multimodal tasks like visual question answering or image captioning. However, inconsistencies between the visual information and the generated text, a phenomenon referred to as hallucinations, remain an unsolved problem with regard to the trustworthiness of LVLMs. To address this problem, recent works proposed to incorporate computationally costly Large (Vision) Language Models in order to detect hallucinations on a sente",
    "arxiv_url": "https://arxiv.org/abs/2405.19186v2",
    "pdf_url": "https://arxiv.org/pdf/2405.19186v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2405.19186",
    "arxiv_authors": [
      "Laura Fieback",
      "Jakob Spiegelberg",
      "Hanno Gottschalk"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=MetaToken%3A+Detecting+Hallucination+in+Image+Descriptions+by+Meta+Classification+Laura+Fieback+Jakob+Spiegelberg+Hanno+Gottschalk",
    "gs_search_success": true,
    "gs_authors": [
      "5cOHZ4MAAAAJ",
      "DPeTNW0AAAAJ"
    ],
    "citation_count": 11,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2302.14574",
    "title": "A Little Bit Attention Is All You Need for Person Re-Identification",
    "year": 2023,
    "published": "2023-02-28T13:54:31Z",
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "abstract": "Person re-identification plays a key role in applications where a mobile robot needs to track its users over a long period of time, even if they are partially unobserved for some time, in order to follow them or be available on demand. In this context, deep-learning based real-time feature extraction on a mobile robot is often performed on special-purpose devices whose computational resources are shared for multiple tasks. Therefore, the inference speed has to be taken into account. In contrast,",
    "arxiv_url": "https://arxiv.org/abs/2302.14574v1",
    "pdf_url": "https://arxiv.org/pdf/2302.14574v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2302.14574",
    "arxiv_authors": [
      "Markus Eisenbach",
      "Jannik Lübberstedt",
      "Dustin Aganian",
      "Horst-Michael Gross"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=A+Little+Bit+Attention+Is+All+You+Need+for+Person+Re-Identification+Markus+Eisenbach+Jannik+L%C3%BCbberstedt+Dustin+Aganian+Horst-Michael+Gross",
    "gs_search_success": true,
    "gs_authors": [
      "9guI-DwAAAAJ",
      "pfytQcsAAAAJ",
      "eKw3T1EAAAAJ"
    ],
    "citation_count": 6,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2312.10758",
    "title": "SHaRPose: Sparse High-Resolution Representation for Human Pose Estimation",
    "year": 2023,
    "published": "2023-12-17T16:29:16Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "High-resolution representation is essential for achieving good performance in human pose estimation models. To obtain such features, existing works utilize high-resolution input images or fine-grained image tokens. However, this dense high-resolution representation brings a significant computational burden. In this paper, we address the following question: \"Only sparse human keypoint locations are detected for human pose estimation, is it really necessary to describe the whole image in a dense, ",
    "arxiv_url": "https://arxiv.org/abs/2312.10758v1",
    "pdf_url": "https://arxiv.org/pdf/2312.10758v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2312.10758",
    "arxiv_authors": [
      "Xiaoqi An",
      "Lin Zhao",
      "Chen Gong",
      "Nannan Wang",
      "Di Wang",
      "Jian Yang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=SHaRPose%3A+Sparse+High-Resolution+Representation+for+Human+Pose+Estimation+Xiaoqi+An+Lin+Zhao+Chen+Gong+Nannan+Wang+Di+Wang",
    "gs_search_success": true,
    "gs_authors": [
      "OJ4qnYcAAAAJ",
      "guttoBwAAAAJ",
      "yGdkEroAAAAJ",
      "6CIDtZQAAAAJ",
      "SRBn7oUAAAAJ",
      "N1Y-n5EAAAAJ"
    ],
    "citation_count": 30,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2407.01230",
    "title": "DaBiT: Depth and Blur informed Transformer for Video Focal Deblurring",
    "year": 2024,
    "published": "2024-07-01T12:22:16Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "In many real-world scenarios, recorded videos suffer from accidental focus blur, and while video deblurring methods exist, most specifically target motion blur or spatial-invariant blur. This paper introduces a framework optimized for the as yet unattempted task of video focal deblurring (refocusing). The proposed method employs novel map-guided transformers, in addition to image propagation, to effectively leverage the continuous spatial variance of focal blur and restore the footage. We also i",
    "arxiv_url": "https://arxiv.org/abs/2407.01230v3",
    "pdf_url": "https://arxiv.org/pdf/2407.01230v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2407.01230",
    "arxiv_authors": [
      "Crispian Morris",
      "Nantheera Anantrasirichai",
      "Fan Zhang",
      "David Bull"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=DaBiT%3A+Depth+and+Blur+informed+Transformer+for+Video+Focal+Deblurring+Crispian+Morris+Nantheera+Anantrasirichai+Fan+Zhang+David+Bull",
    "gs_search_success": true,
    "gs_authors": [
      "hka8QbUAAAAJ",
      "WraDXlkAAAAJ",
      "40VEYswAAAAJ",
      "BBujJNcAAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2312.09579",
    "title": "MobileSAMv2: Faster Segment Anything to Everything",
    "year": 2023,
    "published": "2023-12-15T07:21:12Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Segment anything model (SAM) addresses two practical yet challenging segmentation tasks: \\textbf{segment anything (SegAny)}, which utilizes a certain point to predict the mask for a single object of interest, and \\textbf{segment everything (SegEvery)}, which predicts the masks for all objects on the image. What makes SegAny slow for SAM is its heavyweight image encoder, which has been addressed by MobileSAM via decoupled knowledge distillation. The efficiency bottleneck of SegEvery with SAM, how",
    "arxiv_url": "https://arxiv.org/abs/2312.09579v1",
    "pdf_url": "https://arxiv.org/pdf/2312.09579v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2312.09579",
    "arxiv_authors": [
      "Chaoning Zhang",
      "Dongshen Han",
      "Sheng Zheng",
      "Jinwoo Choi",
      "Tae-Ho Kim",
      "Choong Seon Hong"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=MobileSAMv2%3A+Faster+Segment+Anything+to+Everything+Chaoning+Zhang+Dongshen+Han+Sheng+Zheng+Jinwoo+Choi+Tae-Ho+Kim",
    "gs_search_success": true,
    "gs_authors": [
      "oKANWloAAAAJ",
      "WcCMuIEAAAAJ",
      "ondoTPwAAAAJ",
      "lvhxhyQAAAAJ",
      "TgrJOjoAAAAJ"
    ],
    "citation_count": 49,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2411.10651",
    "title": "Understanding Learning with Sliced-Wasserstein Requires Rethinking Informative Slices",
    "year": 2024,
    "published": "2024-11-16T01:18:27Z",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "stat.AP",
      "stat.CO",
      "stat.ML"
    ],
    "abstract": "The practical applications of Wasserstein distances (WDs) are constrained by their sample and computational complexities. Sliced-Wasserstein distances (SWDs) provide a workaround by projecting distributions onto one-dimensional subspaces, leveraging the more efficient, closed-form WDs for one-dimensional distributions. However, in high dimensions, most random projections become uninformative due to the concentration of measure phenomenon. Although several SWD variants have been proposed to focus",
    "arxiv_url": "https://arxiv.org/abs/2411.10651v1",
    "pdf_url": "https://arxiv.org/pdf/2411.10651v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2411.10651",
    "arxiv_authors": [
      "Huy Tran",
      "Yikun Bai",
      "Ashkan Shahbazi",
      "John R. Hershey",
      "Soheil Kolouri"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Understanding+Learning+with+Sliced-Wasserstein+Requires+Rethinking+Informative+Slices+Huy+Tran+Yikun+Bai+Ashkan+Shahbazi+John+R.+Hershey+Soheil+Kolouri",
    "gs_search_success": true,
    "gs_authors": [
      "lzdU2j8AAAAJ",
      "3fzHR6EAAAAJ",
      "zLm6JOAAAAAJ",
      "ACcAdXgAAAAJ",
      "yREBSy0AAAAJ"
    ],
    "citation_count": 4,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2404.14801",
    "title": "DesignProbe: A Graphic Design Benchmark for Multimodal Large Language Models",
    "year": 2024,
    "published": "2024-04-23T07:31:19Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "A well-executed graphic design typically achieves harmony in two levels, from the fine-grained design elements (color, font and layout) to the overall design. This complexity makes the comprehension of graphic design challenging, for it needs the capability to both recognize the design elements and understand the design. With the rapid development of Multimodal Large Language Models (MLLMs), we establish the DesignProbe, a benchmark to investigate the capability of MLLMs in design. Our benchmark",
    "arxiv_url": "https://arxiv.org/abs/2404.14801v1",
    "pdf_url": "https://arxiv.org/pdf/2404.14801v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2404.14801",
    "arxiv_authors": [
      "Jieru Lin",
      "Danqing Huang",
      "Tiejun Zhao",
      "Dechen Zhan",
      "Chin-Yew Lin"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=DesignProbe%3A+A+Graphic+Design+Benchmark+for+Multimodal+Large+Language+Models+Jieru+Lin+Danqing+Huang+Tiejun+Zhao+Dechen+Zhan+Chin-Yew+Lin",
    "gs_search_success": true,
    "gs_authors": [
      "_1pOuRgAAAAJ",
      "cDF07aYAAAAJ",
      "P55WbwYAAAAJ"
    ],
    "citation_count": 8,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2409.18300",
    "title": "SOAR: Self-supervision Optimized UAV Action Recognition with Efficient Object-Aware Pretraining",
    "year": 2024,
    "published": "2024-09-26T21:15:22Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "cs.RO"
    ],
    "abstract": "We introduce SOAR, a novel Self-supervised pretraining algorithm for aerial footage captured by Unmanned Aerial Vehicles (UAVs). We incorporate human object knowledge throughout the pretraining process to enhance UAV video pretraining efficiency and downstream action recognition performance. This is in contrast to prior works that primarily incorporate object information during the fine-tuning stage. Specifically, we first propose a novel object-aware masking strategy designed to retain the visi",
    "arxiv_url": "https://arxiv.org/abs/2409.18300v1",
    "pdf_url": "https://arxiv.org/pdf/2409.18300v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2409.18300",
    "arxiv_authors": [
      "Ruiqi Xian",
      "Xiyang Wu",
      "Tianrui Guan",
      "Xijun Wang",
      "Boqing Gong",
      "Dinesh Manocha"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=SOAR%3A+Self-supervision+Optimized+UAV+Action+Recognition+with+Efficient+Object-Aware+Pretraining+Ruiqi+Xian+Xiyang+Wu+Tianrui+Guan+Xijun+Wang+Boqing+Gong",
    "gs_search_success": true,
    "gs_authors": [
      "sI05dqQAAAAJ",
      "lv9ZeVUAAAAJ",
      "KlqOhiUAAAAJ",
      "YEQBLOsAAAAJ",
      "_7mX21UAAAAJ",
      "X08l_4IAAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2401.08049",
    "title": "EmoTalker: Emotionally Editable Talking Face Generation via Diffusion Model",
    "year": 2024,
    "published": "2024-01-16T02:02:44Z",
    "categories": [
      "cs.CV",
      "cs.SD",
      "eess.AS"
    ],
    "abstract": "In recent years, the field of talking faces generation has attracted considerable attention, with certain methods adept at generating virtual faces that convincingly imitate human expressions. However, existing methods face challenges related to limited generalization, particularly when dealing with challenging identities. Furthermore, methods for editing expressions are often confined to a singular emotion, failing to adapt to intricate emotions. To overcome these challenges, this paper propose",
    "arxiv_url": "https://arxiv.org/abs/2401.08049v1",
    "pdf_url": "https://arxiv.org/pdf/2401.08049v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2401.08049",
    "arxiv_authors": [
      "Bingyuan Zhang",
      "Xulong Zhang",
      "Ning Cheng",
      "Jun Yu",
      "Jing Xiao",
      "Jianzong Wang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=EmoTalker%3A+Emotionally+Editable+Talking+Face+Generation+via+Diffusion+Model+Bingyuan+Zhang+Xulong+Zhang+Ning+Cheng+Jun+Yu+Jing+Xiao",
    "gs_search_success": true,
    "gs_authors": [
      "1XKLPoAAAAAJ",
      "mcBd8KUAAAAJ",
      "efZyqyQAAAAJ"
    ],
    "citation_count": 21,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2303.10411",
    "title": "Multi-Semantic Interactive Learning for Object Detection",
    "year": 2023,
    "published": "2023-03-18T13:14:15Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Single-branch object detection methods use shared features for localization and classification, yet the shared features are not fit for the two different tasks simultaneously. Multi-branch object detection methods usually use different features for localization and classification separately, ignoring the relevance between different tasks. Therefore, we propose multi-semantic interactive learning (MSIL) to mine the semantic relevance between different branches and extract multi-semantic enhanced ",
    "arxiv_url": "https://arxiv.org/abs/2303.10411v1",
    "pdf_url": "https://arxiv.org/pdf/2303.10411v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2303.10411",
    "arxiv_authors": [
      "Shuxin Wang",
      "Zhichao Zheng",
      "Yanhui Gu",
      "Junsheng Zhou",
      "Yi Chen"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Multi-Semantic+Interactive+Learning+for+Object+Detection+Shuxin+Wang+Zhichao+Zheng+Yanhui+Gu+Junsheng+Zhou+Yi+Chen",
    "gs_search_success": true,
    "gs_authors": [
      "byG9xLMAAAAJ",
      "uixxHFYAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2312.06713",
    "title": "TeTriRF: Temporal Tri-Plane Radiance Fields for Efficient Free-Viewpoint Video",
    "year": 2023,
    "published": "2023-12-10T23:00:24Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Neural Radiance Fields (NeRF) revolutionize the realm of visual media by providing photorealistic Free-Viewpoint Video (FVV) experiences, offering viewers unparalleled immersion and interactivity. However, the technology's significant storage requirements and the computational complexity involved in generation and rendering currently limit its broader application. To close this gap, this paper presents Temporal Tri-Plane Radiance Fields (TeTriRF), a novel technology that significantly reduces th",
    "arxiv_url": "https://arxiv.org/abs/2312.06713v1",
    "pdf_url": "https://arxiv.org/pdf/2312.06713v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2312.06713",
    "arxiv_authors": [
      "Minye Wu",
      "Zehao Wang",
      "Georgios Kouros",
      "Tinne Tuytelaars"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=TeTriRF%3A+Temporal+Tri-Plane+Radiance+Fields+for+Efficient+Free-Viewpoint+Video+Minye+Wu+Zehao+Wang+Georgios+Kouros+Tinne+Tuytelaars",
    "gs_search_success": true,
    "gs_authors": [
      "aUAnceQAAAAJ",
      "kEZX1y8AAAAJ",
      "EuFF9kUAAAAJ",
      "anKnLZEAAAAJ"
    ],
    "citation_count": 26,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2410.09921",
    "title": "The Roles of Contextual Semantic Relevance Metrics in Human Visual Processing",
    "year": 2024,
    "published": "2024-10-13T17:05:47Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Semantic relevance metrics can capture both the inherent semantics of individual objects and their relationships to other elements within a visual scene. Numerous previous research has demonstrated that these metrics can influence human visual processing. However, these studies often did not fully account for contextual information or employ the recent deep learning models for more accurate computation. This study investigates human visual perception and processing by introducing the metrics of ",
    "arxiv_url": "https://arxiv.org/abs/2410.09921v1",
    "pdf_url": "https://arxiv.org/pdf/2410.09921v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2410.09921",
    "arxiv_authors": [
      "Kun Sun",
      "Rong Wang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=The+Roles+of+Contextual+Semantic+Relevance+Metrics+in+Human+Visual+Processing+Kun+Sun+Rong+Wang",
    "gs_search_success": true,
    "gs_authors": [
      "zE_8aHUAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 2
  },
  {
    "arxiv_id": "2407.00267",
    "title": "Learning a Clinically-Relevant Concept Bottleneck for Lesion Detection in Breast Ultrasound",
    "year": 2024,
    "published": "2024-06-29T00:44:33Z",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "Detecting and classifying lesions in breast ultrasound images is a promising application of artificial intelligence (AI) for reducing the burden of cancer in regions with limited access to mammography. Such AI systems are more likely to be useful in a clinical setting if their predictions can be explained to a radiologist. This work proposes an explainable AI model that provides interpretable predictions using a standard lexicon from the American College of Radiology's Breast Imaging and Reporti",
    "arxiv_url": "https://arxiv.org/abs/2407.00267v1",
    "pdf_url": "https://arxiv.org/pdf/2407.00267v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2407.00267",
    "arxiv_authors": [
      "Arianna Bunnell",
      "Yannik Glaser",
      "Dustin Valdez",
      "Thomas Wolfgruber",
      "Aleen Altamirano",
      "Carol Zamora González",
      "Brenda Y. Hernandez",
      "Peter Sadowski",
      "John A. Shepherd"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Learning+a+Clinically-Relevant+Concept+Bottleneck+for+Lesion+Detection+in+Breast+Ultrasound+Arianna+Bunnell+Yannik+Glaser+Dustin+Valdez+Thomas+Wolfgruber+Aleen+Altamirano",
    "gs_search_success": true,
    "gs_authors": [
      "Hm4FEQYAAAAJ",
      "Ng_rU48AAAAJ",
      "YqrPlbQAAAAJ",
      "YYLqd70AAAAJ"
    ],
    "citation_count": 2,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2311.17460",
    "title": "W-HMR: Monocular Human Mesh Recovery in World Space with Weak-Supervised Calibration",
    "year": 2023,
    "published": "2023-11-29T09:02:07Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Previous methods for 3D human motion recovery from monocular images often fall short due to reliance on camera coordinates, leading to inaccuracies in real-world applications. The limited availability and diversity of focal length labels further exacerbate misalignment issues in reconstructed 3D human bodies. To address these challenges, we introduce W-HMR, a weak-supervised calibration method that predicts \"reasonable\" focal lengths based on body distortion information, eliminating the need for",
    "arxiv_url": "https://arxiv.org/abs/2311.17460v6",
    "pdf_url": "https://arxiv.org/pdf/2311.17460v6",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2311.17460",
    "arxiv_authors": [
      "Wei Yao",
      "Hongwen Zhang",
      "Yunlian Sun",
      "Yebin Liu",
      "Jinhui Tang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=W-HMR%3A+Monocular+Human+Mesh+Recovery+in+World+Space+with+Weak-Supervised+Calibration+Wei+Yao+Hongwen+Zhang+Yunlian+Sun+Yebin+Liu+Jinhui+Tang",
    "gs_search_success": true,
    "gs_authors": [
      "6z0m_ZMAAAAJ",
      "GlKbx-4AAAAJ",
      "ByBLlEwAAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2411.18197",
    "title": "Make-It-Animatable: An Efficient Framework for Authoring Animation-Ready 3D Characters",
    "year": 2024,
    "published": "2024-11-27T10:18:06Z",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "abstract": "3D characters are essential to modern creative industries, but making them animatable often demands extensive manual work in tasks like rigging and skinning. Existing automatic rigging tools face several limitations, including the necessity for manual annotations, rigid skeleton topologies, and limited generalization across diverse shapes and poses. An alternative approach is to generate animatable avatars pre-bound to a rigged template mesh. However, this method often lacks flexibility and is t",
    "arxiv_url": "https://arxiv.org/abs/2411.18197v3",
    "pdf_url": "https://arxiv.org/pdf/2411.18197v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2411.18197",
    "arxiv_authors": [
      "Zhiyang Guo",
      "Jinxu Xiang",
      "Kai Ma",
      "Wengang Zhou",
      "Houqiang Li",
      "Ran Zhang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Make-It-Animatable%3A+An+Efficient+Framework+for+Authoring+Animation-Ready+3D+Characters+Zhiyang+Guo+Jinxu+Xiang+Kai+Ma+Wengang+Zhou+Houqiang+Li",
    "gs_search_success": true,
    "gs_authors": [
      "FSSXeyAAAAAJ",
      "8s1JF8YAAAAJ",
      "V5QsRzMAAAAJ",
      "FIJBtNIAAAAJ",
      "uYDfhx4AAAAJ"
    ],
    "citation_count": 13,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2503.23898",
    "title": "An Explainable Neural Radiomic Sequence Model with Spatiotemporal Continuity for Quantifying 4DCT-based Pulmonary Ventilation",
    "year": 2025,
    "published": "2025-03-31T09:47:03Z",
    "categories": [
      "physics.med-ph",
      "cs.CV"
    ],
    "abstract": "Accurate evaluation of regional lung ventilation is essential for the management and treatment of lung cancer patients, supporting assessments of pulmonary function, optimization of therapeutic strategies, and monitoring of treatment response. Currently, ventilation scintigraphy using nuclear medicine techniques is widely employed in clinical practice; however, it is often time-consuming, costly, and entails additional radiation exposure. In this study, we propose an explainable neural radiomic ",
    "arxiv_url": "https://arxiv.org/abs/2503.23898v2",
    "pdf_url": "https://arxiv.org/pdf/2503.23898v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2503.23898",
    "arxiv_authors": [
      "Rihui Zhang",
      "Haiming Zhu",
      "Jingtong Zhao",
      "Lei Zhang",
      "Fang-Fang Yin",
      "Chunhao Wang",
      "Zhenyu Yang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=An+Explainable+Neural+Radiomic+Sequence+Model+with+Spatiotemporal+Continuity+for+Quantifying+4DCT-based+Pulmonary+Ventilation+Rihui+Zhang+Haiming+Zhu+Jingtong+Zhao+Lei+Zhang+Fang-Fang+Yin",
    "gs_search_success": true,
    "gs_authors": [
      "oYqSQSkAAAAJ",
      "F4fxZXkAAAAJ",
      "J6lv7y8AAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 7
  },
  {
    "arxiv_id": "2303.05118",
    "title": "SLCA: Slow Learner with Classifier Alignment for Continual Learning on a Pre-trained Model",
    "year": 2023,
    "published": "2023-03-09T08:57:01Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "abstract": "The goal of continual learning is to improve the performance of recognition models in learning sequentially arrived data. Although most existing works are established on the premise of learning from scratch, growing efforts have been devoted to incorporating the benefits of pre-training. However, how to adaptively exploit the pre-trained knowledge for each incremental task while maintaining its generalizability remains an open question. In this work, we present an extensive analysis for continua",
    "arxiv_url": "https://arxiv.org/abs/2303.05118v4",
    "pdf_url": "https://arxiv.org/pdf/2303.05118v4",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2303.05118",
    "arxiv_authors": [
      "Gengwei Zhang",
      "Liyuan Wang",
      "Guoliang Kang",
      "Ling Chen",
      "Yunchao Wei"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=SLCA%3A+Slow+Learner+with+Classifier+Alignment+for+Continual+Learning+on+a+Pre-trained+Model+Gengwei+Zhang+Liyuan+Wang+Guoliang+Kang+Ling+Chen+Yunchao+Wei",
    "gs_search_success": true,
    "gs_authors": [
      "UAgdoY4AAAAJ",
      "qL9Csv0AAAAJ",
      "YcikIekAAAAJ",
      "L5aYWQcAAAAJ",
      "P24HCsgAAAAJ"
    ],
    "citation_count": 219,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2502.19568",
    "title": "PhenoProfiler: Advancing Phenotypic Learning for Image-based Drug Discovery",
    "year": 2025,
    "published": "2025-02-26T21:20:43Z",
    "categories": [
      "cs.LG",
      "cs.CV",
      "eess.IV"
    ],
    "abstract": "In the field of image-based drug discovery, capturing the phenotypic response of cells to various drug treatments and perturbations is a crucial step. However, existing methods require computationally extensive and complex multi-step procedures, which can introduce inefficiencies, limit generalizability, and increase potential errors. To address these challenges, we present PhenoProfiler, an innovative model designed to efficiently and effectively extract morphological representations, enabling ",
    "arxiv_url": "https://arxiv.org/abs/2502.19568v1",
    "pdf_url": "https://arxiv.org/pdf/2502.19568v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2502.19568",
    "arxiv_authors": [
      "Bo Li",
      "Bob Zhang",
      "Chengyang Zhang",
      "Minghao Zhou",
      "Weiliang Huang",
      "Shihang Wang",
      "Qing Wang",
      "Mengran Li",
      "Yong Zhang",
      "Qianqian Song"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=PhenoProfiler%3A+Advancing+Phenotypic+Learning+for+Image-based+Drug+Discovery+Bo+Li+Bob+Zhang+Chengyang+Zhang+Minghao+Zhou+Weiliang+Huang",
    "gs_search_success": true,
    "gs_authors": [
      "AJXeD2wAAAAJ",
      "Gc69Cj4AAAAJ",
      "y1myk_IAAAAJ",
      "v5hefysAAAAJ",
      "wIYviKIAAAAJ",
      "MLff8bQAAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 10
  },
  {
    "arxiv_id": "2307.16410",
    "title": "HiREN: Towards Higher Supervision Quality for Better Scene Text Image Super-Resolution",
    "year": 2023,
    "published": "2023-07-31T05:32:57Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Scene text image super-resolution (STISR) is an important pre-processing technique for text recognition from low-resolution scene images. Nowadays, various methods have been proposed to extract text-specific information from high-resolution (HR) images to supervise STISR model training. However, due to uncontrollable factors (e.g. shooting equipment, focus, and environment) in manually photographing HR images, the quality of HR images cannot be guaranteed, which unavoidably impacts STISR perform",
    "arxiv_url": "https://arxiv.org/abs/2307.16410v1",
    "pdf_url": "https://arxiv.org/pdf/2307.16410v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2307.16410",
    "arxiv_authors": [
      "Minyi Zhao",
      "Yi Xu",
      "Bingjia Li",
      "Jie Wang",
      "Jihong Guan",
      "Shuigeng Zhou"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=HiREN%3A+Towards+Higher+Supervision+Quality+for+Better+Scene+Text+Image+Super-Resolution+Minyi+Zhao+Yi+Xu+Bingjia+Li+Jie+Wang+Jihong+Guan",
    "gs_search_success": true,
    "gs_authors": [
      "N2xsxV8AAAAJ",
      "yAE-Av4AAAAJ",
      "JfwCR6YAAAAJ",
      "AhJEwYAAAAAJ"
    ],
    "citation_count": 4,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2401.14425",
    "title": "No Longer Trending on Artstation: Prompt Analysis of Generative AI Art",
    "year": 2024,
    "published": "2024-01-24T08:03:13Z",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CV",
      "cs.CY",
      "cs.NE"
    ],
    "abstract": "Image generation using generative AI is rapidly becoming a major new source of visual media, with billions of AI generated images created using diffusion models such as Stable Diffusion and Midjourney over the last few years. In this paper we collect and analyse over 3 million prompts and the images they generate. Using natural language processing, topic analysis and visualisation methods we aim to understand collectively how people are using text prompts, the impact of these systems on artists,",
    "arxiv_url": "https://arxiv.org/abs/2401.14425v1",
    "pdf_url": "https://arxiv.org/pdf/2401.14425v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2401.14425",
    "arxiv_authors": [
      "Jon McCormack",
      "Maria Teresa Llano",
      "Stephen James Krol",
      "Nina Rajcic"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=No+Longer+Trending+on+Artstation%3A+Prompt+Analysis+of+Generative+AI+Art+Jon+McCormack+Maria+Teresa+Llano+Stephen+James+Krol+Nina+Rajcic",
    "gs_search_success": true,
    "gs_authors": [
      "RMl-CJUAAAAJ",
      "AyFhjxAAAAAJ",
      "-wN-sFUAAAAJ"
    ],
    "citation_count": 22,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2410.20880",
    "title": "Evaluating Sugarcane Yield Variability with UAV-Derived Cane Height under Different Water and Nitrogen Conditions",
    "year": 2024,
    "published": "2024-10-28T10:00:56Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "This study investigates the relationship between sugarcane yield and cane height derived under different water and nitrogen conditions from pre-harvest Digital Surface Model (DSM) obtained via Unmanned Aerial Vehicle (UAV) flights over a sugarcane test farm. The farm was divided into 62 blocks based on three water levels (low, medium, and high) and three nitrogen levels (low, medium, and high), with repeated treatments. In pixel distribution of DSM for each block, it provided bimodal distributio",
    "arxiv_url": "https://arxiv.org/abs/2410.20880v1",
    "pdf_url": "https://arxiv.org/pdf/2410.20880v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2410.20880",
    "arxiv_authors": [
      "Rajiv Ranjan",
      "Tejasavi Birdh",
      "Nandan Mandal",
      "Dinesh Kumar",
      "Shashank Tamaskar"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Evaluating+Sugarcane+Yield+Variability+with+UAV-Derived+Cane+Height+under+Different+Water+and+Nitrogen+Conditions+Rajiv+Ranjan+Tejasavi+Birdh+Nandan+Mandal+Dinesh+Kumar+Shashank+Tamaskar",
    "gs_search_success": true,
    "gs_authors": [
      "xLyCBFwAAAAJ",
      "CLbTFjAAAAAJ",
      "4SEF19AAAAAJ"
    ],
    "citation_count": 2,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2409.12379",
    "title": "Enhancing 3D Robotic Vision Robustness by Minimizing Adversarial Mutual Information through a Curriculum Training Approach",
    "year": 2024,
    "published": "2024-09-19T00:44:45Z",
    "categories": [
      "cs.CV",
      "cs.IT",
      "cs.RO"
    ],
    "abstract": "Adversarial attacks exploit vulnerabilities in a model's decision boundaries through small, carefully crafted perturbations that lead to significant mispredictions. In 3D vision, the high dimensionality and sparsity of data greatly expand the attack surface, making 3D vision particularly vulnerable for safety-critical robotics. To enhance 3D vision's adversarial robustness, we propose a training objective that simultaneously minimizes prediction loss and mutual information (MI) under adversarial",
    "arxiv_url": "https://arxiv.org/abs/2409.12379v1",
    "pdf_url": "https://arxiv.org/pdf/2409.12379v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2409.12379",
    "arxiv_authors": [
      "Nastaran Darabi",
      "Dinithi Jayasuriya",
      "Devashri Naik",
      "Theja Tulabandhula",
      "Amit Ranjan Trivedi"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Enhancing+3D+Robotic+Vision+Robustness+by+Minimizing+Adversarial+Mutual+Information+through+a+Curriculum+Training+Approach+Nastaran+Darabi+Dinithi+Jayasuriya+Devashri+Naik+Theja+Tulabandhula+Amit+Ranjan+Trivedi",
    "gs_search_success": true,
    "gs_authors": [
      "K6FIDzYAAAAJ",
      "Thpd0HkAAAAJ",
      "ZKdsKvQAAAAJ",
      "CP6S12cAAAAJ"
    ],
    "citation_count": 4,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2306.09527",
    "title": "Leveraging Human Salience to Improve Calorie Estimation",
    "year": 2023,
    "published": "2023-06-15T22:13:26Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "The following paper investigates the effectiveness of incorporating human salience into the task of calorie prediction from images of food. We observe a 32.2% relative improvement when incorporating saliency maps on the images of food highlighting the most calorie regions. We also attempt to further improve the accuracy by starting the best models using pre-trained weights on similar tasks of mass estimation and food classification. However, we observe no improvement. Surprisingly, we also find ",
    "arxiv_url": "https://arxiv.org/abs/2306.09527v1",
    "pdf_url": "https://arxiv.org/pdf/2306.09527v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2306.09527",
    "arxiv_authors": [
      "Katherine R. Dearstyne",
      "Alberto D. Rodriguez"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Leveraging+Human+Salience+to+Improve+Calorie+Estimation+Katherine+R.+Dearstyne+Alberto+D.+Rodriguez",
    "gs_search_success": true,
    "gs_authors": [
      "pG5E3ykAAAAJ",
      "Whr42jYAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 2
  },
  {
    "arxiv_id": "2406.17688",
    "title": "Unified Auto-Encoding with Masked Diffusion",
    "year": 2024,
    "published": "2024-06-25T16:24:34Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "At the core of both successful generative and self-supervised representation learning models there is a reconstruction objective that incorporates some form of image corruption. Diffusion models implement this approach through a scheduled Gaussian corruption process, while masked auto-encoder models do so by masking patches of the image. Despite their different approaches, the underlying similarity in their methodologies suggests a promising avenue for an auto-encoder capable of both de-noising ",
    "arxiv_url": "https://arxiv.org/abs/2406.17688v1",
    "pdf_url": "https://arxiv.org/pdf/2406.17688v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2406.17688",
    "arxiv_authors": [
      "Philippe Hansen-Estruch",
      "Sriram Vishwanath",
      "Amy Zhang",
      "Manan Tomar"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Unified+Auto-Encoding+with+Masked+Diffusion+Philippe+Hansen-Estruch+Sriram+Vishwanath+Amy+Zhang+Manan+Tomar",
    "gs_search_success": true,
    "gs_authors": [
      "SOB-2hQAAAAJ",
      "UzjHQLcAAAAJ",
      "mXtH1UYAAAAJ",
      "kamjbL0AAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2411.18068",
    "title": "PersonaCraft: Personalized and Controllable Full-Body Multi-Human Scene Generation Using Occlusion-Aware 3D-Conditioned Diffusion",
    "year": 2024,
    "published": "2024-11-27T05:41:15Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "We present PersonaCraft, a framework for controllable and occlusion-robust full-body personalized image synthesis of multiple individuals in complex scenes. Current methods struggle with occlusion-heavy scenarios and complete body personalization, as 2D pose conditioning lacks 3D geometry, often leading to ambiguous occlusions and anatomical distortions, and many approaches focus solely on facial identity. In contrast, our PersonaCraft integrates diffusion models with 3D human modeling, employin",
    "arxiv_url": "https://arxiv.org/abs/2411.18068v2",
    "pdf_url": "https://arxiv.org/pdf/2411.18068v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2411.18068",
    "arxiv_authors": [
      "Gwanghyun Kim",
      "Suh Yoon Jeon",
      "Seunggyu Lee",
      "Se Young Chun"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=PersonaCraft%3A+Personalized+and+Controllable+Full-Body+Multi-Human+Scene+Generation+Using+Occlusion-Aware+3D-Conditioned+Diffusion+Gwanghyun+Kim+Suh+Yoon+Jeon+Seunggyu+Lee+Se+Young+Chun",
    "gs_search_success": true,
    "gs_authors": [
      "SCLtNC4AAAAJ",
      "aIWPv7YAAAAJ",
      "3jLuG64AAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2505.03844",
    "title": "From Spaceborne to Airborne: SAR Image Synthesis Using Foundation Models for Multi-Scale Adaptation",
    "year": 2025,
    "published": "2025-05-05T09:33:06Z",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "abstract": "The availability of Synthetic Aperture Radar (SAR) satellite imagery has increased considerably in recent years, with datasets commercially available. However, the acquisition of high-resolution SAR images in airborne configurations, remains costly and limited. Thus, the lack of open source, well-labeled, or easily exploitable SAR text-image datasets is a barrier to the use of existing foundation models in remote sensing applications. In this context, synthetic image generation is a promising so",
    "arxiv_url": "https://arxiv.org/abs/2505.03844v2",
    "pdf_url": "https://arxiv.org/pdf/2505.03844v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2505.03844",
    "arxiv_authors": [
      "Solene Debuysere",
      "Nicolas Trouve",
      "Nathan Letheule",
      "Olivier Leveque",
      "Elise Colin"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=From+Spaceborne+to+Airborne%3A+SAR+Image+Synthesis+Using+Foundation+Models+for+Multi-Scale+Adaptation+Solene+Debuysere+Nicolas+Trouve+Nathan+Letheule+Olivier+Leveque+Elise+Colin",
    "gs_search_success": false,
    "gs_authors": [],
    "citation_count": null,
    "gs_author_count": null,
    "error_type": "fetch_failed"
  },
  {
    "arxiv_id": "2503.18034",
    "title": "Expanding the Boundaries of Vision Prior Knowledge in Multi-modal Large Language Models",
    "year": 2025,
    "published": "2025-03-23T11:33:09Z",
    "categories": [
      "cs.CV",
      "cs.CL"
    ],
    "abstract": "Does the prior knowledge of the vision encoder constrain the capability boundary of Multi-modal Large Language Models (MLLMs)? While most existing research treats MLLMs as unified systems optimized through end-to-end training, the impact of vision encoder's prior knowledge is seldom investigated. In this work, we introduce a novel metric, $Rank_e$, to quantify the effect of prior knowledge of the vision encoder on MLLM performance. Our analysis reveals a positive correlation between prior knowle",
    "arxiv_url": "https://arxiv.org/abs/2503.18034v2",
    "pdf_url": "https://arxiv.org/pdf/2503.18034v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2503.18034",
    "arxiv_authors": [
      "Qiao Liang",
      "Yanjiang Liu",
      "Weixiang Zhou",
      "Ben He",
      "Yaojie Lu",
      "Hongyu Lin",
      "Jia Zheng",
      "Xianpei Han",
      "Le Sun",
      "Yingfei Sun"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Expanding+the+Boundaries+of+Vision+Prior+Knowledge+in+Multi-modal+Large+Language+Models+Qiao+Liang+Yanjiang+Liu+Weixiang+Zhou+Ben+He+Yaojie+Lu",
    "gs_search_success": true,
    "gs_authors": [
      "ResyWZoAAAAJ",
      "pA88bm4AAAAJ",
      "6bFNhtwAAAAJ",
      "s0mCSI0AAAAJ",
      "3W1-QaMAAAAJ",
      "mu5lLakAAAAJ",
      "EmTFUdQAAAAJ",
      "4B5v1ywAAAAJ"
    ],
    "citation_count": 2,
    "gs_author_count": 10
  },
  {
    "arxiv_id": "2401.10113",
    "title": "Exposing Lip-syncing Deepfakes from Mouth Inconsistencies",
    "year": 2024,
    "published": "2024-01-18T16:35:37Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "A lip-syncing deepfake is a digitally manipulated video in which a person's lip movements are created convincingly using AI models to match altered or entirely new audio. Lip-syncing deepfakes are a dangerous type of deepfakes as the artifacts are limited to the lip region and more difficult to discern. In this paper, we describe a novel approach, LIP-syncing detection based on mouth INConsistency (LIPINC), for lip-syncing deepfake detection by identifying temporal inconsistencies in the mouth r",
    "arxiv_url": "https://arxiv.org/abs/2401.10113v2",
    "pdf_url": "https://arxiv.org/pdf/2401.10113v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2401.10113",
    "arxiv_authors": [
      "Soumyya Kanti Datta",
      "Shan Jia",
      "Siwei Lyu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Exposing+Lip-syncing+Deepfakes+from+Mouth+Inconsistencies+Soumyya+Kanti+Datta+Shan+Jia+Siwei+Lyu",
    "gs_search_success": true,
    "gs_authors": [
      "R-SyM4YAAAAJ",
      "wefAEM4AAAAJ",
      "YTAkshQAAAAJ"
    ],
    "citation_count": 26,
    "gs_author_count": null
  },
  {
    "arxiv_id": "2407.07518",
    "title": "Multi-modal Crowd Counting via a Broker Modality",
    "year": 2024,
    "published": "2024-07-10T10:13:11Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Multi-modal crowd counting involves estimating crowd density from both visual and thermal/depth images. This task is challenging due to the significant gap between these distinct modalities. In this paper, we propose a novel approach by introducing an auxiliary broker modality and on this basis frame the task as a triple-modal learning problem. We devise a fusion-based method to generate this broker modality, leveraging a non-diffusion, lightweight counterpart of modern denoising diffusion-based",
    "arxiv_url": "https://arxiv.org/abs/2407.07518v1",
    "pdf_url": "https://arxiv.org/pdf/2407.07518v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2407.07518",
    "arxiv_authors": [
      "Haoliang Meng",
      "Xiaopeng Hong",
      "Chenhao Wang",
      "Miao Shang",
      "Wangmeng Zuo"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Multi-modal+Crowd+Counting+via+a+Broker+Modality+Haoliang+Meng+Xiaopeng+Hong+Chenhao+Wang+Miao+Shang+Wangmeng+Zuo",
    "gs_search_success": false,
    "gs_authors": [],
    "citation_count": null,
    "gs_author_count": null,
    "error_type": "fetch_failed"
  },
  {
    "arxiv_id": "2308.01613",
    "title": "Real-time Light Estimation and Neural Soft Shadows for AR Indoor Scenarios",
    "year": 2023,
    "published": "2023-08-03T08:41:37Z",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "abstract": "We present a pipeline for realistic embedding of virtual objects into footage of indoor scenes with focus on real-time AR applications. Our pipeline consists of two main components: A light estimator and a neural soft shadow texture generator. Our light estimation is based on deep neural nets and determines the main light direction, light color, ambient color and an opacity parameter for the shadow texture. Our neural soft shadow method encodes object-based realistic soft shadows as light direct",
    "arxiv_url": "https://arxiv.org/abs/2308.01613v1",
    "pdf_url": "https://arxiv.org/pdf/2308.01613v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2308.01613",
    "arxiv_authors": [
      "Alexander Sommer",
      "Ulrich Schwanecke",
      "Elmar Schömer"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Real-time+Light+Estimation+and+Neural+Soft+Shadows+for+AR+Indoor+Scenarios+Alexander+Sommer+Ulrich+Schwanecke+Elmar+Sch%C3%B6mer",
    "gs_search_success": true,
    "gs_authors": [
      "rENFJiYAAAAJ",
      "PkXXc7kAAAAJ",
      "bhi-vkQAAAAJ"
    ],
    "citation_count": 2,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2404.13904",
    "title": "Deep Regression Representation Learning with Topology",
    "year": 2024,
    "published": "2024-04-22T06:28:41Z",
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "abstract": "Most works studying representation learning focus only on classification and neglect regression. Yet, the learning objectives and, therefore, the representation topologies of the two tasks are fundamentally different: classification targets class separation, leading to disconnected representations, whereas regression requires ordinality with respect to the target, leading to continuous representations. We thus wonder how the effectiveness of a regression representation is influenced by its topol",
    "arxiv_url": "https://arxiv.org/abs/2404.13904v4",
    "pdf_url": "https://arxiv.org/pdf/2404.13904v4",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2404.13904",
    "arxiv_authors": [
      "Shihao Zhang",
      "kenji kawaguchi",
      "Angela Yao"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Deep+Regression+Representation+Learning+with+Topology+Shihao+Zhang+kenji+kawaguchi+Angela+Yao",
    "gs_search_success": true,
    "gs_authors": [
      "-LJCZMMAAAAJ",
      "SqS4w6gAAAAJ"
    ],
    "citation_count": 3,
    "gs_author_count": 2
  },
  {
    "arxiv_id": "2312.16451",
    "title": "Domain Generalization with Vital Phase Augmentation",
    "year": 2023,
    "published": "2023-12-27T07:35:17Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Deep neural networks have shown remarkable performance in image classification. However, their performance significantly deteriorates with corrupted input data. Domain generalization methods have been proposed to train robust models against out-of-distribution data. Data augmentation in the frequency domain is one of such approaches that enable a model to learn phase features to establish domain-invariant representations. This approach changes the amplitudes of the input data while preserving th",
    "arxiv_url": "https://arxiv.org/abs/2312.16451v3",
    "pdf_url": "https://arxiv.org/pdf/2312.16451v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2312.16451",
    "arxiv_authors": [
      "Ingyun Lee",
      "Wooju Lee",
      "Hyun Myung"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Domain+Generalization+with+Vital+Phase+Augmentation+Ingyun+Lee+Wooju+Lee+Hyun+Myung",
    "gs_search_success": true,
    "gs_authors": [
      "NrWfJ1gAAAAJ",
      "7Oo6f_cAAAAJ",
      "ChN2UUkAAAAJ"
    ],
    "citation_count": 13,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2312.05179",
    "title": "Video-Based Rendering Techniques: A Survey",
    "year": 2023,
    "published": "2023-12-08T17:03:35Z",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "abstract": "Three-dimensional reconstruction of events recorded on images has been a common challenge between computer vision and computer graphics for a long time. Estimating the real position of objects and surfaces using vision as an input is no trivial task and has been approached in several different ways. Although huge progress has been made so far, there are several open issues to which an answer is needed. The use of videos as an input for a rendering process (video-based rendering, VBR) is somethin",
    "arxiv_url": "https://arxiv.org/abs/2312.05179v1",
    "pdf_url": "https://arxiv.org/pdf/2312.05179v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2312.05179",
    "arxiv_authors": [
      "Rafael Kuffner dos Anjos",
      "João Madeiras Pereira",
      "José Antonio Gaspar"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Video-Based+Rendering+Techniques%3A+A+Survey+Rafael+Kuffner+dos+Anjos+Jo%C3%A3o+Madeiras+Pereira+Jos%C3%A9+Antonio+Gaspar",
    "gs_search_success": true,
    "gs_authors": [
      "kNDBE9kAAAAJ",
      "2qxiV5IAAAAJ",
      "iVM2NhkAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": null
  },
  {
    "arxiv_id": "2503.17359",
    "title": "Position: Interactive Generative Video as Next-Generation Game Engine",
    "year": 2025,
    "published": "2025-03-21T17:59:22Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Modern game development faces significant challenges in creativity and cost due to predetermined content in traditional game engines. Recent breakthroughs in video generation models, capable of synthesizing realistic and interactive virtual environments, present an opportunity to revolutionize game creation. In this position paper, we propose Interactive Generative Video (IGV) as the foundation for Generative Game Engines (GGE), enabling unlimited novel content generation in next-generation gami",
    "arxiv_url": "https://arxiv.org/abs/2503.17359v2",
    "pdf_url": "https://arxiv.org/pdf/2503.17359v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2503.17359",
    "arxiv_authors": [
      "Jiwen Yu",
      "Yiran Qin",
      "Haoxuan Che",
      "Quande Liu",
      "Xintao Wang",
      "Pengfei Wan",
      "Di Zhang",
      "Xihui Liu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Position%3A+Interactive+Generative+Video+as+Next-Generation+Game+Engine+Jiwen+Yu+Yiran+Qin+Haoxuan+Che+Quande+Liu+Xintao+Wang",
    "gs_search_success": true,
    "gs_authors": [
      "uoRPLHIAAAAJ",
      "FQgZpQoAAAAJ",
      "P6MraaYAAAAJ",
      "4YL23GMAAAAJ",
      "rCvK7tcAAAAJ"
    ],
    "citation_count": 7,
    "gs_author_count": 8
  },
  {
    "arxiv_id": "2502.02182",
    "title": "Sequence models for continuous cell cycle stage prediction from brightfield images",
    "year": 2025,
    "published": "2025-02-04T09:57:17Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Understanding cell cycle dynamics is crucial for studying biological processes such as growth, development and disease progression. While fluorescent protein reporters like the Fucci system allow live monitoring of cell cycle phases, they require genetic engineering and occupy additional fluorescence channels, limiting broader applicability in complex experiments. In this study, we conduct a comprehensive evaluation of deep learning methods for predicting continuous Fucci signals using non-fluor",
    "arxiv_url": "https://arxiv.org/abs/2502.02182v1",
    "pdf_url": "https://arxiv.org/pdf/2502.02182v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2502.02182",
    "arxiv_authors": [
      "Louis-Alexandre Leger",
      "Maxine Leonardi",
      "Andrea Salati",
      "Felix Naef",
      "Martin Weigert"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Sequence+models+for+continuous+cell+cycle+stage+prediction+from+brightfield+images+Louis-Alexandre+Leger+Maxine+Leonardi+Andrea+Salati+Felix+Naef+Martin+Weigert",
    "gs_search_success": false,
    "gs_authors": [],
    "citation_count": null,
    "gs_author_count": null,
    "error_type": "fetch_failed"
  },
  {
    "arxiv_id": "2412.19853",
    "title": "Conditional Balance: Improving Multi-Conditioning Trade-Offs in Image Generation",
    "year": 2024,
    "published": "2024-12-25T17:51:55Z",
    "categories": [
      "cs.CV",
      "cs.GR",
      "cs.LG"
    ],
    "abstract": "Balancing content fidelity and artistic style is a pivotal challenge in image generation. While traditional style transfer methods and modern Denoising Diffusion Probabilistic Models (DDPMs) strive to achieve this balance, they often struggle to do so without sacrificing either style, content, or sometimes both. This work addresses this challenge by analyzing the ability of DDPMs to maintain content and style equilibrium. We introduce a novel method to identify sensitivities within the DDPM atte",
    "arxiv_url": "https://arxiv.org/abs/2412.19853v2",
    "pdf_url": "https://arxiv.org/pdf/2412.19853v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2412.19853",
    "arxiv_authors": [
      "Nadav Z. Cohen",
      "Oron Nir",
      "Ariel Shamir"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Conditional+Balance%3A+Improving+Multi-Conditioning+Trade-Offs+in+Image+Generation+Nadav+Z.+Cohen+Oron+Nir+Ariel+Shamir",
    "gs_search_success": true,
    "gs_authors": [
      "fq-Ja3wAAAAJ",
      "-q90a0EAAAAJ",
      "f4v5HMgAAAAJ"
    ],
    "citation_count": 2,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2306.09132",
    "title": "Enlarged Large Margin Loss for Imbalanced Classification",
    "year": 2023,
    "published": "2023-06-15T13:44:49Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "We propose a novel loss function for imbalanced classification. LDAM loss, which minimizes a margin-based generalization bound, is widely utilized for class-imbalanced image classification. Although, by using LDAM loss, it is possible to obtain large margins for the minority classes and small margins for the majority classes, the relevance to a large margin, which is included in the original softmax cross entropy loss, is not be clarified yet. In this study, we reconvert the formula of LDAM loss",
    "arxiv_url": "https://arxiv.org/abs/2306.09132v1",
    "pdf_url": "https://arxiv.org/pdf/2306.09132v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2306.09132",
    "arxiv_authors": [
      "Sota Kato",
      "Kazuhiro Hotta"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Enlarged+Large+Margin+Loss+for+Imbalanced+Classification+Sota+Kato+Kazuhiro+Hotta",
    "gs_search_success": true,
    "gs_authors": [
      "_iVNYVgAAAAJ"
    ],
    "citation_count": 6,
    "gs_author_count": 2
  },
  {
    "arxiv_id": "2501.12489",
    "title": "Large-image Object Detection for Fine-grained Recognition of Punches Patterns in Medieval Panel Painting",
    "year": 2025,
    "published": "2025-01-21T20:30:51Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "abstract": "The attribution of the author of an art piece is typically a laborious manual process, usually relying on subjective evaluations of expert figures. However, there are some situations in which quantitative features of the artwork can support these evaluations. The extraction of these features can sometimes be automated, for instance, with the use of Machine Learning (ML) techniques. An example of these features is represented by repeated, mechanically impressed patterns, called punches, present c",
    "arxiv_url": "https://arxiv.org/abs/2501.12489v2",
    "pdf_url": "https://arxiv.org/pdf/2501.12489v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2501.12489",
    "arxiv_authors": [
      "Josh Bruegger",
      "Diana Ioana Catana",
      "Vanja Macovaz",
      "Matias Valdenegro-Toro",
      "Matthia Sabatelli",
      "Marco Zullich"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Large-image+Object+Detection+for+Fine-grained+Recognition+of+Punches+Patterns+in+Medieval+Panel+Painting+Josh+Bruegger+Diana+Ioana+Catana+Vanja+Macovaz+Matias+Valdenegro-Toro+Matthia+Sabatelli",
    "gs_search_success": true,
    "gs_authors": [
      "YO2Php8AAAAJ",
      "Z3OF878AAAAJ",
      "Fm0Ax7YAAAAJ",
      "2JZ9OxoAAAAJ",
      "Rf03qLsAAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2403.09996",
    "title": "MEDPNet: Achieving High-Precision Adaptive Registration for Complex Die Castings",
    "year": 2024,
    "published": "2024-03-15T03:42:38Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Due to their complex spatial structure and diverse geometric features, achieving high-precision and robust point cloud registration for complex Die Castings has been a significant challenge in the die-casting industry. Existing point cloud registration methods primarily optimize network models using well-established high-quality datasets, often neglecting practical application in real scenarios. To address this gap, this paper proposes a high-precision adaptive registration method called Multisc",
    "arxiv_url": "https://arxiv.org/abs/2403.09996v1",
    "pdf_url": "https://arxiv.org/pdf/2403.09996v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2403.09996",
    "arxiv_authors": [
      "Yu Du",
      "Yu Song",
      "Ce Guo",
      "Xiaojing Tian",
      "Dong Liu",
      "Ming Cong"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=MEDPNet%3A+Achieving+High-Precision+Adaptive+Registration+for+Complex+Die+Castings+Yu+Du+Yu+Song+Ce+Guo+Xiaojing+Tian+Dong+Liu",
    "gs_search_success": true,
    "gs_authors": [],
    "citation_count": null,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2409.18337",
    "title": "Photon Inhibition for Energy-Efficient Single-Photon Imaging",
    "year": 2024,
    "published": "2024-09-26T23:19:44Z",
    "categories": [
      "eess.IV",
      "cs.CV",
      "physics.ins-det"
    ],
    "abstract": "Single-photon cameras (SPCs) are emerging as sensors of choice for various challenging imaging applications. One class of SPCs based on the single-photon avalanche diode (SPAD) detects individual photons using an avalanche process; the raw photon data can then be processed to extract scene information under extremely low light, high dynamic range, and rapid motion. Yet, single-photon sensitivity in SPADs comes at a cost -- each photon detection consumes more energy than that of a CMOS camera. Th",
    "arxiv_url": "https://arxiv.org/abs/2409.18337v1",
    "pdf_url": "https://arxiv.org/pdf/2409.18337v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2409.18337",
    "arxiv_authors": [
      "Lucas J. Koerner",
      "Shantanu Gupta",
      "Atul Ingle",
      "Mohit Gupta"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Photon+Inhibition+for+Energy-Efficient+Single-Photon+Imaging+Lucas+J.+Koerner+Shantanu+Gupta+Atul+Ingle+Mohit+Gupta",
    "gs_search_success": true,
    "gs_authors": [
      "qEKT9UcAAAAJ",
      "Y3wdd8oAAAAJ",
      "jVYxUqEAAAAJ",
      "NH-7wWMAAAAJ"
    ],
    "citation_count": 2,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2502.01286",
    "title": "Template Matching in Images using Segmented Normalized Cross-Correlation",
    "year": 2025,
    "published": "2025-02-03T11:58:33Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "In this paper, a new variant of an algorithm for normalized cross-correlation (NCC) is proposed in the context of template matching in images. The proposed algorithm is based on the precomputation of a template image approximation, enabling more efficient calculation of approximate NCC with the source image than using the original template for exact NCC calculation. The approximate template is precomputed from the template image by a split-and-merge approach, resulting in a decomposition to axis",
    "arxiv_url": "https://arxiv.org/abs/2502.01286v1",
    "pdf_url": "https://arxiv.org/pdf/2502.01286v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2502.01286",
    "arxiv_authors": [
      "Davor Marušić",
      "Siniša Popović",
      "Zoran Kalafatić"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Template+Matching+in+Images+using+Segmented+Normalized+Cross-Correlation+Davor+Maru%C5%A1i%C4%87+Sini%C5%A1a+Popovi%C4%87+Zoran+Kalafati%C4%87",
    "gs_search_success": true,
    "gs_authors": [
      "UhACgzMAAAAJ",
      "QeEZg9wAAAAJ"
    ],
    "citation_count": 2,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2412.04458",
    "title": "Cubify Anything: Scaling Indoor 3D Object Detection",
    "year": 2024,
    "published": "2024-12-05T18:59:09Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "We consider indoor 3D object detection with respect to a single RGB(-D) frame acquired from a commodity handheld device. We seek to significantly advance the status quo with respect to both data and modeling. First, we establish that existing datasets have significant limitations to scale, accuracy, and diversity of objects. As a result, we introduce the Cubify-Anything 1M (CA-1M) dataset, which exhaustively labels over 400K 3D objects on over 1K highly accurate laser-scanned scenes with near-pe",
    "arxiv_url": "https://arxiv.org/abs/2412.04458v1",
    "pdf_url": "https://arxiv.org/pdf/2412.04458v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2412.04458",
    "arxiv_authors": [
      "Justin Lazarow",
      "David Griffiths",
      "Gefen Kohavi",
      "Francisco Crespo",
      "Afshin Dehghan"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Cubify+Anything%3A+Scaling+Indoor+3D+Object+Detection+Justin+Lazarow+David+Griffiths+Gefen+Kohavi+Francisco+Crespo+Afshin+Dehghan",
    "gs_search_success": true,
    "gs_authors": [
      "xrSs8r8AAAAJ",
      "PASh6VEAAAAJ",
      "wcX-UW4AAAAJ"
    ],
    "citation_count": 16,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2305.10983",
    "title": "Assessor360: Multi-sequence Network for Blind Omnidirectional Image Quality Assessment",
    "year": 2023,
    "published": "2023-05-18T13:55:28Z",
    "categories": [
      "cs.CV",
      "eess.IV"
    ],
    "abstract": "Blind Omnidirectional Image Quality Assessment (BOIQA) aims to objectively assess the human perceptual quality of omnidirectional images (ODIs) without relying on pristine-quality image information. It is becoming more significant with the increasing advancement of virtual reality (VR) technology. However, the quality assessment of ODIs is severely hampered by the fact that the existing BOIQA pipeline lacks the modeling of the observer's browsing process. To tackle this issue, we propose a novel",
    "arxiv_url": "https://arxiv.org/abs/2305.10983v3",
    "pdf_url": "https://arxiv.org/pdf/2305.10983v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2305.10983",
    "arxiv_authors": [
      "Tianhe Wu",
      "Shuwei Shi",
      "Haoming Cai",
      "Mingdeng Cao",
      "Jing Xiao",
      "Yinqiang Zheng",
      "Yujiu Yang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Assessor360%3A+Multi-sequence+Network+for+Blind+Omnidirectional+Image+Quality+Assessment+Tianhe+Wu+Shuwei+Shi+Haoming+Cai+Mingdeng+Cao+Jing+Xiao",
    "gs_search_success": false,
    "gs_authors": [],
    "citation_count": null,
    "gs_author_count": null,
    "error_type": "fetch_failed"
  },
  {
    "arxiv_id": "2309.02043",
    "title": "Decomposed Guided Dynamic Filters for Efficient RGB-Guided Depth Completion",
    "year": 2023,
    "published": "2023-09-05T08:37:58Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "RGB-guided depth completion aims at predicting dense depth maps from sparse depth measurements and corresponding RGB images, where how to effectively and efficiently exploit the multi-modal information is a key issue. Guided dynamic filters, which generate spatially-variant depth-wise separable convolutional filters from RGB features to guide depth features, have been proven to be effective in this task. However, the dynamically generated filters require massive model parameters, computational c",
    "arxiv_url": "https://arxiv.org/abs/2309.02043v1",
    "pdf_url": "https://arxiv.org/pdf/2309.02043v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2309.02043",
    "arxiv_authors": [
      "Yufei Wang",
      "Yuxin Mao",
      "Qi Liu",
      "Yuchao Dai"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Decomposed+Guided+Dynamic+Filters+for+Efficient+RGB-Guided+Depth+Completion+Yufei+Wang+Yuxin+Mao+Qi+Liu+Yuchao+Dai",
    "gs_search_success": true,
    "gs_authors": [
      "bFbIwO8AAAAJ",
      "kDaztnkAAAAJ",
      "HPvl-ikAAAAJ",
      "fddAbqsAAAAJ"
    ],
    "citation_count": 25,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2310.00937",
    "title": "Data Efficient Training of a U-Net Based Architecture for Structured Documents Localization",
    "year": 2023,
    "published": "2023-10-02T07:05:19Z",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "Structured documents analysis and recognition are essential for modern online on-boarding processes, and document localization is a crucial step to achieve reliable key information extraction. While deep-learning has become the standard technique used to solve document analysis problems, real-world applications in industry still face the limited availability of labelled data and of computational resources when training or fine-tuning deep-learning models. To tackle these challenges, we propose S",
    "arxiv_url": "https://arxiv.org/abs/2310.00937v1",
    "pdf_url": "https://arxiv.org/pdf/2310.00937v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2310.00937",
    "arxiv_authors": [
      "Anastasiia Kabeshova",
      "Guillaume Betmont",
      "Julien Lerouge",
      "Evgeny Stepankevich",
      "Alexis Bergès"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Data+Efficient+Training+of+a+U-Net+Based+Architecture+for+Structured+Documents+Localization+Anastasiia+Kabeshova+Guillaume+Betmont+Julien+Lerouge+Evgeny+Stepankevich+Alexis+Berg%C3%A8s",
    "gs_search_success": true,
    "gs_authors": [
      "ARqUNk4AAAAJ",
      "qmL0h5cAAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2404.08347",
    "title": "Learning to Rebalance Multi-Modal Optimization by Adaptively Masking Subnetworks",
    "year": 2024,
    "published": "2024-04-12T09:22:24Z",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "Multi-modal learning aims to enhance performance by unifying models from various modalities but often faces the \"modality imbalance\" problem in real data, leading to a bias towards dominant modalities and neglecting others, thereby limiting its overall effectiveness. To address this challenge, the core idea is to balance the optimization of each modality to achieve a joint optimum. Existing approaches often employ a modal-level control mechanism for adjusting the update of each modal parameter. ",
    "arxiv_url": "https://arxiv.org/abs/2404.08347v1",
    "pdf_url": "https://arxiv.org/pdf/2404.08347v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2404.08347",
    "arxiv_authors": [
      "Yang Yang",
      "Hongpeng Pan",
      "Qing-Yuan Jiang",
      "Yi Xu",
      "Jinghui Tang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Learning+to+Rebalance+Multi-Modal+Optimization+by+Adaptively+Masking+Subnetworks+Yang+Yang+Hongpeng+Pan+Qing-Yuan+Jiang+Yi+Xu+Jinghui+Tang",
    "gs_search_success": true,
    "gs_authors": [
      "D4jEMqEAAAAJ",
      "2RtnHp4AAAAJ",
      "_6NJip0AAAAJ",
      "ByBLlEwAAAAJ",
      "cuqCEvcAAAAJ"
    ],
    "citation_count": 16,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2407.13337",
    "title": "Long-Term 3D Point Tracking By Cost Volume Fusion",
    "year": 2024,
    "published": "2024-07-18T09:34:47Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Long-term point tracking is essential to understand non-rigid motion in the physical world better. Deep learning approaches have recently been incorporated into long-term point tracking, but most prior work predominantly functions in 2D. Although these methods benefit from the well-established backbones and matching frameworks, the motions they produce do not always make sense in the 3D physical world. In this paper, we propose the first deep learning framework for long-term point tracking in 3D",
    "arxiv_url": "https://arxiv.org/abs/2407.13337v1",
    "pdf_url": "https://arxiv.org/pdf/2407.13337v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2407.13337",
    "arxiv_authors": [
      "Hung Nguyen",
      "Chanho Kim",
      "Rigved Naukarkar",
      "Li Fuxin"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Long-Term+3D+Point+Tracking+By+Cost+Volume+Fusion+Hung+Nguyen+Chanho+Kim+Rigved+Naukarkar+Li+Fuxin",
    "gs_search_success": true,
    "gs_authors": [
      "mURzVOAAAAAJ",
      "xARSfT4AAAAJ",
      "snDpfA0AAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2304.09527",
    "title": "Single-View View Synthesis with Self-Rectified Pseudo-Stereo",
    "year": 2023,
    "published": "2023-04-19T09:36:13Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Synthesizing novel views from a single view image is a highly ill-posed problem. We discover an effective solution to reduce the learning ambiguity by expanding the single-view view synthesis problem to a multi-view setting. Specifically, we leverage the reliable and explicit stereo prior to generate a pseudo-stereo viewpoint, which serves as an auxiliary input to construct the 3D space. In this way, the challenging novel view synthesis process is decoupled into two simpler problems of stereo sy",
    "arxiv_url": "https://arxiv.org/abs/2304.09527v2",
    "pdf_url": "https://arxiv.org/pdf/2304.09527v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2304.09527",
    "arxiv_authors": [
      "Yang Zhou",
      "Hanjie Wu",
      "Wenxi Liu",
      "Zheng Xiong",
      "Jing Qin",
      "Shengfeng He"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Single-View+View+Synthesis+with+Self-Rectified+Pseudo-Stereo+Yang+Zhou+Hanjie+Wu+Wenxi+Liu+Zheng+Xiong+Jing+Qin",
    "gs_search_success": true,
    "gs_authors": [
      "JB9F4LoAAAAJ",
      "k3DLdIEAAAAJ",
      "rBWnK8wAAAAJ",
      "Y310sT0AAAAJ"
    ],
    "citation_count": 9,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2312.12649",
    "title": "Surf-CDM: Score-Based Surface Cold-Diffusion Model For Medical Image Segmentation",
    "year": 2023,
    "published": "2023-12-19T22:50:02Z",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "abstract": "Diffusion models have shown impressive performance for image generation, often times outperforming other generative models. Since their introduction, researchers have extended the powerful noise-to-image denoising pipeline to discriminative tasks, including image segmentation. In this work we propose a conditional score-based generative modeling framework for medical image segmentation which relies on a parametric surface representation for the segmentation masks. The surface re-parameterization",
    "arxiv_url": "https://arxiv.org/abs/2312.12649v1",
    "pdf_url": "https://arxiv.org/pdf/2312.12649v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2312.12649",
    "arxiv_authors": [
      "Fahim Ahmed Zaman",
      "Mathews Jacob",
      "Amanda Chang",
      "Kan Liu",
      "Milan Sonka",
      "Xiaodong Wu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Surf-CDM%3A+Score-Based+Surface+Cold-Diffusion+Model+For+Medical+Image+Segmentation+Fahim+Ahmed+Zaman+Mathews+Jacob+Amanda+Chang+Kan+Liu+Milan+Sonka",
    "gs_search_success": false,
    "gs_authors": [],
    "citation_count": null,
    "gs_author_count": null,
    "error_type": "fetch_failed"
  },
  {
    "arxiv_id": "2403.12047",
    "title": "Alpha-wolves and Alpha-mammals: Exploring Dictionary Attacks on Iris Recognition Systems",
    "year": 2023,
    "published": "2023-11-20T22:00:55Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "A dictionary attack in a biometric system entails the use of a small number of strategically generated images or templates to successfully match with a large number of identities, thereby compromising security. We focus on dictionary attacks at the template level, specifically the IrisCodes used in iris recognition systems. We present an hitherto unknown vulnerability wherein we mix IrisCodes using simple bitwise operators to generate alpha-mixtures - alpha-wolves (combining a set of \"wolf\" samp",
    "arxiv_url": "https://arxiv.org/abs/2403.12047v1",
    "pdf_url": "https://arxiv.org/pdf/2403.12047v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2403.12047",
    "arxiv_authors": [
      "Sudipta Banerjee",
      "Anubhav Jain",
      "Zehua Jiang",
      "Nasir Memon",
      "Julian Togelius",
      "Arun Ross"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Alpha-wolves+and+Alpha-mammals%3A+Exploring+Dictionary+Attacks+on+Iris+Recognition+Systems+Sudipta+Banerjee+Anubhav+Jain+Zehua+Jiang+Nasir+Memon+Julian+Togelius",
    "gs_search_success": true,
    "gs_authors": [
      "N6yADnMAAAAJ",
      "lr4I9BwAAAAJ",
      "Pz0YttAAAAAJ",
      "6GnwSSYAAAAJ",
      "7IiUQDkAAAAJ",
      "wKRFTrMAAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2407.11348",
    "title": "Flatfish Lesion Detection Based on Part Segmentation Approach and Lesion Image Generation",
    "year": 2024,
    "published": "2024-07-16T03:32:10Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "The flatfish is a major farmed species consumed globally in large quantities. However, due to the densely populated farming environment, flatfish are susceptible to lesions and diseases, making early lesion detection crucial. Traditionally, lesions were detected through visual inspection, but observing large numbers of fish is challenging. Automated approaches based on deep learning technologies have been widely used to address this problem, but accurate detection remains difficult due to the di",
    "arxiv_url": "https://arxiv.org/abs/2407.11348v2",
    "pdf_url": "https://arxiv.org/pdf/2407.11348v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2407.11348",
    "arxiv_authors": [
      "Seo-Bin Hwang",
      "Han-Young Kim",
      "Chae-Yeon Heo",
      "Hie-Yong Jeong",
      "Sung-Ju Jung",
      "Yeong-Jun Cho"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Flatfish+Lesion+Detection+Based+on+Part+Segmentation+Approach+and+Lesion+Image+Generation+Seo-Bin+Hwang+Han-Young+Kim+Chae-Yeon+Heo+Hie-Yong+Jeong+Sung-Ju+Jung",
    "gs_search_success": true,
    "gs_authors": [
      "F-k-eDEAAAAJ",
      "683W_MkAAAAJ",
      "uW0hwnAAAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2405.02512",
    "title": "SatSwinMAE: Efficient Autoencoding for Multiscale Time-series Satellite Imagery",
    "year": 2024,
    "published": "2024-05-03T22:55:56Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Recent advancements in foundation models have significantly impacted various fields, including natural language processing, computer vision, and multi-modal tasks. One area that stands to benefit greatly is Earth observation, where these models can efficiently process large-scale, unlabeled geospatial data. In this work we extend the SwinMAE model to integrate temporal information for satellite time-series data. The architecture employs a hierarchical 3D Masked Autoencoder (MAE) with Video Swin ",
    "arxiv_url": "https://arxiv.org/abs/2405.02512v2",
    "pdf_url": "https://arxiv.org/pdf/2405.02512v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2405.02512",
    "arxiv_authors": [
      "Yohei Nakayama",
      "Jiawei Su",
      "Luis M. Pazos-Outón"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=SatSwinMAE%3A+Efficient+Autoencoding+for+Multiscale+Time-series+Satellite+Imagery+Yohei+Nakayama+Jiawei+Su+Luis+M.+Pazos-Out%C3%B3n",
    "gs_search_success": true,
    "gs_authors": [
      "YtgWrvsAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2412.10804",
    "title": "Medical Manifestation-Aware De-Identification",
    "year": 2024,
    "published": "2024-12-14T12:09:41Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Face de-identification (DeID) has been widely studied for common scenes, but remains under-researched for medical scenes, mostly due to the lack of large-scale patient face datasets. In this paper, we release MeMa, consisting of over 40,000 photo-realistic patient faces. MeMa is re-generated from massive real patient photos. By carefully modulating the generation and data-filtering procedures, MeMa avoids breaching real patient privacy, while ensuring rich and plausible medical manifestations. W",
    "arxiv_url": "https://arxiv.org/abs/2412.10804v1",
    "pdf_url": "https://arxiv.org/pdf/2412.10804v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2412.10804",
    "arxiv_authors": [
      "Yuan Tian",
      "Shuo Wang",
      "Guangtao Zhai"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Medical+Manifestation-Aware+De-Identification+Yuan+Tian+Shuo+Wang+Guangtao+Zhai",
    "gs_search_success": false,
    "gs_authors": [],
    "citation_count": null,
    "gs_author_count": null,
    "error_type": "fetch_failed"
  },
  {
    "arxiv_id": "2502.20220",
    "title": "Avat3r: Large Animatable Gaussian Reconstruction Model for High-fidelity 3D Head Avatars",
    "year": 2025,
    "published": "2025-02-27T16:00:11Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Traditionally, creating photo-realistic 3D head avatars requires a studio-level multi-view capture setup and expensive optimization during test-time, limiting the use of digital human doubles to the VFX industry or offline renderings.   To address this shortcoming, we present Avat3r, which regresses a high-quality and animatable 3D head avatar from just a few input images, vastly reducing compute requirements during inference. More specifically, we make Large Reconstruction Models animatable and",
    "arxiv_url": "https://arxiv.org/abs/2502.20220v2",
    "pdf_url": "https://arxiv.org/pdf/2502.20220v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2502.20220",
    "arxiv_authors": [
      "Tobias Kirschstein",
      "Javier Romero",
      "Artem Sevastopolsky",
      "Matthias Nießner",
      "Shunsuke Saito"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Avat3r%3A+Large+Animatable+Gaussian+Reconstruction+Model+for+High-fidelity+3D+Head+Avatars+Tobias+Kirschstein+Javier+Romero+Artem+Sevastopolsky+Matthias+Nie%C3%9Fner+Shunsuke+Saito",
    "gs_search_success": true,
    "gs_authors": [
      "eUtEs6YAAAAJ",
      "IolN_okAAAAJ",
      "rri0OT0AAAAJ",
      "Wx62iOsAAAAJ",
      "fTSCTYQAAAAJ"
    ],
    "citation_count": 6,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2312.16039",
    "title": "Dual-scale Enhanced and Cross-generative Consistency Learning for Semi-supervised Medical Image Segmentation",
    "year": 2023,
    "published": "2023-12-26T12:56:31Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Medical image segmentation plays a crucial role in computer-aided diagnosis. However, existing methods heavily rely on fully supervised training, which requires a large amount of labeled data with time-consuming pixel-wise annotations. Moreover, accurately segmenting lesions poses challenges due to variations in shape, size, and location. To address these issues, we propose a novel Dual-scale Enhanced and Cross-generative consistency learning framework for semi-supervised medical image Segmentat",
    "arxiv_url": "https://arxiv.org/abs/2312.16039v2",
    "pdf_url": "https://arxiv.org/pdf/2312.16039v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2312.16039",
    "arxiv_authors": [
      "Yunqi Gu",
      "Tao Zhou",
      "Yizhe Zhang",
      "Yi Zhou",
      "Kelei He",
      "Chen Gong",
      "Huazhu Fu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Dual-scale+Enhanced+and+Cross-generative+Consistency+Learning+for+Semi-supervised+Medical+Image+Segmentation+Yunqi+Gu+Tao+Zhou+Yizhe+Zhang+Yi+Zhou+Kelei+He",
    "gs_search_success": true,
    "gs_authors": [
      "jCvUBYMAAAAJ",
      "guttoBwAAAAJ",
      "LPPsgWUAAAAJ",
      "aceABmIAAAAJ",
      "0Do_BMIAAAAJ"
    ],
    "citation_count": 14,
    "gs_author_count": 7
  },
  {
    "arxiv_id": "2304.04540",
    "title": "FreConv: Frequency Branch-and-Integration Convolutional Networks",
    "year": 2023,
    "published": "2023-04-10T12:24:14Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Recent researches indicate that utilizing the frequency information of input data can enhance the performance of networks. However, the existing popular convolutional structure is not designed specifically for utilizing the frequency information contained in datasets. In this paper, we propose a novel and effective module, named FreConv (frequency branch-and-integration convolution), to replace the vanilla convolution. FreConv adopts a dual-branch architecture to extract and integrate high- and ",
    "arxiv_url": "https://arxiv.org/abs/2304.04540v1",
    "pdf_url": "https://arxiv.org/pdf/2304.04540v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2304.04540",
    "arxiv_authors": [
      "Zhaowen Li",
      "Xu Zhao",
      "Peigeng Ding",
      "Zongxin Gao",
      "Yuting Yang",
      "Ming Tang",
      "Jinqiao Wang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=FreConv%3A+Frequency+Branch-and-Integration+Convolutional+Networks+Zhaowen+Li+Xu+Zhao+Peigeng+Ding+Zongxin+Gao+Yuting+Yang",
    "gs_search_success": true,
    "gs_authors": [
      "6D0X8SQAAAAJ",
      "A1__8cUAAAAJ",
      "7_BkyxEAAAAJ"
    ],
    "citation_count": 4,
    "gs_author_count": 7
  },
  {
    "arxiv_id": "2312.08366",
    "title": "See, Say, and Segment: Teaching LMMs to Overcome False Premises",
    "year": 2023,
    "published": "2023-12-13T18:58:04Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Current open-source Large Multimodal Models (LMMs) excel at tasks such as open-vocabulary language grounding and segmentation but can suffer under false premises when queries imply the existence of something that is not actually present in the image. We observe that existing methods that fine-tune an LMM to segment images significantly degrade their ability to reliably determine (\"see\") if an object is present and to interact naturally with humans (\"say\"), a form of catastrophic forgetting. In t",
    "arxiv_url": "https://arxiv.org/abs/2312.08366v1",
    "pdf_url": "https://arxiv.org/pdf/2312.08366v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2312.08366",
    "arxiv_authors": [
      "Tsung-Han Wu",
      "Giscard Biamby",
      "David Chan",
      "Lisa Dunlap",
      "Ritwik Gupta",
      "Xudong Wang",
      "Joseph E. Gonzalez",
      "Trevor Darrell"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=See%2C+Say%2C+and+Segment%3A+Teaching+LMMs+to+Overcome+False+Premises+Tsung-Han+Wu+Giscard+Biamby+David+Chan+Lisa+Dunlap+Ritwik+Gupta",
    "gs_search_success": true,
    "gs_authors": [
      "B96GkdgAAAAJ",
      "ykuVSuEAAAAJ",
      "4Cdwp_MAAAAJ",
      "bh-uRFMAAAAJ",
      "Azf07WcAAAAJ",
      "s0Fof5IAAAAJ",
      "qa4M89wAAAAJ",
      "Vy16O5UAAAAJ"
    ],
    "citation_count": 33,
    "gs_author_count": 8
  },
  {
    "arxiv_id": "2306.10346",
    "title": "Fast Fourier Inception Networks for Occluded Video Prediction",
    "year": 2023,
    "published": "2023-06-17T13:27:29Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Video prediction is a pixel-level task that generates future frames by employing the historical frames. There often exist continuous complex motions, such as object overlapping and scene occlusion in video, which poses great challenges to this task. Previous works either fail to well capture the long-term temporal dynamics or do not handle the occlusion masks. To address these issues, we develop the fully convolutional Fast Fourier Inception Networks for video prediction, termed \\textit{FFINet},",
    "arxiv_url": "https://arxiv.org/abs/2306.10346v1",
    "pdf_url": "https://arxiv.org/pdf/2306.10346v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2306.10346",
    "arxiv_authors": [
      "Ping Li",
      "Chenhan Zhang",
      "Xianghua Xu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Fast+Fourier+Inception+Networks+for+Occluded+Video+Prediction+Ping+Li+Chenhan+Zhang+Xianghua+Xu",
    "gs_search_success": true,
    "gs_authors": [
      "hKnHfogAAAAJ"
    ],
    "citation_count": 14,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2312.16470",
    "title": "ReSynthDetect: A Fundus Anomaly Detection Network with Reconstruction and Synthetic Features",
    "year": 2023,
    "published": "2023-12-27T08:40:23Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Detecting anomalies in fundus images through unsupervised methods is a challenging task due to the similarity between normal and abnormal tissues, as well as their indistinct boundaries. The current methods have limitations in accurately detecting subtle anomalies while avoiding false positives. To address these challenges, we propose the ReSynthDetect network which utilizes a reconstruction network for modeling normal images, and an anomaly generator that produces synthetic anomalies consistent",
    "arxiv_url": "https://arxiv.org/abs/2312.16470v1",
    "pdf_url": "https://arxiv.org/pdf/2312.16470v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2312.16470",
    "arxiv_authors": [
      "Jingqi Niu",
      "Qinji Yu",
      "Shiwen Dong",
      "Zilong Wang",
      "Kang Dang",
      "Xiaowei Ding"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=ReSynthDetect%3A+A+Fundus+Anomaly+Detection+Network+with+Reconstruction+and+Synthetic+Features+Jingqi+Niu+Qinji+Yu+Shiwen+Dong+Zilong+Wang+Kang+Dang",
    "gs_search_success": true,
    "gs_authors": [
      "gOaxHvMAAAAJ",
      "M80j6WIAAAAJ",
      "LzZJR4kAAAAJ"
    ],
    "citation_count": 2,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2311.00213",
    "title": "Consistent Video-to-Video Transfer Using Synthetic Dataset",
    "year": 2023,
    "published": "2023-11-01T01:20:12Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "We introduce a novel and efficient approach for text-based video-to-video editing that eliminates the need for resource-intensive per-video-per-model finetuning. At the core of our approach is a synthetic paired video dataset tailored for video-to-video transfer tasks. Inspired by Instruct Pix2Pix's image transfer via editing instruction, we adapt this paradigm to the video domain. Extending the Prompt-to-Prompt to videos, we efficiently generate paired samples, each with an input video and its ",
    "arxiv_url": "https://arxiv.org/abs/2311.00213v3",
    "pdf_url": "https://arxiv.org/pdf/2311.00213v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2311.00213",
    "arxiv_authors": [
      "Jiaxin Cheng",
      "Tianjun Xiao",
      "Tong He"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Consistent+Video-to-Video+Transfer+Using+Synthetic+Dataset+Jiaxin+Cheng+Tianjun+Xiao+Tong+He",
    "gs_search_success": false,
    "gs_authors": [],
    "citation_count": null,
    "gs_author_count": null,
    "error_type": "fetch_failed"
  },
  {
    "arxiv_id": "2404.14768",
    "title": "Enhancing Prompt Following with Visual Control Through Training-Free Mask-Guided Diffusion",
    "year": 2024,
    "published": "2024-04-23T06:10:43Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Recently, integrating visual controls into text-to-image~(T2I) models, such as ControlNet method, has received significant attention for finer control capabilities. While various training-free methods make efforts to enhance prompt following in T2I models, the issue with visual control is still rarely studied, especially in the scenario that visual controls are misaligned with text prompts. In this paper, we address the challenge of ``Prompt Following With Visual Control\" and propose a training-",
    "arxiv_url": "https://arxiv.org/abs/2404.14768v1",
    "pdf_url": "https://arxiv.org/pdf/2404.14768v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2404.14768",
    "arxiv_authors": [
      "Hongyu Chen",
      "Yiqi Gao",
      "Min Zhou",
      "Peng Wang",
      "Xubin Li",
      "Tiezheng Ge",
      "Bo Zheng"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Enhancing+Prompt+Following+with+Visual+Control+Through+Training-Free+Mask-Guided+Diffusion+Hongyu+Chen+Yiqi+Gao+Min+Zhou+Peng+Wang+Xubin+Li",
    "gs_search_success": true,
    "gs_authors": [
      "A7xfbIYAAAAJ",
      "3gHhO9QAAAAJ",
      "db5ZTlMAAAAJ",
      "P2BO-aUAAAAJ"
    ],
    "citation_count": 7,
    "gs_author_count": 7
  },
  {
    "arxiv_id": "2303.14175",
    "title": "Inherent Consistent Learning for Accurate Semi-supervised Medical Image Segmentation",
    "year": 2023,
    "published": "2023-03-24T17:38:03Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Semi-supervised medical image segmentation has attracted much attention in recent years because of the high cost of medical image annotations. In this paper, we propose a novel Inherent Consistent Learning (ICL) method, aims to learn robust semantic category representations through the semantic consistency guidance of labeled and unlabeled data to help segmentation. In practice, we introduce two external modules, namely Supervised Semantic Proxy Adaptor (SSPA) and Unsupervised Semantic Consisten",
    "arxiv_url": "https://arxiv.org/abs/2303.14175v4",
    "pdf_url": "https://arxiv.org/pdf/2303.14175v4",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2303.14175",
    "arxiv_authors": [
      "Ye Zhu",
      "Jie Yang",
      "Si-Qi Liu",
      "Ruimao Zhang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Inherent+Consistent+Learning+for+Accurate+Semi-supervised+Medical+Image+Segmentation+Ye+Zhu+Jie+Yang+Si-Qi+Liu+Ruimao+Zhang",
    "gs_search_success": true,
    "gs_authors": [
      "xdAReagAAAAJ",
      "t3UmkKIAAAAJ",
      "ZJwZdtgAAAAJ",
      "UVzG9IcAAAAJ"
    ],
    "citation_count": 20,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2412.09936",
    "title": "CaLoRAify: Calorie Estimation with Visual-Text Pairing and LoRA-Driven Visual Language Models",
    "year": 2024,
    "published": "2024-12-13T07:51:32Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "The obesity phenomenon, known as the heavy issue, is a leading cause of preventable chronic diseases worldwide. Traditional calorie estimation tools often rely on specific data formats or complex pipelines, limiting their practicality in real-world scenarios. Recently, vision-language models (VLMs) have excelled in understanding real-world contexts and enabling conversational interactions, making them ideal for downstream tasks such as ingredient analysis. However, applying VLMs to calorie estim",
    "arxiv_url": "https://arxiv.org/abs/2412.09936v1",
    "pdf_url": "https://arxiv.org/pdf/2412.09936v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2412.09936",
    "arxiv_authors": [
      "Dongyu Yao",
      "Keling Yao",
      "Junhong Zhou",
      "Yinghao Zhang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=CaLoRAify%3A+Calorie+Estimation+with+Visual-Text+Pairing+and+LoRA-Driven+Visual+Language+Models+Dongyu+Yao+Keling+Yao+Junhong+Zhou+Yinghao+Zhang",
    "gs_search_success": true,
    "gs_authors": [
      "_TOxPGYAAAAJ",
      "8sEyEXAAAAAJ",
      "DdknfDwAAAAJ",
      "d2kAaQgAAAAJ"
    ],
    "citation_count": 6,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2501.18592",
    "title": "Advances in Multimodal Adaptation and Generalization: From Traditional Approaches to Foundation Models",
    "year": 2025,
    "published": "2025-01-30T18:59:36Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "cs.RO"
    ],
    "abstract": "In real-world scenarios, achieving domain adaptation and generalization poses significant challenges, as models must adapt to or generalize across unknown target distributions. Extending these capabilities to unseen multimodal distributions, i.e., multimodal domain adaptation and generalization, is even more challenging due to the distinct characteristics of different modalities. Significant progress has been made over the years, with applications ranging from action recognition to semantic segm",
    "arxiv_url": "https://arxiv.org/abs/2501.18592v4",
    "pdf_url": "https://arxiv.org/pdf/2501.18592v4",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2501.18592",
    "arxiv_authors": [
      "Hao Dong",
      "Moru Liu",
      "Kaiyang Zhou",
      "Eleni Chatzi",
      "Juho Kannala",
      "Cyrill Stachniss",
      "Olga Fink"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Advances+in+Multimodal+Adaptation+and+Generalization%3A+From+Traditional+Approaches+to+Foundation+Models+Hao+Dong+Moru+Liu+Kaiyang+Zhou+Eleni+Chatzi+Juho+Kannala",
    "gs_search_success": true,
    "gs_authors": [
      "8vib2lAAAAAJ",
      "5jcoGEIAAAAJ",
      "c4mWQPQAAAAJ",
      "2n9Mwt8AAAAJ",
      "gRIejugAAAAJ",
      "eAcIoUgAAAAJ"
    ],
    "citation_count": 14,
    "gs_author_count": 7
  },
  {
    "arxiv_id": "2505.15504",
    "title": "Beyond Linearity: Squeeze-and-Recalibrate Blocks for Few-Shot Whole Slide Image Classification",
    "year": 2025,
    "published": "2025-05-21T13:24:47Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Deep learning has advanced computational pathology but expert annotations remain scarce. Few-shot learning mitigates annotation burdens yet suffers from overfitting and discriminative feature mischaracterization. In addition, the current few-shot multiple instance learning (MIL) approaches leverage pretrained vision-language models to alleviate these issues, but at the cost of complex preprocessing and high computational cost. We propose a Squeeze-and-Recalibrate (SR) block, a drop-in replacemen",
    "arxiv_url": "https://arxiv.org/abs/2505.15504v1",
    "pdf_url": "https://arxiv.org/pdf/2505.15504v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2505.15504",
    "arxiv_authors": [
      "Conghao Xiong",
      "Zhengrui Guo",
      "Zhe Xu",
      "Yifei Zhang",
      "Raymond Kai-Yu Tong",
      "Si Yong Yeo",
      "Hao Chen",
      "Joseph J. Y. Sung",
      "Irwin King"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Beyond+Linearity%3A+Squeeze-and-Recalibrate+Blocks+for+Few-Shot+Whole+Slide+Image+Classification+Conghao+Xiong+Zhengrui+Guo+Zhe+Xu+Yifei+Zhang+Raymond+Kai-Yu+Tong",
    "gs_search_success": true,
    "gs_authors": [
      "OXKSw-cAAAAJ",
      "K-EoeRgAAAAJ",
      "MXvC7tkAAAAJ",
      "9Nl-HmMAAAAJ",
      "c__9eLUAAAAJ",
      "agHXXD4AAAAJ",
      "Z_t5DjwAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 9
  },
  {
    "arxiv_id": "2306.10380",
    "title": "Development of a Deep Learning System for Intra-Operative Identification of Cancer Metastases",
    "year": 2023,
    "published": "2023-06-17T15:41:11Z",
    "categories": [
      "eess.IV",
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "For several cancer patients, operative resection with curative intent can end up in early recurrence of the cancer. Current limitations in peri-operative cancer staging and especially intra-operative misidentification of visible metastases is likely the main reason leading to unnecessary operative interventions in the affected individuals. Here, we evaluate whether an artificial intelligence (AI) system can improve recognition of peritoneal surface metastases on routine staging laparoscopy image",
    "arxiv_url": "https://arxiv.org/abs/2306.10380v1",
    "pdf_url": "https://arxiv.org/pdf/2306.10380v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2306.10380",
    "arxiv_authors": [
      "Thomas Schnelldorfer",
      "Janil Castro",
      "Atoussa Goldar-Najafi",
      "Liping Liu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Development+of+a+Deep+Learning+System+for+Intra-Operative+Identification+of+Cancer+Metastases+Thomas+Schnelldorfer+Janil+Castro+Atoussa+Goldar-Najafi+Liping+Liu",
    "gs_search_success": true,
    "gs_authors": [
      "0CiW2Z0AAAAJ",
      "aFaGunAAAAAJ"
    ],
    "citation_count": 10,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2404.16421",
    "title": "SynCellFactory: Generative Data Augmentation for Cell Tracking",
    "year": 2024,
    "published": "2024-04-25T08:51:59Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Cell tracking remains a pivotal yet challenging task in biomedical research. The full potential of deep learning for this purpose is often untapped due to the limited availability of comprehensive and varied training data sets. In this paper, we present SynCellFactory, a generative cell video augmentation. At the heart of SynCellFactory lies the ControlNet architecture, which has been fine-tuned to synthesize cell imagery with photorealistic accuracy in style and motion patterns. This technique ",
    "arxiv_url": "https://arxiv.org/abs/2404.16421v2",
    "pdf_url": "https://arxiv.org/pdf/2404.16421v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2404.16421",
    "arxiv_authors": [
      "Moritz Sturm",
      "Lorenzo Cerrone",
      "Fred A. Hamprecht"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=SynCellFactory%3A+Generative+Data+Augmentation+for+Cell+Tracking+Moritz+Sturm+Lorenzo+Cerrone+Fred+A.+Hamprecht",
    "gs_search_success": true,
    "gs_authors": [
      "0eIrB2EAAAAJ",
      "lO62bt0AAAAJ",
      "vv8wa_QAAAAJ"
    ],
    "citation_count": 5,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2403.17631",
    "title": "AniArtAvatar: Animatable 3D Art Avatar from a Single Image",
    "year": 2024,
    "published": "2024-03-26T12:08:04Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "We present a novel approach for generating animatable 3D-aware art avatars from a single image, with controllable facial expressions, head poses, and shoulder movements. Unlike previous reenactment methods, our approach utilizes a view-conditioned 2D diffusion model to synthesize multi-view images from a single art portrait with a neutral expression. With the generated colors and normals, we synthesize a static avatar using an SDF-based neural surface. For avatar animation, we extract control po",
    "arxiv_url": "https://arxiv.org/abs/2403.17631v1",
    "pdf_url": "https://arxiv.org/pdf/2403.17631v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2403.17631",
    "arxiv_authors": [
      "Shaoxu Li"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=AniArtAvatar%3A+Animatable+3D+Art+Avatar+from+a+Single+Image+Shaoxu+Li",
    "gs_search_success": false,
    "gs_authors": [],
    "citation_count": null,
    "gs_author_count": null,
    "error_type": "fetch_failed"
  },
  {
    "arxiv_id": "2410.09453",
    "title": "MMAD: A Comprehensive Benchmark for Multimodal Large Language Models in Industrial Anomaly Detection",
    "year": 2024,
    "published": "2024-10-12T09:16:09Z",
    "categories": [
      "cs.AI",
      "cs.CV"
    ],
    "abstract": "In the field of industrial inspection, Multimodal Large Language Models (MLLMs) have a high potential to renew the paradigms in practical applications due to their robust language capabilities and generalization abilities. However, despite their impressive problem-solving skills in many domains, MLLMs' ability in industrial anomaly detection has not been systematically studied. To bridge this gap, we present MMAD, the first-ever full-spectrum MLLMs benchmark in industrial Anomaly Detection. We d",
    "arxiv_url": "https://arxiv.org/abs/2410.09453v3",
    "pdf_url": "https://arxiv.org/pdf/2410.09453v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2410.09453",
    "arxiv_authors": [
      "Xi Jiang",
      "Jian Li",
      "Hanqiu Deng",
      "Yong Liu",
      "Bin-Bin Gao",
      "Yifeng Zhou",
      "Jialin Li",
      "Chengjie Wang",
      "Feng Zheng"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=MMAD%3A+A+Comprehensive+Benchmark+for+Multimodal+Large+Language+Models+in+Industrial+Anomaly+Detection+Xi+Jiang+Jian+Li+Hanqiu+Deng+Yong+Liu+Bin-Bin+Gao",
    "gs_search_success": true,
    "gs_authors": [
      "ACb5C40AAAAJ",
      "yYviZ-oAAAAJ",
      "PcmyXHMAAAAJ",
      "fqte5H4AAAAJ",
      "XtU4nDYAAAAJ",
      "nmNQjgIAAAAJ"
    ],
    "citation_count": 21,
    "gs_author_count": 9
  },
  {
    "arxiv_id": "2412.14123",
    "title": "AnySat: One Earth Observation Model for Many Resolutions, Scales, and Modalities",
    "year": 2024,
    "published": "2024-12-18T18:11:53Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Geospatial models must adapt to the diversity of Earth observation data in terms of resolutions, scales, and modalities. However, existing approaches expect fixed input configurations, which limits their practical applicability. We propose AnySat, a multimodal model based on joint embedding predictive architecture (JEPA) and scale-adaptive spatial encoders, allowing us to train a single model on highly heterogeneous data in a self-supervised manner. To demonstrate the advantages of this unified ",
    "arxiv_url": "https://arxiv.org/abs/2412.14123v3",
    "pdf_url": "https://arxiv.org/pdf/2412.14123v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2412.14123",
    "arxiv_authors": [
      "Guillaume Astruc",
      "Nicolas Gonthier",
      "Clement Mallet",
      "Loic Landrieu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=AnySat%3A+One+Earth+Observation+Model+for+Many+Resolutions%2C+Scales%2C+and+Modalities+Guillaume+Astruc+Nicolas+Gonthier+Clement+Mallet+Loic+Landrieu",
    "gs_search_success": true,
    "gs_authors": [
      "B9VnFRcAAAAJ",
      "y6a-lk0AAAAJ",
      "t9u0QSEAAAAJ",
      "1XoK-YMAAAAJ"
    ],
    "citation_count": 34,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2412.05012",
    "title": "SAMCL: Empowering SAM to Continually Learn from Dynamic Domains",
    "year": 2024,
    "published": "2024-12-06T13:05:50Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Segment Anything Model (SAM) struggles with segmenting objects in the open world, especially across diverse and dynamic domains. Continual segmentation (CS) is a potential technique to solve this issue, but a significant obstacle is the intractable balance between previous domains (stability) and new domains (plasticity) during CS. Furthermore, how to utilize two kinds of features of SAM, images and prompts, in an efficient and effective CS manner remains a significant hurdle. In this work, we p",
    "arxiv_url": "https://arxiv.org/abs/2412.05012v1",
    "pdf_url": "https://arxiv.org/pdf/2412.05012v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2412.05012",
    "arxiv_authors": [
      "Zeqing Wang",
      "Kangye Ji",
      "Di Wang",
      "Fei Cheng"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=SAMCL%3A+Empowering+SAM+to+Continually+Learn+from+Dynamic+Domains+Zeqing+Wang+Kangye+Ji+Di+Wang+Fei+Cheng",
    "gs_search_success": true,
    "gs_authors": [
      "bezEJJYAAAAJ",
      "N1Y-n5EAAAAJ",
      "hr33fY0AAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2305.05991",
    "title": "DMNR: Unsupervised De-noising of Point Clouds Corrupted by Airborne Particles",
    "year": 2023,
    "published": "2023-05-10T08:58:54Z",
    "categories": [
      "cs.CV",
      "eess.IV"
    ],
    "abstract": "LiDAR sensors are critical for autonomous driving and robotics applications due to their ability to provide accurate range measurements and their robustness to lighting conditions. However, airborne particles, such as fog, rain, snow, and dust, will degrade its performance and it is inevitable to encounter these inclement environmental conditions outdoors. It would be a straightforward approach to remove them by supervised semantic segmentation. But annotating these particles point wisely is too",
    "arxiv_url": "https://arxiv.org/abs/2305.05991v1",
    "pdf_url": "https://arxiv.org/pdf/2305.05991v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2305.05991",
    "arxiv_authors": [
      "Chu Chen",
      "Yanqi Ma",
      "Bingcheng Dong",
      "Junjie Cao"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=DMNR%3A+Unsupervised+De-noising+of+Point+Clouds+Corrupted+by+Airborne+Particles+Chu+Chen+Yanqi+Ma+Bingcheng+Dong+Junjie+Cao",
    "gs_search_success": true,
    "gs_authors": [
      "QP_BDK0AAAAJ"
    ],
    "citation_count": 2,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2301.04802",
    "title": "Diffusion-based Data Augmentation for Skin Disease Classification: Impact Across Original Medical Datasets to Fully Synthetic Images",
    "year": 2023,
    "published": "2023-01-12T04:22:23Z",
    "categories": [
      "cs.LG",
      "cs.CV",
      "eess.IV"
    ],
    "abstract": "Despite continued advancement in recent years, deep neural networks still rely on large amounts of training data to avoid overfitting. However, labeled training data for real-world applications such as healthcare is limited and difficult to access given longstanding privacy, and strict data sharing policies. By manipulating image datasets in the pixel or feature space, existing data augmentation techniques represent one of the effective ways to improve the quantity and diversity of training data",
    "arxiv_url": "https://arxiv.org/abs/2301.04802v1",
    "pdf_url": "https://arxiv.org/pdf/2301.04802v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2301.04802",
    "arxiv_authors": [
      "Mohamed Akrout",
      "Bálint Gyepesi",
      "Péter Holló",
      "Adrienn Poór",
      "Blága Kincső",
      "Stephen Solis",
      "Katrina Cirone",
      "Jeremy Kawahara",
      "Dekker Slade",
      "Latif Abid",
      "Máté Kovács",
      "István Fazekas"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Diffusion-based+Data+Augmentation+for+Skin+Disease+Classification%3A+Impact+Across+Original+Medical+Datasets+to+Fully+Synthetic+Images+Mohamed+Akrout+B%C3%A1lint+Gyepesi+P%C3%A9ter+Holl%C3%B3+Adrienn+Po%C3%B3r+Bl%C3%A1ga+Kincs%C5%91",
    "gs_search_success": false,
    "gs_authors": [],
    "citation_count": null,
    "gs_author_count": null,
    "error_type": "fetch_failed"
  },
  {
    "arxiv_id": "2408.01827",
    "title": "ST-SACLF: Style Transfer Informed Self-Attention Classifier for Bias-Aware Painting Classification",
    "year": 2024,
    "published": "2024-08-03T17:31:58Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Painting classification plays a vital role in organizing, finding, and suggesting artwork for digital and classic art galleries. Existing methods struggle with adapting knowledge from the real world to artistic images during training, leading to poor performance when dealing with different datasets. Our innovation lies in addressing these challenges through a two-step process. First, we generate more data using Style Transfer with Adaptive Instance Normalization (AdaIN), bridging the gap between",
    "arxiv_url": "https://arxiv.org/abs/2408.01827v1",
    "pdf_url": "https://arxiv.org/pdf/2408.01827v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2408.01827",
    "arxiv_authors": [
      "Mridula Vijendran",
      "Frederick W. B. Li",
      "Jingjing Deng",
      "Hubert P. H. Shum"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=ST-SACLF%3A+Style+Transfer+Informed+Self-Attention+Classifier+for+Bias-Aware+Painting+Classification+Mridula+Vijendran+Frederick+W.+B.+Li+Jingjing+Deng+Hubert+P.+H.+Shum",
    "gs_search_success": true,
    "gs_authors": [
      "V9XyRX8AAAAJ",
      "CrnPVKwAAAAJ",
      "AVemHcIAAAAJ",
      "pkPLCEYAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2306.10082",
    "title": "DreamCatcher: Revealing the Language of the Brain with fMRI using GPT Embedding",
    "year": 2023,
    "published": "2023-06-16T07:55:20Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "The human brain possesses remarkable abilities in visual processing, including image recognition and scene summarization. Efforts have been made to understand the cognitive capacities of the visual brain, but a comprehensive understanding of the underlying mechanisms still needs to be discovered. Advancements in brain decoding techniques have led to sophisticated approaches like fMRI-to-Image reconstruction, which has implications for cognitive neuroscience and medical imaging. However, challeng",
    "arxiv_url": "https://arxiv.org/abs/2306.10082v1",
    "pdf_url": "https://arxiv.org/pdf/2306.10082v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2306.10082",
    "arxiv_authors": [
      "Subhrasankar Chatterjee",
      "Debasis Samanta"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=DreamCatcher%3A+Revealing+the+Language+of+the+Brain+with+fMRI+using+GPT+Embedding+Subhrasankar+Chatterjee+Debasis+Samanta",
    "gs_search_success": true,
    "gs_authors": [
      "BuIEB3QAAAAJ",
      "RctLVqcAAAAJ"
    ],
    "citation_count": 7,
    "gs_author_count": 2
  },
  {
    "arxiv_id": "2403.02601",
    "title": "Low-Res Leads the Way: Improving Generalization for Super-Resolution by Self-Supervised Learning",
    "year": 2024,
    "published": "2024-03-05T02:29:18Z",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "abstract": "For image super-resolution (SR), bridging the gap between the performance on synthetic datasets and real-world degradation scenarios remains a challenge. This work introduces a novel \"Low-Res Leads the Way\" (LWay) training framework, merging Supervised Pre-training with Self-supervised Learning to enhance the adaptability of SR models to real-world images. Our approach utilizes a low-resolution (LR) reconstruction network to extract degradation embeddings from LR images, merging them with super-",
    "arxiv_url": "https://arxiv.org/abs/2403.02601v1",
    "pdf_url": "https://arxiv.org/pdf/2403.02601v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2403.02601",
    "arxiv_authors": [
      "Haoyu Chen",
      "Wenbo Li",
      "Jinjin Gu",
      "Jingjing Ren",
      "Haoze Sun",
      "Xueyi Zou",
      "Zhensong Zhang",
      "Youliang Yan",
      "Lei Zhu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Low-Res+Leads+the+Way%3A+Improving+Generalization+for+Super-Resolution+by+Self-Supervised+Learning+Haoyu+Chen+Wenbo+Li+Jinjin+Gu+Jingjing+Ren+Haoze+Sun",
    "gs_search_success": true,
    "gs_authors": [
      "AQtqhaYAAAAJ",
      "KWbcBucAAAAJ",
      "0ua28KoAAAAJ",
      "fs8HQxQAAAAJ",
      "foGn_TIAAAAJ",
      "uMQ-G-QAAAAJ",
      "JPUwfAMAAAAJ",
      "aB2KirIAAAAJ",
      "wcuqACgAAAAJ"
    ],
    "citation_count": 32,
    "gs_author_count": 9
  },
  {
    "arxiv_id": "2310.01405",
    "title": "Representation Engineering: A Top-Down Approach to AI Transparency",
    "year": 2023,
    "published": "2023-10-02T17:59:07Z",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.CV",
      "cs.CY"
    ],
    "abstract": "In this paper, we identify and characterize the emerging area of representation engineering (RepE), an approach to enhancing the transparency of AI systems that draws on insights from cognitive neuroscience. RepE places population-level representations, rather than neurons or circuits, at the center of analysis, equipping us with novel methods for monitoring and manipulating high-level cognitive phenomena in deep neural networks (DNNs). We provide baselines and an initial analysis of RepE techni",
    "arxiv_url": "https://arxiv.org/abs/2310.01405v4",
    "pdf_url": "https://arxiv.org/pdf/2310.01405v4",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2310.01405",
    "arxiv_authors": [
      "Andy Zou",
      "Long Phan",
      "Sarah Chen",
      "James Campbell",
      "Phillip Guo",
      "Richard Ren",
      "Alexander Pan",
      "Xuwang Yin",
      "Mantas Mazeika",
      "Ann-Kathrin Dombrowski",
      "Shashwat Goel",
      "Nathaniel Li",
      "Michael J. Byun",
      "Zifan Wang",
      "Alex Mallen",
      "Steven Basart",
      "Sanmi Koyejo",
      "Dawn Song",
      "Matt Fredrikson",
      "J. Zico Kolter",
      "Dan Hendrycks"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Representation+Engineering%3A+A+Top-Down+Approach+to+AI+Transparency+Andy+Zou+Long+Phan+Sarah+Chen+James+Campbell+Phillip+Guo",
    "gs_search_success": true,
    "gs_authors": [
      "htCMHnUAAAAJ",
      "fVRQn4wAAAAJ",
      "Woa18ggAAAAJ",
      "c425B6UAAAAJ",
      "IVnb6XQAAAAJ",
      "o-Vl80UAAAAJ",
      "PaltSA0AAAAJ",
      "zir09KwAAAAJ",
      "fGeEmLQAAAAJ"
    ],
    "citation_count": 627,
    "gs_author_count": 9
  },
  {
    "arxiv_id": "2303.18022",
    "title": "The Topology-Overlap Trade-Off in Retinal Arteriole-Venule Segmentation",
    "year": 2023,
    "published": "2023-03-31T13:01:05Z",
    "categories": [
      "cs.CV",
      "stat.ML"
    ],
    "abstract": "Retinal fundus images can be an invaluable diagnosis tool for screening epidemic diseases like hypertension or diabetes. And they become especially useful when the arterioles and venules they depict are clearly identified and annotated. However, manual annotation of these vessels is extremely time demanding and taxing, which calls for automatic segmentation. Although convolutional neural networks can achieve high overlap between predictions and expert annotations, they often fail to produce topo",
    "arxiv_url": "https://arxiv.org/abs/2303.18022v1",
    "pdf_url": "https://arxiv.org/pdf/2303.18022v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2303.18022",
    "arxiv_authors": [
      "Angel Victor Juanco Muller",
      "Joao F. C. Mota",
      "Keith A. Goatman",
      "Corne Hoogendoorn"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=The+Topology-Overlap+Trade-Off+in+Retinal+Arteriole-Venule+Segmentation+Angel+Victor+Juanco+Muller+Joao+F.+C.+Mota+Keith+A.+Goatman+Corne+Hoogendoorn",
    "gs_search_success": true,
    "gs_authors": [
      "lFjaYrAAAAAJ",
      "54HbSNQAAAAJ",
      "bKkUq60AAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": null
  },
  {
    "arxiv_id": "2409.13887",
    "title": "Brain-Cognition Fingerprinting via Graph-GCCA with Contrastive Learning",
    "year": 2024,
    "published": "2024-09-20T20:36:20Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Many longitudinal neuroimaging studies aim to improve the understanding of brain aging and diseases by studying the dynamic interactions between brain function and cognition. Doing so requires accurate encoding of their multidimensional relationship while accounting for individual variability over time. For this purpose, we propose an unsupervised learning model (called \\underline{\\textbf{Co}}ntrastive Learning-based \\underline{\\textbf{Gra}}ph Generalized \\underline{\\textbf{Ca}}nonical Correlati",
    "arxiv_url": "https://arxiv.org/abs/2409.13887v1",
    "pdf_url": "https://arxiv.org/pdf/2409.13887v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2409.13887",
    "arxiv_authors": [
      "Yixin Wang",
      "Wei Peng",
      "Yu Zhang",
      "Ehsan Adeli",
      "Qingyu Zhao",
      "Kilian M. Pohl"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Brain-Cognition+Fingerprinting+via+Graph-GCCA+with+Contrastive+Learning+Yixin+Wang+Wei+Peng+Yu+Zhang+Ehsan+Adeli+Qingyu+Zhao",
    "gs_search_success": true,
    "gs_authors": [
      "TDFM0QYAAAAJ",
      "xOFOVGMAAAAJ",
      "oDrTEi0AAAAJ",
      "ykYrXtAAAAAJ",
      "7NX_J_cAAAAJ",
      "yHpHM34AAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2409.11172",
    "title": "Annealed Winner-Takes-All for Motion Forecasting",
    "year": 2024,
    "published": "2024-09-17T13:26:17Z",
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "abstract": "In autonomous driving, motion prediction aims at forecasting the future trajectories of nearby agents, helping the ego vehicle to anticipate behaviors and drive safely. A key challenge is generating a diverse set of future predictions, commonly addressed using data-driven models with Multiple Choice Learning (MCL) architectures and Winner-Takes-All (WTA) training objectives. However, these methods face initialization sensitivity and training instabilities. Additionally, to compensate for limited",
    "arxiv_url": "https://arxiv.org/abs/2409.11172v3",
    "pdf_url": "https://arxiv.org/pdf/2409.11172v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2409.11172",
    "arxiv_authors": [
      "Yihong Xu",
      "Victor Letzelter",
      "Mickaël Chen",
      "Éloi Zablocki",
      "Matthieu Cord"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Annealed+Winner-Takes-All+for+Motion+Forecasting+Yihong+Xu+Victor+Letzelter+Micka%C3%ABl+Chen+%C3%89loi+Zablocki+Matthieu+Cord",
    "gs_search_success": true,
    "gs_authors": [
      "SpAotDcAAAAJ",
      "dOkbUmEAAAAJ",
      "YhTdZh8AAAAJ",
      "QnRpMJAAAAAJ",
      "vMLRRVkAAAAJ"
    ],
    "citation_count": 2,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2301.01343",
    "title": "Explainability and Robustness of Deep Visual Classification Models",
    "year": 2023,
    "published": "2023-01-03T20:23:43Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "In the computer vision community, Convolutional Neural Networks (CNNs), first proposed in the 1980's, have become the standard visual classification model. Recently, as alternatives to CNNs, Capsule Networks (CapsNets) and Vision Transformers (ViTs) have been proposed. CapsNets, which were inspired by the information processing of the human brain, are considered to have more inductive bias than CNNs, whereas ViTs are considered to have less inductive bias than CNNs. All three classification mode",
    "arxiv_url": "https://arxiv.org/abs/2301.01343v1",
    "pdf_url": "https://arxiv.org/pdf/2301.01343v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2301.01343",
    "arxiv_authors": [
      "Jindong Gu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Explainability+and+Robustness+of+Deep+Visual+Classification+Models+Jindong+Gu",
    "gs_search_success": true,
    "gs_authors": [
      "mj3ff80AAAAJ"
    ],
    "citation_count": 5,
    "gs_author_count": null
  },
  {
    "arxiv_id": "2502.09688",
    "title": "Towards Virtual Clinical Trials of Radiology AI with Conditional Generative Modeling",
    "year": 2025,
    "published": "2025-02-13T15:53:52Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "abstract": "Artificial intelligence (AI) is poised to transform healthcare by enabling personalized and efficient care through data-driven insights. Although radiology is at the forefront of AI adoption, in practice, the potential of AI models is often overshadowed by severe failures to generalize: AI models can have performance degradation of up to 20% when transitioning from controlled test environments to clinical use by radiologists. This mismatch raises concerns that radiologists will be misled by inco",
    "arxiv_url": "https://arxiv.org/abs/2502.09688v1",
    "pdf_url": "https://arxiv.org/pdf/2502.09688v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2502.09688",
    "arxiv_authors": [
      "Benjamin D. Killeen",
      "Bohua Wan",
      "Aditya V. Kulkarni",
      "Nathan Drenkow",
      "Michael Oberst",
      "Paul H. Yi",
      "Mathias Unberath"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Towards+Virtual+Clinical+Trials+of+Radiology+AI+with+Conditional+Generative+Modeling+Benjamin+D.+Killeen+Bohua+Wan+Aditya+V.+Kulkarni+Nathan+Drenkow+Michael+Oberst",
    "gs_search_success": true,
    "gs_authors": [
      "wg_-gfAAAAAJ",
      "QX7AvxUAAAAJ",
      "tJQ2uNwAAAAJ",
      "o1IZjggAAAAJ",
      "SpbKOH0AAAAJ",
      "1bd5mdIAAAAJ",
      "ghZVzxMAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 7
  },
  {
    "arxiv_id": "2503.17097",
    "title": "R2LDM: An Efficient 4D Radar Super-Resolution Framework Leveraging Diffusion Model",
    "year": 2025,
    "published": "2025-03-21T12:30:33Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "We introduce R2LDM, an innovative approach for generating dense and accurate 4D radar point clouds, guided by corresponding LiDAR point clouds. Instead of utilizing range images or bird's eye view (BEV) images, we represent both LiDAR and 4D radar point clouds using voxel features, which more effectively capture 3D shape information. Subsequently, we propose the Latent Voxel Diffusion Model (LVDM), which performs the diffusion process in the latent space. Additionally, a novel Latent Point Cloud",
    "arxiv_url": "https://arxiv.org/abs/2503.17097v2",
    "pdf_url": "https://arxiv.org/pdf/2503.17097v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2503.17097",
    "arxiv_authors": [
      "Boyuan Zheng",
      "Shouyi Lu",
      "Renbo Huang",
      "Minqing Huang",
      "Fan Lu",
      "Wei Tian",
      "Guirong Zhuo",
      "Lu Xiong"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=R2LDM%3A+An+Efficient+4D+Radar+Super-Resolution+Framework+Leveraging+Diffusion+Model+Boyuan+Zheng+Shouyi+Lu+Renbo+Huang+Minqing+Huang+Fan+Lu",
    "gs_search_success": true,
    "gs_authors": [
      "aYKQn88AAAAJ",
      "Cc_dL88AAAAJ",
      "kRKEO4kAAAAJ"
    ],
    "citation_count": 2,
    "gs_author_count": 8
  },
  {
    "arxiv_id": "2303.09915",
    "title": "Privacy-preserving Pedestrian Tracking using Distributed 3D LiDARs",
    "year": 2023,
    "published": "2023-03-17T12:08:47Z",
    "categories": [
      "cs.CV",
      "cs.HC"
    ],
    "abstract": "The growing demand for intelligent environments unleashes an extraordinary cycle of privacy-aware applications that makes individuals' life more comfortable and safe. Examples of these applications include pedestrian tracking systems in large areas. Although the ubiquity of camera-based systems, they are not a preferable solution due to the vulnerability of leaking the privacy of pedestrians. In this paper, we introduce a novel privacy-preserving system for pedestrian tracking in smart environme",
    "arxiv_url": "https://arxiv.org/abs/2303.09915v3",
    "pdf_url": "https://arxiv.org/pdf/2303.09915v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2303.09915",
    "arxiv_authors": [
      "Masakazu Ohno",
      "Riki Ukyo",
      "Tatsuya Amano",
      "Hamada Rizk",
      "Hirozumi Yamaguchi"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Privacy-preserving+Pedestrian+Tracking+using+Distributed+3D+LiDARs+Masakazu+Ohno+Riki+Ukyo+Tatsuya+Amano+Hamada+Rizk+Hirozumi+Yamaguchi",
    "gs_search_success": true,
    "gs_authors": [
      "oksaStcAAAAJ",
      "XHjxKxQAAAAJ",
      "pUpK9hYAAAAJ"
    ],
    "citation_count": 31,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2412.13983",
    "title": "GraphAvatar: Compact Head Avatars with GNN-Generated 3D Gaussians",
    "year": 2024,
    "published": "2024-12-18T16:05:40Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Rendering photorealistic head avatars from arbitrary viewpoints is crucial for various applications like virtual reality. Although previous methods based on Neural Radiance Fields (NeRF) can achieve impressive results, they lack fidelity and efficiency. Recent methods using 3D Gaussian Splatting (3DGS) have improved rendering quality and real-time performance but still require significant storage overhead. In this paper, we introduce a method called GraphAvatar that utilizes Graph Neural Network",
    "arxiv_url": "https://arxiv.org/abs/2412.13983v1",
    "pdf_url": "https://arxiv.org/pdf/2412.13983v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2412.13983",
    "arxiv_authors": [
      "Xiaobao Wei",
      "Peng Chen",
      "Ming Lu",
      "Hui Chen",
      "Feng Tian"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=GraphAvatar%3A+Compact+Head+Avatars+with+GNN-Generated+3D+Gaussians+Xiaobao+Wei+Peng+Chen+Ming+Lu+Hui+Chen+Feng+Tian",
    "gs_search_success": true,
    "gs_authors": [
      "MGAz5PkAAAAJ"
    ],
    "citation_count": 8,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2410.10766",
    "title": "Adaptive Diffusion Terrain Generator for Autonomous Uneven Terrain Navigation",
    "year": 2024,
    "published": "2024-10-14T17:42:37Z",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV"
    ],
    "abstract": "Model-free reinforcement learning has emerged as a powerful method for developing robust robot control policies capable of navigating through complex and unstructured terrains. The effectiveness of these methods hinges on two essential elements: (1) the use of massively parallel physics simulations to expedite policy training, and (2) an environment generator tasked with crafting sufficiently challenging yet attainable terrains to facilitate continuous policy improvement. Existing methods of env",
    "arxiv_url": "https://arxiv.org/abs/2410.10766v1",
    "pdf_url": "https://arxiv.org/pdf/2410.10766v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2410.10766",
    "arxiv_authors": [
      "Youwei Yu",
      "Junhong Xu",
      "Lantao Liu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Adaptive+Diffusion+Terrain+Generator+for+Autonomous+Uneven+Terrain+Navigation+Youwei+Yu+Junhong+Xu+Lantao+Liu",
    "gs_search_success": true,
    "gs_authors": [
      "L5dHk5cAAAAJ",
      "Dhzwr0YAAAAJ",
      "wBvO3LIAAAAJ"
    ],
    "citation_count": 6,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2407.05811",
    "title": "MapsTP: HD Map Images Based Multimodal Trajectory Prediction for Automated Vehicles",
    "year": 2024,
    "published": "2024-07-08T10:45:30Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Predicting ego vehicle trajectories remains a critical challenge, especially in urban and dense areas due to the unpredictable behaviours of other vehicles and pedestrians. Multimodal trajectory prediction enhances decision-making by considering multiple possible future trajectories based on diverse sources of environmental data. In this approach, we leverage ResNet-50 to extract image features from high-definition map data and use IMU sensor data to calculate speed, acceleration, and yaw rate. ",
    "arxiv_url": "https://arxiv.org/abs/2407.05811v3",
    "pdf_url": "https://arxiv.org/pdf/2407.05811v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2407.05811",
    "arxiv_authors": [
      "Sushil Sharma",
      "Arindam Das",
      "Ganesh Sistu",
      "Mark Halton",
      "Ciarán Eising"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=MapsTP%3A+HD+Map+Images+Based+Multimodal+Trajectory+Prediction+for+Automated+Vehicles+Sushil+Sharma+Arindam+Das+Ganesh+Sistu+Mark+Halton+Ciar%C3%A1n+Eising",
    "gs_search_success": true,
    "gs_authors": [
      "aH6w8VcAAAAJ",
      "W8DTl_gAAAAJ",
      "QnJuRnEAAAAJ",
      "KoWlbIIAAAAJ",
      "356ahmwAAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2305.03944",
    "title": "Structural and Statistical Texture Knowledge Distillation for Semantic Segmentation",
    "year": 2023,
    "published": "2023-05-06T06:01:11Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Existing knowledge distillation works for semantic segmentation mainly focus on transferring high-level contextual knowledge from teacher to student. However, low-level texture knowledge is also of vital importance for characterizing the local structural pattern and global statistical property, such as boundary, smoothness, regularity and color contrast, which may not be well addressed by high-level deep features. In this paper, we are intended to take full advantage of both structural and stati",
    "arxiv_url": "https://arxiv.org/abs/2305.03944v3",
    "pdf_url": "https://arxiv.org/pdf/2305.03944v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2305.03944",
    "arxiv_authors": [
      "Deyi Ji",
      "Haoran Wang",
      "Mingyuan Tao",
      "Jianqiang Huang",
      "Xian-Sheng Hua",
      "Hongtao Lu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Structural+and+Statistical+Texture+Knowledge+Distillation+for+Semantic+Segmentation+Deyi+Ji+Haoran+Wang+Mingyuan+Tao+Jianqiang+Huang+Xian-Sheng+Hua",
    "gs_search_success": true,
    "gs_authors": [
      "6G-l4o0AAAAJ",
      "r9-7am4AAAAJ",
      "GtNuBJcAAAAJ",
      "UqAybqgAAAAJ"
    ],
    "citation_count": 100,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2406.14178",
    "title": "EvSegSNN: Neuromorphic Semantic Segmentation for Event Data",
    "year": 2024,
    "published": "2024-06-20T10:36:24Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Semantic segmentation is an important computer vision task, particularly for scene understanding and navigation of autonomous vehicles and UAVs. Several variations of deep neural network architectures have been designed to tackle this task. However, due to their huge computational costs and their high memory consumption, these models are not meant to be deployed on resource-constrained systems. To address this limitation, we introduce an end-to-end biologically inspired semantic segmentation app",
    "arxiv_url": "https://arxiv.org/abs/2406.14178v1",
    "pdf_url": "https://arxiv.org/pdf/2406.14178v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2406.14178",
    "arxiv_authors": [
      "Dalia Hareb",
      "Jean Martinet"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=EvSegSNN%3A+Neuromorphic+Semantic+Segmentation+for+Event+Data+Dalia+Hareb+Jean+Martinet",
    "gs_search_success": true,
    "gs_authors": [
      "dyxxR_gAAAAJ"
    ],
    "citation_count": 9,
    "gs_author_count": 2
  },
  {
    "arxiv_id": "2309.11109",
    "title": "Self-supervised Domain-agnostic Domain Adaptation for Satellite Images",
    "year": 2023,
    "published": "2023-09-20T07:37:23Z",
    "categories": [
      "cs.CV",
      "eess.IV"
    ],
    "abstract": "Domain shift caused by, e.g., different geographical regions or acquisition conditions is a common issue in machine learning for global scale satellite image processing. A promising method to address this problem is domain adaptation, where the training and the testing datasets are split into two or multiple domains according to their distributions, and an adaptation method is applied to improve the generalizability of the model on the testing dataset. However, defining the domain to which each ",
    "arxiv_url": "https://arxiv.org/abs/2309.11109v2",
    "pdf_url": "https://arxiv.org/pdf/2309.11109v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2309.11109",
    "arxiv_authors": [
      "Fahong Zhang",
      "Yilei Shi",
      "Xiao Xiang Zhu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Self-supervised+Domain-agnostic+Domain+Adaptation+for+Satellite+Images+Fahong+Zhang+Yilei+Shi+Xiao+Xiang+Zhu",
    "gs_search_success": true,
    "gs_authors": [
      "4ykAWEcAAAAJ",
      "4vfau8cAAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2407.08795",
    "title": "Feasibility of Neural Radiance Fields for Crime Scene Video Reconstruction",
    "year": 2024,
    "published": "2024-07-11T18:13:12Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "This paper aims to review and determine the feasibility of using variations of NeRF models in order to reconstruct crime scenes given input videos of the scene. We focus on three main innovations of NeRF when it comes to reconstructing crime scenes: Multi-object Synthesis, Deformable Synthesis, and Lighting. From there, we analyse its innovation progress against the requirements to be met in order to be able to reconstruct crime scenes with given videos of such scenes.",
    "arxiv_url": "https://arxiv.org/abs/2407.08795v1",
    "pdf_url": "https://arxiv.org/pdf/2407.08795v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2407.08795",
    "arxiv_authors": [
      "Shariq Nadeem Malik",
      "Min Hao Chee",
      "Dayan Mario Anthony Perera",
      "Chern Hong Lim"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Feasibility+of+Neural+Radiance+Fields+for+Crime+Scene+Video+Reconstruction+Shariq+Nadeem+Malik+Min+Hao+Chee+Dayan+Mario+Anthony+Perera+Chern+Hong+Lim",
    "gs_search_success": true,
    "gs_authors": [
      "QwYp_uwAAAAJ"
    ],
    "citation_count": 2,
    "gs_author_count": null
  },
  {
    "arxiv_id": "2505.17771",
    "title": "TopoPoint: Enhance Topology Reasoning via Endpoint Detection in Autonomous Driving",
    "year": 2025,
    "published": "2025-05-23T11:42:54Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Topology reasoning, which unifies perception and structured reasoning, plays a vital role in understanding intersections for autonomous driving. However, its performance heavily relies on the accuracy of lane detection, particularly at connected lane endpoints. Existing methods often suffer from lane endpoints deviation, leading to incorrect topology construction. To address this issue, we propose TopoPoint, a novel framework that explicitly detects lane endpoints and jointly reasons over endpoi",
    "arxiv_url": "https://arxiv.org/abs/2505.17771v1",
    "pdf_url": "https://arxiv.org/pdf/2505.17771v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2505.17771",
    "arxiv_authors": [
      "Yanping Fu",
      "Xinyuan Liu",
      "Tianyu Li",
      "Yike Ma",
      "Yucheng Zhang",
      "Feng Dai"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=TopoPoint%3A+Enhance+Topology+Reasoning+via+Endpoint+Detection+in+Autonomous+Driving+Yanping+Fu+Xinyuan+Liu+Tianyu+Li+Yike+Ma+Yucheng+Zhang",
    "gs_search_success": true,
    "gs_authors": [
      "eXwizz8AAAAJ",
      "CVg2ja8AAAAJ",
      "X6vTmEMAAAAJ",
      "qo7La8cAAAAJ"
    ],
    "citation_count": 4,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2303.07321",
    "title": "Collision Cross-entropy for Soft Class Labels and Deep Clustering",
    "year": 2023,
    "published": "2023-03-13T17:42:11Z",
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "abstract": "We propose \"collision cross-entropy\" as a robust alternative to Shannon's cross-entropy (CE) loss when class labels are represented by soft categorical distributions y. In general, soft labels can naturally represent ambiguous targets in classification. They are particularly relevant for self-labeled clustering methods, where latent pseudo-labels are jointly estimated with the model parameters and uncertainty is prevalent. In case of soft labels, Shannon's CE teaches the model predictions to rep",
    "arxiv_url": "https://arxiv.org/abs/2303.07321v3",
    "pdf_url": "https://arxiv.org/pdf/2303.07321v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2303.07321",
    "arxiv_authors": [
      "Zhongwen Zhang",
      "Yuri Boykov"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Collision+Cross-entropy+for+Soft+Class+Labels+and+Deep+Clustering+Zhongwen+Zhang+Yuri+Boykov",
    "gs_search_success": true,
    "gs_authors": [
      "h6_PdYsAAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 2
  },
  {
    "arxiv_id": "2503.00780",
    "title": "Enhanced Multi-Class Classification of Gastrointestinal Endoscopic Images with Interpretable Deep Learning Model",
    "year": 2025,
    "published": "2025-03-02T08:07:50Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Endoscopy serves as an essential procedure for evaluating the gastrointestinal (GI) tract and plays a pivotal role in identifying GI-related disorders. Recent advancements in deep learning have demonstrated substantial progress in detecting abnormalities through intricate models and data augmentation methods.This research introduces a novel approach to enhance classification accuracy using 8,000 labeled endoscopic images from the Kvasir dataset, categorized into eight distinct classes. Leveragin",
    "arxiv_url": "https://arxiv.org/abs/2503.00780v1",
    "pdf_url": "https://arxiv.org/pdf/2503.00780v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2503.00780",
    "arxiv_authors": [
      "Astitva Kamble",
      "Vani Bandodkar",
      "Saakshi Dharmadhikary",
      "Veena Anand",
      "Pradyut Kumar Sanki",
      "Mei X. Wu",
      "Biswabandhu Jana"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Enhanced+Multi-Class+Classification+of+Gastrointestinal+Endoscopic+Images+with+Interpretable+Deep+Learning+Model+Astitva+Kamble+Vani+Bandodkar+Saakshi+Dharmadhikary+Veena+Anand+Pradyut+Kumar+Sanki",
    "gs_search_success": true,
    "gs_authors": [
      "iFKzrLkAAAAJ",
      "xxlEkXMAAAAJ",
      "ss9ZpuEAAAAJ"
    ],
    "citation_count": 3,
    "gs_author_count": 7
  },
  {
    "arxiv_id": "2410.03010",
    "title": "MMP: Towards Robust Multi-Modal Learning with Masked Modality Projection",
    "year": 2024,
    "published": "2024-10-03T21:41:12Z",
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "abstract": "Multimodal learning seeks to combine data from multiple input sources to enhance the performance of different downstream tasks. In real-world scenarios, performance can degrade substantially if some input modalities are missing. Existing methods that can handle missing modalities involve custom training or adaptation steps for each input modality combination. These approaches are either tied to specific modalities or become computationally expensive as the number of input modalities increases. I",
    "arxiv_url": "https://arxiv.org/abs/2410.03010v2",
    "pdf_url": "https://arxiv.org/pdf/2410.03010v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2410.03010",
    "arxiv_authors": [
      "Niki Nezakati",
      "Md Kaykobad Reza",
      "Ameya Patil",
      "Mashhour Solh",
      "M. Salman Asif"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=MMP%3A+Towards+Robust+Multi-Modal+Learning+with+Masked+Modality+Projection+Niki+Nezakati+Md+Kaykobad+Reza+Ameya+Patil+Mashhour+Solh+M.+Salman+Asif",
    "gs_search_success": true,
    "gs_authors": [
      "so4VH4IAAAAJ",
      "GgXLwMsAAAAJ",
      "tCjPO0oAAAAJ",
      "Dl0puDcAAAAJ",
      "_2uT_RAAAAAJ"
    ],
    "citation_count": 4,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2504.15380",
    "title": "Plug-and-Play Versatile Compressed Video Enhancement",
    "year": 2025,
    "published": "2025-04-21T18:39:31Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "As a widely adopted technique in data transmission, video compression effectively reduces the size of files, making it possible for real-time cloud computing. However, it comes at the cost of visual quality, posing challenges to the robustness of downstream vision models. In this work, we present a versatile codec-aware enhancement framework that reuses codec information to adaptively enhance videos under different compression settings, assisting various downstream vision tasks without introduci",
    "arxiv_url": "https://arxiv.org/abs/2504.15380v1",
    "pdf_url": "https://arxiv.org/pdf/2504.15380v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2504.15380",
    "arxiv_authors": [
      "Huimin Zeng",
      "Jiacheng Li",
      "Zhiwei Xiong"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Plug-and-Play+Versatile+Compressed+Video+Enhancement+Huimin+Zeng+Jiacheng+Li+Zhiwei+Xiong",
    "gs_search_success": true,
    "gs_authors": [
      "jZJhKDEAAAAJ",
      "Snl0HPEAAAAJ",
      "bFikGjoAAAAJ"
    ],
    "citation_count": 2,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2312.08952",
    "title": "UCMCTrack: Multi-Object Tracking with Uniform Camera Motion Compensation",
    "year": 2023,
    "published": "2023-12-14T14:01:35Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Multi-object tracking (MOT) in video sequences remains a challenging task, especially in scenarios with significant camera movements. This is because targets can drift considerably on the image plane, leading to erroneous tracking outcomes. Addressing such challenges typically requires supplementary appearance cues or Camera Motion Compensation (CMC). While these strategies are effective, they also introduce a considerable computational burden, posing challenges for real-time MOT. In response to",
    "arxiv_url": "https://arxiv.org/abs/2312.08952v2",
    "pdf_url": "https://arxiv.org/pdf/2312.08952v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2312.08952",
    "arxiv_authors": [
      "Kefu Yi",
      "Kai Luo",
      "Xiaolei Luo",
      "Jiangui Huang",
      "Hao Wu",
      "Rongdong Hu",
      "Wei Hao"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=UCMCTrack%3A+Multi-Object+Tracking+with+Uniform+Camera+Motion+Compensation+Kefu+Yi+Kai+Luo+Xiaolei+Luo+Jiangui+Huang+Hao+Wu",
    "gs_search_success": true,
    "gs_authors": [
      "ezlbBgUAAAAJ",
      "kQGE03YAAAAJ"
    ],
    "citation_count": 95,
    "gs_author_count": 7
  },
  {
    "arxiv_id": "2503.02424",
    "title": "Exploring Intrinsic Normal Prototypes within a Single Image for Universal Anomaly Detection",
    "year": 2025,
    "published": "2025-03-04T09:10:32Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Anomaly detection (AD) is essential for industrial inspection, yet existing methods typically rely on ``comparing'' test images to normal references from a training set. However, variations in appearance and positioning often complicate the alignment of these references with the test image, limiting detection accuracy. We observe that most anomalies manifest as local variations, meaning that even within anomalous images, valuable normal information remains. We argue that this information is usef",
    "arxiv_url": "https://arxiv.org/abs/2503.02424v2",
    "pdf_url": "https://arxiv.org/pdf/2503.02424v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2503.02424",
    "arxiv_authors": [
      "Wei Luo",
      "Yunkang Cao",
      "Haiming Yao",
      "Xiaotian Zhang",
      "Jianan Lou",
      "Yuqi Cheng",
      "Weiming Shen",
      "Wenyong Yu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Exploring+Intrinsic+Normal+Prototypes+within+a+Single+Image+for+Universal+Anomaly+Detection+Wei+Luo+Yunkang+Cao+Haiming+Yao+Xiaotian+Zhang+Jianan+Lou",
    "gs_search_success": true,
    "gs_authors": [
      "qLyFfWYAAAAJ",
      "02BC-WgAAAAJ",
      "aLJ8_G4AAAAJ",
      "Z312h_YAAAAJ",
      "FuSHsx4AAAAJ",
      "pHWjvoQAAAAJ"
    ],
    "citation_count": 33,
    "gs_author_count": 8
  },
  {
    "arxiv_id": "2503.06632",
    "title": "Towards More Accurate Personalized Image Generation: Addressing Overfitting and Evaluation Bias",
    "year": 2025,
    "published": "2025-03-09T14:14:02Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Personalized image generation via text prompts has great potential to improve daily life and professional work by facilitating the creation of customized visual content. The aim of image personalization is to create images based on a user-provided subject while maintaining both consistency of the subject and flexibility to accommodate various textual descriptions of that subject. However, current methods face challenges in ensuring fidelity to the text prompt while not overfitting to the trainin",
    "arxiv_url": "https://arxiv.org/abs/2503.06632v1",
    "pdf_url": "https://arxiv.org/pdf/2503.06632v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2503.06632",
    "arxiv_authors": [
      "Mingxiao Li",
      "Tingyu Qu",
      "Tinne Tuytelaars",
      "Marie-Francine Moens"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Towards+More+Accurate+Personalized+Image+Generation%3A+Addressing+Overfitting+and+Evaluation+Bias+Mingxiao+Li+Tingyu+Qu+Tinne+Tuytelaars+Marie-Francine+Moens",
    "gs_search_success": true,
    "gs_authors": [
      "d18-zLYAAAAJ",
      "0t2f7joAAAAJ",
      "EuFF9kUAAAAJ",
      "O9hYMUUAAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2307.04749",
    "title": "Divide, Evaluate, and Refine: Evaluating and Improving Text-to-Image Alignment with Iterative VQA Feedback",
    "year": 2023,
    "published": "2023-07-10T17:54:57Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "cs.MM",
      "stat.ML"
    ],
    "abstract": "The field of text-conditioned image generation has made unparalleled progress with the recent advent of latent diffusion models. While remarkable, as the complexity of given text input increases, the state-of-the-art diffusion models may still fail in generating images which accurately convey the semantics of the given prompt. Furthermore, it has been observed that such misalignments are often left undetected by pretrained multi-modal models such as CLIP. To address these problems, in this paper",
    "arxiv_url": "https://arxiv.org/abs/2307.04749v2",
    "pdf_url": "https://arxiv.org/pdf/2307.04749v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2307.04749",
    "arxiv_authors": [
      "Jaskirat Singh",
      "Liang Zheng"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Divide%2C+Evaluate%2C+and+Refine%3A+Evaluating+and+Improving+Text-to-Image+Alignment+with+Iterative+VQA+Feedback+Jaskirat+Singh+Liang+Zheng",
    "gs_search_success": true,
    "gs_authors": [
      "HAmEM_4AAAAJ",
      "vNHqr3oAAAAJ"
    ],
    "citation_count": 38,
    "gs_author_count": 2
  },
  {
    "arxiv_id": "2308.06022",
    "title": "Scale-Preserving Automatic Concept Extraction (SPACE)",
    "year": 2023,
    "published": "2023-08-11T08:54:45Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Convolutional Neural Networks (CNN) have become a common choice for industrial quality control, as well as other critical applications in the Industry 4.0. When these CNNs behave in ways unexpected to human users or developers, severe consequences can arise, such as economic losses or an increased risk to human life. Concept extraction techniques can be applied to increase the reliability and transparency of CNNs through generating global explanations for trained neural network models. The decis",
    "arxiv_url": "https://arxiv.org/abs/2308.06022v1",
    "pdf_url": "https://arxiv.org/pdf/2308.06022v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2308.06022",
    "arxiv_authors": [
      "Andrés Felipe Posada-Moreno",
      "Lukas Kreisköther",
      "Tassilo Glander",
      "Sebastian Trimpe"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Scale-Preserving+Automatic+Concept+Extraction+%28SPACE%29+Andr%C3%A9s+Felipe+Posada-Moreno+Lukas+Kreisk%C3%B6ther+Tassilo+Glander+Sebastian+Trimpe",
    "gs_search_success": true,
    "gs_authors": [
      "9kzHZssAAAAJ",
      "BnCVQsUAAAAJ"
    ],
    "citation_count": 4,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2411.15605",
    "title": "GIFT: A Framework for Global Interpretable Faithful Textual Explanations of Vision Classifiers",
    "year": 2024,
    "published": "2024-11-23T16:52:22Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Understanding deep models is crucial for deploying them in safety-critical applications. We introduce GIFT, a framework for deriving post-hoc, global, interpretable, and faithful textual explanations for vision classifiers. GIFT starts from local faithful visual counterfactual explanations and employs (vision) language models to translate those into global textual explanations. Crucially, GIFT provides a verification stage measuring the causal effect of the proposed explanations on the classifie",
    "arxiv_url": "https://arxiv.org/abs/2411.15605v2",
    "pdf_url": "https://arxiv.org/pdf/2411.15605v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2411.15605",
    "arxiv_authors": [
      "Éloi Zablocki",
      "Valentin Gerard",
      "Amaia Cardiel",
      "Eric Gaussier",
      "Matthieu Cord",
      "Eduardo Valle"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=GIFT%3A+A+Framework+for+Global+Interpretable+Faithful+Textual+Explanations+of+Vision+Classifiers+%C3%89loi+Zablocki+Valentin+Gerard+Amaia+Cardiel+Eric+Gaussier+Matthieu+Cord",
    "gs_search_success": true,
    "gs_authors": [
      "lxWPqWAAAAAJ",
      "SpAotDcAAAAJ",
      "QKdN4SAAAAAJ",
      "rCpJslsAAAAJ",
      "dOkbUmEAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2306.08865",
    "title": "One-Shot Learning of Visual Path Navigation for Autonomous Vehicles",
    "year": 2023,
    "published": "2023-06-15T05:27:46Z",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "Autonomous driving presents many challenges due to the large number of scenarios the autonomous vehicle (AV) may encounter. End-to-end deep learning models are comparatively simplistic models that can handle a broad set of scenarios. However, end-to-end models require large amounts of diverse data to perform well. This paper presents a novel deep neural network that performs image-to-steering path navigation that helps with the data problem by adding one-shot learning to the system. Presented wi",
    "arxiv_url": "https://arxiv.org/abs/2306.08865v1",
    "pdf_url": "https://arxiv.org/pdf/2306.08865v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2306.08865",
    "arxiv_authors": [
      "Zhongying CuiZhu",
      "Francois Charette",
      "Amin Ghafourian",
      "Debo Shi",
      "Matthew Cui",
      "Anjali Krishnamachar",
      "Iman Soltani"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=One-Shot+Learning+of+Visual+Path+Navigation+for+Autonomous+Vehicles+Zhongying+CuiZhu+Francois+Charette+Amin+Ghafourian+Debo+Shi+Matthew+Cui",
    "gs_search_success": true,
    "gs_authors": [
      "sVVoYt8AAAAJ",
      "MPoGC3YAAAAJ",
      "BJ2teVoAAAAJ",
      "JITRligAAAAJ"
    ],
    "citation_count": 2,
    "gs_author_count": 7
  },
  {
    "arxiv_id": "2402.19020",
    "title": "Unsupervised Learning of High-resolution Light Field Imaging via Beam Splitter-based Hybrid Lenses",
    "year": 2024,
    "published": "2024-02-29T10:30:02Z",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "abstract": "In this paper, we design a beam splitter-based hybrid light field imaging prototype to record 4D light field image and high-resolution 2D image simultaneously, and make a hybrid light field dataset. The 2D image could be considered as the high-resolution ground truth corresponding to the low-resolution central sub-aperture image of 4D light field image. Subsequently, we propose an unsupervised learning-based super-resolution framework with the hybrid light field dataset, which adaptively settles",
    "arxiv_url": "https://arxiv.org/abs/2402.19020v1",
    "pdf_url": "https://arxiv.org/pdf/2402.19020v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2402.19020",
    "arxiv_authors": [
      "Jianxin Lei",
      "Chengcai Xu",
      "Langqing Shi",
      "Junhui Hou",
      "Ping Zhou"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Unsupervised+Learning+of+High-resolution+Light+Field+Imaging+via+Beam+Splitter-based+Hybrid+Lenses+Jianxin+Lei+Chengcai+Xu+Langqing+Shi+Junhui+Hou+Ping+Zhou",
    "gs_search_success": true,
    "gs_authors": [
      "j6eefhwAAAAJ"
    ],
    "citation_count": 2,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2308.07009",
    "title": "ACTIVE: Towards Highly Transferable 3D Physical Camouflage for Universal and Robust Vehicle Evasion",
    "year": 2023,
    "published": "2023-08-14T08:52:41Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Adversarial camouflage has garnered attention for its ability to attack object detectors from any viewpoint by covering the entire object's surface. However, universality and robustness in existing methods often fall short as the transferability aspect is often overlooked, thus restricting their application only to a specific target with limited performance. To address these challenges, we present Adversarial Camouflage for Transferable and Intensive Vehicle Evasion (ACTIVE), a state-of-the-art ",
    "arxiv_url": "https://arxiv.org/abs/2308.07009v2",
    "pdf_url": "https://arxiv.org/pdf/2308.07009v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2308.07009",
    "arxiv_authors": [
      "Naufal Suryanto",
      "Yongsu Kim",
      "Harashta Tatimma Larasati",
      "Hyoeun Kang",
      "Thi-Thu-Huong Le",
      "Yoonyoung Hong",
      "Hunmin Yang",
      "Se-Yoon Oh",
      "Howon Kim"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=ACTIVE%3A+Towards+Highly+Transferable+3D+Physical+Camouflage+for+Universal+and+Robust+Vehicle+Evasion+Naufal+Suryanto+Yongsu+Kim+Harashta+Tatimma+Larasati+Hyoeun+Kang+Thi-Thu-Huong+Le",
    "gs_search_success": true,
    "gs_authors": [
      "DXb797cAAAAJ",
      "GeQi_D4AAAAJ",
      "mDxJj2AAAAAJ",
      "S8lwCEUAAAAJ",
      "vXEPNi4AAAAJ",
      "UptzPYsAAAAJ"
    ],
    "citation_count": 43,
    "gs_author_count": 9
  },
  {
    "arxiv_id": "2505.20326",
    "title": "Cultural Awareness in Vision-Language Models: A Cross-Country Exploration",
    "year": 2025,
    "published": "2025-05-23T18:47:52Z",
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CL",
      "cs.CV"
    ],
    "abstract": "Vision-Language Models (VLMs) are increasingly deployed in diverse cultural contexts, yet their internal biases remain poorly understood. In this work, we propose a novel framework to systematically evaluate how VLMs encode cultural differences and biases related to race, gender, and physical traits across countries. We introduce three retrieval-based tasks: (1) Race to Country retrieval, which examines the association between individuals from specific racial groups (East Asian, White, Middle Ea",
    "arxiv_url": "https://arxiv.org/abs/2505.20326v1",
    "pdf_url": "https://arxiv.org/pdf/2505.20326v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2505.20326",
    "arxiv_authors": [
      "Avinash Madasu",
      "Vasudev Lal",
      "Phillip Howard"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Cultural+Awareness+in+Vision-Language+Models%3A+A+Cross-Country+Exploration+Avinash+Madasu+Vasudev+Lal+Phillip+Howard",
    "gs_search_success": true,
    "gs_authors": [
      "Qbu4oKwAAAAJ",
      "YRe0ruYAAAAJ",
      "EKh822gAAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2303.03667",
    "title": "Run, Don't Walk: Chasing Higher FLOPS for Faster Neural Networks",
    "year": 2023,
    "published": "2023-03-07T06:05:30Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "To design fast neural networks, many works have been focusing on reducing the number of floating-point operations (FLOPs). We observe that such reduction in FLOPs, however, does not necessarily lead to a similar level of reduction in latency. This mainly stems from inefficiently low floating-point operations per second (FLOPS). To achieve faster networks, we revisit popular operators and demonstrate that such low FLOPS is mainly due to frequent memory access of the operators, especially the dept",
    "arxiv_url": "https://arxiv.org/abs/2303.03667v3",
    "pdf_url": "https://arxiv.org/pdf/2303.03667v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2303.03667",
    "arxiv_authors": [
      "Jierun Chen",
      "Shiu-hong Kao",
      "Hao He",
      "Weipeng Zhuo",
      "Song Wen",
      "Chul-Ho Lee",
      "S. -H. Gary Chan"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Run%2C+Don%27t+Walk%3A+Chasing+Higher+FLOPS+for+Faster+Neural+Networks+Jierun+Chen+Shiu-hong+Kao+Hao+He+Weipeng+Zhuo+Song+Wen",
    "gs_search_success": false,
    "gs_authors": [],
    "citation_count": null,
    "gs_author_count": null,
    "error_type": "fetch_failed"
  },
  {
    "arxiv_id": "2501.06151",
    "title": "PySpatial: A High-Speed Whole Slide Image Pathomics Toolkit",
    "year": 2025,
    "published": "2025-01-10T18:24:00Z",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "abstract": "Whole Slide Image (WSI) analysis plays a crucial role in modern digital pathology, enabling large-scale feature extraction from tissue samples. However, traditional feature extraction pipelines based on tools like CellProfiler often involve lengthy workflows, requiring WSI segmentation into patches, feature extraction at the patch level, and subsequent mapping back to the original WSI. To address these challenges, we present PySpatial, a high-speed pathomics toolkit specifically designed for WSI",
    "arxiv_url": "https://arxiv.org/abs/2501.06151v1",
    "pdf_url": "https://arxiv.org/pdf/2501.06151v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2501.06151",
    "arxiv_authors": [
      "Yuechen Yang",
      "Yu Wang",
      "Tianyuan Yao",
      "Ruining Deng",
      "Mengmeng Yin",
      "Shilin Zhao",
      "Haichun Yang",
      "Yuankai Huo"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=PySpatial%3A+A+High-Speed+Whole+Slide+Image+Pathomics+Toolkit+Yuechen+Yang+Yu+Wang+Tianyuan+Yao+Ruining+Deng+Mengmeng+Yin",
    "gs_search_success": true,
    "gs_authors": [
      "B3UojrgAAAAJ",
      "DeADZl0AAAAJ",
      "ojjuOGgAAAAJ",
      "bx39iAEAAAAJ",
      "WRxmxNgAAAAJ",
      "QEZEaLsAAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 8
  },
  {
    "arxiv_id": "2403.02736",
    "title": "Bootstrapping Rare Object Detection in High-Resolution Satellite Imagery",
    "year": 2024,
    "published": "2024-03-05T07:44:13Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Rare object detection is a fundamental task in applied geospatial machine learning, however is often challenging due to large amounts of high-resolution satellite or aerial imagery and few or no labeled positive samples to start with. This paper addresses the problem of bootstrapping such a rare object detection task assuming there is no labeled data and no spatial prior over the area of interest. We propose novel offline and online cluster-based approaches for sampling patches that are signific",
    "arxiv_url": "https://arxiv.org/abs/2403.02736v1",
    "pdf_url": "https://arxiv.org/pdf/2403.02736v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2403.02736",
    "arxiv_authors": [
      "Akram Zaytar",
      "Caleb Robinson",
      "Gilles Q. Hacheme",
      "Girmaw A. Tadesse",
      "Rahul Dodhia",
      "Juan M. Lavista Ferres",
      "Lacey F. Hughey",
      "Jared A. Stabach",
      "Irene Amoke"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Bootstrapping+Rare+Object+Detection+in+High-Resolution+Satellite+Imagery+Akram+Zaytar+Caleb+Robinson+Gilles+Q.+Hacheme+Girmaw+A.+Tadesse+Rahul+Dodhia",
    "gs_search_success": true,
    "gs_authors": [
      "cjYgLT0AAAAJ",
      "yMMUBIUAAAAJ",
      "m1J5OaIAAAAJ",
      "hn9b-aUAAAAJ",
      "i_lGZUIAAAAJ",
      "qDy5Bb0AAAAJ",
      "ZWfuPHUAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 7
  },
  {
    "arxiv_id": "2308.06393",
    "title": "R2S100K: Road-Region Segmentation Dataset For Semi-Supervised Autonomous Driving in the Wild",
    "year": 2023,
    "published": "2023-08-11T21:31:37Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Semantic understanding of roadways is a key enabling factor for safe autonomous driving. However, existing autonomous driving datasets provide well-structured urban roads while ignoring unstructured roadways containing distress, potholes, water puddles, and various kinds of road patches i.e., earthen, gravel etc. To this end, we introduce Road Region Segmentation dataset (R2S100K) -- a large-scale dataset and benchmark for training and evaluation of road segmentation in aforementioned challengin",
    "arxiv_url": "https://arxiv.org/abs/2308.06393v1",
    "pdf_url": "https://arxiv.org/pdf/2308.06393v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2308.06393",
    "arxiv_authors": [
      "Muhammad Atif Butt",
      "Hassan Ali",
      "Adnan Qayyum",
      "Waqas Sultani",
      "Ala Al-Fuqaha",
      "Junaid Qadir"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=R2S100K%3A+Road-Region+Segmentation+Dataset+For+Semi-Supervised+Autonomous+Driving+in+the+Wild+Muhammad+Atif+Butt+Hassan+Ali+Adnan+Qayyum+Waqas+Sultani+Ala+Al-Fuqaha",
    "gs_search_success": true,
    "gs_authors": [
      "MhiaZiQAAAAJ",
      "keWNlTIAAAAJ",
      "SqcjV8EAAAAJ",
      "vf7PeaoAAAAJ"
    ],
    "citation_count": 2,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2505.19853",
    "title": "Two Causally Related Needles in a Video Haystack",
    "year": 2025,
    "published": "2025-05-26T11:37:34Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Properly evaluating the ability of Video-Language Models (VLMs) to understand long videos remains a challenge. We propose a long-context video understanding benchmark, Causal2Needles, that assesses two crucial abilities insufficiently addressed by existing benchmarks: (1) extracting information from two separate locations (two needles) in a long video and understanding them jointly, and (2) modeling the world in terms of cause and effect in human behaviors. Causal2Needles evaluates these abiliti",
    "arxiv_url": "https://arxiv.org/abs/2505.19853v3",
    "pdf_url": "https://arxiv.org/pdf/2505.19853v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2505.19853",
    "arxiv_authors": [
      "Miaoyu Li",
      "Qin Chao",
      "Boyang Li"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Two+Causally+Related+Needles+in+a+Video+Haystack+Miaoyu+Li+Qin+Chao+Boyang+Li",
    "gs_search_success": true,
    "gs_authors": [
      "QwL4z2UAAAAJ",
      "4W9s4jMAAAAJ",
      "tm9WjJwAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2410.08509",
    "title": "A Bayesian Approach to Weakly-supervised Laparoscopic Image Segmentation",
    "year": 2024,
    "published": "2024-10-11T04:19:48Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "In this paper, we study weakly-supervised laparoscopic image segmentation with sparse annotations. We introduce a novel Bayesian deep learning approach designed to enhance both the accuracy and interpretability of the model's segmentation, founded upon a comprehensive Bayesian framework, ensuring a robust and theoretically validated method. Our approach diverges from conventional methods that directly train using observed images and their corresponding weak annotations. Instead, we estimate the ",
    "arxiv_url": "https://arxiv.org/abs/2410.08509v1",
    "pdf_url": "https://arxiv.org/pdf/2410.08509v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2410.08509",
    "arxiv_authors": [
      "Zhou Zheng",
      "Yuichiro Hayashi",
      "Masahiro Oda",
      "Takayuki Kitasaka",
      "Kensaku Mori"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=A+Bayesian+Approach+to+Weakly-supervised+Laparoscopic+Image+Segmentation+Zhou+Zheng+Yuichiro+Hayashi+Masahiro+Oda+Takayuki+Kitasaka+Kensaku+Mori",
    "gs_search_success": true,
    "gs_authors": [
      "o0tylKsAAAAJ",
      "OYIZMScAAAAJ"
    ],
    "citation_count": 2,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2311.17338",
    "title": "MagDiff: Multi-Alignment Diffusion for High-Fidelity Video Generation and Editing",
    "year": 2023,
    "published": "2023-11-29T03:36:07Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "The diffusion model is widely leveraged for either video generation or video editing. As each field has its task-specific problems, it is difficult to merely develop a single diffusion for completing both tasks simultaneously. Video diffusion sorely relying on the text prompt can be adapted to unify the two tasks. However, it lacks a high capability of aligning heterogeneous modalities between text and image, leading to various misalignment problems. In this work, we are the first to propose a u",
    "arxiv_url": "https://arxiv.org/abs/2311.17338v3",
    "pdf_url": "https://arxiv.org/pdf/2311.17338v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2311.17338",
    "arxiv_authors": [
      "Haoyu Zhao",
      "Tianyi Lu",
      "Jiaxi Gu",
      "Xing Zhang",
      "Qingping Zheng",
      "Zuxuan Wu",
      "Hang Xu",
      "Yu-Gang Jiang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=MagDiff%3A+Multi-Alignment+Diffusion+for+High-Fidelity+Video+Generation+and+Editing+Haoyu+Zhao+Tianyi+Lu+Jiaxi+Gu+Xing+Zhang+Qingping+Zheng",
    "gs_search_success": true,
    "gs_authors": [
      "J_8TX6sAAAAJ",
      "pCGM7jwAAAAJ",
      "f3_FP8AAAAAJ",
      "7t12hVkAAAAJ",
      "dG46Q8wAAAAJ",
      "FjBhxhkAAAAJ",
      "l0Y7emkAAAAJ"
    ],
    "citation_count": 14,
    "gs_author_count": 8
  },
  {
    "arxiv_id": "2305.11481",
    "title": "CM-MaskSD: Cross-Modality Masked Self-Distillation for Referring Image Segmentation",
    "year": 2023,
    "published": "2023-05-19T07:17:27Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Referring image segmentation (RIS) is a fundamental vision-language task that intends to segment a desired object from an image based on a given natural language expression. Due to the essentially distinct data properties between image and text, most of existing methods either introduce complex designs towards fine-grained vision-language alignment or lack required dense alignment, resulting in scalability issues or mis-segmentation problems such as over- or under-segmentation. To achieve effect",
    "arxiv_url": "https://arxiv.org/abs/2305.11481v3",
    "pdf_url": "https://arxiv.org/pdf/2305.11481v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2305.11481",
    "arxiv_authors": [
      "Wenxuan Wang",
      "Jing Liu",
      "Xingjian He",
      "Yisi Zhang",
      "Chen Chen",
      "Jiachen Shen",
      "Yan Zhang",
      "Jiangyun Li"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=CM-MaskSD%3A+Cross-Modality+Masked+Self-Distillation+for+Referring+Image+Segmentation+Wenxuan+Wang+Jing+Liu+Xingjian+He+Yisi+Zhang+Chen+Chen",
    "gs_search_success": true,
    "gs_authors": [
      "sOI-S7oAAAAJ",
      "jjVTk6wAAAAJ",
      "XWunp9YAAAAJ",
      "OaGRHWYAAAAJ",
      "YG7_e9AAAAAJ",
      "75OyC-oAAAAJ"
    ],
    "citation_count": 27,
    "gs_author_count": 7
  },
  {
    "arxiv_id": "2309.03008",
    "title": "Sparse 3D Reconstruction via Object-Centric Ray Sampling",
    "year": 2023,
    "published": "2023-09-06T13:54:31Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "We propose a novel method for 3D object reconstruction from a sparse set of views captured from a 360-degree calibrated camera rig. We represent the object surface through a hybrid model that uses both an MLP-based neural representation and a triangle mesh. A key contribution in our work is a novel object-centric sampling scheme of the neural representation, where rays are shared among all views. This efficiently concentrates and reduces the number of samples used to update the neural model at e",
    "arxiv_url": "https://arxiv.org/abs/2309.03008v2",
    "pdf_url": "https://arxiv.org/pdf/2309.03008v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2309.03008",
    "arxiv_authors": [
      "Llukman Cerkezi",
      "Paolo Favaro"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Sparse+3D+Reconstruction+via+Object-Centric+Ray+Sampling+Llukman+Cerkezi+Paolo+Favaro",
    "gs_search_success": true,
    "gs_authors": [
      "w_XDRRsAAAAJ",
      "ZxfliGcAAAAJ"
    ],
    "citation_count": 5,
    "gs_author_count": 2
  },
  {
    "arxiv_id": "2505.12667",
    "title": "Safe-Sora: Safe Text-to-Video Generation via Graphical Watermarking",
    "year": 2025,
    "published": "2025-05-19T03:31:31Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "The explosive growth of generative video models has amplified the demand for reliable copyright preservation of AI-generated content. Despite its popularity in image synthesis, invisible generative watermarking remains largely underexplored in video generation. To address this gap, we propose Safe-Sora, the first framework to embed graphical watermarks directly into the video generation process. Motivated by the observation that watermarking performance is closely tied to the visual similarity b",
    "arxiv_url": "https://arxiv.org/abs/2505.12667v2",
    "pdf_url": "https://arxiv.org/pdf/2505.12667v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2505.12667",
    "arxiv_authors": [
      "Zihan Su",
      "Xuerui Qiu",
      "Hongbin Xu",
      "Tangyu Jiang",
      "Junhao Zhuang",
      "Chun Yuan",
      "Ming Li",
      "Shengfeng He",
      "Fei Richard Yu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Safe-Sora%3A+Safe+Text-to-Video+Generation+via+Graphical+Watermarking+Zihan+Su+Xuerui+Qiu+Hongbin+Xu+Tangyu+Jiang+Junhao+Zhuang",
    "gs_search_success": true,
    "gs_authors": [
      "zuGMGBoAAAAJ",
      "rBWnK8wAAAAJ",
      "QlBHlj0AAAAJ",
      "J3olRccAAAAJ",
      "bMwW4e8AAAAJ",
      "2wySPkcAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 9
  },
  {
    "arxiv_id": "2407.11664",
    "title": "Mask-guided cross-image attention for zero-shot in-silico histopathologic image generation with a diffusion model",
    "year": 2024,
    "published": "2024-07-16T12:36:26Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Creating in-silico data with generative AI promises a cost-effective alternative to staining, imaging, and annotating whole slide images in computational pathology. Diffusion models are the state-of-the-art solution for generating in-silico images, offering unparalleled fidelity and realism. Using appearance transfer diffusion models allows for zero-shot image generation, facilitating fast application and making model training unnecessary. However current appearance transfer diffusion models are",
    "arxiv_url": "https://arxiv.org/abs/2407.11664v3",
    "pdf_url": "https://arxiv.org/pdf/2407.11664v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2407.11664",
    "arxiv_authors": [
      "Dominik Winter",
      "Nicolas Triltsch",
      "Marco Rosati",
      "Anatoliy Shumilov",
      "Ziya Kokaragac",
      "Yuri Popov",
      "Thomas Padel",
      "Laura Sebastian Monasor",
      "Ross Hill",
      "Markus Schick",
      "Nicolas Brieu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Mask-guided+cross-image+attention+for+zero-shot+in-silico+histopathologic+image+generation+with+a+diffusion+model+Dominik+Winter+Nicolas+Triltsch+Marco+Rosati+Anatoliy+Shumilov+Ziya+Kokaragac",
    "gs_search_success": true,
    "gs_authors": [
      "EYISyIIAAAAJ",
      "u6JvOD4AAAAJ"
    ],
    "citation_count": 4,
    "gs_author_count": 8
  },
  {
    "arxiv_id": "2404.10474",
    "title": "Toward a Realistic Benchmark for Out-of-Distribution Detection",
    "year": 2024,
    "published": "2024-04-16T11:29:43Z",
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "abstract": "Deep neural networks are increasingly used in a wide range of technologies and services, but remain highly susceptible to out-of-distribution (OOD) samples, that is, drawn from a different distribution than the original training set. A common approach to address this issue is to endow deep neural networks with the ability to detect OOD samples. Several benchmarks have been proposed to design and validate OOD detection techniques. However, many of them are based on far-OOD samples drawn from very",
    "arxiv_url": "https://arxiv.org/abs/2404.10474v1",
    "pdf_url": "https://arxiv.org/pdf/2404.10474v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2404.10474",
    "arxiv_authors": [
      "Pietro Recalcati",
      "Fabio Garcea",
      "Luca Piano",
      "Fabrizio Lamberti",
      "Lia Morra"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Toward+a+Realistic+Benchmark+for+Out-of-Distribution+Detection+Pietro+Recalcati+Fabio+Garcea+Luca+Piano+Fabrizio+Lamberti+Lia+Morra",
    "gs_search_success": true,
    "gs_authors": [
      "w-x8r1QAAAAJ",
      "oRCo5_UAAAAJ",
      "XqM3IW0AAAAJ",
      "1Dmd6GYAAAAJ"
    ],
    "citation_count": 2,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2302.04544",
    "title": "GMConv: Modulating Effective Receptive Fields for Convolutional Kernels",
    "year": 2023,
    "published": "2023-02-09T10:17:17Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "In convolutional neural networks, the convolutions are conventionally performed using a square kernel with a fixed N $\\times$ N receptive field (RF). However, what matters most to the network is the effective receptive field (ERF) that indicates the extent with which input pixels contribute to an output pixel. Inspired by the property that ERFs typically exhibit a Gaussian distribution, we propose a Gaussian Mask convolutional kernel (GMConv) in this work. Specifically, GMConv utilizes the Gauss",
    "arxiv_url": "https://arxiv.org/abs/2302.04544v3",
    "pdf_url": "https://arxiv.org/pdf/2302.04544v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2302.04544",
    "arxiv_authors": [
      "Qi Chen",
      "Chao Li",
      "Jia Ning",
      "Stephen Lin",
      "Kun He"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=GMConv%3A+Modulating+Effective+Receptive+Fields+for+Convolutional+Kernels+Qi+Chen+Chao+Li+Jia+Ning+Stephen+Lin+Kun+He",
    "gs_search_success": true,
    "gs_authors": [
      "c3PYmxUAAAAJ",
      "Wik8bkIAAAAJ",
      "YTQnGJsAAAAJ"
    ],
    "citation_count": 7,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2307.04028",
    "title": "Measuring the Success of Diffusion Models at Imitating Human Artists",
    "year": 2023,
    "published": "2023-07-08T18:31:25Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "abstract": "Modern diffusion models have set the state-of-the-art in AI image generation. Their success is due, in part, to training on Internet-scale data which often includes copyrighted work. This prompts questions about the extent to which these models learn from, imitate, or copy the work of human artists. This work suggests that tying copyright liability to the capabilities of the model may be useful given the evolving ecosystem of generative models. Specifically, much of the legal analysis of copyrig",
    "arxiv_url": "https://arxiv.org/abs/2307.04028v1",
    "pdf_url": "https://arxiv.org/pdf/2307.04028v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2307.04028",
    "arxiv_authors": [
      "Stephen Casper",
      "Zifan Guo",
      "Shreya Mogulothu",
      "Zachary Marinov",
      "Chinmay Deshpande",
      "Rui-Jie Yew",
      "Zheng Dai",
      "Dylan Hadfield-Menell"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Measuring+the+Success+of+Diffusion+Models+at+Imitating+Human+Artists+Stephen+Casper+Zifan+Guo+Shreya+Mogulothu+Zachary+Marinov+Chinmay+Deshpande",
    "gs_search_success": true,
    "gs_authors": [
      "21gQ_owAAAAJ",
      "R6uOw_sAAAAJ",
      "4mVPFQ8AAAAJ",
      "zaF8UJcAAAAJ",
      "2nUoXpsAAAAJ"
    ],
    "citation_count": 10,
    "gs_author_count": 8
  },
  {
    "arxiv_id": "2502.07855",
    "title": "Vision-Language Models for Edge Networks: A Comprehensive Survey",
    "year": 2025,
    "published": "2025-02-11T14:04:43Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "abstract": "Vision Large Language Models (VLMs) combine visual understanding with natural language processing, enabling tasks like image captioning, visual question answering, and video analysis. While VLMs show impressive capabilities across domains such as autonomous vehicles, smart surveillance, and healthcare, their deployment on resource-constrained edge devices remains challenging due to processing power, memory, and energy limitations. This survey explores recent advancements in optimizing VLMs for e",
    "arxiv_url": "https://arxiv.org/abs/2502.07855v2",
    "pdf_url": "https://arxiv.org/pdf/2502.07855v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2502.07855",
    "arxiv_authors": [
      "Ahmed Sharshar",
      "Latif U. Khan",
      "Waseem Ullah",
      "Mohsen Guizani"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Vision-Language+Models+for+Edge+Networks%3A+A+Comprehensive+Survey+Ahmed+Sharshar+Latif+U.+Khan+Waseem+Ullah+Mohsen+Guizani",
    "gs_search_success": true,
    "gs_authors": [
      "RigrYkcAAAAJ",
      "9iN1OqYAAAAJ",
      "cVzxZ6YAAAAJ",
      "yGQSwRMAAAAJ"
    ],
    "citation_count": 25,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2303.13761",
    "title": "Unsupervised Hierarchical Domain Adaptation for Adverse Weather Optical Flow",
    "year": 2023,
    "published": "2023-03-24T02:17:51Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Optical flow estimation has made great progress, but usually suffers from degradation under adverse weather. Although semi/full-supervised methods have made good attempts, the domain shift between the synthetic and real adverse weather images would deteriorate their performance. To alleviate this issue, our start point is to unsupervisedly transfer the knowledge from source clean domain to target degraded domain. Our key insight is that adverse weather does not change the intrinsic optical flow ",
    "arxiv_url": "https://arxiv.org/abs/2303.13761v1",
    "pdf_url": "https://arxiv.org/pdf/2303.13761v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2303.13761",
    "arxiv_authors": [
      "Hanyu Zhou",
      "Yi Chang",
      "Gang Chen",
      "Luxin Yan"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Unsupervised+Hierarchical+Domain+Adaptation+for+Adverse+Weather+Optical+Flow+Hanyu+Zhou+Yi+Chang+Gang+Chen+Luxin+Yan",
    "gs_search_success": true,
    "gs_authors": [
      "bRXguCgAAAAJ",
      "7GwIDigAAAAJ",
      "I1nZ67YAAAAJ"
    ],
    "citation_count": 5,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2411.16407",
    "title": "A Study on Unsupervised Domain Adaptation for Semantic Segmentation in the Era of Vision-Language Models",
    "year": 2024,
    "published": "2024-11-25T14:12:24Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Despite the recent progress in deep learning based computer vision, domain shifts are still one of the major challenges. Semantic segmentation for autonomous driving faces a wide range of domain shifts, e.g. caused by changing weather conditions, new geolocations and the frequent use of synthetic data in model training. Unsupervised domain adaptation (UDA) methods have emerged which adapt a model to a new target domain by only using unlabeled data of that domain. The variety of UDA methods is la",
    "arxiv_url": "https://arxiv.org/abs/2411.16407v1",
    "pdf_url": "https://arxiv.org/pdf/2411.16407v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2411.16407",
    "arxiv_authors": [
      "Manuel Schwonberg",
      "Claus Werner",
      "Hanno Gottschalk",
      "Carsten Meyer"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=A+Study+on+Unsupervised+Domain+Adaptation+for+Semantic+Segmentation+in+the+Era+of+Vision-Language+Models+Manuel+Schwonberg+Claus+Werner+Hanno+Gottschalk+Carsten+Meyer",
    "gs_search_success": true,
    "gs_authors": [
      "eqsXwGIAAAAJ",
      "qq9RqVIAAAAJ",
      "5cOHZ4MAAAAJ",
      "sH8tc4UAAAAJ"
    ],
    "citation_count": 2,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2503.06542",
    "title": "ARMOR: Empowering Multimodal Understanding Model with Interleaved Multimodal Generation Capability",
    "year": 2025,
    "published": "2025-03-09T10:15:39Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Unified multimodal understanding and generation have recently received much attention in the area of vision and language. Existing UniMs are designed to simultaneously learn both multimodal understanding and generation capabilities, demanding substantial computational resources, and often struggle to generate interleaved text-image. We present ARMOR, a resource-efficient and pure autoregressive framework that achieves both understanding and generation by fine-tuning existing multimodal large lan",
    "arxiv_url": "https://arxiv.org/abs/2503.06542v2",
    "pdf_url": "https://arxiv.org/pdf/2503.06542v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2503.06542",
    "arxiv_authors": [
      "Jianwen Sun",
      "Yukang Feng",
      "Chuanhao Li",
      "Fanrui Zhang",
      "Zizhen Li",
      "Jiaxin Ai",
      "Sizhuo Zhou",
      "Yu Dai",
      "Shenglin Zhang",
      "Kaipeng Zhang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=ARMOR%3A+Empowering+Multimodal+Understanding+Model+with+Interleaved+Multimodal+Generation+Capability+Jianwen+Sun+Yukang+Feng+Chuanhao+Li+Fanrui+Zhang+Zizhen+Li",
    "gs_search_success": true,
    "gs_authors": [
      "4OqZBmYAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 10
  },
  {
    "arxiv_id": "2404.04629",
    "title": "DifFUSER: Diffusion Model for Robust Multi-Sensor Fusion in 3D Object Detection and BEV Segmentation",
    "year": 2024,
    "published": "2024-04-06T13:25:29Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Diffusion models have recently gained prominence as powerful deep generative models, demonstrating unmatched performance across various domains. However, their potential in multi-sensor fusion remains largely unexplored. In this work, we introduce DifFUSER, a novel approach that leverages diffusion models for multi-modal fusion in 3D object detection and BEV map segmentation. Benefiting from the inherent denoising property of diffusion, DifFUSER is able to refine or even synthesize sensor featur",
    "arxiv_url": "https://arxiv.org/abs/2404.04629v2",
    "pdf_url": "https://arxiv.org/pdf/2404.04629v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2404.04629",
    "arxiv_authors": [
      "Duy-Tho Le",
      "Hengcan Shi",
      "Jianfei Cai",
      "Hamid Rezatofighi"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=DifFUSER%3A+Diffusion+Model+for+Robust+Multi-Sensor+Fusion+in+3D+Object+Detection+and+BEV+Segmentation+Duy-Tho+Le+Hengcan+Shi+Jianfei+Cai+Hamid+Rezatofighi",
    "gs_search_success": true,
    "gs_authors": [
      "VxAuxMwAAAAJ",
      "KcPnbLoAAAAJ",
      "ufvAmAIAAAAJ",
      "N6czCoUAAAAJ"
    ],
    "citation_count": 12,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2411.02385",
    "title": "How Far is Video Generation from World Model: A Physical Law Perspective",
    "year": 2024,
    "published": "2024-11-04T18:53:05Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "OpenAI's Sora highlights the potential of video generation for developing world models that adhere to fundamental physical laws. However, the ability of video generation models to discover such laws purely from visual data without human priors can be questioned. A world model learning the true law should give predictions robust to nuances and correctly extrapolate on unseen scenarios. In this work, we evaluate across three key scenarios: in-distribution, out-of-distribution, and combinatorial ge",
    "arxiv_url": "https://arxiv.org/abs/2411.02385v2",
    "pdf_url": "https://arxiv.org/pdf/2411.02385v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2411.02385",
    "arxiv_authors": [
      "Bingyi Kang",
      "Yang Yue",
      "Rui Lu",
      "Zhijie Lin",
      "Yang Zhao",
      "Kaixin Wang",
      "Gao Huang",
      "Jiashi Feng"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=How+Far+is+Video+Generation+from+World+Model%3A+A+Physical+Law+Perspective+Bingyi+Kang+Yang+Yue+Rui+Lu+Zhijie+Lin+Yang+Zhao",
    "gs_search_success": false,
    "gs_authors": [],
    "citation_count": null,
    "gs_author_count": null,
    "error_type": "fetch_failed"
  },
  {
    "arxiv_id": "2501.08170",
    "title": "Benchmarking Multimodal Models for Fine-Grained Image Analysis: A Comparative Study Across Diverse Visual Features",
    "year": 2025,
    "published": "2025-01-14T14:50:57Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "This article introduces a benchmark designed to evaluate the capabilities of multimodal models in analyzing and interpreting images. The benchmark focuses on seven key visual aspects: main object, additional objects, background, detail, dominant colors, style, and viewpoint. A dataset of 14,580 images, generated from diverse text prompts, was used to assess the performance of seven leading multimodal models. These models were evaluated on their ability to accurately identify and describe each vi",
    "arxiv_url": "https://arxiv.org/abs/2501.08170v1",
    "pdf_url": "https://arxiv.org/pdf/2501.08170v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2501.08170",
    "arxiv_authors": [
      "Evgenii Evstafev"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Benchmarking+Multimodal+Models+for+Fine-Grained+Image+Analysis%3A+A+Comparative+Study+Across+Diverse+Visual+Features+Evgenii+Evstafev",
    "gs_search_success": true,
    "gs_authors": [
      "cYLfW7QAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 1
  },
  {
    "arxiv_id": "2502.08200",
    "title": "ActiveSSF: An Active-Learning-Guided Self-Supervised Framework for Long-Tailed Megakaryocyte Classification",
    "year": 2025,
    "published": "2025-02-12T08:24:36Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Precise classification of megakaryocytes is crucial for diagnosing myelodysplastic syndromes. Although self-supervised learning has shown promise in medical image analysis, its application to classifying megakaryocytes in stained slides faces three main challenges: (1) pervasive background noise that obscures cellular details, (2) a long-tailed distribution that limits data for rare subtypes, and (3) complex morphological variations leading to high intra-class variability. To address these issue",
    "arxiv_url": "https://arxiv.org/abs/2502.08200v2",
    "pdf_url": "https://arxiv.org/pdf/2502.08200v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2502.08200",
    "arxiv_authors": [
      "Linghao Zhuang",
      "Ying Zhang",
      "Gege Yuan",
      "Xingyue Zhao",
      "Zhiping Jiang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=ActiveSSF%3A+An+Active-Learning-Guided+Self-Supervised+Framework+for+Long-Tailed+Megakaryocyte+Classification+Linghao+Zhuang+Ying+Zhang+Gege+Yuan+Xingyue+Zhao+Zhiping+Jiang",
    "gs_search_success": true,
    "gs_authors": [
      "d2gg0hQAAAAJ",
      "fZuqWe0AAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2504.17829",
    "title": "Fine-Tuning Adversarially-Robust Transformers for Single-Image Dehazing",
    "year": 2025,
    "published": "2025-04-24T08:52:14Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Single-image dehazing is an important topic in remote sensing applications, enhancing the quality of acquired images and increasing object detection precision. However, the reliability of such structures has not been sufficiently analyzed, which poses them to the risk of imperceptible perturbations that can significantly hinder their performance. In this work, we show that state-of-the-art image-to-image dehazing transformers are susceptible to adversarial noise, with even 1 pixel change being a",
    "arxiv_url": "https://arxiv.org/abs/2504.17829v1",
    "pdf_url": "https://arxiv.org/pdf/2504.17829v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2504.17829",
    "arxiv_authors": [
      "Vlad Vasilescu",
      "Ana Neacsu",
      "Daniela Faur"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Fine-Tuning+Adversarially-Robust+Transformers+for+Single-Image+Dehazing+Vlad+Vasilescu+Ana+Neacsu+Daniela+Faur",
    "gs_search_success": true,
    "gs_authors": [
      "-2mCZvsAAAAJ",
      "yp3fV5QAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2403.17409",
    "title": "Neural Clustering based Visual Representation Learning",
    "year": 2024,
    "published": "2024-03-26T06:04:50Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "We investigate a fundamental aspect of machine vision: the measurement of features, by revisiting clustering, one of the most classic approaches in machine learning and data analysis. Existing visual feature extractors, including ConvNets, ViTs, and MLPs, represent an image as rectangular regions. Though prevalent, such a grid-style paradigm is built upon engineering practice and lacks explicit modeling of data distribution. In this work, we propose feature extraction with clustering (FEC), a co",
    "arxiv_url": "https://arxiv.org/abs/2403.17409v1",
    "pdf_url": "https://arxiv.org/pdf/2403.17409v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2403.17409",
    "arxiv_authors": [
      "Guikun Chen",
      "Xia Li",
      "Yi Yang",
      "Wenguan Wang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Neural+Clustering+based+Visual+Representation+Learning+Guikun+Chen+Xia+Li+Yi+Yang+Wenguan+Wang",
    "gs_search_success": true,
    "gs_authors": [
      "XKGZhEcAAAAJ",
      "RMSuNFwAAAAJ",
      "CqAQQkgAAAAJ",
      "I1TOdpkAAAAJ"
    ],
    "citation_count": 30,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2304.09842",
    "title": "Chameleon: Plug-and-Play Compositional Reasoning with Large Language Models",
    "year": 2023,
    "published": "2023-04-19T17:47:47Z",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "Large language models (LLMs) have achieved remarkable progress in solving various natural language processing tasks due to emergent reasoning abilities. However, LLMs have inherent limitations as they are incapable of accessing up-to-date information (stored on the Web or in task-specific knowledge bases), using external tools, and performing precise mathematical and logical reasoning. In this paper, we present Chameleon, an AI system that mitigates these limitations by augmenting LLMs with plug",
    "arxiv_url": "https://arxiv.org/abs/2304.09842v3",
    "pdf_url": "https://arxiv.org/pdf/2304.09842v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2304.09842",
    "arxiv_authors": [
      "Pan Lu",
      "Baolin Peng",
      "Hao Cheng",
      "Michel Galley",
      "Kai-Wei Chang",
      "Ying Nian Wu",
      "Song-Chun Zhu",
      "Jianfeng Gao"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Chameleon%3A+Plug-and-Play+Compositional+Reasoning+with+Large+Language+Models+Pan+Lu+Baolin+Peng+Hao+Cheng+Michel+Galley+Kai-Wei+Chang",
    "gs_search_success": true,
    "gs_authors": [
      "CQ1cqKkAAAAJ",
      "IyucsdQAAAAJ",
      "fqDBtzYAAAAJ",
      "7k_1QFIAAAAJ",
      "u1CNjgwAAAAJ",
      "d9s3sbQAAAAJ",
      "rs1M7CAAAAAJ"
    ],
    "citation_count": 602,
    "gs_author_count": 8
  },
  {
    "arxiv_id": "2406.16093",
    "title": "Towards Natural Language-Driven Assembly Using Foundation Models",
    "year": 2024,
    "published": "2024-06-23T12:14:37Z",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "Large Language Models (LLMs) and strong vision models have enabled rapid research and development in the field of Vision-Language-Action models that enable robotic control. The main objective of these methods is to develop a generalist policy that can control robots with various embodiments. However, in industrial robotic applications such as automated assembly and disassembly, some tasks, such as insertion, demand greater accuracy and involve intricate factors like contact engagement, friction ",
    "arxiv_url": "https://arxiv.org/abs/2406.16093v1",
    "pdf_url": "https://arxiv.org/pdf/2406.16093v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2406.16093",
    "arxiv_authors": [
      "Omkar Joglekar",
      "Tal Lancewicki",
      "Shir Kozlovsky",
      "Vladimir Tchuiev",
      "Zohar Feldman",
      "Dotan Di Castro"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Towards+Natural+Language-Driven+Assembly+Using+Foundation+Models+Omkar+Joglekar+Tal+Lancewicki+Shir+Kozlovsky+Vladimir+Tchuiev+Zohar+Feldman",
    "gs_search_success": true,
    "gs_authors": [
      "EUvJChIAAAAJ",
      "SGLnpNMAAAAJ",
      "KxrnkosAAAAJ",
      "zhQaFaMAAAAJ",
      "hwG4VAoAAAAJ",
      "wn0Gl5MAAAAJ"
    ],
    "citation_count": 2,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2404.09991",
    "title": "EgoPet: Egomotion and Interaction Data from an Animal's Perspective",
    "year": 2024,
    "published": "2024-04-15T17:59:47Z",
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "abstract": "Animals perceive the world to plan their actions and interact with other agents to accomplish complex tasks, demonstrating capabilities that are still unmatched by AI systems. To advance our understanding and reduce the gap between the capabilities of animals and AI systems, we introduce a dataset of pet egomotion imagery with diverse examples of simultaneous egomotion and multi-agent interaction. Current video datasets separately contain egomotion and interaction examples, but rarely both at th",
    "arxiv_url": "https://arxiv.org/abs/2404.09991v1",
    "pdf_url": "https://arxiv.org/pdf/2404.09991v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2404.09991",
    "arxiv_authors": [
      "Amir Bar",
      "Arya Bakhtiar",
      "Danny Tran",
      "Antonio Loquercio",
      "Jathushan Rajasegaran",
      "Yann LeCun",
      "Amir Globerson",
      "Trevor Darrell"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=EgoPet%3A+Egomotion+and+Interaction+Data+from+an+Animal%27s+Perspective+Amir+Bar+Arya+Bakhtiar+Danny+Tran+Antonio+Loquercio+Jathushan+Rajasegaran",
    "gs_search_success": true,
    "gs_authors": [
      "WLN3QrAAAAAJ",
      "azjJi_8AAAAJ",
      "Ctp3igcAAAAJ",
      "Y83XwowAAAAJ",
      "5JserkUAAAAJ",
      "bh-uRFMAAAAJ",
      "pbmjtZsAAAAJ",
      "L__n1LUAAAAJ"
    ],
    "citation_count": 9,
    "gs_author_count": 8
  },
  {
    "arxiv_id": "2403.17503",
    "title": "DS-AL: A Dual-Stream Analytic Learning for Exemplar-Free Class-Incremental Learning",
    "year": 2024,
    "published": "2024-03-26T09:04:18Z",
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "abstract": "Class-incremental learning (CIL) under an exemplar-free constraint has presented a significant challenge. Existing methods adhering to this constraint are prone to catastrophic forgetting, far more so than replay-based techniques that retain access to past samples. In this paper, to solve the exemplar-free CIL problem, we propose a Dual-Stream Analytic Learning (DS-AL) approach. The DS-AL contains a main stream offering an analytical (i.e., closed-form) linear solution, and a compensation stream",
    "arxiv_url": "https://arxiv.org/abs/2403.17503v1",
    "pdf_url": "https://arxiv.org/pdf/2403.17503v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2403.17503",
    "arxiv_authors": [
      "Huiping Zhuang",
      "Run He",
      "Kai Tong",
      "Ziqian Zeng",
      "Cen Chen",
      "Zhiping Lin"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=DS-AL%3A+A+Dual-Stream+Analytic+Learning+for+Exemplar-Free+Class-Incremental+Learning+Huiping+Zhuang+Run+He+Kai+Tong+Ziqian+Zeng+Cen+Chen",
    "gs_search_success": true,
    "gs_authors": [
      "92D0pkwAAAAJ",
      "vCXxuLkAAAAJ",
      "pPsNBWUAAAAJ",
      "cN4SxagAAAAJ"
    ],
    "citation_count": 51,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2411.04155",
    "title": "MINDSETS: Multi-omics Integration with Neuroimaging for Dementia Subtyping and Effective Temporal Study",
    "year": 2024,
    "published": "2024-11-06T10:13:28Z",
    "categories": [
      "eess.IV",
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "In the complex realm of cognitive disorders, Alzheimer's disease (AD) and vascular dementia (VaD) are the two most prevalent dementia types, presenting entangled symptoms yet requiring distinct treatment approaches. The crux of effective treatment in slowing neurodegeneration lies in early, accurate diagnosis, as this significantly assists doctors in determining the appropriate course of action. However, current diagnostic practices often delay VaD diagnosis, impeding timely intervention and adv",
    "arxiv_url": "https://arxiv.org/abs/2411.04155v1",
    "pdf_url": "https://arxiv.org/pdf/2411.04155v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2411.04155",
    "arxiv_authors": [
      "Salma Hassan",
      "Dawlat Akaila",
      "Maryam Arjemandi",
      "Vijay Papineni",
      "Mohammad Yaqub"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=MINDSETS%3A+Multi-omics+Integration+with+Neuroimaging+for+Dementia+Subtyping+and+Effective+Temporal+Study+Salma+Hassan+Dawlat+Akaila+Maryam+Arjemandi+Vijay+Papineni+Mohammad+Yaqub",
    "gs_search_success": true,
    "gs_authors": [
      "0DhSy_EAAAAJ",
      "c_6jdkYAAAAJ",
      "9dfn5GkAAAAJ"
    ],
    "citation_count": 10,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2408.12671",
    "title": "Joint Image De-noising and Enhancement for Satellite-Based SAR",
    "year": 2024,
    "published": "2024-08-06T18:44:16Z",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "abstract": "The reconstructed images from the Synthetic Aperture Radar (SAR) data suffer from multiplicative noise as well as low contrast level. These two factors impact the quality of the SAR images significantly and prevent any attempt to extract valuable information from the processed data. The necessity for mitigating these effects in the field of SAR imaging is of high importance. Therefore, in this paper, we address the aforementioned issues and propose a technique to handle these shortcomings simult",
    "arxiv_url": "https://arxiv.org/abs/2408.12671v2",
    "pdf_url": "https://arxiv.org/pdf/2408.12671v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2408.12671",
    "arxiv_authors": [
      "Shahrokh Hamidi"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Joint+Image+De-noising+and+Enhancement+for+Satellite-Based+SAR+Shahrokh+Hamidi",
    "gs_search_success": true,
    "gs_authors": [
      "fPFBTC0AAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 1
  },
  {
    "arxiv_id": "2403.07289",
    "title": "Rediscovering BCE Loss for Uniform Classification",
    "year": 2024,
    "published": "2024-03-12T03:44:40Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "This paper introduces the concept of uniform classification, which employs a unified threshold to classify all samples rather than adaptive threshold classifying each individual sample. We also propose the uniform classification accuracy as a metric to measure the model's performance in uniform classification. Furthermore, begin with a naive loss, we mathematically derive a loss function suitable for the uniform classification, which is the BCE function integrated with a unified bias. We demonst",
    "arxiv_url": "https://arxiv.org/abs/2403.07289v1",
    "pdf_url": "https://arxiv.org/pdf/2403.07289v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2403.07289",
    "arxiv_authors": [
      "Qiufu Li",
      "Xi Jia",
      "Jiancan Zhou",
      "Linlin Shen",
      "Jinming Duan"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Rediscovering+BCE+Loss+for+Uniform+Classification+Qiufu+Li+Xi+Jia+Jiancan+Zhou+Linlin+Shen+Jinming+Duan",
    "gs_search_success": true,
    "gs_authors": [
      "s2tA2YgAAAAJ",
      "kpoePUQAAAAJ",
      "AZ_y9HgAAAAJ",
      "LEvFR3MAAAAJ"
    ],
    "citation_count": 37,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2406.17670",
    "title": "Brain Tumor Classification using Vision Transformer with Selective Cross-Attention Mechanism and Feature Calibration",
    "year": 2024,
    "published": "2024-06-25T15:58:56Z",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "abstract": "Brain tumor classification is a challenging task in medical image analysis. In this paper, we propose a novel approach to brain tumor classification using a vision transformer with a novel cross-attention mechanism. Our approach leverages the strengths of transformers in modeling long-range dependencies and multi-scale feature fusion. We introduce two new mechanisms to improve the performance of the cross-attention fusion module: Feature Calibration Mechanism (FCM) and Selective Cross-Attention ",
    "arxiv_url": "https://arxiv.org/abs/2406.17670v2",
    "pdf_url": "https://arxiv.org/pdf/2406.17670v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2406.17670",
    "arxiv_authors": [
      "Mohammad Ali Labbaf Khaniki",
      "Marzieh Mirzaeibonehkhater",
      "Mohammad Manthouri",
      "Elham Hasani"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Brain+Tumor+Classification+using+Vision+Transformer+with+Selective+Cross-Attention+Mechanism+and+Feature+Calibration+Mohammad+Ali+Labbaf+Khaniki+Marzieh+Mirzaeibonehkhater+Mohammad+Manthouri+Elham+Hasani",
    "gs_search_success": true,
    "gs_authors": [
      "EDX3n0cAAAAJ"
    ],
    "citation_count": 14,
    "gs_author_count": null
  },
  {
    "arxiv_id": "2308.08658",
    "title": "A Data-Theoretic Approach to Identifying Violent Facial Expressions in Social Crime Contexts",
    "year": 2023,
    "published": "2023-08-16T20:12:43Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Human Facial Expressions plays an important role in identifying human actions or intention. Facial expressions can represent any specific action of any person and the pattern of violent behavior of any person strongly depends on the geographic region. Here we have designed an automated system by using a Convolutional Neural Network which can detect whether a person has any intention to commit any crime or not. Here we proposed a new method that can identify criminal intentions or violent behavio",
    "arxiv_url": "https://arxiv.org/abs/2308.08658v2",
    "pdf_url": "https://arxiv.org/pdf/2308.08658v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2308.08658",
    "arxiv_authors": [
      "Arindam Kumar Paul"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=A+Data-Theoretic+Approach+to+Identifying+Violent+Facial+Expressions+in+Social+Crime+Contexts+Arindam+Kumar+Paul",
    "gs_search_success": true,
    "gs_authors": [],
    "citation_count": null,
    "gs_author_count": 1
  },
  {
    "arxiv_id": "2407.06174",
    "title": "The Tug-of-War Between Deepfake Generation and Detection",
    "year": 2024,
    "published": "2024-07-08T17:49:41Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Multimodal generative models are rapidly evolving, leading to a surge in the generation of realistic video and audio that offers exciting possibilities but also serious risks. Deepfake videos, which can convincingly impersonate individuals, have particularly garnered attention due to their potential misuse in spreading misinformation and creating fraudulent content. This survey paper examines the dual landscape of deepfake video generation and detection, emphasizing the need for effective counte",
    "arxiv_url": "https://arxiv.org/abs/2407.06174v4",
    "pdf_url": "https://arxiv.org/pdf/2407.06174v4",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2407.06174",
    "arxiv_authors": [
      "Hannah Lee",
      "Changyeon Lee",
      "Kevin Farhat",
      "Lin Qiu",
      "Steve Geluso",
      "Aerin Kim",
      "Oren Etzioni"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=The+Tug-of-War+Between+Deepfake+Generation+and+Detection+Hannah+Lee+Changyeon+Lee+Kevin+Farhat+Lin+Qiu+Steve+Geluso",
    "gs_search_success": true,
    "gs_authors": [
      "CCx08AgAAAAJ",
      "sFsWE74AAAAJ",
      "Cj9C_d4AAAAJ",
      "EXU4zxcAAAAJ",
      "lSqYhxYAAAAJ",
      "EMwpxJsAAAAJ"
    ],
    "citation_count": 15,
    "gs_author_count": 7
  },
  {
    "arxiv_id": "2305.09928",
    "title": "Tinto: Multisensor Benchmark for 3D Hyperspectral Point Cloud Segmentation in the Geosciences",
    "year": 2023,
    "published": "2023-05-17T03:24:08Z",
    "categories": [
      "cs.CV",
      "cs.LG",
      "eess.IV"
    ],
    "abstract": "The increasing use of deep learning techniques has reduced interpretation time and, ideally, reduced interpreter bias by automatically deriving geological maps from digital outcrop models. However, accurate validation of these automated mapping approaches is a significant challenge due to the subjective nature of geological mapping and the difficulty in collecting quantitative validation data. Additionally, many state-of-the-art deep learning methods are limited to 2D image data, which is insuff",
    "arxiv_url": "https://arxiv.org/abs/2305.09928v2",
    "pdf_url": "https://arxiv.org/pdf/2305.09928v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2305.09928",
    "arxiv_authors": [
      "Ahmed J. Afifi",
      "Samuel T. Thiele",
      "Aldino Rizaldy",
      "Sandra Lorenz",
      "Pedram Ghamisi",
      "Raimon Tolosana-Delgado",
      "Moritz Kirsch",
      "Richard Gloaguen",
      "Michael Heizmann"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Tinto%3A+Multisensor+Benchmark+for+3D+Hyperspectral+Point+Cloud+Segmentation+in+the+Geosciences+Ahmed+J.+Afifi+Samuel+T.+Thiele+Aldino+Rizaldy+Sandra+Lorenz+Pedram+Ghamisi",
    "gs_search_success": true,
    "gs_authors": [
      "RO_VVEEAAAAJ",
      "mvQ44gkAAAAJ",
      "ZuY9At8AAAAJ",
      "aoCqZksAAAAJ",
      "KAtpu0wAAAAJ",
      "hnHJsyEAAAAJ",
      "Gr9afd0AAAAJ"
    ],
    "citation_count": 20,
    "gs_author_count": 7
  },
  {
    "arxiv_id": "2505.01050",
    "title": "Transferable Adversarial Attacks on Black-Box Vision-Language Models",
    "year": 2025,
    "published": "2025-05-02T06:51:11Z",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "Vision Large Language Models (VLLMs) are increasingly deployed to offer advanced capabilities on inputs comprising both text and images. While prior research has shown that adversarial attacks can transfer from open-source to proprietary black-box models in text-only and vision-only contexts, the extent and effectiveness of such vulnerabilities remain underexplored for VLLMs. We present a comprehensive analysis demonstrating that targeted adversarial examples are highly transferable to widely-us",
    "arxiv_url": "https://arxiv.org/abs/2505.01050v1",
    "pdf_url": "https://arxiv.org/pdf/2505.01050v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2505.01050",
    "arxiv_authors": [
      "Kai Hu",
      "Weichen Yu",
      "Li Zhang",
      "Alexander Robey",
      "Andy Zou",
      "Chengming Xu",
      "Haoqi Hu",
      "Matt Fredrikson"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Transferable+Adversarial+Attacks+on+Black-Box+Vision-Language+Models+Kai+Hu+Weichen+Yu+Li+Zhang+Alexander+Robey+Andy+Zou",
    "gs_search_success": true,
    "gs_authors": [
      "pjcYzvYAAAAJ",
      "V5NWZc8AAAAJ",
      "zIHfBlMAAAAJ",
      "tMYCvLAAAAAJ",
      "LeCleC4AAAAJ",
      "zir09KwAAAAJ",
      "Dn1rSvkAAAAJ"
    ],
    "citation_count": 8,
    "gs_author_count": 8
  },
  {
    "arxiv_id": "2412.13179",
    "title": "A Pipeline and NIR-Enhanced Dataset for Parking Lot Segmentation",
    "year": 2024,
    "published": "2024-12-09T00:38:34Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Discussions of minimum parking requirement policies often include maps of parking lots, which are time consuming to construct manually. Open source datasets for such parking lots are scarce, particularly for US cities. This paper introduces the idea of using Near-Infrared (NIR) channels as input and several post-processing techniques to improve the prediction of off-street surface parking lots using satellite imagery. We constructed two datasets with 12,617 image-mask pairs each: one with 3-chan",
    "arxiv_url": "https://arxiv.org/abs/2412.13179v1",
    "pdf_url": "https://arxiv.org/pdf/2412.13179v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2412.13179",
    "arxiv_authors": [
      "Shirin Qiam",
      "Saipraneeth Devunuri",
      "Lewis J. Lehe"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=A+Pipeline+and+NIR-Enhanced+Dataset+for+Parking+Lot+Segmentation+Shirin+Qiam+Saipraneeth+Devunuri+Lewis+J.+Lehe",
    "gs_search_success": true,
    "gs_authors": [
      "z2Z271UAAAAJ",
      "7bRMk4oAAAAJ",
      "Igxd5_0AAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2412.03021",
    "title": "PEMF-VTO: Point-Enhanced Video Virtual Try-on via Mask-free Paradigm",
    "year": 2024,
    "published": "2024-12-04T04:24:15Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Video Virtual Try-on aims to seamlessly transfer a reference garment onto a target person in a video while preserving both visual fidelity and temporal coherence. Existing methods typically rely on inpainting masks to define the try-on area, enabling accurate garment transfer for simple scenes (e.g., in-shop videos). However, these mask-based approaches struggle with complex real-world scenarios, as overly large and inconsistent masks often destroy spatial-temporal information, leading to distor",
    "arxiv_url": "https://arxiv.org/abs/2412.03021v5",
    "pdf_url": "https://arxiv.org/pdf/2412.03021v5",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2412.03021",
    "arxiv_authors": [
      "Tianyu Chang",
      "Xiaohao Chen",
      "Zhichao Wei",
      "Xuanpu Zhang",
      "Qing-Guo Chen",
      "Weihua Luo",
      "Peipei Song",
      "Xun Yang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=PEMF-VTO%3A+Point-Enhanced+Video+Virtual+Try-on+via+Mask-free+Paradigm+Tianyu+Chang+Xiaohao+Chen+Zhichao+Wei+Xuanpu+Zhang+Qing-Guo+Chen",
    "gs_search_success": true,
    "gs_authors": [
      "EzPr96kAAAAJ",
      "GlqRHLcAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 8
  },
  {
    "arxiv_id": "2410.01216",
    "title": "RS-FME-SwinT: A Novel Feature Map Enhancement Framework Integrating Customized SwinT with Residual and Spatial CNN for Monkeypox Diagnosis",
    "year": 2024,
    "published": "2024-10-02T03:57:57Z",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "abstract": "Monkeypox (MPox) has emerged as a significant global concern, with cases steadily increasing daily. Conventional detection methods, including polymerase chain reaction (PCR) and manual examination, exhibit challenges of low sensitivity, high cost, and substantial workload. Therefore, deep learning offers an automated solution; however, the datasets include data scarcity, texture, contrast, inter-intra class variability, and similarities with other skin infectious diseases. In this regard, a nove",
    "arxiv_url": "https://arxiv.org/abs/2410.01216v1",
    "pdf_url": "https://arxiv.org/pdf/2410.01216v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2410.01216",
    "arxiv_authors": [
      "Saddam Hussain Khan",
      "Rashid Iqbal"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=RS-FME-SwinT%3A+A+Novel+Feature+Map+Enhancement+Framework+Integrating+Customized+SwinT+with+Residual+and+Spatial+CNN+for+Monkeypox+Diagnosis+Saddam+Hussain+Khan+Rashid+Iqbal",
    "gs_search_success": true,
    "gs_authors": [
      "j_ImpdYAAAAJ"
    ],
    "citation_count": 6,
    "gs_author_count": null
  },
  {
    "arxiv_id": "2406.17915",
    "title": "Semi-supervised classification of dental conditions in panoramic radiographs using large language model and instance segmentation: A real-world dataset evaluation",
    "year": 2024,
    "published": "2024-06-25T19:56:12Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Dental panoramic radiographs offer vast diagnostic opportunities, but training supervised deep learning networks for automatic analysis of those radiology images is hampered by a shortage of labeled data. Here, a different perspective on this problem is introduced. A semi-supervised learning framework is proposed to classify thirteen dental conditions on panoramic radiographs, with a particular emphasis on teeth. Large language models were explored to annotate the most common dental conditions b",
    "arxiv_url": "https://arxiv.org/abs/2406.17915v1",
    "pdf_url": "https://arxiv.org/pdf/2406.17915v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2406.17915",
    "arxiv_authors": [
      "Bernardo Silva",
      "Jefferson Fontinele",
      "Carolina Letícia Zilli Vieira",
      "João Manuel R. S. Tavares",
      "Patricia Ramos Cury",
      "Luciano Oliveira"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Semi-supervised+classification+of+dental+conditions+in+panoramic+radiographs+using+large+language+model+and+instance+segmentation%3A+A+real-world+dataset+evaluation+Bernardo+Silva+Jefferson+Fontinele+Carolina+Let%C3%ADcia+Zilli+Vieira+Jo%C3%A3o+Manuel+R.+S.+Tavares+Patricia+Ramos+Cury",
    "gs_search_success": true,
    "gs_authors": [
      "P6xPZi4AAAAJ",
      "x048QhQAAAAJ",
      "EkGg3cMAAAAJ",
      "yhQ2Q0QAAAAJ",
      "FbhYXwsAAAAJ",
      "urdxG3EAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2404.14343",
    "title": "Heterogeneous Face Recognition Using Domain Invariant Units",
    "year": 2024,
    "published": "2024-04-22T16:58:37Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Heterogeneous Face Recognition (HFR) aims to expand the applicability of Face Recognition (FR) systems to challenging scenarios, enabling the matching of face images across different domains, such as matching thermal images to visible spectra. However, the development of HFR systems is challenging because of the significant domain gap between modalities and the lack of availability of large-scale paired multi-channel data. In this work, we leverage a pretrained face recognition model as a teache",
    "arxiv_url": "https://arxiv.org/abs/2404.14343v1",
    "pdf_url": "https://arxiv.org/pdf/2404.14343v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2404.14343",
    "arxiv_authors": [
      "Anjith George",
      "Sebastien Marcel"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Heterogeneous+Face+Recognition+Using+Domain+Invariant+Units+Anjith+George+Sebastien+Marcel",
    "gs_search_success": true,
    "gs_authors": [
      "4CCrDp0AAAAJ",
      "K9ku4jYAAAAJ"
    ],
    "citation_count": 6,
    "gs_author_count": 2
  },
  {
    "arxiv_id": "2505.10873",
    "title": "Hashing for Structure-based Anomaly Detection",
    "year": 2025,
    "published": "2025-05-16T05:31:50Z",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "stat.ML"
    ],
    "abstract": "We focus on the problem of identifying samples in a set that do not conform to structured patterns represented by low-dimensional manifolds. An effective way to solve this problem is to embed data in a high dimensional space, called Preference Space, where anomalies can be identified as the most isolated points. In this work, we employ Locality Sensitive Hashing to avoid explicit computation of distances in high dimensions and thus improve Anomaly Detection efficiency. Specifically, we present a",
    "arxiv_url": "https://arxiv.org/abs/2505.10873v1",
    "pdf_url": "https://arxiv.org/pdf/2505.10873v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2505.10873",
    "arxiv_authors": [
      "Filippo Leveni",
      "Luca Magri",
      "Cesare Alippi",
      "Giacomo Boracchi"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Hashing+for+Structure-based+Anomaly+Detection+Filippo+Leveni+Luca+Magri+Cesare+Alippi+Giacomo+Boracchi",
    "gs_search_success": true,
    "gs_authors": [
      "dF3qKXMAAAAJ",
      "zyzNf4AAAAAJ",
      "lBa_mnYAAAAJ",
      "dYX0BRwAAAAJ"
    ],
    "citation_count": 4,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2502.20490",
    "title": "EgoNormia: Benchmarking Physical Social Norm Understanding",
    "year": 2025,
    "published": "2025-02-27T19:54:16Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "abstract": "Human activity is moderated by norms; however, supervision for normative reasoning is sparse, particularly where norms are physically- or socially-grounded. We thus present EGONORMIA $\\|ε\\|$, comprising 1,853 (200 for EGONORMIA-verified) multiple choice questions (MCQs) grounded within egocentric videos of human interactions, enabling the evaluation and improvement of normative reasoning in vision-language models (VLMs). EGONORMIA spans seven norm categories: safety, privacy, proxemics, politene",
    "arxiv_url": "https://arxiv.org/abs/2502.20490v5",
    "pdf_url": "https://arxiv.org/pdf/2502.20490v5",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2502.20490",
    "arxiv_authors": [
      "MohammadHossein Rezaei",
      "Yicheng Fu",
      "Phil Cuvin",
      "Caleb Ziems",
      "Yanzhe Zhang",
      "Hao Zhu",
      "Diyi Yang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=EgoNormia%3A+Benchmarking+Physical+Social+Norm+Understanding+MohammadHossein+Rezaei+Yicheng+Fu+Phil+Cuvin+Caleb+Ziems+Yanzhe+Zhang",
    "gs_search_success": true,
    "gs_authors": [
      "INq7JWIAAAAJ",
      "iJImxvUAAAAJ",
      "j9jhYqQAAAAJ",
      "Phj7N40AAAAJ",
      "Hm4XL1AAAAAJ",
      "-3yFcsMAAAAJ"
    ],
    "citation_count": 8,
    "gs_author_count": 7
  },
  {
    "arxiv_id": "2408.00491",
    "title": "GalleryGPT: Analyzing Paintings with Large Multimodal Models",
    "year": 2024,
    "published": "2024-08-01T11:52:56Z",
    "categories": [
      "cs.CL",
      "cs.CV",
      "cs.MM"
    ],
    "abstract": "Artwork analysis is important and fundamental skill for art appreciation, which could enrich personal aesthetic sensibility and facilitate the critical thinking ability. Understanding artworks is challenging due to its subjective nature, diverse interpretations, and complex visual elements, requiring expertise in art history, cultural background, and aesthetic theory. However, limited by the data collection and model ability, previous works for automatically analyzing artworks mainly focus on cl",
    "arxiv_url": "https://arxiv.org/abs/2408.00491v1",
    "pdf_url": "https://arxiv.org/pdf/2408.00491v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2408.00491",
    "arxiv_authors": [
      "Yi Bin",
      "Wenhao Shi",
      "Yujuan Ding",
      "Zhiqiang Hu",
      "Zheng Wang",
      "Yang Yang",
      "See-Kiong Ng",
      "Heng Tao Shen"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=GalleryGPT%3A+Analyzing+Paintings+with+Large+Multimodal+Models+Yi+Bin+Wenhao+Shi+Yujuan+Ding+Zhiqiang+Hu+Zheng+Wang",
    "gs_search_success": false,
    "gs_authors": [],
    "citation_count": null,
    "gs_author_count": null,
    "error_type": "fetch_failed"
  },
  {
    "arxiv_id": "2503.17804",
    "title": "DVG-Diffusion: Dual-View Guided Diffusion Model for CT Reconstruction from X-Rays",
    "year": 2025,
    "published": "2025-03-22T16:03:18Z",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "abstract": "Directly reconstructing 3D CT volume from few-view 2D X-rays using an end-to-end deep learning network is a challenging task, as X-ray images are merely projection views of the 3D CT volume. In this work, we facilitate complex 2D X-ray image to 3D CT mapping by incorporating new view synthesis, and reduce the learning difficulty through view-guided feature alignment. Specifically, we propose a dual-view guided diffusion model (DVG-Diffusion), which couples a real input X-ray view and a synthesiz",
    "arxiv_url": "https://arxiv.org/abs/2503.17804v1",
    "pdf_url": "https://arxiv.org/pdf/2503.17804v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2503.17804",
    "arxiv_authors": [
      "Xing Xie",
      "Jiawei Liu",
      "Huijie Fan",
      "Zhi Han",
      "Yandong Tang",
      "Liangqiong Qu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=DVG-Diffusion%3A+Dual-View+Guided+Diffusion+Model+for+CT+Reconstruction+from+X-Rays+Xing+Xie+Jiawei+Liu+Huijie+Fan+Zhi+Han+Yandong+Tang",
    "gs_search_success": true,
    "gs_authors": [
      "Zl54a74AAAAJ",
      "g-UUlvcAAAAJ",
      "ruKpgzwAAAAJ",
      "qg58Xx8AAAAJ",
      "CaglmKAAAAAJ",
      "tbPr7WsAAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2311.07623",
    "title": "PadChannel: Improving CNN Performance through Explicit Padding Encoding",
    "year": 2023,
    "published": "2023-11-13T07:44:56Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "In convolutional neural networks (CNNs), padding plays a pivotal role in preserving spatial dimensions throughout the layers. Traditional padding techniques do not explicitly distinguish between the actual image content and the padded regions, potentially causing CNNs to incorrectly interpret the boundary pixels or regions that resemble boundaries. This ambiguity can lead to suboptimal feature extraction. To address this, we propose PadChannel, a novel padding method that encodes padding statuse",
    "arxiv_url": "https://arxiv.org/abs/2311.07623v2",
    "pdf_url": "https://arxiv.org/pdf/2311.07623v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2311.07623",
    "arxiv_authors": [
      "Juho Kim"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=PadChannel%3A+Improving+CNN+Performance+through+Explicit+Padding+Encoding+Juho+Kim",
    "gs_search_success": false,
    "gs_authors": [],
    "citation_count": null,
    "gs_author_count": null,
    "error_type": "fetch_failed"
  },
  {
    "arxiv_id": "2311.13069",
    "title": "FuseNet: Self-Supervised Dual-Path Network for Medical Image Segmentation",
    "year": 2023,
    "published": "2023-11-22T00:03:16Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Semantic segmentation, a crucial task in computer vision, often relies on labor-intensive and costly annotated datasets for training. In response to this challenge, we introduce FuseNet, a dual-stream framework for self-supervised semantic segmentation that eliminates the need for manual annotation. FuseNet leverages the shared semantic dependencies between the original and augmented images to create a clustering space, effectively assigning pixels to semantically related clusters, and ultimatel",
    "arxiv_url": "https://arxiv.org/abs/2311.13069v1",
    "pdf_url": "https://arxiv.org/pdf/2311.13069v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2311.13069",
    "arxiv_authors": [
      "Amirhossein Kazerouni",
      "Sanaz Karimijafarbigloo",
      "Reza Azad",
      "Yury Velichko",
      "Ulas Bagci",
      "Dorit Merhof"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=FuseNet%3A+Self-Supervised+Dual-Path+Network+for+Medical+Image+Segmentation+Amirhossein+Kazerouni+Sanaz+Karimijafarbigloo+Reza+Azad+Yury+Velichko+Ulas+Bagci",
    "gs_search_success": true,
    "gs_authors": [
      "U_vmD3sAAAAJ",
      "aKDCc3MAAAAJ",
      "0c0rMr0AAAAJ",
      "9LUdPM4AAAAJ"
    ],
    "citation_count": 5,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2406.16863",
    "title": "FreeTraj: Tuning-Free Trajectory Control in Video Diffusion Models",
    "year": 2024,
    "published": "2024-06-24T17:59:56Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Diffusion model has demonstrated remarkable capability in video generation, which further sparks interest in introducing trajectory control into the generation process. While existing works mainly focus on training-based methods (e.g., conditional adapter), we argue that diffusion model itself allows decent control over the generated content without requiring any training. In this study, we introduce a tuning-free framework to achieve trajectory-controllable video generation, by imposing guidanc",
    "arxiv_url": "https://arxiv.org/abs/2406.16863v1",
    "pdf_url": "https://arxiv.org/pdf/2406.16863v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2406.16863",
    "arxiv_authors": [
      "Haonan Qiu",
      "Zhaoxi Chen",
      "Zhouxia Wang",
      "Yingqing He",
      "Menghan Xia",
      "Ziwei Liu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=FreeTraj%3A+Tuning-Free+Trajectory+Control+in+Video+Diffusion+Models+Haonan+Qiu+Zhaoxi+Chen+Zhouxia+Wang+Yingqing+He+Menghan+Xia",
    "gs_search_success": true,
    "gs_authors": [
      "lc45xlcAAAAJ",
      "HsV0WbwAAAAJ",
      "IJxaAQQAAAAJ",
      "JWds_bQAAAAJ",
      "8Ss6ahEAAAAJ"
    ],
    "citation_count": 33,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2303.04364",
    "title": "Dynamic Scenario Representation Learning for Motion Forecasting with Heterogeneous Graph Convolutional Recurrent Networks",
    "year": 2023,
    "published": "2023-03-08T04:10:04Z",
    "categories": [
      "cs.AI",
      "cs.CV",
      "cs.LG",
      "cs.RO"
    ],
    "abstract": "Due to the complex and changing interactions in dynamic scenarios, motion forecasting is a challenging problem in autonomous driving. Most existing works exploit static road graphs to characterize scenarios and are limited in modeling evolving spatio-temporal dependencies in dynamic scenarios. In this paper, we resort to dynamic heterogeneous graphs to model the scenario. Various scenario components including vehicles (agents) and lanes, multi-type interactions, and their changes over time are j",
    "arxiv_url": "https://arxiv.org/abs/2303.04364v1",
    "pdf_url": "https://arxiv.org/pdf/2303.04364v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2303.04364",
    "arxiv_authors": [
      "Xing Gao",
      "Xiaogang Jia",
      "Yikang Li",
      "Hongkai Xiong"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Dynamic+Scenario+Representation+Learning+for+Motion+Forecasting+with+Heterogeneous+Graph+Convolutional+Recurrent+Networks+Xing+Gao+Xiaogang+Jia+Yikang+Li+Hongkai+Xiong",
    "gs_search_success": true,
    "gs_authors": [
      "E7Tja9gAAAAJ",
      "G9b6hpYAAAAJ",
      "6rjfzQUAAAAJ",
      "bB16iN4AAAAJ"
    ],
    "citation_count": 58,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2503.01122",
    "title": "ACCORD: Alleviating Concept Coupling through Dependence Regularization for Text-to-Image Diffusion Personalization",
    "year": 2025,
    "published": "2025-03-03T03:00:04Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Image personalization has garnered attention for its ability to customize Text-to-Image generation using only a few reference images. However, a key challenge in image personalization is the issue of conceptual coupling, where the limited number of reference images leads the model to form unwanted associations between the personalization target and other concepts. Current methods attempt to tackle this issue indirectly, leading to a suboptimal balance between text control and personalization fid",
    "arxiv_url": "https://arxiv.org/abs/2503.01122v1",
    "pdf_url": "https://arxiv.org/pdf/2503.01122v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2503.01122",
    "arxiv_authors": [
      "Shizhan Liu",
      "Hao Zheng",
      "Hang Yu",
      "Jianguo Li"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=ACCORD%3A+Alleviating+Concept+Coupling+through+Dependence+Regularization+for+Text-to-Image+Diffusion+Personalization+Shizhan+Liu+Hao+Zheng+Hang+Yu+Jianguo+Li",
    "gs_search_success": true,
    "gs_authors": [
      "LtTzNK4AAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2503.06003",
    "title": "Integrating Frequency-Domain Representations with Low-Rank Adaptation in Vision-Language Models",
    "year": 2025,
    "published": "2025-03-08T01:22:10Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Situational awareness applications rely heavily on real-time processing of visual and textual data to provide actionable insights. Vision language models (VLMs) have become essential tools for interpreting complex environments by connecting visual inputs with natural language descriptions. However, these models often face computational challenges, especially when required to perform efficiently in real environments. This research presents a novel vision language model (VLM) framework that levera",
    "arxiv_url": "https://arxiv.org/abs/2503.06003v1",
    "pdf_url": "https://arxiv.org/pdf/2503.06003v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2503.06003",
    "arxiv_authors": [
      "Md Azim Khan",
      "Aryya Gangopadhyay",
      "Jianwu Wang",
      "Robert F. Erbacher"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Integrating+Frequency-Domain+Representations+with+Low-Rank+Adaptation+in+Vision-Language+Models+Md+Azim+Khan+Aryya+Gangopadhyay+Jianwu+Wang+Robert+F.+Erbacher",
    "gs_search_success": true,
    "gs_authors": [
      "2QFqeJIAAAAJ",
      "sYW3nvAAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": null
  },
  {
    "arxiv_id": "2412.04077",
    "title": "SoMA: Singular Value Decomposed Minor Components Adaptation for Domain Generalizable Representation Learning",
    "year": 2024,
    "published": "2024-12-05T11:17:57Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Domain generalization (DG) aims to adapt a model using one or multiple source domains to ensure robust performance in unseen target domains. Recently, Parameter-Efficient Fine-Tuning (PEFT) of foundation models has shown promising results in the context of DG problem. Nevertheless, existing PEFT methods still struggle to strike a balance between preserving generalizable components of the pre-trained model and learning task-specific features. To gain insights into the distribution of generalizabl",
    "arxiv_url": "https://arxiv.org/abs/2412.04077v2",
    "pdf_url": "https://arxiv.org/pdf/2412.04077v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2412.04077",
    "arxiv_authors": [
      "Seokju Yun",
      "Seunghye Chae",
      "Dongheon Lee",
      "Youngmin Ro"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=SoMA%3A+Singular+Value+Decomposed+Minor+Components+Adaptation+for+Domain+Generalizable+Representation+Learning+Seokju+Yun+Seunghye+Chae+Dongheon+Lee+Youngmin+Ro",
    "gs_search_success": true,
    "gs_authors": [
      "f5zk6PsAAAAJ",
      "-2MnHEIAAAAJ",
      "EzEll8kAAAAJ"
    ],
    "citation_count": 8,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2504.09424",
    "title": "Comparing Performance of Preprocessing Techniques for Traffic Sign Recognition Using a HOG-SVM",
    "year": 2025,
    "published": "2025-04-13T04:13:12Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "This study compares the performance of various preprocessing techniques for Traffic Sign Recognition (TSR) using Histogram of Oriented Gradients (HOG) and Support Vector Machine (SVM) on the German Traffic Sign Recognition Benchmark (GTSRB) dataset. Techniques such as CLAHE, HUE, and YUV were evaluated for their impact on classification accuracy. Results indicate that YUV in particular significantly enhance the performance of the HOG-SVM classifier (improving accuracy from 89.65% to 91.25%), pro",
    "arxiv_url": "https://arxiv.org/abs/2504.09424v1",
    "pdf_url": "https://arxiv.org/pdf/2504.09424v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2504.09424",
    "arxiv_authors": [
      "Luis Vieira"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Comparing+Performance+of+Preprocessing+Techniques+for+Traffic+Sign+Recognition+Using+a+HOG-SVM+Luis+Vieira",
    "gs_search_success": true,
    "gs_authors": [],
    "citation_count": null,
    "gs_author_count": 1
  },
  {
    "arxiv_id": "2410.10816",
    "title": "LVD-2M: A Long-take Video Dataset with Temporally Dense Captions",
    "year": 2024,
    "published": "2024-10-14T17:59:56Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "abstract": "The efficacy of video generation models heavily depends on the quality of their training datasets. Most previous video generation models are trained on short video clips, while recently there has been increasing interest in training long video generation models directly on longer videos. However, the lack of such high-quality long videos impedes the advancement of long video generation. To promote research in long video generation, we desire a new dataset with four key features essential for tra",
    "arxiv_url": "https://arxiv.org/abs/2410.10816v1",
    "pdf_url": "https://arxiv.org/pdf/2410.10816v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2410.10816",
    "arxiv_authors": [
      "Tianwei Xiong",
      "Yuqing Wang",
      "Daquan Zhou",
      "Zhijie Lin",
      "Jiashi Feng",
      "Xihui Liu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=LVD-2M%3A+A+Long-take+Video+Dataset+with+Temporally+Dense+Captions+Tianwei+Xiong+Yuqing+Wang+Daquan+Zhou+Zhijie+Lin+Jiashi+Feng",
    "gs_search_success": true,
    "gs_authors": [
      "tTMKGSYAAAAJ",
      "xXMj6_EAAAAJ",
      "DdCAbWwAAAAJ",
      "QC7nNe0AAAAJ",
      "4YL23GMAAAAJ",
      "Q8iay0gAAAAJ"
    ],
    "citation_count": 17,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2302.00386",
    "title": "EfficientRep:An Efficient Repvgg-style ConvNets with Hardware-aware Neural Network Design",
    "year": 2023,
    "published": "2023-02-01T11:46:04Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "We present a hardware-efficient architecture of convolutional neural network, which has a repvgg-like architecture. Flops or parameters are traditional metrics to evaluate the efficiency of networks which are not sensitive to hardware including computing ability and memory bandwidth. Thus, how to design a neural network to efficiently use the computing ability and memory bandwidth of hardware is a critical problem. This paper proposes a method how to design hardware-aware neural network. Based o",
    "arxiv_url": "https://arxiv.org/abs/2302.00386v1",
    "pdf_url": "https://arxiv.org/pdf/2302.00386v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2302.00386",
    "arxiv_authors": [
      "Kaiheng Weng",
      "Xiangxiang Chu",
      "Xiaoming Xu",
      "Junshi Huang",
      "Xiaoming Wei"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=EfficientRep%3AAn+Efficient+Repvgg-style+ConvNets+with+Hardware-aware+Neural+Network+Design+Kaiheng+Weng+Xiangxiang+Chu+Xiaoming+Xu+Junshi+Huang+Xiaoming+Wei",
    "gs_search_success": true,
    "gs_authors": [
      "JXV5yrZxj5MC",
      "jn21pUsAAAAJ",
      "FFB6lzQAAAAJ",
      "pZx0kAsAAAAJ"
    ],
    "citation_count": 63,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2305.08000",
    "title": "DNN-Compressed Domain Visual Recognition with Feature Adaptation",
    "year": 2023,
    "published": "2023-05-13T20:45:17Z",
    "categories": [
      "cs.CV",
      "eess.IV"
    ],
    "abstract": "Learning-based image compression was shown to achieve a competitive performance with state-of-the-art transform-based codecs. This motivated the development of new learning-based visual compression standards such as JPEG-AI. Of particular interest to these emerging standards is the development of learning-based image compression systems targeting both humans and machines. This paper is concerned with learning-based compression schemes whose compressed-domain representations can be utilized to pe",
    "arxiv_url": "https://arxiv.org/abs/2305.08000v2",
    "pdf_url": "https://arxiv.org/pdf/2305.08000v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2305.08000",
    "arxiv_authors": [
      "Yingpeng Deng",
      "Lina J. Karam"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=DNN-Compressed+Domain+Visual+Recognition+with+Feature+Adaptation+Yingpeng+Deng+Lina+J.+Karam",
    "gs_search_success": true,
    "gs_authors": [
      "TQxAPYsAAAAJ",
      "mgDJGUUAAAAJ"
    ],
    "citation_count": 3,
    "gs_author_count": 2
  },
  {
    "arxiv_id": "2504.12795",
    "title": "EarthGPT-X: A Spatial MLLM for Multi-level Multi-Source Remote Sensing Imagery Understanding with Visual Prompting",
    "year": 2025,
    "published": "2025-04-17T09:56:35Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Recent advances in natural-domain multi-modal large language models (MLLMs) have demonstrated effective spatial reasoning through visual and textual prompting. However, their direct transfer to remote sensing (RS) is hindered by heterogeneous sensing physics, diverse modalities, and unique spatial scales. Existing RS MLLMs are mainly limited to optical imagery and plain language interaction, preventing flexible and scalable real-world applications. In this article, EarthGPT-X is proposed, the fi",
    "arxiv_url": "https://arxiv.org/abs/2504.12795v3",
    "pdf_url": "https://arxiv.org/pdf/2504.12795v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2504.12795",
    "arxiv_authors": [
      "Wei Zhang",
      "Miaoxin Cai",
      "Yaqian Ning",
      "Tong Zhang",
      "Yin Zhuang",
      "Shijian Lu",
      "He Chen",
      "Jun Li",
      "Xuerui Mao"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=EarthGPT-X%3A+A+Spatial+MLLM+for+Multi-level+Multi-Source+Remote+Sensing+Imagery+Understanding+with+Visual+Prompting+Wei+Zhang+Miaoxin+Cai+Yaqian+Ning+Tong+Zhang+Yin+Zhuang",
    "gs_search_success": true,
    "gs_authors": [
      "uYmK-A0AAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 9
  },
  {
    "arxiv_id": "2301.06961",
    "title": "Composite Deep Network with Feature Weighting for Improved Delineation of COVID Infection in Lung CT",
    "year": 2023,
    "published": "2023-01-17T15:36:27Z",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "abstract": "An early effective screening and grading of COVID-19 has become imperative towards optimizing the limited available resources of the medical facilities. An automated segmentation of the infected volumes in lung CT is expected to significantly aid in the diagnosis and care of patients. However, an accurate demarcation of lesions remains problematic due to their irregular structure and location(s) within the lung. A novel deep learning architecture, Composite Deep network with Feature Weighting (C",
    "arxiv_url": "https://arxiv.org/abs/2301.06961v2",
    "pdf_url": "https://arxiv.org/pdf/2301.06961v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2301.06961",
    "arxiv_authors": [
      "Pallabi Dutta",
      "Sushmita Mitra"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Composite+Deep+Network+with+Feature+Weighting+for+Improved+Delineation+of+COVID+Infection+in+Lung+CT+Pallabi+Dutta+Sushmita+Mitra",
    "gs_search_success": true,
    "gs_authors": [
      "4J683WEAAAAJ",
      "PrUB1fAAAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 2
  },
  {
    "arxiv_id": "2403.17188",
    "title": "LOTUS: Evasive and Resilient Backdoor Attacks through Sub-Partitioning",
    "year": 2024,
    "published": "2024-03-25T21:01:29Z",
    "categories": [
      "cs.CV",
      "cs.CR"
    ],
    "abstract": "Backdoor attack poses a significant security threat to Deep Learning applications. Existing attacks are often not evasive to established backdoor detection techniques. This susceptibility primarily stems from the fact that these attacks typically leverage a universal trigger pattern or transformation function, such that the trigger can cause misclassification for any input. In response to this, recent papers have introduced attacks using sample-specific invisible triggers crafted through special",
    "arxiv_url": "https://arxiv.org/abs/2403.17188v1",
    "pdf_url": "https://arxiv.org/pdf/2403.17188v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2403.17188",
    "arxiv_authors": [
      "Siyuan Cheng",
      "Guanhong Tao",
      "Yingqi Liu",
      "Guangyu Shen",
      "Shengwei An",
      "Shiwei Feng",
      "Xiangzhe Xu",
      "Kaiyuan Zhang",
      "Shiqing Ma",
      "Xiangyu Zhang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=LOTUS%3A+Evasive+and+Resilient+Backdoor+Attacks+through+Sub-Partitioning+Siyuan+Cheng+Guanhong+Tao+Yingqi+Liu+Guangyu+Shen+Shengwei+An",
    "gs_search_success": true,
    "gs_authors": [
      "YiMTVwgAAAAJ",
      "gOPVK2UAAAAJ",
      "X_mDnjkAAAAJ",
      "3gtm2egAAAAJ",
      "GcL9AFMAAAAJ",
      "EfWRQcMAAAAJ",
      "PXbu1wIAAAAJ",
      "qcmmzeEAAAAJ",
      "ff7RmjEAAAAJ",
      "2c6VTAkAAAAJ"
    ],
    "citation_count": 12,
    "gs_author_count": 10
  },
  {
    "arxiv_id": "2412.14961",
    "title": "TDCNet: Transparent Objects Depth Completion with CNN-Transformer Dual-Branch Parallel Network",
    "year": 2024,
    "published": "2024-12-19T15:42:21Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "The sensing and manipulation of transparent objects present a critical challenge in industrial and laboratory robotics. Conventional sensors face challenges in obtaining the full depth of transparent objects due to the refraction and reflection of light on their surfaces and their lack of visible texture. Previous research has attempted to obtain complete depth maps of transparent objects from RGB and damaged depth maps (collected by depth sensor) using deep learning models. However, existing me",
    "arxiv_url": "https://arxiv.org/abs/2412.14961v1",
    "pdf_url": "https://arxiv.org/pdf/2412.14961v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2412.14961",
    "arxiv_authors": [
      "Xianghui Fan",
      "Chao Ye",
      "Anping Deng",
      "Xiaotian Wu",
      "Mengyang Pan",
      "Hang Yang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=TDCNet%3A+Transparent+Objects+Depth+Completion+with+CNN-Transformer+Dual-Branch+Parallel+Network+Xianghui+Fan+Chao+Ye+Anping+Deng+Xiaotian+Wu+Mengyang+Pan",
    "gs_search_success": true,
    "gs_authors": [
      "4g5HzecAAAAJ",
      "ArnkrbwAAAAJ"
    ],
    "citation_count": 5,
    "gs_author_count": 7
  },
  {
    "arxiv_id": "2403.05687",
    "title": "Scene Graph Aided Radiology Report Generation",
    "year": 2024,
    "published": "2024-03-08T21:43:28Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Radiology report generation (RRG) methods often lack sufficient medical knowledge to produce clinically accurate reports. The scene graph contains rich information to describe the objects in an image. We explore enriching the medical knowledge for RRG via a scene graph, which has not been done in the current RRG literature. To this end, we propose the Scene Graph aided RRG (SGRRG) network, a framework that generates region-level visual features, predicts anatomical attributes, and leverages an a",
    "arxiv_url": "https://arxiv.org/abs/2403.05687v1",
    "pdf_url": "https://arxiv.org/pdf/2403.05687v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2403.05687",
    "arxiv_authors": [
      "Jun Wang",
      "Lixing Zhu",
      "Abhir Bhalerao",
      "Yulan He"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Scene+Graph+Aided+Radiology+Report+Generation+Jun+Wang+Lixing+Zhu+Abhir+Bhalerao+Yulan+He",
    "gs_search_success": true,
    "gs_authors": [
      "SP9r32UAAAAJ",
      "b_jEBHEAAAAJ",
      "e4U9SxIAAAAJ",
      "XfBoSP4AAAAJ"
    ],
    "citation_count": 2,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2406.13007",
    "title": "NTIRE 2024 Challenge on Night Photography Rendering",
    "year": 2024,
    "published": "2024-06-18T18:56:25Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "This paper presents a review of the NTIRE 2024 challenge on night photography rendering. The goal of the challenge was to find solutions that process raw camera images taken in nighttime conditions, and thereby produce a photo-quality output images in the standard RGB (sRGB) space. Unlike the previous year's competition, the challenge images were collected with a mobile phone and the speed of algorithms was also measured alongside the quality of their output. To evaluate the results, a sufficien",
    "arxiv_url": "https://arxiv.org/abs/2406.13007v1",
    "pdf_url": "https://arxiv.org/pdf/2406.13007v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2406.13007",
    "arxiv_authors": [
      "Egor Ershov",
      "Artyom Panshin",
      "Oleg Karasev",
      "Sergey Korchagin",
      "Shepelev Lev",
      "Alexandr Startsev",
      "Daniil Vladimirov",
      "Ekaterina Zaychenkova",
      "Nikola Banić",
      "Dmitrii Iarchuk",
      "Maria Efimova",
      "Radu Timofte",
      "Arseniy Terekhin",
      "Shuwei Yue",
      "Yuyang Liu",
      "Minchen Wei",
      "Lu Xu",
      "Chao Zhang",
      "Yasi Wang",
      "Furkan Kınlı",
      "Doğa Yılmaz",
      "Barış Özcan",
      "Furkan Kıraç",
      "Shuai Liu",
      "Jingyuan Xiao",
      "Chaoyu Feng",
      "Hao Wang",
      "Guangqi Shao",
      "Yuqian Zhang",
      "Yibin Huang",
      "Wei Luo",
      "Liming Wang",
      "Xiaotao Wang",
      "Lei Lei",
      "Simone Zini",
      "Claudio Rota",
      "Marco Buzzelli",
      "Simone Bianco",
      "Raimondo Schettini",
      "Jin Guo",
      "Tianli Liu",
      "Mohao Wu",
      "Ben Shao",
      "Qirui Yang",
      "Xianghui Li",
      "Qihua Cheng",
      "Fangpu Zhang",
      "Zhiqiang Xu",
      "Jingyu Yang",
      "Huanjing Yue"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=NTIRE+2024+Challenge+on+Night+Photography+Rendering+Egor+Ershov+Artyom+Panshin+Oleg+Karasev+Sergey+Korchagin+Shepelev+Lev",
    "gs_search_success": true,
    "gs_authors": [
      "QSH8C_QAAAAJ",
      "FrmHioMAAAAJ",
      "LOOceWAAAAAJ"
    ],
    "citation_count": 14,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2503.05584",
    "title": "QArtSR: Quantization via Reverse-Module and Timestep-Retraining in One-Step Diffusion based Image Super-Resolution",
    "year": 2025,
    "published": "2025-03-07T17:11:07Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "One-step diffusion-based image super-resolution (OSDSR) models are showing increasingly superior performance nowadays. However, although their denoising steps are reduced to one and they can be quantized to 8-bit to reduce the costs further, there is still significant potential for OSDSR to quantize to lower bits. To explore more possibilities of quantized OSDSR, we propose an efficient method, Quantization via reverse-module and timestep-retraining for OSDSR, named QArtSR. Firstly, we investiga",
    "arxiv_url": "https://arxiv.org/abs/2503.05584v1",
    "pdf_url": "https://arxiv.org/pdf/2503.05584v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2503.05584",
    "arxiv_authors": [
      "Libo Zhu",
      "Haotong Qin",
      "Kaicheng Yang",
      "Wenbo Li",
      "Yong Guo",
      "Yulun Zhang",
      "Susanto Rahardja",
      "Xiaokang Yang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=QArtSR%3A+Quantization+via+Reverse-Module+and+Timestep-Retraining+in+One-Step+Diffusion+based+Image+Super-Resolution+Libo+Zhu+Haotong+Qin+Kaicheng+Yang+Wenbo+Li+Yong+Guo",
    "gs_search_success": true,
    "gs_authors": [
      "yDEavdMAAAAJ",
      "mK6n-KgAAAAJ",
      "VmLZOc0AAAAJ",
      "foGn_TIAAAAJ",
      "wMa6S3kAAAAJ",
      "OdkA4jMAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 8
  },
  {
    "arxiv_id": "2307.14243",
    "title": "Fluorescent Neuronal Cells v2: Multi-Task, Multi-Format Annotations for Deep Learning in Microscopy",
    "year": 2023,
    "published": "2023-07-26T15:14:10Z",
    "categories": [
      "cs.CV",
      "cs.LG",
      "physics.app-ph"
    ],
    "abstract": "Fluorescent Neuronal Cells v2 is a collection of fluorescence microscopy images and the corresponding ground-truth annotations, designed to foster innovative research in the domains of Life Sciences and Deep Learning. This dataset encompasses three image collections in which rodent neuronal cells' nuclei and cytoplasm are stained with diverse markers to highlight their anatomical or functional characteristics. Alongside the images, we provide ground-truth annotations for several learning tasks, ",
    "arxiv_url": "https://arxiv.org/abs/2307.14243v1",
    "pdf_url": "https://arxiv.org/pdf/2307.14243v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2307.14243",
    "arxiv_authors": [
      "Luca Clissa",
      "Antonio Macaluso",
      "Roberto Morelli",
      "Alessandra Occhinegro",
      "Emiliana Piscitiello",
      "Ludovico Taddei",
      "Marco Luppi",
      "Roberto Amici",
      "Matteo Cerri",
      "Timna Hitrec",
      "Lorenzo Rinaldi",
      "Antonio Zoccoli"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Fluorescent+Neuronal+Cells+v2%3A+Multi-Task%2C+Multi-Format+Annotations+for+Deep+Learning+in+Microscopy+Luca+Clissa+Antonio+Macaluso+Roberto+Morelli+Alessandra+Occhinegro+Emiliana+Piscitiello",
    "gs_search_success": true,
    "gs_authors": [
      "ScUY6DUAAAAJ",
      "Z2R5HuEAAAAJ",
      "mK_NTWkAAAAJ",
      "C739y6QAAAAJ",
      "IKM_cmwAAAAJ",
      "8CR1h-MAAAAJ",
      "d_KvZIoAAAAJ"
    ],
    "citation_count": 9,
    "gs_author_count": 8
  },
  {
    "arxiv_id": "2410.08091",
    "title": "Distribution Guidance Network for Weakly Supervised Point Cloud Semantic Segmentation",
    "year": 2024,
    "published": "2024-10-10T16:33:27Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Despite alleviating the dependence on dense annotations inherent to fully supervised methods, weakly supervised point cloud semantic segmentation suffers from inadequate supervision signals. In response to this challenge, we introduce a novel perspective that imparts auxiliary constraints by regulating the feature space under weak supervision. Our initial investigation identifies which distributions accurately characterize the feature space, subsequently leveraging this priori to guide the align",
    "arxiv_url": "https://arxiv.org/abs/2410.08091v2",
    "pdf_url": "https://arxiv.org/pdf/2410.08091v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2410.08091",
    "arxiv_authors": [
      "Zhiyi Pan",
      "Wei Gao",
      "Shan Liu",
      "Ge Li"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Distribution+Guidance+Network+for+Weakly+Supervised+Point+Cloud+Semantic+Segmentation+Zhiyi+Pan+Wei+Gao+Shan+Liu+Ge+Li",
    "gs_search_success": true,
    "gs_authors": [
      "bdBZ43wAAAAJ",
      "Mdtik3oAAAAJ",
      "KdXy-kgAAAAJ"
    ],
    "citation_count": 17,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2308.12537",
    "title": "HuBo-VLM: Unified Vision-Language Model designed for HUman roBOt interaction tasks",
    "year": 2023,
    "published": "2023-08-24T03:47:27Z",
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "abstract": "Human robot interaction is an exciting task, which aimed to guide robots following instructions from human. Since huge gap lies between human natural language and machine codes, end to end human robot interaction models is fair challenging. Further, visual information receiving from sensors of robot is also a hard language for robot to perceive. In this work, HuBo-VLM is proposed to tackle perception tasks associated with human robot interaction including object detection and visual grounding by",
    "arxiv_url": "https://arxiv.org/abs/2308.12537v1",
    "pdf_url": "https://arxiv.org/pdf/2308.12537v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2308.12537",
    "arxiv_authors": [
      "Zichao Dong",
      "Weikun Zhang",
      "Xufeng Huang",
      "Hang Ji",
      "Xin Zhan",
      "Junbo Chen"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=HuBo-VLM%3A+Unified+Vision-Language+Model+designed+for+HUman+roBOt+interaction+tasks+Zichao+Dong+Weikun+Zhang+Xufeng+Huang+Hang+Ji+Xin+Zhan",
    "gs_search_success": true,
    "gs_authors": [],
    "citation_count": 11,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2307.03538",
    "title": "Language-free Compositional Action Generation via Decoupling Refinement",
    "year": 2023,
    "published": "2023-07-07T12:00:38Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Composing simple elements into complex concepts is crucial yet challenging, especially for 3D action generation. Existing methods largely rely on extensive neural language annotations to discern composable latent semantics, a process that is often costly and labor-intensive. In this study, we introduce a novel framework to generate compositional actions without reliance on language auxiliaries. Our approach consists of three main components: Action Coupling, Conditional Action Generation, and De",
    "arxiv_url": "https://arxiv.org/abs/2307.03538v3",
    "pdf_url": "https://arxiv.org/pdf/2307.03538v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2307.03538",
    "arxiv_authors": [
      "Xiao Liu",
      "Guangyi Chen",
      "Yansong Tang",
      "Guangrun Wang",
      "Xiao-Ping Zhang",
      "Ser-Nam Lim"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Language-free+Compositional+Action+Generation+via+Decoupling+Refinement+Xiao+Liu+Guangyi+Chen+Yansong+Tang+Guangrun+Wang+Xiao-Ping+Zhang",
    "gs_search_success": true,
    "gs_authors": [
      "nuHIZx0AAAAJ",
      "TIbistUAAAAJ",
      "1fzb_z8AAAAJ",
      "3lr1jTgAAAAJ",
      "HX0BfLYAAAAJ"
    ],
    "citation_count": 3,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2408.05952",
    "title": "Optimizing Vision Transformers with Data-Free Knowledge Transfer",
    "year": 2024,
    "published": "2024-08-12T07:03:35Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "The groundbreaking performance of transformers in Natural Language Processing (NLP) tasks has led to their replacement of traditional Convolutional Neural Networks (CNNs), owing to the efficiency and accuracy achieved through the self-attention mechanism. This success has inspired researchers to explore the use of transformers in computer vision tasks to attain enhanced long-term semantic awareness. Vision transformers (ViTs) have excelled in various computer vision tasks due to their superior a",
    "arxiv_url": "https://arxiv.org/abs/2408.05952v1",
    "pdf_url": "https://arxiv.org/pdf/2408.05952v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2408.05952",
    "arxiv_authors": [
      "Gousia Habib",
      "Damandeep Singh",
      "Ishfaq Ahmad Malik",
      "Brejesh Lall"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Optimizing+Vision+Transformers+with+Data-Free+Knowledge+Transfer+Gousia+Habib+Damandeep+Singh+Ishfaq+Ahmad+Malik+Brejesh+Lall",
    "gs_search_success": true,
    "gs_authors": [
      "keEJ7X8AAAAJ",
      "PVBkKCAAAAAJ",
      "7EnEz7gaXWcC"
    ],
    "citation_count": 3,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2309.02561",
    "title": "Physically Grounded Vision-Language Models for Robotic Manipulation",
    "year": 2023,
    "published": "2023-09-05T20:21:03Z",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV"
    ],
    "abstract": "Recent advances in vision-language models (VLMs) have led to improved performance on tasks such as visual question answering and image captioning. Consequently, these models are now well-positioned to reason about the physical world, particularly within domains such as robotic manipulation. However, current VLMs are limited in their understanding of the physical concepts (e.g., material, fragility) of common objects, which restricts their usefulness for robotic manipulation tasks that involve in",
    "arxiv_url": "https://arxiv.org/abs/2309.02561v4",
    "pdf_url": "https://arxiv.org/pdf/2309.02561v4",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2309.02561",
    "arxiv_authors": [
      "Jensen Gao",
      "Bidipta Sarkar",
      "Fei Xia",
      "Ted Xiao",
      "Jiajun Wu",
      "Brian Ichter",
      "Anirudha Majumdar",
      "Dorsa Sadigh"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Physically+Grounded+Vision-Language+Models+for+Robotic+Manipulation+Jensen+Gao+Bidipta+Sarkar+Fei+Xia+Ted+Xiao+Jiajun+Wu",
    "gs_search_success": true,
    "gs_authors": [
      "-w5DuHgAAAAJ",
      "ZaJEZpYAAAAJ",
      "2efgcS0AAAAJ",
      "LIJQ_ZYAAAAJ",
      "wr9RgmcAAAAJ",
      "pqP5_PgAAAAJ",
      "ibu3FwsAAAAJ",
      "SnTMyGUAAAAJ"
    ],
    "citation_count": 215,
    "gs_author_count": 8
  },
  {
    "arxiv_id": "2307.05801",
    "title": "Differentiable Forward Projector for X-ray Computed Tomography",
    "year": 2023,
    "published": "2023-07-11T20:52:46Z",
    "categories": [
      "cs.LG",
      "cs.CV",
      "cs.MS"
    ],
    "abstract": "Data-driven deep learning has been successfully applied to various computed tomographic reconstruction problems. The deep inference models may outperform existing analytical and iterative algorithms, especially in ill-posed CT reconstruction. However, those methods often predict images that do not agree with the measured projection data. This paper presents an accurate differentiable forward and back projection software library to ensure the consistency between the predicted images and the origi",
    "arxiv_url": "https://arxiv.org/abs/2307.05801v1",
    "pdf_url": "https://arxiv.org/pdf/2307.05801v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2307.05801",
    "arxiv_authors": [
      "Hyojin Kim",
      "Kyle Champley"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Differentiable+Forward+Projector+for+X-ray+Computed+Tomography+Hyojin+Kim+Kyle+Champley",
    "gs_search_success": true,
    "gs_authors": [
      "mfPgZbYAAAAJ",
      "fze4cTgAAAAJ"
    ],
    "citation_count": 20,
    "gs_author_count": 2
  },
  {
    "arxiv_id": "2401.09424",
    "title": "Precipitation Prediction Using an Ensemble of Lightweight Learners",
    "year": 2023,
    "published": "2023-11-30T02:20:16Z",
    "categories": [
      "physics.ao-ph",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "Precipitation prediction plays a crucial role in modern agriculture and industry. However, it poses significant challenges due to the diverse patterns and dynamics in time and space, as well as the scarcity of high precipitation events.   To address this challenge, we propose an ensemble learning framework that leverages multiple learners to capture the diverse patterns of precipitation distribution. Specifically, the framework consists of a precipitation predictor with multiple lightweight head",
    "arxiv_url": "https://arxiv.org/abs/2401.09424v1",
    "pdf_url": "https://arxiv.org/pdf/2401.09424v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2401.09424",
    "arxiv_authors": [
      "Xinzhe Li",
      "Sun Rui",
      "Yiming Niu",
      "Yao Liu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Precipitation+Prediction+Using+an+Ensemble+of+Lightweight+Learners+Xinzhe+Li+Sun+Rui+Yiming+Niu+Yao+Liu",
    "gs_search_success": true,
    "gs_authors": [],
    "citation_count": 1,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2503.08531",
    "title": "Visual Attention Graph",
    "year": 2025,
    "published": "2025-03-11T15:22:44Z",
    "categories": [
      "cs.CV",
      "q-bio.QM"
    ],
    "abstract": "Visual attention plays a critical role when our visual system executes active visual tasks by interacting with the physical scene. However, how to encode the visual object relationship in the psychological world of our brain deserves to be explored. In the field of computer vision, predicting visual fixations or scanpaths is a usual way to explore the visual attention and behaviors of human observers when viewing a scene. Most existing methods encode visual attention using individual fixations o",
    "arxiv_url": "https://arxiv.org/abs/2503.08531v1",
    "pdf_url": "https://arxiv.org/pdf/2503.08531v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2503.08531",
    "arxiv_authors": [
      "Kai-Fu Yang",
      "Yong-Jie Li"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Visual+Attention+Graph+Kai-Fu+Yang+Yong-Jie+Li",
    "gs_search_success": true,
    "gs_authors": [
      "i0VkF0EAAAAJ",
      "CFJF3ugAAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 2
  },
  {
    "arxiv_id": "2403.10061",
    "title": "PAME: Self-Supervised Masked Autoencoder for No-Reference Point Cloud Quality Assessment",
    "year": 2024,
    "published": "2024-03-15T07:01:33Z",
    "categories": [
      "cs.CV",
      "cs.MM"
    ],
    "abstract": "No-reference point cloud quality assessment (NR-PCQA) aims to automatically predict the perceptual quality of point clouds without reference, which has achieved remarkable performance due to the utilization of deep learning-based models. However, these data-driven models suffer from the scarcity of labeled data and perform unsatisfactorily in cross-dataset evaluations. To address this problem, we propose a self-supervised pre-training framework using masked autoencoders (PAME) to help the model ",
    "arxiv_url": "https://arxiv.org/abs/2403.10061v1",
    "pdf_url": "https://arxiv.org/pdf/2403.10061v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2403.10061",
    "arxiv_authors": [
      "Ziyu Shan",
      "Yujie Zhang",
      "Qi Yang",
      "Haichen Yang",
      "Yiling Xu",
      "Shan Liu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=PAME%3A+Self-Supervised+Masked+Autoencoder+for+No-Reference+Point+Cloud+Quality+Assessment+Ziyu+Shan+Yujie+Zhang+Qi+Yang+Haichen+Yang+Yiling+Xu",
    "gs_search_success": true,
    "gs_authors": [
      "87nXDYAAAAAJ",
      "AisjLHwAAAAJ",
      "638kRwkAAAAJ",
      "McMyJ_0AAAAJ"
    ],
    "citation_count": 8,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2407.20653",
    "title": "FACL-Attack: Frequency-Aware Contrastive Learning for Transferable Adversarial Attacks",
    "year": 2024,
    "published": "2024-07-30T08:50:06Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "abstract": "Deep neural networks are known to be vulnerable to security risks due to the inherent transferable nature of adversarial examples. Despite the success of recent generative model-based attacks demonstrating strong transferability, it still remains a challenge to design an efficient attack strategy in a real-world strict black-box setting, where both the target domain and model architectures are unknown. In this paper, we seek to explore a feature contrastive approach in the frequency domain to ge",
    "arxiv_url": "https://arxiv.org/abs/2407.20653v1",
    "pdf_url": "https://arxiv.org/pdf/2407.20653v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2407.20653",
    "arxiv_authors": [
      "Hunmin Yang",
      "Jongoh Jeong",
      "Kuk-Jin Yoon"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=FACL-Attack%3A+Frequency-Aware+Contrastive+Learning+for+Transferable+Adversarial+Attacks+Hunmin+Yang+Jongoh+Jeong+Kuk-Jin+Yoon",
    "gs_search_success": true,
    "gs_authors": [
      "VOfoz6AAAAAJ",
      "mDxJj2AAAAAJ",
      "1NvBj_gAAAAJ"
    ],
    "citation_count": 13,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2404.00901",
    "title": "Slightly Shift New Classes to Remember Old Classes for Video Class-Incremental Learning",
    "year": 2024,
    "published": "2024-04-01T03:58:51Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Recent video class-incremental learning usually excessively pursues the accuracy of the newly seen classes and relies on memory sets to mitigate catastrophic forgetting of the old classes. However, limited storage only allows storing a few representative videos. So we propose SNRO, which slightly shifts the features of new classes to remember old classes. Specifically, SNRO contains Examples Sparse(ES) and Early Break(EB). ES decimates at a lower sample rate to build memory sets and uses interpo",
    "arxiv_url": "https://arxiv.org/abs/2404.00901v1",
    "pdf_url": "https://arxiv.org/pdf/2404.00901v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2404.00901",
    "arxiv_authors": [
      "Jian Jiao",
      "Yu Dai",
      "Hefei Mei",
      "Heqian Qiu",
      "Chuanyang Gong",
      "Shiyuan Tang",
      "Xinpeng Hao",
      "Hongliang Li"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Slightly+Shift+New+Classes+to+Remember+Old+Classes+for+Video+Class-Incremental+Learning+Jian+Jiao+Yu+Dai+Hefei+Mei+Heqian+Qiu+Chuanyang+Gong",
    "gs_search_success": true,
    "gs_authors": [
      "O9-QDwEAAAAJ",
      "vCs7em0AAAAJ",
      "H04y9fEAAAAJ",
      "dJx4SDEAAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 8
  },
  {
    "arxiv_id": "2501.02964",
    "title": "Socratic Questioning: Learn to Self-guide Multimodal Reasoning in the Wild",
    "year": 2025,
    "published": "2025-01-06T12:16:56Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Complex visual reasoning remains a key challenge today. Typically, the challenge is tackled using methodologies such as Chain of Thought (COT) and visual instruction tuning. However, how to organically combine these two methodologies for greater success remains unexplored. Also, issues like hallucinations and high training cost still need to be addressed. In this work, we devise an innovative multi-round training and reasoning framework suitable for lightweight Multimodal Large Language Models (",
    "arxiv_url": "https://arxiv.org/abs/2501.02964v2",
    "pdf_url": "https://arxiv.org/pdf/2501.02964v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2501.02964",
    "arxiv_authors": [
      "Wanpeng Hu",
      "Haodi Liu",
      "Lin Chen",
      "Feng Zhou",
      "Changming Xiao",
      "Qi Yang",
      "Changshui Zhang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Socratic+Questioning%3A+Learn+to+Self-guide+Multimodal+Reasoning+in+the+Wild+Wanpeng+Hu+Haodi+Liu+Lin+Chen+Feng+Zhou+Changming+Xiao",
    "gs_search_success": true,
    "gs_authors": [
      "HbCAb3AAAAAJ"
    ],
    "citation_count": 4,
    "gs_author_count": 7
  },
  {
    "arxiv_id": "2502.02452",
    "title": "Personalization Toolkit: Training Free Personalization of Large Vision Language Models",
    "year": 2025,
    "published": "2025-02-04T16:19:20Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Personalization of Large Vision-Language Models (LVLMs) involves customizing models to recognize specific users and object instances, and to generate contextually tailored responses. Existing approaches typically rely on time-consuming test-time training for each user or object, making them impractical for real-world deployment, a limitation reflected in current personalization benchmarks, which are focused on object-centric, single-concept evaluations. In this paper, we present a novel training",
    "arxiv_url": "https://arxiv.org/abs/2502.02452v3",
    "pdf_url": "https://arxiv.org/pdf/2502.02452v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2502.02452",
    "arxiv_authors": [
      "Soroush Seifi",
      "Vaggelis Dorovatas",
      "Daniel Olmeda Reino",
      "Rahaf Aljundi"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Personalization+Toolkit%3A+Training+Free+Personalization+of+Large+Vision+Language+Models+Soroush+Seifi+Vaggelis+Dorovatas+Daniel+Olmeda+Reino+Rahaf+Aljundi",
    "gs_search_success": true,
    "gs_authors": [
      "RWTJmjcAAAAJ",
      "YLh7yrwAAAAJ",
      "JmQOYvQAAAAJ"
    ],
    "citation_count": 2,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2310.08304",
    "title": "CHIP: Contrastive Hierarchical Image Pretraining",
    "year": 2023,
    "published": "2023-10-12T13:11:38Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "abstract": "Few-shot object classification is the task of classifying objects in an image with limited number of examples as supervision. We propose a one-shot/few-shot classification model that can classify an object of any unseen class into a relatively general category in an hierarchically based classification. Our model uses a three-level hierarchical contrastive loss based ResNet152 classifier for classifying an object based on its features extracted from Image embedding, not used during the training p",
    "arxiv_url": "https://arxiv.org/abs/2310.08304v1",
    "pdf_url": "https://arxiv.org/pdf/2310.08304v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2310.08304",
    "arxiv_authors": [
      "Arpit Mittal",
      "Harshil Jhaveri",
      "Swapnil Mallick",
      "Abhishek Ajmera"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=CHIP%3A+Contrastive+Hierarchical+Image+Pretraining+Arpit+Mittal+Harshil+Jhaveri+Swapnil+Mallick+Abhishek+Ajmera",
    "gs_search_success": true,
    "gs_authors": [
      "fDORjGoAAAAJ",
      "C0dx92kAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2405.15033",
    "title": "Fractured Glass, Failing Cameras: Simulating Physics-Based Adversarial Samples for Autonomous Driving Systems",
    "year": 2024,
    "published": "2024-05-23T20:11:20Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "eess.IV"
    ],
    "abstract": "While much research has recently focused on generating physics-based adversarial samples, a critical yet often overlooked category originates from physical failures within on-board cameras-components essential to the perception systems of autonomous vehicles. Camera failures, whether due to external stresses causing hardware breakdown or internal component faults, can directly jeopardize the safety and reliability of autonomous driving systems. Firstly, we motivate the study using two separate r",
    "arxiv_url": "https://arxiv.org/abs/2405.15033v3",
    "pdf_url": "https://arxiv.org/pdf/2405.15033v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2405.15033",
    "arxiv_authors": [
      "Manav Prabhakar",
      "Jwalandhar Girnar",
      "Arpan Kusari"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Fractured+Glass%2C+Failing+Cameras%3A+Simulating+Physics-Based+Adversarial+Samples+for+Autonomous+Driving+Systems+Manav+Prabhakar+Jwalandhar+Girnar+Arpan+Kusari",
    "gs_search_success": false,
    "gs_authors": [],
    "citation_count": null,
    "gs_author_count": null,
    "error_type": "fetch_failed"
  },
  {
    "arxiv_id": "2301.09219",
    "title": "Applied Deep Learning to Identify and Localize Polyps from Endoscopic Images",
    "year": 2023,
    "published": "2023-01-22T22:14:25Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Deep learning based neural networks have gained popularity for a variety of biomedical imaging applications. In the last few years several works have shown the use of these methods for colon cancer detection and the early results have been promising. These methods can potentially be utilized to assist doctor's and may help in identifying the number of lesions or abnormalities in a diagnosis session. From our literature survey we found out that there is a lack of publicly available labeled data. ",
    "arxiv_url": "https://arxiv.org/abs/2301.09219v1",
    "pdf_url": "https://arxiv.org/pdf/2301.09219v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2301.09219",
    "arxiv_authors": [
      "Chandana Raju",
      "Sumedh Vilas Datar",
      "Kushala Hari",
      "Kavin Vijay",
      "Suma Ningappa"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Applied+Deep+Learning+to+Identify+and+Localize+Polyps+from+Endoscopic+Images+Chandana+Raju+Sumedh+Vilas+Datar+Kushala+Hari+Kavin+Vijay+Suma+Ningappa",
    "gs_search_success": true,
    "gs_authors": [
      "b8Vy4Y0AAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2306.09613",
    "title": "UTOPIA: Unconstrained Tracking Objects without Preliminary Examination via Cross-Domain Adaptation",
    "year": 2023,
    "published": "2023-06-16T04:06:15Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Multiple Object Tracking (MOT) aims to find bounding boxes and identities of targeted objects in consecutive video frames. While fully-supervised MOT methods have achieved high accuracy on existing datasets, they cannot generalize well on a newly obtained dataset or a new unseen domain. In this work, we first address the MOT problem from the cross-domain point of view, imitating the process of new data acquisition in practice. Then, a new cross-domain MOT adaptation from existing datasets is pro",
    "arxiv_url": "https://arxiv.org/abs/2306.09613v1",
    "pdf_url": "https://arxiv.org/pdf/2306.09613v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2306.09613",
    "arxiv_authors": [
      "Pha Nguyen",
      "Kha Gia Quach",
      "John Gauch",
      "Samee U. Khan",
      "Bhiksha Raj",
      "Khoa Luu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=UTOPIA%3A+Unconstrained+Tracking+Objects+without+Preliminary+Examination+via+Cross-Domain+Adaptation+Pha+Nguyen+Kha+Gia+Quach+John+Gauch+Samee+U.+Khan+Bhiksha+Raj",
    "gs_search_success": true,
    "gs_authors": [
      "JPAl8-gAAAAJ",
      "IWcGY98AAAAJ",
      "YOzpM0AAAAAJ",
      "AQ-4ioEAAAAJ",
      "R0EQOKEAAAAJ"
    ],
    "citation_count": 3,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2307.07929",
    "title": "DocTr: Document Transformer for Structured Information Extraction in Documents",
    "year": 2023,
    "published": "2023-07-16T02:59:30Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "We present a new formulation for structured information extraction (SIE) from visually rich documents. It aims to address the limitations of existing IOB tagging or graph-based formulations, which are either overly reliant on the correct ordering of input text or struggle with decoding a complex graph. Instead, motivated by anchor-based object detectors in vision, we represent an entity as an anchor word and a bounding box, and represent entity linking as the association between anchor words. Th",
    "arxiv_url": "https://arxiv.org/abs/2307.07929v1",
    "pdf_url": "https://arxiv.org/pdf/2307.07929v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2307.07929",
    "arxiv_authors": [
      "Haofu Liao",
      "Aruni RoyChowdhury",
      "Weijian Li",
      "Ankan Bansal",
      "Yuting Zhang",
      "Zhuowen Tu",
      "Ravi Kumar Satzoda",
      "R. Manmatha",
      "Vijay Mahadevan"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=DocTr%3A+Document+Transformer+for+Structured+Information+Extraction+in+Documents+Haofu+Liao+Aruni+RoyChowdhury+Weijian+Li+Ankan+Bansal+Yuting+Zhang",
    "gs_search_success": true,
    "gs_authors": [
      "3Lwd4CwAAAAJ",
      "9UfZJskAAAAJ",
      "dYZVGOQAAAAJ",
      "Qtn-ZlIAAAAJ",
      "9oz-dvgAAAAJ",
      "jDkvHcAAAAAJ",
      "_0aMq28AAAAJ",
      "4ngycwIAAAAJ"
    ],
    "citation_count": 31,
    "gs_author_count": 8
  },
  {
    "arxiv_id": "2406.07043",
    "title": "1st Place Solution for MeViS Track in CVPR 2024 PVUW Workshop: Motion Expression guided Video Segmentation",
    "year": 2024,
    "published": "2024-06-11T08:05:26Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Motion Expression guided Video Segmentation (MeViS), as an emerging task, poses many new challenges to the field of referring video object segmentation (RVOS). In this technical report, we investigated and validated the effectiveness of static-dominant data and frame sampling on this challenging setting. Our solution achieves a J&F score of 0.5447 in the competition phase and ranks 1st in the MeViS track of the PVUW Challenge. The code is available at: https://github.com/Tapall-AI/MeViS_Track_So",
    "arxiv_url": "https://arxiv.org/abs/2406.07043v1",
    "pdf_url": "https://arxiv.org/pdf/2406.07043v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2406.07043",
    "arxiv_authors": [
      "Mingqi Gao",
      "Jingnan Luo",
      "Jinyu Yang",
      "Jungong Han",
      "Feng Zheng"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=1st+Place+Solution+for+MeViS+Track+in+CVPR+2024+PVUW+Workshop%3A+Motion+Expression+guided+Video+Segmentation+Mingqi+Gao+Jingnan+Luo+Jinyu+Yang+Jungong+Han+Feng+Zheng",
    "gs_search_success": true,
    "gs_authors": [
      "ECCd0hwAAAAJ",
      "hNi1gxAAAAAJ",
      "PcmyXHMAAAAJ"
    ],
    "citation_count": 3,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2307.09715",
    "title": "Semantic-Aware Dual Contrastive Learning for Multi-label Image Classification",
    "year": 2023,
    "published": "2023-07-19T01:57:31Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Extracting image semantics effectively and assigning corresponding labels to multiple objects or attributes for natural images is challenging due to the complex scene contents and confusing label dependencies. Recent works have focused on modeling label relationships with graph and understanding object regions using class activation maps (CAM). However, these methods ignore the complex intra- and inter-category relationships among specific semantic features, and CAM is prone to generate noisy in",
    "arxiv_url": "https://arxiv.org/abs/2307.09715v4",
    "pdf_url": "https://arxiv.org/pdf/2307.09715v4",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2307.09715",
    "arxiv_authors": [
      "Leilei Ma",
      "Dengdi Sun",
      "Lei Wang",
      "Haifeng Zhao",
      "Bin Luo"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Semantic-Aware+Dual+Contrastive+Learning+for+Multi-label+Image+Classification+Leilei+Ma+Dengdi+Sun+Lei+Wang+Haifeng+Zhao+Bin+Luo",
    "gs_search_success": true,
    "gs_authors": [
      "ymg2g4EAAAAJ",
      "blXZTGkAAAAJ",
      "tuayA3cAAAAJ",
      "SWUHieAAAAAJ",
      "0qaDapcAAAAJ"
    ],
    "citation_count": 14,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2303.09245",
    "title": "Cross-head Supervision for Crowd Counting with Noisy Annotations",
    "year": 2023,
    "published": "2023-03-16T11:45:52Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Noisy annotations such as missing annotations and location shifts often exist in crowd counting datasets due to multi-scale head sizes, high occlusion, etc. These noisy annotations severely affect the model training, especially for density map-based methods. To alleviate the negative impact of noisy annotations, we propose a novel crowd counting model with one convolution head and one transformer head, in which these two heads can supervise each other in noisy areas, called Cross-Head Supervisio",
    "arxiv_url": "https://arxiv.org/abs/2303.09245v1",
    "pdf_url": "https://arxiv.org/pdf/2303.09245v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2303.09245",
    "arxiv_authors": [
      "Mingliang Dai",
      "Zhizhong Huang",
      "Jiaqi Gao",
      "Hongming Shan",
      "Junping Zhang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Cross-head+Supervision+for+Crowd+Counting+with+Noisy+Annotations+Mingliang+Dai+Zhizhong+Huang+Jiaqi+Gao+Hongming+Shan+Junping+Zhang",
    "gs_search_success": true,
    "gs_authors": [
      "Aib_NTYAAAAJ",
      "6Itl2tMAAAAJ",
      "egyYq1EAAAAJ",
      "RYfSzKwAAAAJ",
      "wbPGvY8AAAAJ"
    ],
    "citation_count": 40,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2504.19828",
    "title": "HOIGaze: Gaze Estimation During Hand-Object Interactions in Extended Reality Exploiting Eye-Hand-Head Coordination",
    "year": 2025,
    "published": "2025-04-28T14:31:43Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "We present HOIGaze - a novel learning-based approach for gaze estimation during hand-object interactions (HOI) in extended reality (XR). HOIGaze addresses the challenging HOI setting by building on one key insight: The eye, hand, and head movements are closely coordinated during HOIs and this coordination can be exploited to identify samples that are most useful for gaze estimator training - as such, effectively denoising the training data. This denoising approach is in stark contrast to previou",
    "arxiv_url": "https://arxiv.org/abs/2504.19828v1",
    "pdf_url": "https://arxiv.org/pdf/2504.19828v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2504.19828",
    "arxiv_authors": [
      "Zhiming Hu",
      "Daniel Haeufle",
      "Syn Schmitt",
      "Andreas Bulling"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=HOIGaze%3A+Gaze+Estimation+During+Hand-Object+Interactions+in+Extended+Reality+Exploiting+Eye-Hand-Head+Coordination+Zhiming+Hu+Daniel+Haeufle+Syn+Schmitt+Andreas+Bulling",
    "gs_search_success": true,
    "gs_authors": [
      "QURZIzUAAAAJ",
      "HO3nVAoAAAAJ",
      "OLB_xBEAAAAJ"
    ],
    "citation_count": 4,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2404.08027",
    "title": "SurvMamba: State Space Model with Multi-grained Multi-modal Interaction for Survival Prediction",
    "year": 2024,
    "published": "2024-04-11T15:58:12Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "q-bio.QM"
    ],
    "abstract": "Multi-modal learning that combines pathological images with genomic data has significantly enhanced the accuracy of survival prediction. Nevertheless, existing methods have not fully utilized the inherent hierarchical structure within both whole slide images (WSIs) and transcriptomic data, from which better intra-modal representations and inter-modal integration could be derived. Moreover, many existing studies attempt to improve multi-modal representations through attention mechanisms, which in",
    "arxiv_url": "https://arxiv.org/abs/2404.08027v2",
    "pdf_url": "https://arxiv.org/pdf/2404.08027v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2404.08027",
    "arxiv_authors": [
      "Ying Chen",
      "Jiajing Xie",
      "Yuxiang Lin",
      "Yuhang Song",
      "Wenxian Yang",
      "Rongshan Yu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=SurvMamba%3A+State+Space+Model+with+Multi-grained+Multi-modal+Interaction+for+Survival+Prediction+Ying+Chen+Jiajing+Xie+Yuxiang+Lin+Yuhang+Song+Wenxian+Yang",
    "gs_search_success": true,
    "gs_authors": [
      "Uh1EpKQAAAAJ",
      "vZFjxIYAAAAJ",
      "TgJ6W0YAAAAJ",
      "bYfM8fEAAAAJ"
    ],
    "citation_count": 15,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2505.21904",
    "title": "CAST: Contrastive Adaptation and Distillation for Semi-Supervised Instance Segmentation",
    "year": 2025,
    "published": "2025-05-28T02:45:42Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Instance segmentation demands costly per-pixel annotations and computationally expensive models. We introduce CAST, a semi-supervised knowledge distillation (SSKD) framework that compresses pre-trained vision foundation models (VFM) into compact experts using limited labeled and abundant unlabeled data. CAST unfolds in three stages: (1) domain adaptation of the VFM(s) via self-training with contrastive calibration, (2) knowledge transfer through a unified multi-objective loss, and (3) student re",
    "arxiv_url": "https://arxiv.org/abs/2505.21904v4",
    "pdf_url": "https://arxiv.org/pdf/2505.21904v4",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2505.21904",
    "arxiv_authors": [
      "Pardis Taghavi",
      "Tian Liu",
      "Renjie Li",
      "Reza Langari",
      "Zhengzhong Tu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=CAST%3A+Contrastive+Adaptation+and+Distillation+for+Semi-Supervised+Instance+Segmentation+Pardis+Taghavi+Tian+Liu+Renjie+Li+Reza+Langari+Zhengzhong+Tu",
    "gs_search_success": true,
    "gs_authors": [
      "TX9IfGoAAAAJ",
      "qgmIiSsAAAAJ",
      "IozzI8sAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2308.11568",
    "title": "SPANet: Frequency-balancing Token Mixer using Spectral Pooling Aggregation Modulation",
    "year": 2023,
    "published": "2023-08-22T17:14:19Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Recent studies show that self-attentions behave like low-pass filters (as opposed to convolutions) and enhancing their high-pass filtering capability improves model performance. Contrary to this idea, we investigate existing convolution-based models with spectral analysis and observe that improving the low-pass filtering in convolution operations also leads to performance improvement. To account for this observation, we hypothesize that utilizing optimal token mixers that capture balanced repres",
    "arxiv_url": "https://arxiv.org/abs/2308.11568v1",
    "pdf_url": "https://arxiv.org/pdf/2308.11568v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2308.11568",
    "arxiv_authors": [
      "Guhnoo Yun",
      "Juhan Yoo",
      "Kijung Kim",
      "Jeongho Lee",
      "Dong Hwan Kim"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=SPANet%3A+Frequency-balancing+Token+Mixer+using+Spectral+Pooling+Aggregation+Modulation+Guhnoo+Yun+Juhan+Yoo+Kijung+Kim+Jeongho+Lee+Dong+Hwan+Kim",
    "gs_search_success": true,
    "gs_authors": [
      "rxYXwyAAAAAJ",
      "L47h01YAAAAJ",
      "rSdbLsYAAAAJ",
      "xP2p_3YAAAAJ"
    ],
    "citation_count": 26,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2412.04715",
    "title": "Addressing Text Embedding Leakage in Diffusion-based Image Editing",
    "year": 2024,
    "published": "2024-12-06T02:10:07Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Text-based image editing, powered by generative diffusion models, lets users modify images through natural-language prompts and has dramatically simplified traditional workflows. Despite these advances, current methods still suffer from a critical problem: attribute leakage, where edits meant for specific objects unintentionally affect unrelated regions or other target objects. Our analysis reveals the root cause as the semantic entanglement inherent in End-of-Sequence (EOS) embeddings generated",
    "arxiv_url": "https://arxiv.org/abs/2412.04715v4",
    "pdf_url": "https://arxiv.org/pdf/2412.04715v4",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2412.04715",
    "arxiv_authors": [
      "Sunung Mun",
      "Jinhwan Nam",
      "Sunghyun Cho",
      "Jungseul Ok"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Addressing+Text+Embedding+Leakage+in+Diffusion-based+Image+Editing+Sunung+Mun+Jinhwan+Nam+Sunghyun+Cho+Jungseul+Ok",
    "gs_search_success": true,
    "gs_authors": [
      "X9B6HQ4AAAAJ",
      "JcmBvtUAAAAJ",
      "KWG3UUMAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2312.08865",
    "title": "Improving Cross-modal Alignment with Synthetic Pairs for Text-only Image Captioning",
    "year": 2023,
    "published": "2023-12-14T12:39:29Z",
    "categories": [
      "cs.CV",
      "cs.CL"
    ],
    "abstract": "Although image captioning models have made significant advancements in recent years, the majority of them heavily depend on high-quality datasets containing paired images and texts which are costly to acquire. Previous works leverage the CLIP's cross-modal association ability for image captioning, relying solely on textual information under unsupervised settings. However, not only does a modality gap exist between CLIP text and image features, but a discrepancy also arises between training and i",
    "arxiv_url": "https://arxiv.org/abs/2312.08865v1",
    "pdf_url": "https://arxiv.org/pdf/2312.08865v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2312.08865",
    "arxiv_authors": [
      "Zhiyue Liu",
      "Jinyuan Liu",
      "Fanrong Ma"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Improving+Cross-modal+Alignment+with+Synthetic+Pairs+for+Text-only+Image+Captioning+Zhiyue+Liu+Jinyuan+Liu+Fanrong+Ma",
    "gs_search_success": true,
    "gs_authors": [
      "Sp4xqzkAAAAJ"
    ],
    "citation_count": 22,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2310.03205",
    "title": "A Large-Scale 3D Face Mesh Video Dataset via Neural Re-parameterized Optimization",
    "year": 2023,
    "published": "2023-10-04T23:24:22Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "We propose NeuFace, a 3D face mesh pseudo annotation method on videos via neural re-parameterized optimization. Despite the huge progress in 3D face reconstruction methods, generating reliable 3D face labels for in-the-wild dynamic videos remains challenging. Using NeuFace optimization, we annotate the per-view/-frame accurate and consistent face meshes on large-scale face videos, called the NeuFace-dataset. We investigate how neural re-parameterization helps to reconstruct image-aligned facial ",
    "arxiv_url": "https://arxiv.org/abs/2310.03205v2",
    "pdf_url": "https://arxiv.org/pdf/2310.03205v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2310.03205",
    "arxiv_authors": [
      "Kim Youwang",
      "Lee Hyun",
      "Kim Sung-Bin",
      "Suekyeong Nam",
      "Janghoon Ju",
      "Tae-Hyun Oh"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=A+Large-Scale+3D+Face+Mesh+Video+Dataset+via+Neural+Re-parameterized+Optimization+Kim+Youwang+Lee+Hyun+Kim+Sung-Bin+Suekyeong+Nam+Janghoon+Ju",
    "gs_search_success": true,
    "gs_authors": [
      "bWsRk0MAAAAJ",
      "jVg4BCAAAAAJ",
      "gKXTrF8AAAAJ",
      "21Fo7cwAAAAJ",
      "Ff7f5RwAAAAJ"
    ],
    "citation_count": 3,
    "gs_author_count": null
  },
  {
    "arxiv_id": "2412.12735",
    "title": "GIRAFFE: Design Choices for Extending the Context Length of Visual Language Models",
    "year": 2024,
    "published": "2024-12-17T09:57:21Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "abstract": "Visual Language Models (VLMs) demonstrate impressive capabilities in processing multimodal inputs, yet applications such as visual agents, which require handling multiple images and high-resolution videos, demand enhanced long-range modeling. Moreover, existing open-source VLMs lack systematic exploration into extending their context length, and commercial models often provide limited details. To tackle this, we aim to establish an effective solution that enhances long context performance of VLM",
    "arxiv_url": "https://arxiv.org/abs/2412.12735v1",
    "pdf_url": "https://arxiv.org/pdf/2412.12735v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2412.12735",
    "arxiv_authors": [
      "Mukai Li",
      "Lei Li",
      "Shansan Gong",
      "Qi Liu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=GIRAFFE%3A+Design+Choices+for+Extending+the+Context+Length+of+Visual+Language+Models+Mukai+Li+Lei+Li+Shansan+Gong+Qi+Liu",
    "gs_search_success": false,
    "gs_authors": [],
    "citation_count": null,
    "gs_author_count": null,
    "error_type": "fetch_failed"
  },
  {
    "arxiv_id": "2504.03857",
    "title": "Can ChatGPT Learn My Life From a Week of First-Person Video?",
    "year": 2025,
    "published": "2025-04-04T18:33:45Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "abstract": "Motivated by recent improvements in generative AI and wearable camera devices (e.g. smart glasses and AI-enabled pins), I investigate the ability of foundation models to learn about the wearer's personal life through first-person camera data. To test this, I wore a camera headset for 54 hours over the course of a week, generated summaries of various lengths (e.g. minute-long, hour-long, and day-long summaries), and fine-tuned both GPT-4o and GPT-4o-mini on the resulting summary hierarchy. By que",
    "arxiv_url": "https://arxiv.org/abs/2504.03857v1",
    "pdf_url": "https://arxiv.org/pdf/2504.03857v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2504.03857",
    "arxiv_authors": [
      "Keegan Harris"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Can+ChatGPT+Learn+My+Life+From+a+Week+of+First-Person+Video%3F+Keegan+Harris",
    "gs_search_success": true,
    "gs_authors": [
      "TnvQIrYAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 1
  },
  {
    "arxiv_id": "2311.10605",
    "title": "CA-Jaccard: Camera-aware Jaccard Distance for Person Re-identification",
    "year": 2023,
    "published": "2023-11-17T16:01:06Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Person re-identification (re-ID) is a challenging task that aims to learn discriminative features for person retrieval. In person re-ID, Jaccard distance is a widely used distance metric, especially in re-ranking and clustering scenarios. However, we discover that camera variation has a significant negative impact on the reliability of Jaccard distance. In particular, Jaccard distance calculates the distance based on the overlap of relevant neighbors. Due to camera variation, intra-camera sample",
    "arxiv_url": "https://arxiv.org/abs/2311.10605v2",
    "pdf_url": "https://arxiv.org/pdf/2311.10605v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2311.10605",
    "arxiv_authors": [
      "Yiyu Chen",
      "Zheyi Fan",
      "Zhaoru Chen",
      "Yixuan Zhu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=CA-Jaccard%3A+Camera-aware+Jaccard+Distance+for+Person+Re-identification+Yiyu+Chen+Zheyi+Fan+Zhaoru+Chen+Yixuan+Zhu",
    "gs_search_success": true,
    "gs_authors": [
      "HsRMOBUAAAAJ"
    ],
    "citation_count": 30,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2311.08746",
    "title": "A Diffusion Model Based Quality Enhancement Method for HEVC Compressed Video",
    "year": 2023,
    "published": "2023-11-15T07:29:23Z",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "abstract": "Video post-processing methods can improve the quality of compressed videos at the decoder side. Most of the existing methods need to train corresponding models for compressed videos with different quantization parameters to improve the quality of compressed videos. However, in most cases, the quantization parameters of the decoded video are unknown. This makes existing methods have their limitations in improving video quality. To tackle this problem, this work proposes a diffusion model based po",
    "arxiv_url": "https://arxiv.org/abs/2311.08746v1",
    "pdf_url": "https://arxiv.org/pdf/2311.08746v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2311.08746",
    "arxiv_authors": [
      "Zheng Liu",
      "Honggang Qi"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=A+Diffusion+Model+Based+Quality+Enhancement+Method+for+HEVC+Compressed+Video+Zheng+Liu+Honggang+Qi",
    "gs_search_success": true,
    "gs_authors": [],
    "citation_count": 1,
    "gs_author_count": 2
  },
  {
    "arxiv_id": "2306.11113",
    "title": "Learn to Accumulate Evidence from All Training Samples: Theory and Practice",
    "year": 2023,
    "published": "2023-06-19T18:27:12Z",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "abstract": "Evidential deep learning, built upon belief theory and subjective logic, offers a principled and computationally efficient way to turn a deterministic neural network uncertainty-aware. The resultant evidential models can quantify fine-grained uncertainty using the learned evidence. To ensure theoretically sound evidential models, the evidence needs to be non-negative, which requires special activation functions for model training and inference. This constraint often leads to inferior predictive ",
    "arxiv_url": "https://arxiv.org/abs/2306.11113v2",
    "pdf_url": "https://arxiv.org/pdf/2306.11113v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2306.11113",
    "arxiv_authors": [
      "Deep Pandey",
      "Qi Yu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Learn+to+Accumulate+Evidence+from+All+Training+Samples%3A+Theory+and+Practice+Deep+Pandey+Qi+Yu",
    "gs_search_success": true,
    "gs_authors": [
      "ohGaxXIAAAAJ",
      "L3gWdfEAAAAJ"
    ],
    "citation_count": 30,
    "gs_author_count": 2
  },
  {
    "arxiv_id": "2409.00050",
    "title": "Extending Machine Learning Based RF Coverage Predictions to 3D",
    "year": 2024,
    "published": "2024-08-19T16:45:38Z",
    "categories": [
      "eess.SP",
      "cs.CV",
      "cs.IT",
      "cs.LG"
    ],
    "abstract": "This paper discusses recent advancements made in the fast prediction of signal power in mmWave communications environments. Using machine learning (ML) it is possible to train models that provide power estimates with both good accuracy and with real-time simulation speeds. Work involving improved training data pre-processing as well as 3D predictions with arbitrary transmitter height is discussed.",
    "arxiv_url": "https://arxiv.org/abs/2409.00050v1",
    "pdf_url": "https://arxiv.org/pdf/2409.00050v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2409.00050",
    "arxiv_authors": [
      "Muyao Chen",
      "Mathieu Châteauvert",
      "Jonathan Ethier"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Extending+Machine+Learning+Based+RF+Coverage+Predictions+to+3D+Muyao+Chen+Mathieu+Ch%C3%A2teauvert+Jonathan+Ethier",
    "gs_search_success": true,
    "gs_authors": [
      "PEak-nIAAAAJ"
    ],
    "citation_count": 10,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2310.06525",
    "title": "Perceptual MAE for Image Manipulation Localization: A High-level Vision Learner Focusing on Low-level Features",
    "year": 2023,
    "published": "2023-10-10T11:14:29Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Nowadays, multimedia forensics faces unprecedented challenges due to the rapid advancement of multimedia generation technology thereby making Image Manipulation Localization (IML) crucial in the pursuit of truth. The key to IML lies in revealing the artifacts or inconsistencies between the tampered and authentic areas, which are evident under pixel-level features. Consequently, existing studies treat IML as a low-level vision task, focusing on allocating tampered masks by crafting pixel-level fe",
    "arxiv_url": "https://arxiv.org/abs/2310.06525v1",
    "pdf_url": "https://arxiv.org/pdf/2310.06525v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2310.06525",
    "arxiv_authors": [
      "Xiaochen Ma",
      "Jizhe Zhou",
      "Xiong Xu",
      "Zhuohang Jiang",
      "Chi-Man Pun"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Perceptual+MAE+for+Image+Manipulation+Localization%3A+A+High-level+Vision+Learner+Focusing+on+Low-level+Features+Xiaochen+Ma+Jizhe+Zhou+Xiong+Xu+Zhuohang+Jiang+Chi-Man+Pun",
    "gs_search_success": true,
    "gs_authors": [
      "hGEIyCEAAAAJ",
      "-cNWmJMAAAAJ",
      "h3vEhrgAAAAJ",
      "JTkP_EAAAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2304.04421",
    "title": "Local-Global Temporal Difference Learning for Satellite Video Super-Resolution",
    "year": 2023,
    "published": "2023-04-10T07:04:40Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Optical-flow-based and kernel-based approaches have been extensively explored for temporal compensation in satellite Video Super-Resolution (VSR). However, these techniques are less generalized in large-scale or complex scenarios, especially in satellite videos. In this paper, we propose to exploit the well-defined temporal difference for efficient and effective temporal compensation. To fully utilize the local and global temporal information within frames, we systematically modeled the short-te",
    "arxiv_url": "https://arxiv.org/abs/2304.04421v3",
    "pdf_url": "https://arxiv.org/pdf/2304.04421v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2304.04421",
    "arxiv_authors": [
      "Yi Xiao",
      "Qiangqiang Yuan",
      "Kui Jiang",
      "Xianyu Jin",
      "Jiang He",
      "Liangpei Zhang",
      "Chia-Wen Lin"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Local-Global+Temporal+Difference+Learning+for+Satellite+Video+Super-Resolution+Yi+Xiao+Qiangqiang+Yuan+Kui+Jiang+Xianyu+Jin+Jiang+He",
    "gs_search_success": true,
    "gs_authors": [
      "AbOLE9QAAAAJ",
      "yFEl8hcAAAAJ",
      "nX5yzcsAAAAJ",
      "aItnA-sAAAAJ",
      "czAEHHoAAAAJ",
      "fXN3dl0AAAAJ"
    ],
    "citation_count": 147,
    "gs_author_count": 7
  },
  {
    "arxiv_id": "2306.11290",
    "title": "Habitat Synthetic Scenes Dataset (HSSD-200): An Analysis of 3D Scene Scale and Realism Tradeoffs for ObjectGoal Navigation",
    "year": 2023,
    "published": "2023-06-20T05:07:23Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "We contribute the Habitat Synthetic Scene Dataset, a dataset of 211 high-quality 3D scenes, and use it to test navigation agent generalization to realistic 3D environments. Our dataset represents real interiors and contains a diverse set of 18,656 models of real-world objects. We investigate the impact of synthetic 3D scene dataset scale and realism on the task of training embodied agents to find and navigate to objects (ObjectGoal navigation). By comparing to synthetic 3D scene datasets from pr",
    "arxiv_url": "https://arxiv.org/abs/2306.11290v3",
    "pdf_url": "https://arxiv.org/pdf/2306.11290v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2306.11290",
    "arxiv_authors": [
      "Mukul Khanna",
      "Yongsen Mao",
      "Hanxiao Jiang",
      "Sanjay Haresh",
      "Brennan Shacklett",
      "Dhruv Batra",
      "Alexander Clegg",
      "Eric Undersander",
      "Angel X. Chang",
      "Manolis Savva"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Habitat+Synthetic+Scenes+Dataset+%28HSSD-200%29%3A+An+Analysis+of+3D+Scene+Scale+and+Realism+Tradeoffs+for+ObjectGoal+Navigation+Mukul+Khanna+Yongsen+Mao+Hanxiao+Jiang+Sanjay+Haresh+Brennan+Shacklett",
    "gs_search_success": true,
    "gs_authors": [
      "_bs7PqgAAAAJ",
      "bm9JqwMAAAAJ",
      "p463opcAAAAJ",
      "-XWZKZAAAAAJ",
      "boFO-7gAAAAJ",
      "kWAlOAkAAAAJ"
    ],
    "citation_count": 100,
    "gs_author_count": 8
  },
  {
    "arxiv_id": "2302.10301",
    "title": "Artificial Intelligence System for Detection and Screening of Cardiac Abnormalities using Electrocardiogram Images",
    "year": 2023,
    "published": "2023-02-10T10:54:33Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "The artificial intelligence (AI) system has achieved expert-level performance in electrocardiogram (ECG) signal analysis. However, in underdeveloped countries or regions where the healthcare information system is imperfect, only paper ECGs can be provided. Analysis of real-world ECG images (photos or scans of paper ECGs) remains challenging due to complex environments or interference. In this study, we present an AI system developed to detect and screen cardiac abnormalities (CAs) from real-worl",
    "arxiv_url": "https://arxiv.org/abs/2302.10301v1",
    "pdf_url": "https://arxiv.org/pdf/2302.10301v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2302.10301",
    "arxiv_authors": [
      "Deyun Zhang",
      "Shijia Geng",
      "Yang Zhou",
      "Weilun Xu",
      "Guodong Wei",
      "Kai Wang",
      "Jie Yu",
      "Qiang Zhu",
      "Yongkui Li",
      "Yonghong Zhao",
      "Xingyue Chen",
      "Rui Zhang",
      "Zhaoji Fu",
      "Rongbo Zhou",
      "Yanqi E",
      "Sumei Fan",
      "Qinghao Zhao",
      "Chuandong Cheng",
      "Nan Peng",
      "Liang Zhang",
      "Linlin Zheng",
      "Jianjun Chu",
      "Hongbin Xu",
      "Chen Tan",
      "Jian Liu",
      "Huayue Tao",
      "Tong Liu",
      "Kangyin Chen",
      "Chenyang Jiang",
      "Xingpeng Liu",
      "Shenda Hong"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Artificial+Intelligence+System+for+Detection+and+Screening+of+Cardiac+Abnormalities+using+Electrocardiogram+Images+Deyun+Zhang+Shijia+Geng+Yang+Zhou+Weilun+Xu+Guodong+Wei",
    "gs_search_success": true,
    "gs_authors": [
      "Q0L04uYAAAAJ",
      "QKTsRW8AAAAJ"
    ],
    "citation_count": 4,
    "gs_author_count": 11
  },
  {
    "arxiv_id": "2407.21654",
    "title": "MTA-CLIP: Language-Guided Semantic Segmentation with Mask-Text Alignment",
    "year": 2024,
    "published": "2024-07-31T14:56:42Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Recent approaches have shown that large-scale vision-language models such as CLIP can improve semantic segmentation performance. These methods typically aim for pixel-level vision-language alignment, but often rely on low resolution image features from CLIP, resulting in class ambiguities along boundaries. Moreover, the global scene representations in CLIP text embeddings do not directly correlate with the local and detailed pixel-level features, making meaningful alignment more difficult. To ad",
    "arxiv_url": "https://arxiv.org/abs/2407.21654v1",
    "pdf_url": "https://arxiv.org/pdf/2407.21654v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2407.21654",
    "arxiv_authors": [
      "Anurag Das",
      "Xinting Hu",
      "Li Jiang",
      "Bernt Schiele"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=MTA-CLIP%3A+Language-Guided+Semantic+Segmentation+with+Mask-Text+Alignment+Anurag+Das+Xinting+Hu+Li+Jiang+Bernt+Schiele",
    "gs_search_success": true,
    "gs_authors": [
      "o6h6sVMAAAAJ",
      "5cIodxsAAAAJ",
      "pchPMh0AAAAJ",
      "z76PBfYAAAAJ"
    ],
    "citation_count": 12,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2308.14852",
    "title": "SynthDistill: Face Recognition with Knowledge Distillation from Synthetic Data",
    "year": 2023,
    "published": "2023-08-28T19:15:27Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "State-of-the-art face recognition networks are often computationally expensive and cannot be used for mobile applications. Training lightweight face recognition models also requires large identity-labeled datasets. Meanwhile, there are privacy and ethical concerns with collecting and using large face recognition datasets. While generating synthetic datasets for training face recognition models is an alternative option, it is challenging to generate synthetic data with sufficient intra-class vari",
    "arxiv_url": "https://arxiv.org/abs/2308.14852v1",
    "pdf_url": "https://arxiv.org/pdf/2308.14852v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2308.14852",
    "arxiv_authors": [
      "Hatef Otroshi Shahreza",
      "Anjith George",
      "Sébastien Marcel"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=SynthDistill%3A+Face+Recognition+with+Knowledge+Distillation+from+Synthetic+Data+Hatef+Otroshi+Shahreza+Anjith+George+S%C3%A9bastien+Marcel",
    "gs_search_success": true,
    "gs_authors": [
      "4CCrDp0AAAAJ",
      "K9ku4jYAAAAJ",
      "zFHgwcAAAAAJ"
    ],
    "citation_count": 18,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2309.12700",
    "title": "mixed attention auto encoder for multi-class industrial anomaly detection",
    "year": 2023,
    "published": "2023-09-22T08:17:48Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Most existing methods for unsupervised industrial anomaly detection train a separate model for each object category. This kind of approach can easily capture the category-specific feature distributions, but results in high storage cost and low training efficiency. In this paper, we propose a unified mixed-attention auto encoder (MAAE) to implement multi-class anomaly detection with a single model. To alleviate the performance degradation due to the diverse distribution patterns of different cate",
    "arxiv_url": "https://arxiv.org/abs/2309.12700v1",
    "pdf_url": "https://arxiv.org/pdf/2309.12700v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2309.12700",
    "arxiv_authors": [
      "Jiangqi Liu",
      "Feng Wang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=mixed+attention+auto+encoder+for+multi-class+industrial+anomaly+detection+Jiangqi+Liu+Feng+Wang",
    "gs_search_success": true,
    "gs_authors": [],
    "citation_count": 14,
    "gs_author_count": 2
  },
  {
    "arxiv_id": "2308.02588",
    "title": "Unmasking Parkinson's Disease with Smile: An AI-enabled Screening Framework",
    "year": 2023,
    "published": "2023-08-03T18:23:37Z",
    "categories": [
      "eess.IV",
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "We present an efficient and accessible PD screening method by leveraging AI-driven models enabled by the largest video dataset of facial expressions from 1,059 unique participants. This dataset includes 256 individuals with PD, 165 clinically diagnosed, and 91 self-reported. Participants used webcams to record themselves mimicking three facial expressions (smile, disgust, and surprise) from diverse sources encompassing their homes across multiple countries, a US clinic, and a PD wellness center ",
    "arxiv_url": "https://arxiv.org/abs/2308.02588v2",
    "pdf_url": "https://arxiv.org/pdf/2308.02588v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2308.02588",
    "arxiv_authors": [
      "Tariq Adnan",
      "Md Saiful Islam",
      "Wasifur Rahman",
      "Sangwu Lee",
      "Sutapa Dey Tithi",
      "Kazi Noshin",
      "Imran Sarker",
      "M Saifur Rahman",
      "Ehsan Hoque"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Unmasking+Parkinson%27s+Disease+with+Smile%3A+An+AI-enabled+Screening+Framework+Tariq+Adnan+Md+Saiful+Islam+Wasifur+Rahman+Sangwu+Lee+Sutapa+Dey+Tithi",
    "gs_search_success": true,
    "gs_authors": [
      "FC3AM-wAAAAJ",
      "vJ4Yn0EAAAAJ",
      "Cw9T0MUAAAAJ",
      "E-MAzJ4AAAAJ",
      "FBJeGpAAAAAJ",
      "ZJrR0KQAAAAJ",
      "LmRLizgAAAAJ"
    ],
    "citation_count": 7,
    "gs_author_count": 9
  },
  {
    "arxiv_id": "2410.01089",
    "title": "FMBench: Benchmarking Fairness in Multimodal Large Language Models on Medical Tasks",
    "year": 2024,
    "published": "2024-10-01T21:38:15Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Advancements in Multimodal Large Language Models (MLLMs) have significantly improved medical task performance, such as Visual Question Answering (VQA) and Report Generation (RG). However, the fairness of these models across diverse demographic groups remains underexplored, despite its importance in healthcare. This oversight is partly due to the lack of demographic diversity in existing medical multimodal datasets, which complicates the evaluation of fairness. In response, we propose FMBench, th",
    "arxiv_url": "https://arxiv.org/abs/2410.01089v1",
    "pdf_url": "https://arxiv.org/pdf/2410.01089v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2410.01089",
    "arxiv_authors": [
      "Peiran Wu",
      "Che Liu",
      "Canyu Chen",
      "Jun Li",
      "Cosmin I. Bercea",
      "Rossella Arcucci"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=FMBench%3A+Benchmarking+Fairness+in+Multimodal+Large+Language+Models+on+Medical+Tasks+Peiran+Wu+Che+Liu+Canyu+Chen+Jun+Li+Cosmin+I.+Bercea",
    "gs_search_success": true,
    "gs_authors": [
      "6M7srVcAAAAJ",
      "t9vB5TgAAAAJ",
      "oxy2ZQoAAAAJ",
      "iKfWNy0AAAAJ",
      "t9-PPzQAAAAJ"
    ],
    "citation_count": 11,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2311.15540",
    "title": "EAFP-Med: An Efficient Adaptive Feature Processing Module Based on Prompts for Medical Image Detection",
    "year": 2023,
    "published": "2023-11-27T05:10:15Z",
    "categories": [
      "cs.CV",
      "cs.MM"
    ],
    "abstract": "In the face of rapid advances in medical imaging, cross-domain adaptive medical image detection is challenging due to the differences in lesion representations across various medical imaging technologies. To address this issue, we draw inspiration from large language models to propose EAFP-Med, an efficient adaptive feature processing module based on prompts for medical image detection. EAFP-Med can efficiently extract lesion features of different scales from a diverse range of medical images ba",
    "arxiv_url": "https://arxiv.org/abs/2311.15540v1",
    "pdf_url": "https://arxiv.org/pdf/2311.15540v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2311.15540",
    "arxiv_authors": [
      "Xiang Li",
      "Long Lan",
      "Husam Lahza",
      "Shaowu Yang",
      "Shuihua Wang",
      "Wenjing Yang",
      "Hengzhu Liu",
      "Yudong Zhang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=EAFP-Med%3A+An+Efficient+Adaptive+Feature+Processing+Module+Based+on+Prompts+for+Medical+Image+Detection+Xiang+Li+Long+Lan+Husam+Lahza+Shaowu+Yang+Shuihua+Wang",
    "gs_search_success": false,
    "gs_authors": [],
    "citation_count": null,
    "gs_author_count": null,
    "error_type": "fetch_failed"
  },
  {
    "arxiv_id": "2308.09098",
    "title": "ImGeoNet: Image-induced Geometry-aware Voxel Representation for Multi-view 3D Object Detection",
    "year": 2023,
    "published": "2023-08-17T16:49:38Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "We propose ImGeoNet, a multi-view image-based 3D object detection framework that models a 3D space by an image-induced geometry-aware voxel representation. Unlike previous methods which aggregate 2D features into 3D voxels without considering geometry, ImGeoNet learns to induce geometry from multi-view images to alleviate the confusion arising from voxels of free space, and during the inference phase, only images from multiple views are required. Besides, a powerful pre-trained 2D feature extrac",
    "arxiv_url": "https://arxiv.org/abs/2308.09098v1",
    "pdf_url": "https://arxiv.org/pdf/2308.09098v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2308.09098",
    "arxiv_authors": [
      "Tao Tu",
      "Shun-Po Chuang",
      "Yu-Lun Liu",
      "Cheng Sun",
      "Ke Zhang",
      "Donna Roy",
      "Cheng-Hao Kuo",
      "Min Sun"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=ImGeoNet%3A+Image-induced+Geometry-aware+Voxel+Representation+for+Multi-view+3D+Object+Detection+Tao+Tu+Shun-Po+Chuang+Yu-Lun+Liu+Cheng+Sun+Ke+Zhang",
    "gs_search_success": true,
    "gs_authors": [
      "1Rf6sGcAAAAJ",
      "CYDgtRoAAAAJ",
      "JwvnA1gAAAAJ",
      "JP0fuKAAAAAJ",
      "nvQampwAAAAJ",
      "gliihzoAAAAJ",
      "biks_VUAAAAJ"
    ],
    "citation_count": 21,
    "gs_author_count": 8
  },
  {
    "arxiv_id": "2302.06604",
    "title": "ALAN: Autonomously Exploring Robotic Agents in the Real World",
    "year": 2023,
    "published": "2023-02-13T18:59:09Z",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV",
      "cs.LG",
      "eess.SY"
    ],
    "abstract": "Robotic agents that operate autonomously in the real world need to continuously explore their environment and learn from the data collected, with minimal human supervision. While it is possible to build agents that can learn in such a manner without supervision, current methods struggle to scale to the real world. Thus, we propose ALAN, an autonomously exploring robotic agent, that can perform tasks in the real world with little training and interaction time. This is enabled by measuring environ",
    "arxiv_url": "https://arxiv.org/abs/2302.06604v1",
    "pdf_url": "https://arxiv.org/pdf/2302.06604v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2302.06604",
    "arxiv_authors": [
      "Russell Mendonca",
      "Shikhar Bahl",
      "Deepak Pathak"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=ALAN%3A+Autonomously+Exploring+Robotic+Agents+in+the+Real+World+Russell+Mendonca+Shikhar+Bahl+Deepak+Pathak",
    "gs_search_success": true,
    "gs_authors": [
      "bdHgGgEAAAAJ",
      "Uly5spMAAAAJ",
      "AEsPCAUAAAAJ"
    ],
    "citation_count": 27,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2408.12825",
    "title": "MergeUp-augmented Semi-Weakly Supervised Learning for WSI Classification",
    "year": 2024,
    "published": "2024-08-23T04:08:30Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Recent advancements in computational pathology and artificial intelligence have significantly improved whole slide image (WSI) classification. However, the gigapixel resolution of WSIs and the scarcity of manual annotations present substantial challenges. Multiple instance learning (MIL) is a promising weakly supervised learning approach for WSI classification. Recently research revealed employing pseudo bag augmentation can encourage models to learn various data, thus bolstering models' perform",
    "arxiv_url": "https://arxiv.org/abs/2408.12825v1",
    "pdf_url": "https://arxiv.org/pdf/2408.12825v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2408.12825",
    "arxiv_authors": [
      "Mingxi Ouyang",
      "Yuqiu Fu",
      "Renao Yan",
      "ShanShan Shi",
      "Xitong Ling",
      "Lianghui Zhu",
      "Yonghong He",
      "Tian Guan"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=MergeUp-augmented+Semi-Weakly+Supervised+Learning+for+WSI+Classification+Mingxi+Ouyang+Yuqiu+Fu+Renao+Yan+ShanShan+Shi+Xitong+Ling",
    "gs_search_success": true,
    "gs_authors": [
      "CPAya9gAAAAJ",
      "YU72pyQAAAAJ",
      "nDJI-9oAAAAJ",
      "5lNlpagAAAAJ"
    ],
    "citation_count": 8,
    "gs_author_count": 8
  },
  {
    "arxiv_id": "2407.14737",
    "title": "Early Detection of Coffee Leaf Rust Through Convolutional Neural Networks Trained on Low-Resolution Images",
    "year": 2024,
    "published": "2024-07-20T03:24:25Z",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "Coffee leaf rust, a foliar disease caused by the fungus Hemileia vastatrix, poses a major threat to coffee production, especially in Central America. Climate change further aggravates this issue, as it shortens the latency period between initial infection and the emergence of visible symptoms in diseases like leaf rust. Shortened latency periods can lead to more severe plant epidemics and faster spread of diseases. There is, hence, an urgent need for effective disease management strategies. To a",
    "arxiv_url": "https://arxiv.org/abs/2407.14737v1",
    "pdf_url": "https://arxiv.org/pdf/2407.14737v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2407.14737",
    "arxiv_authors": [
      "Angelly Cabrera",
      "Kleanthis Avramidis",
      "Shrikanth Narayanan"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Early+Detection+of+Coffee+Leaf+Rust+Through+Convolutional+Neural+Networks+Trained+on+Low-Resolution+Images+Angelly+Cabrera+Kleanthis+Avramidis+Shrikanth+Narayanan",
    "gs_search_success": true,
    "gs_authors": [
      "_u4VyUcAAAAJ",
      "8EDHmYkAAAAJ",
      "mxLN1rUAAAAJ"
    ],
    "citation_count": 5,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2407.01905",
    "title": "Enhancing Multi-Class Anomaly Detection via Diffusion Refinement with Dual Conditioning",
    "year": 2024,
    "published": "2024-07-02T03:09:40Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Anomaly detection, the technique of identifying abnormal samples using only normal samples, has attracted widespread interest in industry. Existing one-model-per-category methods often struggle with limited generalization capabilities due to their focus on a single category, and can fail when encountering variations in product. Recent feature reconstruction methods, as representatives in one-model-all-categories schemes, face challenges including reconstructing anomalous samples and blurry recon",
    "arxiv_url": "https://arxiv.org/abs/2407.01905v1",
    "pdf_url": "https://arxiv.org/pdf/2407.01905v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2407.01905",
    "arxiv_authors": [
      "Jiawei Zhan",
      "Jinxiang Lai",
      "Bin-Bin Gao",
      "Jun Liu",
      "Xiaochen Chen",
      "Chengjie Wang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Enhancing+Multi-Class+Anomaly+Detection+via+Diffusion+Refinement+with+Dual+Conditioning+Jiawei+Zhan+Jinxiang+Lai+Bin-Bin+Gao+Jun+Liu+Xiaochen+Chen",
    "gs_search_success": true,
    "gs_authors": [
      "JIKuf4AAAAAJ",
      "v9qtGm0AAAAJ",
      "yYviZ-oAAAAJ",
      "6jAXwlwAAAAJ",
      "fqte5H4AAAAJ"
    ],
    "citation_count": 3,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2410.04833",
    "title": "Multimodal Fusion Strategies for Mapping Biophysical Landscape Features",
    "year": 2024,
    "published": "2024-10-07T08:40:29Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "abstract": "Multimodal aerial data are used to monitor natural systems, and machine learning can significantly accelerate the classification of landscape features within such imagery to benefit ecology and conservation. It remains under-explored, however, how these multiple modalities ought to be fused in a deep learning model. As a step towards filling this gap, we study three strategies (Early fusion, Late fusion, and Mixture of Experts) for fusing thermal, RGB, and LiDAR imagery using a dataset of spatia",
    "arxiv_url": "https://arxiv.org/abs/2410.04833v1",
    "pdf_url": "https://arxiv.org/pdf/2410.04833v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2410.04833",
    "arxiv_authors": [
      "Lucia Gordon",
      "Nico Lang",
      "Catherine Ressijac",
      "Andrew Davies"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Multimodal+Fusion+Strategies+for+Mapping+Biophysical+Landscape+Features+Lucia+Gordon+Nico+Lang+Catherine+Ressijac+Andrew+Davies",
    "gs_search_success": true,
    "gs_authors": [
      "DNO7ibYAAAAJ",
      "8hFcK0cAAAAJ",
      "CSzf-xYAAAAJ",
      "bVZtKsQAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2305.14059",
    "title": "Accelerated Coordinate Encoding: Learning to Relocalize in Minutes using RGB and Poses",
    "year": 2023,
    "published": "2023-05-23T13:38:01Z",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "Learning-based visual relocalizers exhibit leading pose accuracy, but require hours or days of training. Since training needs to happen on each new scene again, long training times make learning-based relocalization impractical for most applications, despite its promise of high accuracy. In this paper we show how such a system can actually achieve the same accuracy in less than 5 minutes. We start from the obvious: a relocalization network can be split in a scene-agnostic feature backbone, and a",
    "arxiv_url": "https://arxiv.org/abs/2305.14059v1",
    "pdf_url": "https://arxiv.org/pdf/2305.14059v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2305.14059",
    "arxiv_authors": [
      "Eric Brachmann",
      "Tommaso Cavallari",
      "Victor Adrian Prisacariu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Accelerated+Coordinate+Encoding%3A+Learning+to+Relocalize+in+Minutes+using+RGB+and+Poses+Eric+Brachmann+Tommaso+Cavallari+Victor+Adrian+Prisacariu",
    "gs_search_success": true,
    "gs_authors": [
      "cAIshsYAAAAJ",
      "GmWA-LoAAAAJ",
      "r7osSm0AAAAJ"
    ],
    "citation_count": 116,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2403.05912",
    "title": "Mask-Enhanced Segment Anything Model for Tumor Lesion Semantic Segmentation",
    "year": 2024,
    "published": "2024-03-09T13:37:02Z",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "abstract": "Tumor lesion segmentation on CT or MRI images plays a critical role in cancer diagnosis and treatment planning. Considering the inherent differences in tumor lesion segmentation data across various medical imaging modalities and equipment, integrating medical knowledge into the Segment Anything Model (SAM) presents promising capability due to its versatility and generalization potential. Recent studies have attempted to enhance SAM with medical expertise by pre-training on large-scale medical se",
    "arxiv_url": "https://arxiv.org/abs/2403.05912v2",
    "pdf_url": "https://arxiv.org/pdf/2403.05912v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2403.05912",
    "arxiv_authors": [
      "Hairong Shi",
      "Songhao Han",
      "Shaofei Huang",
      "Yue Liao",
      "Guanbin Li",
      "Xiangxing Kong",
      "Hua Zhu",
      "Xiaomu Wang",
      "Si Liu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Mask-Enhanced+Segment+Anything+Model+for+Tumor+Lesion+Semantic+Segmentation+Hairong+Shi+Songhao+Han+Shaofei+Huang+Yue+Liao+Guanbin+Li",
    "gs_search_success": true,
    "gs_authors": [
      "mIt-3fEAAAAJ",
      "2A2Bx2UAAAAJ",
      "bgb9UpgAAAAJ",
      "s-exUYYAAAAJ",
      "hVbSuo0AAAAJ"
    ],
    "citation_count": 11,
    "gs_author_count": 9
  },
  {
    "arxiv_id": "2304.03135",
    "title": "VLPD: Context-Aware Pedestrian Detection via Vision-Language Semantic Self-Supervision",
    "year": 2023,
    "published": "2023-04-06T15:16:29Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.MM"
    ],
    "abstract": "Detecting pedestrians accurately in urban scenes is significant for realistic applications like autonomous driving or video surveillance. However, confusing human-like objects often lead to wrong detections, and small scale or heavily occluded pedestrians are easily missed due to their unusual appearances. To address these challenges, only object regions are inadequate, thus how to fully utilize more explicit and semantic contexts becomes a key problem. Meanwhile, previous context-aware pedestri",
    "arxiv_url": "https://arxiv.org/abs/2304.03135v1",
    "pdf_url": "https://arxiv.org/pdf/2304.03135v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2304.03135",
    "arxiv_authors": [
      "Mengyin Liu",
      "Jie Jiang",
      "Chao Zhu",
      "Xu-Cheng Yin"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=VLPD%3A+Context-Aware+Pedestrian+Detection+via+Vision-Language+Semantic+Self-Supervision+Mengyin+Liu+Jie+Jiang+Chao+Zhu+Xu-Cheng+Yin",
    "gs_search_success": true,
    "gs_authors": [
      "hN7koAYAAAAJ",
      "V3hiob0AAAAJ"
    ],
    "citation_count": 54,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2303.03645",
    "title": "Filter Pruning based on Information Capacity and Independence",
    "year": 2023,
    "published": "2023-03-07T04:26:44Z",
    "categories": [
      "cs.CV",
      "cs.CC"
    ],
    "abstract": "Filter pruning has gained widespread adoption for the purpose of compressing and speeding up convolutional neural networks (CNNs). However, existing approaches are still far from practical applications due to biased filter selection and heavy computation cost. This paper introduces a new filter pruning method that selects filters in an interpretable, multi-perspective, and lightweight manner. Specifically, we evaluate the contributions of filters from both individual and overall perspectives. Fo",
    "arxiv_url": "https://arxiv.org/abs/2303.03645v2",
    "pdf_url": "https://arxiv.org/pdf/2303.03645v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2303.03645",
    "arxiv_authors": [
      "Xiaolong Tang",
      "Shuo Ye",
      "Yufeng Shi",
      "Tianheng Hu",
      "Qinmu Peng",
      "Xinge You"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Filter+Pruning+based+on+Information+Capacity+and+Independence+Xiaolong+Tang+Shuo+Ye+Yufeng+Shi+Tianheng+Hu+Qinmu+Peng",
    "gs_search_success": true,
    "gs_authors": [
      "v7bRZX8AAAAJ",
      "frL8yhIAAAAJ",
      "rEaA30IAAAAJ",
      "EWN-IogAAAAJ"
    ],
    "citation_count": 5,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2404.09447",
    "title": "kNN-CLIP: Retrieval Enables Training-Free Segmentation on Continually Expanding Large Vocabularies",
    "year": 2024,
    "published": "2024-04-15T04:20:01Z",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "Continual segmentation has not yet tackled the challenge of improving open-vocabulary segmentation models with training data for accurate segmentation across large, continually expanding vocabularies. We discover that traditional continual training results in severe catastrophic forgetting, failing to outperform a zero-shot segmentation baseline. We introduce a novel training-free strategy, kNN-CLIP, which augments the model with a database of instance embeddings for semantic and panoptic segmen",
    "arxiv_url": "https://arxiv.org/abs/2404.09447v3",
    "pdf_url": "https://arxiv.org/pdf/2404.09447v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2404.09447",
    "arxiv_authors": [
      "Zhongrui Gui",
      "Shuyang Sun",
      "Runjia Li",
      "Jianhao Yuan",
      "Zhaochong An",
      "Karsten Roth",
      "Ameya Prabhu",
      "Philip Torr"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=kNN-CLIP%3A+Retrieval+Enables+Training-Free+Segmentation+on+Continually+Expanding+Large+Vocabularies+Zhongrui+Gui+Shuyang+Sun+Runjia+Li+Jianhao+Yuan+Zhaochong+An",
    "gs_search_success": true,
    "gs_authors": [
      "BUJPCegAAAAJ",
      "HRnLy5wAAAAJ",
      "kPxa2w0AAAAJ",
      "PoAvGRMAAAAJ",
      "93ZjIs0AAAAJ",
      "K21_pXcAAAAJ",
      "0kK7sSAAAAAJ",
      "Zu3Rwn8AAAAJ"
    ],
    "citation_count": 6,
    "gs_author_count": 8
  },
  {
    "arxiv_id": "2401.14831",
    "title": "The Machine Vision Iceberg Explained: Advancing Dynamic Testing by Considering Holistic Environmental Relations",
    "year": 2024,
    "published": "2024-01-26T12:59:26Z",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV",
      "cs.SE",
      "eess.IV"
    ],
    "abstract": "Machine Vision (MV) is essential for solving driving automation. This paper examines potential shortcomings in current MV testing strategies for highly automated driving (HAD) systems. We argue for a more comprehensive understanding of the performance factors that must be considered during the MV evaluation process, noting that neglecting these factors can lead to significant risks. This is not only relevant to MV component testing, but also to integration testing. To illustrate this point, we d",
    "arxiv_url": "https://arxiv.org/abs/2401.14831v3",
    "pdf_url": "https://arxiv.org/pdf/2401.14831v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2401.14831",
    "arxiv_authors": [
      "Hubert Padusinski",
      "Christian Steinhauser",
      "Thilo Braun",
      "Lennart Ries",
      "Eric Sax"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=The+Machine+Vision+Iceberg+Explained%3A+Advancing+Dynamic+Testing+by+Considering+Holistic+Environmental+Relations+Hubert+Padusinski+Christian+Steinhauser+Thilo+Braun+Lennart+Ries+Eric+Sax",
    "gs_search_success": true,
    "gs_authors": [
      "aRP8-2AAAAAJ",
      "q-6Bbm0AAAAJ",
      "R0av4HEAAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2303.11327",
    "title": "3D Concept Learning and Reasoning from Multi-View Images",
    "year": 2023,
    "published": "2023-03-20T17:59:49Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "cs.RO"
    ],
    "abstract": "Humans are able to accurately reason in 3D by gathering multi-view observations of the surrounding world. Inspired by this insight, we introduce a new large-scale benchmark for 3D multi-view visual question answering (3DMV-VQA). This dataset is collected by an embodied agent actively moving and capturing RGB images in an environment using the Habitat simulator. In total, it consists of approximately 5k scenes, 600k images, paired with 50k questions. We evaluate various state-of-the-art models fo",
    "arxiv_url": "https://arxiv.org/abs/2303.11327v1",
    "pdf_url": "https://arxiv.org/pdf/2303.11327v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2303.11327",
    "arxiv_authors": [
      "Yining Hong",
      "Chunru Lin",
      "Yilun Du",
      "Zhenfang Chen",
      "Joshua B. Tenenbaum",
      "Chuang Gan"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=3D+Concept+Learning+and+Reasoning+from+Multi-View+Images+Yining+Hong+Chunru+Lin+Yilun+Du+Zhenfang+Chen+Joshua+B.+Tenenbaum",
    "gs_search_success": true,
    "gs_authors": [
      "GRMMc_MAAAAJ",
      "QSRdIzAAAAAJ",
      "PTeSCbIAAAAJ",
      "PTYVWdIAAAAJ",
      "rRJ9wTJMUB8C",
      "PTYxORcAAAAJ"
    ],
    "citation_count": 82,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2406.05271",
    "title": "USE: Universal Segment Embeddings for Open-Vocabulary Image Segmentation",
    "year": 2024,
    "published": "2024-06-07T21:41:18Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "The open-vocabulary image segmentation task involves partitioning images into semantically meaningful segments and classifying them with flexible text-defined categories. The recent vision-based foundation models such as the Segment Anything Model (SAM) have shown superior performance in generating class-agnostic image segments. The main challenge in open-vocabulary image segmentation now lies in accurately classifying these segments into text-defined categories. In this paper, we introduce the ",
    "arxiv_url": "https://arxiv.org/abs/2406.05271v1",
    "pdf_url": "https://arxiv.org/pdf/2406.05271v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2406.05271",
    "arxiv_authors": [
      "Xiaoqi Wang",
      "Wenbin He",
      "Xiwei Xuan",
      "Clint Sebastian",
      "Jorge Piazentin Ono",
      "Xin Li",
      "Sima Behpour",
      "Thang Doan",
      "Liang Gou",
      "Han Wei Shen",
      "Liu Ren"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=USE%3A+Universal+Segment+Embeddings+for+Open-Vocabulary+Image+Segmentation+Xiaoqi+Wang+Wenbin+He+Xiwei+Xuan+Clint+Sebastian+Jorge+Piazentin+Ono",
    "gs_search_success": true,
    "gs_authors": [
      "95Z6-isAAAAJ",
      "UYO_Vj4AAAAJ",
      "x3VK0fAAAAAJ",
      "FzH85ygAAAAJ",
      "h21TM74AAAAJ",
      "CFZ9Kp4AAAAJ",
      "KkPdvB8AAAAJ",
      "REeuDFAAAAAJ",
      "BQG5angAAAAJ",
      "i__pLDEAAAAJ"
    ],
    "citation_count": 15,
    "gs_author_count": 10
  },
  {
    "arxiv_id": "2409.18694",
    "title": "Learning from Pattern Completion: Self-supervised Controllable Generation",
    "year": 2024,
    "published": "2024-09-27T12:28:47Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "The human brain exhibits a strong ability to spontaneously associate different visual attributes of the same or similar visual scene, such as associating sketches and graffiti with real-world visual objects, usually without supervising information. In contrast, in the field of artificial intelligence, controllable generation methods like ControlNet heavily rely on annotated training datasets such as depth maps, semantic segmentation maps, and poses, which limits the method's scalability. Inspire",
    "arxiv_url": "https://arxiv.org/abs/2409.18694v2",
    "pdf_url": "https://arxiv.org/pdf/2409.18694v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2409.18694",
    "arxiv_authors": [
      "Zhiqiang Chen",
      "Guofan Fan",
      "Jinying Gao",
      "Lei Ma",
      "Bo Lei",
      "Tiejun Huang",
      "Shan Yu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Learning+from+Pattern+Completion%3A+Self-supervised+Controllable+Generation+Zhiqiang+Chen+Guofan+Fan+Jinying+Gao+Lei+Ma+Bo+Lei",
    "gs_search_success": true,
    "gs_authors": [
      "91CwQH4AAAAJ",
      "BfvjfkwAAAAJ",
      "mJwm3PUAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2504.02382",
    "title": "Benchmark of Segmentation Techniques for Pelvic Fracture in CT and X-ray: Summary of the PENGWIN 2024 Challenge",
    "year": 2025,
    "published": "2025-04-03T08:19:36Z",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "abstract": "The segmentation of pelvic fracture fragments in CT and X-ray images is crucial for trauma diagnosis, surgical planning, and intraoperative guidance. However, accurately and efficiently delineating the bone fragments remains a significant challenge due to complex anatomy and imaging limitations. The PENGWIN challenge, organized as a MICCAI 2024 satellite event, aimed to advance automated fracture segmentation by benchmarking state-of-the-art algorithms on these complex tasks. A diverse dataset o",
    "arxiv_url": "https://arxiv.org/abs/2504.02382v1",
    "pdf_url": "https://arxiv.org/pdf/2504.02382v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2504.02382",
    "arxiv_authors": [
      "Yudi Sang",
      "Yanzhen Liu",
      "Sutuke Yibulayimu",
      "Yunning Wang",
      "Benjamin D. Killeen",
      "Mingxu Liu",
      "Ping-Cheng Ku",
      "Ole Johannsen",
      "Karol Gotkowski",
      "Maximilian Zenk",
      "Klaus Maier-Hein",
      "Fabian Isensee",
      "Peiyan Yue",
      "Yi Wang",
      "Haidong Yu",
      "Zhaohong Pan",
      "Yutong He",
      "Xiaokun Liang",
      "Daiqi Liu",
      "Fuxin Fan",
      "Artur Jurgas",
      "Andrzej Skalski",
      "Yuxi Ma",
      "Jing Yang",
      "Szymon Płotka",
      "Rafał Litka",
      "Gang Zhu",
      "Yingchun Song",
      "Mathias Unberath",
      "Mehran Armand",
      "Dan Ruan",
      "S. Kevin Zhou",
      "Qiyong Cao",
      "Chunpeng Zhao",
      "Xinbao Wu",
      "Yu Wang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Benchmark+of+Segmentation+Techniques+for+Pelvic+Fracture+in+CT+and+X-ray%3A+Summary+of+the+PENGWIN+2024+Challenge+Yudi+Sang+Yanzhen+Liu+Sutuke+Yibulayimu+Yunning+Wang+Benjamin+D.+Killeen",
    "gs_search_success": true,
    "gs_authors": [
      "9Cs2LQEAAAAJ",
      "QX7AvxUAAAAJ",
      "66y18QsAAAAJ",
      "emTKlqUAAAAJ",
      "4wSUzoMAAAAJ",
      "I7lsomcAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 9
  },
  {
    "arxiv_id": "2407.14170",
    "title": "Forbes: Face Obfuscation Rendering via Backpropagation Refinement Scheme",
    "year": 2024,
    "published": "2024-07-19T09:58:00Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "A novel algorithm for face obfuscation, called Forbes, which aims to obfuscate facial appearance recognizable by humans but preserve the identity and attributes decipherable by machines, is proposed in this paper. Forbes first applies multiple obfuscating transformations with random parameters to an image to remove the identity information distinguishable by humans. Then, it optimizes the parameters to make the transformed image decipherable by machines based on the backpropagation refinement sc",
    "arxiv_url": "https://arxiv.org/abs/2407.14170v1",
    "pdf_url": "https://arxiv.org/pdf/2407.14170v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2407.14170",
    "arxiv_authors": [
      "Jintae Kim",
      "Seungwon yang",
      "Seong-Gyun Jeong",
      "Chang-Su Kim"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Forbes%3A+Face+Obfuscation+Rendering+via+Backpropagation+Refinement+Scheme+Jintae+Kim+Seungwon+yang+Seong-Gyun+Jeong+Chang-Su+Kim",
    "gs_search_success": true,
    "gs_authors": [
      "lCBVDWwAAAAJ",
      "uFUMmSoAAAAJ",
      "xv7OKWkAAAAJ",
      "KOdKwNsAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2306.08366",
    "title": "SaliencyCut: Augmenting Plausible Anomalies for Anomaly Detection",
    "year": 2023,
    "published": "2023-06-14T08:55:36Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Anomaly detection under open-set scenario is a challenging task that requires learning discriminative fine-grained features to detect anomalies that were even unseen during training. As a cheap yet effective approach, data augmentation has been widely used to create pseudo anomalies for better training of such models. Recent wisdom of augmentation methods focuses on generating random pseudo instances that may lead to a mixture of augmented instances with seen anomalies, or out of the typical ran",
    "arxiv_url": "https://arxiv.org/abs/2306.08366v2",
    "pdf_url": "https://arxiv.org/pdf/2306.08366v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2306.08366",
    "arxiv_authors": [
      "Jianan Ye",
      "Yijie Hu",
      "Xi Yang",
      "Qiu-Feng Wang",
      "Chao Huang",
      "Kaizhu Huang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=SaliencyCut%3A+Augmenting+Plausible+Anomalies+for+Anomaly+Detection+Jianan+Ye+Yijie+Hu+Xi+Yang+Qiu-Feng+Wang+Chao+Huang",
    "gs_search_success": true,
    "gs_authors": [
      "z5houZAAAAAJ",
      "A0Z8mKIAAAAJ",
      "__hrr0YAAAAJ",
      "qj66yXAAAAAJ",
      "3l5B0joAAAAJ",
      "8u0GpekAAAAJ"
    ],
    "citation_count": 11,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2308.10755",
    "title": "WanJuan: A Comprehensive Multimodal Dataset for Advancing English and Chinese Large Models",
    "year": 2023,
    "published": "2023-08-21T14:40:48Z",
    "categories": [
      "cs.CL",
      "cs.CV"
    ],
    "abstract": "The rise in popularity of ChatGPT and GPT-4 has significantly accelerated the development of large models, leading to the creation of numerous impressive large language models(LLMs) and multimodal large language models (MLLMs). These cutting-edge models owe their remarkable performance to high-quality data. However, the details of the training data used in leading paradigms are often kept confidential. This lack of transparency, coupled with the scarcity of open-source data, impedes further deve",
    "arxiv_url": "https://arxiv.org/abs/2308.10755v3",
    "pdf_url": "https://arxiv.org/pdf/2308.10755v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2308.10755",
    "arxiv_authors": [
      "Conghui He",
      "Zhenjiang Jin",
      "Chao Xu",
      "Jiantao Qiu",
      "Bin Wang",
      "Wei Li",
      "Hang Yan",
      "Jiaqi Wang",
      "Dahua Lin"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=WanJuan%3A+A+Comprehensive+Multimodal+Dataset+for+Advancing+English+and+Chinese+Large+Models+Conghui+He+Zhenjiang+Jin+Chao+Xu+Jiantao+Qiu+Bin+Wang",
    "gs_search_success": true,
    "gs_authors": [
      "U0UKdakAAAAJ",
      "yigHzW8AAAAJ",
      "WljXYoYAAAAJ",
      "GDvt570AAAAJ",
      "3J8OquAAAAAJ",
      "GMzzRRUAAAAJ",
      "PopTv7kAAAAJ"
    ],
    "citation_count": 56,
    "gs_author_count": 9
  },
  {
    "arxiv_id": "2307.13855",
    "title": "Exploring the Sharpened Cosine Similarity",
    "year": 2023,
    "published": "2023-07-25T23:02:35Z",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "Convolutional layers have long served as the primary workhorse for image classification. Recently, an alternative to convolution was proposed using the Sharpened Cosine Similarity (SCS), which in theory may serve as a better feature detector. While multiple sources report promising results, there has not been to date a full-scale empirical analysis of neural network performance using these new layers. In our work, we explore SCS's parameter behavior and potential as a drop-in replacement for con",
    "arxiv_url": "https://arxiv.org/abs/2307.13855v1",
    "pdf_url": "https://arxiv.org/pdf/2307.13855v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2307.13855",
    "arxiv_authors": [
      "Skyler Wu",
      "Fred Lu",
      "Edward Raff",
      "James Holt"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Exploring+the+Sharpened+Cosine+Similarity+Skyler+Wu+Fred+Lu+Edward+Raff+James+Holt",
    "gs_search_success": true,
    "gs_authors": [
      "GtVgGjkAAAAJ",
      "8BjErXQAAAAJ",
      "tpguLYkAAAAJ",
      "debM2bUAAAAJ"
    ],
    "citation_count": 6,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2312.03517",
    "title": "FRDiff : Feature Reuse for Universal Training-free Acceleration of Diffusion Models",
    "year": 2023,
    "published": "2023-12-06T14:24:26Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "The substantial computational costs of diffusion models, especially due to the repeated denoising steps necessary for high-quality image generation, present a major obstacle to their widespread adoption. While several studies have attempted to address this issue by reducing the number of score function evaluations (NFE) using advanced ODE solvers without fine-tuning, the decreased number of denoising iterations misses the opportunity to update fine details, resulting in noticeable quality degrad",
    "arxiv_url": "https://arxiv.org/abs/2312.03517v3",
    "pdf_url": "https://arxiv.org/pdf/2312.03517v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2312.03517",
    "arxiv_authors": [
      "Junhyuk So",
      "Jungwon Lee",
      "Eunhyeok Park"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=FRDiff+%3A+Feature+Reuse+for+Universal+Training-free+Acceleration+of+Diffusion+Models+Junhyuk+So+Jungwon+Lee+Eunhyeok+Park",
    "gs_search_success": true,
    "gs_authors": [
      "uYE7vtEAAAAJ",
      "TbzDLMUAAAAJ",
      "pBr1GV4AAAAJ"
    ],
    "citation_count": 12,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2312.12444",
    "title": "What Makes Pre-Trained Visual Representations Successful for Robust Manipulation?",
    "year": 2023,
    "published": "2023-11-03T18:09:08Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.RO"
    ],
    "abstract": "Inspired by the success of transfer learning in computer vision, roboticists have investigated visual pre-training as a means to improve the learning efficiency and generalization ability of policies learned from pixels. To that end, past work has favored large object interaction datasets, such as first-person videos of humans completing diverse tasks, in pursuit of manipulation-relevant features. Although this approach improves the efficiency of policy learning, it remains unclear how reliable ",
    "arxiv_url": "https://arxiv.org/abs/2312.12444v1",
    "pdf_url": "https://arxiv.org/pdf/2312.12444v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2312.12444",
    "arxiv_authors": [
      "Kaylee Burns",
      "Zach Witzel",
      "Jubayer Ibn Hamid",
      "Tianhe Yu",
      "Chelsea Finn",
      "Karol Hausman"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=What+Makes+Pre-Trained+Visual+Representations+Successful+for+Robust+Manipulation%3F+Kaylee+Burns+Zach+Witzel+Jubayer+Ibn+Hamid+Tianhe+Yu+Chelsea+Finn",
    "gs_search_success": true,
    "gs_authors": [
      "vfPE6hgAAAAJ",
      "5VaXUQsAAAAJ",
      "3G2EbP4AAAAJ",
      "yy0UFOwAAAAJ",
      "N_rVVG8AAAAJ"
    ],
    "citation_count": 26,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2307.02595",
    "title": "GNEP Based Dynamic Segmentation and Motion Estimation for Neuromorphic Imaging",
    "year": 2023,
    "published": "2023-07-05T18:44:51Z",
    "categories": [
      "cs.CV",
      "cs.GT",
      "math.OC"
    ],
    "abstract": "This paper explores the application of event-based cameras in the domains of image segmentation and motion estimation. These cameras offer a groundbreaking technology by capturing visual information as a continuous stream of asynchronous events, departing from the conventional frame-based image acquisition. We introduce a Generalized Nash Equilibrium based framework that leverages the temporal and spatial information derived from the event stream to carry out segmentation and velocity estimation",
    "arxiv_url": "https://arxiv.org/abs/2307.02595v2",
    "pdf_url": "https://arxiv.org/pdf/2307.02595v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2307.02595",
    "arxiv_authors": [
      "Harbir Antil",
      "David Sayre"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=GNEP+Based+Dynamic+Segmentation+and+Motion+Estimation+for+Neuromorphic+Imaging+Harbir+Antil+David+Sayre",
    "gs_search_success": true,
    "gs_authors": [
      "Cu79_KcAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 2
  },
  {
    "arxiv_id": "2307.12239",
    "title": "Learning Dynamic Query Combinations for Transformer-based Object Detection and Segmentation",
    "year": 2023,
    "published": "2023-07-23T06:26:27Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Transformer-based detection and segmentation methods use a list of learned detection queries to retrieve information from the transformer network and learn to predict the location and category of one specific object from each query. We empirically find that random convex combinations of the learned queries are still good for the corresponding models. We then propose to learn a convex combination with dynamic coefficients based on the high-level semantics of the image. The generated dynamic queri",
    "arxiv_url": "https://arxiv.org/abs/2307.12239v2",
    "pdf_url": "https://arxiv.org/pdf/2307.12239v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2307.12239",
    "arxiv_authors": [
      "Yiming Cui",
      "Linjie Yang",
      "Haichao Yu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Learning+Dynamic+Query+Combinations+for+Transformer-based+Object+Detection+and+Segmentation+Yiming+Cui+Linjie+Yang+Haichao+Yu",
    "gs_search_success": true,
    "gs_authors": [
      "6vBNzOsAAAAJ",
      "XptEO8oAAAAJ"
    ],
    "citation_count": 17,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2407.07755",
    "title": "Neural Geometry Processing via Spherical Neural Surfaces",
    "year": 2024,
    "published": "2024-07-10T15:28:02Z",
    "categories": [
      "cs.GR",
      "cs.AI",
      "cs.CV"
    ],
    "abstract": "Neural surfaces (e.g., neural map encoding, deep implicits and neural radiance fields) have recently gained popularity because of their generic structure (e.g., multi-layer perceptron) and easy integration with modern learning-based setups. Traditionally, we have a rich toolbox of geometry processing algorithms designed for polygonal meshes to analyze and operate on surface geometry. In the absence of an analogous toolbox, neural representations are typically discretized and converted into a mes",
    "arxiv_url": "https://arxiv.org/abs/2407.07755v3",
    "pdf_url": "https://arxiv.org/pdf/2407.07755v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2407.07755",
    "arxiv_authors": [
      "Romy Williamson",
      "Niloy J. Mitra"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Neural+Geometry+Processing+via+Spherical+Neural+Surfaces+Romy+Williamson+Niloy+J.+Mitra",
    "gs_search_success": true,
    "gs_authors": [
      "dPrZJWMAAAAJ"
    ],
    "citation_count": 5,
    "gs_author_count": 1
  },
  {
    "arxiv_id": "2501.14885",
    "title": "Hybrid Interpretable Deep Learning Framework for Skin Cancer Diagnosis: Integrating Radial Basis Function Networks with Explainable AI",
    "year": 2025,
    "published": "2025-01-24T19:19:02Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Skin cancer is one of the most prevalent and potentially life-threatening diseases worldwide, necessitating early and accurate diagnosis to improve patient outcomes. Conventional diagnostic methods, reliant on clinical expertise and histopathological analysis, are often time-intensive, subjective, and prone to variability. To address these limitations, we propose a novel hybrid deep learning framework that integrates convolutional neural networks (CNNs) with Radial Basis Function (RBF) Networks ",
    "arxiv_url": "https://arxiv.org/abs/2501.14885v1",
    "pdf_url": "https://arxiv.org/pdf/2501.14885v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2501.14885",
    "arxiv_authors": [
      "Mirza Ahsan Ullah",
      "Tehseen Zia"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Hybrid+Interpretable+Deep+Learning+Framework+for+Skin+Cancer+Diagnosis%3A+Integrating+Radial+Basis+Function+Networks+with+Explainable+AI+Mirza+Ahsan+Ullah+Tehseen+Zia",
    "gs_search_success": false,
    "gs_authors": [],
    "citation_count": null,
    "gs_author_count": null,
    "error_type": "fetch_failed"
  },
  {
    "arxiv_id": "2402.05568",
    "title": "Neural Graphics Primitives-based Deformable Image Registration for On-the-fly Motion Extraction",
    "year": 2024,
    "published": "2024-02-08T11:09:27Z",
    "categories": [
      "physics.med-ph",
      "cs.CV"
    ],
    "abstract": "Intra-fraction motion in radiotherapy is commonly modeled using deformable image registration (DIR). However, existing methods often struggle to balance speed and accuracy, limiting their applicability in clinical scenarios. This study introduces a novel approach that harnesses Neural Graphics Primitives (NGP) to optimize the displacement vector field (DVF). Our method leverages learned primitives, processed as splats, and interpolates within space using a shallow neural network. Uniquely, it en",
    "arxiv_url": "https://arxiv.org/abs/2402.05568v1",
    "pdf_url": "https://arxiv.org/pdf/2402.05568v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2402.05568",
    "arxiv_authors": [
      "Xia Li",
      "Fabian Zhang",
      "Muheng Li",
      "Damien Weber",
      "Antony Lomax",
      "Joachim Buhmann",
      "Ye Zhang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Neural+Graphics+Primitives-based+Deformable+Image+Registration+for+On-the-fly+Motion+Extraction+Xia+Li+Fabian+Zhang+Muheng+Li+Damien+Weber+Antony+Lomax",
    "gs_search_success": true,
    "gs_authors": [
      "XKGZhEcAAAAJ",
      "CcTr_ZQAAAAJ",
      "zQWbCzYAAAAJ",
      "FC5zGu4AAAAJ",
      "N0aisycAAAAJ",
      "NZz1l-YAAAAJ",
      "U3J4L6IAAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 7
  },
  {
    "arxiv_id": "2401.11228",
    "title": "Unifying Visual and Vision-Language Tracking via Contrastive Learning",
    "year": 2024,
    "published": "2024-01-20T13:20:54Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Single object tracking aims to locate the target object in a video sequence according to the state specified by different modal references, including the initial bounding box (BBOX), natural language (NL), or both (NL+BBOX). Due to the gap between different modalities, most existing trackers are designed for single or partial of these reference settings and overspecialize on the specific modality. Differently, we present a unified tracker called UVLTrack, which can simultaneously handle all thre",
    "arxiv_url": "https://arxiv.org/abs/2401.11228v1",
    "pdf_url": "https://arxiv.org/pdf/2401.11228v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2401.11228",
    "arxiv_authors": [
      "Yinchao Ma",
      "Yuyang Tang",
      "Wenfei Yang",
      "Tianzhu Zhang",
      "Jinpeng Zhang",
      "Mengxue Kang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Unifying+Visual+and+Vision-Language+Tracking+via+Contrastive+Learning+Yinchao+Ma+Yuyang+Tang+Wenfei+Yang+Tianzhu+Zhang+Jinpeng+Zhang",
    "gs_search_success": true,
    "gs_authors": [
      "9sCGe-gAAAAJ",
      "9K1_K1wAAAAJ",
      "rtO5VmQAAAAJ",
      "XFLCbj0AAAAJ"
    ],
    "citation_count": 58,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2305.07161",
    "title": "A Deep Learning-based Compression and Classification Technique for Whole Slide Histopathology Images",
    "year": 2023,
    "published": "2023-05-11T22:20:05Z",
    "categories": [
      "eess.IV",
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "This paper presents an autoencoder-based neural network architecture to compress histopathological images while retaining the denser and more meaningful representation of the original images. Current research into improving compression algorithms is focused on methods allowing lower compression rates for Regions of Interest (ROI-based approaches). Neural networks are great at extracting meaningful semantic representations from images, therefore are able to select the regions to be considered of ",
    "arxiv_url": "https://arxiv.org/abs/2305.07161v1",
    "pdf_url": "https://arxiv.org/pdf/2305.07161v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2305.07161",
    "arxiv_authors": [
      "Agnes Barsi",
      "Suvendu Chandan Nayak",
      "Sasmita Parida",
      "Raj Mani Shukla"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=A+Deep+Learning-based+Compression+and+Classification+Technique+for+Whole+Slide+Histopathology+Images+Agnes+Barsi+Suvendu+Chandan+Nayak+Sasmita+Parida+Raj+Mani+Shukla",
    "gs_search_success": true,
    "gs_authors": [
      "8wPHUpYAAAAJ",
      "VVp74ZMAAAAJ",
      "bQ3XdtYAAAAJ"
    ],
    "citation_count": 18,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2302.00624",
    "title": "Open-VCLIP: Transforming CLIP to an Open-vocabulary Video Model via Interpolated Weight Optimization",
    "year": 2023,
    "published": "2023-02-01T17:44:17Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Contrastive Language-Image Pretraining (CLIP) has demonstrated impressive zero-shot learning abilities for image understanding, yet limited effort has been made to investigate CLIP for zero-shot video recognition. We introduce Open-VCLIP, a simple yet effective approach that transforms CLIP into a strong zero-shot video classifier that can recognize unseen actions and events at test time. Our framework extends CLIP with minimal modifications to model spatial-temporal relationships in videos, mak",
    "arxiv_url": "https://arxiv.org/abs/2302.00624v3",
    "pdf_url": "https://arxiv.org/pdf/2302.00624v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2302.00624",
    "arxiv_authors": [
      "Zejia Weng",
      "Xitong Yang",
      "Ang Li",
      "Zuxuan Wu",
      "Yu-Gang Jiang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Open-VCLIP%3A+Transforming+CLIP+to+an+Open-vocabulary+Video+Model+via+Interpolated+Weight+Optimization+Zejia+Weng+Xitong+Yang+Ang+Li+Zuxuan+Wu+Yu-Gang+Jiang",
    "gs_search_success": true,
    "gs_authors": [
      "f3_FP8AAAAAJ",
      "7t12hVkAAAAJ",
      "6bRXWXEAAAAJ",
      "qMT0sqAAAAAJ",
      "k0qC-7AAAAAJ"
    ],
    "citation_count": 81,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2305.17033",
    "title": "The Brain Tumor Segmentation (BraTS) Challenge 2023: Focus on Pediatrics (CBTN-CONNECT-DIPGR-ASNR-MICCAI BraTS-PEDs)",
    "year": 2023,
    "published": "2023-05-26T15:40:11Z",
    "categories": [
      "eess.IV",
      "cs.CV",
      "cs.LG",
      "q-bio.QM"
    ],
    "abstract": "Pediatric tumors of the central nervous system are the most common cause of cancer-related death in children. The five-year survival rate for high-grade gliomas in children is less than 20\\%. Due to their rarity, the diagnosis of these entities is often delayed, their treatment is mainly based on historic treatment concepts, and clinical trials require multi-institutional collaborations. The MICCAI Brain Tumor Segmentation (BraTS) Challenge is a landmark community benchmark event with a successf",
    "arxiv_url": "https://arxiv.org/abs/2305.17033v7",
    "pdf_url": "https://arxiv.org/pdf/2305.17033v7",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2305.17033",
    "arxiv_authors": [
      "Anahita Fathi Kazerooni",
      "Nastaran Khalili",
      "Xinyang Liu",
      "Debanjan Haldar",
      "Zhifan Jiang",
      "Syed Muhammed Anwar",
      "Jake Albrecht",
      "Maruf Adewole",
      "Udunna Anazodo",
      "Hannah Anderson",
      "Sina Bagheri",
      "Ujjwal Baid",
      "Timothy Bergquist",
      "Austin J. Borja",
      "Evan Calabrese",
      "Verena Chung",
      "Gian-Marco Conte",
      "Farouk Dako",
      "James Eddy",
      "Ivan Ezhov",
      "Ariana Familiar",
      "Keyvan Farahani",
      "Shuvanjan Haldar",
      "Juan Eugenio Iglesias",
      "Anastasia Janas",
      "Elaine Johansen",
      "Blaise V Jones",
      "Florian Kofler",
      "Dominic LaBella",
      "Hollie Anne Lai",
      "Koen Van Leemput",
      "Hongwei Bran Li",
      "Nazanin Maleki",
      "Aaron S McAllister",
      "Zeke Meier",
      "Bjoern Menze",
      "Ahmed W Moawad",
      "Khanak K Nandolia",
      "Julija Pavaine",
      "Marie Piraud",
      "Tina Poussaint",
      "Sanjay P Prabhu",
      "Zachary Reitman",
      "Andres Rodriguez",
      "Jeffrey D Rudie",
      "Mariana Sanchez-Montano",
      "Ibraheem Salman Shaikh",
      "Lubdha M. Shah",
      "Nakul Sheth",
      "Russel Taki Shinohara",
      "Wenxin Tu",
      "Karthik Viswanathan",
      "Chunhao Wang",
      "Jeffrey B Ware",
      "Benedikt Wiestler",
      "Walter Wiggins",
      "Anna Zapaishchykova",
      "Mariam Aboian",
      "Miriam Bornhorst",
      "Peter de Blank",
      "Michelle Deutsch",
      "Maryam Fouladi",
      "Lindsey Hoffman",
      "Benjamin Kann",
      "Margot Lazow",
      "Leonie Mikael",
      "Ali Nabavizadeh",
      "Roger Packer",
      "Adam Resnick",
      "Brian Rood",
      "Arastoo Vossough",
      "Spyridon Bakas",
      "Marius George Linguraru"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=The+Brain+Tumor+Segmentation+%28BraTS%29+Challenge+2023%3A+Focus+on+Pediatrics+%28CBTN-CONNECT-DIPGR-ASNR-MICCAI+BraTS-PEDs%29+Anahita+Fathi+Kazerooni+Nastaran+Khalili+Xinyang+Liu+Debanjan+Haldar+Zhifan+Jiang",
    "gs_search_success": true,
    "gs_authors": [
      "bIyGEdMAAAAJ",
      "OItbWycAAAAJ",
      "ISjUim0AAAAJ",
      "bOnw_44AAAAJ",
      "X6ponrMAAAAJ"
    ],
    "citation_count": 122,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2308.09268",
    "title": "Progression-Guided Temporal Action Detection in Videos",
    "year": 2023,
    "published": "2023-08-18T03:14:05Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "We present a novel framework, Action Progression Network (APN), for temporal action detection (TAD) in videos. The framework locates actions in videos by detecting the action evolution process. To encode the action evolution, we quantify a complete action process into 101 ordered stages (0\\%, 1\\%, ..., 100\\%), referred to as action progressions. We then train a neural network to recognize the action progressions. The framework detects action boundaries by detecting complete action processes in t",
    "arxiv_url": "https://arxiv.org/abs/2308.09268v1",
    "pdf_url": "https://arxiv.org/pdf/2308.09268v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2308.09268",
    "arxiv_authors": [
      "Chongkai Lu",
      "Man-Wai Mak",
      "Ruimin Li",
      "Zheru Chi",
      "Hong Fu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Progression-Guided+Temporal+Action+Detection+in+Videos+Chongkai+Lu+Man-Wai+Mak+Ruimin+Li+Zheru+Chi+Hong+Fu",
    "gs_search_success": true,
    "gs_authors": [
      "iXuY9CgAAAAJ",
      "Q2nNMycAAAAJ",
      "MkwdOHIAAAAJ",
      "XCVE6ukAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2409.01541",
    "title": "Agentic Copyright Watermarking against Adversarial Evidence Forgery with Purification-Agnostic Curriculum Proxy Learning",
    "year": 2024,
    "published": "2024-09-03T02:18:45Z",
    "categories": [
      "cs.CV",
      "cs.CR"
    ],
    "abstract": "With the proliferation of AI agents in various domains, protecting the ownership of AI models has become crucial due to the significant investment in their development. Unauthorized use and illegal distribution of these models pose serious threats to intellectual property, necessitating effective copyright protection measures. Model watermarking has emerged as a key technique to address this issue, embedding ownership information within models to assert rightful ownership during copyright disput",
    "arxiv_url": "https://arxiv.org/abs/2409.01541v2",
    "pdf_url": "https://arxiv.org/pdf/2409.01541v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2409.01541",
    "arxiv_authors": [
      "Erjin Bao",
      "Ching-Chun Chang",
      "Hanrui Wang",
      "Isao Echizen"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Agentic+Copyright+Watermarking+against+Adversarial+Evidence+Forgery+with+Purification-Agnostic+Curriculum+Proxy+Learning+Erjin+Bao+Ching-Chun+Chang+Hanrui+Wang+Isao+Echizen",
    "gs_search_success": true,
    "gs_authors": [
      "P-tAbagAAAAJ",
      "1XTseJEAAAAJ",
      "RduUnVIAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2301.02241",
    "title": "CiT: Curation in Training for Effective Vision-Language Data",
    "year": 2023,
    "published": "2023-01-05T18:59:57Z",
    "categories": [
      "cs.CV",
      "cs.CL"
    ],
    "abstract": "Large vision-language models are generally applicable to many downstream tasks, but come at an exorbitant training cost that only large institutions can afford. This paper trades generality for efficiency and presents Curation in Training (CiT), a simple and efficient vision-text learning algorithm that couples a data objective into training. CiT automatically yields quality data to speed-up contrastive image-text training and alleviates the need for an offline data filtering pipeline, allowing ",
    "arxiv_url": "https://arxiv.org/abs/2301.02241v1",
    "pdf_url": "https://arxiv.org/pdf/2301.02241v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2301.02241",
    "arxiv_authors": [
      "Hu Xu",
      "Saining Xie",
      "Po-Yao Huang",
      "Licheng Yu",
      "Russell Howes",
      "Gargi Ghosh",
      "Luke Zettlemoyer",
      "Christoph Feichtenhofer"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=CiT%3A+Curation+in+Training+for+Effective+Vision-Language+Data+Hu+Xu+Saining+Xie+Po-Yao+Huang+Licheng+Yu+Russell+Howes",
    "gs_search_success": true,
    "gs_authors": [
      "E8K25LIAAAAJ",
      "k5akwCcAAAAJ",
      "UjpbO6IAAAAJ",
      "76IWQk8AAAAJ",
      "Y2GtJkAAAAAJ",
      "SaH2yWMAAAAJ",
      "UxuqG1EAAAAJ"
    ],
    "citation_count": 33,
    "gs_author_count": 8
  },
  {
    "arxiv_id": "2306.11891",
    "title": "VitalVideos-Europe: A dataset of face videos with PPG and blood pressure ground truths",
    "year": 2023,
    "published": "2023-06-02T17:47:29Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "We collected a large dataset consisting of 850 unique participants. For every participant we recorded two 30 second uncompressed videos, synchronized PPG waveforms and a single blood pressure measurement. Gender, age and skin color were also registered for every participant. The dataset includes roughly equal numbers of males and females, as well as participants of all ages. While the skin color distribution could have been more balanced, the dataset contains individuals from every skin color. T",
    "arxiv_url": "https://arxiv.org/abs/2306.11891v3",
    "pdf_url": "https://arxiv.org/pdf/2306.11891v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2306.11891",
    "arxiv_authors": [
      "Pieter-Jan Toye"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=VitalVideos-Europe%3A+A+dataset+of+face+videos+with+PPG+and+blood+pressure+ground+truths+Pieter-Jan+Toye",
    "gs_search_success": true,
    "gs_authors": [],
    "citation_count": 2,
    "gs_author_count": 1
  },
  {
    "arxiv_id": "2410.17610",
    "title": "ImDy: Human Inverse Dynamics from Imitated Observations",
    "year": 2024,
    "published": "2024-10-23T07:06:08Z",
    "categories": [
      "cs.AI",
      "cs.CV",
      "cs.GR",
      "cs.RO"
    ],
    "abstract": "Inverse dynamics (ID), which aims at reproducing the driven torques from human kinematic observations, has been a critical tool for gait analysis. However, it is hindered from wider application to general motion due to its limited scalability. Conventional optimization-based ID requires expensive laboratory setups, restricting its availability. To alleviate this problem, we propose to exploit the recently progressive human motion imitation algorithms to learn human inverse dynamics in a data-dri",
    "arxiv_url": "https://arxiv.org/abs/2410.17610v3",
    "pdf_url": "https://arxiv.org/pdf/2410.17610v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2410.17610",
    "arxiv_authors": [
      "Xinpeng Liu",
      "Junxuan Liang",
      "Zili Lin",
      "Haowen Hou",
      "Yong-Lu Li",
      "Cewu Lu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=ImDy%3A+Human+Inverse+Dynamics+from+Imitated+Observations+Xinpeng+Liu+Junxuan+Liang+Zili+Lin+Haowen+Hou+Yong-Lu+Li",
    "gs_search_success": true,
    "gs_authors": [
      "7fv3Gz0AAAAJ",
      "Z9oVykIAAAAJ",
      "DBE-ju8AAAAJ",
      "QZVQEWAAAAAJ",
      "UExAaVgAAAAJ"
    ],
    "citation_count": 4,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2412.15095",
    "title": "A Full Transformer-based Framework for Automatic Pain Estimation using Videos",
    "year": 2024,
    "published": "2024-12-19T17:45:08Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "abstract": "The automatic estimation of pain is essential in designing an optimal pain management system offering reliable assessment and reducing the suffering of patients. In this study, we present a novel full transformer-based framework consisting of a Transformer in Transformer (TNT) model and a Transformer leveraging cross-attention and self-attention blocks. Elaborating on videos from the BioVid database, we demonstrate state-of-the-art performances, showing the efficacy, efficiency, and generalizati",
    "arxiv_url": "https://arxiv.org/abs/2412.15095v1",
    "pdf_url": "https://arxiv.org/pdf/2412.15095v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2412.15095",
    "arxiv_authors": [
      "Stefanos Gkikas",
      "Manolis Tsiknakis"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=A+Full+Transformer-based+Framework+for+Automatic+Pain+Estimation+using+Videos+Stefanos+Gkikas+Manolis+Tsiknakis",
    "gs_search_success": true,
    "gs_authors": [
      "Khj_WYgAAAAJ",
      "Pm1tx9oAAAAJ"
    ],
    "citation_count": 18,
    "gs_author_count": 2
  },
  {
    "arxiv_id": "2412.05386",
    "title": "DIFEM: Key-points Interaction based Feature Extraction Module for Violence Recognition in Videos",
    "year": 2024,
    "published": "2024-12-06T19:25:47Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Violence detection in surveillance videos is a critical task for ensuring public safety. As a result, there is increasing need for efficient and lightweight systems for automatic detection of violent behaviours. In this work, we propose an effective method which leverages human skeleton key-points to capture inherent properties of violence, such as rapid movement of specific joints and their close proximity. At the heart of our method is our novel Dynamic Interaction Feature Extraction Module (D",
    "arxiv_url": "https://arxiv.org/abs/2412.05386v1",
    "pdf_url": "https://arxiv.org/pdf/2412.05386v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2412.05386",
    "arxiv_authors": [
      "Himanshu Mittal",
      "Suvramalya Basak",
      "Anjali Gautam"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=DIFEM%3A+Key-points+Interaction+based+Feature+Extraction+Module+for+Violence+Recognition+in+Videos+Himanshu+Mittal+Suvramalya+Basak+Anjali+Gautam",
    "gs_search_success": true,
    "gs_authors": [
      "A1wSAHwAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2310.09247",
    "title": "Hypernymy Understanding Evaluation of Text-to-Image Models via WordNet Hierarchy",
    "year": 2023,
    "published": "2023-10-13T16:53:25Z",
    "categories": [
      "cs.CV",
      "cs.CL",
      "cs.LG"
    ],
    "abstract": "Text-to-image synthesis has recently attracted widespread attention due to rapidly improving quality and numerous practical applications. However, the language understanding capabilities of text-to-image models are still poorly understood, which makes it difficult to reason about prompt formulations that a given model would understand well. In this work, we measure the capability of popular text-to-image models to understand $\\textit{hypernymy}$, or the \"is-a\" relation between words. We design t",
    "arxiv_url": "https://arxiv.org/abs/2310.09247v1",
    "pdf_url": "https://arxiv.org/pdf/2310.09247v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2310.09247",
    "arxiv_authors": [
      "Anton Baryshnikov",
      "Max Ryabinin"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Hypernymy+Understanding+Evaluation+of+Text-to-Image+Models+via+WordNet+Hierarchy+Anton+Baryshnikov+Max+Ryabinin",
    "gs_search_success": true,
    "gs_authors": [
      "930PERsAAAAJ",
      "a4Si5a0AAAAJ"
    ],
    "citation_count": 3,
    "gs_author_count": 2
  },
  {
    "arxiv_id": "2411.02969",
    "title": "Multi-modal NeRF Self-Supervision for LiDAR Semantic Segmentation",
    "year": 2024,
    "published": "2024-11-05T10:13:23Z",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "abstract": "LiDAR Semantic Segmentation is a fundamental task in autonomous driving perception consisting of associating each LiDAR point to a semantic label. Fully-supervised models have widely tackled this task, but they require labels for each scan, which either limits their domain or requires impractical amounts of expensive annotations. Camera images, which are generally recorded alongside LiDAR pointclouds, can be processed by the widely available 2D foundation models, which are generic and dataset-ag",
    "arxiv_url": "https://arxiv.org/abs/2411.02969v1",
    "pdf_url": "https://arxiv.org/pdf/2411.02969v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2411.02969",
    "arxiv_authors": [
      "Xavier Timoneda",
      "Markus Herb",
      "Fabian Duerr",
      "Daniel Goehring",
      "Fisher Yu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Multi-modal+NeRF+Self-Supervision+for+LiDAR+Semantic+Segmentation+Xavier+Timoneda+Markus+Herb+Fabian+Duerr+Daniel+Goehring+Fisher+Yu",
    "gs_search_success": true,
    "gs_authors": [
      "SLZdcCcAAAAJ",
      "XIBgaEoAAAAJ",
      "-yZse64AAAAJ",
      "9NM2PJIAAAAJ"
    ],
    "citation_count": 2,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2308.12416",
    "title": "Reframing the Brain Age Prediction Problem to a More Interpretable and Quantitative Approach",
    "year": 2023,
    "published": "2023-08-23T20:33:22Z",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV",
      "q-bio.QM"
    ],
    "abstract": "Deep learning models have achieved state-of-the-art results in estimating brain age, which is an important brain health biomarker, from magnetic resonance (MR) images. However, most of these models only provide a global age prediction, and rely on techniques, such as saliency maps to interpret their results. These saliency maps highlight regions in the input image that were significant for the model's predictions, but they are hard to be interpreted, and saliency map values are not directly comp",
    "arxiv_url": "https://arxiv.org/abs/2308.12416v1",
    "pdf_url": "https://arxiv.org/pdf/2308.12416v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2308.12416",
    "arxiv_authors": [
      "Neha Gianchandani",
      "Mahsa Dibaji",
      "Mariana Bento",
      "Ethan MacDonald",
      "Roberto Souza"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Reframing+the+Brain+Age+Prediction+Problem+to+a+More+Interpretable+and+Quantitative+Approach+Neha+Gianchandani+Mahsa+Dibaji+Mariana+Bento+Ethan+MacDonald+Roberto+Souza",
    "gs_search_success": true,
    "gs_authors": [
      "zW3pptAAAAAJ",
      "OhCYnIcAAAAJ",
      "3DxVbpcAAAAJ",
      "3ipTl3cAAAAJ",
      "G2V4oBIAAAAJ"
    ],
    "citation_count": 2,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2502.00717",
    "title": "MINT: Mitigating Hallucinations in Large Vision-Language Models via Token Reduction",
    "year": 2025,
    "published": "2025-02-02T08:34:57Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Hallucination has been a long-standing and inevitable problem that hinders the application of Large Vision-Language Models (LVLMs) in domains that require high reliability. Various methods focus on improvement depending on data annotations or training strategies, yet place less emphasis on LLM's inherent problems. To fill this gap, we delve into the attention mechanism of the decoding process in the LVLM. Intriguingly, our investigation uncovers the prevalent attention redundancy within the hier",
    "arxiv_url": "https://arxiv.org/abs/2502.00717v1",
    "pdf_url": "https://arxiv.org/pdf/2502.00717v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2502.00717",
    "arxiv_authors": [
      "Chao Wang",
      "Jianming Yang",
      "Yang Zhou"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=MINT%3A+Mitigating+Hallucinations+in+Large+Vision-Language+Models+via+Token+Reduction+Chao+Wang+Jianming+Yang+Yang+Zhou",
    "gs_search_success": true,
    "gs_authors": [
      "f0vWPgkAAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2402.09569",
    "title": "Automated Plaque Detection and Agatston Score Estimation on Non-Contrast CT Scans: A Multicenter Study",
    "year": 2024,
    "published": "2024-02-14T20:41:37Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Coronary artery calcification (CAC) is a strong and independent predictor of cardiovascular disease (CVD). However, manual assessment of CAC often requires radiological expertise, time, and invasive imaging techniques. The purpose of this multicenter study is to validate an automated cardiac plaque detection model using a 3D multiclass nnU-Net for gated and non-gated non-contrast chest CT volumes. CT scans were performed at three tertiary care hospitals and collected as three datasets, respectiv",
    "arxiv_url": "https://arxiv.org/abs/2402.09569v1",
    "pdf_url": "https://arxiv.org/pdf/2402.09569v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2402.09569",
    "arxiv_authors": [
      "Andrew M. Nguyen",
      "Jianfei Liu",
      "Tejas Sudharshan Mathai",
      "Peter C. Grayson",
      "Ronald M. Summers"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Automated+Plaque+Detection+and+Agatston+Score+Estimation+on+Non-Contrast+CT+Scans%3A+A+Multicenter+Study+Andrew+M.+Nguyen+Jianfei+Liu+Tejas+Sudharshan+Mathai+Peter+C.+Grayson+Ronald+M.+Summers",
    "gs_search_success": true,
    "gs_authors": [
      "JGlh1IkAAAAJ",
      "juZ0O1sAAAAJ",
      "j4cvq80AAAAJ"
    ],
    "citation_count": 2,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2409.13557",
    "title": "Trustworthy Hate Speech Detection Through Visual Augmentation",
    "year": 2024,
    "published": "2024-09-20T14:57:34Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "The surge of hate speech on social media platforms poses a significant challenge, with hate speech detection~(HSD) becoming increasingly critical. Current HSD methods focus on enriching contextual information to enhance detection performance, but they overlook the inherent uncertainty of hate speech. We propose a novel HSD method, named trustworthy hate speech detection method through visual augmentation (TrusV-HSD), which enhances semantic information through integration with diffused visual im",
    "arxiv_url": "https://arxiv.org/abs/2409.13557v1",
    "pdf_url": "https://arxiv.org/pdf/2409.13557v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2409.13557",
    "arxiv_authors": [
      "Ziyuan Yang",
      "Ming Yan",
      "Yingyu Chen",
      "Hui Wang",
      "Zexin Lu",
      "Yi Zhang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Trustworthy+Hate+Speech+Detection+Through+Visual+Augmentation+Ziyuan+Yang+Ming+Yan+Yingyu+Chen+Hui+Wang+Zexin+Lu",
    "gs_search_success": true,
    "gs_authors": [
      "nRknP8UAAAAJ",
      "2vZsJskAAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2303.13810",
    "title": "Evidence-aware multi-modal data fusion and its application to total knee replacement prediction",
    "year": 2023,
    "published": "2023-03-24T05:06:02Z",
    "categories": [
      "cs.CV",
      "eess.IV"
    ],
    "abstract": "Deep neural networks have been widely studied for predicting a medical condition, such as total knee replacement (TKR). It has shown that data of different modalities, such as imaging data, clinical variables and demographic information, provide complementary information and thus can improve the prediction accuracy together. However, the data sources of various modalities may not always be of high quality, and each modality may have only partial information of medical condition. Thus, prediction",
    "arxiv_url": "https://arxiv.org/abs/2303.13810v1",
    "pdf_url": "https://arxiv.org/pdf/2303.13810v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2303.13810",
    "arxiv_authors": [
      "Xinwen Liu",
      "Jing Wang",
      "S. Kevin Zhou",
      "Craig Engstrom",
      "Shekhar S. Chandra"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Evidence-aware+multi-modal+data+fusion+and+its+application+to+total+knee+replacement+prediction+Xinwen+Liu+Jing+Wang+S.+Kevin+Zhou+Craig+Engstrom+Shekhar+S.+Chandra",
    "gs_search_success": true,
    "gs_authors": [
      "8eNm2GMAAAAJ",
      "tKe_olQAAAAJ",
      "u2pFDa0AAAAJ",
      "k33AbSYAAAAJ"
    ],
    "citation_count": 3,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2403.01663",
    "title": "PillarGen: Enhancing Radar Point Cloud Density and Quality via Pillar-based Point Generation Network",
    "year": 2024,
    "published": "2024-03-04T01:18:29Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "In this paper, we present a novel point generation model, referred to as Pillar-based Point Generation Network (PillarGen), which facilitates the transformation of point clouds from one domain into another. PillarGen can produce synthetic point clouds with enhanced density and quality based on the provided input point clouds. The PillarGen model performs the following three steps: 1) pillar encoding, 2) Occupied Pillar Prediction (OPP), and 3) Pillar to Point Generation (PPG). The input point cl",
    "arxiv_url": "https://arxiv.org/abs/2403.01663v2",
    "pdf_url": "https://arxiv.org/pdf/2403.01663v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2403.01663",
    "arxiv_authors": [
      "Jisong Kim",
      "Geonho Bang",
      "Kwangjin Choi",
      "Minjae Seong",
      "Jaechang Yoo",
      "Eunjong Pyo",
      "Jun Won Choi"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=PillarGen%3A+Enhancing+Radar+Point+Cloud+Density+and+Quality+via+Pillar-based+Point+Generation+Network+Jisong+Kim+Geonho+Bang+Kwangjin+Choi+Minjae+Seong+Jaechang+Yoo",
    "gs_search_success": true,
    "gs_authors": [
      "BJkT1ksAAAAJ",
      "B93t3mgAAAAJ",
      "yD7Led0AAAAJ",
      "IHH2PyYAAAAJ",
      "Dr8U8TAAAAAJ"
    ],
    "citation_count": 2,
    "gs_author_count": 7
  },
  {
    "arxiv_id": "2503.10602",
    "title": "TruthPrInt: Mitigating LVLM Object Hallucination Via Latent Truthful-Guided Pre-Intervention",
    "year": 2025,
    "published": "2025-03-13T17:46:06Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "abstract": "Object Hallucination (OH) has been acknowledged as one of the major trustworthy challenges in Large Vision-Language Models (LVLMs). Recent advancements in Large Language Models (LLMs) indicate that internal states, such as hidden states, encode the \"overall truthfulness\" of generated responses. However, it remains under-explored how internal states in LVLMs function and whether they could serve as \"per-token\" hallucination indicators, which is essential for mitigating OH. In this paper, we first",
    "arxiv_url": "https://arxiv.org/abs/2503.10602v2",
    "pdf_url": "https://arxiv.org/pdf/2503.10602v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2503.10602",
    "arxiv_authors": [
      "Jinhao Duan",
      "Fei Kong",
      "Hao Cheng",
      "James Diffenderfer",
      "Bhavya Kailkhura",
      "Lichao Sun",
      "Xiaofeng Zhu",
      "Xiaoshuang Shi",
      "Kaidi Xu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=TruthPrInt%3A+Mitigating+LVLM+Object+Hallucination+Via+Latent+Truthful-Guided+Pre-Intervention+Jinhao+Duan+Fei+Kong+Hao+Cheng+James+Diffenderfer+Bhavya+Kailkhura",
    "gs_search_success": true,
    "gs_authors": [
      "aWeTAXYAAAAJ",
      "nRr24_QAAAAJ",
      "-bk1CrcAAAAJ",
      "lYK0wlsAAAAJ",
      "SQpJmOgAAAAJ",
      "FdmRKFUAAAAJ",
      "WhGUE7AAAAAJ"
    ],
    "citation_count": 11,
    "gs_author_count": 9
  },
  {
    "arxiv_id": "2311.11642",
    "title": "Video Face Re-Aging: Toward Temporally Consistent Face Re-Aging",
    "year": 2023,
    "published": "2023-11-20T10:01:13Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "cs.MM"
    ],
    "abstract": "Video face re-aging deals with altering the apparent age of a person to the target age in videos. This problem is challenging due to the lack of paired video datasets maintaining temporal consistency in identity and age. Most re-aging methods process each image individually without considering the temporal consistency of videos. While some existing works address the issue of temporal coherence through video facial attribute manipulation in latent space, they often fail to deliver satisfactory pe",
    "arxiv_url": "https://arxiv.org/abs/2311.11642v3",
    "pdf_url": "https://arxiv.org/pdf/2311.11642v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2311.11642",
    "arxiv_authors": [
      "Abdul Muqeet",
      "Kyuchul Lee",
      "Bumsoo Kim",
      "Yohan Hong",
      "Hyungrae Lee",
      "Woonggon Kim",
      "KwangHee Lee"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Video+Face+Re-Aging%3A+Toward+Temporally+Consistent+Face+Re-Aging+Abdul+Muqeet+Kyuchul+Lee+Bumsoo+Kim+Yohan+Hong+Hyungrae+Lee",
    "gs_search_success": true,
    "gs_authors": [
      "jXiLuh8AAAAJ",
      "-BBpuKMAAAAJ",
      "o4a-2IEAAAAJ",
      "i79faMoAAAAJ",
      "JlNb4R8AAAAJ"
    ],
    "citation_count": 4,
    "gs_author_count": 7
  },
  {
    "arxiv_id": "2403.14743",
    "title": "VURF: A General-purpose Reasoning and Self-refinement Framework for Video Understanding",
    "year": 2024,
    "published": "2024-03-21T18:00:00Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Recent studies have demonstrated the effectiveness of Large Language Models (LLMs) as reasoning modules that can deconstruct complex tasks into more manageable sub-tasks, particularly when applied to visual reasoning tasks for images. In contrast, this paper introduces a Video Understanding and Reasoning Framework (VURF) based on the reasoning power of LLMs. Ours is a novel approach to extend the utility of LLMs in the context of video tasks, leveraging their capacity to generalize from minimal ",
    "arxiv_url": "https://arxiv.org/abs/2403.14743v3",
    "pdf_url": "https://arxiv.org/pdf/2403.14743v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2403.14743",
    "arxiv_authors": [
      "Ahmad Mahmood",
      "Ashmal Vayani",
      "Muzammal Naseer",
      "Salman Khan",
      "Fahad Shahbaz Khan"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=VURF%3A+A+General-purpose+Reasoning+and+Self-refinement+Framework+for+Video+Understanding+Ahmad+Mahmood+Ashmal+Vayani+Muzammal+Naseer+Salman+Khan+Fahad+Shahbaz+Khan",
    "gs_search_success": true,
    "gs_authors": [
      "K4KF1SwAAAAJ",
      "tM9xKA8AAAAJ",
      "zvaeYnUAAAAJ",
      "M59O9lkAAAAJ",
      "GlYoUysAAAAJ"
    ],
    "citation_count": 11,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2412.03093",
    "title": "Expanding Event Modality Applications through a Robust CLIP-Based Encoder",
    "year": 2024,
    "published": "2024-12-04T07:44:58Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "This paper introduces a powerful encoder that transfers CLIP`s capabilities to event-based data, enhancing its utility and expanding its applicability across diverse domains. While large-scale datasets have significantly advanced image-based models, the scarcity of comprehensive event datasets has limited performance potential in event modality. To address this challenge, we adapt CLIP`s architecture to align event embeddings with image embeddings, supporting zero-shot learning and preserving te",
    "arxiv_url": "https://arxiv.org/abs/2412.03093v2",
    "pdf_url": "https://arxiv.org/pdf/2412.03093v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2412.03093",
    "arxiv_authors": [
      "Sungheon Jeong",
      "Hanning Chen",
      "Sanggeon Yun",
      "Suhyeon Cho",
      "Wenjun Huang",
      "Xiangjian Liu",
      "Mohsen Imani"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Expanding+Event+Modality+Applications+through+a+Robust+CLIP-Based+Encoder+Sungheon+Jeong+Hanning+Chen+Sanggeon+Yun+Suhyeon+Cho+Wenjun+Huang",
    "gs_search_success": true,
    "gs_authors": [
      "Qw2sWuMAAAAJ",
      "FY5WEKcAAAAJ",
      "i_CamtAAAAAJ",
      "IEgLrnQAAAAJ",
      "JFQQxBQAAAAJ",
      "pO3spAkAAAAJ"
    ],
    "citation_count": 3,
    "gs_author_count": 7
  },
  {
    "arxiv_id": "2503.21367",
    "title": "Multimodal surface defect detection from wooden logs for sawing optimization",
    "year": 2025,
    "published": "2025-03-27T10:58:45Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "We propose a novel, good-quality, and less demanding method for detecting knots on the surface of wooden logs using multimodal data fusion. Knots are a primary factor affecting the quality of sawn timber, making their detection fundamental to any timber grading or cutting optimization system. While X-ray computed tomography provides accurate knot locations and internal structures, it is often too slow or expensive for practical use. An attractive alternative is to use fast and cost-effective log",
    "arxiv_url": "https://arxiv.org/abs/2503.21367v1",
    "pdf_url": "https://arxiv.org/pdf/2503.21367v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2503.21367",
    "arxiv_authors": [
      "Bořek Reich",
      "Matej Kunda",
      "Fedor Zolotarev",
      "Tuomas Eerola",
      "Pavel Zemčík",
      "Tomi Kauppi"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Multimodal+surface+defect+detection+from+wooden+logs+for+sawing+optimization+Bo%C5%99ek+Reich+Matej+Kunda+Fedor+Zolotarev+Tuomas+Eerola+Pavel+Zem%C4%8D%C3%ADk",
    "gs_search_success": true,
    "gs_authors": [
      "hGJxPDkAAAAJ",
      "WK7-dIkAAAAJ",
      "dq1L9hgAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2405.07905",
    "title": "PLUTO: Pathology-Universal Transformer",
    "year": 2024,
    "published": "2024-05-13T16:40:17Z",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "abstract": "Pathology is the study of microscopic inspection of tissue, and a pathology diagnosis is often the medical gold standard to diagnose disease. Pathology images provide a unique challenge for computer-vision-based analysis: a single pathology Whole Slide Image (WSI) is gigapixel-sized and often contains hundreds of thousands to millions of objects of interest across multiple resolutions. In this work, we propose PathoLogy Universal TransfOrmer (PLUTO): a light-weight pathology FM that is pre-train",
    "arxiv_url": "https://arxiv.org/abs/2405.07905v1",
    "pdf_url": "https://arxiv.org/pdf/2405.07905v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2405.07905",
    "arxiv_authors": [
      "Dinkar Juyal",
      "Harshith Padigela",
      "Chintan Shah",
      "Daniel Shenker",
      "Natalia Harguindeguy",
      "Yi Liu",
      "Blake Martin",
      "Yibo Zhang",
      "Michael Nercessian",
      "Miles Markey",
      "Isaac Finberg",
      "Kelsey Luu",
      "Daniel Borders",
      "Syed Ashar Javed",
      "Emma Krause",
      "Raymond Biju",
      "Aashish Sood",
      "Allen Ma",
      "Jackson Nyman",
      "John Shamshoian",
      "Guillaume Chhor",
      "Darpan Sanghavi",
      "Marc Thibault",
      "Limin Yu",
      "Fedaa Najdawi",
      "Jennifer A. Hipp",
      "Darren Fahy",
      "Benjamin Glass",
      "Eric Walk",
      "John Abel",
      "Harsha Pokkalla",
      "Andrew H. Beck",
      "Sean Grullon"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=PLUTO%3A+Pathology-Universal+Transformer+Dinkar+Juyal+Harshith+Padigela+Chintan+Shah+Daniel+Shenker+Natalia+Harguindeguy",
    "gs_search_success": true,
    "gs_authors": [
      "Xft-kFoAAAAJ",
      "LUOd17cAAAAJ",
      "ze1FZr8AAAAJ",
      "aRZes80AAAAJ",
      "WOJgVp4AAAAJ"
    ],
    "citation_count": 31,
    "gs_author_count": 8
  },
  {
    "arxiv_id": "2401.00391",
    "title": "SAFE-SIM: Safety-Critical Closed-Loop Traffic Simulation with Diffusion-Controllable Adversaries",
    "year": 2023,
    "published": "2023-12-31T04:14:43Z",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "Evaluating the performance of autonomous vehicle planning algorithms necessitates simulating long-tail safety-critical traffic scenarios. However, traditional methods for generating such scenarios often fall short in terms of controllability and realism; they also neglect the dynamics of agent interactions. To address these limitations, we introduce SAFE-SIM, a novel diffusion-based controllable closed-loop safety-critical simulation framework. Our approach yields two distinct advantages: 1) gen",
    "arxiv_url": "https://arxiv.org/abs/2401.00391v3",
    "pdf_url": "https://arxiv.org/pdf/2401.00391v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2401.00391",
    "arxiv_authors": [
      "Wei-Jer Chang",
      "Francesco Pittaluga",
      "Masayoshi Tomizuka",
      "Wei Zhan",
      "Manmohan Chandraker"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=SAFE-SIM%3A+Safety-Critical+Closed-Loop+Traffic+Simulation+with+Diffusion-Controllable+Adversaries+Wei-Jer+Chang+Francesco+Pittaluga+Masayoshi+Tomizuka+Wei+Zhan+Manmohan+Chandraker",
    "gs_search_success": true,
    "gs_authors": [
      "Ja-8AFsAAAAJ",
      "tF-OmYgAAAAJ",
      "bIeCNNoAAAAJ",
      "8m8taGEAAAAJ",
      "xVN3UxYAAAAJ"
    ],
    "citation_count": 28,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2310.06753",
    "title": "TopoMLP: A Simple yet Strong Pipeline for Driving Topology Reasoning",
    "year": 2023,
    "published": "2023-10-10T16:24:51Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Topology reasoning aims to comprehensively understand road scenes and present drivable routes in autonomous driving. It requires detecting road centerlines (lane) and traffic elements, further reasoning their topology relationship, i.e., lane-lane topology, and lane-traffic topology. In this work, we first present that the topology score relies heavily on detection performance on lane and traffic elements. Therefore, we introduce a powerful 3D lane detector and an improved 2D traffic element det",
    "arxiv_url": "https://arxiv.org/abs/2310.06753v2",
    "pdf_url": "https://arxiv.org/pdf/2310.06753v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2310.06753",
    "arxiv_authors": [
      "Dongming Wu",
      "Jiahao Chang",
      "Fan Jia",
      "Yingfei Liu",
      "Tiancai Wang",
      "Jianbing Shen"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=TopoMLP%3A+A+Simple+yet+Strong+Pipeline+for+Driving+Topology+Reasoning+Dongming+Wu+Jiahao+Chang+Fan+Jia+Yingfei+Liu+Tiancai+Wang",
    "gs_search_success": true,
    "gs_authors": [
      "_Q3NTToAAAAJ",
      "YI0sRroAAAAJ",
      "pF9KA1sAAAAJ"
    ],
    "citation_count": 49,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2502.02307",
    "title": "UniGaze: Towards Universal Gaze Estimation via Large-scale Pre-Training",
    "year": 2025,
    "published": "2025-02-04T13:24:23Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Despite decades of research on data collection and model architectures, current gaze estimation models encounter significant challenges in generalizing across diverse data domains. Recent advances in self-supervised pre-training have shown remarkable performances in generalization across various vision tasks. However, their effectiveness in gaze estimation remains unexplored. We propose UniGaze, for the first time, leveraging large-scale in-the-wild facial datasets for gaze estimation through se",
    "arxiv_url": "https://arxiv.org/abs/2502.02307v2",
    "pdf_url": "https://arxiv.org/pdf/2502.02307v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2502.02307",
    "arxiv_authors": [
      "Jiawei Qin",
      "Xucong Zhang",
      "Yusuke Sugano"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=UniGaze%3A+Towards+Universal+Gaze+Estimation+via+Large-scale+Pre-Training+Jiawei+Qin+Xucong+Zhang+Yusuke+Sugano",
    "gs_search_success": true,
    "gs_authors": [
      "lDfmDk4AAAAJ",
      "gWzZNCwAAAAJ",
      "WCrRFtkAAAAJ"
    ],
    "citation_count": 6,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2501.19066",
    "title": "Concept Steerers: Leveraging K-Sparse Autoencoders for Test-Time Controllable Generations",
    "year": 2025,
    "published": "2025-01-31T11:52:47Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Despite the remarkable progress in text-to-image generative models, they are prone to adversarial attacks and inadvertently generate unsafe, unethical content. Existing approaches often rely on fine-tuning models to remove specific concepts, which is computationally expensive, lacks scalability, and/or compromises generation quality. In this work, we propose a novel framework leveraging k-sparse autoencoders (k-SAEs) to enable efficient and interpretable concept manipulation in diffusion models.",
    "arxiv_url": "https://arxiv.org/abs/2501.19066v3",
    "pdf_url": "https://arxiv.org/pdf/2501.19066v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2501.19066",
    "arxiv_authors": [
      "Dahye Kim",
      "Deepti Ghadiyaram"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Concept+Steerers%3A+Leveraging+K-Sparse+Autoencoders+for+Test-Time+Controllable+Generations+Dahye+Kim+Deepti+Ghadiyaram",
    "gs_search_success": true,
    "gs_authors": [
      "QgBn1BgAAAAJ",
      "psS9UC4AAAAJ"
    ],
    "citation_count": 12,
    "gs_author_count": 2
  },
  {
    "arxiv_id": "2404.11769",
    "title": "QGen: On the Ability to Generalize in Quantization Aware Training",
    "year": 2024,
    "published": "2024-04-17T21:52:21Z",
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "abstract": "Quantization lowers memory usage, computational requirements, and latency by utilizing fewer bits to represent model weights and activations. In this work, we investigate the generalization properties of quantized neural networks, a characteristic that has received little attention despite its implications on model performance. In particular, first, we develop a theoretical model for quantization in neural networks and demonstrate how quantization functions as a form of regularization. Second, m",
    "arxiv_url": "https://arxiv.org/abs/2404.11769v2",
    "pdf_url": "https://arxiv.org/pdf/2404.11769v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2404.11769",
    "arxiv_authors": [
      "MohammadHossein AskariHemmat",
      "Ahmadreza Jeddi",
      "Reyhane Askari Hemmat",
      "Ivan Lazarevich",
      "Alexander Hoffman",
      "Sudhakar Sah",
      "Ehsan Saboori",
      "Yvon Savaria",
      "Jean-Pierre David"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=QGen%3A+On+the+Ability+to+Generalize+in+Quantization+Aware+Training+MohammadHossein+AskariHemmat+Ahmadreza+Jeddi+Reyhane+Askari+Hemmat+Ivan+Lazarevich+Alexander+Hoffman",
    "gs_search_success": true,
    "gs_authors": [
      "Ruq8ZywAAAAJ",
      "5t_-enoAAAAJ",
      "jTCaRa4AAAAJ",
      "8DjCIS8AAAAJ",
      "ZGDfPM4AAAAJ",
      "adj8P3AAAAAJ",
      "dRezqREAAAAJ"
    ],
    "citation_count": 4,
    "gs_author_count": 7
  },
  {
    "arxiv_id": "2303.18001",
    "title": "You Only Train Once: Learning a General Anomaly Enhancement Network with Random Masks for Hyperspectral Anomaly Detection",
    "year": 2023,
    "published": "2023-03-31T12:23:56Z",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "abstract": "In this paper, we introduce a new approach to address the challenge of generalization in hyperspectral anomaly detection (AD). Our method eliminates the need for adjusting parameters or retraining on new test scenes as required by most existing methods. Employing an image-level training paradigm, we achieve a general anomaly enhancement network for hyperspectral AD that only needs to be trained once. Trained on a set of anomaly-free hyperspectral images with random masks, our network can learn t",
    "arxiv_url": "https://arxiv.org/abs/2303.18001v1",
    "pdf_url": "https://arxiv.org/pdf/2303.18001v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2303.18001",
    "arxiv_authors": [
      "Zhaoxu Li",
      "Yingqian Wang",
      "Chao Xiao",
      "Qiang Ling",
      "Zaiping Lin",
      "Wei An"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=You+Only+Train+Once%3A+Learning+a+General+Anomaly+Enhancement+Network+with+Random+Masks+for+Hyperspectral+Anomaly+Detection+Zhaoxu+Li+Yingqian+Wang+Chao+Xiao+Qiang+Ling+Zaiping+Lin",
    "gs_search_success": true,
    "gs_authors": [
      "tBA4alMAAAAJ"
    ],
    "citation_count": 60,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2503.17080",
    "title": "Seeing What Matters: Empowering CLIP with Patch Generation-to-Selection",
    "year": 2025,
    "published": "2025-03-21T12:10:38Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "The CLIP model has demonstrated significant advancements in aligning visual and language modalities through large-scale pre-training on image-text pairs, enabling strong zero-shot classification and retrieval capabilities on various domains. However, CLIP's training remains computationally intensive, with high demands on both data processing and memory. To address these challenges, recent masking strategies have emerged, focusing on the selective removal of image patches to improve training effi",
    "arxiv_url": "https://arxiv.org/abs/2503.17080v1",
    "pdf_url": "https://arxiv.org/pdf/2503.17080v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2503.17080",
    "arxiv_authors": [
      "Gensheng Pei",
      "Tao Chen",
      "Yujia Wang",
      "Xinhao Cai",
      "Xiangbo Shu",
      "Tianfei Zhou",
      "Yazhou Yao"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Seeing+What+Matters%3A+Empowering+CLIP+with+Patch+Generation-to-Selection+Gensheng+Pei+Tao+Chen+Yujia+Wang+Xinhao+Cai+Xiangbo+Shu",
    "gs_search_success": true,
    "gs_authors": [
      "9RvG9F0AAAAJ",
      "_3Ucwv4AAAAJ",
      "FQfcm5oAAAAJ",
      "ihU_QpsAAAAJ",
      "wU8NHBsAAAAJ",
      "-_33ccMAAAAJ",
      "5iZNErQAAAAJ"
    ],
    "citation_count": 2,
    "gs_author_count": 7
  },
  {
    "arxiv_id": "2407.21272",
    "title": "Automated Quantification of Hyperreflective Foci in SD-OCT With Diabetic Retinopathy",
    "year": 2024,
    "published": "2024-07-31T01:33:47Z",
    "categories": [
      "cs.AI",
      "cs.CV"
    ],
    "abstract": "The presence of hyperreflective foci (HFs) is related to retinal disease progression, and the quantity has proven to be a prognostic factor of visual and anatomical outcome in various retinal diseases. However, lack of efficient quantitative tools for evaluating the HFs has deprived ophthalmologist of assessing the volume of HFs. For this reason, we propose an automated quantification algorithm to segment and quantify HFs in spectral domain optical coherence tomography (SD-OCT). The proposed alg",
    "arxiv_url": "https://arxiv.org/abs/2407.21272v1",
    "pdf_url": "https://arxiv.org/pdf/2407.21272v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2407.21272",
    "arxiv_authors": [
      "Idowu Paul Okuwobi",
      "Zexuan Ji",
      "Wen Fan",
      "Songtao Yuan",
      "Loza Bekalo",
      "Qiang Chen"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Automated+Quantification+of+Hyperreflective+Foci+in+SD-OCT+With+Diabetic+Retinopathy+Idowu+Paul+Okuwobi+Zexuan+Ji+Wen+Fan+Songtao+Yuan+Loza+Bekalo",
    "gs_search_success": true,
    "gs_authors": [
      "D-JT0QQAAAAJ",
      "Mjc8ZYsAAAAJ"
    ],
    "citation_count": 54,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2403.08252",
    "title": "PNeSM: Arbitrary 3D Scene Stylization via Prompt-Based Neural Style Mapping",
    "year": 2024,
    "published": "2024-03-13T05:08:47Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "3D scene stylization refers to transform the appearance of a 3D scene to match a given style image, ensuring that images rendered from different viewpoints exhibit the same style as the given style image, while maintaining the 3D consistency of the stylized scene. Several existing methods have obtained impressive results in stylizing 3D scenes. However, the models proposed by these methods need to be re-trained when applied to a new scene. In other words, their models are coupled with a specific",
    "arxiv_url": "https://arxiv.org/abs/2403.08252v1",
    "pdf_url": "https://arxiv.org/pdf/2403.08252v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2403.08252",
    "arxiv_authors": [
      "Jiafu Chen",
      "Wei Xing",
      "Jiakai Sun",
      "Tianyi Chu",
      "Yiling Huang",
      "Boyan Ji",
      "Lei Zhao",
      "Huaizhong Lin",
      "Haibo Chen",
      "Zhizhong Wang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=PNeSM%3A+Arbitrary+3D+Scene+Stylization+via+Prompt-Based+Neural+Style+Mapping+Jiafu+Chen+Wei+Xing+Jiakai+Sun+Tianyi+Chu+Yiling+Huang",
    "gs_search_success": true,
    "gs_authors": [
      "KKBFG6kAAAAJ",
      "fDPvse4AAAAJ",
      "G5U8a2YAAAAJ",
      "0W34qh8AAAAJ",
      "FAKmXzoAAAAJ"
    ],
    "citation_count": 4,
    "gs_author_count": 10
  },
  {
    "arxiv_id": "2408.12774",
    "title": "Semi-Supervised Variational Adversarial Active Learning via Learning to Rank and Agreement-Based Pseudo Labeling",
    "year": 2024,
    "published": "2024-08-23T00:35:07Z",
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "abstract": "Active learning aims to alleviate the amount of labor involved in data labeling by automating the selection of unlabeled samples via an acquisition function. For example, variational adversarial active learning (VAAL) leverages an adversarial network to discriminate unlabeled samples from labeled ones using latent space information. However, VAAL has the following shortcomings: (i) it does not exploit target task information, and (ii) unlabeled data is only used for sample selection rather than ",
    "arxiv_url": "https://arxiv.org/abs/2408.12774v1",
    "pdf_url": "https://arxiv.org/pdf/2408.12774v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2408.12774",
    "arxiv_authors": [
      "Zongyao Lyu",
      "William J. Beksi"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Semi-Supervised+Variational+Adversarial+Active+Learning+via+Learning+to+Rank+and+Agreement-Based+Pseudo+Labeling+Zongyao+Lyu+William+J.+Beksi",
    "gs_search_success": true,
    "gs_authors": [
      "lU2Z7MMAAAAJ",
      "9Hd5LooAAAAJ"
    ],
    "citation_count": 4,
    "gs_author_count": 2
  },
  {
    "arxiv_id": "2305.02743",
    "title": "Incremental 3D Semantic Scene Graph Prediction from RGB Sequences",
    "year": 2023,
    "published": "2023-05-04T11:32:16Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "3D semantic scene graphs are a powerful holistic representation as they describe the individual objects and depict the relation between them. They are compact high-level graphs that enable many tasks requiring scene reasoning. In real-world settings, existing 3D estimation methods produce robust predictions that mostly rely on dense inputs. In this work, we propose a real-time framework that incrementally builds a consistent 3D semantic scene graph of a scene given an RGB image sequence. Our met",
    "arxiv_url": "https://arxiv.org/abs/2305.02743v2",
    "pdf_url": "https://arxiv.org/pdf/2305.02743v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2305.02743",
    "arxiv_authors": [
      "Shun-Cheng Wu",
      "Keisuke Tateno",
      "Nassir Navab",
      "Federico Tombari"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Incremental+3D+Semantic+Scene+Graph+Prediction+from+RGB+Sequences+Shun-Cheng+Wu+Keisuke+Tateno+Nassir+Navab+Federico+Tombari",
    "gs_search_success": true,
    "gs_authors": [
      "TFsE4BIAAAAJ",
      "qEo9eiMAAAAJ",
      "kzoVUPYAAAAJ",
      "ml3laqEAAAAJ"
    ],
    "citation_count": 38,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2308.10809",
    "title": "Improving Continuous Sign Language Recognition with Cross-Lingual Signs",
    "year": 2023,
    "published": "2023-08-21T15:58:47Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "This work dedicates to continuous sign language recognition (CSLR), which is a weakly supervised task dealing with the recognition of continuous signs from videos, without any prior knowledge about the temporal boundaries between consecutive signs. Data scarcity heavily impedes the progress of CSLR. Existing approaches typically train CSLR models on a monolingual corpus, which is orders of magnitude smaller than that of speech recognition. In this work, we explore the feasibility of utilizing mu",
    "arxiv_url": "https://arxiv.org/abs/2308.10809v1",
    "pdf_url": "https://arxiv.org/pdf/2308.10809v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2308.10809",
    "arxiv_authors": [
      "Fangyun Wei",
      "Yutong Chen"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Improving+Continuous+Sign+Language+Recognition+with+Cross-Lingual+Signs+Fangyun+Wei+Yutong+Chen",
    "gs_search_success": true,
    "gs_authors": [
      "-ncz2s8AAAAJ",
      "lwT6xPoAAAAJ"
    ],
    "citation_count": 53,
    "gs_author_count": 2
  },
  {
    "arxiv_id": "2303.04291",
    "title": "Diffusion in the Dark: A Diffusion Model for Low-Light Text Recognition",
    "year": 2023,
    "published": "2023-03-07T23:52:51Z",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "abstract": "Capturing images is a key part of automation for high-level tasks such as scene text recognition. Low-light conditions pose a challenge for high-level perception stacks, which are often optimized on well-lit, artifact-free images. Reconstruction methods for low-light images can produce well-lit counterparts, but typically at the cost of high-frequency details critical for downstream tasks. We propose Diffusion in the Dark (DiD), a diffusion model for low-light image reconstruction for text recog",
    "arxiv_url": "https://arxiv.org/abs/2303.04291v2",
    "pdf_url": "https://arxiv.org/pdf/2303.04291v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2303.04291",
    "arxiv_authors": [
      "Cindy M. Nguyen",
      "Eric R. Chan",
      "Alexander W. Bergman",
      "Gordon Wetzstein"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Diffusion+in+the+Dark%3A+A+Diffusion+Model+for+Low-Light+Text+Recognition+Cindy+M.+Nguyen+Eric+R.+Chan+Alexander+W.+Bergman+Gordon+Wetzstein",
    "gs_search_success": true,
    "gs_authors": [
      "TLgngQYAAAAJ",
      "VOf45S0AAAAJ",
      "-DqNXmAAAAAJ",
      "ckr1ZcIAAAAJ"
    ],
    "citation_count": 37,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2308.16738",
    "title": "SFUSNet: A Spatial-Frequency domain-based Multi-branch Network for diagnosis of Cervical Lymph Node Lesions in Ultrasound Images",
    "year": 2023,
    "published": "2023-08-31T13:54:57Z",
    "categories": [
      "eess.IV",
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "Booming deep learning has substantially improved the diagnosis for diverse lesions in ultrasound images, but a conspicuous research gap concerning cervical lymph node lesions still remains. The objective of this work is to diagnose cervical lymph node lesions in ultrasound images by leveraging a deep learning model. To this end, we first collected 3392 cervical ultrasound images containing normal lymph nodes, benign lymph node lesions, malignant primary lymph node lesions, and malignant metastat",
    "arxiv_url": "https://arxiv.org/abs/2308.16738v2",
    "pdf_url": "https://arxiv.org/pdf/2308.16738v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2308.16738",
    "arxiv_authors": [
      "Yubiao Yue",
      "Jun Xue",
      "Haihua Liang",
      "Bingchun Luo",
      "Zhenzhang Li"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=SFUSNet%3A+A+Spatial-Frequency+domain-based+Multi-branch+Network+for+diagnosis+of+Cervical+Lymph+Node+Lesions+in+Ultrasound+Images+Yubiao+Yue+Jun+Xue+Haihua+Liang+Bingchun+Luo+Zhenzhang+Li",
    "gs_search_success": true,
    "gs_authors": [],
    "citation_count": null,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2411.16421",
    "title": "Machine Learning for the Digital Typhoon Dataset: Extensions to Multiple Basins and New Developments in Representations and Tasks",
    "year": 2024,
    "published": "2024-11-25T14:25:39Z",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "This paper presents the Digital Typhoon Dataset V2, a new version of the longest typhoon satellite image dataset for 40+ years aimed at benchmarking machine learning models for long-term spatio-temporal data. The new addition in Dataset V2 is tropical cyclone data from the southern hemisphere, in addition to the northern hemisphere data in Dataset V1. Having data from two hemispheres allows us to ask new research questions about regional differences across basins and hemispheres. We also discuss",
    "arxiv_url": "https://arxiv.org/abs/2411.16421v1",
    "pdf_url": "https://arxiv.org/pdf/2411.16421v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2411.16421",
    "arxiv_authors": [
      "Asanobu Kitamoto",
      "Erwan Dzik",
      "Gaspar Faure"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Machine+Learning+for+the+Digital+Typhoon+Dataset%3A+Extensions+to+Multiple+Basins+and+New+Developments+in+Representations+and+Tasks+Asanobu+Kitamoto+Erwan+Dzik+Gaspar+Faure",
    "gs_search_success": true,
    "gs_authors": [
      "8ij51OEAAAAJ",
      "TZG-qp0AAAAJ"
    ],
    "citation_count": 2,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2408.17433",
    "title": "DARES: Depth Anything in Robotic Endoscopic Surgery with Self-supervised Vector-LoRA of the Foundation Model",
    "year": 2024,
    "published": "2024-08-30T17:35:06Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Robotic-assisted surgery (RAS) relies on accurate depth estimation for 3D reconstruction and visualization. While foundation models like Depth Anything Models (DAM) show promise, directly applying them to surgery often yields suboptimal results. Fully fine-tuning on limited surgical data can cause overfitting and catastrophic forgetting, compromising model robustness and generalization. Although Low-Rank Adaptation (LoRA) addresses some adaptation issues, its uniform parameter distribution negle",
    "arxiv_url": "https://arxiv.org/abs/2408.17433v2",
    "pdf_url": "https://arxiv.org/pdf/2408.17433v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2408.17433",
    "arxiv_authors": [
      "Mona Sheikh Zeinoddin",
      "Chiara Lena",
      "Jiongqi Qu",
      "Luca Carlini",
      "Mattia Magro",
      "Seunghoi Kim",
      "Elena De Momi",
      "Sophia Bano",
      "Matthew Grech-Sollars",
      "Evangelos Mazomenos",
      "Daniel C. Alexander",
      "Danail Stoyanov",
      "Matthew J. Clarkson",
      "Mobarakol Islam"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=DARES%3A+Depth+Anything+in+Robotic+Endoscopic+Surgery+with+Self-supervised+Vector-LoRA+of+the+Foundation+Model+Mona+Sheikh+Zeinoddin+Chiara+Lena+Jiongqi+Qu+Luca+Carlini+Mattia+Magro",
    "gs_search_success": true,
    "gs_authors": [
      "O0wHK1sAAAAJ",
      "eDsibrQAAAAJ",
      "qdzaJIgAAAAJ"
    ],
    "citation_count": 8,
    "gs_author_count": null
  },
  {
    "arxiv_id": "2310.01667",
    "title": "STARS: Zero-shot Sim-to-Real Transfer for Segmentation of Shipwrecks in Sonar Imagery",
    "year": 2023,
    "published": "2023-10-02T21:58:32Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "In this paper, we address the problem of sim-to-real transfer for object segmentation when there is no access to real examples of an object of interest during training, i.e. zero-shot sim-to-real transfer for segmentation. We focus on the application of shipwreck segmentation in side scan sonar imagery. Our novel segmentation network, STARS, addresses this challenge by fusing a predicted deformation field and anomaly volume, allowing it to generalize better to real sonar images and achieve more ",
    "arxiv_url": "https://arxiv.org/abs/2310.01667v1",
    "pdf_url": "https://arxiv.org/pdf/2310.01667v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2310.01667",
    "arxiv_authors": [
      "Advaith Venkatramanan Sethuraman",
      "Katherine A. Skinner"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=STARS%3A+Zero-shot+Sim-to-Real+Transfer+for+Segmentation+of+Shipwrecks+in+Sonar+Imagery+Advaith+Venkatramanan+Sethuraman+Katherine+A.+Skinner",
    "gs_search_success": true,
    "gs_authors": [
      "3FNO098AAAAJ",
      "DsN9c-YAAAAJ"
    ],
    "citation_count": 8,
    "gs_author_count": null
  },
  {
    "arxiv_id": "2307.12400",
    "title": "TransNet: Transparent Object Manipulation Through Category-Level Pose Estimation",
    "year": 2023,
    "published": "2023-07-23T18:38:42Z",
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "abstract": "Transparent objects present multiple distinct challenges to visual perception systems. First, their lack of distinguishing visual features makes transparent objects harder to detect and localize than opaque objects. Even humans find certain transparent surfaces with little specular reflection or refraction, like glass doors, difficult to perceive. A second challenge is that depth sensors typically used for opaque object perception cannot obtain accurate depth measurements on transparent surfaces",
    "arxiv_url": "https://arxiv.org/abs/2307.12400v1",
    "pdf_url": "https://arxiv.org/pdf/2307.12400v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2307.12400",
    "arxiv_authors": [
      "Huijie Zhang",
      "Anthony Opipari",
      "Xiaotong Chen",
      "Jiyue Zhu",
      "Zeren Yu",
      "Odest Chadwicke Jenkins"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=TransNet%3A+Transparent+Object+Manipulation+Through+Category-Level+Pose+Estimation+Huijie+Zhang+Anthony+Opipari+Xiaotong+Chen+Jiyue+Zhu+Zeren+Yu",
    "gs_search_success": false,
    "gs_authors": [],
    "citation_count": null,
    "gs_author_count": null,
    "error_type": "fetch_failed"
  },
  {
    "arxiv_id": "2405.12601",
    "title": "FFAM: Feature Factorization Activation Map for Explanation of 3D Detectors",
    "year": 2024,
    "published": "2024-05-21T08:55:10Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "LiDAR-based 3D object detection has made impressive progress recently, yet most existing models are black-box, lacking interpretability. Previous explanation approaches primarily focus on analyzing image-based models and are not readily applicable to LiDAR-based 3D detectors. In this paper, we propose a feature factorization activation map (FFAM) to generate high-quality visual explanations for 3D detectors. FFAM employs non-negative matrix factorization to generate concept activation maps and s",
    "arxiv_url": "https://arxiv.org/abs/2405.12601v1",
    "pdf_url": "https://arxiv.org/pdf/2405.12601v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2405.12601",
    "arxiv_authors": [
      "Shuai Liu",
      "Boyang Li",
      "Zhiyu Fang",
      "Mingyue Cui",
      "Kai Huang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=FFAM%3A+Feature+Factorization+Activation+Map+for+Explanation+of+3D+Detectors+Shuai+Liu+Boyang+Li+Zhiyu+Fang+Mingyue+Cui+Kai+Huang",
    "gs_search_success": true,
    "gs_authors": [
      "70_wl8kAAAAJ",
      "jM9DezUAAAAJ",
      "iwDISDEAAAAJ",
      "loIo-50AAAAJ"
    ],
    "citation_count": 3,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2504.01019",
    "title": "MixerMDM: Learnable Composition of Human Motion Diffusion Models",
    "year": 2025,
    "published": "2025-04-01T17:59:44Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Generating human motion guided by conditions such as textual descriptions is challenging due to the need for datasets with pairs of high-quality motion and their corresponding conditions. The difficulty increases when aiming for finer control in the generation. To that end, prior works have proposed to combine several motion diffusion models pre-trained on datasets with different types of conditions, thus allowing control with multiple conditions. However, the proposed merging strategies overloo",
    "arxiv_url": "https://arxiv.org/abs/2504.01019v1",
    "pdf_url": "https://arxiv.org/pdf/2504.01019v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2504.01019",
    "arxiv_authors": [
      "Pablo Ruiz-Ponce",
      "German Barquero",
      "Cristina Palmero",
      "Sergio Escalera",
      "José García-Rodríguez"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=MixerMDM%3A+Learnable+Composition+of+Human+Motion+Diffusion+Models+Pablo+Ruiz-Ponce+German+Barquero+Cristina+Palmero+Sergio+Escalera+Jos%C3%A9+Garc%C3%ADa-Rodr%C3%ADguez",
    "gs_search_success": true,
    "gs_authors": [
      "25rkMIwAAAAJ",
      "V0c9xx0AAAAJ",
      "oI6AIkMAAAAJ",
      "pRC8DwcAAAAJ",
      "GNTkqaYAAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2411.05698",
    "title": "Visual-TCAV: Concept-based Attribution and Saliency Maps for Post-hoc Explainability in Image Classification",
    "year": 2024,
    "published": "2024-11-08T16:52:52Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "abstract": "Convolutional Neural Networks (CNNs) have seen significant performance improvements in recent years. However, due to their size and complexity, they function as black-boxes, leading to transparency concerns. State-of-the-art saliency methods generate local explanations that highlight the area in the input image where a class is identified but cannot explain how a concept of interest contributes to the prediction, which is essential for bias mitigation. On the other hand, concept-based methods, s",
    "arxiv_url": "https://arxiv.org/abs/2411.05698v2",
    "pdf_url": "https://arxiv.org/pdf/2411.05698v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2411.05698",
    "arxiv_authors": [
      "Antonio De Santis",
      "Riccardo Campi",
      "Matteo Bianchi",
      "Marco Brambilla"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Visual-TCAV%3A+Concept-based+Attribution+and+Saliency+Maps+for+Post-hoc+Explainability+in+Image+Classification+Antonio+De+Santis+Riccardo+Campi+Matteo+Bianchi+Marco+Brambilla",
    "gs_search_success": true,
    "gs_authors": [
      "JWSoz7EAAAAJ",
      "S6MMstgAAAAJ",
      "Xwx26YgAAAAJ",
      "YQV_ChkAAAAJ"
    ],
    "citation_count": 6,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2305.11566",
    "title": "StereoVAE: A lightweight stereo-matching system using embedded GPUs",
    "year": 2023,
    "published": "2023-05-19T10:08:39Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.MM",
      "cs.RO"
    ],
    "abstract": "We present a lightweight system for stereo matching through embedded GPUs. It breaks the trade-off between accuracy and processing speed in stereo matching, enabling our embedded system to further improve the matching accuracy while ensuring real-time processing. The main idea of our method is to construct a tiny neural network based on variational auto-encoder (VAE) to upsample and refinement a small size of coarse disparity map, which is first generated by a traditional matching method. The pr",
    "arxiv_url": "https://arxiv.org/abs/2305.11566v3",
    "pdf_url": "https://arxiv.org/pdf/2305.11566v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2305.11566",
    "arxiv_authors": [
      "Qiong Chang",
      "Xiang Li",
      "Xin Xu",
      "Xin Liu",
      "Yun Li",
      "Miyazaki Jun"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=StereoVAE%3A+A+lightweight+stereo-matching+system+using+embedded+GPUs+Qiong+Chang+Xiang+Li+Xin+Xu+Xin+Liu+Yun+Li",
    "gs_search_success": true,
    "gs_authors": [
      "cPh-vn0AAAAJ",
      "GkfJ564AAAAJ",
      "wviRTP0AAAAJ",
      "pog7XscAAAAJ"
    ],
    "citation_count": 10,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2502.11532",
    "title": "Control-CLIP: Decoupling Category and Style Guidance in CLIP for Specific-Domain Generation",
    "year": 2025,
    "published": "2025-02-17T08:03:55Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Text-to-image diffusion models have shown remarkable capabilities of generating high-quality images closely aligned with textual inputs. However, the effectiveness of text guidance heavily relies on the CLIP text encoder, which is trained to pay more attention to general content but struggles to capture semantics in specific domains like styles. As a result, generation models tend to fail on prompts like \"a photo of a cat in Pokemon style\" in terms of simply producing images depicting \"a photo o",
    "arxiv_url": "https://arxiv.org/abs/2502.11532v1",
    "pdf_url": "https://arxiv.org/pdf/2502.11532v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2502.11532",
    "arxiv_authors": [
      "Zexi Jia",
      "Chuanwei Huang",
      "Hongyan Fei",
      "Yeshuang Zhu",
      "Zhiqiang Yuan",
      "Jinchao Zhang",
      "Jie Zhou"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Control-CLIP%3A+Decoupling+Category+and+Style+Guidance+in+CLIP+for+Specific-Domain+Generation+Zexi+Jia+Chuanwei+Huang+Hongyan+Fei+Yeshuang+Zhu+Zhiqiang+Yuan",
    "gs_search_success": true,
    "gs_authors": [
      "vH9YLsAAAAAJ",
      "z8_HxH0AAAAJ",
      "ew7aKloAAAAJ",
      "uFo7ET0AAAAJ",
      "gtfpYs8AAAAJ"
    ],
    "citation_count": 2,
    "gs_author_count": 7
  },
  {
    "arxiv_id": "2401.11115",
    "title": "MotionMix: Weakly-Supervised Diffusion for Controllable Motion Generation",
    "year": 2024,
    "published": "2024-01-20T04:58:06Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Controllable generation of 3D human motions becomes an important topic as the world embraces digital transformation. Existing works, though making promising progress with the advent of diffusion models, heavily rely on meticulously captured and annotated (e.g., text) high-quality motion corpus, a resource-intensive endeavor in the real world. This motivates our proposed MotionMix, a simple yet effective weakly-supervised diffusion model that leverages both noisy and unannotated motion sequences.",
    "arxiv_url": "https://arxiv.org/abs/2401.11115v3",
    "pdf_url": "https://arxiv.org/pdf/2401.11115v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2401.11115",
    "arxiv_authors": [
      "Nhat M. Hoang",
      "Kehong Gong",
      "Chuan Guo",
      "Michael Bi Mi"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=MotionMix%3A+Weakly-Supervised+Diffusion+for+Controllable+Motion+Generation+Nhat+M.+Hoang+Kehong+Gong+Chuan+Guo+Michael+Bi+Mi",
    "gs_search_success": true,
    "gs_authors": [
      "HOItlroAAAAJ",
      "zHKIX_gAAAAJ",
      "eCdqvJoAAAAJ"
    ],
    "citation_count": 11,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2404.17936",
    "title": "FDCE-Net: Underwater Image Enhancement with Embedding Frequency and Dual Color Encoder",
    "year": 2024,
    "published": "2024-04-27T15:16:34Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Underwater images often suffer from various issues such as low brightness, color shift, blurred details, and noise due to light absorption and scattering caused by water and suspended particles. Previous underwater image enhancement (UIE) methods have primarily focused on spatial domain enhancement, neglecting the frequency domain information inherent in the images. However, the degradation factors of underwater images are closely intertwined in the spatial domain. Although certain methods focus",
    "arxiv_url": "https://arxiv.org/abs/2404.17936v1",
    "pdf_url": "https://arxiv.org/pdf/2404.17936v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2404.17936",
    "arxiv_authors": [
      "Zheng Cheng",
      "Guodong Fan",
      "Jingchun Zhou",
      "Min Gan",
      "C. L. Philip Chen"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=FDCE-Net%3A+Underwater+Image+Enhancement+with+Embedding+Frequency+and+Dual+Color+Encoder+Zheng+Cheng+Guodong+Fan+Jingchun+Zhou+Min+Gan+C.+L.+Philip+Chen",
    "gs_search_success": true,
    "gs_authors": [
      "79h9z8QAAAAJ",
      "Q5248zwAAAAJ",
      "Ls5Fx4IAAAAJ"
    ],
    "citation_count": 30,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2403.19497",
    "title": "Surface-based parcellation and vertex-wise analysis of ultra high-resolution ex vivo 7 tesla MRI in Alzheimer's disease and related dementias",
    "year": 2024,
    "published": "2024-03-28T15:27:34Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Magnetic resonance imaging (MRI) is the standard modality to understand human brain structure and function in vivo (antemortem). Decades of research in human neuroimaging has led to the widespread development of methods and tools to provide automated volume-based segmentations and surface-based parcellations which help localize brain functions to specialized anatomical regions. Recently ex vivo (postmortem) imaging of the brain has opened-up avenues to study brain structure at sub-millimeter ult",
    "arxiv_url": "https://arxiv.org/abs/2403.19497v2",
    "pdf_url": "https://arxiv.org/pdf/2403.19497v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2403.19497",
    "arxiv_authors": [
      "Pulkit Khandelwal",
      "Michael Tran Duong",
      "Lisa Levorse",
      "Constanza Fuentes",
      "Amanda Denning",
      "Winifred Trotman",
      "Ranjit Ittyerah",
      "Alejandra Bahena",
      "Theresa Schuck",
      "Marianna Gabrielyan",
      "Karthik Prabhakaran",
      "Daniel Ohm",
      "Gabor Mizsei",
      "John Robinson",
      "Monica Munoz",
      "John Detre",
      "Edward Lee",
      "David Irwin",
      "Corey McMillan",
      "M. Dylan Tisdall",
      "Sandhitsu Das",
      "David Wolk",
      "Paul A. Yushkevich"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Surface-based+parcellation+and+vertex-wise+analysis+of+ultra+high-resolution+ex+vivo+7+tesla+MRI+in+Alzheimer%27s+disease+and+related+dementias+Pulkit+Khandelwal+Michael+Tran+Duong+Lisa+Levorse+Constanza+Fuentes+Amanda+Denning",
    "gs_search_success": true,
    "gs_authors": [
      "6BOwPcoAAAAJ",
      "5EUr0Y0AAAAJ",
      "TDZcWG0AAAAJ"
    ],
    "citation_count": 2,
    "gs_author_count": 7
  },
  {
    "arxiv_id": "2505.22564",
    "title": "PRISM: Video Dataset Condensation with Progressive Refinement and Insertion for Sparse Motion",
    "year": 2025,
    "published": "2025-05-28T16:42:10Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "abstract": "Video dataset condensation has emerged as a critical technique for addressing the computational challenges associated with large-scale video data processing in deep learning applications. While significant progress has been made in image dataset condensation, the video domain presents unique challenges due to the complex interplay between spatial content and temporal dynamics. This paper introduces PRISM, Progressive Refinement and Insertion for Sparse Motion, for video dataset condensation, a n",
    "arxiv_url": "https://arxiv.org/abs/2505.22564v1",
    "pdf_url": "https://arxiv.org/pdf/2505.22564v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2505.22564",
    "arxiv_authors": [
      "Jaehyun Choi",
      "Jiwan Hur",
      "Gyojin Han",
      "Jaemyung Yu",
      "Junmo Kim"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=PRISM%3A+Video+Dataset+Condensation+with+Progressive+Refinement+and+Insertion+for+Sparse+Motion+Jaehyun+Choi+Jiwan+Hur+Gyojin+Han+Jaemyung+Yu+Junmo+Kim",
    "gs_search_success": true,
    "gs_authors": [
      "PYXTN_QAAAAJ",
      "eaXyBkUAAAAJ",
      "3jag3z4AAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2312.04885",
    "title": "VISAGE: Video Instance Segmentation with Appearance-Guided Enhancement",
    "year": 2023,
    "published": "2023-12-08T07:48:03Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "In recent years, online Video Instance Segmentation (VIS) methods have shown remarkable advancement with their powerful query-based detectors. Utilizing the output queries of the detector at the frame-level, these methods achieve high accuracy on challenging benchmarks. However, our observations demonstrate that these methods heavily rely on location information, which often causes incorrect associations between objects. This paper presents that a key axis of object matching in trackers is appea",
    "arxiv_url": "https://arxiv.org/abs/2312.04885v2",
    "pdf_url": "https://arxiv.org/pdf/2312.04885v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2312.04885",
    "arxiv_authors": [
      "Hanjung Kim",
      "Jaehyun Kang",
      "Miran Heo",
      "Sukjun Hwang",
      "Seoung Wug Oh",
      "Seon Joo Kim"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=VISAGE%3A+Video+Instance+Segmentation+with+Appearance-Guided+Enhancement+Hanjung+Kim+Jaehyun+Kang+Miran+Heo+Sukjun+Hwang+Seoung+Wug+Oh",
    "gs_search_success": true,
    "gs_authors": [
      "BWME3BoAAAAJ",
      "GVklAzsAAAAJ",
      "-c22oFIAAAAJ",
      "1F2czKYAAAAJ",
      "3d31SzwAAAAJ",
      "rW5dpvMAAAAJ"
    ],
    "citation_count": 9,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2409.08461",
    "title": "VistaFormer: Scalable Vision Transformers for Satellite Image Time Series Segmentation",
    "year": 2024,
    "published": "2024-09-13T01:19:53Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "We introduce VistaFormer, a lightweight Transformer-based model architecture for the semantic segmentation of remote-sensing images. This model uses a multi-scale Transformer-based encoder with a lightweight decoder that aggregates global and local attention captured in the encoder blocks. VistaFormer uses position-free self-attention layers which simplifies the model architecture and removes the need to interpolate temporal and spatial codes, which can reduce model performance when training and",
    "arxiv_url": "https://arxiv.org/abs/2409.08461v1",
    "pdf_url": "https://arxiv.org/pdf/2409.08461v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2409.08461",
    "arxiv_authors": [
      "Ezra MacDonald",
      "Derek Jacoby",
      "Yvonne Coady"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=VistaFormer%3A+Scalable+Vision+Transformers+for+Satellite+Image+Time+Series+Segmentation+Ezra+MacDonald+Derek+Jacoby+Yvonne+Coady",
    "gs_search_success": false,
    "gs_authors": [],
    "citation_count": null,
    "gs_author_count": null,
    "error_type": "fetch_failed"
  },
  {
    "arxiv_id": "2408.02054",
    "title": "Step Saver: Predicting Minimum Denoising Steps for Diffusion Model Image Generation",
    "year": 2024,
    "published": "2024-08-04T15:01:23Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "In this paper, we introduce an innovative NLP model specifically fine-tuned to determine the minimal number of denoising steps required for any given text prompt. This advanced model serves as a real-time tool that recommends the ideal denoise steps for generating high-quality images efficiently. It is designed to work seamlessly with the Diffusion model, ensuring that images are produced with superior quality in the shortest possible time. Although our explanation focuses on the DDIM scheduler,",
    "arxiv_url": "https://arxiv.org/abs/2408.02054v1",
    "pdf_url": "https://arxiv.org/pdf/2408.02054v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2408.02054",
    "arxiv_authors": [
      "Jean Yu",
      "Haim Barad"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Step+Saver%3A+Predicting+Minimum+Denoising+Steps+for+Diffusion+Model+Image+Generation+Jean+Yu+Haim+Barad",
    "gs_search_success": true,
    "gs_authors": [
      "VW2wlHsAAAAJ"
    ],
    "citation_count": 3,
    "gs_author_count": 2
  },
  {
    "arxiv_id": "2501.05669",
    "title": "LPRnet: A self-supervised registration network for LiDAR and photogrammetric point clouds",
    "year": 2025,
    "published": "2025-01-10T02:36:37Z",
    "categories": [
      "cs.CV",
      "eess.IV"
    ],
    "abstract": "LiDAR and photogrammetry are active and passive remote sensing techniques for point cloud acquisition, respectively, offering complementary advantages and heterogeneous. Due to the fundamental differences in sensing mechanisms, spatial distributions and coordinate systems, their point clouds exhibit significant discrepancies in density, precision, noise, and overlap. Coupled with the lack of ground truth for large-scale scenes, integrating the heterogeneous point clouds is a highly challenging t",
    "arxiv_url": "https://arxiv.org/abs/2501.05669v1",
    "pdf_url": "https://arxiv.org/pdf/2501.05669v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2501.05669",
    "arxiv_authors": [
      "Chen Wang",
      "Yanfeng Gu",
      "Xian Li"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=LPRnet%3A+A+self-supervised+registration+network+for+LiDAR+and+photogrammetric+point+clouds+Chen+Wang+Yanfeng+Gu+Xian+Li",
    "gs_search_success": true,
    "gs_authors": [
      "k-9e_DYAAAAJ",
      "WHkRZscAAAAJ"
    ],
    "citation_count": 7,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2504.13150",
    "title": "Readable Twins of Unreadable Models",
    "year": 2025,
    "published": "2025-04-17T17:55:34Z",
    "categories": [
      "cs.AI",
      "cs.CV"
    ],
    "abstract": "Creating responsible artificial intelligence (AI) systems is an important issue in contemporary research and development of works on AI. One of the characteristics of responsible AI systems is their explainability. In the paper, we are interested in explainable deep learning (XDL) systems. On the basis of the creation of digital twins of physical objects, we introduce the idea of creating readable twins (in the form of imprecise information flow models) for unreadable deep learning models. The c",
    "arxiv_url": "https://arxiv.org/abs/2504.13150v1",
    "pdf_url": "https://arxiv.org/pdf/2504.13150v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2504.13150",
    "arxiv_authors": [
      "Krzysztof Pancerz",
      "Piotr Kulicki",
      "Michał Kalisz",
      "Andrzej Burda",
      "Maciej Stanisławski",
      "Jaromir Sarzyński"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Readable+Twins+of+Unreadable+Models+Krzysztof+Pancerz+Piotr+Kulicki+Micha%C5%82+Kalisz+Andrzej+Burda+Maciej+Stanis%C5%82awski",
    "gs_search_success": true,
    "gs_authors": [
      "vBPjqLgAAAAJ",
      "ehV2djsAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2409.19461",
    "title": "Accelerating Malware Classification: A Vision Transformer Solution",
    "year": 2024,
    "published": "2024-09-28T21:34:14Z",
    "categories": [
      "cs.CR",
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "The escalating frequency and scale of recent malware attacks underscore the urgent need for swift and precise malware classification in the ever-evolving cybersecurity landscape. Key challenges include accurately categorizing closely related malware families. To tackle this evolving threat landscape, this paper proposes a novel architecture LeViT-MC which produces state-of-the-art results in malware detection and classification. LeViT-MC leverages a vision transformer-based architecture, an imag",
    "arxiv_url": "https://arxiv.org/abs/2409.19461v1",
    "pdf_url": "https://arxiv.org/pdf/2409.19461v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2409.19461",
    "arxiv_authors": [
      "Shrey Bavishi",
      "Shrey Modi"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Accelerating+Malware+Classification%3A+A+Vision+Transformer+Solution+Shrey+Bavishi+Shrey+Modi",
    "gs_search_success": true,
    "gs_authors": [
      "yayHJ-MAAAAJ",
      "JdcpFSMAAAAJ"
    ],
    "citation_count": 4,
    "gs_author_count": 2
  },
  {
    "arxiv_id": "2307.08319",
    "title": "Soft Curriculum for Learning Conditional GANs with Noisy-Labeled and Uncurated Unlabeled Data",
    "year": 2023,
    "published": "2023-07-17T08:31:59Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Label-noise or curated unlabeled data is used to compensate for the assumption of clean labeled data in training the conditional generative adversarial network; however, satisfying such an extended assumption is occasionally laborious or impractical. As a step towards generative modeling accessible to everyone, we introduce a novel conditional image generation framework that accepts noisy-labeled and uncurated unlabeled data during training: (i) closed-set and open-set label noise in labeled dat",
    "arxiv_url": "https://arxiv.org/abs/2307.08319v1",
    "pdf_url": "https://arxiv.org/pdf/2307.08319v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2307.08319",
    "arxiv_authors": [
      "Kai Katsumata",
      "Duc Minh Vo",
      "Tatsuya Harada",
      "Hideki Nakayama"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Soft+Curriculum+for+Learning+Conditional+GANs+with+Noisy-Labeled+and+Uncurated+Unlabeled+Data+Kai+Katsumata+Duc+Minh+Vo+Tatsuya+Harada+Hideki+Nakayama",
    "gs_search_success": true,
    "gs_authors": [
      "lZAYGJoAAAAJ",
      "pfKTAYwAAAAJ",
      "k8rlJ8AAAAAJ",
      "Qxle99gAAAAJ"
    ],
    "citation_count": 2,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2405.17718",
    "title": "AdapNet: Adaptive Noise-Based Network for Low-Quality Image Retrieval",
    "year": 2024,
    "published": "2024-05-28T00:25:41Z",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "Image retrieval aims to identify visually similar images within a database using a given query image. Traditional methods typically employ both global and local features extracted from images for matching, and may also apply re-ranking techniques to enhance accuracy. However, these methods often fail to account for the noise present in query images, which can stem from natural or human-induced factors, thereby negatively impacting retrieval performance. To mitigate this issue, we introduce a nov",
    "arxiv_url": "https://arxiv.org/abs/2405.17718v1",
    "pdf_url": "https://arxiv.org/pdf/2405.17718v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2405.17718",
    "arxiv_authors": [
      "Sihe Zhang",
      "Qingdong He",
      "Jinlong Peng",
      "Yuxi Li",
      "Zhengkai Jiang",
      "Jiafu Wu",
      "Mingmin Chi",
      "Yabiao Wang",
      "Chengjie Wang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=AdapNet%3A+Adaptive+Noise-Based+Network+for+Low-Quality+Image+Retrieval+Sihe+Zhang+Qingdong+He+Jinlong+Peng+Yuxi+Li+Zhengkai+Jiang",
    "gs_search_success": true,
    "gs_authors": [
      "Y8b1W00AAAAJ",
      "gUJWww0AAAAJ",
      "tiQ_rv0AAAAJ",
      "fqte5H4AAAAJ",
      "xiK4nFUAAAAJ",
      "i5I-cIEAAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 9
  },
  {
    "arxiv_id": "2407.13759",
    "title": "Streetscapes: Large-scale Consistent Street View Generation Using Autoregressive Video Diffusion",
    "year": 2024,
    "published": "2024-07-18T17:56:30Z",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "abstract": "We present a method for generating Streetscapes-long sequences of views through an on-the-fly synthesized city-scale scene. Our generation is conditioned by language input (e.g., city name, weather), as well as an underlying map/layout hosting the desired trajectory. Compared to recent models for video generation or 3D view synthesis, our method can scale to much longer-range camera trajectories, spanning several city blocks, while maintaining visual quality and consistency. To achieve this goal",
    "arxiv_url": "https://arxiv.org/abs/2407.13759v2",
    "pdf_url": "https://arxiv.org/pdf/2407.13759v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2407.13759",
    "arxiv_authors": [
      "Boyang Deng",
      "Richard Tucker",
      "Zhengqi Li",
      "Leonidas Guibas",
      "Noah Snavely",
      "Gordon Wetzstein"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Streetscapes%3A+Large-scale+Consistent+Street+View+Generation+Using+Autoregressive+Video+Diffusion+Boyang+Deng+Richard+Tucker+Zhengqi+Li+Leonidas+Guibas+Noah+Snavely",
    "gs_search_success": true,
    "gs_authors": [
      "Nr_uRhQAAAAJ",
      "IkpNZAoAAAAJ",
      "Db4BCX8AAAAJ",
      "QLW0lzcAAAAJ",
      "5JlEyTAAAAAJ",
      "VOf45S0AAAAJ"
    ],
    "citation_count": 33,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2402.08994",
    "title": "CLIP-MUSED: CLIP-Guided Multi-Subject Visual Neural Information Semantic Decoding",
    "year": 2024,
    "published": "2024-02-14T07:41:48Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "The study of decoding visual neural information faces challenges in generalizing single-subject decoding models to multiple subjects, due to individual differences. Moreover, the limited availability of data from a single subject has a constraining impact on model performance. Although prior multi-subject decoding methods have made significant progress, they still suffer from several limitations, including difficulty in extracting global neural response features, linear scaling of model paramete",
    "arxiv_url": "https://arxiv.org/abs/2402.08994v1",
    "pdf_url": "https://arxiv.org/pdf/2402.08994v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2402.08994",
    "arxiv_authors": [
      "Qiongyi Zhou",
      "Changde Du",
      "Shengpei Wang",
      "Huiguang He"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=CLIP-MUSED%3A+CLIP-Guided+Multi-Subject+Visual+Neural+Information+Semantic+Decoding+Qiongyi+Zhou+Changde+Du+Shengpei+Wang+Huiguang+He",
    "gs_search_success": true,
    "gs_authors": [
      "AqoWVdsAAAAJ",
      "ef2EFsYAAAAJ",
      "dlvs3e4AAAAJ",
      "-pE0e5IAAAAJ"
    ],
    "citation_count": 23,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2405.19819",
    "title": "Gated Fields: Learning Scene Reconstruction from Gated Videos",
    "year": 2024,
    "published": "2024-05-30T08:26:47Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Reconstructing outdoor 3D scenes from temporal observations is a challenge that recent work on neural fields has offered a new avenue for. However, existing methods that recover scene properties, such as geometry, appearance, or radiance, solely from RGB captures often fail when handling poorly-lit or texture-deficient regions. Similarly, recovering scenes with scanning LiDAR sensors is also difficult due to their low angular sampling rate which makes recovering expansive real-world scenes diffi",
    "arxiv_url": "https://arxiv.org/abs/2405.19819v1",
    "pdf_url": "https://arxiv.org/pdf/2405.19819v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2405.19819",
    "arxiv_authors": [
      "Andrea Ramazzina",
      "Stefanie Walz",
      "Pragyan Dahal",
      "Mario Bijelic",
      "Felix Heide"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Gated+Fields%3A+Learning+Scene+Reconstruction+from+Gated+Videos+Andrea+Ramazzina+Stefanie+Walz+Pragyan+Dahal+Mario+Bijelic+Felix+Heide",
    "gs_search_success": true,
    "gs_authors": [
      "ZeHkzXwAAAAJ",
      "8BXZvdwAAAAJ",
      "aJV5jRYAAAAJ",
      "dtLybe4AAAAJ",
      "gRqzSHsAAAAJ"
    ],
    "citation_count": 3,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2311.16835",
    "title": "Unified-modal Salient Object Detection via Adaptive Prompt Learning",
    "year": 2023,
    "published": "2023-11-28T14:51:08Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Existing single-modal and multi-modal salient object detection (SOD) methods focus on designing specific architectures tailored for their respective tasks. However, developing completely different models for different tasks leads to labor and time consumption, as well as high computational and practical deployment costs. In this paper, we attempt to address both single-modal and multi-modal SOD in a unified framework called UniSOD, which fully exploits the overlapping prior knowledge between dif",
    "arxiv_url": "https://arxiv.org/abs/2311.16835v5",
    "pdf_url": "https://arxiv.org/pdf/2311.16835v5",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2311.16835",
    "arxiv_authors": [
      "Kunpeng Wang",
      "Chenglong Li",
      "Zhengzheng Tu",
      "Zhengyi Liu",
      "Bin Luo"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Unified-modal+Salient+Object+Detection+via+Adaptive+Prompt+Learning+Kunpeng+Wang+Chenglong+Li+Zhengzheng+Tu+Zhengyi+Liu+Bin+Luo",
    "gs_search_success": true,
    "gs_authors": [
      "0qaDapcAAAAJ",
      "6iJZSjwAAAAJ",
      "6kM00LQAAAAJ",
      "41T-jN8AAAAJ"
    ],
    "citation_count": 7,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2409.20003",
    "title": "Multibiometrics Using a Single Face Image",
    "year": 2024,
    "published": "2024-09-30T06:55:01Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Multibiometrics, which uses multiple biometric traits to improve recognition performance instead of using only one biometric trait to authenticate individuals, has been investigated. Previous studies have combined individually acquired biometric traits or have not fully considered the convenience of the system. Focusing on a single face image, we propose a novel multibiometric method that combines five biometric traits, i.e., face, iris, periocular, nose, eyebrow, that can be extracted from a si",
    "arxiv_url": "https://arxiv.org/abs/2409.20003v2",
    "pdf_url": "https://arxiv.org/pdf/2409.20003v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2409.20003",
    "arxiv_authors": [
      "Koichi Ito",
      "Taito Tonosaki",
      "Takafumi Aoki",
      "Tetsushi Ohki",
      "Masakatsu Nishigaki"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Multibiometrics+Using+a+Single+Face+Image+Koichi+Ito+Taito+Tonosaki+Takafumi+Aoki+Tetsushi+Ohki+Masakatsu+Nishigaki",
    "gs_search_success": true,
    "gs_authors": [
      "opzSvFUAAAAJ",
      "HWBwO0sAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2306.02741",
    "title": "ZIGNeRF: Zero-shot 3D Scene Representation with Invertible Generative Neural Radiance Fields",
    "year": 2023,
    "published": "2023-06-05T09:41:51Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Generative Neural Radiance Fields (NeRFs) have demonstrated remarkable proficiency in synthesizing multi-view images by learning the distribution of a set of unposed images. Despite the aptitude of existing generative NeRFs in generating 3D-consistent high-quality random samples within data distribution, the creation of a 3D representation of a singular input image remains a formidable challenge. In this manuscript, we introduce ZIGNeRF, an innovative model that executes zero-shot Generative Adv",
    "arxiv_url": "https://arxiv.org/abs/2306.02741v1",
    "pdf_url": "https://arxiv.org/pdf/2306.02741v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2306.02741",
    "arxiv_authors": [
      "Kanghyeok Ko",
      "Minhyeok Lee"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=ZIGNeRF%3A+Zero-shot+3D+Scene+Representation+with+Invertible+Generative+Neural+Radiance+Fields+Kanghyeok+Ko+Minhyeok+Lee",
    "gs_search_success": false,
    "gs_authors": [],
    "citation_count": null,
    "gs_author_count": null,
    "error_type": "fetch_failed"
  },
  {
    "arxiv_id": "2407.12875",
    "title": "ChatBCG: Can AI Read Your Slide Deck?",
    "year": 2024,
    "published": "2024-07-16T06:00:45Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Multimodal models like GPT4o and Gemini Flash are exceptional at inference and summarization tasks, which approach human-level in performance. However, we find that these models underperform compared to humans when asked to do very specific 'reading and estimation' tasks, particularly in the context of visual charts in business decks. This paper evaluates the accuracy of GPT 4o and Gemini Flash-1.5 in answering straightforward questions about data on labeled charts (where data is clearly annotat",
    "arxiv_url": "https://arxiv.org/abs/2407.12875v1",
    "pdf_url": "https://arxiv.org/pdf/2407.12875v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2407.12875",
    "arxiv_authors": [
      "Nikita Singh",
      "Rob Balian",
      "Lukas Martinelli"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=ChatBCG%3A+Can+AI+Read+Your+Slide+Deck%3F+Nikita+Singh+Rob+Balian+Lukas+Martinelli",
    "gs_search_success": true,
    "gs_authors": [],
    "citation_count": null,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2309.12245",
    "title": "Adaptive Input-image Normalization for Solving the Mode Collapse Problem in GAN-based X-ray Images",
    "year": 2023,
    "published": "2023-09-21T16:43:29Z",
    "categories": [
      "eess.IV",
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "Biomedical image datasets can be imbalanced due to the rarity of targeted diseases. Generative Adversarial Networks play a key role in addressing this imbalance by enabling the generation of synthetic images to augment datasets. It is important to generate synthetic images that incorporate a diverse range of features to accurately represent the distribution of features present in the training imagery. Furthermore, the absence of diverse features in synthetic images can degrade the performance of",
    "arxiv_url": "https://arxiv.org/abs/2309.12245v3",
    "pdf_url": "https://arxiv.org/pdf/2309.12245v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2309.12245",
    "arxiv_authors": [
      "Muhammad Muneeb Saad",
      "Mubashir Husain Rehmani",
      "Ruairi O'Reilly"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Adaptive+Input-image+Normalization+for+Solving+the+Mode+Collapse+Problem+in+GAN-based+X-ray+Images+Muhammad+Muneeb+Saad+Mubashir+Husain+Rehmani+Ruairi+O%27Reilly",
    "gs_search_success": true,
    "gs_authors": [
      "x4OZtdAAAAAJ",
      "0IpB0aUAAAAJ",
      "86x5oQgAAAAJ"
    ],
    "citation_count": 3,
    "gs_author_count": null
  },
  {
    "arxiv_id": "2402.01462",
    "title": "3D Vertebrae Measurements: Assessing Vertebral Dimensions in Human Spine Mesh Models Using Local Anatomical Vertebral Axes",
    "year": 2024,
    "published": "2024-02-02T14:52:41Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Vertebral morphological measurements are important across various disciplines, including spinal biomechanics and clinical applications, pre- and post-operatively. These measurements also play a crucial role in anthropological longitudinal studies, where spinal metrics are repeatedly documented over extended periods. Traditionally, such measurements have been manually conducted, a process that is time-consuming. In this study, we introduce a novel, fully automated method for measuring vertebral m",
    "arxiv_url": "https://arxiv.org/abs/2402.01462v1",
    "pdf_url": "https://arxiv.org/pdf/2402.01462v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2402.01462",
    "arxiv_authors": [
      "Ivanna Kramer",
      "Vinzent Rittel",
      "Lara Blomenkamp",
      "Sabine Bauer",
      "Dietrich Paulus"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=3D+Vertebrae+Measurements%3A+Assessing+Vertebral+Dimensions+in+Human+Spine+Mesh+Models+Using+Local+Anatomical+Vertebral+Axes+Ivanna+Kramer+Vinzent+Rittel+Lara+Blomenkamp+Sabine+Bauer+Dietrich+Paulus",
    "gs_search_success": true,
    "gs_authors": [
      "3SeodI4AAAAJ"
    ],
    "citation_count": 2,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2305.10345",
    "title": "MM-Fi: Multi-Modal Non-Intrusive 4D Human Dataset for Versatile Wireless Sensing",
    "year": 2023,
    "published": "2023-05-12T05:18:52Z",
    "categories": [
      "eess.SP",
      "cs.AI",
      "cs.CV",
      "cs.MM"
    ],
    "abstract": "4D human perception plays an essential role in a myriad of applications, such as home automation and metaverse avatar simulation. However, existing solutions which mainly rely on cameras and wearable devices are either privacy intrusive or inconvenient to use. To address these issues, wireless sensing has emerged as a promising alternative, leveraging LiDAR, mmWave radar, and WiFi signals for device-free human sensing. In this paper, we propose MM-Fi, the first multi-modal non-intrusive 4D human",
    "arxiv_url": "https://arxiv.org/abs/2305.10345v2",
    "pdf_url": "https://arxiv.org/pdf/2305.10345v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2305.10345",
    "arxiv_authors": [
      "Jianfei Yang",
      "He Huang",
      "Yunjiao Zhou",
      "Xinyan Chen",
      "Yuecong Xu",
      "Shenghai Yuan",
      "Han Zou",
      "Chris Xiaoxuan Lu",
      "Lihua Xie"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=MM-Fi%3A+Multi-Modal+Non-Intrusive+4D+Human+Dataset+for+Versatile+Wireless+Sensing+Jianfei+Yang+He+Huang+Yunjiao+Zhou+Xinyan+Chen+Yuecong+Xu",
    "gs_search_success": true,
    "gs_authors": [
      "cqeOXE4AAAAJ",
      "g46qu98AAAAJ",
      "Fmrv3J8AAAAJ",
      "idu78-EAAAAJ",
      "XGQNPHAAAAAJ",
      "0milAlsAAAAJ",
      "V25k08UAAAAJ",
      "XcV_sesAAAAJ"
    ],
    "citation_count": 120,
    "gs_author_count": 9
  },
  {
    "arxiv_id": "2410.11187",
    "title": "Multiview Scene Graph",
    "year": 2024,
    "published": "2024-10-15T02:04:05Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "A proper scene representation is central to the pursuit of spatial intelligence where agents can robustly reconstruct and efficiently understand 3D scenes. A scene representation is either metric, such as landmark maps in 3D reconstruction, 3D bounding boxes in object detection, or voxel grids in occupancy prediction, or topological, such as pose graphs with loop closures in SLAM or visibility graphs in SfM. In this work, we propose to build Multiview Scene Graphs (MSG) from unposed images, repr",
    "arxiv_url": "https://arxiv.org/abs/2410.11187v3",
    "pdf_url": "https://arxiv.org/pdf/2410.11187v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2410.11187",
    "arxiv_authors": [
      "Juexiao Zhang",
      "Gao Zhu",
      "Sihang Li",
      "Xinhao Liu",
      "Haorui Song",
      "Xinran Tang",
      "Chen Feng"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Multiview+Scene+Graph+Juexiao+Zhang+Gao+Zhu+Sihang+Li+Xinhao+Liu+Haorui+Song",
    "gs_search_success": true,
    "gs_authors": [
      "6pI4Xa4AAAAJ",
      "YeG8ZM0AAAAJ",
      "90IoeJsAAAAJ",
      "TYxPbcEAAAAJ",
      "orP0gGMAAAAJ",
      "wfilnV4AAAAJ"
    ],
    "citation_count": 8,
    "gs_author_count": 7
  },
  {
    "arxiv_id": "2505.17097",
    "title": "CAMA: Enhancing Multimodal In-Context Learning with Context-Aware Modulated Attention",
    "year": 2025,
    "published": "2025-05-21T04:25:23Z",
    "categories": [
      "cs.CV",
      "cs.CL"
    ],
    "abstract": "Multimodal in-context learning (ICL) is emerging as a key capability that enables large vision-language models (LVLMs) to adapt to novel tasks without parameter updates, expanding their utility across various real-world applications. However, ICL remains unstable, even with well-matched in-context demonstrations (ICDs), suggesting that LVLMs struggle to fully utilize the provided context. While existing efforts focus on prompt engineering or post-hoc logit calibration, we instead investigate the",
    "arxiv_url": "https://arxiv.org/abs/2505.17097v2",
    "pdf_url": "https://arxiv.org/pdf/2505.17097v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2505.17097",
    "arxiv_authors": [
      "Yanshu Li",
      "Jianjiang Yang",
      "Ziteng Yang",
      "Bozheng Li",
      "Hongyang He",
      "Zhengtao Yao",
      "Ligong Han",
      "Yingjie Victor Chen",
      "Songlin Fei",
      "Dongfang Liu",
      "Ruixiang Tang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=CAMA%3A+Enhancing+Multimodal+In-Context+Learning+with+Context-Aware+Modulated+Attention+Yanshu+Li+Jianjiang+Yang+Ziteng+Yang+Bozheng+Li+Hongyang+He",
    "gs_search_success": true,
    "gs_authors": [
      "n2v43R4AAAAJ",
      "xZk3FfsAAAAJ",
      "uICY0vEAAAAJ",
      "eTzFWiUAAAAJ",
      "OfPcaskAAAAJ",
      "cbC26HkAAAAJ",
      "lCYsNwIAAAAJ"
    ],
    "citation_count": 7,
    "gs_author_count": 11
  },
  {
    "arxiv_id": "2306.09372",
    "title": "SAFER: Situation Aware Facial Emotion Recognition",
    "year": 2023,
    "published": "2023-06-14T20:42:26Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "In this paper, we present SAFER, a novel system for emotion recognition from facial expressions. It employs state-of-the-art deep learning techniques to extract various features from facial images and incorporates contextual information, such as background and location type, to enhance its performance. The system has been designed to operate in an open-world setting, meaning it can adapt to unseen and varied facial expressions, making it suitable for real-world applications. An extensive evaluat",
    "arxiv_url": "https://arxiv.org/abs/2306.09372v1",
    "pdf_url": "https://arxiv.org/pdf/2306.09372v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2306.09372",
    "arxiv_authors": [
      "Mijanur Palash",
      "Bharat Bhargava"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=SAFER%3A+Situation+Aware+Facial+Emotion+Recognition+Mijanur+Palash+Bharat+Bhargava",
    "gs_search_success": true,
    "gs_authors": [
      "_zcC-0AAAAAJ"
    ],
    "citation_count": 3,
    "gs_author_count": 2
  },
  {
    "arxiv_id": "2306.14250",
    "title": "Introducing A Novel Method For Adaptive Thresholding In Brain Tumor Medical Image Segmentation",
    "year": 2023,
    "published": "2023-06-25T13:50:50Z",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "abstract": "One of the most significant challenges in the field of deep learning and medical image segmentation is to determine an appropriate threshold for classifying each pixel. This threshold is a value above which the model's output is considered to belong to a specific class. Manual thresholding based on personal experience is error-prone and time-consuming, particularly for complex problems such as medical images. Traditional methods for thresholding are not effective for determining the threshold va",
    "arxiv_url": "https://arxiv.org/abs/2306.14250v2",
    "pdf_url": "https://arxiv.org/pdf/2306.14250v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2306.14250",
    "arxiv_authors": [
      "Ali Fayzi",
      "Mohammad Fayzi",
      "Mostafa Forotan"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Introducing+A+Novel+Method+For+Adaptive+Thresholding+In+Brain+Tumor+Medical+Image+Segmentation+Ali+Fayzi+Mohammad+Fayzi+Mostafa+Forotan",
    "gs_search_success": true,
    "gs_authors": [
      "VwvZ5KUAAAAJ"
    ],
    "citation_count": 2,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2405.13278",
    "title": "Single color digital H&E staining with In-and-Out Net",
    "year": 2024,
    "published": "2024-05-22T01:17:27Z",
    "categories": [
      "cs.CV",
      "physics.med-ph"
    ],
    "abstract": "Virtual staining streamlines traditional staining procedures by digitally generating stained images from unstained or differently stained images. While conventional staining methods involve time-consuming chemical processes, virtual staining offers an efficient and low infrastructure alternative. Leveraging microscopy-based techniques, such as confocal microscopy, researchers can expedite tissue analysis without the need for physical sectioning. However, interpreting grayscale or pseudo-color mi",
    "arxiv_url": "https://arxiv.org/abs/2405.13278v2",
    "pdf_url": "https://arxiv.org/pdf/2405.13278v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2405.13278",
    "arxiv_authors": [
      "Mengkun Chen",
      "Yen-Tung Liu",
      "Fadeel Sher Khan",
      "Matthew C. Fox",
      "Jason S. Reichenberg",
      "Fabiana C. P. S. Lopes",
      "Katherine R. Sebastian",
      "Mia K. Markey",
      "James W. Tunnell"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Single+color+digital+H%26E+staining+with+In-and-Out+Net+Mengkun+Chen+Yen-Tung+Liu+Fadeel+Sher+Khan+Matthew+C.+Fox+Jason+S.+Reichenberg",
    "gs_search_success": true,
    "gs_authors": [
      "lECfimQAAAAJ",
      "lnQrcR8AAAAJ",
      "TUclTjMAAAAJ"
    ],
    "citation_count": 8,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2304.02291",
    "title": "Trap-Based Pest Counting: Multiscale and Deformable Attention CenterNet Integrating Internal LR and HR Joint Feature Learning",
    "year": 2023,
    "published": "2023-04-05T08:23:17Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Pest counting, which predicts the number of pests in the early stage, is very important because it enables rapid pest control, reduces damage to crops, and improves productivity. In recent years, light traps have been increasingly used to lure and photograph pests for pest counting. However, pest images have a wide range of variability in pest appearance owing to severe occlusion, wide pose variation, and even scale variation. This makes pest counting more challenging. To address these issues, t",
    "arxiv_url": "https://arxiv.org/abs/2304.02291v1",
    "pdf_url": "https://arxiv.org/pdf/2304.02291v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2304.02291",
    "arxiv_authors": [
      "Jae-Hyeon Lee",
      "Chang-Hwan Son"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Trap-Based+Pest+Counting%3A+Multiscale+and+Deformable+Attention+CenterNet+Integrating+Internal+LR+and+HR+Joint+Feature+Learning+Jae-Hyeon+Lee+Chang-Hwan+Son",
    "gs_search_success": true,
    "gs_authors": [
      "8P9OsEgAAAAJ",
      "YELDOMkAAAAJ"
    ],
    "citation_count": 15,
    "gs_author_count": 2
  },
  {
    "arxiv_id": "2502.13023",
    "title": "Detection and Geographic Localization of Natural Objects in the Wild: A Case Study on Palms",
    "year": 2025,
    "published": "2025-02-18T16:43:11Z",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "Palms are ecologically and economically indicators of tropical forest health, biodiversity, and human impact that support local economies and global forest product supply chains. While palm detection in plantations is well-studied, efforts to map naturally occurring palms in dense forests remain limited by overlapping crowns, uneven shading, and heterogeneous landscapes. We develop PRISM (Processing, Inference, Segmentation, and Mapping), a flexible pipeline for detecting and localizing palms in",
    "arxiv_url": "https://arxiv.org/abs/2502.13023v2",
    "pdf_url": "https://arxiv.org/pdf/2502.13023v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2502.13023",
    "arxiv_authors": [
      "Kangning Cui",
      "Rongkun Zhu",
      "Manqi Wang",
      "Wei Tang",
      "Gregory D. Larsen",
      "Victor P. Pauca",
      "Sarra Alqahtani",
      "Fan Yang",
      "David Segurado",
      "David Lutz",
      "Jean-Michel Morel",
      "Miles R. Silman"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Detection+and+Geographic+Localization+of+Natural+Objects+in+the+Wild%3A+A+Case+Study+on+Palms+Kangning+Cui+Rongkun+Zhu+Manqi+Wang+Wei+Tang+Gregory+D.+Larsen",
    "gs_search_success": true,
    "gs_authors": [
      "RXFeW-8AAAAJ",
      "vzCZaIwAAAAJ",
      "ujCeUW0AAAAJ",
      "D-IHI1kAAAAJ",
      "BQgfBPQAAAAJ",
      "4QgVZaAAAAAJ"
    ],
    "citation_count": 15,
    "gs_author_count": 9
  },
  {
    "arxiv_id": "2412.06234",
    "title": "Generative Densification: Learning to Densify Gaussians for High-Fidelity Generalizable 3D Reconstruction",
    "year": 2024,
    "published": "2024-12-09T06:20:51Z",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "abstract": "Generalized feed-forward Gaussian models have achieved significant progress in sparse-view 3D reconstruction by leveraging prior knowledge from large multi-view datasets. However, these models often struggle to represent high-frequency details due to the limited number of Gaussians. While the densification strategy used in per-scene 3D Gaussian splatting (3D-GS) optimization can be adapted to the feed-forward models, it may not be ideally suited for generalized scenarios. In this paper, we propo",
    "arxiv_url": "https://arxiv.org/abs/2412.06234v3",
    "pdf_url": "https://arxiv.org/pdf/2412.06234v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2412.06234",
    "arxiv_authors": [
      "Seungtae Nam",
      "Xiangyu Sun",
      "Gyeongjin Kang",
      "Younggeun Lee",
      "Seungjun Oh",
      "Eunbyung Park"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Generative+Densification%3A+Learning+to+Densify+Gaussians+for+High-Fidelity+Generalizable+3D+Reconstruction+Seungtae+Nam+Xiangyu+Sun+Gyeongjin+Kang+Younggeun+Lee+Seungjun+Oh",
    "gs_search_success": true,
    "gs_authors": [
      "iPyuJmQAAAAJ",
      "VLzxTrAAAAAJ",
      "HMnjBk0AAAAJ",
      "8NKPmmwCmrAC",
      "iyQ16vIAAAAJ",
      "E4iEBFsAAAAJ"
    ],
    "citation_count": 6,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2406.13413",
    "title": "Recurrent Inference Machine for Medical Image Registration",
    "year": 2024,
    "published": "2024-06-19T10:06:35Z",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "abstract": "Image registration is essential for medical image applications where alignment of voxels across multiple images is needed for qualitative or quantitative analysis. With recent advancements in deep neural networks and parallel computing, deep learning-based medical image registration methods become competitive with their flexible modelling and fast inference capabilities. However, compared to traditional optimization-based registration methods, the speed advantage may come at the cost of registra",
    "arxiv_url": "https://arxiv.org/abs/2406.13413v2",
    "pdf_url": "https://arxiv.org/pdf/2406.13413v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2406.13413",
    "arxiv_authors": [
      "Yi Zhang",
      "Yidong Zhao",
      "Hui Xue",
      "Peter Kellman",
      "Stefan Klein",
      "Qian Tao"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Recurrent+Inference+Machine+for+Medical+Image+Registration+Yi+Zhang+Yidong+Zhao+Hui+Xue+Peter+Kellman+Stefan+Klein",
    "gs_search_success": true,
    "gs_authors": [
      "iaAFK0MAAAAJ",
      "wpXfORUAAAAJ",
      "a8iTwl4AAAAJ",
      "ruoIu7QAAAAJ"
    ],
    "citation_count": 2,
    "gs_author_count": null
  },
  {
    "arxiv_id": "2406.04426",
    "title": "DeTra: A Unified Model for Object Detection and Trajectory Forecasting",
    "year": 2024,
    "published": "2024-06-06T18:12:04Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "cs.RO"
    ],
    "abstract": "The tasks of object detection and trajectory forecasting play a crucial role in understanding the scene for autonomous driving. These tasks are typically executed in a cascading manner, making them prone to compounding errors. Furthermore, there is usually a very thin interface between the two tasks, creating a lossy information bottleneck. To address these challenges, our approach formulates the union of the two tasks as a trajectory refinement problem, where the first pose is the detection (cu",
    "arxiv_url": "https://arxiv.org/abs/2406.04426v2",
    "pdf_url": "https://arxiv.org/pdf/2406.04426v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2406.04426",
    "arxiv_authors": [
      "Sergio Casas",
      "Ben Agro",
      "Jiageng Mao",
      "Thomas Gilles",
      "Alexander Cui",
      "Thomas Li",
      "Raquel Urtasun"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=DeTra%3A+A+Unified+Model+for+Object+Detection+and+Trajectory+Forecasting+Sergio+Casas+Ben+Agro+Jiageng+Mao+Thomas+Gilles+Alexander+Cui",
    "gs_search_success": true,
    "gs_authors": [
      "FzHJoMcAAAAJ",
      "IrPWvM0AAAAJ",
      "lYnBOcYAAAAJ",
      "5S9eZbcAAAAJ",
      "jyxO2akAAAAJ",
      "Vgo1x9YAAAAJ",
      "Bk4LuGYAAAAJ"
    ],
    "citation_count": 14,
    "gs_author_count": 7
  },
  {
    "arxiv_id": "2412.07721",
    "title": "ObjCtrl-2.5D: Training-free Object Control with Camera Poses",
    "year": 2024,
    "published": "2024-12-10T18:14:30Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "This study aims to achieve more precise and versatile object control in image-to-video (I2V) generation. Current methods typically represent the spatial movement of target objects with 2D trajectories, which often fail to capture user intention and frequently produce unnatural results. To enhance control, we present ObjCtrl-2.5D, a training-free object control approach that uses a 3D trajectory, extended from a 2D trajectory with depth information, as a control signal. By modeling object movemen",
    "arxiv_url": "https://arxiv.org/abs/2412.07721v2",
    "pdf_url": "https://arxiv.org/pdf/2412.07721v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2412.07721",
    "arxiv_authors": [
      "Zhouxia Wang",
      "Yushi Lan",
      "Shangchen Zhou",
      "Chen Change Loy"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=ObjCtrl-2.5D%3A+Training-free+Object+Control+with+Camera+Poses+Zhouxia+Wang+Yushi+Lan+Shangchen+Zhou+Chen+Change+Loy",
    "gs_search_success": false,
    "gs_authors": [],
    "citation_count": null,
    "gs_author_count": null,
    "error_type": "fetch_failed"
  },
  {
    "arxiv_id": "2402.02346",
    "title": "Closed-Loop Unsupervised Representation Disentanglement with $β$-VAE Distillation and Diffusion Probabilistic Feedback",
    "year": 2024,
    "published": "2024-02-04T05:03:22Z",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "Representation disentanglement may help AI fundamentally understand the real world and thus benefit both discrimination and generation tasks. It currently has at least three unresolved core issues: (i) heavy reliance on label annotation and synthetic data -- causing poor generalization on natural scenarios; (ii) heuristic/hand-craft disentangling constraints make it hard to adaptively achieve an optimal training trade-off; (iii) lacking reasonable evaluation metric, especially for the real label",
    "arxiv_url": "https://arxiv.org/abs/2402.02346v2",
    "pdf_url": "https://arxiv.org/pdf/2402.02346v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2402.02346",
    "arxiv_authors": [
      "Xin Jin",
      "Bohan Li",
      "BAAO Xie",
      "Wenyao Zhang",
      "Jinming Liu",
      "Ziqiang Li",
      "Tao Yang",
      "Wenjun Zeng"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Closed-Loop+Unsupervised+Representation+Disentanglement+with+%24%CE%B2%24-VAE+Distillation+and+Diffusion+Probabilistic+Feedback+Xin+Jin+Bohan+Li+BAAO+Xie+Wenyao+Zhang+Jinming+Liu",
    "gs_search_success": false,
    "gs_authors": [],
    "citation_count": null,
    "gs_author_count": null,
    "error_type": "fetch_failed"
  },
  {
    "arxiv_id": "2504.00526",
    "title": "High-Quality Pseudo-Label Generation Based on Visual Prompt Assisted Cloud Model Update",
    "year": 2025,
    "published": "2025-04-01T08:20:16Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Generating high-quality pseudo-labels on the cloud is crucial for cloud-edge object detection, especially in dynamic traffic monitoring where data distributions evolve. Existing methods often assume reliable cloud models, neglecting potential errors or struggling with complex distribution shifts. This paper proposes Cloud-Adaptive High-Quality Pseudo-label generation (CA-HQP), addressing these limitations by incorporating a learnable Visual Prompt Generator (VPG) and dual feature alignment into ",
    "arxiv_url": "https://arxiv.org/abs/2504.00526v1",
    "pdf_url": "https://arxiv.org/pdf/2504.00526v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2504.00526",
    "arxiv_authors": [
      "Xinrun Xu",
      "Qiuhong Zhang",
      "Jianwen Yang",
      "Zhanbiao Lian",
      "Jin Yan",
      "Zhiming Ding",
      "Shan Jiang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=High-Quality+Pseudo-Label+Generation+Based+on+Visual+Prompt+Assisted+Cloud+Model+Update+Xinrun+Xu+Qiuhong+Zhang+Jianwen+Yang+Zhanbiao+Lian+Jin+Yan",
    "gs_search_success": true,
    "gs_authors": [
      "LteiqOIAAAAJ",
      "X4J3klUAAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 7
  },
  {
    "arxiv_id": "2412.15491",
    "title": "GCA-3D: Towards Generalized and Consistent Domain Adaptation of 3D Generators",
    "year": 2024,
    "published": "2024-12-20T02:13:11Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Recently, 3D generative domain adaptation has emerged to adapt the pre-trained generator to other domains without collecting massive datasets and camera pose distributions. Typically, they leverage large-scale pre-trained text-to-image diffusion models to synthesize images for the target domain and then fine-tune the 3D model. However, they suffer from the tedious pipeline of data generation, which inevitably introduces pose bias between the source domain and synthetic dataset. Furthermore, they",
    "arxiv_url": "https://arxiv.org/abs/2412.15491v1",
    "pdf_url": "https://arxiv.org/pdf/2412.15491v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2412.15491",
    "arxiv_authors": [
      "Hengjia Li",
      "Yang Liu",
      "Yibo Zhao",
      "Haoran Cheng",
      "Yang Yang",
      "Linxuan Xia",
      "Zekai Luo",
      "Qibo Qiu",
      "Boxi Wu",
      "Tu Zheng",
      "Zheng Yang",
      "Deng Cai"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=GCA-3D%3A+Towards+Generalized+and+Consistent+Domain+Adaptation+of+3D+Generators+Hengjia+Li+Yang+Liu+Yibo+Zhao+Haoran+Cheng+Yang+Yang",
    "gs_search_success": true,
    "gs_authors": [
      "4s389vQAAAAJ",
      "AqDe35sAAAAJ",
      "wrP3avMAAAAJ",
      "Fq7jXZ4AAAAJ",
      "h2RiSdMAAAAJ",
      "FstE4t0AAAAJ"
    ],
    "citation_count": 3,
    "gs_author_count": 12
  },
  {
    "arxiv_id": "2501.01722",
    "title": "AR4D: Autoregressive 4D Generation from Monocular Videos",
    "year": 2025,
    "published": "2025-01-03T09:27:36Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Recent advancements in generative models have ignited substantial interest in dynamic 3D content creation (\\ie, 4D generation). Existing approaches primarily rely on Score Distillation Sampling (SDS) to infer novel-view videos, typically leading to issues such as limited diversity, spatial-temporal inconsistency and poor prompt alignment, due to the inherent randomness of SDS. To tackle these problems, we propose AR4D, a novel paradigm for SDS-free 4D generation. Specifically, our paradigm consi",
    "arxiv_url": "https://arxiv.org/abs/2501.01722v1",
    "pdf_url": "https://arxiv.org/pdf/2501.01722v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2501.01722",
    "arxiv_authors": [
      "Hanxin Zhu",
      "Tianyu He",
      "Xiqian Yu",
      "Junliang Guo",
      "Zhibo Chen",
      "Jiang Bian"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=AR4D%3A+Autoregressive+4D+Generation+from+Monocular+Videos+Hanxin+Zhu+Tianyu+He+Xiqian+Yu+Junliang+Guo+Zhibo+Chen",
    "gs_search_success": true,
    "gs_authors": [
      "CKWKIscAAAAJ",
      "P08KU1YAAAAJ",
      "MbVZAGQAAAAJ",
      "1ayDJfsAAAAJ",
      "pZBEnY8AAAAJ",
      "S88C9ewAAAAJ"
    ],
    "citation_count": 11,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2406.17547",
    "title": "Detection of Synthetic Face Images: Accuracy, Robustness, Generalization",
    "year": 2024,
    "published": "2024-06-25T13:34:50Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "An experimental study on detecting synthetic face images is presented. We collected a dataset, called FF5, of five fake face image generators, including recent diffusion models. We find that a simple model trained on a specific image generator can achieve near-perfect accuracy in separating synthetic and real images. The model handles common image distortions (reduced resolution, compression) by using data augmentation. Moreover, partial manipulations, where synthetic images are blended into rea",
    "arxiv_url": "https://arxiv.org/abs/2406.17547v2",
    "pdf_url": "https://arxiv.org/pdf/2406.17547v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2406.17547",
    "arxiv_authors": [
      "Nela Petrzelkova",
      "Jan Cech"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Detection+of+Synthetic+Face+Images%3A+Accuracy%2C+Robustness%2C+Generalization+Nela+Petrzelkova+Jan+Cech",
    "gs_search_success": false,
    "gs_authors": [],
    "citation_count": null,
    "gs_author_count": null,
    "error_type": "fetch_failed"
  },
  {
    "arxiv_id": "2504.08654",
    "title": "The Invisible EgoHand: 3D Hand Forecasting through EgoBody Pose Estimation",
    "year": 2025,
    "published": "2025-04-11T15:58:31Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Forecasting hand motion and pose from an egocentric perspective is essential for understanding human intention. However, existing methods focus solely on predicting positions without considering articulation, and only when the hands are visible in the field of view. This limitation overlooks the fact that approximate hand positions can still be inferred even when they are outside the camera's view. In this paper, we propose a method to forecast the 3D trajectories and poses of both hands from an",
    "arxiv_url": "https://arxiv.org/abs/2504.08654v1",
    "pdf_url": "https://arxiv.org/pdf/2504.08654v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2504.08654",
    "arxiv_authors": [
      "Masashi Hatano",
      "Zhifan Zhu",
      "Hideo Saito",
      "Dima Damen"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=The+Invisible+EgoHand%3A+3D+Hand+Forecasting+through+EgoBody+Pose+Estimation+Masashi+Hatano+Zhifan+Zhu+Hideo+Saito+Dima+Damen",
    "gs_search_success": true,
    "gs_authors": [
      "DNhPag0AAAAJ",
      "JU9x-bcAAAAJ",
      "9_7rBUIAAAAJ",
      "OxL9Wn8AAAAJ"
    ],
    "citation_count": 3,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2503.00695",
    "title": "MoSFormer: Augmenting Temporal Context with Memory of Surgery for Surgical Phase Recognition",
    "year": 2025,
    "published": "2025-03-02T02:26:21Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Surgical phase recognition from video enables various downstream applications. Transformer-based sliding window approaches have set the state-of-the-art by capturing rich spatial-temporal features. However, while transformers can theoretically handle arbitrary-length sequences, in practice they are limited by memory and compute constraints, resulting in fixed context windows that struggle with maintaining temporal consistency across lengthy surgical procedures. This often leads to fragmented pre",
    "arxiv_url": "https://arxiv.org/abs/2503.00695v1",
    "pdf_url": "https://arxiv.org/pdf/2503.00695v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2503.00695",
    "arxiv_authors": [
      "Hao Ding",
      "Xu Lian",
      "Mathias Unberath"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=MoSFormer%3A+Augmenting+Temporal+Context+with+Memory+of+Surgery+for+Surgical+Phase+Recognition+Hao+Ding+Xu+Lian+Mathias+Unberath",
    "gs_search_success": true,
    "gs_authors": [
      "1bd5mdIAAAAJ",
      "NIP-G-cAAAAJ"
    ],
    "citation_count": 3,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2505.09529",
    "title": "Contactless Cardiac Pulse Monitoring Using Event Cameras",
    "year": 2025,
    "published": "2025-05-14T16:24:22Z",
    "categories": [
      "cs.CV",
      "cs.ET",
      "cs.LG",
      "eess.IV"
    ],
    "abstract": "Time event cameras are a novel technology for recording scene information at extremely low latency and with low power consumption. Event cameras output a stream of events that encapsulate pixel-level light intensity changes within the scene, capturing information with a higher dynamic range and temporal resolution than traditional cameras. This study investigates the contact-free reconstruction of an individual's cardiac pulse signal from time event recording of their face using a supervised con",
    "arxiv_url": "https://arxiv.org/abs/2505.09529v2",
    "pdf_url": "https://arxiv.org/pdf/2505.09529v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2505.09529",
    "arxiv_authors": [
      "Mohamed Moustafa",
      "Joseph Lemley",
      "Peter Corcoran"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Contactless+Cardiac+Pulse+Monitoring+Using+Event+Cameras+Mohamed+Moustafa+Joseph+Lemley+Peter+Corcoran",
    "gs_search_success": true,
    "gs_authors": [
      "J6YWBB4AAAAJ",
      "ciZE8sIAAAAJ",
      "QGdReUEAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2310.03661",
    "title": "Robustness-Guided Image Synthesis for Data-Free Quantization",
    "year": 2023,
    "published": "2023-10-05T16:39:14Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Quantization has emerged as a promising direction for model compression. Recently, data-free quantization has been widely studied as a promising method to avoid privacy concerns, which synthesizes images as an alternative to real training data. Existing methods use classification loss to ensure the reliability of the synthesized images. Unfortunately, even if these images are well-classified by the pre-trained model, they still suffer from low semantics and homogenization issues. Intuitively, th",
    "arxiv_url": "https://arxiv.org/abs/2310.03661v3",
    "pdf_url": "https://arxiv.org/pdf/2310.03661v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2310.03661",
    "arxiv_authors": [
      "Jianhong Bai",
      "Yuchen Yang",
      "Huanpeng Chu",
      "Hualiang Wang",
      "Zuozhu Liu",
      "Ruizhe Chen",
      "Xiaoxuan He",
      "Lianrui Mu",
      "Chengfei Cai",
      "Haoji Hu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Robustness-Guided+Image+Synthesis+for+Data-Free+Quantization+Jianhong+Bai+Yuchen+Yang+Huanpeng+Chu+Hualiang+Wang+Zuozhu+Liu",
    "gs_search_success": true,
    "gs_authors": [
      "Wr2K2sMAAAAJ",
      "4lzd8NsAAAAJ",
      "h602wLIAAAAJ",
      "Q8EDNIgAAAAJ",
      "dCik-2YAAAAJ",
      "aiy5G0oAAAAJ",
      "U926UgYAAAAJ"
    ],
    "citation_count": 8,
    "gs_author_count": 10
  },
  {
    "arxiv_id": "2312.07623",
    "title": "Supervised Contrastive Learning for Fine-grained Chromosome Recognition",
    "year": 2023,
    "published": "2023-12-12T06:12:21Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Chromosome recognition is an essential task in karyotyping, which plays a vital role in birth defect diagnosis and biomedical research. However, existing classification methods face significant challenges due to the inter-class similarity and intra-class variation of chromosomes. To address this issue, we propose a supervised contrastive learning strategy that is tailored to train model-agnostic deep networks for reliable chromosome classification. This method enables extracting fine-grained chr",
    "arxiv_url": "https://arxiv.org/abs/2312.07623v1",
    "pdf_url": "https://arxiv.org/pdf/2312.07623v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2312.07623",
    "arxiv_authors": [
      "Ruijia Chang",
      "Suncheng Xiang",
      "Chengyu Zhou",
      "Kui Su",
      "Dahong Qian",
      "Jun Wang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Supervised+Contrastive+Learning+for+Fine-grained+Chromosome+Recognition+Ruijia+Chang+Suncheng+Xiang+Chengyu+Zhou+Kui+Su+Dahong+Qian",
    "gs_search_success": true,
    "gs_authors": [
      "oymfXUIAAAAJ",
      "IyrSeG8AAAAJ"
    ],
    "citation_count": 2,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2412.18178",
    "title": "VisionGRU: A Linear-Complexity RNN Model for Efficient Image Analysis",
    "year": 2024,
    "published": "2024-12-24T05:27:11Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs) are two dominant models for image analysis. While CNNs excel at extracting multi-scale features and ViTs effectively capture global dependencies, both suffer from high computational costs, particularly when processing high-resolution images. Recently, state-space models (SSMs) and recurrent neural networks (RNNs) have attracted attention due to their efficiency. However, their performance in image classification tasks remains li",
    "arxiv_url": "https://arxiv.org/abs/2412.18178v2",
    "pdf_url": "https://arxiv.org/pdf/2412.18178v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2412.18178",
    "arxiv_authors": [
      "Shicheng Yin",
      "Kaixuan Yin",
      "Weixing Chen",
      "Enbo Huang",
      "Yang Liu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=VisionGRU%3A+A+Linear-Complexity+RNN+Model+for+Efficient+Image+Analysis+Shicheng+Yin+Kaixuan+Yin+Weixing+Chen+Enbo+Huang+Yang+Liu",
    "gs_search_success": true,
    "gs_authors": [
      "iuIS7c8AAAAJ",
      "l0z2QNQAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2412.15574",
    "title": "J-EDI QA: Benchmark for deep-sea organism-specific multimodal LLM",
    "year": 2024,
    "published": "2024-12-20T05:11:51Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Japan Agency for Marine-Earth Science and Technology (JAMSTEC) has made available the JAMSTEC Earth Deep-sea Image (J-EDI), a deep-sea video and image archive (https://www.godac.jamstec.go.jp/jedi/e/index.html). This archive serves as a valuable resource for researchers and scholars interested in deep-sea imagery. The dataset comprises images and videos of deep-sea phenomena, predominantly of marine organisms, but also of the seafloor and physical processes. In this study, we propose J-EDI QA, a",
    "arxiv_url": "https://arxiv.org/abs/2412.15574v1",
    "pdf_url": "https://arxiv.org/pdf/2412.15574v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2412.15574",
    "arxiv_authors": [
      "Takero Yoshida",
      "Yuikazu Ito",
      "Yoshihiro Fujiwara",
      "Shinji Tsuchida",
      "Daisuke Sugiyama",
      "Daisuke Matsuoka"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=J-EDI+QA%3A+Benchmark+for+deep-sea+organism-specific+multimodal+LLM+Takero+Yoshida+Yuikazu+Ito+Yoshihiro+Fujiwara+Shinji+Tsuchida+Daisuke+Sugiyama",
    "gs_search_success": true,
    "gs_authors": [
      "RKZ-kk0AAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2403.06124",
    "title": "PSS-BA: LiDAR Bundle Adjustment with Progressive Spatial Smoothing",
    "year": 2024,
    "published": "2024-03-10T07:56:54Z",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "abstract": "Accurate and consistent construction of point clouds from LiDAR scanning data is fundamental for 3D modeling applications. Current solutions, such as multiview point cloud registration and LiDAR bundle adjustment, predominantly depend on the local plane assumption, which may be inadequate in complex environments lacking of planar geometries or substantial initial pose errors. To mitigate this problem, this paper presents a LiDAR bundle adjustment with progressive spatial smoothing, which is suit",
    "arxiv_url": "https://arxiv.org/abs/2403.06124v2",
    "pdf_url": "https://arxiv.org/pdf/2403.06124v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2403.06124",
    "arxiv_authors": [
      "Jianping Li",
      "Thien-Minh Nguyen",
      "Shenghai Yuan",
      "Lihua Xie"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=PSS-BA%3A+LiDAR+Bundle+Adjustment+with+Progressive+Spatial+Smoothing+Jianping+Li+Thien-Minh+Nguyen+Shenghai+Yuan+Lihua+Xie",
    "gs_search_success": true,
    "gs_authors": [
      "Fmrv3J8AAAAJ",
      "XcV_sesAAAAJ",
      "fDY335cAAAAJ"
    ],
    "citation_count": 12,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2401.15893",
    "title": "Arbitrary-Scale Downscaling of Tidal Current Data Using Implicit Continuous Representation",
    "year": 2024,
    "published": "2024-01-29T05:16:19Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Numerical models have long been used to understand geoscientific phenomena, including tidal currents, crucial for renewable energy production and coastal engineering. However, their computational cost hinders generating data of varying resolutions. As an alternative, deep learning-based downscaling methods have gained traction due to their faster inference speeds. But most of them are limited to only inference fixed scale and overlook important characteristics of target geoscientific data. In th",
    "arxiv_url": "https://arxiv.org/abs/2401.15893v2",
    "pdf_url": "https://arxiv.org/pdf/2401.15893v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2401.15893",
    "arxiv_authors": [
      "Dongheon Lee",
      "Seungmyong Jeong",
      "Youngmin Ro"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Arbitrary-Scale+Downscaling+of+Tidal+Current+Data+Using+Implicit+Continuous+Representation+Dongheon+Lee+Seungmyong+Jeong+Youngmin+Ro",
    "gs_search_success": true,
    "gs_authors": [
      "-2MnHEIAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2505.18664",
    "title": "Memory-Efficient Super-Resolution of 3D Micro-CT Images Using Octree-Based GANs: Enhancing Resolution and Segmentation Accuracy",
    "year": 2025,
    "published": "2025-05-24T11:57:08Z",
    "categories": [
      "eess.IV",
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "We present a memory-efficient algorithm for significantly enhancing the quality of segmented 3D micro-Computed Tomography (micro-CT) images of rocks using a generative model. The proposed model achieves a 16x increase in resolution and corrects inaccuracies in segmentation caused by the overlapping X-ray attenuation in micro-CT measurements across different minerals. The generative model employed is a 3D Octree-based convolutional Wasserstein generative adversarial network with gradient penalty.",
    "arxiv_url": "https://arxiv.org/abs/2505.18664v1",
    "pdf_url": "https://arxiv.org/pdf/2505.18664v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2505.18664",
    "arxiv_authors": [
      "Evgeny Ugolkov",
      "Xupeng He",
      "Hyung Kwak",
      "Hussein Hoteit"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Memory-Efficient+Super-Resolution+of+3D+Micro-CT+Images+Using+Octree-Based+GANs%3A+Enhancing+Resolution+and+Segmentation+Accuracy+Evgeny+Ugolkov+Xupeng+He+Hyung+Kwak+Hussein+Hoteit",
    "gs_search_success": true,
    "gs_authors": [
      "e6KK_P4AAAAJ",
      "WOh8qIYAAAAJ",
      "vtU-5dYAAAAJ"
    ],
    "citation_count": 2,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2503.11905",
    "title": "Upcycling Text-to-Image Diffusion Models for Multi-Task Capabilities",
    "year": 2025,
    "published": "2025-03-14T22:19:20Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Text-to-image synthesis has witnessed remarkable advancements in recent years. Many attempts have been made to adopt text-to-image models to support multiple tasks. However, existing approaches typically require resource-intensive re-training or additional parameters to accommodate for the new tasks, which makes the model inefficient for on-device deployment. We propose Multi-Task Upcycling (MTU), a simple yet effective recipe that extends the capabilities of a pre-trained text-to-image diffusio",
    "arxiv_url": "https://arxiv.org/abs/2503.11905v1",
    "pdf_url": "https://arxiv.org/pdf/2503.11905v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2503.11905",
    "arxiv_authors": [
      "Ruchika Chavhan",
      "Abhinav Mehrotra",
      "Malcolm Chadwick",
      "Alberto Gil Ramos",
      "Luca Morreale",
      "Mehdi Noroozi",
      "Sourav Bhattacharya"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Upcycling+Text-to-Image+Diffusion+Models+for+Multi-Task+Capabilities+Ruchika+Chavhan+Abhinav+Mehrotra+Malcolm+Chadwick+Alberto+Gil+Ramos+Luca+Morreale",
    "gs_search_success": true,
    "gs_authors": [
      "GjRLhK8AAAAJ",
      "AbeyFKwAAAAJ",
      "m8p3WAEAAAAJ",
      "vWDTlWoAAAAJ",
      "EU-ESvsAAAAJ",
      "NbW68EAAAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 7
  },
  {
    "arxiv_id": "2504.18127",
    "title": "Salient Region-Guided Spacecraft Image Arbitrary-Scale Super-Resolution Network",
    "year": 2025,
    "published": "2025-04-25T07:23:13Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Spacecraft image super-resolution seeks to enhance low-resolution spacecraft images into high-resolution ones. Although existing arbitrary-scale super-resolution methods perform well on general images, they tend to overlook the difference in features between the spacecraft core region and the large black space background, introducing irrelevant noise. In this paper, we propose a salient region-guided spacecraft image arbitrary-scale super-resolution network (SGSASR), which uses features from the",
    "arxiv_url": "https://arxiv.org/abs/2504.18127v1",
    "pdf_url": "https://arxiv.org/pdf/2504.18127v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2504.18127",
    "arxiv_authors": [
      "Jingfan Yang",
      "Hu Gao",
      "Ying Zhang",
      "Depeng Dang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Salient+Region-Guided+Spacecraft+Image+Arbitrary-Scale+Super-Resolution+Network+Jingfan+Yang+Hu+Gao+Ying+Zhang+Depeng+Dang",
    "gs_search_success": true,
    "gs_authors": [
      "5Fodd58AAAAJ",
      "QZdFO9IAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2407.17780",
    "title": "HF-Fed: Hierarchical based customized Federated Learning Framework for X-Ray Imaging",
    "year": 2024,
    "published": "2024-07-25T05:21:48Z",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "abstract": "In clinical applications, X-ray technology is vital for noninvasive examinations like mammography, providing essential anatomical information. However, the radiation risk associated with X-ray procedures raises concerns. X-ray reconstruction is crucial in medical imaging for detailed visual representations of internal structures, aiding diagnosis and treatment without invasive procedures. Recent advancements in deep learning (DL) have shown promise in X-ray reconstruction, but conventional DL me",
    "arxiv_url": "https://arxiv.org/abs/2407.17780v1",
    "pdf_url": "https://arxiv.org/pdf/2407.17780v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2407.17780",
    "arxiv_authors": [
      "Tajamul Ashraf",
      "Tisha Madame"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=HF-Fed%3A+Hierarchical+based+customized+Federated+Learning+Framework+for+X-Ray+Imaging+Tajamul+Ashraf+Tisha+Madame",
    "gs_search_success": true,
    "gs_authors": [
      "n6fSkQ4AAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 2
  },
  {
    "arxiv_id": "2410.22194",
    "title": "ADAM: An Embodied Causal Agent in Open-World Environments",
    "year": 2024,
    "published": "2024-10-29T16:32:01Z",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.CV"
    ],
    "abstract": "In open-world environments like Minecraft, existing agents face challenges in continuously learning structured knowledge, particularly causality. These challenges stem from the opacity inherent in black-box models and an excessive reliance on prior knowledge during training, which impair their interpretability and generalization capability. To this end, we introduce ADAM, An emboDied causal Agent in Minecraft, that can autonomously navigate the open world, perceive multimodal contexts, learn cau",
    "arxiv_url": "https://arxiv.org/abs/2410.22194v1",
    "pdf_url": "https://arxiv.org/pdf/2410.22194v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2410.22194",
    "arxiv_authors": [
      "Shu Yu",
      "Chaochao Lu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=ADAM%3A+An+Embodied+Causal+Agent+in+Open-World+Environments+Shu+Yu+Chaochao+Lu",
    "gs_search_success": true,
    "gs_authors": [
      "yWAvO4sAAAAJ",
      "C_Qxt0IAAAAJ"
    ],
    "citation_count": 12,
    "gs_author_count": 2
  },
  {
    "arxiv_id": "2402.01220",
    "title": "Delving into Decision-based Black-box Attacks on Semantic Segmentation",
    "year": 2024,
    "published": "2024-02-02T08:42:45Z",
    "categories": [
      "cs.CV",
      "cs.CR"
    ],
    "abstract": "Semantic segmentation is a fundamental visual task that finds extensive deployment in applications with security-sensitive considerations. Nonetheless, recent work illustrates the adversarial vulnerability of semantic segmentation models to white-box attacks. However, its adversarial robustness against black-box attacks has not been fully explored. In this paper, we present the first exploration of black-box decision-based attacks on semantic segmentation. First, we analyze the challenges that s",
    "arxiv_url": "https://arxiv.org/abs/2402.01220v1",
    "pdf_url": "https://arxiv.org/pdf/2402.01220v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2402.01220",
    "arxiv_authors": [
      "Zhaoyu Chen",
      "Zhengyang Shan",
      "Jingwen Chang",
      "Kaixun Jiang",
      "Dingkang Yang",
      "Yiting Cheng",
      "Wenqiang Zhang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Delving+into+Decision-based+Black-box+Attacks+on+Semantic+Segmentation+Zhaoyu+Chen+Zhengyang+Shan+Jingwen+Chang+Kaixun+Jiang+Dingkang+Yang",
    "gs_search_success": true,
    "gs_authors": [
      "jvlDhkcAAAAJ",
      "gGTpBIMAAAAJ",
      "1FbMihMAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 7
  },
  {
    "arxiv_id": "2305.18371",
    "title": "ColibriUAV: An Ultra-Fast, Energy-Efficient Neuromorphic Edge Processing UAV-Platform with Event-Based and Frame-Based Cameras",
    "year": 2023,
    "published": "2023-05-27T23:08:22Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.AR",
      "eess.SY"
    ],
    "abstract": "The interest in dynamic vision sensor (DVS)-powered unmanned aerial vehicles (UAV) is raising, especially due to the microsecond-level reaction time of the bio-inspired event sensor, which increases robustness and reduces latency of the perception tasks compared to a RGB camera. This work presents ColibriUAV, a UAV platform with both frame-based and event-based cameras interfaces for efficient perception and near-sensor processing. The proposed platform is designed around Kraken, a novel low-pow",
    "arxiv_url": "https://arxiv.org/abs/2305.18371v1",
    "pdf_url": "https://arxiv.org/pdf/2305.18371v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2305.18371",
    "arxiv_authors": [
      "Sizhen Bian",
      "Lukas Schulthess",
      "Georg Rutishauser",
      "Alfio Di Mauro",
      "Luca Benini",
      "Michele Magno"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=ColibriUAV%3A+An+Ultra-Fast%2C+Energy-Efficient+Neuromorphic+Edge+Processing+UAV-Platform+with+Event-Based+and+Frame-Based+Cameras+Sizhen+Bian+Lukas+Schulthess+Georg+Rutishauser+Alfio+Di+Mauro+Luca+Benini",
    "gs_search_success": true,
    "gs_authors": [
      "nVuY91QAAAAJ",
      "ytj7UUcAAAAJ",
      "8riq3sYAAAAJ",
      "lkVy1RQAAAAJ",
      "hbqQIB8AAAAJ",
      "_I5aa68AAAAJ"
    ],
    "citation_count": 17,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2308.08935",
    "title": "SDDNet: Style-guided Dual-layer Disentanglement Network for Shadow Detection",
    "year": 2023,
    "published": "2023-08-17T12:10:51Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Despite significant progress in shadow detection, current methods still struggle with the adverse impact of background color, which may lead to errors when shadows are present on complex backgrounds. Drawing inspiration from the human visual system, we treat the input shadow image as a composition of a background layer and a shadow layer, and design a Style-guided Dual-layer Disentanglement Network (SDDNet) to model these layers independently. To achieve this, we devise a Feature Separation and ",
    "arxiv_url": "https://arxiv.org/abs/2308.08935v2",
    "pdf_url": "https://arxiv.org/pdf/2308.08935v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2308.08935",
    "arxiv_authors": [
      "Runmin Cong",
      "Yuchen Guan",
      "Jinpeng Chen",
      "Wei Zhang",
      "Yao Zhao",
      "Sam Kwong"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=SDDNet%3A+Style-guided+Dual-layer+Disentanglement+Network+for+Shadow+Detection+Runmin+Cong+Yuchen+Guan+Jinpeng+Chen+Wei+Zhang+Yao+Zhao",
    "gs_search_success": true,
    "gs_authors": [
      "-VrKJ0EAAAAJ",
      "qCWuPHsAAAAJ",
      "eXXePDsAAAAJ",
      "HdnFJ5kAAAAJ",
      "_PVI6EAAAAAJ",
      "XIkIJrgAAAAJ"
    ],
    "citation_count": 14,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2310.06385",
    "title": "3DS-SLAM: A 3D Object Detection based Semantic SLAM towards Dynamic Indoor Environments",
    "year": 2023,
    "published": "2023-10-10T07:48:40Z",
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "abstract": "The existence of variable factors within the environment can cause a decline in camera localization accuracy, as it violates the fundamental assumption of a static environment in Simultaneous Localization and Mapping (SLAM) algorithms. Recent semantic SLAM systems towards dynamic environments either rely solely on 2D semantic information, or solely on geometric information, or combine their results in a loosely integrated manner. In this research paper, we introduce 3DS-SLAM, 3D Semantic SLAM, t",
    "arxiv_url": "https://arxiv.org/abs/2310.06385v1",
    "pdf_url": "https://arxiv.org/pdf/2310.06385v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2310.06385",
    "arxiv_authors": [
      "Ghanta Sai Krishna",
      "Kundrapu Supriya",
      "Sabur Baidya"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=3DS-SLAM%3A+A+3D+Object+Detection+based+Semantic+SLAM+towards+Dynamic+Indoor+Environments+Ghanta+Sai+Krishna+Kundrapu+Supriya+Sabur+Baidya",
    "gs_search_success": true,
    "gs_authors": [
      "lrK_Y8AAAAAJ",
      "UY1UAKUAAAAJ",
      "GbWTxv0AAAAJ"
    ],
    "citation_count": 8,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2404.18062",
    "title": "Compressed Image Captioning using CNN-based Encoder-Decoder Framework",
    "year": 2024,
    "published": "2024-04-28T03:47:48Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "In today's world, image processing plays a crucial role across various fields, from scientific research to industrial applications. But one particularly exciting application is image captioning. The potential impact of effective image captioning is vast. It can significantly boost the accuracy of search engines, making it easier to find relevant information. Moreover, it can greatly enhance accessibility for visually impaired individuals, providing them with a more immersive experience of digita",
    "arxiv_url": "https://arxiv.org/abs/2404.18062v1",
    "pdf_url": "https://arxiv.org/pdf/2404.18062v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2404.18062",
    "arxiv_authors": [
      "Md Alif Rahman Ridoy",
      "M Mahmud Hasan",
      "Shovon Bhowmick"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Compressed+Image+Captioning+using+CNN-based+Encoder-Decoder+Framework+Md+Alif+Rahman+Ridoy+M+Mahmud+Hasan+Shovon+Bhowmick",
    "gs_search_success": true,
    "gs_authors": [
      "X1_QjHUAAAAJ",
      "PayqUYAAAAAJ"
    ],
    "citation_count": 2,
    "gs_author_count": null
  },
  {
    "arxiv_id": "2403.12457",
    "title": "Privacy-Preserving Face Recognition Using Trainable Feature Subtraction",
    "year": 2024,
    "published": "2024-03-19T05:27:52Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "The widespread adoption of face recognition has led to increasing privacy concerns, as unauthorized access to face images can expose sensitive personal information. This paper explores face image protection against viewing and recovery attacks. Inspired by image compression, we propose creating a visually uninformative face image through feature subtraction between an original face and its model-produced regeneration. Recognizable identity features within the image are encouraged by co-training ",
    "arxiv_url": "https://arxiv.org/abs/2403.12457v1",
    "pdf_url": "https://arxiv.org/pdf/2403.12457v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2403.12457",
    "arxiv_authors": [
      "Yuxi Mi",
      "Zhizhou Zhong",
      "Yuge Huang",
      "Jiazhen Ji",
      "Jianqing Xu",
      "Jun Wang",
      "Shaoming Wang",
      "Shouhong Ding",
      "Shuigeng Zhou"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Privacy-Preserving+Face+Recognition+Using+Trainable+Feature+Subtraction+Yuxi+Mi+Zhizhou+Zhong+Yuge+Huang+Jiazhen+Ji+Jianqing+Xu",
    "gs_search_success": true,
    "gs_authors": [
      "t88nyvsAAAAJ",
      "yAE-Av4AAAAJ",
      "R9w5G9cAAAAJ",
      "4tnOTrQAAAAJ",
      "OGf40fkAAAAJ",
      "AOtSxy8AAAAJ"
    ],
    "citation_count": 39,
    "gs_author_count": 9
  },
  {
    "arxiv_id": "2405.16848",
    "title": "A re-calibration method for object detection with multi-modal alignment bias in autonomous driving",
    "year": 2024,
    "published": "2024-05-27T05:46:37Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Multi-modal object detection in autonomous driving has achieved great breakthroughs due to the usage of fusing complementary information from different sensors. The calibration in fusion between sensors such as LiDAR and camera was always supposed to be precise in previous work. However, in reality, calibration matrices are fixed when the vehicles leave the factory, but mechanical vibration, road bumps, and data lags may cause calibration bias. As there is relatively limited research on the impa",
    "arxiv_url": "https://arxiv.org/abs/2405.16848v3",
    "pdf_url": "https://arxiv.org/pdf/2405.16848v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2405.16848",
    "arxiv_authors": [
      "Zhihang Song",
      "Dingyi Yao",
      "Ruibo Ming",
      "Lihui Peng",
      "Danya Yao",
      "Yi Zhang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=A+re-calibration+method+for+object+detection+with+multi-modal+alignment+bias+in+autonomous+driving+Zhihang+Song+Dingyi+Yao+Ruibo+Ming+Lihui+Peng+Danya+Yao",
    "gs_search_success": true,
    "gs_authors": [
      "7gmd_rgAAAAJ",
      "Wb4wx8kAAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2312.06106",
    "title": "AUGCAL: Improving Sim2Real Adaptation by Uncertainty Calibration on Augmented Synthetic Images",
    "year": 2023,
    "published": "2023-12-11T04:24:11Z",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "Synthetic data (SIM) drawn from simulators have emerged as a popular alternative for training models where acquiring annotated real-world images is difficult. However, transferring models trained on synthetic images to real-world applications can be challenging due to appearance disparities. A commonly employed solution to counter this SIM2REAL gap is unsupervised domain adaptation, where models are trained using labeled SIM data and unlabeled REAL data. Mispredictions made by such SIM2REAL adap",
    "arxiv_url": "https://arxiv.org/abs/2312.06106v3",
    "pdf_url": "https://arxiv.org/pdf/2312.06106v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2312.06106",
    "arxiv_authors": [
      "Prithvijit Chattopadhyay",
      "Bharat Goyal",
      "Boglarka Ecsedi",
      "Viraj Prabhu",
      "Judy Hoffman"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=AUGCAL%3A+Improving+Sim2Real+Adaptation+by+Uncertainty+Calibration+on+Augmented+Synthetic+Images+Prithvijit+Chattopadhyay+Bharat+Goyal+Boglarka+Ecsedi+Viraj+Prabhu+Judy+Hoffman",
    "gs_search_success": true,
    "gs_authors": [
      "eoMip2gAAAAJ",
      "mqpjAt4AAAAJ",
      "rIK7AMkAAAAJ",
      "1E7m_VsAAAAJ"
    ],
    "citation_count": 3,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2308.10603",
    "title": "A step towards understanding why classification helps regression",
    "year": 2023,
    "published": "2023-08-21T10:00:46Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "A number of computer vision deep regression approaches report improved results when adding a classification loss to the regression loss. Here, we explore why this is useful in practice and when it is beneficial. To do so, we start from precisely controlled dataset variations and data samplings and find that the effect of adding a classification loss is the most pronounced for regression with imbalanced data. We explain these empirical findings by formalizing the relation between the balanced and",
    "arxiv_url": "https://arxiv.org/abs/2308.10603v1",
    "pdf_url": "https://arxiv.org/pdf/2308.10603v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2308.10603",
    "arxiv_authors": [
      "Silvia L. Pintea",
      "Yancong Lin",
      "Jouke Dijkstra",
      "Jan C. van Gemert"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=A+step+towards+understanding+why+classification+helps+regression+Silvia+L.+Pintea+Yancong+Lin+Jouke+Dijkstra+Jan+C.+van+Gemert",
    "gs_search_success": true,
    "gs_authors": [
      "JUdMRGcAAAAJ",
      "shTkx9EAAAAJ",
      "rFd-QnsAAAAJ",
      "VCzj5q8AAAAJ"
    ],
    "citation_count": 22,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2505.14346",
    "title": "Egocentric Action-aware Inertial Localization in Point Clouds with Vision-Language Guidance",
    "year": 2025,
    "published": "2025-05-20T13:29:33Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "This paper presents a novel inertial localization framework named Egocentric Action-aware Inertial Localization (EAIL), which leverages egocentric action cues from head-mounted IMU signals to localize the target individual within a 3D point cloud. Human inertial localization is challenging due to IMU sensor noise that causes trajectory drift over time. The diversity of human actions further complicates IMU signal processing by introducing various motion patterns. Nevertheless, we observe that so",
    "arxiv_url": "https://arxiv.org/abs/2505.14346v2",
    "pdf_url": "https://arxiv.org/pdf/2505.14346v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2505.14346",
    "arxiv_authors": [
      "Mingfang Zhang",
      "Ryo Yonetani",
      "Yifei Huang",
      "Liangyang Ouyang",
      "Ruicong Liu",
      "Yoichi Sato"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Egocentric+Action-aware+Inertial+Localization+in+Point+Clouds+with+Vision-Language+Guidance+Mingfang+Zhang+Ryo+Yonetani+Yifei+Huang+Liangyang+Ouyang+Ruicong+Liu",
    "gs_search_success": true,
    "gs_authors": [
      "KnQO5GcAAAAJ",
      "RU8gNcgAAAAJ",
      "DYXnRWEAAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2408.14977",
    "title": "LN-Gen: Rectal Lymph Nodes Generation via Anatomical Features",
    "year": 2024,
    "published": "2024-08-27T11:40:23Z",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "abstract": "Accurate segmentation of rectal lymph nodes is crucial for the staging and treatment planning of rectal cancer. However, the complexity of the surrounding anatomical structures and the scarcity of annotated data pose significant challenges. This study introduces a novel lymph node synthesis technique aimed at generating diverse and realistic synthetic rectal lymph node samples to mitigate the reliance on manual annotation. Unlike direct diffusion methods, which often produce masks that are disco",
    "arxiv_url": "https://arxiv.org/abs/2408.14977v1",
    "pdf_url": "https://arxiv.org/pdf/2408.14977v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2408.14977",
    "arxiv_authors": [
      "Weidong Guo",
      "Hantao Zhang",
      "Shouhong Wan",
      "Bingbing Zou",
      "Wanqin Wang",
      "Peiquan Jin"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=LN-Gen%3A+Rectal+Lymph+Nodes+Generation+via+Anatomical+Features+Weidong+Guo+Hantao+Zhang+Shouhong+Wan+Bingbing+Zou+Wanqin+Wang",
    "gs_search_success": true,
    "gs_authors": [
      "7RJH908AAAAJ",
      "ySMPv1kAAAAJ",
      "XwVRvLIAAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2404.18423",
    "title": "OCK: Unsupervised Dynamic Video Prediction with Object-Centric Kinematics",
    "year": 2024,
    "published": "2024-04-29T04:47:23Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Human perception involves decomposing complex multi-object scenes into time-static object appearance (i.e., size, shape, color) and time-varying object motion (i.e., position, velocity, acceleration). For machines to achieve human-like intelligence in real-world interactions, understanding these physical properties of objects is essential, forming the foundation for dynamic video prediction. While recent advancements in object-centric transformers have demonstrated potential in video prediction,",
    "arxiv_url": "https://arxiv.org/abs/2404.18423v3",
    "pdf_url": "https://arxiv.org/pdf/2404.18423v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2404.18423",
    "arxiv_authors": [
      "Yeon-Ji Song",
      "Jaein Kim",
      "Suhyung Choi",
      "Jin-Hwa Kim",
      "Byoung-Tak Zhang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=OCK%3A+Unsupervised+Dynamic+Video+Prediction+with+Object-Centric+Kinematics+Yeon-Ji+Song+Jaein+Kim+Suhyung+Choi+Jin-Hwa+Kim+Byoung-Tak+Zhang",
    "gs_search_success": true,
    "gs_authors": [
      "jPcpT2MAAAAJ",
      "CK_unFIAAAAJ",
      "3f2wPekAAAAJ",
      "sYTUOu8AAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2401.13998",
    "title": "WAL-Net: Weakly supervised auxiliary task learning network for carotid plaques classification",
    "year": 2024,
    "published": "2024-01-25T07:53:54Z",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "abstract": "The classification of carotid artery ultrasound images is a crucial means for diagnosing carotid plaques, holding significant clinical relevance for predicting the risk of stroke. Recent research suggests that utilizing plaque segmentation as an auxiliary task for classification can enhance performance by leveraging the correlation between segmentation and classification tasks. However, this approach relies on obtaining a substantial amount of challenging-to-acquire segmentation annotations. Thi",
    "arxiv_url": "https://arxiv.org/abs/2401.13998v2",
    "pdf_url": "https://arxiv.org/pdf/2401.13998v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2401.13998",
    "arxiv_authors": [
      "Haitao Gan",
      "Lingchao Fu",
      "Ran Zhou",
      "Weiyan Gan",
      "Furong Wang",
      "Xiaoyan Wu",
      "Zhi Yang",
      "Zhongwei Huang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=WAL-Net%3A+Weakly+supervised+auxiliary+task+learning+network+for+carotid+plaques+classification+Haitao+Gan+Lingchao+Fu+Ran+Zhou+Weiyan+Gan+Furong+Wang",
    "gs_search_success": false,
    "gs_authors": [],
    "citation_count": null,
    "gs_author_count": null,
    "error_type": "fetch_failed"
  },
  {
    "arxiv_id": "2401.05676",
    "title": "Exploring Self- and Cross-Triplet Correlations for Human-Object Interaction Detection",
    "year": 2024,
    "published": "2024-01-11T05:38:24Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Human-Object Interaction (HOI) detection plays a vital role in scene understanding, which aims to predict the HOI triplet in the form of <human, object, action>. Existing methods mainly extract multi-modal features (e.g., appearance, object semantics, human pose) and then fuse them together to directly predict HOI triplets. However, most of these methods focus on seeking for self-triplet aggregation, but ignore the potential cross-triplet dependencies, resulting in ambiguity of action prediction",
    "arxiv_url": "https://arxiv.org/abs/2401.05676v1",
    "pdf_url": "https://arxiv.org/pdf/2401.05676v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2401.05676",
    "arxiv_authors": [
      "Weibo Jiang",
      "Weihong Ren",
      "Jiandong Tian",
      "Liangqiong Qu",
      "Zhiyong Wang",
      "Honghai Liu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Exploring+Self-+and+Cross-Triplet+Correlations+for+Human-Object+Interaction+Detection+Weibo+Jiang+Weihong+Ren+Jiandong+Tian+Liangqiong+Qu+Zhiyong+Wang",
    "gs_search_success": true,
    "gs_authors": [
      "bsabUhgAAAAJ",
      "Pns4bfMAAAAJ",
      "yu56w28AAAAJ",
      "GBTBGdEAAAAJ",
      "ruKpgzwAAAAJ"
    ],
    "citation_count": 12,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2304.14291",
    "title": "EDAPS: Enhanced Domain-Adaptive Panoptic Segmentation",
    "year": 2023,
    "published": "2023-04-27T15:51:19Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "With autonomous industries on the rise, domain adaptation of the visual perception stack is an important research direction due to the cost savings promise. Much prior art was dedicated to domain-adaptive semantic segmentation in the synthetic-to-real context. Despite being a crucial output of the perception stack, panoptic segmentation has been largely overlooked by the domain adaptation community. Therefore, we revisit well-performing domain adaptation strategies from other fields, adapt them ",
    "arxiv_url": "https://arxiv.org/abs/2304.14291v2",
    "pdf_url": "https://arxiv.org/pdf/2304.14291v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2304.14291",
    "arxiv_authors": [
      "Suman Saha",
      "Lukas Hoyer",
      "Anton Obukhov",
      "Dengxin Dai",
      "Luc Van Gool"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=EDAPS%3A+Enhanced+Domain-Adaptive+Panoptic+Segmentation+Suman+Saha+Lukas+Hoyer+Anton+Obukhov+Dengxin+Dai+Luc+Van+Gool",
    "gs_search_success": true,
    "gs_authors": [
      "EyE8nngAAAAJ",
      "rw4oNkYAAAAJ",
      "dTS80eQAAAAJ",
      "TwMib_QAAAAJ"
    ],
    "citation_count": 21,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2412.10452",
    "title": "Structurally Consistent MRI Colorization using Cross-modal Fusion Learning",
    "year": 2024,
    "published": "2024-12-12T06:40:14Z",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "abstract": "Medical image colorization can greatly enhance the interpretability of the underlying imaging modality and provide insights into human anatomy. The objective of medical image colorization is to transfer a diverse spectrum of colors distributed across human anatomy from Cryosection data to source MRI data while retaining the structures of the MRI. To achieve this, we propose a novel architecture for structurally consistent color transfer to the source MRI data. Our architecture fuses segmentation",
    "arxiv_url": "https://arxiv.org/abs/2412.10452v1",
    "pdf_url": "https://arxiv.org/pdf/2412.10452v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2412.10452",
    "arxiv_authors": [
      "Mayuri Mathur",
      "Anav Chaudhary",
      "Saurabh Kumar Gupta",
      "Ojaswa Sharma"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Structurally+Consistent+MRI+Colorization+using+Cross-modal+Fusion+Learning+Mayuri+Mathur+Anav+Chaudhary+Saurabh+Kumar+Gupta+Ojaswa+Sharma",
    "gs_search_success": true,
    "gs_authors": [
      "Op48lbsAAAAJ",
      "jikZgTsAAAAJ",
      "W4hFld0AAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2407.05255",
    "title": "Estimation of the Area and Precipitation Associated with a Tropical Cyclone Biparjoy by using Image Processing",
    "year": 2024,
    "published": "2024-07-07T04:41:27Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "The rainfall associated with Topical Cyclone(TC) contributes a major amount to the annual rainfall in India. Due to the limited research on the quantitative precipitation associated with Tropical Cyclones (TC), the prediction of the amount of precipitation and area that it may cover remains a challenge. This paper proposes an approach to estimate the accumulated precipitation and impact on affected area using Remote Sensing data. For this study, an instance of Extremely Severe Cyclonic Storm, Bi",
    "arxiv_url": "https://arxiv.org/abs/2407.05255v1",
    "pdf_url": "https://arxiv.org/pdf/2407.05255v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2407.05255",
    "arxiv_authors": [
      "Shikha Verma",
      "Kuldeep Srivastava",
      "Akhilesh Tiwari",
      "Shekhar Verma"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Estimation+of+the+Area+and+Precipitation+Associated+with+a+Tropical+Cyclone+Biparjoy+by+using+Image+Processing+Shikha+Verma+Kuldeep+Srivastava+Akhilesh+Tiwari+Shekhar+Verma",
    "gs_search_success": true,
    "gs_authors": [
      "Q40Z304AAAAJ",
      "Xj3fTpUAAAAJ",
      "Wk5Qk3oAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2305.00738",
    "title": "FCA: Taming Long-tailed Federated Medical Image Classification by Classifier Anchoring",
    "year": 2023,
    "published": "2023-05-01T09:36:48Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Limited training data and severe class imbalance impose significant challenges to developing clinically robust deep learning models. Federated learning (FL) addresses the former by enabling different medical clients to collaboratively train a deep model without sharing data. However, the class imbalance problem persists due to inter-client class distribution variations. To overcome this, we propose federated classifier anchoring (FCA) by adding a personalized classifier at each client to guide a",
    "arxiv_url": "https://arxiv.org/abs/2305.00738v1",
    "pdf_url": "https://arxiv.org/pdf/2305.00738v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2305.00738",
    "arxiv_authors": [
      "Jeffry Wicaksana",
      "Zengqiang Yan",
      "Kwang-Ting Cheng"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=FCA%3A+Taming+Long-tailed+Federated+Medical+Image+Classification+by+Classifier+Anchoring+Jeffry+Wicaksana+Zengqiang+Yan+Kwang-Ting+Cheng",
    "gs_search_success": true,
    "gs_authors": [
      "-SgpaF8AAAAJ",
      "pZerfhQAAAAJ",
      "j0-TqGAAAAAJ"
    ],
    "citation_count": 9,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2505.04622",
    "title": "PrimitiveAnything: Human-Crafted 3D Primitive Assembly Generation with Auto-Regressive Transformer",
    "year": 2025,
    "published": "2025-05-07T17:59:46Z",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "abstract": "Shape primitive abstraction, which decomposes complex 3D shapes into simple geometric elements, plays a crucial role in human visual cognition and has broad applications in computer vision and graphics. While recent advances in 3D content generation have shown remarkable progress, existing primitive abstraction methods either rely on geometric optimization with limited semantic understanding or learn from small-scale, category-specific datasets, struggling to generalize across diverse shape cate",
    "arxiv_url": "https://arxiv.org/abs/2505.04622v1",
    "pdf_url": "https://arxiv.org/pdf/2505.04622v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2505.04622",
    "arxiv_authors": [
      "Jingwen Ye",
      "Yuze He",
      "Yanning Zhou",
      "Yiqin Zhu",
      "Kaiwen Xiao",
      "Yong-Jin Liu",
      "Wei Yang",
      "Xiao Han"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=PrimitiveAnything%3A+Human-Crafted+3D+Primitive+Assembly+Generation+with+Auto-Regressive+Transformer+Jingwen+Ye+Yuze+He+Yanning+Zhou+Yiqin+Zhu+Kaiwen+Xiao",
    "gs_search_success": true,
    "gs_authors": [
      "ZH9cp50AAAAJ",
      "bYeKwD8AAAAJ",
      "XGVV3gEAAAAJ",
      "GNDtwWQAAAAJ",
      "8U0_eiEAAAAJ",
      "7blugpkAAAAJ"
    ],
    "citation_count": 3,
    "gs_author_count": 8
  },
  {
    "arxiv_id": "2311.03774",
    "title": "Meta-Adapter: An Online Few-shot Learner for Vision-Language Model",
    "year": 2023,
    "published": "2023-11-07T07:27:16Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "The contrastive vision-language pre-training, known as CLIP, demonstrates remarkable potential in perceiving open-world visual concepts, enabling effective zero-shot image recognition. Nevertheless, few-shot learning methods based on CLIP typically require offline fine-tuning of the parameters on few-shot samples, resulting in longer inference time and the risk of over-fitting in certain domains. To tackle these challenges, we propose the Meta-Adapter, a lightweight residual-style adapter, to re",
    "arxiv_url": "https://arxiv.org/abs/2311.03774v2",
    "pdf_url": "https://arxiv.org/pdf/2311.03774v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2311.03774",
    "arxiv_authors": [
      "Cheng Cheng",
      "Lin Song",
      "Ruoyi Xue",
      "Hang Wang",
      "Hongbin Sun",
      "Yixiao Ge",
      "Ying Shan"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Meta-Adapter%3A+An+Online+Few-shot+Learner+for+Vision-Language+Model+Cheng+Cheng+Lin+Song+Ruoyi+Xue+Hang+Wang+Hongbin+Sun",
    "gs_search_success": true,
    "gs_authors": [
      "TtU74NAAAAAJ",
      "4oXBp9UAAAAJ",
      "6Ra2TgQAAAAJ"
    ],
    "citation_count": 50,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2409.14316",
    "title": "MVPGS: Excavating Multi-view Priors for Gaussian Splatting from Sparse Input Views",
    "year": 2024,
    "published": "2024-09-22T05:07:20Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Recently, the Neural Radiance Field (NeRF) advancement has facilitated few-shot Novel View Synthesis (NVS), which is a significant challenge in 3D vision applications. Despite numerous attempts to reduce the dense input requirement in NeRF, it still suffers from time-consumed training and rendering processes. More recently, 3D Gaussian Splatting (3DGS) achieves real-time high-quality rendering with an explicit point-based representation. However, similar to NeRF, it tends to overfit the train vi",
    "arxiv_url": "https://arxiv.org/abs/2409.14316v1",
    "pdf_url": "https://arxiv.org/pdf/2409.14316v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2409.14316",
    "arxiv_authors": [
      "Wangze Xu",
      "Huachen Gao",
      "Shihe Shen",
      "Rui Peng",
      "Jianbo Jiao",
      "Ronggang Wang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=MVPGS%3A+Excavating+Multi-view+Priors+for+Gaussian+Splatting+from+Sparse+Input+Views+Wangze+Xu+Huachen+Gao+Shihe+Shen+Rui+Peng+Jianbo+Jiao",
    "gs_search_success": true,
    "gs_authors": [
      "HkEiMMwAAAAJ",
      "90J1X_YAAAAJ",
      "Hfz_H50AAAAJ",
      "CEEvb64AAAAJ",
      "x8zi_cEAAAAJ"
    ],
    "citation_count": 31,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2305.11875",
    "title": "FR-Net:A Light-weight FFT Residual Net For Gaze Estimation",
    "year": 2023,
    "published": "2023-05-04T12:49:07Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Gaze estimation is a crucial task in computer vision, however, existing methods suffer from high computational costs, which limit their practical deployment in resource-limited environments. In this paper, we propose a novel lightweight model, FR-Net, for accurate gaze angle estimation while significantly reducing computational complexity. FR-Net utilizes the Fast Fourier Transform (FFT) to extract gaze-relevant features in frequency domains while reducing the number of parameters. Additionally,",
    "arxiv_url": "https://arxiv.org/abs/2305.11875v1",
    "pdf_url": "https://arxiv.org/pdf/2305.11875v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2305.11875",
    "arxiv_authors": [
      "Tao Xu",
      "Bo Wu",
      "Ruilong Fan",
      "Yun Zhou",
      "Di Huang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=FR-Net%3AA+Light-weight+FFT+Residual+Net+For+Gaze+Estimation+Tao+Xu+Bo+Wu+Ruilong+Fan+Yun+Zhou+Di+Huang",
    "gs_search_success": true,
    "gs_authors": [
      "bDXedd4AAAAJ",
      "Uxh_vbMAAAAJ",
      "rTQ9XoQAAAAJ"
    ],
    "citation_count": 8,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2407.03794",
    "title": "CardioSpectrum: Comprehensive Myocardium Motion Analysis with 3D Deep Learning and Geometric Insights",
    "year": 2024,
    "published": "2024-07-04T09:57:44Z",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "abstract": "The ability to map left ventricle (LV) myocardial motion using computed tomography angiography (CTA) is essential to diagnosing cardiovascular conditions and guiding interventional procedures. Due to their inherent locality, conventional neural networks typically have difficulty predicting subtle tangential movements, which considerably lessens the level of precision at which myocardium three-dimensional (3D) mapping can be performed. Using 3D optical flow techniques and Functional Maps (FMs), w",
    "arxiv_url": "https://arxiv.org/abs/2407.03794v2",
    "pdf_url": "https://arxiv.org/pdf/2407.03794v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2407.03794",
    "arxiv_authors": [
      "Shahar Zuler",
      "Shai Tejman-Yarden",
      "Dan Raviv"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=CardioSpectrum%3A+Comprehensive+Myocardium+Motion+Analysis+with+3D+Deep+Learning+and+Geometric+Insights+Shahar+Zuler+Shai+Tejman-Yarden+Dan+Raviv",
    "gs_search_success": true,
    "gs_authors": [
      "a93gZekAAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2410.21743",
    "title": "EI-Nexus: Towards Unmediated and Flexible Inter-Modality Local Feature Extraction and Matching for Event-Image Data",
    "year": 2024,
    "published": "2024-10-29T05:10:34Z",
    "categories": [
      "cs.CV",
      "cs.RO",
      "eess.IV"
    ],
    "abstract": "Event cameras, with high temporal resolution and high dynamic range, have limited research on the inter-modality local feature extraction and matching of event-image data. We propose EI-Nexus, an unmediated and flexible framework that integrates two modality-specific keypoint extractors and a feature matcher. To achieve keypoint extraction across viewpoint and modality changes, we bring Local Feature Distillation (LFD), which transfers the viewpoint consistency from a well-learned image extracto",
    "arxiv_url": "https://arxiv.org/abs/2410.21743v1",
    "pdf_url": "https://arxiv.org/pdf/2410.21743v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2410.21743",
    "arxiv_authors": [
      "Zhonghua Yi",
      "Hao Shi",
      "Qi Jiang",
      "Kailun Yang",
      "Ze Wang",
      "Diyang Gu",
      "Yufan Zhang",
      "Kaiwei Wang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=EI-Nexus%3A+Towards+Unmediated+and+Flexible+Inter-Modality+Local+Feature+Extraction+and+Matching+for+Event-Image+Data+Zhonghua+Yi+Hao+Shi+Qi+Jiang+Kailun+Yang+Ze+Wang",
    "gs_search_success": true,
    "gs_authors": [
      "COwAoUcAAAAJ",
      "6jmjj8EAAAAJ",
      "B6xWNvgAAAAJ",
      "7iVQ_9sAAAAJ",
      "pKFqWhgAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 8
  },
  {
    "arxiv_id": "2308.12938",
    "title": "Perspective-aware Convolution for Monocular 3D Object Detection",
    "year": 2023,
    "published": "2023-08-24T17:25:36Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Monocular 3D object detection is a crucial and challenging task for autonomous driving vehicle, while it uses only a single camera image to infer 3D objects in the scene. To address the difficulty of predicting depth using only pictorial clue, we propose a novel perspective-aware convolutional layer that captures long-range dependencies in images. By enforcing convolutional kernels to extract features along the depth axis of every image pixel, we incorporates perspective information into network",
    "arxiv_url": "https://arxiv.org/abs/2308.12938v1",
    "pdf_url": "https://arxiv.org/pdf/2308.12938v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2308.12938",
    "arxiv_authors": [
      "Jia-Quan Yu",
      "Soo-Chang Pei"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Perspective-aware+Convolution+for+Monocular+3D+Object+Detection+Jia-Quan+Yu+Soo-Chang+Pei",
    "gs_search_success": true,
    "gs_authors": [],
    "citation_count": 2,
    "gs_author_count": 2
  },
  {
    "arxiv_id": "2406.08231",
    "title": "Using Deep Convolutional Neural Networks to Detect Rendered Glitches in Video Games",
    "year": 2024,
    "published": "2024-06-12T13:59:45Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "In this paper, we present a method using Deep Convolutional Neural Networks (DCNNs) to detect common glitches in video games. The problem setting consists of an image (800x800 RGB) as input to be classified into one of five defined classes, normal image, or one of four different kinds of glitches (stretched, low resolution, missing and placeholder textures). Using a supervised approach, we train a ShuffleNetV2 using generated data. This work focuses on detecting texture graphical anomalies achie",
    "arxiv_url": "https://arxiv.org/abs/2406.08231v1",
    "pdf_url": "https://arxiv.org/pdf/2406.08231v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2406.08231",
    "arxiv_authors": [
      "Carlos Garcia Ling",
      "Konrad Tollmar",
      "Linus Gisslen"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Using+Deep+Convolutional+Neural+Networks+to+Detect+Rendered+Glitches+in+Video+Games+Carlos+Garcia+Ling+Konrad+Tollmar+Linus+Gisslen",
    "gs_search_success": true,
    "gs_authors": [
      "5MTY7-wAAAAJ",
      "lXBUU7EAAAAJ"
    ],
    "citation_count": 25,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2407.01811",
    "title": "Active Human Pose Estimation via an Autonomous UAV Agent",
    "year": 2024,
    "published": "2024-07-01T21:20:52Z",
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "abstract": "One of the core activities of an active observer involves moving to secure a \"better\" view of the scene, where the definition of \"better\" is task-dependent. This paper focuses on the task of human pose estimation from videos capturing a person's activity. Self-occlusions within the scene can complicate or even prevent accurate human pose estimation. To address this, relocating the camera to a new vantage point is necessary to clarify the view, thereby improving 2D human pose estimation. This pap",
    "arxiv_url": "https://arxiv.org/abs/2407.01811v1",
    "pdf_url": "https://arxiv.org/pdf/2407.01811v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2407.01811",
    "arxiv_authors": [
      "Jingxi Chen",
      "Botao He",
      "Chahat Deep Singh",
      "Cornelia Fermuller",
      "Yiannis Aloimonos"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Active+Human+Pose+Estimation+via+an+Autonomous+UAV+Agent+Jingxi+Chen+Botao+He+Chahat+Deep+Singh+Cornelia+Fermuller+Yiannis+Aloimonos",
    "gs_search_success": true,
    "gs_authors": [
      "9ytS6o8AAAAJ",
      "7QmEsOwAAAAJ",
      "0gEOJSEAAAAJ",
      "ASaUMpsAAAAJ",
      "jcfUmocAAAAJ"
    ],
    "citation_count": 2,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2401.16224",
    "title": "Diffutoon: High-Resolution Editable Toon Shading via Diffusion Models",
    "year": 2024,
    "published": "2024-01-29T15:21:37Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Toon shading is a type of non-photorealistic rendering task of animation. Its primary purpose is to render objects with a flat and stylized appearance. As diffusion models have ascended to the forefront of image synthesis methodologies, this paper delves into an innovative form of toon shading based on diffusion models, aiming to directly render photorealistic videos into anime styles. In video stylization, extant methods encounter persistent challenges, notably in maintaining consistency and ac",
    "arxiv_url": "https://arxiv.org/abs/2401.16224v1",
    "pdf_url": "https://arxiv.org/pdf/2401.16224v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2401.16224",
    "arxiv_authors": [
      "Zhongjie Duan",
      "Chengyu Wang",
      "Cen Chen",
      "Weining Qian",
      "Jun Huang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Diffutoon%3A+High-Resolution+Editable+Toon+Shading+via+Diffusion+Models+Zhongjie+Duan+Chengyu+Wang+Cen+Chen+Weining+Qian+Jun+Huang",
    "gs_search_success": true,
    "gs_authors": [
      "3Mn4S9UAAAAJ",
      "_AVfRnQAAAAJ",
      "P84J9ikAAAAJ",
      "KqqoR6gAAAAJ"
    ],
    "citation_count": 8,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2503.00452",
    "title": "Customer Analytics using Surveillance Video",
    "year": 2025,
    "published": "2025-03-01T11:26:31Z",
    "categories": [
      "cs.CY",
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "The analysis of sales information, is a vital step in designing an effective marketing strategy. This work proposes a novel approach to analyse the shopping behaviour of customers to identify their purchase patterns. An extended version of the Multi-Cluster Overlapping k-Means Extension (MCOKE) algorithm with weighted k-Means algorithm is utilized to map customers to the garments of interest. The age & gender traits of the customer; the time spent and the expressions exhibited while selecting ga",
    "arxiv_url": "https://arxiv.org/abs/2503.00452v1",
    "pdf_url": "https://arxiv.org/pdf/2503.00452v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2503.00452",
    "arxiv_authors": [
      "Earnest Paul Ijjina",
      "Aniruddha Srinivas Joshi",
      "Goutham Kanahasabai",
      "Keerthi Priyanka P"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Customer+Analytics+using+Surveillance+Video+Earnest+Paul+Ijjina+Aniruddha+Srinivas+Joshi+Goutham+Kanahasabai+Keerthi+Priyanka+P",
    "gs_search_success": true,
    "gs_authors": [
      "OVWm3v8AAAAJ",
      "UhrX8r0AAAAJ",
      "slYE_6AAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2311.01233",
    "title": "Long Story Short: a Summarize-then-Search Method for Long Video Question Answering",
    "year": 2023,
    "published": "2023-11-02T13:36:11Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Large language models such as GPT-3 have demonstrated an impressive capability to adapt to new tasks without requiring task-specific training data. This capability has been particularly effective in settings such as narrative question answering, where the diversity of tasks is immense, but the available supervision data is small. In this work, we investigate if such language models can extend their zero-shot reasoning abilities to long multimodal narratives in multimedia content such as drama, m",
    "arxiv_url": "https://arxiv.org/abs/2311.01233v1",
    "pdf_url": "https://arxiv.org/pdf/2311.01233v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2311.01233",
    "arxiv_authors": [
      "Jiwan Chung",
      "Youngjae Yu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Long+Story+Short%3A+a+Summarize-then-Search+Method+for+Long+Video+Question+Answering+Jiwan+Chung+Youngjae+Yu",
    "gs_search_success": true,
    "gs_authors": [
      "l4UBOZAAAAAJ",
      "WDO24ZYAAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 2
  },
  {
    "arxiv_id": "2404.09474",
    "title": "TCCT-Net: Two-Stream Network Architecture for Fast and Efficient Engagement Estimation via Behavioral Feature Signals",
    "year": 2024,
    "published": "2024-04-15T06:01:48Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Engagement analysis finds various applications in healthcare, education, advertisement, services. Deep Neural Networks, used for analysis, possess complex architecture and need large amounts of input data, computational power, inference time. These constraints challenge embedding systems into devices for real-time use. To address these limitations, we present a novel two-stream feature fusion \"Tensor-Convolution and Convolution-Transformer Network\" (TCCT-Net) architecture. To better learn the me",
    "arxiv_url": "https://arxiv.org/abs/2404.09474v2",
    "pdf_url": "https://arxiv.org/pdf/2404.09474v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2404.09474",
    "arxiv_authors": [
      "Alexander Vedernikov",
      "Puneet Kumar",
      "Haoyu Chen",
      "Tapio Seppanen",
      "Xiaobai Li"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=TCCT-Net%3A+Two-Stream+Network+Architecture+for+Fast+and+Efficient+Engagement+Estimation+via+Behavioral+Feature+Signals+Alexander+Vedernikov+Puneet+Kumar+Haoyu+Chen+Tapio+Seppanen+Xiaobai+Li",
    "gs_search_success": true,
    "gs_authors": [
      "oiW3S7oAAAAJ",
      "QgbraMIAAAAJ",
      "8ieLBZ8AAAAJ",
      "MRSV_E8AAAAJ",
      "JTFfexYAAAAJ"
    ],
    "citation_count": 7,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2502.16302",
    "title": "DualNeRF: Text-Driven 3D Scene Editing via Dual-Field Representation",
    "year": 2025,
    "published": "2025-02-22T17:21:55Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Recently, denoising diffusion models have achieved promising results in 2D image generation and editing. Instruct-NeRF2NeRF (IN2N) introduces the success of diffusion into 3D scene editing through an \"Iterative dataset update\" (IDU) strategy. Though achieving fascinating results, IN2N suffers from problems of blurry backgrounds and trapping in local optima. The first problem is caused by IN2N's lack of efficient guidance for background maintenance, while the second stems from the interaction bet",
    "arxiv_url": "https://arxiv.org/abs/2502.16302v1",
    "pdf_url": "https://arxiv.org/pdf/2502.16302v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2502.16302",
    "arxiv_authors": [
      "Yuxuan Xiong",
      "Yue Shi",
      "Yishun Dou",
      "Bingbing Ni"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=DualNeRF%3A+Text-Driven+3D+Scene+Editing+via+Dual-Field+Representation+Yuxuan+Xiong+Yue+Shi+Yishun+Dou+Bingbing+Ni",
    "gs_search_success": true,
    "gs_authors": [
      "NRrwyvYAAAAJ",
      "z4EHKzwAAAAJ",
      "eUbmKwYAAAAJ",
      "BrQQHiEAAAAJ"
    ],
    "citation_count": 2,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2410.07971",
    "title": "Generalizable and Animatable Gaussian Head Avatar",
    "year": 2024,
    "published": "2024-10-10T14:29:00Z",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "abstract": "In this paper, we propose Generalizable and Animatable Gaussian head Avatar (GAGAvatar) for one-shot animatable head avatar reconstruction. Existing methods rely on neural radiance fields, leading to heavy rendering consumption and low reenactment speeds. To address these limitations, we generate the parameters of 3D Gaussians from a single image in a single forward pass. The key innovation of our work is the proposed dual-lifting method, which produces high-fidelity 3D Gaussians that capture id",
    "arxiv_url": "https://arxiv.org/abs/2410.07971v1",
    "pdf_url": "https://arxiv.org/pdf/2410.07971v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2410.07971",
    "arxiv_authors": [
      "Xuangeng Chu",
      "Tatsuya Harada"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Generalizable+and+Animatable+Gaussian+Head+Avatar+Xuangeng+Chu+Tatsuya+Harada",
    "gs_search_success": true,
    "gs_authors": [
      "k8rlJ8AAAAAJ",
      "yr4kSUsAAAAJ"
    ],
    "citation_count": 43,
    "gs_author_count": 2
  },
  {
    "arxiv_id": "2406.00334",
    "title": "Image Captioning via Dynamic Path Customization",
    "year": 2024,
    "published": "2024-06-01T07:23:21Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "This paper explores a novel dynamic network for vision and language tasks, where the inferring structure is customized on the fly for different inputs. Most previous state-of-the-art approaches are static and hand-crafted networks, which not only heavily rely on expert knowledge, but also ignore the semantic diversity of input samples, therefore resulting in suboptimal performance. To address these issues, we propose a novel Dynamic Transformer Network (DTNet) for image captioning, which dynamic",
    "arxiv_url": "https://arxiv.org/abs/2406.00334v1",
    "pdf_url": "https://arxiv.org/pdf/2406.00334v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2406.00334",
    "arxiv_authors": [
      "Yiwei Ma",
      "Jiayi Ji",
      "Xiaoshuai Sun",
      "Yiyi Zhou",
      "Xiaopeng Hong",
      "Yongjian Wu",
      "Rongrong Ji"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Image+Captioning+via+Dynamic+Path+Customization+Yiwei+Ma+Jiayi+Ji+Xiaoshuai+Sun+Yiyi+Zhou+Xiaopeng+Hong",
    "gs_search_success": true,
    "gs_authors": [
      "lRSD7PQAAAAJ",
      "w3_2ep0AAAAJ",
      "KPMK3B4AAAAJ",
      "xp_rICcAAAAJ",
      "x3X-qysAAAAJ",
      "KIDY5pUAAAAJ"
    ],
    "citation_count": 11,
    "gs_author_count": 7
  },
  {
    "arxiv_id": "2504.01591",
    "title": "Leveraging Modality Tags for Enhanced Cross-Modal Video Retrieval",
    "year": 2025,
    "published": "2025-04-02T10:56:01Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Video retrieval requires aligning visual content with corresponding natural language descriptions. In this paper, we introduce Modality Auxiliary Concepts for Video Retrieval (MAC-VR), a novel approach that leverages modality-specific tags -- automatically extracted from foundation models -- to enhance video retrieval. We propose to align modalities in a latent space, along with learning and aligning auxiliary latent concepts derived from the features of a video and its corresponding caption. We",
    "arxiv_url": "https://arxiv.org/abs/2504.01591v3",
    "pdf_url": "https://arxiv.org/pdf/2504.01591v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2504.01591",
    "arxiv_authors": [
      "Adriano Fragomeni",
      "Dima Damen",
      "Michael Wray"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Leveraging+Modality+Tags+for+Enhanced+Cross-Modal+Video+Retrieval+Adriano+Fragomeni+Dima+Damen+Michael+Wray",
    "gs_search_success": true,
    "gs_authors": [
      "OxL9Wn8AAAAJ",
      "gFQcKZMAAAAJ",
      "ImHVnvsAAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2311.04079",
    "title": "Augmenting Lane Perception and Topology Understanding with Standard Definition Navigation Maps",
    "year": 2023,
    "published": "2023-11-07T15:42:22Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Autonomous driving has traditionally relied heavily on costly and labor-intensive High Definition (HD) maps, hindering scalability. In contrast, Standard Definition (SD) maps are more affordable and have worldwide coverage, offering a scalable alternative. In this work, we systematically explore the effect of SD maps for real-time lane-topology understanding. We propose a novel framework to integrate SD maps into online map prediction and propose a Transformer-based encoder, SD Map Encoder Repre",
    "arxiv_url": "https://arxiv.org/abs/2311.04079v1",
    "pdf_url": "https://arxiv.org/pdf/2311.04079v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2311.04079",
    "arxiv_authors": [
      "Katie Z Luo",
      "Xinshuo Weng",
      "Yan Wang",
      "Shuang Wu",
      "Jie Li",
      "Kilian Q Weinberger",
      "Yue Wang",
      "Marco Pavone"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Augmenting+Lane+Perception+and+Topology+Understanding+with+Standard+Definition+Navigation+Maps+Katie+Z+Luo+Xinshuo+Weng+Yan+Wang+Shuang+Wu+Jie+Li",
    "gs_search_success": true,
    "gs_authors": [
      "RhOpyXcAAAAJ",
      "8RVWMycAAAAJ",
      "LKk1jdMAAAAJ",
      "v-AEFIEAAAAJ",
      "qlmK27YAAAAJ",
      "nZsD8XwAAAAJ",
      "dthSEsoAAAAJ"
    ],
    "citation_count": 57,
    "gs_author_count": 8
  },
  {
    "arxiv_id": "2408.00343",
    "title": "IN-Sight: Interactive Navigation through Sight",
    "year": 2024,
    "published": "2024-08-01T07:27:54Z",
    "categories": [
      "cs.RO",
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "Current visual navigation systems often treat the environment as static, lacking the ability to adaptively interact with obstacles. This limitation leads to navigation failure when encountering unavoidable obstructions. In response, we introduce IN-Sight, a novel approach to self-supervised path planning, enabling more effective navigation strategies through interaction with obstacles. Utilizing RGB-D observations, IN-Sight calculates traversability scores and incorporates them into a semantic m",
    "arxiv_url": "https://arxiv.org/abs/2408.00343v2",
    "pdf_url": "https://arxiv.org/pdf/2408.00343v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2408.00343",
    "arxiv_authors": [
      "Philipp Schoch",
      "Fan Yang",
      "Yuntao Ma",
      "Stefan Leutenegger",
      "Marco Hutter",
      "Quentin Leboutet"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=IN-Sight%3A+Interactive+Navigation+through+Sight+Philipp+Schoch+Fan+Yang+Yuntao+Ma+Stefan+Leutenegger+Marco+Hutter",
    "gs_search_success": true,
    "gs_authors": [
      "R3OeSRoAAAAJ",
      "SmGQ48gAAAAJ",
      "WnjRS5EAAAAJ",
      "DO3quJYAAAAJ",
      "SfiqI4AAAAAJ"
    ],
    "citation_count": 5,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2504.03006",
    "title": "DiSRT-In-Bed: Diffusion-Based Sim-to-Real Transfer Framework for In-Bed Human Mesh Recovery",
    "year": 2025,
    "published": "2025-04-03T19:57:16Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "In-bed human mesh recovery can be crucial and enabling for several healthcare applications, including sleep pattern monitoring, rehabilitation support, and pressure ulcer prevention. However, it is difficult to collect large real-world visual datasets in this domain, in part due to privacy and expense constraints, which in turn presents significant challenges for training and deploying deep learning models. Existing in-bed human mesh estimation methods often rely heavily on real-world data, limi",
    "arxiv_url": "https://arxiv.org/abs/2504.03006v1",
    "pdf_url": "https://arxiv.org/pdf/2504.03006v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2504.03006",
    "arxiv_authors": [
      "Jing Gao",
      "Ce Zheng",
      "Laszlo A. Jeni",
      "Zackory Erickson"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=DiSRT-In-Bed%3A+Diffusion-Based+Sim-to-Real+Transfer+Framework+for+In-Bed+Human+Mesh+Recovery+Jing+Gao+Ce+Zheng+Laszlo+A.+Jeni+Zackory+Erickson",
    "gs_search_success": true,
    "gs_authors": [
      "YFKLC58AAAAJ",
      "Wdnc-mEAAAAJ",
      "wElkTtIAAAAJ",
      "4VXiscUAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2405.15939",
    "title": "Diversifying Human Pose in Synthetic Data for Aerial-view Human Detection",
    "year": 2024,
    "published": "2024-05-24T21:08:27Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Synthetic data generation has emerged as a promising solution to the data scarcity issue in aerial-view human detection. However, creating datasets that accurately reflect varying real-world human appearances, particularly diverse poses, remains challenging and labor-intensive. To address this, we propose SynPoseDiv, a novel framework that diversifies human poses within existing synthetic datasets. SynPoseDiv tackles two key challenges: generating realistic, diverse 3D human poses using a diffus",
    "arxiv_url": "https://arxiv.org/abs/2405.15939v2",
    "pdf_url": "https://arxiv.org/pdf/2405.15939v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2405.15939",
    "arxiv_authors": [
      "Yi-Ting Shen",
      "Hyungtae Lee",
      "Heesung Kwon",
      "Shuvra S. Bhattacharyya"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Diversifying+Human+Pose+in+Synthetic+Data+for+Aerial-view+Human+Detection+Yi-Ting+Shen+Hyungtae+Lee+Heesung+Kwon+Shuvra+S.+Bhattacharyya",
    "gs_search_success": true,
    "gs_authors": [
      "ayph-mwAAAAJ",
      "rNpUIKAAAAAJ",
      "PzjeN7MAAAAJ"
    ],
    "citation_count": 5,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2412.14424",
    "title": "FedPIA -- Permuting and Integrating Adapters leveraging Wasserstein Barycenters for Finetuning Foundation Models in Multi-Modal Federated Learning",
    "year": 2024,
    "published": "2024-12-19T00:24:00Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "abstract": "Large Vision-Language Models typically require large text and image datasets for effective fine-tuning. However, collecting data from various sites, especially in healthcare, is challenging due to strict privacy regulations. An alternative is to fine-tune these models on end-user devices, such as in medical clinics, without sending data to a server. These local clients typically have limited computing power and small datasets, which are not enough for fully fine-tuning large VLMs on their own. A",
    "arxiv_url": "https://arxiv.org/abs/2412.14424v1",
    "pdf_url": "https://arxiv.org/pdf/2412.14424v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2412.14424",
    "arxiv_authors": [
      "Pramit Saha",
      "Divyanshu Mishra",
      "Felix Wagner",
      "Konstantinos Kamnitsas",
      "J. Alison Noble"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=FedPIA+--+Permuting+and+Integrating+Adapters+leveraging+Wasserstein+Barycenters+for+Finetuning+Foundation+Models+in+Multi-Modal+Federated+Learning+Pramit+Saha+Divyanshu+Mishra+Felix+Wagner+Konstantinos+Kamnitsas+J.+Alison+Noble",
    "gs_search_success": true,
    "gs_authors": [
      "VC3rTrAAAAAJ",
      "b0tmmYMAAAAJ",
      "e1QWXJIAAAAJ",
      "rm7rbz8AAAAJ",
      "Sb7d-rsAAAAJ"
    ],
    "citation_count": 11,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2308.02490",
    "title": "MM-Vet: Evaluating Large Multimodal Models for Integrated Capabilities",
    "year": 2023,
    "published": "2023-08-04T17:59:47Z",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "We propose MM-Vet, an evaluation benchmark that examines large multimodal models (LMMs) on complicated multimodal tasks. Recent LMMs have shown various intriguing abilities, such as solving math problems written on the blackboard, reasoning about events and celebrities in news images, and explaining visual jokes. Rapid model advancements pose challenges to evaluation benchmark development. Problems include: (1) How to systematically structure and evaluate the complicated multimodal tasks; (2) Ho",
    "arxiv_url": "https://arxiv.org/abs/2308.02490v4",
    "pdf_url": "https://arxiv.org/pdf/2308.02490v4",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2308.02490",
    "arxiv_authors": [
      "Weihao Yu",
      "Zhengyuan Yang",
      "Linjie Li",
      "Jianfeng Wang",
      "Kevin Lin",
      "Zicheng Liu",
      "Xinchao Wang",
      "Lijuan Wang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=MM-Vet%3A+Evaluating+Large+Multimodal+Models+for+Integrated+Capabilities+Weihao+Yu+Zhengyuan+Yang+Linjie+Li+Jianfeng+Wang+Kevin+Lin",
    "gs_search_success": true,
    "gs_authors": [
      "bkALdvsAAAAJ",
      "LKSy1kwAAAAJ",
      "LYxjt1QAAAAJ",
      "rP02ve8AAAAJ",
      "cDcWXuIAAAAJ",
      "WR875gYAAAAJ",
      "w69Buq0AAAAJ"
    ],
    "citation_count": 1020,
    "gs_author_count": 8
  },
  {
    "arxiv_id": "2403.09616",
    "title": "Explore In-Context Segmentation via Latent Diffusion Models",
    "year": 2024,
    "published": "2024-03-14T17:52:31Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "In-context segmentation has drawn increasing attention with the advent of vision foundation models. Its goal is to segment objects using given reference images. Most existing approaches adopt metric learning or masked image modeling to build the correlation between visual prompts and input image queries. This work approaches the problem from a fresh perspective - unlocking the capability of the latent diffusion model (LDM) for in-context segmentation and investigating different design choices. S",
    "arxiv_url": "https://arxiv.org/abs/2403.09616v2",
    "pdf_url": "https://arxiv.org/pdf/2403.09616v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2403.09616",
    "arxiv_authors": [
      "Chaoyang Wang",
      "Xiangtai Li",
      "Henghui Ding",
      "Lu Qi",
      "Jiangning Zhang",
      "Yunhai Tong",
      "Chen Change Loy",
      "Shuicheng Yan"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Explore+In-Context+Segmentation+via+Latent+Diffusion+Models+Chaoyang+Wang+Xiangtai+Li+Henghui+Ding+Lu+Qi+Jiangning+Zhang",
    "gs_search_success": true,
    "gs_authors": [
      "FL3ReD0AAAAJ",
      "eaK5b2oAAAAJ",
      "559LF80AAAAJ",
      "DNuiPHwAAAAJ",
      "2hA4X9wAAAAJ",
      "WI_flSwAAAAJ",
      "SSI90d4AAAAJ",
      "T4gqdPkAAAAJ"
    ],
    "citation_count": 16,
    "gs_author_count": 8
  },
  {
    "arxiv_id": "2402.06251",
    "title": "Single Channel EEG Based Insomnia Identification Without Sleep Stage Annotations",
    "year": 2024,
    "published": "2024-02-09T08:59:37Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "This paper proposes a new approach to identifying patients with insomnia using a single EEG channel, without the need for sleep stage annotation. Data preprocessing, feature extraction, feature selection, and classification techniques are used to automatically detect insomnia based on features extracted from spectral and temporal domains, including relative power in the delta, sigma, beta and gamma bands, total power, absolute slow wave power, power ratios, mean, zero crossing rate, mobility, an",
    "arxiv_url": "https://arxiv.org/abs/2402.06251v3",
    "pdf_url": "https://arxiv.org/pdf/2402.06251v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2402.06251",
    "arxiv_authors": [
      "Chan-Yun Yang",
      "Nilantha Premakumara",
      "Hooman Samani",
      "Chinthaka Premachandra"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Single+Channel+EEG+Based+Insomnia+Identification+Without+Sleep+Stage+Annotations+Chan-Yun+Yang+Nilantha+Premakumara+Hooman+Samani+Chinthaka+Premachandra",
    "gs_search_success": true,
    "gs_authors": [
      "-jcQ3FkAAAAJ",
      "LwcEJEYAAAAJ",
      "TcFtgawAAAAJ"
    ],
    "citation_count": 2,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2404.09003",
    "title": "THQA: A Perceptual Quality Assessment Database for Talking Heads",
    "year": 2024,
    "published": "2024-04-13T13:08:57Z",
    "categories": [
      "cs.CV",
      "eess.IV"
    ],
    "abstract": "In the realm of media technology, digital humans have gained prominence due to rapid advancements in computer technology. However, the manual modeling and control required for the majority of digital humans pose significant obstacles to efficient development. The speech-driven methods offer a novel avenue for manipulating the mouth shape and expressions of digital humans. Despite the proliferation of driving methods, the quality of many generated talking head (TH) videos remains a concern, impac",
    "arxiv_url": "https://arxiv.org/abs/2404.09003v1",
    "pdf_url": "https://arxiv.org/pdf/2404.09003v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2404.09003",
    "arxiv_authors": [
      "Yingjie Zhou",
      "Zicheng Zhang",
      "Wei Sun",
      "Xiaohong Liu",
      "Xiongkuo Min",
      "Zhihua Wang",
      "Xiao-Ping Zhang",
      "Guangtao Zhai"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=THQA%3A+A+Perceptual+Quality+Assessment+Database+for+Talking+Heads+Yingjie+Zhou+Zicheng+Zhang+Wei+Sun+Xiaohong+Liu+Xiongkuo+Min",
    "gs_search_success": true,
    "gs_authors": [
      "0gKwEKMAAAAJ",
      "nDlEBJ8AAAAJ",
      "QICTEckAAAAJ",
      "85yWgIcAAAAJ",
      "1fzb_z8AAAAJ",
      "Tq2hoMQAAAAJ",
      "91sjuWIAAAAJ",
      "E6zbSYgAAAAJ"
    ],
    "citation_count": 20,
    "gs_author_count": 8
  },
  {
    "arxiv_id": "2403.20253",
    "title": "MedCLIP-SAM: Bridging Text and Image Towards Universal Medical Image Segmentation",
    "year": 2024,
    "published": "2024-03-29T15:59:11Z",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "Medical image segmentation of anatomical structures and pathology is crucial in modern clinical diagnosis, disease study, and treatment planning. To date, great progress has been made in deep learning-based segmentation techniques, but most methods still lack data efficiency, generalizability, and interactability. Consequently, the development of new, precise segmentation methods that demand fewer labeled datasets is of utmost importance in medical image analysis. Recently, the emergence of foun",
    "arxiv_url": "https://arxiv.org/abs/2403.20253v2",
    "pdf_url": "https://arxiv.org/pdf/2403.20253v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2403.20253",
    "arxiv_authors": [
      "Taha Koleilat",
      "Hojat Asgariandehkordi",
      "Hassan Rivaz",
      "Yiming Xiao"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=MedCLIP-SAM%3A+Bridging+Text+and+Image+Towards+Universal+Medical+Image+Segmentation+Taha+Koleilat+Hojat+Asgariandehkordi+Hassan+Rivaz+Yiming+Xiao",
    "gs_search_success": true,
    "gs_authors": [
      "ELHjilEAAAAJ",
      "chB2OjUAAAAJ",
      "FWDzqVUAAAAJ",
      "ndXNye4AAAAJ"
    ],
    "citation_count": 61,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2311.12704",
    "title": "Cascade Learning Localises Discriminant Features in Visual Scene Classification",
    "year": 2023,
    "published": "2023-11-21T16:19:14Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Lack of interpretability of deep convolutional neural networks (DCNN) is a well-known problem particularly in the medical domain as clinicians want trustworthy automated decisions. One way to improve trust is to demonstrate the localisation of feature representations with respect to expert labeled regions of interest. In this work, we investigate the localisation of features learned via two varied learning paradigms and demonstrate the superiority of one learning approach with respect to localis",
    "arxiv_url": "https://arxiv.org/abs/2311.12704v2",
    "pdf_url": "https://arxiv.org/pdf/2311.12704v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2311.12704",
    "arxiv_authors": [
      "Junwen Wang",
      "Katayoun Farrahi"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Cascade+Learning+Localises+Discriminant+Features+in+Visual+Scene+Classification+Junwen+Wang+Katayoun+Farrahi",
    "gs_search_success": true,
    "gs_authors": [
      "c_E3E4kAAAAJ",
      "SJqgjYsAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 2
  },
  {
    "arxiv_id": "2403.18717",
    "title": "Semi-Supervised Learning for Deep Causal Generative Models",
    "year": 2024,
    "published": "2024-03-27T16:06:37Z",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "stat.ML"
    ],
    "abstract": "Developing models that are capable of answering questions of the form \"How would x change if y had been z?'\" is fundamental to advancing medical image analysis. Training causal generative models that address such counterfactual questions, though, currently requires that all relevant variables have been observed and that the corresponding labels are available in the training data. However, clinical data may not have complete records for all patients and state of the art causal generative models a",
    "arxiv_url": "https://arxiv.org/abs/2403.18717v2",
    "pdf_url": "https://arxiv.org/pdf/2403.18717v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2403.18717",
    "arxiv_authors": [
      "Yasin Ibrahim",
      "Hermione Warr",
      "Konstantinos Kamnitsas"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Semi-Supervised+Learning+for+Deep+Causal+Generative+Models+Yasin+Ibrahim+Hermione+Warr+Konstantinos+Kamnitsas",
    "gs_search_success": true,
    "gs_authors": [
      "e1QWXJIAAAAJ",
      "w0wB2AIAAAAJ",
      "Qztkr4AAAAAJ"
    ],
    "citation_count": 2,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2406.04280",
    "title": "xMIL: Insightful Explanations for Multiple Instance Learning in Histopathology",
    "year": 2024,
    "published": "2024-06-06T17:26:40Z",
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "abstract": "Multiple instance learning (MIL) is an effective and widely used approach for weakly supervised machine learning. In histopathology, MIL models have achieved remarkable success in tasks like tumor detection, biomarker prediction, and outcome prognostication. However, MIL explanation methods are still lagging behind, as they are limited to small bag sizes or disregard instance interactions. We revisit MIL through the lens of explainable AI (XAI) and introduce xMIL, a refined framework with more g",
    "arxiv_url": "https://arxiv.org/abs/2406.04280v3",
    "pdf_url": "https://arxiv.org/pdf/2406.04280v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2406.04280",
    "arxiv_authors": [
      "Julius Hense",
      "Mina Jamshidi Idaji",
      "Oliver Eberle",
      "Thomas Schnake",
      "Jonas Dippel",
      "Laure Ciernik",
      "Oliver Buchstab",
      "Andreas Mock",
      "Frederick Klauschen",
      "Klaus-Robert Müller"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=xMIL%3A+Insightful+Explanations+for+Multiple+Instance+Learning+in+Histopathology+Julius+Hense+Mina+Jamshidi+Idaji+Oliver+Eberle+Thomas+Schnake+Jonas+Dippel",
    "gs_search_success": true,
    "gs_authors": [
      "ZLQCgRoAAAAJ",
      "MptjWzgAAAAJ",
      "ZqXZKacAAAAJ",
      "vZB4qw0AAAAJ",
      "GFLi9XIAAAAJ",
      "S3ybh7cAAAAJ"
    ],
    "citation_count": 11,
    "gs_author_count": 8
  },
  {
    "arxiv_id": "2501.07451",
    "title": "A Survey on Dynamic Neural Networks: from Computer Vision to Multi-modal Sensor Fusion",
    "year": 2025,
    "published": "2025-01-13T16:24:49Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Model compression is essential in the deployment of large Computer Vision models on embedded devices. However, static optimization techniques (e.g. pruning, quantization, etc.) neglect the fact that different inputs have different complexities, thus requiring different amount of computations. Dynamic Neural Networks allow to condition the number of computations to the specific input. The current literature on the topic is very extensive and fragmented. We present a comprehensive survey that synt",
    "arxiv_url": "https://arxiv.org/abs/2501.07451v2",
    "pdf_url": "https://arxiv.org/pdf/2501.07451v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2501.07451",
    "arxiv_authors": [
      "Fabio Montello",
      "Ronja Güldenring",
      "Simone Scardapane",
      "Lazaros Nalpantidis"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=A+Survey+on+Dynamic+Neural+Networks%3A+from+Computer+Vision+to+Multi-modal+Sensor+Fusion+Fabio+Montello+Ronja+G%C3%BCldenring+Simone+Scardapane+Lazaros+Nalpantidis",
    "gs_search_success": true,
    "gs_authors": [
      "ZYf1TaEAAAAJ",
      "aSuosYoAAAAJ",
      "YAx9230AAAAJ",
      "t2mUkG4AAAAJ"
    ],
    "citation_count": 3,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2501.17062",
    "title": "EdgeMLOps: Operationalizing ML models with Cumulocity IoT and thin-edge.io for Visual quality Inspection",
    "year": 2025,
    "published": "2025-01-28T16:40:40Z",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "abstract": "This paper introduces EdgeMLOps, a framework leveraging Cumulocity IoT and thin-edge.io for deploying and managing machine learning models on resource-constrained edge devices. We address the challenges of model optimization, deployment, and lifecycle management in edge environments. The framework's efficacy is demonstrated through a visual quality inspection (VQI) use case where images of assets are processed on edge devices, enabling real-time condition updates within an asset management syste",
    "arxiv_url": "https://arxiv.org/abs/2501.17062v1",
    "pdf_url": "https://arxiv.org/pdf/2501.17062v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2501.17062",
    "arxiv_authors": [
      "Kanishk Chaturvedi",
      "Johannes Gasthuber",
      "Mohamed Abdelaal"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=EdgeMLOps%3A+Operationalizing+ML+models+with+Cumulocity+IoT+and+thin-edge.io+for+Visual+quality+Inspection+Kanishk+Chaturvedi+Johannes+Gasthuber+Mohamed+Abdelaal",
    "gs_search_success": true,
    "gs_authors": [
      "v7Xhkf4AAAAJ",
      "cmxDCFAAAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2503.00948",
    "title": "Extrapolating and Decoupling Image-to-Video Generation Models: Motion Modeling is Easier Than You Think",
    "year": 2025,
    "published": "2025-03-02T16:06:16Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Image-to-Video (I2V) generation aims to synthesize a video clip according to a given image and condition (e.g., text). The key challenge of this task lies in simultaneously generating natural motions while preserving the original appearance of the images. However, current I2V diffusion models (I2V-DMs) often produce videos with limited motion degrees or exhibit uncontrollable motion that conflicts with the textual condition. To address these limitations, we propose a novel Extrapolating and Deco",
    "arxiv_url": "https://arxiv.org/abs/2503.00948v1",
    "pdf_url": "https://arxiv.org/pdf/2503.00948v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2503.00948",
    "arxiv_authors": [
      "Jie Tian",
      "Xiaoye Qu",
      "Zhenyi Lu",
      "Wei Wei",
      "Sichen Liu",
      "Yu Cheng"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Extrapolating+and+Decoupling+Image-to-Video+Generation+Models%3A+Motion+Modeling+is+Easier+Than+You+Think+Jie+Tian+Xiaoye+Qu+Zhenyi+Lu+Wei+Wei+Sichen+Liu",
    "gs_search_success": true,
    "gs_authors": [
      "_JaPEsAAAAAJ",
      "rT3hqdcAAAAJ",
      "ORPxbV4AAAAJ",
      "xPNDnCAAAAAJ"
    ],
    "citation_count": 6,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2412.02202",
    "title": "3D representation in 512-Byte:Variational tokenizer is the key for autoregressive 3D generation",
    "year": 2024,
    "published": "2024-12-03T06:31:25Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Autoregressive transformers have revolutionized high-fidelity image generation. One crucial ingredient lies in the tokenizer, which compresses high-resolution image patches into manageable discrete tokens with a scanning or hierarchical order suitable for large language models. Extending these tokenizers to 3D generation, however, presents a significant challenge: unlike image patches that naturally exhibit spatial sequence and multi-scale relationships, 3D data lacks an inherent order, making i",
    "arxiv_url": "https://arxiv.org/abs/2412.02202v1",
    "pdf_url": "https://arxiv.org/pdf/2412.02202v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2412.02202",
    "arxiv_authors": [
      "Jinzhi Zhang",
      "Feng Xiong",
      "Mu Xu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=3D+representation+in+512-Byte%3AVariational+tokenizer+is+the+key+for+autoregressive+3D+generation+Jinzhi+Zhang+Feng+Xiong+Mu+Xu",
    "gs_search_success": true,
    "gs_authors": [
      "zDQNEwQAAAAJ",
      "_X4MQ-gAAAAJ",
      "JGi4S0EAAAAJ"
    ],
    "citation_count": 7,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2410.21313",
    "title": "Towards Robust Out-of-Distribution Generalization: Data Augmentation and Neural Architecture Search Approaches",
    "year": 2024,
    "published": "2024-10-25T20:50:32Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "abstract": "Deep learning has been demonstrated with tremendous success in recent years. Despite so, its performance in practice often degenerates drastically when encountering out-of-distribution (OoD) data, i.e. training and test data are sampled from different distributions. In this thesis, we study ways toward robust OoD generalization for deep learning, i.e., its performance is not susceptible to distribution shift in the test data.   We first propose a novel and effective approach to disentangle the s",
    "arxiv_url": "https://arxiv.org/abs/2410.21313v1",
    "pdf_url": "https://arxiv.org/pdf/2410.21313v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2410.21313",
    "arxiv_authors": [
      "Haoyue Bai"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Towards+Robust+Out-of-Distribution+Generalization%3A+Data+Augmentation+and+Neural+Architecture+Search+Approaches+Haoyue+Bai",
    "gs_search_success": true,
    "gs_authors": [
      "KQ9HkfcAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 1
  },
  {
    "arxiv_id": "2412.16901",
    "title": "Learning to Generate Gradients for Test-Time Adaptation via Test-Time Training Layers",
    "year": 2024,
    "published": "2024-12-22T07:24:09Z",
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "abstract": "Test-time adaptation (TTA) aims to fine-tune a trained model online using unlabeled testing data to adapt to new environments or out-of-distribution data, demonstrating broad application potential in real-world scenarios. However, in this optimization process, unsupervised learning objectives like entropy minimization frequently encounter noisy learning signals. These signals produce unreliable gradients, which hinder the model ability to converge to an optimal solution quickly and introduce sig",
    "arxiv_url": "https://arxiv.org/abs/2412.16901v1",
    "pdf_url": "https://arxiv.org/pdf/2412.16901v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2412.16901",
    "arxiv_authors": [
      "Qi Deng",
      "Shuaicheng Niu",
      "Ronghao Zhang",
      "Yaofo Chen",
      "Runhao Zeng",
      "Jian Chen",
      "Xiping Hu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Learning+to+Generate+Gradients+for+Test-Time+Adaptation+via+Test-Time+Training+Layers+Qi+Deng+Shuaicheng+Niu+Ronghao+Zhang+Yaofo+Chen+Runhao+Zeng",
    "gs_search_success": true,
    "gs_authors": [
      "s3X4YHwAAAAJ",
      "gi30SZ0AAAAJ",
      "NHZCt2EAAAAJ",
      "Mc93YSEAAAAJ"
    ],
    "citation_count": 7,
    "gs_author_count": 7
  },
  {
    "arxiv_id": "2401.02649",
    "title": "Enhancing 3D-Air Signature by Pen Tip Tail Trajectory Awareness: Dataset and Featuring by Novel Spatio-temporal CNN",
    "year": 2024,
    "published": "2024-01-05T05:40:59Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "This work proposes a novel process of using pen tip and tail 3D trajectory for air signature. To acquire the trajectories we developed a new pen tool and a stereo camera was used. We proposed SliT-CNN, a novel 2D spatial-temporal convolutional neural network (CNN) for better featuring of the air signature. In addition, we also collected an air signature dataset from $45$ signers. Skilled forgery signatures per user are also collected. A detailed benchmarking of the proposed dataset using existin",
    "arxiv_url": "https://arxiv.org/abs/2401.02649v1",
    "pdf_url": "https://arxiv.org/pdf/2401.02649v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2401.02649",
    "arxiv_authors": [
      "Saurabh Atreya",
      "Maheswar Bora",
      "Aritra Mukherjee",
      "Abhijit Das"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Enhancing+3D-Air+Signature+by+Pen+Tip+Tail+Trajectory+Awareness%3A+Dataset+and+Featuring+by+Novel+Spatio-temporal+CNN+Saurabh+Atreya+Maheswar+Bora+Aritra+Mukherjee+Abhijit+Das",
    "gs_search_success": true,
    "gs_authors": [
      "jji1MhQAAAAJ",
      "FdPJeOUAAAAJ",
      "u7Byo20AAAAJ",
      "7GyxqiAAAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2405.08911",
    "title": "CLIP with Quality Captions: A Strong Pretraining for Vision Tasks",
    "year": 2024,
    "published": "2024-05-14T19:06:24Z",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "CLIP models perform remarkably well on zero-shot classification and retrieval tasks. But recent studies have shown that learnt representations in CLIP are not well suited for dense prediction tasks like object detection, semantic segmentation or depth estimation. More recently, multi-stage training methods for CLIP models was introduced to mitigate the weak performance of CLIP on downstream tasks. In this work, we find that simply improving the quality of captions in image-text datasets improves",
    "arxiv_url": "https://arxiv.org/abs/2405.08911v1",
    "pdf_url": "https://arxiv.org/pdf/2405.08911v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2405.08911",
    "arxiv_authors": [
      "Pavan Kumar Anasosalu Vasu",
      "Hadi Pouransari",
      "Fartash Faghri",
      "Oncel Tuzel"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=CLIP+with+Quality+Captions%3A+A+Strong+Pretraining+for+Vision+Tasks+Pavan+Kumar+Anasosalu+Vasu+Hadi+Pouransari+Fartash+Faghri+Oncel+Tuzel",
    "gs_search_success": true,
    "gs_authors": [
      "KUG_tG0AAAAJ",
      "Fe7NTe0AAAAJ",
      "besz69AAAAAJ",
      "F3KISxoAAAAJ"
    ],
    "citation_count": 9,
    "gs_author_count": null
  },
  {
    "arxiv_id": "2308.05976",
    "title": "Zero-shot Text-driven Physically Interpretable Face Editing",
    "year": 2023,
    "published": "2023-08-11T07:20:24Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "This paper proposes a novel and physically interpretable method for face editing based on arbitrary text prompts. Different from previous GAN-inversion-based face editing methods that manipulate the latent space of GANs, or diffusion-based methods that model image manipulation as a reverse diffusion process, we regard the face editing process as imposing vector flow fields on face images, representing the offset of spatial coordinates and color for each image pixel. Under the above-proposed para",
    "arxiv_url": "https://arxiv.org/abs/2308.05976v1",
    "pdf_url": "https://arxiv.org/pdf/2308.05976v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2308.05976",
    "arxiv_authors": [
      "Yapeng Meng",
      "Songru Yang",
      "Xu Hu",
      "Rui Zhao",
      "Lincheng Li",
      "Zhenwei Shi",
      "Zhengxia Zou"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Zero-shot+Text-driven+Physically+Interpretable+Face+Editing+Yapeng+Meng+Songru+Yang+Xu+Hu+Rui+Zhao+Lincheng+Li",
    "gs_search_success": true,
    "gs_authors": [
      "x5XTHCYAAAAJ",
      "kNhFWQIAAAAJ",
      "wYs7vogAAAAJ",
      "QOzih00AAAAJ",
      "DzwoyZsAAAAJ",
      "9Y0v8gYAAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 7
  },
  {
    "arxiv_id": "2503.02036",
    "title": "Robustness to Geographic Distribution Shift Using Location Encoders",
    "year": 2025,
    "published": "2025-03-03T20:24:07Z",
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "abstract": "Geographic distribution shift arises when the distribution of locations on Earth in a training dataset is different from what is seen at test time. The most common approaches to tackling geographic distribution shift treat regions delimited by administrative boundaries such as countries or continents as separate domains and apply standard domain adaptation methods, ignoring geographic coordinates that are often available as metadata. This paper proposes the use of location encoders for modeling ",
    "arxiv_url": "https://arxiv.org/abs/2503.02036v2",
    "pdf_url": "https://arxiv.org/pdf/2503.02036v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2503.02036",
    "arxiv_authors": [
      "Ruth Crasto"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Robustness+to+Geographic+Distribution+Shift+Using+Location+Encoders+Ruth+Crasto",
    "gs_search_success": true,
    "gs_authors": [
      "k7GBkUEAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 1
  },
  {
    "arxiv_id": "2505.01713",
    "title": "Vision and Intention Boost Large Language Model in Long-Term Action Anticipation",
    "year": 2025,
    "published": "2025-05-03T06:33:54Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Long-term action anticipation (LTA) aims to predict future actions over an extended period. Previous approaches primarily focus on learning exclusively from video data but lack prior knowledge. Recent researches leverage large language models (LLMs) by utilizing text-based inputs which suffer severe information loss. To tackle these limitations single-modality methods face, we propose a novel Intention-Conditioned Vision-Language (ICVL) model in this study that fully leverages the rich semantic ",
    "arxiv_url": "https://arxiv.org/abs/2505.01713v1",
    "pdf_url": "https://arxiv.org/pdf/2505.01713v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2505.01713",
    "arxiv_authors": [
      "Congqi Cao",
      "Lanshu Hu",
      "Yating Yu",
      "Yanning Zhang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Vision+and+Intention+Boost+Large+Language+Model+in+Long-Term+Action+Anticipation+Congqi+Cao+Lanshu+Hu+Yating+Yu+Yanning+Zhang",
    "gs_search_success": true,
    "gs_authors": [
      "fB66KE0AAAAJ"
    ],
    "citation_count": 2,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2310.00093",
    "title": "DataDAM: Efficient Dataset Distillation with Attention Matching",
    "year": 2023,
    "published": "2023-09-29T19:07:48Z",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "Researchers have long tried to minimize training costs in deep learning while maintaining strong generalization across diverse datasets. Emerging research on dataset distillation aims to reduce training costs by creating a small synthetic set that contains the information of a larger real dataset and ultimately achieves test accuracy equivalent to a model trained on the whole dataset. Unfortunately, the synthetic data generated by previous methods are not guaranteed to distribute and discriminat",
    "arxiv_url": "https://arxiv.org/abs/2310.00093v3",
    "pdf_url": "https://arxiv.org/pdf/2310.00093v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2310.00093",
    "arxiv_authors": [
      "Ahmad Sajedi",
      "Samir Khaki",
      "Ehsan Amjadian",
      "Lucy Z. Liu",
      "Yuri A. Lawryshyn",
      "Konstantinos N. Plataniotis"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=DataDAM%3A+Efficient+Dataset+Distillation+with+Attention+Matching+Ahmad+Sajedi+Samir+Khaki+Ehsan+Amjadian+Lucy+Z.+Liu+Yuri+A.+Lawryshyn",
    "gs_search_success": true,
    "gs_authors": [
      "vqZiHiMAAAAJ",
      "XUyUZY4AAAAJ",
      "W-4N_2gAAAAJ",
      "ySByvNkAAAAJ"
    ],
    "citation_count": 103,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2505.04979",
    "title": "Federated Deconfounding and Debiasing Learning for Out-of-Distribution Generalization",
    "year": 2025,
    "published": "2025-05-08T06:32:59Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Attribute bias in federated learning (FL) typically leads local models to optimize inconsistently due to the learning of non-causal associations, resulting degraded performance. Existing methods either use data augmentation for increasing sample diversity or knowledge distillation for learning invariant representations to address this problem. However, they lack a comprehensive analysis of the inference paths, and the interference from confounding factors limits their performance. To address the",
    "arxiv_url": "https://arxiv.org/abs/2505.04979v2",
    "pdf_url": "https://arxiv.org/pdf/2505.04979v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2505.04979",
    "arxiv_authors": [
      "Zhuang Qi",
      "Sijin Zhou",
      "Lei Meng",
      "Han Hu",
      "Han Yu",
      "Xiangxu Meng"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Federated+Deconfounding+and+Debiasing+Learning+for+Out-of-Distribution+Generalization+Zhuang+Qi+Sijin+Zhou+Lei+Meng+Han+Hu+Han+Yu",
    "gs_search_success": true,
    "gs_authors": [
      "Lo2qKKsAAAAJ",
      "eXgoTXMAAAAJ",
      "l7wxjVwAAAAJ"
    ],
    "citation_count": 10,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2307.07184",
    "title": "TVPR: Text-to-Video Person Retrieval and a New Benchmark",
    "year": 2023,
    "published": "2023-07-14T06:34:00Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Most existing methods for text-based person retrieval focus on text-to-image person retrieval. Nevertheless, due to the lack of dynamic information provided by isolated frames, the performance is hampered when the person is obscured or variable motion details are missed in isolated frames. To overcome this, we propose a novel Text-to-Video Person Retrieval (TVPR) task. Since there is no dataset or benchmark that describes person videos with natural language, we construct a large-scale cross-moda",
    "arxiv_url": "https://arxiv.org/abs/2307.07184v3",
    "pdf_url": "https://arxiv.org/pdf/2307.07184v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2307.07184",
    "arxiv_authors": [
      "Xu Zhang",
      "Fan Ni",
      "Guan-Nan Dong",
      "Aichun Zhu",
      "Jianhui Wu",
      "Mingcheng Ni",
      "Hui Liu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=TVPR%3A+Text-to-Video+Person+Retrieval+and+a+New+Benchmark+Xu+Zhang+Fan+Ni+Guan-Nan+Dong+Aichun+Zhu+Jianhui+Wu",
    "gs_search_success": true,
    "gs_authors": [
      "dBLBkfEAAAAJ",
      "pnr9JKMAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 7
  },
  {
    "arxiv_id": "2412.11715",
    "title": "Discrepancy-Aware Attention Network for Enhanced Audio-Visual Zero-Shot Learning",
    "year": 2024,
    "published": "2024-12-16T12:35:56Z",
    "categories": [
      "cs.CV",
      "cs.MM",
      "cs.SD",
      "eess.AS"
    ],
    "abstract": "Audio-visual Zero-Shot Learning (ZSL) has attracted significant attention for its ability to identify unseen classes and perform well in video classification tasks. However, modal imbalance in (G)ZSL leads to over-reliance on the optimal modality, reducing discriminative capabilities for unseen classes. Some studies have attempted to address this issue by modifying parameter gradients, but two challenges still remain: (a) Quality discrepancies, where modalities offer differing quantities and qua",
    "arxiv_url": "https://arxiv.org/abs/2412.11715v1",
    "pdf_url": "https://arxiv.org/pdf/2412.11715v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2412.11715",
    "arxiv_authors": [
      "RunLin Yu",
      "Yipu Gong",
      "Wenrui Li",
      "Aiwen Sun",
      "Mengren Zheng"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Discrepancy-Aware+Attention+Network+for+Enhanced+Audio-Visual+Zero-Shot+Learning+RunLin+Yu+Yipu+Gong+Wenrui+Li+Aiwen+Sun+Mengren+Zheng",
    "gs_search_success": false,
    "gs_authors": [],
    "citation_count": null,
    "gs_author_count": null,
    "error_type": "fetch_failed"
  },
  {
    "arxiv_id": "2310.13165",
    "title": "CycleNet: Rethinking Cycle Consistency in Text-Guided Diffusion for Image Manipulation",
    "year": 2023,
    "published": "2023-10-19T21:32:21Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "abstract": "Diffusion models (DMs) have enabled breakthroughs in image synthesis tasks but lack an intuitive interface for consistent image-to-image (I2I) translation. Various methods have been explored to address this issue, including mask-based methods, attention-based methods, and image-conditioning. However, it remains a critical challenge to enable unpaired I2I translation with pre-trained DMs while maintaining satisfying consistency. This paper introduces Cyclenet, a novel but simple method that incor",
    "arxiv_url": "https://arxiv.org/abs/2310.13165v2",
    "pdf_url": "https://arxiv.org/pdf/2310.13165v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2310.13165",
    "arxiv_authors": [
      "Sihan Xu",
      "Ziqiao Ma",
      "Yidong Huang",
      "Honglak Lee",
      "Joyce Chai"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=CycleNet%3A+Rethinking+Cycle+Consistency+in+Text-Guided+Diffusion+for+Image+Manipulation+Sihan+Xu+Ziqiao+Ma+Yidong+Huang+Honglak+Lee+Joyce+Chai",
    "gs_search_success": true,
    "gs_authors": [
      "fmSHtE8AAAAJ",
      "WbybssYAAAAJ",
      "zH6skxYAAAAJ",
      "SdEBnsoAAAAJ"
    ],
    "citation_count": 45,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2405.10754",
    "title": "Stable Phase Retrieval with Mirror Descent",
    "year": 2024,
    "published": "2024-05-17T13:07:14Z",
    "categories": [
      "math.OC",
      "cs.CV",
      "cs.IT"
    ],
    "abstract": "In this paper, we aim to reconstruct an n-dimensional real vector from m phaseless measurements corrupted by an additive noise. We extend the noiseless framework developed in [15], based on mirror descent (or Bregman gradient descent), to deal with noisy measurements and prove that the procedure is stable to (small enough) additive noise. In the deterministic case, we show that mirror descent converges to a critical point of the phase retrieval problem, and if the algorithm is well initialized a",
    "arxiv_url": "https://arxiv.org/abs/2405.10754v2",
    "pdf_url": "https://arxiv.org/pdf/2405.10754v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2405.10754",
    "arxiv_authors": [
      "Jean-Jacques Godeme",
      "Jalal Fadili",
      "Claude Amra",
      "Myriam Zerrad"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Stable+Phase+Retrieval+with+Mirror+Descent+Jean-Jacques+Godeme+Jalal+Fadili+Claude+Amra+Myriam+Zerrad",
    "gs_search_success": true,
    "gs_authors": [
      "puuBBP8AAAAJ",
      "cNrXGAcAAAAJ",
      "2drBRNUAAAAJ",
      "lZRU5hYAAAAJ"
    ],
    "citation_count": 4,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2305.09397",
    "title": "EXPRESSNET: An Explainable Residual Slim Network for Fingerprint Presentation Attack Detection",
    "year": 2023,
    "published": "2023-05-16T12:29:50Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Presentation attack is a challenging issue that persists in the security of automatic fingerprint recognition systems. This paper proposes a novel explainable residual slim network that detects the presentation attack by representing the visual features in the input fingerprint sample. The encoder-decoder of this network along with the channel attention block converts the input sample into its heatmap representation while the modified residual convolutional neural network classifier discriminate",
    "arxiv_url": "https://arxiv.org/abs/2305.09397v2",
    "pdf_url": "https://arxiv.org/pdf/2305.09397v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2305.09397",
    "arxiv_authors": [
      "Anuj Rai",
      "Somnath Dey"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=EXPRESSNET%3A+An+Explainable+Residual+Slim+Network+for+Fingerprint+Presentation+Attack+Detection+Anuj+Rai+Somnath+Dey",
    "gs_search_success": true,
    "gs_authors": [
      "F8a1pmQAAAAJ",
      "rjzlx8wAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 2
  },
  {
    "arxiv_id": "2406.01040",
    "title": "Synthetic Data Generation for 3D Myocardium Deformation Analysis",
    "year": 2024,
    "published": "2024-06-03T06:40:53Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Accurate analysis of 3D myocardium deformation using high-resolution computerized tomography (CT) datasets with ground truth (GT) annotations is crucial for advancing cardiovascular imaging research. However, the scarcity of such datasets poses a significant challenge for developing robust myocardium deformation analysis models. To address this, we propose a novel approach to synthetic data generation for enriching cardiovascular imaging datasets.   We introduce a synthetic data generation metho",
    "arxiv_url": "https://arxiv.org/abs/2406.01040v1",
    "pdf_url": "https://arxiv.org/pdf/2406.01040v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2406.01040",
    "arxiv_authors": [
      "Shahar Zuler",
      "Dan Raviv"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Synthetic+Data+Generation+for+3D+Myocardium+Deformation+Analysis+Shahar+Zuler+Dan+Raviv",
    "gs_search_success": true,
    "gs_authors": [
      "a93gZekAAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 2
  },
  {
    "arxiv_id": "2502.18523",
    "title": "End-to-End Deep Learning for Structural Brain Imaging: A Unified Framework",
    "year": 2025,
    "published": "2025-02-23T20:08:24Z",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "abstract": "Brain imaging analysis is fundamental in neuroscience, providing valuable insights into brain structure and function. Traditional workflows follow a sequential pipeline-brain extraction, registration, segmentation, parcellation, network generation, and classification-treating each step as an independent task. These methods rely heavily on task-specific training data and expert intervention to correct intermediate errors, making them particularly burdensome for high-dimensional neuroimaging data,",
    "arxiv_url": "https://arxiv.org/abs/2502.18523v1",
    "pdf_url": "https://arxiv.org/pdf/2502.18523v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2502.18523",
    "arxiv_authors": [
      "Yao Su",
      "Keqi Han",
      "Mingjie Zeng",
      "Lichao Sun",
      "Liang Zhan",
      "Carl Yang",
      "Lifang He",
      "Xiangnan Kong"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=End-to-End+Deep+Learning+for+Structural+Brain+Imaging%3A+A+Unified+Framework+Yao+Su+Keqi+Han+Mingjie+Zeng+Lichao+Sun+Liang+Zhan",
    "gs_search_success": true,
    "gs_authors": [
      "COrJW_gAAAAJ",
      "_r0POMcAAAAJ",
      "mOINlwcAAAAJ",
      "GxFkOs4AAAAJ",
      "Lh7VfoMAAAAJ",
      "obgTcyoAAAAJ",
      "WhGUE7AAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 8
  },
  {
    "arxiv_id": "2305.07904",
    "title": "Temporal Consistent Automatic Video Colorization via Semantic Correspondence",
    "year": 2023,
    "published": "2023-05-13T12:06:09Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Video colorization task has recently attracted wide attention. Recent methods mainly work on the temporal consistency in adjacent frames or frames with small interval. However, it still faces severe challenge of the inconsistency between frames with large interval.To address this issue, we propose a novel video colorization framework, which combines semantic correspondence into automatic video colorization to keep long-range consistency. Firstly, a reference colorization network is designed to a",
    "arxiv_url": "https://arxiv.org/abs/2305.07904v1",
    "pdf_url": "https://arxiv.org/pdf/2305.07904v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2305.07904",
    "arxiv_authors": [
      "Yu Zhang",
      "Siqi Chen",
      "Mingdao Wang",
      "Xianlin Zhang",
      "Chuang Zhu",
      "Yue Zhang",
      "Xueming Li"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Temporal+Consistent+Automatic+Video+Colorization+via+Semantic+Correspondence+Yu+Zhang+Siqi+Chen+Mingdao+Wang+Xianlin+Zhang+Chuang+Zhu",
    "gs_search_success": true,
    "gs_authors": [
      "FqGppAsAAAAJ",
      "FuCo7AkAAAAJ"
    ],
    "citation_count": 8,
    "gs_author_count": 7
  },
  {
    "arxiv_id": "2410.19538",
    "title": "Utilizing Image Transforms and Diffusion Models for Generative Modeling of Short and Long Time Series",
    "year": 2024,
    "published": "2024-10-25T13:06:18Z",
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "abstract": "Lately, there has been a surge in interest surrounding generative modeling of time series data. Most existing approaches are designed either to process short sequences or to handle long-range sequences. This dichotomy can be attributed to gradient issues with recurrent networks, computational costs associated with transformers, and limited expressiveness of state space models. Towards a unified generative model for varying-length time series, we propose in this work to transform sequences into i",
    "arxiv_url": "https://arxiv.org/abs/2410.19538v1",
    "pdf_url": "https://arxiv.org/pdf/2410.19538v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2410.19538",
    "arxiv_authors": [
      "Ilan Naiman",
      "Nimrod Berman",
      "Itai Pemper",
      "Idan Arbiv",
      "Gal Fadlon",
      "Omri Azencot"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Utilizing+Image+Transforms+and+Diffusion+Models+for+Generative+Modeling+of+Short+and+Long+Time+Series+Ilan+Naiman+Nimrod+Berman+Itai+Pemper+Idan+Arbiv+Gal+Fadlon",
    "gs_search_success": true,
    "gs_authors": [
      "MEGuRmAAAAAJ",
      "d0BVaqEAAAAJ",
      "C3_AEWQAAAAJ",
      "--u546kAAAAJ",
      "ndAi30kAAAAJ",
      "Fglytk8AAAAJ"
    ],
    "citation_count": 34,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2410.10563",
    "title": "MEGA-Bench: Scaling Multimodal Evaluation to over 500 Real-World Tasks",
    "year": 2024,
    "published": "2024-10-14T14:42:12Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "We present MEGA-Bench, an evaluation suite that scales multimodal evaluation to over 500 real-world tasks, to address the highly heterogeneous daily use cases of end users. Our objective is to optimize for a set of high-quality data samples that cover a highly diverse and rich set of multimodal tasks, while enabling cost-effective and accurate model evaluation. In particular, we collected 505 realistic tasks encompassing over 8,000 samples from 16 expert annotators to extensively cover the multi",
    "arxiv_url": "https://arxiv.org/abs/2410.10563v3",
    "pdf_url": "https://arxiv.org/pdf/2410.10563v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2410.10563",
    "arxiv_authors": [
      "Jiacheng Chen",
      "Tianhao Liang",
      "Sherman Siu",
      "Zhengqing Wang",
      "Kai Wang",
      "Yubo Wang",
      "Yuansheng Ni",
      "Wang Zhu",
      "Ziyan Jiang",
      "Bohan Lyu",
      "Dongfu Jiang",
      "Xuan He",
      "Yuan Liu",
      "Hexiang Hu",
      "Xiang Yue",
      "Wenhu Chen"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=MEGA-Bench%3A+Scaling+Multimodal+Evaluation+to+over+500+Real-World+Tasks+Jiacheng+Chen+Tianhao+Liang+Sherman+Siu+Zhengqing+Wang+Kai+Wang",
    "gs_search_success": true,
    "gs_authors": [
      "dBVSZWAAAAAJ",
      "tLQP5EAAAAAJ",
      "dMkqNF8AAAAJ",
      "3xsG1y0AAAAJ",
      "HWFZ1P4AAAAJ",
      "s_ZW7voAAAAJ",
      "nSrKZpIAAAAJ",
      "kciKEPUAAAAJ",
      "wDJjd2IAAAAJ",
      "lYAcjN0AAAAJ"
    ],
    "citation_count": 24,
    "gs_author_count": 11
  },
  {
    "arxiv_id": "2311.08393",
    "title": "MVSA-Net: Multi-View State-Action Recognition for Robust and Deployable Trajectory Generation",
    "year": 2023,
    "published": "2023-11-14T18:53:28Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "cs.RO"
    ],
    "abstract": "The learn-from-observation (LfO) paradigm is a human-inspired mode for a robot to learn to perform a task simply by watching it being performed. LfO can facilitate robot integration on factory floors by minimizing disruption and reducing tedious programming. A key component of the LfO pipeline is a transformation of the depth camera frames to the corresponding task state and action pairs, which are then relayed to learning techniques such as imitation or inverse reinforcement learning for unders",
    "arxiv_url": "https://arxiv.org/abs/2311.08393v3",
    "pdf_url": "https://arxiv.org/pdf/2311.08393v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2311.08393",
    "arxiv_authors": [
      "Ehsan Asali",
      "Prashant Doshi",
      "Jin Sun"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=MVSA-Net%3A+Multi-View+State-Action+Recognition+for+Robust+and+Deployable+Trajectory+Generation+Ehsan+Asali+Prashant+Doshi+Jin+Sun",
    "gs_search_success": true,
    "gs_authors": [
      "3PkyzawAAAAJ",
      "Xi3owWoAAAAJ",
      "Gw10rFEAAAAJ"
    ],
    "citation_count": 2,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2503.19452",
    "title": "SparseGS-W: Sparse-View 3D Gaussian Splatting in the Wild with Generative Priors",
    "year": 2025,
    "published": "2025-03-25T08:40:40Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Synthesizing novel views of large-scale scenes from unconstrained in-the-wild images is an important but challenging task in computer vision. Existing methods, which optimize per-image appearance and transient occlusion through implicit neural networks from dense training views (approximately 1000 images), struggle to perform effectively under sparse input conditions, resulting in noticeable artifacts. To this end, we propose SparseGS-W, a novel framework based on 3D Gaussian Splatting that enab",
    "arxiv_url": "https://arxiv.org/abs/2503.19452v1",
    "pdf_url": "https://arxiv.org/pdf/2503.19452v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2503.19452",
    "arxiv_authors": [
      "Yiqing Li",
      "Xuan Wang",
      "Jiawei Wu",
      "Yikun Ma",
      "Zhi Jin"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=SparseGS-W%3A+Sparse-View+3D+Gaussian+Splatting+in+the+Wild+with+Generative+Priors+Yiqing+Li+Xuan+Wang+Jiawei+Wu+Yikun+Ma+Zhi+Jin",
    "gs_search_success": true,
    "gs_authors": [
      "v70dNBoAAAAJ",
      "wwHeXF0AAAAJ",
      "9KpZp-IAAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2411.08926",
    "title": "DG-PPU: Dynamical Graphs based Post-processing of Point Clouds extracted from Knee Ultrasounds",
    "year": 2024,
    "published": "2024-11-12T14:04:42Z",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "abstract": "Patients undergoing total knee arthroplasty (TKA) often experience non-specific anterior knee pain, arising from abnormal patellofemoral joint (PFJ) instability. Tracking PFJ motion is challenging since static imaging modalities like CT and MRI are limited by field of view and metal artefact interference. Ultrasounds offer an alternative modality for dynamic musculoskeletal imaging. We aim to achieve accurate visualisation of patellar tracking and PFJ motion, using 3D registration of point cloud",
    "arxiv_url": "https://arxiv.org/abs/2411.08926v2",
    "pdf_url": "https://arxiv.org/pdf/2411.08926v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2411.08926",
    "arxiv_authors": [
      "Injune Hwang",
      "Karthik Saravanan",
      "Caterina V Coralli",
      "S Jack Tu",
      "Stephen J Mellon"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=DG-PPU%3A+Dynamical+Graphs+based+Post-processing+of+Point+Clouds+extracted+from+Knee+Ultrasounds+Injune+Hwang+Karthik+Saravanan+Caterina+V+Coralli+S+Jack+Tu+Stephen+J+Mellon",
    "gs_search_success": true,
    "gs_authors": [
      "Jw5ZtNUAAAAJ",
      "gXjcUf4AAAAJ"
    ],
    "citation_count": 2,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2408.13656",
    "title": "Localize-and-Stitch: Efficient Model Merging via Sparse Task Arithmetic",
    "year": 2024,
    "published": "2024-08-24T19:14:02Z",
    "categories": [
      "cs.LG",
      "cs.CL",
      "cs.CV"
    ],
    "abstract": "Model merging offers an effective strategy to combine the strengths of multiple finetuned models into a unified model that preserves the specialized capabilities of each. Existing methods merge models in a global manner, performing arithmetic operations across all model parameters. However, such global merging often leads to task interference, degrading the performance of the merged model. In this work, we introduce Localize-and-Stitch, a novel approach that merges models in a localized way. Our",
    "arxiv_url": "https://arxiv.org/abs/2408.13656v2",
    "pdf_url": "https://arxiv.org/pdf/2408.13656v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2408.13656",
    "arxiv_authors": [
      "Yifei He",
      "Yuzheng Hu",
      "Yong Lin",
      "Tong Zhang",
      "Han Zhao"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Localize-and-Stitch%3A+Efficient+Model+Merging+via+Sparse+Task+Arithmetic+Yifei+He+Yuzheng+Hu+Yong+Lin+Tong+Zhang+Han+Zhao",
    "gs_search_success": true,
    "gs_authors": [
      "x942ipYAAAAJ",
      "M4g0ZvMAAAAJ",
      "LurWtuYAAAAJ",
      "VgfCr8MAAAAJ",
      "cVVimVcAAAAJ"
    ],
    "citation_count": 39,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2306.10684",
    "title": "Visually-Guided Sound Source Separation with Audio-Visual Predictive Coding",
    "year": 2023,
    "published": "2023-06-19T03:10:57Z",
    "categories": [
      "cs.SD",
      "cs.CV",
      "cs.MM",
      "eess.AS"
    ],
    "abstract": "The framework of visually-guided sound source separation generally consists of three parts: visual feature extraction, multimodal feature fusion, and sound signal processing. An ongoing trend in this field has been to tailor involved visual feature extractor for informative visual guidance and separately devise module for feature fusion, while utilizing U-Net by default for sound analysis. However, such divide-and-conquer paradigm is parameter inefficient and, meanwhile, may obtain suboptimal pe",
    "arxiv_url": "https://arxiv.org/abs/2306.10684v1",
    "pdf_url": "https://arxiv.org/pdf/2306.10684v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2306.10684",
    "arxiv_authors": [
      "Zengjie Song",
      "Zhaoxiang Zhang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Visually-Guided+Sound+Source+Separation+with+Audio-Visual+Predictive+Coding+Zengjie+Song+Zhaoxiang+Zhang",
    "gs_search_success": true,
    "gs_authors": [
      "qxWfV6cAAAAJ",
      "nZhxj2AAAAAJ"
    ],
    "citation_count": 8,
    "gs_author_count": 2
  },
  {
    "arxiv_id": "2410.01180",
    "title": "UAL-Bench: The First Comprehensive Unusual Activity Localization Benchmark",
    "year": 2024,
    "published": "2024-10-02T02:33:09Z",
    "categories": [
      "cs.CV",
      "cs.CL"
    ],
    "abstract": "Localizing unusual activities, such as human errors or surveillance incidents, in videos holds practical significance. However, current video understanding models struggle with localizing these unusual events likely because of their insufficient representation in models' pretraining datasets. To explore foundation models' capability in localizing unusual activity, we introduce UAL-Bench, a comprehensive benchmark for unusual activity localization, featuring three video datasets: UAG-OOPS, UAG-SS",
    "arxiv_url": "https://arxiv.org/abs/2410.01180v1",
    "pdf_url": "https://arxiv.org/pdf/2410.01180v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2410.01180",
    "arxiv_authors": [
      "Hasnat Md Abdullah",
      "Tian Liu",
      "Kangda Wei",
      "Shu Kong",
      "Ruihong Huang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=UAL-Bench%3A+The+First+Comprehensive+Unusual+Activity+Localization+Benchmark+Hasnat+Md+Abdullah+Tian+Liu+Kangda+Wei+Shu+Kong+Ruihong+Huang",
    "gs_search_success": true,
    "gs_authors": [
      "sm9FdLoAAAAJ",
      "NU2aHWUAAAAJ",
      "qgmIiSsAAAAJ",
      "jFgmL9wAAAAJ",
      "hQ1bio8AAAAJ"
    ],
    "citation_count": 4,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2402.01779",
    "title": "Plug-and-Play image restoration with Stochastic deNOising REgularization",
    "year": 2024,
    "published": "2024-02-01T18:05:47Z",
    "categories": [
      "eess.IV",
      "cs.CV",
      "cs.LG",
      "stat.ML"
    ],
    "abstract": "Plug-and-Play (PnP) algorithms are a class of iterative algorithms that address image inverse problems by combining a physical model and a deep neural network for regularization. Even if they produce impressive image restoration results, these algorithms rely on a non-standard use of a denoiser on images that are less and less noisy along the iterations, which contrasts with recent algorithms based on Diffusion Models (DM), where the denoiser is applied only on re-noised images. We propose a new",
    "arxiv_url": "https://arxiv.org/abs/2402.01779v3",
    "pdf_url": "https://arxiv.org/pdf/2402.01779v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2402.01779",
    "arxiv_authors": [
      "Marien Renaud",
      "Jean Prost",
      "Arthur Leclaire",
      "Nicolas Papadakis"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Plug-and-Play+image+restoration+with+Stochastic+deNOising+REgularization+Marien+Renaud+Jean+Prost+Arthur+Leclaire+Nicolas+Papadakis",
    "gs_search_success": true,
    "gs_authors": [
      "l8V9zvAAAAAJ",
      "sm-xR4wAAAAJ",
      "NCpjnqwAAAAJ",
      "hfyLiLYAAAAJ"
    ],
    "citation_count": 19,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2312.03692",
    "title": "Memory Triggers: Unveiling Memorization in Text-To-Image Generative Models through Word-Level Duplication",
    "year": 2023,
    "published": "2023-12-06T18:54:44Z",
    "categories": [
      "cs.CR",
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "Diffusion-based models, such as the Stable Diffusion model, have revolutionized text-to-image synthesis with their ability to produce high-quality, high-resolution images. These advancements have prompted significant progress in image generation and editing tasks. However, these models also raise concerns due to their tendency to memorize and potentially replicate exact training samples, posing privacy risks and enabling adversarial attacks. Duplication in training datasets is recognized as a ma",
    "arxiv_url": "https://arxiv.org/abs/2312.03692v1",
    "pdf_url": "https://arxiv.org/pdf/2312.03692v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2312.03692",
    "arxiv_authors": [
      "Ali Naseh",
      "Jaechul Roh",
      "Amir Houmansadr"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Memory+Triggers%3A+Unveiling+Memorization+in+Text-To-Image+Generative+Models+through+Word-Level+Duplication+Ali+Naseh+Jaechul+Roh+Amir+Houmansadr",
    "gs_search_success": true,
    "gs_authors": [
      "knCeRjsAAAAJ",
      "cTTFHNwAAAAJ",
      "1K2KvWwAAAAJ"
    ],
    "citation_count": 5,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2303.00340",
    "title": "A Practical Upper Bound for the Worst-Case Attribution Deviations",
    "year": 2023,
    "published": "2023-03-01T09:07:27Z",
    "categories": [
      "cs.LG",
      "cs.CR",
      "cs.CV"
    ],
    "abstract": "Model attribution is a critical component of deep neural networks (DNNs) for its interpretability to complex models. Recent studies bring up attention to the security of attribution methods as they are vulnerable to attribution attacks that generate similar images with dramatically different attributions. Existing works have been investigating empirically improving the robustness of DNNs against those attacks; however, none of them explicitly quantifies the actual deviations of attributions. In ",
    "arxiv_url": "https://arxiv.org/abs/2303.00340v1",
    "pdf_url": "https://arxiv.org/pdf/2303.00340v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2303.00340",
    "arxiv_authors": [
      "Fan Wang",
      "Adams Wai-Kin Kong"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=A+Practical+Upper+Bound+for+the+Worst-Case+Attribution+Deviations+Fan+Wang+Adams+Wai-Kin+Kong",
    "gs_search_success": true,
    "gs_authors": [
      "2GfXvbUAAAAJ",
      "JayyfYMAAAAJ"
    ],
    "citation_count": 6,
    "gs_author_count": 2
  },
  {
    "arxiv_id": "2505.00746",
    "title": "Entropy Heat-Mapping: Localizing GPT-Based OCR Errors with Sliding-Window Shannon Analysis",
    "year": 2025,
    "published": "2025-04-30T09:05:49Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Vision-language models such as OpenAI GPT-4o can transcribe mathematical documents directly from images, yet their token-level confidence signals are seldom used to pinpoint local recognition mistakes. We present an entropy-heat-mapping proof-of-concept that turns per-token Shannon entropy into a visual ''uncertainty landscape''. By scanning the entropy sequence with a fixed-length sliding window, we obtain hotspots that are likely to contain OCR errors such as missing symbols, mismatched braces",
    "arxiv_url": "https://arxiv.org/abs/2505.00746v2",
    "pdf_url": "https://arxiv.org/pdf/2505.00746v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2505.00746",
    "arxiv_authors": [
      "Alexei Kaltchenko"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Entropy+Heat-Mapping%3A+Localizing+GPT-Based+OCR+Errors+with+Sliding-Window+Shannon+Analysis+Alexei+Kaltchenko",
    "gs_search_success": true,
    "gs_authors": [],
    "citation_count": 3,
    "gs_author_count": 1
  },
  {
    "arxiv_id": "2304.04639",
    "title": "EKILA: Synthetic Media Provenance and Attribution for Generative Art",
    "year": 2023,
    "published": "2023-04-10T15:11:26Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "We present EKILA; a decentralized framework that enables creatives to receive recognition and reward for their contributions to generative AI (GenAI). EKILA proposes a robust visual attribution technique and combines this with an emerging content provenance standard (C2PA) to address the problem of synthetic image provenance -- determining the generative model and training data responsible for an AI-generated image. Furthermore, EKILA extends the non-fungible token (NFT) ecosystem to introduce a",
    "arxiv_url": "https://arxiv.org/abs/2304.04639v1",
    "pdf_url": "https://arxiv.org/pdf/2304.04639v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2304.04639",
    "arxiv_authors": [
      "Kar Balan",
      "Shruti Agarwal",
      "Simon Jenni",
      "Andy Parsons",
      "Andrew Gilbert",
      "John Collomosse"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=EKILA%3A+Synthetic+Media+Provenance+and+Attribution+for+Generative+Art+Kar+Balan+Shruti+Agarwal+Simon+Jenni+Andy+Parsons+Andrew+Gilbert",
    "gs_search_success": true,
    "gs_authors": [
      "yVnCUg0AAAAJ",
      "oLUdxEUAAAAJ",
      "AlA3q94AAAAJ",
      "gcuoIb0AAAAJ"
    ],
    "citation_count": 25,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2503.15666",
    "title": "Toward Scalable, Flexible Scene Flow for Point Clouds",
    "year": 2025,
    "published": "2025-03-19T19:33:14Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Scene flow estimation is the task of describing 3D motion between temporally successive observations. This thesis aims to build the foundation for building scene flow estimators with two important properties: they are scalable, i.e. they improve with access to more data and computation, and they are flexible, i.e. they work out-of-the-box in a variety of domains and on a variety of motion patterns without requiring significant hyperparameter tuning.   In this dissertation we present several conc",
    "arxiv_url": "https://arxiv.org/abs/2503.15666v1",
    "pdf_url": "https://arxiv.org/pdf/2503.15666v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2503.15666",
    "arxiv_authors": [
      "Kyle Vedder"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Toward+Scalable%2C+Flexible+Scene+Flow+for+Point+Clouds+Kyle+Vedder",
    "gs_search_success": true,
    "gs_authors": [
      "Ml6RzmEAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 1
  },
  {
    "arxiv_id": "2503.23012",
    "title": "Multi-label classification for multi-temporal, multi-spatial coral reef condition monitoring using vision foundation model with adapter learning",
    "year": 2025,
    "published": "2025-03-29T08:32:44Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Coral reef ecosystems provide essential ecosystem services, but face significant threats from climate change and human activities. Although advances in deep learning have enabled automatic classification of coral reef conditions, conventional deep models struggle to achieve high performance when processing complex underwater ecological images. Vision foundation models, known for their high accuracy and cross-domain generalizability, offer promising solutions. However, fine-tuning these models re",
    "arxiv_url": "https://arxiv.org/abs/2503.23012v1",
    "pdf_url": "https://arxiv.org/pdf/2503.23012v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2503.23012",
    "arxiv_authors": [
      "Xinlei Shao",
      "Hongruixuan Chen",
      "Fan Zhao",
      "Kirsty Magson",
      "Jundong Chen",
      "Peiran Li",
      "Jiaqi Wang",
      "Jun Sasaki"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Multi-label+classification+for+multi-temporal%2C+multi-spatial+coral+reef+condition+monitoring+using+vision+foundation+model+with+adapter+learning+Xinlei+Shao+Hongruixuan+Chen+Fan+Zhao+Kirsty+Magson+Jundong+Chen",
    "gs_search_success": false,
    "gs_authors": [],
    "citation_count": null,
    "gs_author_count": null,
    "error_type": "fetch_failed"
  },
  {
    "arxiv_id": "2311.08075",
    "title": "GlanceSeg: Real-time microaneurysm lesion segmentation with gaze-map-guided foundation model for early detection of diabetic retinopathy",
    "year": 2023,
    "published": "2023-11-14T10:59:45Z",
    "categories": [
      "eess.IV",
      "cs.CV",
      "cs.HC"
    ],
    "abstract": "Early-stage diabetic retinopathy (DR) presents challenges in clinical diagnosis due to inconspicuous and minute microangioma lesions, resulting in limited research in this area. Additionally, the potential of emerging foundation models, such as the segment anything model (SAM), in medical scenarios remains rarely explored. In this work, we propose a human-in-the-loop, label-free early DR diagnosis framework called GlanceSeg, based on SAM. GlanceSeg enables real-time segmentation of microangioma ",
    "arxiv_url": "https://arxiv.org/abs/2311.08075v1",
    "pdf_url": "https://arxiv.org/pdf/2311.08075v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2311.08075",
    "arxiv_authors": [
      "Hongyang Jiang",
      "Mengdi Gao",
      "Zirong Liu",
      "Chen Tang",
      "Xiaoqing Zhang",
      "Shuai Jiang",
      "Wu Yuan",
      "Jiang Liu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=GlanceSeg%3A+Real-time+microaneurysm+lesion+segmentation+with+gaze-map-guided+foundation+model+for+early+detection+of+diabetic+retinopathy+Hongyang+Jiang+Mengdi+Gao+Zirong+Liu+Chen+Tang+Xiaoqing+Zhang",
    "gs_search_success": true,
    "gs_authors": [
      "GQ0UTjcAAAAJ",
      "QxWIB_QAAAAJ",
      "3A_h6-sAAAAJ"
    ],
    "citation_count": 27,
    "gs_author_count": 8
  },
  {
    "arxiv_id": "2311.02122",
    "title": "Lost Your Style? Navigating with Semantic-Level Approach for Text-to-Outfit Retrieval",
    "year": 2023,
    "published": "2023-11-03T07:23:21Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Fashion stylists have historically bridged the gap between consumers' desires and perfect outfits, which involve intricate combinations of colors, patterns, and materials. Although recent advancements in fashion recommendation systems have made strides in outfit compatibility prediction and complementary item retrieval, these systems rely heavily on pre-selected customer choices. Therefore, we introduce a groundbreaking approach to fashion recommendations: text-to-outfit retrieval task that gene",
    "arxiv_url": "https://arxiv.org/abs/2311.02122v1",
    "pdf_url": "https://arxiv.org/pdf/2311.02122v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2311.02122",
    "arxiv_authors": [
      "Junkyu Jang",
      "Eugene Hwang",
      "Sung-Hyuk Park"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Lost+Your+Style%3F+Navigating+with+Semantic-Level+Approach+for+Text-to-Outfit+Retrieval+Junkyu+Jang+Eugene+Hwang+Sung-Hyuk+Park",
    "gs_search_success": true,
    "gs_authors": [
      "LqD9ymsAAAAJ",
      "_B36WMgAAAAJ",
      "B_hDSWgAAAAJ"
    ],
    "citation_count": 4,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2401.09258",
    "title": "Efficient Training of Generalizable Visuomotor Policies via Control-Aware Augmentation",
    "year": 2024,
    "published": "2024-01-17T15:05:00Z",
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "abstract": "Improving generalization is one key challenge in embodied AI, where obtaining large-scale datasets across diverse scenarios is costly. Traditional weak augmentations, such as cropping and flipping, are insufficient for improving a model's performance in new environments. Existing data augmentation methods often disrupt task-relevant information in images, potentially degrading performance. To overcome these challenges, we introduce EAGLE, an efficient training framework for generalizable visuomo",
    "arxiv_url": "https://arxiv.org/abs/2401.09258v2",
    "pdf_url": "https://arxiv.org/pdf/2401.09258v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2401.09258",
    "arxiv_authors": [
      "Yinuo Zhao",
      "Kun Wu",
      "Tianjiao Yi",
      "Zhiyuan Xu",
      "Xiaozhu Ju",
      "Zhengping Che",
      "Chi Harold Liu",
      "Jian Tang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Efficient+Training+of+Generalizable+Visuomotor+Policies+via+Control-Aware+Augmentation+Yinuo+Zhao+Kun+Wu+Tianjiao+Yi+Zhiyuan+Xu+Xiaozhu+Ju",
    "gs_search_success": true,
    "gs_authors": [
      "jKHMVnYAAAAJ",
      "zNqGz-IAAAAJ",
      "hdhbY-wAAAAJ",
      "iqbx6RQAAAAJ",
      "3IgFTEkAAAAJ",
      "IirM9zMAAAAJ",
      "f6uvd6kAAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 7
  },
  {
    "arxiv_id": "2302.09572",
    "title": "Rethinking Data-Free Quantization as a Zero-Sum Game",
    "year": 2023,
    "published": "2023-02-19T13:22:40Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Data-free quantization (DFQ) recovers the performance of quantized network (Q) without accessing the real data, but generates the fake sample via a generator (G) by learning from full-precision network (P) instead. However, such sample generation process is totally independent of Q, specialized as failing to consider the adaptability of the generated samples, i.e., beneficial or adversarial, over the learning process of Q, resulting into non-ignorable performance loss. Building on this, several ",
    "arxiv_url": "https://arxiv.org/abs/2302.09572v1",
    "pdf_url": "https://arxiv.org/pdf/2302.09572v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2302.09572",
    "arxiv_authors": [
      "Biao Qian",
      "Yang Wang",
      "Richang Hong",
      "Meng Wang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Rethinking+Data-Free+Quantization+as+a+Zero-Sum+Game+Biao+Qian+Yang+Wang+Richang+Hong+Meng+Wang",
    "gs_search_success": true,
    "gs_authors": [
      "rHagaaIAAAAJ",
      "-ReoUxUAAAAJ",
      "uzljmU8AAAAJ",
      "hSaWNR0AAAAJ"
    ],
    "citation_count": 40,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2403.18593",
    "title": "Homogeneous Tokenizer Matters: Homogeneous Visual Tokenizer for Remote Sensing Image Understanding",
    "year": 2024,
    "published": "2024-03-27T14:18:09Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "The tokenizer, as one of the fundamental components of large models, has long been overlooked or even misunderstood in visual tasks. One key factor of the great comprehension power of the large language model is that natural language tokenizers utilize meaningful words or subwords as the basic elements of language. In contrast, mainstream visual tokenizers, represented by patch-based methods such as Patch Embed, rely on meaningless rectangular patches as basic elements of vision, which cannot se",
    "arxiv_url": "https://arxiv.org/abs/2403.18593v2",
    "pdf_url": "https://arxiv.org/pdf/2403.18593v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2403.18593",
    "arxiv_authors": [
      "Run Shao",
      "Zhaoyang Zhang",
      "Chao Tao",
      "Yunsheng Zhang",
      "Chengli Peng",
      "Haifeng Li"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Homogeneous+Tokenizer+Matters%3A+Homogeneous+Visual+Tokenizer+for+Remote+Sensing+Image+Understanding+Run+Shao+Zhaoyang+Zhang+Chao+Tao+Yunsheng+Zhang+Chengli+Peng",
    "gs_search_success": false,
    "gs_authors": [],
    "citation_count": null,
    "gs_author_count": null,
    "error_type": "fetch_failed"
  },
  {
    "arxiv_id": "2408.02001",
    "title": "AdaCBM: An Adaptive Concept Bottleneck Model for Explainable and Accurate Diagnosis",
    "year": 2024,
    "published": "2024-08-04T11:59:09Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "The integration of vision-language models such as CLIP and Concept Bottleneck Models (CBMs) offers a promising approach to explaining deep neural network (DNN) decisions using concepts understandable by humans, addressing the black-box concern of DNNs. While CLIP provides both explainability and zero-shot classification capability, its pre-training on generic image and text data may limit its classification accuracy and applicability to medical image diagnostic tasks, creating a transfer learnin",
    "arxiv_url": "https://arxiv.org/abs/2408.02001v1",
    "pdf_url": "https://arxiv.org/pdf/2408.02001v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2408.02001",
    "arxiv_authors": [
      "Townim F. Chowdhury",
      "Vu Minh Hieu Phan",
      "Kewen Liao",
      "Minh-Son To",
      "Yutong Xie",
      "Anton van den Hengel",
      "Johan W. Verjans",
      "Zhibin Liao"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=AdaCBM%3A+An+Adaptive+Concept+Bottleneck+Model+for+Explainable+and+Accurate+Diagnosis+Townim+F.+Chowdhury+Vu+Minh+Hieu+Phan+Kewen+Liao+Minh-Son+To+Yutong+Xie",
    "gs_search_success": true,
    "gs_authors": [
      "nMGZ2ZQAAAAJ",
      "gSEw8EsAAAAJ",
      "HvWTE0IAAAAJ",
      "57JVdyIAAAAJ",
      "ddDL9HMAAAAJ",
      "NIc4qPsAAAAJ",
      "Jwefna0AAAAJ",
      "KggdfSwAAAAJ"
    ],
    "citation_count": 11,
    "gs_author_count": 8
  },
  {
    "arxiv_id": "2409.13538",
    "title": "First Place Solution to the Multiple-choice Video QA Track of The Second Perception Test Challenge",
    "year": 2024,
    "published": "2024-09-20T14:31:13Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "abstract": "In this report, we present our first-place solution to the Multiple-choice Video Question Answering (QA) track of The Second Perception Test Challenge. This competition posed a complex video understanding task, requiring models to accurately comprehend and answer questions about video content. To address this challenge, we leveraged the powerful QwenVL2 (7B) model and fine-tune it on the provided training set. Additionally, we employed model ensemble strategies and Test Time Augmentation to boos",
    "arxiv_url": "https://arxiv.org/abs/2409.13538v1",
    "pdf_url": "https://arxiv.org/pdf/2409.13538v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2409.13538",
    "arxiv_authors": [
      "Yingzhe Peng",
      "Yixiao Yuan",
      "Zitian Ao",
      "Huapeng Zhou",
      "Kangqi Wang",
      "Qipeng Zhu",
      "Xu Yang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=First+Place+Solution+to+the+Multiple-choice+Video+QA+Track+of+The+Second+Perception+Test+Challenge+Yingzhe+Peng+Yixiao+Yuan+Zitian+Ao+Huapeng+Zhou+Kangqi+Wang",
    "gs_search_success": true,
    "gs_authors": [
      "e6pjuMUAAAAJ",
      "VZM_gSkAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 7
  },
  {
    "arxiv_id": "2503.11163",
    "title": "A Benchmarking Study of Vision-based Robotic Grasping Algorithms",
    "year": 2025,
    "published": "2025-03-14T08:03:20Z",
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "abstract": "We present a benchmarking study of vision-based robotic grasping algorithms with distinct approaches, and provide a comparative analysis. In particular, we compare two machine-learning-based and two analytical algorithms using an existing benchmarking protocol from the literature and determine the algorithm's strengths and weaknesses under different experimental conditions. These conditions include variations in lighting, background textures, cameras with different noise levels, and grippers. We",
    "arxiv_url": "https://arxiv.org/abs/2503.11163v2",
    "pdf_url": "https://arxiv.org/pdf/2503.11163v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2503.11163",
    "arxiv_authors": [
      "Bharath K Rameshbabu",
      "Sumukh S Balakrishna",
      "Brian Flynn",
      "Vinarak Kapoor",
      "Adam Norton",
      "Holly Yanco",
      "Berk Calli"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=A+Benchmarking+Study+of+Vision-based+Robotic+Grasping+Algorithms+Bharath+K+Rameshbabu+Sumukh+S+Balakrishna+Brian+Flynn+Vinarak+Kapoor+Adam+Norton",
    "gs_search_success": true,
    "gs_authors": [
      "3WEijrIAAAAJ",
      "oH7drVcAAAAJ",
      "HxJTHvMAAAAJ",
      "HnUozJwAAAAJ"
    ],
    "citation_count": 2,
    "gs_author_count": 7
  },
  {
    "arxiv_id": "2312.11467",
    "title": "Glioblastoma Tumor Segmentation using an Ensemble of Vision Transformers",
    "year": 2023,
    "published": "2023-11-09T18:55:27Z",
    "categories": [
      "eess.IV",
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "Glioblastoma is one of the most aggressive and deadliest types of brain cancer, with low survival rates compared to other types of cancer. Analysis of Magnetic Resonance Imaging (MRI) scans is one of the most effective methods for the diagnosis and treatment of brain cancers such as glioblastoma. Accurate tumor segmentation in MRI images is often required for treatment planning and risk assessment of treatment methods. Here, we propose a novel pipeline, Brain Radiology Aided by Intelligent Neura",
    "arxiv_url": "https://arxiv.org/abs/2312.11467v1",
    "pdf_url": "https://arxiv.org/pdf/2312.11467v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2312.11467",
    "arxiv_authors": [
      "Huafeng Liu",
      "Benjamin Dowdell",
      "Todd Engelder",
      "Zarah Pulmano",
      "Nicolas Osa",
      "Arko Barman"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Glioblastoma+Tumor+Segmentation+using+an+Ensemble+of+Vision+Transformers+Huafeng+Liu+Benjamin+Dowdell+Todd+Engelder+Zarah+Pulmano+Nicolas+Osa",
    "gs_search_success": true,
    "gs_authors": [
      "qOLjQiYAAAAJ"
    ],
    "citation_count": 2,
    "gs_author_count": null
  },
  {
    "arxiv_id": "2502.12096",
    "title": "Token Communications: A Large Model-Driven Framework for Cross-modal Context-aware Semantic Communications",
    "year": 2025,
    "published": "2025-02-17T18:14:18Z",
    "categories": [
      "cs.MM",
      "cs.CV",
      "cs.IT",
      "eess.SP"
    ],
    "abstract": "In this paper, we introduce token communications (TokCom), a large model-driven framework to leverage cross-modal context information in generative semantic communications (GenSC). TokCom is a new paradigm, motivated by the recent success of generative foundation models and multimodal large language models (GFM/MLLMs), where the communication units are tokens, enabling efficient transformer-based token processing at the transmitter and receiver. In this paper, we introduce the potential opportun",
    "arxiv_url": "https://arxiv.org/abs/2502.12096v4",
    "pdf_url": "https://arxiv.org/pdf/2502.12096v4",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2502.12096",
    "arxiv_authors": [
      "Li Qiao",
      "Mahdi Boloursaz Mashhadi",
      "Zhen Gao",
      "Rahim Tafazolli",
      "Mehdi Bennis",
      "Dusit Niyato"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Token+Communications%3A+A+Large+Model-Driven+Framework+for+Cross-modal+Context-aware+Semantic+Communications+Li+Qiao+Mahdi+Boloursaz+Mashhadi+Zhen+Gao+Rahim+Tafazolli+Mehdi+Bennis",
    "gs_search_success": true,
    "gs_authors": [
      "_qVyHGMAAAAJ",
      "22xuv98AAAAJ",
      "RW4sJu8AAAAJ",
      "E8uwGXMAAAAJ",
      "T8sVhLMAAAAJ",
      "4ylyExkAAAAJ"
    ],
    "citation_count": 6,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2410.10511",
    "title": "Customize Your Visual Autoregressive Recipe with Set Autoregressive Modeling",
    "year": 2024,
    "published": "2024-10-14T13:49:06Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "We introduce a new paradigm for AutoRegressive (AR) image generation, termed Set AutoRegressive Modeling (SAR). SAR generalizes the conventional AR to the next-set setting, i.e., splitting the sequence into arbitrary sets containing multiple tokens, rather than outputting each token in a fixed raster order. To accommodate SAR, we develop a straightforward architecture termed Fully Masked Transformer. We reveal that existing AR variants correspond to specific design choices of sequence order and ",
    "arxiv_url": "https://arxiv.org/abs/2410.10511v1",
    "pdf_url": "https://arxiv.org/pdf/2410.10511v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2410.10511",
    "arxiv_authors": [
      "Wenze Liu",
      "Le Zhuo",
      "Yi Xin",
      "Sheng Xia",
      "Peng Gao",
      "Xiangyu Yue"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Customize+Your+Visual+Autoregressive+Recipe+with+Set+Autoregressive+Modeling+Wenze+Liu+Le+Zhuo+Yi+Xin+Sheng+Xia+Peng+Gao",
    "gs_search_success": true,
    "gs_authors": [
      "FbJyUYoAAAAJ",
      "NxNC8qgAAAAJ",
      "-xQ-C1sAAAAJ"
    ],
    "citation_count": 14,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2412.11237",
    "title": "On the Generalizability of Iterative Patch Selection for Memory-Efficient High-Resolution Image Classification",
    "year": 2024,
    "published": "2024-12-15T16:25:30Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Classifying large images with small or tiny regions of interest (ROI) is challenging due to computational and memory constraints. Weakly supervised memory-efficient patch selectors have achieved results comparable with strongly supervised methods. However, low signal-to-noise ratios and low entropy attention still cause overfitting. We explore these issues using a novel testbed on a memory-efficient cross-attention transformer with Iterative Patch Selection (IPS) as the patch selection module. O",
    "arxiv_url": "https://arxiv.org/abs/2412.11237v1",
    "pdf_url": "https://arxiv.org/pdf/2412.11237v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2412.11237",
    "arxiv_authors": [
      "Max Riffi-Aslett",
      "Christina Fell"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=On+the+Generalizability+of+Iterative+Patch+Selection+for+Memory-Efficient+High-Resolution+Image+Classification+Max+Riffi-Aslett+Christina+Fell",
    "gs_search_success": true,
    "gs_authors": [],
    "citation_count": null,
    "gs_author_count": 2
  },
  {
    "arxiv_id": "2403.20105",
    "title": "FreeSeg-Diff: Training-Free Open-Vocabulary Segmentation with Diffusion Models",
    "year": 2024,
    "published": "2024-03-29T10:38:25Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Foundation models have exhibited unprecedented capabilities in tackling many domains and tasks. Models such as CLIP are currently widely used to bridge cross-modal representations, and text-to-image diffusion models are arguably the leading models in terms of realistic image generation. Image generative models are trained on massive datasets that provide them with powerful internal spatial representations. In this work, we explore the potential benefits of such representations, beyond image gene",
    "arxiv_url": "https://arxiv.org/abs/2403.20105v2",
    "pdf_url": "https://arxiv.org/pdf/2403.20105v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2403.20105",
    "arxiv_authors": [
      "Barbara Toniella Corradini",
      "Mustafa Shukor",
      "Paul Couairon",
      "Guillaume Couairon",
      "Franco Scarselli",
      "Matthieu Cord"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=FreeSeg-Diff%3A+Training-Free+Open-Vocabulary+Segmentation+with+Diffusion+Models+Barbara+Toniella+Corradini+Mustafa+Shukor+Paul+Couairon+Guillaume+Couairon+Franco+Scarselli",
    "gs_search_success": true,
    "gs_authors": [
      "lhp9mRgAAAAJ",
      "yQRnP7YAAAAJ",
      "tSD25CkAAAAJ"
    ],
    "citation_count": 12,
    "gs_author_count": null
  },
  {
    "arxiv_id": "2402.11843",
    "title": "WildFake: A Large-scale Challenging Dataset for AI-Generated Images Detection",
    "year": 2024,
    "published": "2024-02-19T05:13:39Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "The extraordinary ability of generative models enabled the generation of images with such high quality that human beings cannot distinguish Artificial Intelligence (AI) generated images from real-life photographs. The development of generation techniques opened up new opportunities but concurrently introduced potential risks to privacy, authenticity, and security. Therefore, the task of detecting AI-generated imagery is of paramount importance to prevent illegal activities. To assess the general",
    "arxiv_url": "https://arxiv.org/abs/2402.11843v1",
    "pdf_url": "https://arxiv.org/pdf/2402.11843v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2402.11843",
    "arxiv_authors": [
      "Yan Hong",
      "Jianfu Zhang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=WildFake%3A+A+Large-scale+Challenging+Dataset+for+AI-Generated+Images+Detection+Yan+Hong+Jianfu+Zhang",
    "gs_search_success": true,
    "gs_authors": [
      "ztq5-xcAAAAJ",
      "jSiStc4AAAAJ"
    ],
    "citation_count": 28,
    "gs_author_count": 2
  },
  {
    "arxiv_id": "2403.08119",
    "title": "CMax-SLAM: Event-based Rotational-Motion Bundle Adjustment and SLAM System using Contrast Maximization",
    "year": 2024,
    "published": "2024-03-12T23:05:10Z",
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "abstract": "Event cameras are bio-inspired visual sensors that capture pixel-wise intensity changes and output asynchronous event streams. They show great potential over conventional cameras to handle challenging scenarios in robotics and computer vision, such as high-speed and high dynamic range. This paper considers the problem of rotational motion estimation using event cameras. Several event-based rotation estimation methods have been developed in the past decade, but their performance has not been eval",
    "arxiv_url": "https://arxiv.org/abs/2403.08119v1",
    "pdf_url": "https://arxiv.org/pdf/2403.08119v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2403.08119",
    "arxiv_authors": [
      "Shuang Guo",
      "Guillermo Gallego"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=CMax-SLAM%3A+Event-based+Rotational-Motion+Bundle+Adjustment+and+SLAM+System+using+Contrast+Maximization+Shuang+Guo+Guillermo+Gallego",
    "gs_search_success": true,
    "gs_authors": [
      "UGpx8IIAAAAJ",
      "v0_XxF0AAAAJ"
    ],
    "citation_count": 49,
    "gs_author_count": 2
  },
  {
    "arxiv_id": "2404.01580",
    "title": "Learning Temporal Cues by Predicting Objects Move for Multi-camera 3D Object Detection",
    "year": 2024,
    "published": "2024-04-02T02:20:47Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "In autonomous driving and robotics, there is a growing interest in utilizing short-term historical data to enhance multi-camera 3D object detection, leveraging the continuous and correlated nature of input video streams. Recent work has focused on spatially aligning BEV-based features over timesteps. However, this is often limited as its gain does not scale well with long-term past observations. To address this, we advocate for supervising a model to predict objects' poses given past observation",
    "arxiv_url": "https://arxiv.org/abs/2404.01580v1",
    "pdf_url": "https://arxiv.org/pdf/2404.01580v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2404.01580",
    "arxiv_authors": [
      "Seokha Moon",
      "Hongbeen Park",
      "Jungphil Kwon",
      "Jaekoo Lee",
      "Jinkyu Kim"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Learning+Temporal+Cues+by+Predicting+Objects+Move+for+Multi-camera+3D+Object+Detection+Seokha+Moon+Hongbeen+Park+Jungphil+Kwon+Jaekoo+Lee+Jinkyu+Kim",
    "gs_search_success": true,
    "gs_authors": [
      "89CnwkcAAAAJ",
      "HhvS9d4AAAAJ",
      "yFw-VdcAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2503.07472",
    "title": "A Review on Geometry and Surface Inspection in 3D Concrete Printing",
    "year": 2025,
    "published": "2025-03-10T15:48:17Z",
    "categories": [
      "cs.CV",
      "eess.IV"
    ],
    "abstract": "Given the substantial growth in the use of additive manufacturing in construction (AMC), it is necessary to ensure the quality of printed specimens which can be much more complex than conventionally manufactured parts. This study explores the various aspects of geometry and surface quality control for 3D concrete printing (3DCP), with a particular emphasis on deposition-based methods, namely extrusion and shotcrete 3D printing (SC3DP). A comprehensive overview of existing quality control (QC) me",
    "arxiv_url": "https://arxiv.org/abs/2503.07472v1",
    "pdf_url": "https://arxiv.org/pdf/2503.07472v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2503.07472",
    "arxiv_authors": [
      "K. Mawas",
      "M. Maboudi",
      "M. Gerke"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=A+Review+on+Geometry+and+Surface+Inspection+in+3D+Concrete+Printing+K.+Mawas+M.+Maboudi+M.+Gerke",
    "gs_search_success": true,
    "gs_authors": [
      "Cy4pRKkAAAAJ",
      "PDmdJVYAAAAJ",
      "P-x4TbXUIaAC"
    ],
    "citation_count": 3,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2404.04546",
    "title": "A self-attention model for robust rigid slice-to-volume registration of functional MRI",
    "year": 2024,
    "published": "2024-04-06T08:02:18Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Functional Magnetic Resonance Imaging (fMRI) is vital in neuroscience, enabling investigations into brain disorders, treatment monitoring, and brain function mapping. However, head motion during fMRI scans, occurring between shots of slice acquisition, can result in distortion, biased analyses, and increased costs due to the need for scan repetitions. Therefore, retrospective slice-level motion correction through slice-to-volume registration (SVR) is crucial. Previous studies have utilized deep ",
    "arxiv_url": "https://arxiv.org/abs/2404.04546v1",
    "pdf_url": "https://arxiv.org/pdf/2404.04546v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2404.04546",
    "arxiv_authors": [
      "Samah Khawaled",
      "Simon K. Warfield",
      "Moti Freiman"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=A+self-attention+model+for+robust+rigid+slice-to-volume+registration+of+functional+MRI+Samah+Khawaled+Simon+K.+Warfield+Moti+Freiman",
    "gs_search_success": true,
    "gs_authors": [
      "K5DX7poAAAAJ",
      "bfDMeTgAAAAJ",
      "oIigS0EAAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2408.14514",
    "title": "An Empirical Study on Improving SimCLR's Nonlinear Projection Head using Pretrained Autoencoder Embeddings",
    "year": 2024,
    "published": "2024-08-25T11:10:33Z",
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "abstract": "This paper focuses on improving the effectiveness of the standard 2-layer MLP projection head featured in the SimCLR framework through the use of pretrained autoencoder embeddings. Given a contrastive learning task with a largely unlabeled image classification dataset, we first train a shallow autoencoder architecture and extract its compressed representations contained in the encoder's embedding layer. After freezing the weights within this pretrained layer, we use it as a drop-in replacement f",
    "arxiv_url": "https://arxiv.org/abs/2408.14514v2",
    "pdf_url": "https://arxiv.org/pdf/2408.14514v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2408.14514",
    "arxiv_authors": [
      "Andreas Schliebitz",
      "Heiko Tapken",
      "Martin Atzmueller"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=An+Empirical+Study+on+Improving+SimCLR%27s+Nonlinear+Projection+Head+using+Pretrained+Autoencoder+Embeddings+Andreas+Schliebitz+Heiko+Tapken+Martin+Atzmueller",
    "gs_search_success": true,
    "gs_authors": [
      "pDNn26oAAAAJ",
      "PSxBcWwAAAAJ",
      "BHxwhlQAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2311.18491",
    "title": "ZeST-NeRF: Using temporal aggregation for Zero-Shot Temporal NeRFs",
    "year": 2023,
    "published": "2023-11-30T12:06:15Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR",
      "cs.LG"
    ],
    "abstract": "In the field of media production, video editing techniques play a pivotal role. Recent approaches have had great success at performing novel view image synthesis of static scenes. But adding temporal information adds an extra layer of complexity. Previous models have focused on implicitly representing static and dynamic scenes using NeRF. These models achieve impressive results but are costly at training and inference time. They overfit an MLP to describe the scene implicitly as a function of po",
    "arxiv_url": "https://arxiv.org/abs/2311.18491v1",
    "pdf_url": "https://arxiv.org/pdf/2311.18491v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2311.18491",
    "arxiv_authors": [
      "Violeta Menéndez González",
      "Andrew Gilbert",
      "Graeme Phillipson",
      "Stephen Jolly",
      "Simon Hadfield"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=ZeST-NeRF%3A+Using+temporal+aggregation+for+Zero-Shot+Temporal+NeRFs+Violeta+Men%C3%A9ndez+Gonz%C3%A1lez+Andrew+Gilbert+Graeme+Phillipson+Stephen+Jolly+Simon+Hadfield",
    "gs_search_success": true,
    "gs_authors": [
      "sle-D7AAAAAJ",
      "oLUdxEUAAAAJ",
      "vxdkgJkAAAAJ",
      "mh5ENMoAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": null
  },
  {
    "arxiv_id": "2503.19576",
    "title": "SINR: Sparsity Driven Compressed Implicit Neural Representations",
    "year": 2025,
    "published": "2025-03-25T11:53:51Z",
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "abstract": "Implicit Neural Representations (INRs) are increasingly recognized as a versatile data modality for representing discretized signals, offering benefits such as infinite query resolution and reduced storage requirements. Existing signal compression approaches for INRs typically employ one of two strategies: 1. direct quantization with entropy coding of the trained INR; 2. deriving a latent code on top of the INR through a learnable transformation. Thus, their performance is heavily dependent on t",
    "arxiv_url": "https://arxiv.org/abs/2503.19576v1",
    "pdf_url": "https://arxiv.org/pdf/2503.19576v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2503.19576",
    "arxiv_authors": [
      "Dhananjaya Jayasundara",
      "Sudarshan Rajagopalan",
      "Yasiru Ranasinghe",
      "Trac D. Tran",
      "Vishal M. Patel"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=SINR%3A+Sparsity+Driven+Compressed+Implicit+Neural+Representations+Dhananjaya+Jayasundara+Sudarshan+Rajagopalan+Yasiru+Ranasinghe+Trac+D.+Tran+Vishal+M.+Patel",
    "gs_search_success": true,
    "gs_authors": [
      "AkEXTbIAAAAJ",
      "SGty2eUAAAAJ",
      "ZQkzCdsAAAAJ",
      "RyRjA0cAAAAJ",
      "sG77m5UAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2503.21122",
    "title": "One Snapshot is All You Need: A Generalized Method for mmWave Signal Generation",
    "year": 2025,
    "published": "2025-03-27T03:24:10Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Wireless sensing systems, particularly those using mmWave technology, offer distinct advantages over traditional vision-based approaches, such as enhanced privacy and effectiveness in poor lighting conditions. These systems, leveraging FMCW signals, have shown success in human-centric applications like localization, gesture recognition, and so on. However, comprehensive mmWave datasets for diverse applications are scarce, often constrained by pre-processed signatures (e.g., point clouds or RA he",
    "arxiv_url": "https://arxiv.org/abs/2503.21122v1",
    "pdf_url": "https://arxiv.org/pdf/2503.21122v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2503.21122",
    "arxiv_authors": [
      "Teng Huang",
      "Han Ding",
      "Wenxin Sun",
      "Cui Zhao",
      "Ge Wang",
      "Fei Wang",
      "Kun Zhao",
      "Zhi Wang",
      "Wei Xi"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=One+Snapshot+is+All+You+Need%3A+A+Generalized+Method+for+mmWave+Signal+Generation+Teng+Huang+Han+Ding+Wenxin+Sun+Cui+Zhao+Ge+Wang",
    "gs_search_success": true,
    "gs_authors": [
      "LKPpmXQAAAAJ",
      "Q8fKux4AAAAJ",
      "ngEGALwAAAAJ",
      "A8HajqIAAAAJ",
      "Xt3XP-EAAAAJ",
      "0sLAHGUAAAAJ"
    ],
    "citation_count": 2,
    "gs_author_count": 8
  },
  {
    "arxiv_id": "2404.08363",
    "title": "Let-It-Flow: Simultaneous Optimization of 3D Flow and Object Clustering",
    "year": 2024,
    "published": "2024-04-12T10:04:03Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "We study the problem of self-supervised 3D scene flow estimation from real large-scale raw point cloud sequences, which is crucial to various tasks like trajectory prediction or instance segmentation. In the absence of ground truth scene flow labels, contemporary approaches concentrate on deducing optimizing flow across sequential pairs of point clouds by incorporating structure based regularization on flow and object rigidity. The rigid objects are estimated by a variety of 3D spatial clusterin",
    "arxiv_url": "https://arxiv.org/abs/2404.08363v3",
    "pdf_url": "https://arxiv.org/pdf/2404.08363v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2404.08363",
    "arxiv_authors": [
      "Patrik Vacek",
      "David Hurych",
      "Tomáš Svoboda",
      "Karel Zimmermann"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Let-It-Flow%3A+Simultaneous+Optimization+of+3D+Flow+and+Object+Clustering+Patrik+Vacek+David+Hurych+Tom%C3%A1%C5%A1+Svoboda+Karel+Zimmermann",
    "gs_search_success": true,
    "gs_authors": [
      "88aRFk8AAAAJ",
      "UROU6RgAAAAJ",
      "ec4ZOF0AAAAJ",
      "XY1PVwYAAAAJ"
    ],
    "citation_count": 6,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2411.18671",
    "title": "TAPTRv3: Spatial and Temporal Context Foster Robust Tracking of Any Point in Long Video",
    "year": 2024,
    "published": "2024-11-27T17:37:22Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "In this paper, built upon TAPTRv2, we present TAPTRv3. TAPTRv2 is a simple yet effective DETR-like point tracking framework that works fine in regular videos but tends to fail in long videos. TAPTRv3 improves TAPTRv2 by addressing its shortcomings in querying high-quality features from long videos, where the target tracking points normally undergo increasing variation over time. In TAPTRv3, we propose to utilize both spatial and temporal context to bring better feature querying along the spatial",
    "arxiv_url": "https://arxiv.org/abs/2411.18671v2",
    "pdf_url": "https://arxiv.org/pdf/2411.18671v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2411.18671",
    "arxiv_authors": [
      "Jinyuan Qu",
      "Hongyang Li",
      "Shilong Liu",
      "Tianhe Ren",
      "Zhaoyang Zeng",
      "Lei Zhang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=TAPTRv3%3A+Spatial+and+Temporal+Context+Foster+Robust+Tracking+of+Any+Point+in+Long+Video+Jinyuan+Qu+Hongyang+Li+Shilong+Liu+Tianhe+Ren+Zhaoyang+Zeng",
    "gs_search_success": true,
    "gs_authors": [
      "U_cvvUwAAAAJ",
      "fIlGZToAAAAJ",
      "cW4ILs0AAAAJ",
      "nkSVY3MAAAAJ",
      "zdgHNmkAAAAJ",
      "-RSeOl0AAAAJ"
    ],
    "citation_count": 4,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2304.06351",
    "title": "Neuromorphic Event-based Facial Expression Recognition",
    "year": 2023,
    "published": "2023-04-13T09:02:10Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Recently, event cameras have shown large applicability in several computer vision fields especially concerning tasks that require high temporal resolution. In this work, we investigate the usage of such kind of data for emotion recognition by presenting NEFER, a dataset for Neuromorphic Event-based Facial Expression Recognition. NEFER is composed of paired RGB and event videos representing human faces labeled with the respective emotions and also annotated with face bounding boxes and facial lan",
    "arxiv_url": "https://arxiv.org/abs/2304.06351v1",
    "pdf_url": "https://arxiv.org/pdf/2304.06351v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2304.06351",
    "arxiv_authors": [
      "Lorenzo Berlincioni",
      "Luca Cultrera",
      "Chiara Albisani",
      "Lisa Cresti",
      "Andrea Leonardo",
      "Sara Picchioni",
      "Federico Becattini",
      "Alberto Del Bimbo"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Neuromorphic+Event-based+Facial+Expression+Recognition+Lorenzo+Berlincioni+Luca+Cultrera+Chiara+Albisani+Lisa+Cresti+Andrea+Leonardo",
    "gs_search_success": true,
    "gs_authors": [
      "qvPebZQAAAAJ",
      "ty0aAwQAAAAJ",
      "cQT8SvQAAAAJ",
      "t0sXvkAAAAAJ"
    ],
    "citation_count": 38,
    "gs_author_count": 7
  },
  {
    "arxiv_id": "2408.05777",
    "title": "Seg-CycleGAN : SAR-to-optical image translation guided by a downstream task",
    "year": 2024,
    "published": "2024-08-11T14:01:21Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "eess.IV"
    ],
    "abstract": "Optical remote sensing and Synthetic Aperture Radar(SAR) remote sensing are crucial for earth observation, offering complementary capabilities. While optical sensors provide high-quality images, they are limited by weather and lighting conditions. In contrast, SAR sensors can operate effectively under adverse conditions. This letter proposes a GAN-based SAR-to-optical image translation method named Seg-CycleGAN, designed to enhance the accuracy of ship target translation by leveraging semantic i",
    "arxiv_url": "https://arxiv.org/abs/2408.05777v1",
    "pdf_url": "https://arxiv.org/pdf/2408.05777v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2408.05777",
    "arxiv_authors": [
      "Hannuo Zhang",
      "Huihui Li",
      "Jiarui Lin",
      "Yujie Zhang",
      "Jianghua Fan",
      "Hang Liu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Seg-CycleGAN+%3A+SAR-to-optical+image+translation+guided+by+a+downstream+task+Hannuo+Zhang+Huihui+Li+Jiarui+Lin+Yujie+Zhang+Jianghua+Fan",
    "gs_search_success": true,
    "gs_authors": [
      "ASXcXEMAAAAJ"
    ],
    "citation_count": 2,
    "gs_author_count": 7
  },
  {
    "arxiv_id": "2302.01788",
    "title": "IMPORTANT-Net: Integrated MRI Multi-Parameter Reinforcement Fusion Generator with Attention Network for Synthesizing Absent Data",
    "year": 2023,
    "published": "2023-02-03T14:56:10Z",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "abstract": "Magnetic resonance imaging (MRI) is highly sensitive for lesion detection in the breasts. Sequences obtained with different settings can capture the specific characteristics of lesions. Such multi-parameter MRI information has been shown to improve radiologist performance in lesion classification, as well as improving the performance of artificial intelligence models in various tasks. However, obtaining multi-parameter MRI makes the examination costly in both financial and time perspectives, and",
    "arxiv_url": "https://arxiv.org/abs/2302.01788v1",
    "pdf_url": "https://arxiv.org/pdf/2302.01788v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2302.01788",
    "arxiv_authors": [
      "Tianyu Zhang",
      "Tao Tan",
      "Luyi Han",
      "Xin Wang",
      "Yuan Gao",
      "Jonas Teuwen",
      "Regina Beets-Tan",
      "Ritse Mann"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=IMPORTANT-Net%3A+Integrated+MRI+Multi-Parameter+Reinforcement+Fusion+Generator+with+Attention+Network+for+Synthesizing+Absent+Data+Tianyu+Zhang+Tao+Tan+Luyi+Han+Xin+Wang+Yuan+Gao",
    "gs_search_success": true,
    "gs_authors": [
      "83YbCHQAAAAJ",
      "Bx7HVd4AAAAJ",
      "Ko3BBS8AAAAJ",
      "cDGQbLsAAAAJ",
      "Jz3tRZMAAAAJ",
      "4R04g08AAAAJ",
      "lLg3WRkAAAAJ",
      "vPwW_6EAAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 8
  },
  {
    "arxiv_id": "2407.15317",
    "title": "Open-CD: A Comprehensive Toolbox for Change Detection",
    "year": 2024,
    "published": "2024-07-22T01:04:16Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "We present Open-CD, a change detection toolbox that contains a rich set of change detection methods as well as related components and modules. The toolbox started from a series of open source general vision task tools, including OpenMMLab Toolkits, PyTorch Image Models, etc. It gradually evolves into a unified platform that covers many popular change detection methods and contemporary modules. It not only includes training and inference codes, but also provides some useful scripts for data analy",
    "arxiv_url": "https://arxiv.org/abs/2407.15317v2",
    "pdf_url": "https://arxiv.org/pdf/2407.15317v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2407.15317",
    "arxiv_authors": [
      "Kaiyu Li",
      "Jiawei Jiang",
      "Andrea Codegoni",
      "Chengxi Han",
      "Yupeng Deng",
      "Keyan Chen",
      "Zhuo Zheng",
      "Hao Chen",
      "Ziyuan Liu",
      "Yuantao Gu",
      "Zhengxia Zou",
      "Zhenwei Shi",
      "Sheng Fang",
      "Deyu Meng",
      "Zhi Wang",
      "Xiangyong Cao"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Open-CD%3A+A+Comprehensive+Toolbox+for+Change+Detection+Kaiyu+Li+Jiawei+Jiang+Andrea+Codegoni+Chengxi+Han+Yupeng+Deng",
    "gs_search_success": true,
    "gs_authors": [
      "jTAxkbEAAAAJ",
      "5RF4ia8AAAAJ",
      "BEDNoZIAAAAJ",
      "8ZED-EwAAAAJ",
      "kNhFWQIAAAAJ",
      "Vca9f3YAAAAJ",
      "FL61g6wAAAAJ",
      "lFZpfE4AAAAJ",
      "CREpn_AAAAAJ",
      "DzwoyZsAAAAJ",
      "Lmqy-D4AAAAJ"
    ],
    "citation_count": 16,
    "gs_author_count": 12
  },
  {
    "arxiv_id": "2303.01047",
    "title": "Task-Specific Context Decoupling for Object Detection",
    "year": 2023,
    "published": "2023-03-02T08:02:14Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Classification and localization are two main sub-tasks in object detection. Nonetheless, these two tasks have inconsistent preferences for feature context, i.e., localization expects more boundary-aware features to accurately regress the bounding box, while more semantic context is preferred for object classification. Exsiting methods usually leverage disentangled heads to learn different feature context for each task. However, the heads are still applied on the same input features, which leads ",
    "arxiv_url": "https://arxiv.org/abs/2303.01047v1",
    "pdf_url": "https://arxiv.org/pdf/2303.01047v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2303.01047",
    "arxiv_authors": [
      "Jiayuan Zhuang",
      "Zheng Qin",
      "Hao Yu",
      "Xucan Chen"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Task-Specific+Context+Decoupling+for+Object+Detection+Jiayuan+Zhuang+Zheng+Qin+Hao+Yu+Xucan+Chen",
    "gs_search_success": true,
    "gs_authors": [
      "DnHBAN0AAAAJ",
      "g7JfRn4AAAAJ"
    ],
    "citation_count": 82,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2303.07065",
    "title": "MSINet: Twins Contrastive Search of Multi-Scale Interaction for Object ReID",
    "year": 2023,
    "published": "2023-03-13T12:39:59Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Neural Architecture Search (NAS) has been increasingly appealing to the society of object Re-Identification (ReID), for that task-specific architectures significantly improve the retrieval performance. Previous works explore new optimizing targets and search spaces for NAS ReID, yet they neglect the difference of training schemes between image classification and ReID. In this work, we propose a novel Twins Contrastive Mechanism (TCM) to provide more appropriate supervision for ReID architecture ",
    "arxiv_url": "https://arxiv.org/abs/2303.07065v1",
    "pdf_url": "https://arxiv.org/pdf/2303.07065v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2303.07065",
    "arxiv_authors": [
      "Jianyang Gu",
      "Kai Wang",
      "Hao Luo",
      "Chen Chen",
      "Wei Jiang",
      "Yuqiang Fang",
      "Shanghang Zhang",
      "Yang You",
      "Jian Zhao"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=MSINet%3A+Twins+Contrastive+Search+of+Multi-Scale+Interaction+for+Object+ReID+Jianyang+Gu+Kai+Wang+Hao+Luo+Chen+Chen+Wei+Jiang",
    "gs_search_success": true,
    "gs_authors": [
      "i2II0XIAAAAJ",
      "PzRaLt8AAAAJ",
      "yYpQrj0AAAAJ",
      "8ZXbT18AAAAJ",
      "vko6ra4AAAAJ",
      "7QvWnzMAAAAJ",
      "voqw10cAAAAJ"
    ],
    "citation_count": 103,
    "gs_author_count": 9
  },
  {
    "arxiv_id": "2504.00665",
    "title": "Monocular and Generalizable Gaussian Talking Head Animation",
    "year": 2025,
    "published": "2025-04-01T11:16:52Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "In this work, we introduce Monocular and Generalizable Gaussian Talking Head Animation (MGGTalk), which requires monocular datasets and generalizes to unseen identities without personalized re-training. Compared with previous 3D Gaussian Splatting (3DGS) methods that requires elusive multi-view datasets or tedious personalized learning/inference, MGGtalk enables more practical and broader applications. However, in the absence of multi-view and personalized training data, the incompleteness of ge",
    "arxiv_url": "https://arxiv.org/abs/2504.00665v1",
    "pdf_url": "https://arxiv.org/pdf/2504.00665v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2504.00665",
    "arxiv_authors": [
      "Shengjie Gong",
      "Haojie Li",
      "Jiapeng Tang",
      "Dongming Hu",
      "Shuangping Huang",
      "Hao Chen",
      "Tianshui Chen",
      "Zhuoman Liu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Monocular+and+Generalizable+Gaussian+Talking+Head+Animation+Shengjie+Gong+Haojie+Li+Jiapeng+Tang+Dongming+Hu+Shuangping+Huang",
    "gs_search_success": true,
    "gs_authors": [
      "llkY7TEAAAAJ",
      "yEvxW0sAAAAJ",
      "aNvkklMAAAAJ",
      "yw7MH-4AAAAJ"
    ],
    "citation_count": 2,
    "gs_author_count": 8
  },
  {
    "arxiv_id": "2301.02468",
    "title": "Deep Learning For Classification Of Chest X-Ray Images (Covid 19)",
    "year": 2023,
    "published": "2023-01-06T11:44:57Z",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "abstract": "In medical practice, the contribution of information technology can be considerable. Most of these practices include the images that medical assistance uses to identify different pathologies of the human body. One of them is X-ray images which cover much of our work in this paper. Chest x-rays have played an important role in Covid 19 identification and diagnosis. The Covid 19 virus has been declared a global pandemic since 2020 after the first case found in Wuhan China in December 2019. Our goa",
    "arxiv_url": "https://arxiv.org/abs/2301.02468v1",
    "pdf_url": "https://arxiv.org/pdf/2301.02468v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2301.02468",
    "arxiv_authors": [
      "Benbakreti Samir",
      "Said Mwanahija",
      "Benbakreti Soumia",
      "Umut Özkaya"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Deep+Learning+For+Classification+Of+Chest+X-Ray+Images+%28Covid+19%29+Benbakreti+Samir+Said+Mwanahija+Benbakreti+Soumia+Umut+%C3%96zkaya",
    "gs_search_success": false,
    "gs_authors": [],
    "citation_count": null,
    "gs_author_count": null,
    "error_type": "fetch_failed"
  },
  {
    "arxiv_id": "2308.12408",
    "title": "An Initial Exploration: Learning to Generate Realistic Audio for Silent Video",
    "year": 2023,
    "published": "2023-08-23T20:08:56Z",
    "categories": [
      "cs.SD",
      "cs.CV",
      "eess.AS"
    ],
    "abstract": "Generating realistic audio effects for movies and other media is a challenging task that is accomplished today primarily through physical techniques known as Foley art. Foley artists create sounds with common objects (e.g., boxing gloves, broken glass) in time with video as it is playing to generate captivating audio tracks. In this work, we aim to develop a deep-learning based framework that does much the same - observes video in it's natural sequence and generates realistic audio to accompany ",
    "arxiv_url": "https://arxiv.org/abs/2308.12408v1",
    "pdf_url": "https://arxiv.org/pdf/2308.12408v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2308.12408",
    "arxiv_authors": [
      "Matthew Martel",
      "Jackson Wagner"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=An+Initial+Exploration%3A+Learning+to+Generate+Realistic+Audio+for+Silent+Video+Matthew+Martel+Jackson+Wagner",
    "gs_search_success": true,
    "gs_authors": [],
    "citation_count": null,
    "gs_author_count": 2
  },
  {
    "arxiv_id": "2502.09143",
    "title": "Feature-based Graph Attention Networks Improve Online Continual Learning",
    "year": 2025,
    "published": "2025-02-13T10:18:44Z",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "Online continual learning for image classification is crucial for models to adapt to new data while retaining knowledge of previously learned tasks. This capability is essential to address real-world challenges involving dynamic environments and evolving data distributions. Traditional approaches predominantly employ Convolutional Neural Networks, which are limited to processing images as grids and primarily capture local patterns rather than relational information. Although the emergence of tra",
    "arxiv_url": "https://arxiv.org/abs/2502.09143v1",
    "pdf_url": "https://arxiv.org/pdf/2502.09143v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2502.09143",
    "arxiv_authors": [
      "Adjovi Sim",
      "Zhengkui Wang",
      "Aik Beng Ng",
      "Shalini De Mello",
      "Simon See",
      "Wonmin Byeon"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Feature-based+Graph+Attention+Networks+Improve+Online+Continual+Learning+Adjovi+Sim+Zhengkui+Wang+Aik+Beng+Ng+Shalini+De+Mello+Simon+See",
    "gs_search_success": true,
    "gs_authors": [
      "FkOwHcAAAAAJ",
      "OuAerIgAAAAJ",
      "xQM4BlMAAAAJ",
      "ebIHTEoAAAAJ",
      "0497CHoAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2409.08273",
    "title": "Hand-Object Interaction Pretraining from Videos",
    "year": 2024,
    "published": "2024-09-12T17:59:07Z",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV"
    ],
    "abstract": "We present an approach to learn general robot manipulation priors from 3D hand-object interaction trajectories. We build a framework to use in-the-wild videos to generate sensorimotor robot trajectories. We do so by lifting both the human hand and the manipulated object in a shared 3D space and retargeting human motions to robot actions. Generative modeling on this data gives us a task-agnostic base policy. This policy captures a general yet flexible manipulation prior. We empirically demonstrat",
    "arxiv_url": "https://arxiv.org/abs/2409.08273v1",
    "pdf_url": "https://arxiv.org/pdf/2409.08273v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2409.08273",
    "arxiv_authors": [
      "Himanshu Gaurav Singh",
      "Antonio Loquercio",
      "Carmelo Sferrazza",
      "Jane Wu",
      "Haozhi Qi",
      "Pieter Abbeel",
      "Jitendra Malik"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Hand-Object+Interaction+Pretraining+from+Videos+Himanshu+Gaurav+Singh+Antonio+Loquercio+Carmelo+Sferrazza+Jane+Wu+Haozhi+Qi",
    "gs_search_success": true,
    "gs_authors": [
      "qav_fQYAAAAJ",
      "x0_lwNYAAAAJ",
      "pbmjtZsAAAAJ"
    ],
    "citation_count": 23,
    "gs_author_count": null
  },
  {
    "arxiv_id": "2406.04933",
    "title": "Leveraging Activations for Superpixel Explanations",
    "year": 2024,
    "published": "2024-06-07T13:37:45Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Saliency methods have become standard in the explanation toolkit of deep neural networks. Recent developments specific to image classifiers have investigated region-based explanations with either new methods or by adapting well-established ones using ad-hoc superpixel algorithms. In this paper, we aim to avoid relying on these segmenters by extracting a segmentation from the activations of a deep neural network image classifier without fine-tuning the network. Our so-called Neuro-Activated Super",
    "arxiv_url": "https://arxiv.org/abs/2406.04933v1",
    "pdf_url": "https://arxiv.org/pdf/2406.04933v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2406.04933",
    "arxiv_authors": [
      "Ahcène Boubekki",
      "Samuel G. Fadel",
      "Sebastian Mair"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Leveraging+Activations+for+Superpixel+Explanations+Ahc%C3%A8ne+Boubekki+Samuel+G.+Fadel+Sebastian+Mair",
    "gs_search_success": true,
    "gs_authors": [
      "rdJQOxwAAAAJ",
      "fpVjl4gAAAAJ",
      "IV7luZsAAAAJ"
    ],
    "citation_count": 2,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2306.02878",
    "title": "Single-Stage 3D Geometry-Preserving Depth Estimation Model Training on Dataset Mixtures with Uncalibrated Stereo Data",
    "year": 2023,
    "published": "2023-06-05T13:49:24Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Nowadays, robotics, AR, and 3D modeling applications attract considerable attention to single-view depth estimation (SVDE) as it allows estimating scene geometry from a single RGB image. Recent works have demonstrated that the accuracy of an SVDE method hugely depends on the diversity and volume of the training data. However, RGB-D datasets obtained via depth capturing or 3D reconstruction are typically small, synthetic datasets are not photorealistic enough, and all these datasets lack diversit",
    "arxiv_url": "https://arxiv.org/abs/2306.02878v1",
    "pdf_url": "https://arxiv.org/pdf/2306.02878v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2306.02878",
    "arxiv_authors": [
      "Nikolay Patakin",
      "Mikhail Romanov",
      "Anna Vorontsova",
      "Mikhail Artemyev",
      "Anton Konushin"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Single-Stage+3D+Geometry-Preserving+Depth+Estimation+Model+Training+on+Dataset+Mixtures+with+Uncalibrated+Stereo+Data+Nikolay+Patakin+Mikhail+Romanov+Anna+Vorontsova+Mikhail+Artemyev+Anton+Konushin",
    "gs_search_success": true,
    "gs_authors": [
      "lYrPHWsAAAAJ",
      "ZT_k-wMAAAAJ",
      "ShNU3WAAAAAJ",
      "HiVoQCIAAAAJ"
    ],
    "citation_count": 9,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2311.00180",
    "title": "Object-centric Video Representation for Long-term Action Anticipation",
    "year": 2023,
    "published": "2023-10-31T22:54:31Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "This paper focuses on building object-centric representations for long-term action anticipation in videos. Our key motivation is that objects provide important cues to recognize and predict human-object interactions, especially when the predictions are longer term, as an observed \"background\" object could be used by the human actor in the future. We observe that existing object-based video recognition frameworks either assume the existence of in-domain supervised object detectors or follow a ful",
    "arxiv_url": "https://arxiv.org/abs/2311.00180v1",
    "pdf_url": "https://arxiv.org/pdf/2311.00180v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2311.00180",
    "arxiv_authors": [
      "Ce Zhang",
      "Changcheng Fu",
      "Shijie Wang",
      "Nakul Agarwal",
      "Kwonjoon Lee",
      "Chiho Choi",
      "Chen Sun"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Object-centric+Video+Representation+for+Long-term+Action+Anticipation+Ce+Zhang+Changcheng+Fu+Shijie+Wang+Nakul+Agarwal+Kwonjoon+Lee",
    "gs_search_success": true,
    "gs_authors": [
      "2V3Y_KcAAAAJ",
      "TduSHQwAAAAJ",
      "UjIFtgEAAAAJ",
      "iSFDVj4AAAAJ",
      "zGA2ReUAAAAJ",
      "vQa7heEAAAAJ",
      "C6Wu8M0AAAAJ"
    ],
    "citation_count": 34,
    "gs_author_count": 7
  },
  {
    "arxiv_id": "2403.08974",
    "title": "$TrIND$: Representing Anatomical Trees by Denoising Diffusion of Implicit Neural Fields",
    "year": 2024,
    "published": "2024-03-13T21:43:24Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Anatomical trees play a central role in clinical diagnosis and treatment planning. However, accurately representing anatomical trees is challenging due to their varying and complex topology and geometry. Traditional methods for representing tree structures, captured using medical imaging, while invaluable for visualizing vascular and bronchial networks, exhibit drawbacks in terms of limited resolution, flexibility, and efficiency. Recently, implicit neural representations (INRs) have emerged as ",
    "arxiv_url": "https://arxiv.org/abs/2403.08974v3",
    "pdf_url": "https://arxiv.org/pdf/2403.08974v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2403.08974",
    "arxiv_authors": [
      "Ashish Sinha",
      "Ghassan Hamarneh"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=%24TrIND%24%3A+Representing+Anatomical+Trees+by+Denoising+Diffusion+of+Implicit+Neural+Fields+Ashish+Sinha+Ghassan+Hamarneh",
    "gs_search_success": true,
    "gs_authors": [
      "61DdlkAAAAAJ",
      "9JNZpoQAAAAJ"
    ],
    "citation_count": 7,
    "gs_author_count": null
  },
  {
    "arxiv_id": "2406.18544",
    "title": "GS-ROR$^2$: Bidirectional-guided 3DGS and SDF for Reflective Object Relighting and Reconstruction",
    "year": 2024,
    "published": "2024-05-22T09:40:25Z",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has shown a powerful capability for novel view synthesis due to its detailed expressive ability and highly efficient rendering speed. Unfortunately, creating relightable 3D assets and reconstructing faithful geometry with 3DGS is still problematic, particularly for reflective objects, as its discontinuous representation raises difficulties in constraining geometries. Volumetric signed distance field (SDF) methods provide robust geometry reconstruction, while the expe",
    "arxiv_url": "https://arxiv.org/abs/2406.18544v3",
    "pdf_url": "https://arxiv.org/pdf/2406.18544v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2406.18544",
    "arxiv_authors": [
      "Zuo-Liang Zhu",
      "Beibei Wang",
      "Jian Yang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=GS-ROR%24%5E2%24%3A+Bidirectional-guided+3DGS+and+SDF+for+Reflective+Object+Relighting+and+Reconstruction+Zuo-Liang+Zhu+Beibei+Wang+Jian+Yang",
    "gs_search_success": true,
    "gs_authors": [],
    "citation_count": null,
    "gs_author_count": 0
  },
  {
    "arxiv_id": "2403.11761",
    "title": "BEVCar: Camera-Radar Fusion for BEV Map and Object Segmentation",
    "year": 2024,
    "published": "2024-03-18T13:14:46Z",
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "abstract": "Semantic scene segmentation from a bird's-eye-view (BEV) perspective plays a crucial role in facilitating planning and decision-making for mobile robots. Although recent vision-only methods have demonstrated notable advancements in performance, they often struggle under adverse illumination conditions such as rain or nighttime. While active sensors offer a solution to this challenge, the prohibitively high cost of LiDARs remains a limiting factor. Fusing camera data with automotive radars poses ",
    "arxiv_url": "https://arxiv.org/abs/2403.11761v2",
    "pdf_url": "https://arxiv.org/pdf/2403.11761v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2403.11761",
    "arxiv_authors": [
      "Jonas Schramm",
      "Niclas Vödisch",
      "Kürsat Petek",
      "B Ravi Kiran",
      "Senthil Yogamani",
      "Wolfram Burgard",
      "Abhinav Valada"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=BEVCar%3A+Camera-Radar+Fusion+for+BEV+Map+and+Object+Segmentation+Jonas+Schramm+Niclas+V%C3%B6disch+K%C3%BCrsat+Petek+B+Ravi+Kiran+Senthil+Yogamani",
    "gs_search_success": true,
    "gs_authors": [
      "qvXusvwAAAAJ",
      "zj6FavAAAAAJ",
      "96SlLi8AAAAJ",
      "z0a8rsYAAAAJ",
      "LcARjz0AAAAJ",
      "lmNXQDYAAAAJ"
    ],
    "citation_count": 28,
    "gs_author_count": 7
  },
  {
    "arxiv_id": "2504.01449",
    "title": "Multimodal Point Cloud Semantic Segmentation With Virtual Point Enhancement",
    "year": 2025,
    "published": "2025-04-02T08:02:06Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "LiDAR-based 3D point cloud recognition has been proven beneficial in various applications. However, the sparsity and varying density pose a significant challenge in capturing intricate details of objects, particularly for medium-range and small targets. Therefore, we propose a multi-modal point cloud semantic segmentation method based on Virtual Point Enhancement (VPE), which integrates virtual points generated from images to address these issues. These virtual points are dense but noisy, and di",
    "arxiv_url": "https://arxiv.org/abs/2504.01449v1",
    "pdf_url": "https://arxiv.org/pdf/2504.01449v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2504.01449",
    "arxiv_authors": [
      "Zaipeng Duan",
      "Xuzhong Hu",
      "Pei An",
      "Jie Ma"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Multimodal+Point+Cloud+Semantic+Segmentation+With+Virtual+Point+Enhancement+Zaipeng+Duan+Xuzhong+Hu+Pei+An+Jie+Ma",
    "gs_search_success": true,
    "gs_authors": [
      "rVlaY5UAAAAJ",
      "IhmY1uMAAAAJ",
      "FGtqwMMAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2312.01671",
    "title": "Multimodality-guided Image Style Transfer using Cross-modal GAN Inversion",
    "year": 2023,
    "published": "2023-12-04T06:38:23Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Image Style Transfer (IST) is an interdisciplinary topic of computer vision and art that continuously attracts researchers' interests. Different from traditional Image-guided Image Style Transfer (IIST) methods that require a style reference image as input to define the desired style, recent works start to tackle the problem in a text-guided manner, i.e., Text-guided Image Style Transfer (TIST). Compared to IIST, such approaches provide more flexibility with text-specified styles, which are usef",
    "arxiv_url": "https://arxiv.org/abs/2312.01671v1",
    "pdf_url": "https://arxiv.org/pdf/2312.01671v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2312.01671",
    "arxiv_authors": [
      "Hanyu Wang",
      "Pengxiang Wu",
      "Kevin Dela Rosa",
      "Chen Wang",
      "Abhinav Shrivastava"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Multimodality-guided+Image+Style+Transfer+using+Cross-modal+GAN+Inversion+Hanyu+Wang+Pengxiang+Wu+Kevin+Dela+Rosa+Chen+Wang+Abhinav+Shrivastava",
    "gs_search_success": true,
    "gs_authors": [
      "1rUZm3wAAAAJ",
      "mIF9BowAAAAJ",
      "8Pc5MiUAAAAJ",
      "akSVMYoAAAAJ",
      "MXLs7GcAAAAJ"
    ],
    "citation_count": 15,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2405.05787",
    "title": "Autonomous Robotic Ultrasound System for Liver Follow-up Diagnosis: Pilot Phantom Study",
    "year": 2024,
    "published": "2024-05-09T14:11:20Z",
    "categories": [
      "cs.RO",
      "cs.CV",
      "eess.SY"
    ],
    "abstract": "The paper introduces a novel autonomous robot ultrasound (US) system targeting liver follow-up scans for outpatients in local communities. Given a computed tomography (CT) image with specific target regions of interest, the proposed system carries out the autonomous follow-up scan in three steps: (i) initial robot contact to surface, (ii) coordinate mapping between CT image and robot, and (iii) target US scan. Utilizing 3D US-CT registration and deep learning-based segmentation networks, we can ",
    "arxiv_url": "https://arxiv.org/abs/2405.05787v1",
    "pdf_url": "https://arxiv.org/pdf/2405.05787v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2405.05787",
    "arxiv_authors": [
      "Tianpeng Zhang",
      "Sekeun Kim",
      "Jerome Charton",
      "Haitong Ma",
      "Kyungsang Kim",
      "Na Li",
      "Quanzheng Li"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Autonomous+Robotic+Ultrasound+System+for+Liver+Follow-up+Diagnosis%3A+Pilot+Phantom+Study+Tianpeng+Zhang+Sekeun+Kim+Jerome+Charton+Haitong+Ma+Kyungsang+Kim",
    "gs_search_success": true,
    "gs_authors": [
      "pdewAHgAAAAJ",
      "Hr6z0g4AAAAJ",
      "qdGelXoAAAAJ",
      "tU6pacYAAAAJ",
      "MHq2z7oAAAAJ"
    ],
    "citation_count": 2,
    "gs_author_count": 7
  },
  {
    "arxiv_id": "2303.04803",
    "title": "Open-Vocabulary Panoptic Segmentation with Text-to-Image Diffusion Models",
    "year": 2023,
    "published": "2023-03-08T18:58:26Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "We present ODISE: Open-vocabulary DIffusion-based panoptic SEgmentation, which unifies pre-trained text-image diffusion and discriminative models to perform open-vocabulary panoptic segmentation. Text-to-image diffusion models have the remarkable ability to generate high-quality images with diverse open-vocabulary language descriptions. This demonstrates that their internal representation space is highly correlated with open concepts in the real world. Text-image discriminative models like CLIP,",
    "arxiv_url": "https://arxiv.org/abs/2303.04803v4",
    "pdf_url": "https://arxiv.org/pdf/2303.04803v4",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2303.04803",
    "arxiv_authors": [
      "Jiarui Xu",
      "Sifei Liu",
      "Arash Vahdat",
      "Wonmin Byeon",
      "Xiaolong Wang",
      "Shalini De Mello"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Open-Vocabulary+Panoptic+Segmentation+with+Text-to-Image+Diffusion+Models+Jiarui+Xu+Sifei+Liu+Arash+Vahdat+Wonmin+Byeon+Xiaolong+Wang",
    "gs_search_success": true,
    "gs_authors": [
      "Y8O9N_0AAAAJ",
      "xQM4BlMAAAAJ",
      "2GKLw94AAAAJ",
      "j4pcHV4AAAAJ",
      "0497CHoAAAAJ",
      "p9-nlRIAAAAJ"
    ],
    "citation_count": 646,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2306.14408",
    "title": "Text-Anchored Score Composition: Tackling Condition Misalignment in Text-to-Image Diffusion Models",
    "year": 2023,
    "published": "2023-06-26T03:48:15Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Text-to-image diffusion models have advanced towards more controllable generation via supporting various additional conditions (e.g.,depth map, bounding box) beyond text. However, these models are learned based on the premise of perfect alignment between the text and extra conditions. If this alignment is not satisfied, the final output could be either dominated by one condition, or ambiguity may arise, failing to meet user expectations. To address this issue, we present a training free approach",
    "arxiv_url": "https://arxiv.org/abs/2306.14408v3",
    "pdf_url": "https://arxiv.org/pdf/2306.14408v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2306.14408",
    "arxiv_authors": [
      "Luozhou Wang",
      "Guibao Shen",
      "Wenhang Ge",
      "Guangyong Chen",
      "Yijun Li",
      "Ying-cong Chen"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Text-Anchored+Score+Composition%3A+Tackling+Condition+Misalignment+in+Text-to-Image+Diffusion+Models+Luozhou+Wang+Guibao+Shen+Wenhang+Ge+Guangyong+Chen+Yijun+Li",
    "gs_search_success": true,
    "gs_authors": [
      "AUpqepUAAAAJ",
      "d8VVM4UAAAAJ",
      "nrsWSt4AAAAJ",
      "FMoFIBUAAAAJ",
      "gzPpG0QAAAAJ"
    ],
    "citation_count": 6,
    "gs_author_count": null
  },
  {
    "arxiv_id": "2402.19091",
    "title": "Leveraging Representations from Intermediate Encoder-blocks for Synthetic Image Detection",
    "year": 2024,
    "published": "2024-02-29T12:18:43Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "The recently developed and publicly available synthetic image generation methods and services make it possible to create extremely realistic imagery on demand, raising great risks for the integrity and safety of online information. State-of-the-art Synthetic Image Detection (SID) research has led to strong evidence on the advantages of feature extraction from foundation models. However, such extracted features mostly encapsulate high-level visual semantics instead of fine-grained details, which ",
    "arxiv_url": "https://arxiv.org/abs/2402.19091v2",
    "pdf_url": "https://arxiv.org/pdf/2402.19091v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2402.19091",
    "arxiv_authors": [
      "Christos Koutlis",
      "Symeon Papadopoulos"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Leveraging+Representations+from+Intermediate+Encoder-blocks+for+Synthetic+Image+Detection+Christos+Koutlis+Symeon+Papadopoulos",
    "gs_search_success": true,
    "gs_authors": [
      "mFQe8uQAAAAJ",
      "GuhyORoAAAAJ"
    ],
    "citation_count": 52,
    "gs_author_count": 2
  },
  {
    "arxiv_id": "2405.17903",
    "title": "Reliable Object Tracking by Multimodal Hybrid Feature Extraction and Transformer-Based Fusion",
    "year": 2024,
    "published": "2024-05-28T07:24:56Z",
    "categories": [
      "cs.CV",
      "q-bio.NC"
    ],
    "abstract": "Visual object tracking, which is primarily based on visible light image sequences, encounters numerous challenges in complicated scenarios, such as low light conditions, high dynamic ranges, and background clutter. To address these challenges, incorporating the advantages of multiple visual modalities is a promising solution for achieving reliable object tracking. However, the existing approaches usually integrate multimodal inputs through adaptive local feature interactions, which cannot levera",
    "arxiv_url": "https://arxiv.org/abs/2405.17903v1",
    "pdf_url": "https://arxiv.org/pdf/2405.17903v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2405.17903",
    "arxiv_authors": [
      "Hongze Sun",
      "Rui Liu",
      "Wuque Cai",
      "Jun Wang",
      "Yue Wang",
      "Huajin Tang",
      "Yan Cui",
      "Dezhong Yao",
      "Daqing Guo"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Reliable+Object+Tracking+by+Multimodal+Hybrid+Feature+Extraction+and+Transformer-Based+Fusion+Hongze+Sun+Rui+Liu+Wuque+Cai+Jun+Wang+Yue+Wang",
    "gs_search_success": true,
    "gs_authors": [
      "U041O4QAAAAJ",
      "wotvHt4AAAAJ",
      "r3XU9PEAAAAJ",
      "ClUoWqsAAAAJ",
      "RNa8D1sAAAAJ"
    ],
    "citation_count": 16,
    "gs_author_count": 9
  },
  {
    "arxiv_id": "2501.06939",
    "title": "Super-Resolution of 3D Micro-CT Images Using Generative Adversarial Networks: Enhancing Resolution and Segmentation Accuracy",
    "year": 2025,
    "published": "2025-01-12T21:33:06Z",
    "categories": [
      "eess.IV",
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "We develop a procedure for substantially improving the quality of segmented 3D micro-Computed Tomography (micro-CT) images of rocks with a Machine Learning (ML) Generative Model. The proposed model enhances the resolution eightfold (8x) and addresses segmentation inaccuracies due to the overlapping X-ray attenuation in micro-CT measurement for different rock minerals and phases. The proposed generative model is a 3D Deep Convolutional Wasserstein Generative Adversarial Network with Gradient Pena",
    "arxiv_url": "https://arxiv.org/abs/2501.06939v1",
    "pdf_url": "https://arxiv.org/pdf/2501.06939v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2501.06939",
    "arxiv_authors": [
      "Evgeny Ugolkov",
      "Xupeng He",
      "Hyung Kwak",
      "Hussein Hoteit"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Super-Resolution+of+3D+Micro-CT+Images+Using+Generative+Adversarial+Networks%3A+Enhancing+Resolution+and+Segmentation+Accuracy+Evgeny+Ugolkov+Xupeng+He+Hyung+Kwak+Hussein+Hoteit",
    "gs_search_success": true,
    "gs_authors": [
      "e6KK_P4AAAAJ",
      "WOh8qIYAAAAJ",
      "vtU-5dYAAAAJ"
    ],
    "citation_count": 4,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2312.05907",
    "title": "Hypergraph-Guided Disentangled Spectrum Transformer Networks for Near-Infrared Facial Expression Recognition",
    "year": 2023,
    "published": "2023-12-10T15:15:50Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.HC"
    ],
    "abstract": "With the strong robusticity on illumination variations, near-infrared (NIR) can be an effective and essential complement to visible (VIS) facial expression recognition in low lighting or complete darkness conditions. However, facial expression recognition (FER) from NIR images presents more challenging problem than traditional FER due to the limitations imposed by the data scale and the difficulty of extracting discriminative features from incomplete visible lighting contents. In this paper, we ",
    "arxiv_url": "https://arxiv.org/abs/2312.05907v1",
    "pdf_url": "https://arxiv.org/pdf/2312.05907v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2312.05907",
    "arxiv_authors": [
      "Bingjun Luo",
      "Haowen Wang",
      "Jinpeng Wang",
      "Junjie Zhu",
      "Xibin Zhao",
      "Yue Gao"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Hypergraph-Guided+Disentangled+Spectrum+Transformer+Networks+for+Near-Infrared+Facial+Expression+Recognition+Bingjun+Luo+Haowen+Wang+Jinpeng+Wang+Junjie+Zhu+Xibin+Zhao",
    "gs_search_success": true,
    "gs_authors": [
      "UTDfWocAAAAJ",
      "GpczvQYAAAAJ",
      "zZJBCz4AAAAJ"
    ],
    "citation_count": 3,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2304.01986",
    "title": "USTC FLICAR: A Sensors Fusion Dataset of LiDAR-Inertial-Camera for Heavy-duty Autonomous Aerial Work Robots",
    "year": 2023,
    "published": "2023-04-04T17:45:06Z",
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "abstract": "In this paper, we present the USTC FLICAR Dataset, which is dedicated to the development of simultaneous localization and mapping and precise 3D reconstruction of the workspace for heavy-duty autonomous aerial work robots. In recent years, numerous public datasets have played significant roles in the advancement of autonomous cars and unmanned aerial vehicles (UAVs). However, these two platforms differ from aerial work robots: UAVs are limited in their payload capacity, while cars are restricted",
    "arxiv_url": "https://arxiv.org/abs/2304.01986v2",
    "pdf_url": "https://arxiv.org/pdf/2304.01986v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2304.01986",
    "arxiv_authors": [
      "Ziming Wang",
      "Yujiang Liu",
      "Yifan Duan",
      "Xingchen Li",
      "Xinran Zhang",
      "Jianmin Ji",
      "Erbao Dong",
      "Yanyong Zhang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=USTC+FLICAR%3A+A+Sensors+Fusion+Dataset+of+LiDAR-Inertial-Camera+for+Heavy-duty+Autonomous+Aerial+Work+Robots+Ziming+Wang+Yujiang+Liu+Yifan+Duan+Xingchen+Li+Xinran+Zhang",
    "gs_search_success": true,
    "gs_authors": [
      "qfbPQ1YAAAAJ",
      "eky7QqwAAAAJ",
      "pqtFkYkAAAAJ",
      "o2L3FIQAAAAJ",
      "dbpeb5sAAAAJ"
    ],
    "citation_count": 6,
    "gs_author_count": 8
  },
  {
    "arxiv_id": "2409.20018",
    "title": "Visual Context Window Extension: A New Perspective for Long Video Understanding",
    "year": 2024,
    "published": "2024-09-30T07:25:16Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Large Multimodal Models (LMMs) have demonstrated impressive performance in short video understanding tasks but face great challenges when applied to long video understanding. In contrast, Large Language Models (LLMs) exhibit outstanding capabilities in modeling long texts. Existing work attempts to address this issue by introducing long video-text pairs during training. However, these approaches require substantial computational and data resources. In this paper, we tackle the challenge of long ",
    "arxiv_url": "https://arxiv.org/abs/2409.20018v2",
    "pdf_url": "https://arxiv.org/pdf/2409.20018v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2409.20018",
    "arxiv_authors": [
      "Hongchen Wei",
      "Zhenzhong Chen"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Visual+Context+Window+Extension%3A+A+New+Perspective+for+Long+Video+Understanding+Hongchen+Wei+Zhenzhong+Chen",
    "gs_search_success": true,
    "gs_authors": [
      "w_BcpK8AAAAJ",
      "CCQ3YsEAAAAJ"
    ],
    "citation_count": 13,
    "gs_author_count": 2
  },
  {
    "arxiv_id": "2404.00380",
    "title": "DHR: Dual Features-Driven Hierarchical Rebalancing in Inter- and Intra-Class Regions for Weakly-Supervised Semantic Segmentation",
    "year": 2024,
    "published": "2024-03-30T14:35:31Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Weakly-supervised semantic segmentation (WSS) ensures high-quality segmentation with limited data and excels when employed as input seed masks for large-scale vision models such as Segment Anything. However, WSS faces challenges related to minor classes since those are overlooked in images with adjacent multiple classes, a limitation originating from the overfitting of traditional expansion methods like Random Walk. We first address this by employing unsupervised and weakly-supervised feature ma",
    "arxiv_url": "https://arxiv.org/abs/2404.00380v2",
    "pdf_url": "https://arxiv.org/pdf/2404.00380v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2404.00380",
    "arxiv_authors": [
      "Sanghyun Jo",
      "Fei Pan",
      "In-Jae Yu",
      "Kyungsu Kim"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=DHR%3A+Dual+Features-Driven+Hierarchical+Rebalancing+in+Inter-+and+Intra-Class+Regions+for+Weakly-Supervised+Semantic+Segmentation+Sanghyun+Jo+Fei+Pan+In-Jae+Yu+Kyungsu+Kim",
    "gs_search_success": true,
    "gs_authors": [
      "Q5qZWJoAAAAJ",
      "VGE3DlYAAAAJ",
      "RbJDbtgAAAAJ",
      "xgP6q2YAAAAJ"
    ],
    "citation_count": 3,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2405.19334",
    "title": "LLMs Meet Multimodal Generation and Editing: A Survey",
    "year": 2024,
    "published": "2024-05-29T17:59:20Z",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.CV",
      "cs.MM",
      "cs.SD"
    ],
    "abstract": "With the recent advancement in large language models (LLMs), there is a growing interest in combining LLMs with multimodal learning. Previous surveys of multimodal large language models (MLLMs) mainly focus on multimodal understanding. This survey elaborates on multimodal generation and editing across various domains, comprising image, video, 3D, and audio. Specifically, we summarize the notable advancements with milestone works in these fields and categorize these studies into LLM-based and CLI",
    "arxiv_url": "https://arxiv.org/abs/2405.19334v2",
    "pdf_url": "https://arxiv.org/pdf/2405.19334v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2405.19334",
    "arxiv_authors": [
      "Yingqing He",
      "Zhaoyang Liu",
      "Jingye Chen",
      "Zeyue Tian",
      "Hongyu Liu",
      "Xiaowei Chi",
      "Runtao Liu",
      "Ruibin Yuan",
      "Yazhou Xing",
      "Wenhai Wang",
      "Jifeng Dai",
      "Yong Zhang",
      "Wei Xue",
      "Qifeng Liu",
      "Yike Guo",
      "Qifeng Chen"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=LLMs+Meet+Multimodal+Generation+and+Editing%3A+A+Survey+Yingqing+He+Zhaoyang+Liu+Jingye+Chen+Zeyue+Tian+Hongyu+Liu",
    "gs_search_success": true,
    "gs_authors": [
      "bLRjUzAAAAAJ",
      "zfjjlw8AAAAJ",
      "bKIUDxUAAAAJ",
      "WM0OglcAAAAJ",
      "SH_-B_AAAAAJ",
      "Vl1X_-sAAAAJ",
      "Qd_hX1cAAAAJ",
      "dghq4MQAAAAJ"
    ],
    "citation_count": 44,
    "gs_author_count": 12
  },
  {
    "arxiv_id": "2310.10123",
    "title": "AutoDIR: Automatic All-in-One Image Restoration with Latent Diffusion",
    "year": 2023,
    "published": "2023-10-16T07:00:32Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "We present AutoDIR, an innovative all-in-one image restoration system incorporating latent diffusion. AutoDIR excels in its ability to automatically identify and restore images suffering from a range of unknown degradations. AutoDIR offers intuitive open-vocabulary image editing, empowering users to customize and enhance images according to their preferences. Specifically, AutoDIR consists of two key stages: a Blind Image Quality Assessment (BIQA) stage based on a semantic-agnostic vision-langua",
    "arxiv_url": "https://arxiv.org/abs/2310.10123v5",
    "pdf_url": "https://arxiv.org/pdf/2310.10123v5",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2310.10123",
    "arxiv_authors": [
      "Yitong Jiang",
      "Zhaoyang Zhang",
      "Tianfan Xue",
      "Jinwei Gu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=AutoDIR%3A+Automatic+All-in-One+Image+Restoration+with+Latent+Diffusion+Yitong+Jiang+Zhaoyang+Zhang+Tianfan+Xue+Jinwei+Gu",
    "gs_search_success": true,
    "gs_authors": [
      "Pf6o7uAAAAAJ",
      "k_T8t30AAAAJ",
      "RfSQKrIAAAAJ",
      "5gStTm4AAAAJ"
    ],
    "citation_count": 74,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2411.09077",
    "title": "Drone Detection using Deep Neural Networks Trained on Pure Synthetic Data",
    "year": 2024,
    "published": "2024-11-13T23:09:53Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "abstract": "Drone detection has benefited from improvements in deep neural networks, but like many other applications, suffers from the availability of accurate data for training. Synthetic data provides a potential for low-cost data generation and has been shown to improve data availability and quality. However, models trained on synthetic datasets need to prove their ability to perform on real-world data, known as the problem of sim-to-real transferability. Here, we present a drone detection Faster-RCNN m",
    "arxiv_url": "https://arxiv.org/abs/2411.09077v1",
    "pdf_url": "https://arxiv.org/pdf/2411.09077v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2411.09077",
    "arxiv_authors": [
      "Mariusz Wisniewski",
      "Zeeshan A. Rana",
      "Ivan Petrunin",
      "Alan Holt",
      "Stephen Harman"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Drone+Detection+using+Deep+Neural+Networks+Trained+on+Pure+Synthetic+Data+Mariusz+Wisniewski+Zeeshan+A.+Rana+Ivan+Petrunin+Alan+Holt+Stephen+Harman",
    "gs_search_success": true,
    "gs_authors": [
      "WpYcufQAAAAJ",
      "nxDPceEAAAAJ",
      "72oEv0UAAAAJ"
    ],
    "citation_count": 5,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2505.19944",
    "title": "Can Visual Encoder Learn to See Arrows?",
    "year": 2025,
    "published": "2025-05-26T13:09:31Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "abstract": "The diagram is a visual representation of a relationship illustrated with edges (lines or arrows), which is widely used in industrial and scientific communication. Although recognizing diagrams is essential for vision language models (VLMs) to comprehend domain-specific knowledge, recent studies reveal that many VLMs fail to identify edges in images. We hypothesize that these failures stem from an over-reliance on textual and positional biases, preventing VLMs from learning explicit edge feature",
    "arxiv_url": "https://arxiv.org/abs/2505.19944v1",
    "pdf_url": "https://arxiv.org/pdf/2505.19944v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2505.19944",
    "arxiv_authors": [
      "Naoyuki Terashita",
      "Yusuke Tozaki",
      "Hideaki Omote",
      "Congkha Nguyen",
      "Ryosuke Nakamoto",
      "Yuta Koreeda",
      "Hiroaki Ozaki"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Can+Visual+Encoder+Learn+to+See+Arrows%3F+Naoyuki+Terashita+Yusuke+Tozaki+Hideaki+Omote+Congkha+Nguyen+Ryosuke+Nakamoto",
    "gs_search_success": true,
    "gs_authors": [
      "zUJ8Ao4AAAAJ",
      "eJUrwu4AAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 7
  },
  {
    "arxiv_id": "2305.02572",
    "title": "High-fidelity Generalized Emotional Talking Face Generation with Multi-modal Emotion Space Learning",
    "year": 2023,
    "published": "2023-05-04T05:59:34Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Recently, emotional talking face generation has received considerable attention. However, existing methods only adopt one-hot coding, image, or audio as emotion conditions, thus lacking flexible control in practical applications and failing to handle unseen emotion styles due to limited semantics. They either ignore the one-shot setting or the quality of generated faces. In this paper, we propose a more flexible and generalized framework. Specifically, we supplement the emotion style in text pro",
    "arxiv_url": "https://arxiv.org/abs/2305.02572v2",
    "pdf_url": "https://arxiv.org/pdf/2305.02572v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2305.02572",
    "arxiv_authors": [
      "Chao Xu",
      "Junwei Zhu",
      "Jiangning Zhang",
      "Yue Han",
      "Wenqing Chu",
      "Ying Tai",
      "Chengjie Wang",
      "Zhifeng Xie",
      "Yong Liu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=High-fidelity+Generalized+Emotional+Talking+Face+Generation+with+Multi-modal+Emotion+Space+Learning+Chao+Xu+Junwei+Zhu+Jiangning+Zhang+Yue+Han+Wenqing+Chu",
    "gs_search_success": true,
    "gs_authors": [
      "08E500gAAAAJ",
      "zlq2S_0AAAAJ",
      "qYcgBbEAAAAJ",
      "2hA4X9wAAAAJ",
      "1Ae0CMgAAAAJ",
      "NKaiUasAAAAJ",
      "fqte5H4AAAAJ",
      "-OxQlHsAAAAJ"
    ],
    "citation_count": 58,
    "gs_author_count": 9
  },
  {
    "arxiv_id": "2403.06403",
    "title": "PointSeg: A Training-Free Paradigm for 3D Scene Segmentation via Foundation Models",
    "year": 2024,
    "published": "2024-03-11T03:28:20Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Recent success of vision foundation models have shown promising performance for the 2D perception tasks. However, it is difficult to train a 3D foundation network directly due to the limited dataset and it remains under explored whether existing foundation models can be lifted to 3D space seamlessly. In this paper, we present PointSeg, a novel training-free paradigm that leverages off-the-shelf vision foundation models to address 3D scene perception tasks. PointSeg can segment anything in 3D sce",
    "arxiv_url": "https://arxiv.org/abs/2403.06403v5",
    "pdf_url": "https://arxiv.org/pdf/2403.06403v5",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2403.06403",
    "arxiv_authors": [
      "Qingdong He",
      "Jinlong Peng",
      "Zhengkai Jiang",
      "Xiaobin Hu",
      "Jiangning Zhang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=PointSeg%3A+A+Training-Free+Paradigm+for+3D+Scene+Segmentation+via+Foundation+Models+Qingdong+He+Jinlong+Peng+Zhengkai+Jiang+Xiaobin+Hu+Jiangning+Zhang",
    "gs_search_success": true,
    "gs_authors": [
      "ooBQi6EAAAAJ",
      "2hA4X9wAAAAJ",
      "gUJWww0AAAAJ",
      "3lMuodUAAAAJ",
      "i5I-cIEAAAAJ"
    ],
    "citation_count": 7,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2502.18496",
    "title": "Physical Depth-aware Early Accident Anticipation: A Multi-dimensional Visual Feature Fusion Framework",
    "year": 2025,
    "published": "2025-02-19T09:07:38Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Early accident anticipation from dashcam videos is a highly desirable yet challenging task for improving the safety of intelligent vehicles. Existing advanced accident anticipation approaches commonly model the interaction among traffic agents (e.g., vehicles, pedestrians, etc.) in the coarse 2D image space, which may not adequately capture their true positions and interactions. To address this limitation, we propose a physical depth-aware learning framework that incorporates the monocular depth",
    "arxiv_url": "https://arxiv.org/abs/2502.18496v1",
    "pdf_url": "https://arxiv.org/pdf/2502.18496v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2502.18496",
    "arxiv_authors": [
      "Hongpu Huang",
      "Wei Zhou",
      "Chen Wang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Physical+Depth-aware+Early+Accident+Anticipation%3A+A+Multi-dimensional+Visual+Feature+Fusion+Framework+Hongpu+Huang+Wei+Zhou+Chen+Wang",
    "gs_search_success": true,
    "gs_authors": [
      "pJQ6KKsAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2308.04126",
    "title": "OmniDataComposer: A Unified Data Structure for Multimodal Data Fusion and Infinite Data Generation",
    "year": 2023,
    "published": "2023-08-08T08:30:16Z",
    "categories": [
      "cs.CV",
      "cs.LG",
      "cs.MM",
      "cs.SD",
      "eess.AS"
    ],
    "abstract": "This paper presents OmniDataComposer, an innovative approach for multimodal data fusion and unlimited data generation with an intent to refine and uncomplicate interplay among diverse data modalities. Coming to the core breakthrough, it introduces a cohesive data structure proficient in processing and merging multimodal data inputs, which include video, audio, and text.   Our crafted algorithm leverages advancements across multiple operations such as video/image caption extraction, dense caption",
    "arxiv_url": "https://arxiv.org/abs/2308.04126v2",
    "pdf_url": "https://arxiv.org/pdf/2308.04126v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2308.04126",
    "arxiv_authors": [
      "Dongyang Yu",
      "Shihao Wang",
      "Yuan Fang",
      "Wangpeng An"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=OmniDataComposer%3A+A+Unified+Data+Structure+for+Multimodal+Data+Fusion+and+Infinite+Data+Generation+Dongyang+Yu+Shihao+Wang+Yuan+Fang+Wangpeng+An",
    "gs_search_success": true,
    "gs_authors": [
      "tka8lTYAAAAJ",
      "Fdhx884AAAAJ",
      "jSHhH8SwANwC"
    ],
    "citation_count": null,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2312.04076",
    "title": "Large Language Models are Good Prompt Learners for Low-Shot Image Classification",
    "year": 2023,
    "published": "2023-12-07T06:43:34Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Low-shot image classification, where training images are limited or inaccessible, has benefited from recent progress on pre-trained vision-language (VL) models with strong generalizability, e.g. CLIP. Prompt learning methods built with VL models generate text features from the class names that only have confined class-specific information. Large Language Models (LLMs), with their vast encyclopedic knowledge, emerge as the complement. Thus, in this paper, we discuss the integration of LLMs to enh",
    "arxiv_url": "https://arxiv.org/abs/2312.04076v2",
    "pdf_url": "https://arxiv.org/pdf/2312.04076v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2312.04076",
    "arxiv_authors": [
      "Zhaoheng Zheng",
      "Jingmin Wei",
      "Xuefeng Hu",
      "Haidong Zhu",
      "Ram Nevatia"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Large+Language+Models+are+Good+Prompt+Learners+for+Low-Shot+Image+Classification+Zhaoheng+Zheng+Jingmin+Wei+Xuefeng+Hu+Haidong+Zhu+Ram+Nevatia",
    "gs_search_success": true,
    "gs_authors": [
      "36e4ADAAAAAJ",
      "EUMYhUvzt6IC",
      "oBc9CCkAAAAJ",
      "-7PW10UAAAAJ",
      "Ee616UkAAAAJ"
    ],
    "citation_count": 30,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2310.13585",
    "title": "POTLoc: Pseudo-Label Oriented Transformer for Point-Supervised Temporal Action Localization",
    "year": 2023,
    "published": "2023-10-20T15:28:06Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "This paper tackles the challenge of point-supervised temporal action detection, wherein only a single frame is annotated for each action instance in the training set. Most of the current methods, hindered by the sparse nature of annotated points, struggle to effectively represent the continuous structure of actions or the inherent temporal and semantic dependencies within action instances. Consequently, these methods frequently learn merely the most distinctive segments of actions, leading to th",
    "arxiv_url": "https://arxiv.org/abs/2310.13585v2",
    "pdf_url": "https://arxiv.org/pdf/2310.13585v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2310.13585",
    "arxiv_authors": [
      "Elahe Vahdani",
      "Yingli Tian"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=POTLoc%3A+Pseudo-Label+Oriented+Transformer+for+Point-Supervised+Temporal+Action+Localization+Elahe+Vahdani+Yingli+Tian",
    "gs_search_success": true,
    "gs_authors": [
      "8UCruqIAAAAJ",
      "aAWeB4wAAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 1
  },
  {
    "arxiv_id": "2411.07621",
    "title": "Mix from Failure: Confusion-Pairing Mixup for Long-Tailed Recognition",
    "year": 2024,
    "published": "2024-11-12T08:08:31Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Long-tailed image recognition is a computer vision problem considering a real-world class distribution rather than an artificial uniform. Existing methods typically detour the problem by i) adjusting a loss function, ii) decoupling classifier learning, or iii) proposing a new multi-head architecture called experts. In this paper, we tackle the problem from a different perspective to augment a training dataset to enhance the sample diversity of minority classes. Specifically, our method, namely C",
    "arxiv_url": "https://arxiv.org/abs/2411.07621v2",
    "pdf_url": "https://arxiv.org/pdf/2411.07621v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2411.07621",
    "arxiv_authors": [
      "Youngseok Yoon",
      "Sangwoo Hong",
      "Hyungjun Joo",
      "Yao Qin",
      "Haewon Jeong",
      "Jungwoo Lee"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Mix+from+Failure%3A+Confusion-Pairing+Mixup+for+Long-Tailed+Recognition+Youngseok+Yoon+Sangwoo+Hong+Hyungjun+Joo+Yao+Qin+Haewon+Jeong",
    "gs_search_success": true,
    "gs_authors": [
      "zYn8YZ8AAAAJ",
      "GdqNkxEAAAAJ",
      "h8wIUwUAAAAJ",
      "b3MwZEwAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2311.16480",
    "title": "WsiCaption: Multiple Instance Generation of Pathology Reports for Gigapixel Whole-Slide Images",
    "year": 2023,
    "published": "2023-11-27T05:05:41Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "abstract": "Whole slide images are the foundation of digital pathology for the diagnosis and treatment of carcinomas. Writing pathology reports is laborious and error-prone for inexperienced pathologists. To reduce the workload and improve clinical automation, we investigate how to generate pathology reports given whole slide images. On the data end, we curated the largest WSI-text dataset (PathText). In specific, we collected nearly 10000 high-quality WSI-text pairs for visual-language models by recognizin",
    "arxiv_url": "https://arxiv.org/abs/2311.16480v4",
    "pdf_url": "https://arxiv.org/pdf/2311.16480v4",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2311.16480",
    "arxiv_authors": [
      "Pingyi Chen",
      "Honglin Li",
      "Chenglu Zhu",
      "Sunyi Zheng",
      "Zhongyi Shui",
      "Lin Yang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=WsiCaption%3A+Multiple+Instance+Generation+of+Pathology+Reports+for+Gigapixel+Whole-Slide+Images+Pingyi+Chen+Honglin+Li+Chenglu+Zhu+Sunyi+Zheng+Zhongyi+Shui",
    "gs_search_success": true,
    "gs_authors": [
      "kv5ypDwAAAAJ",
      "K3HiLBwAAAAJ",
      "CTGCfsgAAAAJ",
      "fskm0zEAAAAJ",
      "VJYS9dMAAAAJ",
      "DhEvAAwAAAAJ"
    ],
    "citation_count": 26,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2409.19459",
    "title": "Language-guided Robust Navigation for Mobile Robots in Dynamically-changing Environments",
    "year": 2024,
    "published": "2024-09-28T21:30:23Z",
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "abstract": "In this paper, we develop an embodied AI system for human-in-the-loop navigation with a wheeled mobile robot. We propose a direct yet effective method of monitoring the robot's current plan to detect changes in the environment that impact the intended trajectory of the robot significantly and then query a human for feedback. We also develop a means to parse human feedback expressed in natural language into local navigation waypoints and integrate it into a global planning system, by leveraging a",
    "arxiv_url": "https://arxiv.org/abs/2409.19459v1",
    "pdf_url": "https://arxiv.org/pdf/2409.19459v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2409.19459",
    "arxiv_authors": [
      "Cody Simons",
      "Zhichao Liu",
      "Brandon Marcus",
      "Amit K. Roy-Chowdhury",
      "Konstantinos Karydis"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Language-guided+Robust+Navigation+for+Mobile+Robots+in+Dynamically-changing+Environments+Cody+Simons+Zhichao+Liu+Brandon+Marcus+Amit+K.+Roy-Chowdhury+Konstantinos+Karydis",
    "gs_search_success": true,
    "gs_authors": [
      "YVGT7oEAAAAJ",
      "3VQgD7UAAAAJ",
      "4Urexvi1sIcC",
      "98Dftx4AAAAJ",
      "hfgwx0oAAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2402.13796",
    "title": "Scalable Methods for Brick Kiln Detection and Compliance Monitoring from Satellite Imagery: A Deployment Case Study in India",
    "year": 2024,
    "published": "2024-02-21T13:26:00Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Air pollution kills 7 million people annually. Brick manufacturing industry is the second largest consumer of coal contributing to 8%-14% of air pollution in Indo-Gangetic plain (highly populated tract of land in the Indian subcontinent). As brick kilns are an unorganized sector and present in large numbers, detecting policy violations such as distance from habitat is non-trivial. Air quality and other domain experts rely on manual human annotation to maintain brick kiln inventory. Previous work",
    "arxiv_url": "https://arxiv.org/abs/2402.13796v1",
    "pdf_url": "https://arxiv.org/pdf/2402.13796v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2402.13796",
    "arxiv_authors": [
      "Rishabh Mondal",
      "Zeel B Patel",
      "Vannsh Jani",
      "Nipun Batra"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Scalable+Methods+for+Brick+Kiln+Detection+and+Compliance+Monitoring+from+Satellite+Imagery%3A+A+Deployment+Case+Study+in+India+Rishabh+Mondal+Zeel+B+Patel+Vannsh+Jani+Nipun+Batra",
    "gs_search_success": true,
    "gs_authors": [
      "oGYVSX0AAAAJ",
      "rFGzHlIAAAAJ",
      "7LY1_u4AAAAJ"
    ],
    "citation_count": 2,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2312.07538",
    "title": "Anatomically Constrained Implicit Face Models",
    "year": 2023,
    "published": "2023-12-12T18:59:21Z",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "abstract": "Coordinate based implicit neural representations have gained rapid popularity in recent years as they have been successfully used in image, geometry and scene modeling tasks. In this work, we present a novel use case for such implicit representations in the context of learning anatomically constrained face models. Actor specific anatomically constrained face models are the state of the art in both facial performance capture and performance retargeting. Despite their practical success, these anat",
    "arxiv_url": "https://arxiv.org/abs/2312.07538v1",
    "pdf_url": "https://arxiv.org/pdf/2312.07538v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2312.07538",
    "arxiv_authors": [
      "Prashanth Chandran",
      "Gaspard Zoss"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Anatomically+Constrained+Implicit+Face+Models+Prashanth+Chandran+Gaspard+Zoss",
    "gs_search_success": true,
    "gs_authors": [
      "5_6WiFQAAAAJ",
      "4hdhV7AAAAAJ"
    ],
    "citation_count": 5,
    "gs_author_count": 2
  },
  {
    "arxiv_id": "2505.22067",
    "title": "From Failures to Fixes: LLM-Driven Scenario Repair for Self-Evolving Autonomous Driving",
    "year": 2025,
    "published": "2025-05-28T07:46:19Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.RO"
    ],
    "abstract": "Ensuring robust and generalizable autonomous driving requires not only broad scenario coverage but also efficient repair of failure cases, particularly those related to challenging and safety-critical scenarios. However, existing scenario generation and selection methods often lack adaptivity and semantic relevance, limiting their impact on performance improvement. In this paper, we propose \\textbf{SERA}, an LLM-powered framework that enables autonomous driving systems to self-evolve by repairin",
    "arxiv_url": "https://arxiv.org/abs/2505.22067v1",
    "pdf_url": "https://arxiv.org/pdf/2505.22067v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2505.22067",
    "arxiv_authors": [
      "Xinyu Xia",
      "Xingjun Ma",
      "Yunfeng Hu",
      "Ting Qu",
      "Hong Chen",
      "Xun Gong"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=From+Failures+to+Fixes%3A+LLM-Driven+Scenario+Repair+for+Self-Evolving+Autonomous+Driving+Xinyu+Xia+Xingjun+Ma+Yunfeng+Hu+Ting+Qu+Hong+Chen",
    "gs_search_success": true,
    "gs_authors": [
      "XQViiyYAAAAJ",
      "9Y6NdJAAAAAJ",
      "gpY6lCcAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2502.14023",
    "title": "Dynamic Activation with Knowledge Distillation for Energy-Efficient Spiking NN Ensembles",
    "year": 2025,
    "published": "2025-02-19T18:50:08Z",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "cs.NE"
    ],
    "abstract": "While foundation AI models excel at tasks like classification and decision-making, their high energy consumption makes them unsuitable for energy-constrained applications. Inspired by the brain's efficiency, spiking neural networks (SNNs) have emerged as a viable alternative due to their event-driven nature and compatibility with neuromorphic chips. This work introduces a novel system that combines knowledge distillation and ensemble learning to bridge the performance gap between artificial neur",
    "arxiv_url": "https://arxiv.org/abs/2502.14023v1",
    "pdf_url": "https://arxiv.org/pdf/2502.14023v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2502.14023",
    "arxiv_authors": [
      "Orestis Konstantaropoulos",
      "Theodoris Mallios",
      "Maria Papadopouli"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Dynamic+Activation+with+Knowledge+Distillation+for+Energy-Efficient+Spiking+NN+Ensembles+Orestis+Konstantaropoulos+Theodoris+Mallios+Maria+Papadopouli",
    "gs_search_success": true,
    "gs_authors": [
      "wZFtgBwAAAAJ",
      "BWyI_HQAAAAJ"
    ],
    "citation_count": 3,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2309.10255",
    "title": "RGB-based Category-level Object Pose Estimation via Decoupled Metric Scale Recovery",
    "year": 2023,
    "published": "2023-09-19T02:20:26Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "While showing promising results, recent RGB-D camera-based category-level object pose estimation methods have restricted applications due to the heavy reliance on depth sensors. RGB-only methods provide an alternative to this problem yet suffer from inherent scale ambiguity stemming from monocular observations. In this paper, we propose a novel pipeline that decouples the 6D pose and size estimation to mitigate the influence of imperfect scales on rigid transformations. Specifically, we leverage",
    "arxiv_url": "https://arxiv.org/abs/2309.10255v2",
    "pdf_url": "https://arxiv.org/pdf/2309.10255v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2309.10255",
    "arxiv_authors": [
      "Jiaxin Wei",
      "Xibin Song",
      "Weizhe Liu",
      "Laurent Kneip",
      "Hongdong Li",
      "Pan Ji"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=RGB-based+Category-level+Object+Pose+Estimation+via+Decoupled+Metric+Scale+Recovery+Jiaxin+Wei+Xibin+Song+Weizhe+Liu+Laurent+Kneip+Hongdong+Li",
    "gs_search_success": true,
    "gs_authors": [
      "CQdOGEAAAAAJ",
      "8twuSywAAAAJ",
      "p351VxAAAAAJ",
      "Mq89JAcAAAAJ",
      "lTmh1e0AAAAJ",
      "2gudyEQAAAAJ"
    ],
    "citation_count": 21,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2405.17447",
    "title": "How to train your ViT for OOD Detection",
    "year": 2024,
    "published": "2024-05-21T08:36:30Z",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "VisionTransformers have been shown to be powerful out-of-distribution detectors for ImageNet-scale settings when finetuned from publicly available checkpoints, often outperforming other model types on popular benchmarks. In this work, we investigate the impact of both the pretraining and finetuning scheme on the performance of ViTs on this task by analyzing a large pool of models. We find that the exact type of pretraining has a strong impact on which method works well and on OOD detection perfo",
    "arxiv_url": "https://arxiv.org/abs/2405.17447v1",
    "pdf_url": "https://arxiv.org/pdf/2405.17447v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2405.17447",
    "arxiv_authors": [
      "Maximilian Mueller",
      "Matthias Hein"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=How+to+train+your+ViT+for+OOD+Detection+Maximilian+Mueller+Matthias+Hein",
    "gs_search_success": true,
    "gs_authors": [
      "0ZAb3tsAAAAJ",
      "ii_xC_sAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 2
  },
  {
    "arxiv_id": "2302.13748",
    "title": "Unsupervised Video Anomaly Detection for Stereotypical Behaviours in Autism",
    "year": 2023,
    "published": "2023-02-27T13:24:08Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Monitoring and analyzing stereotypical behaviours is important for early intervention and care taking in Autism Spectrum Disorder (ASD). This paper focuses on automatically detecting stereotypical behaviours with computer vision techniques. Off-the-shelf methods tackle this task by supervised classification and activity recognition techniques. However, the unbounded types of stereotypical behaviours and the difficulty in collecting video recordings of ASD patients largely limit the feasibility o",
    "arxiv_url": "https://arxiv.org/abs/2302.13748v1",
    "pdf_url": "https://arxiv.org/pdf/2302.13748v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2302.13748",
    "arxiv_authors": [
      "Jiaqi Gao",
      "Xinyang Jiang",
      "Yuqing Yang",
      "Dongsheng Li",
      "Lili Qiu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Unsupervised+Video+Anomaly+Detection+for+Stereotypical+Behaviours+in+Autism+Jiaqi+Gao+Xinyang+Jiang+Yuqing+Yang+Dongsheng+Li+Lili+Qiu",
    "gs_search_success": true,
    "gs_authors": [
      "JiTfWVMAAAAJ",
      "4BtNQAEAAAAJ",
      "egyYq1EAAAAJ",
      "_WrK108AAAAJ",
      "16posrQAAAAJ"
    ],
    "citation_count": 10,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2312.02158",
    "title": "PaSCo: Urban 3D Panoptic Scene Completion with Uncertainty Awareness",
    "year": 2023,
    "published": "2023-12-04T18:59:59Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "We propose the task of Panoptic Scene Completion (PSC) which extends the recently popular Semantic Scene Completion (SSC) task with instance-level information to produce a richer understanding of the 3D scene. Our PSC proposal utilizes a hybrid mask-based technique on the non-empty voxels from sparse multi-scale completions. Whereas the SSC literature overlooks uncertainty which is critical for robotics applications, we instead propose an efficient ensembling to estimate both voxel-wise and inst",
    "arxiv_url": "https://arxiv.org/abs/2312.02158v2",
    "pdf_url": "https://arxiv.org/pdf/2312.02158v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2312.02158",
    "arxiv_authors": [
      "Anh-Quan Cao",
      "Angela Dai",
      "Raoul de Charette"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=PaSCo%3A+Urban+3D+Panoptic+Scene+Completion+with+Uncertainty+Awareness+Anh-Quan+Cao+Angela+Dai+Raoul+de+Charette",
    "gs_search_success": true,
    "gs_authors": [
      "kQ5VuJEAAAAJ",
      "g-tGztMAAAAJ",
      "ygOSeCQAAAAJ"
    ],
    "citation_count": 36,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2307.02783",
    "title": "UIT-Saviors at MEDVQA-GI 2023: Improving Multimodal Learning with Image Enhancement for Gastrointestinal Visual Question Answering",
    "year": 2023,
    "published": "2023-07-06T05:22:20Z",
    "categories": [
      "cs.CV",
      "cs.HC"
    ],
    "abstract": "In recent years, artificial intelligence has played an important role in medicine and disease diagnosis, with many applications to be mentioned, one of which is Medical Visual Question Answering (MedVQA). By combining computer vision and natural language processing, MedVQA systems can assist experts in extracting relevant information from medical image based on a given question and providing precise diagnostic answers. The ImageCLEFmed-MEDVQA-GI-2023 challenge carried out visual question answeri",
    "arxiv_url": "https://arxiv.org/abs/2307.02783v2",
    "pdf_url": "https://arxiv.org/pdf/2307.02783v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2307.02783",
    "arxiv_authors": [
      "Triet M. Thai",
      "Anh T. Vo",
      "Hao K. Tieu",
      "Linh N. P. Bui",
      "Thien T. B. Nguyen"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=UIT-Saviors+at+MEDVQA-GI+2023%3A+Improving+Multimodal+Learning+with+Image+Enhancement+for+Gastrointestinal+Visual+Question+Answering+Triet+M.+Thai+Anh+T.+Vo+Hao+K.+Tieu+Linh+N.+P.+Bui+Thien+T.+B.+Nguyen",
    "gs_search_success": true,
    "gs_authors": [
      "NDjES6EAAAAJ",
      "b__ugOkAAAAJ",
      "mnCsJGgAAAAJ"
    ],
    "citation_count": 4,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2310.14532",
    "title": "Practical Deep Dispersed Watermarking with Synchronization and Fusion",
    "year": 2023,
    "published": "2023-10-23T03:34:05Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Deep learning based blind watermarking works have gradually emerged and achieved impressive performance. However, previous deep watermarking studies mainly focus on fixed low-resolution images while paying less attention to arbitrary resolution images, especially widespread high-resolution images nowadays. Moreover, most works usually demonstrate robustness against typical non-geometric attacks (\\textit{e.g.}, JPEG compression) but ignore common geometric attacks (\\textit{e.g.}, Rotate) and more",
    "arxiv_url": "https://arxiv.org/abs/2310.14532v1",
    "pdf_url": "https://arxiv.org/pdf/2310.14532v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2310.14532",
    "arxiv_authors": [
      "Hengchang Guo",
      "Qilong Zhang",
      "Junwei Luo",
      "Feng Guo",
      "Wenbin Zhang",
      "Xiaodong Su",
      "Minglei Li"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Practical+Deep+Dispersed+Watermarking+with+Synchronization+and+Fusion+Hengchang+Guo+Qilong+Zhang+Junwei+Luo+Feng+Guo+Wenbin+Zhang",
    "gs_search_success": true,
    "gs_authors": [
      "AMtyhw4AAAAJ",
      "IgPyQWYAAAAJ"
    ],
    "citation_count": 22,
    "gs_author_count": 7
  },
  {
    "arxiv_id": "2409.00372",
    "title": "First Competition on Presentation Attack Detection on ID Card",
    "year": 2024,
    "published": "2024-08-31T07:24:19Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "This paper summarises the Competition on Presentation Attack Detection on ID Cards (PAD-IDCard) held at the 2024 International Joint Conference on Biometrics (IJCB2024). The competition attracted a total of ten registered teams, both from academia and industry. In the end, the participating teams submitted five valid submissions, with eight models to be evaluated by the organisers. The competition presented an independent assessment of current state-of-the-art algorithms. Today, no independent e",
    "arxiv_url": "https://arxiv.org/abs/2409.00372v1",
    "pdf_url": "https://arxiv.org/pdf/2409.00372v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2409.00372",
    "arxiv_authors": [
      "Juan E. Tapia",
      "Naser Damer",
      "Christoph Busch",
      "Juan M. Espin",
      "Javier Barrachina",
      "Alvaro S. Rocamora",
      "Kristof Ocvirk",
      "Leon Alessio",
      "Borut Batagelj",
      "Sushrut Patwardhan",
      "Raghavendra Ramachandra",
      "Raghavendra Mudgalgundurao",
      "Kiran Raja",
      "Daniel Schulz",
      "Carlos Aravena"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=First+Competition+on+Presentation+Attack+Detection+on+ID+Card+Juan+E.+Tapia+Naser+Damer+Christoph+Busch+Juan+M.+Espin+Javier+Barrachina",
    "gs_search_success": true,
    "gs_authors": [
      "bAyT17sAAAAJ",
      "ITzLe2YAAAAJ",
      "2nHVd-0AAAAJ",
      "qsopcXIAAAAJ",
      "BoCjjmMAAAAJ"
    ],
    "citation_count": 13,
    "gs_author_count": 8
  },
  {
    "arxiv_id": "2503.13236",
    "title": "Gradient Extrapolation for Debiased Representation Learning",
    "year": 2025,
    "published": "2025-03-17T14:48:57Z",
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "abstract": "Machine learning classification models trained with empirical risk minimization (ERM) often inadvertently rely on spurious correlations. When absent in the test data, these unintended associations between non-target attributes and target labels lead to poor generalization. This paper addresses this problem from a model optimization perspective and proposes a novel method, Gradient Extrapolation for Debiased Representation Learning (GERNE), designed to learn debiased representations in both known",
    "arxiv_url": "https://arxiv.org/abs/2503.13236v2",
    "pdf_url": "https://arxiv.org/pdf/2503.13236v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2503.13236",
    "arxiv_authors": [
      "Ihab Asaad",
      "Maha Shadaydeh",
      "Joachim Denzler"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Gradient+Extrapolation+for+Debiased+Representation+Learning+Ihab+Asaad+Maha+Shadaydeh+Joachim+Denzler",
    "gs_search_success": true,
    "gs_authors": [
      "EbOfr_YAAAAJ",
      "bhpi3vgAAAAJ",
      "ElQ7URQAAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2306.00595",
    "title": "Revisit Weakly-Supervised Audio-Visual Video Parsing from the Language Perspective",
    "year": 2023,
    "published": "2023-06-01T12:12:22Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "We focus on the weakly-supervised audio-visual video parsing task (AVVP), which aims to identify and locate all the events in audio/visual modalities. Previous works only concentrate on video-level overall label denoising across modalities, but overlook the segment-level label noise, where adjacent video segments (i.e., 1-second video clips) may contain different events. However, recognizing events in the segment is challenging because its label could be any combination of events that occur in t",
    "arxiv_url": "https://arxiv.org/abs/2306.00595v6",
    "pdf_url": "https://arxiv.org/pdf/2306.00595v6",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2306.00595",
    "arxiv_authors": [
      "Yingying Fan",
      "Yu Wu",
      "Bo Du",
      "Yutian Lin"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Revisit+Weakly-Supervised+Audio-Visual+Video+Parsing+from+the+Language+Perspective+Yingying+Fan+Yu+Wu+Bo+Du+Yutian+Lin",
    "gs_search_success": true,
    "gs_authors": [
      "Shy1gnMAAAAJ",
      "Pb_40UcAAAAJ",
      "23SZHUwAAAAJ",
      "gB6Xq5IAAAAJ"
    ],
    "citation_count": 18,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2412.04714",
    "title": "PCTreeS: 3D Point Cloud Tree Species Classification Using Airborne LiDAR Images",
    "year": 2024,
    "published": "2024-12-06T02:09:52Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Reliable large-scale data on the state of forests is crucial for monitoring ecosystem health, carbon stock, and the impact of climate change. Current knowledge of tree species distribution relies heavily on manual data collection in the field, which often takes years to complete, resulting in limited datasets that cover only a small subset of the world's forests. Recent works show that state-of-the-art deep learning models using Light Detection and Ranging (LiDAR) images enable accurate and scal",
    "arxiv_url": "https://arxiv.org/abs/2412.04714v1",
    "pdf_url": "https://arxiv.org/pdf/2412.04714v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2412.04714",
    "arxiv_authors": [
      "Hongjin Lin",
      "Matthew Nazari",
      "Derek Zheng"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=PCTreeS%3A+3D+Point+Cloud+Tree+Species+Classification+Using+Airborne+LiDAR+Images+Hongjin+Lin+Matthew+Nazari+Derek+Zheng",
    "gs_search_success": true,
    "gs_authors": [
      "46y1bUEAAAAJ"
    ],
    "citation_count": 4,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2504.05306",
    "title": "CREA: A Collaborative Multi-Agent Framework for Creative Image Editing and Generation",
    "year": 2025,
    "published": "2025-04-07T17:59:51Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Creativity in AI imagery remains a fundamental challenge, requiring not only the generation of visually compelling content but also the capacity to add novel, expressive, and artistically rich transformations to images. Unlike conventional editing tasks that rely on direct prompt-based modifications, creative image editing requires an autonomous, iterative approach that balances originality, coherence, and artistic intent. To address this, we introduce CREA, a novel multi-agent collaborative fra",
    "arxiv_url": "https://arxiv.org/abs/2504.05306v2",
    "pdf_url": "https://arxiv.org/pdf/2504.05306v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2504.05306",
    "arxiv_authors": [
      "Kavana Venkatesh",
      "Connor Dunlop",
      "Pinar Yanardag"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=CREA%3A+A+Collaborative+Multi-Agent+Framework+for+Creative+Image+Editing+and+Generation+Kavana+Venkatesh+Connor+Dunlop+Pinar+Yanardag",
    "gs_search_success": true,
    "gs_authors": [
      "ss9eXTQAAAAJ",
      "PJXB6GAAAAAJ",
      "qzczdd8AAAAJ"
    ],
    "citation_count": 3,
    "gs_author_count": null
  },
  {
    "arxiv_id": "2310.05938",
    "title": "Component attention network for multimodal dance improvisation recognition",
    "year": 2023,
    "published": "2023-08-24T15:04:30Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.SD",
      "eess.AS"
    ],
    "abstract": "Dance improvisation is an active research topic in the arts. Motion analysis of improvised dance can be challenging due to its unique dynamics. Data-driven dance motion analysis, including recognition and generation, is often limited to skeletal data. However, data of other modalities, such as audio, can be recorded and benefit downstream tasks. This paper explores the application and performance of multimodal fusion methods for human motion recognition in the context of dance improvisation. We ",
    "arxiv_url": "https://arxiv.org/abs/2310.05938v1",
    "pdf_url": "https://arxiv.org/pdf/2310.05938v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2310.05938",
    "arxiv_authors": [
      "Jia Fu",
      "Jiarui Tan",
      "Wenjie Yin",
      "Sepideh Pashami",
      "Mårten Björkman"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Component+attention+network+for+multimodal+dance+improvisation+recognition+Jia+Fu+Jiarui+Tan+Wenjie+Yin+Sepideh+Pashami+M%C3%A5rten+Bj%C3%B6rkman",
    "gs_search_success": true,
    "gs_authors": [
      "ghzuGbcAAAAJ",
      "dP8O7_AAAAAJ",
      "fD3ldVMAAAAJ",
      "vK6jKMoAAAAJ"
    ],
    "citation_count": 4,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2305.17489",
    "title": "Text-to-image Editing by Image Information Removal",
    "year": 2023,
    "published": "2023-05-27T14:48:05Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Diffusion models have demonstrated impressive performance in text-guided image generation. Current methods that leverage the knowledge of these models for image editing either fine-tune them using the input image (e.g., Imagic) or incorporate structure information as additional constraints (e.g., ControlNet). However, fine-tuning large-scale diffusion models on a single image can lead to severe overfitting issues and lengthy inference time. Information leakage from pretrained models also make it",
    "arxiv_url": "https://arxiv.org/abs/2305.17489v2",
    "pdf_url": "https://arxiv.org/pdf/2305.17489v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2305.17489",
    "arxiv_authors": [
      "Zhongping Zhang",
      "Jian Zheng",
      "Jacob Zhiyuan Fang",
      "Bryan A. Plummer"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Text-to-image+Editing+by+Image+Information+Removal+Zhongping+Zhang+Jian+Zheng+Jacob+Zhiyuan+Fang+Bryan+A.+Plummer",
    "gs_search_success": true,
    "gs_authors": [
      "fHWXpq4AAAAJ",
      "0yfhzZgAAAAJ",
      "329B_BEAAAAJ",
      "6C20vTwAAAAJ"
    ],
    "citation_count": 21,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2305.14039",
    "title": "Learning a Single Convolutional Layer Model for Low Light Image Enhancement",
    "year": 2023,
    "published": "2023-05-23T13:12:00Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Low-light image enhancement (LLIE) aims to improve the illuminance of images due to insufficient light exposure. Recently, various lightweight learning-based LLIE methods have been proposed to handle the challenges of unfavorable prevailing low contrast, low brightness, etc. In this paper, we have streamlined the architecture of the network to the utmost degree. By utilizing the effective structural re-parameterization technique, a single convolutional layer model (SCLM) is proposed that provide",
    "arxiv_url": "https://arxiv.org/abs/2305.14039v1",
    "pdf_url": "https://arxiv.org/pdf/2305.14039v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2305.14039",
    "arxiv_authors": [
      "Yuantong Zhang",
      "Baoxin Teng",
      "Daiqin Yang",
      "Zhenzhong Chen",
      "Haichuan Ma",
      "Gang Li",
      "Wenpeng Ding"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Learning+a+Single+Convolutional+Layer+Model+for+Low+Light+Image+Enhancement+Yuantong+Zhang+Baoxin+Teng+Daiqin+Yang+Zhenzhong+Chen+Haichuan+Ma",
    "gs_search_success": true,
    "gs_authors": [
      "w_BcpK8AAAAJ"
    ],
    "citation_count": 19,
    "gs_author_count": 7
  },
  {
    "arxiv_id": "2503.22936",
    "title": "Enhancing Learnable Descriptive Convolutional Vision Transformer for Face Anti-Spoofing",
    "year": 2025,
    "published": "2025-03-29T01:55:48Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Face anti-spoofing (FAS) heavily relies on identifying live/spoof discriminative features to counter face presentation attacks. Recently, we proposed LDCformer to successfully incorporate the Learnable Descriptive Convolution (LDC) into ViT, to model long-range dependency of locally descriptive features for FAS. In this paper, we propose three novel training strategies to effectively enhance the training of LDCformer to largely boost its feature characterization capability. The first strategy, d",
    "arxiv_url": "https://arxiv.org/abs/2503.22936v1",
    "pdf_url": "https://arxiv.org/pdf/2503.22936v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2503.22936",
    "arxiv_authors": [
      "Pei-Kai Huanga",
      "Jun-Xiong Chong",
      "Ming-Tsung Hsu",
      "Fang-Yu Hsu",
      "Chiou-Ting Hsu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Enhancing+Learnable+Descriptive+Convolutional+Vision+Transformer+for+Face+Anti-Spoofing+Pei-Kai+Huanga+Jun-Xiong+Chong+Ming-Tsung+Hsu+Fang-Yu+Hsu+Chiou-Ting+Hsu",
    "gs_search_success": true,
    "gs_authors": [
      "T0hELCEAAAAJ",
      "5vBXXWsAAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2311.12028",
    "title": "Hourglass Tokenizer for Efficient Transformer-Based 3D Human Pose Estimation",
    "year": 2023,
    "published": "2023-11-20T18:59:51Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "abstract": "Transformers have been successfully applied in the field of video-based 3D human pose estimation. However, the high computational costs of these video pose transformers (VPTs) make them impractical on resource-constrained devices. In this paper, we present a plug-and-play pruning-and-recovering framework, called Hourglass Tokenizer (HoT), for efficient transformer-based 3D human pose estimation from videos. Our HoT begins with pruning pose tokens of redundant frames and ends with recovering full",
    "arxiv_url": "https://arxiv.org/abs/2311.12028v2",
    "pdf_url": "https://arxiv.org/pdf/2311.12028v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2311.12028",
    "arxiv_authors": [
      "Wenhao Li",
      "Mengyuan Liu",
      "Hong Liu",
      "Pichao Wang",
      "Jialun Cai",
      "Nicu Sebe"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Hourglass+Tokenizer+for+Efficient+Transformer-Based+3D+Human+Pose+Estimation+Wenhao+Li+Mengyuan+Liu+Hong+Liu+Pichao+Wang+Jialun+Cai",
    "gs_search_success": true,
    "gs_authors": [
      "QozdnnoAAAAJ",
      "bINgnyEAAAAJ",
      "stFCYOAAAAAJ",
      "woX_4AcAAAAJ",
      "U5Y7BjkAAAAJ",
      "WLMUAjsAAAAJ"
    ],
    "citation_count": 58,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2405.12927",
    "title": "On Image Registration and Subpixel Estimation",
    "year": 2024,
    "published": "2024-05-21T16:53:03Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Image registration is a classical problem in machine vision which seeks methods to align discrete images of the same scene to subpixel accuracy in general situations. As with all estimation problems, the underlying difficulty is the partial information available about the ground truth. We consider a basic and idealized one-dimensional image registration problem motivated by questions about measurement and about quantization, and we demonstrate that the extent to which subinterval/subpixel infere",
    "arxiv_url": "https://arxiv.org/abs/2405.12927v1",
    "pdf_url": "https://arxiv.org/pdf/2405.12927v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2405.12927",
    "arxiv_authors": [
      "Serap A. Savari"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=On+Image+Registration+and+Subpixel+Estimation+Serap+A.+Savari",
    "gs_search_success": true,
    "gs_authors": [
      "2guN_7YAAAAJ"
    ],
    "citation_count": 4,
    "gs_author_count": 1
  },
  {
    "arxiv_id": "2405.16260",
    "title": "Enhancing Consistency-Based Image Generation via Adversarialy-Trained Classification and Energy-Based Discrimination",
    "year": 2024,
    "published": "2024-05-25T14:53:52Z",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "The recently introduced Consistency models pose an efficient alternative to diffusion algorithms, enabling rapid and good quality image synthesis. These methods overcome the slowness of diffusion models by directly mapping noise to data, while maintaining a (relatively) simpler training. Consistency models enable a fast one- or few-step generation, but they typically fall somewhat short in sample quality when compared to their diffusion origins. In this work we propose a novel and highly effecti",
    "arxiv_url": "https://arxiv.org/abs/2405.16260v2",
    "pdf_url": "https://arxiv.org/pdf/2405.16260v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2405.16260",
    "arxiv_authors": [
      "Shelly Golan",
      "Roy Ganz",
      "Michael Elad"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Enhancing+Consistency-Based+Image+Generation+via+Adversarialy-Trained+Classification+and+Energy-Based+Discrimination+Shelly+Golan+Roy+Ganz+Michael+Elad",
    "gs_search_success": true,
    "gs_authors": [
      "UpZbV44AAAAJ",
      "2E0FHMoAAAAJ",
      "_MHRaNIAAAAJ"
    ],
    "citation_count": 4,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2303.00369",
    "title": "Indescribable Multi-modal Spatial Evaluator",
    "year": 2023,
    "published": "2023-03-01T09:50:39Z",
    "categories": [
      "cs.CV",
      "eess.IV"
    ],
    "abstract": "Multi-modal image registration spatially aligns two images with different distributions. One of its major challenges is that images acquired from different imaging machines have different imaging distributions, making it difficult to focus only on the spatial aspect of the images and ignore differences in distributions. In this study, we developed a self-supervised approach, Indescribable Multi-model Spatial Evaluator (IMSE), to address multi-modal image registration. IMSE creates an accurate mu",
    "arxiv_url": "https://arxiv.org/abs/2303.00369v2",
    "pdf_url": "https://arxiv.org/pdf/2303.00369v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2303.00369",
    "arxiv_authors": [
      "Lingke Kong",
      "X. Sharon Qi",
      "Qijin Shen",
      "Jiacheng Wang",
      "Jingyi Zhang",
      "Yanle Hu",
      "Qichao Zhou"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Indescribable+Multi-modal+Spatial+Evaluator+Lingke+Kong+X.+Sharon+Qi+Qijin+Shen+Jiacheng+Wang+Jingyi+Zhang",
    "gs_search_success": false,
    "gs_authors": [],
    "citation_count": null,
    "gs_author_count": null,
    "error_type": "fetch_failed"
  },
  {
    "arxiv_id": "2310.20607",
    "title": "What a Whole Slide Image Can Tell? Subtype-guided Masked Transformer for Pathological Image Captioning",
    "year": 2023,
    "published": "2023-10-31T16:43:03Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Pathological captioning of Whole Slide Images (WSIs), though is essential in computer-aided pathological diagnosis, has rarely been studied due to the limitations in datasets and model training efficacy. In this paper, we propose a new paradigm Subtype-guided Masked Transformer (SGMT) for pathological captioning based on Transformers, which treats a WSI as a sequence of sparse patches and generates an overall caption sentence from the sequence. An accompanying subtype prediction is introduced in",
    "arxiv_url": "https://arxiv.org/abs/2310.20607v1",
    "pdf_url": "https://arxiv.org/pdf/2310.20607v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2310.20607",
    "arxiv_authors": [
      "Wenkang Qin",
      "Rui Xu",
      "Peixiang Huang",
      "Xiaomin Wu",
      "Heyu Zhang",
      "Lin Luo"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=What+a+Whole+Slide+Image+Can+Tell%3F+Subtype-guided+Masked+Transformer+for+Pathological+Image+Captioning+Wenkang+Qin+Rui+Xu+Peixiang+Huang+Xiaomin+Wu+Heyu+Zhang",
    "gs_search_success": true,
    "gs_authors": [
      "0mtitwcAAAAJ",
      "TE9stNgAAAAJ",
      "SvVfZtUAAAAJ"
    ],
    "citation_count": 10,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2312.10115",
    "title": "SkySense: A Multi-Modal Remote Sensing Foundation Model Towards Universal Interpretation for Earth Observation Imagery",
    "year": 2023,
    "published": "2023-12-15T09:57:21Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Prior studies on Remote Sensing Foundation Model (RSFM) reveal immense potential towards a generic model for Earth Observation. Nevertheless, these works primarily focus on a single modality without temporal and geo-context modeling, hampering their capabilities for diverse tasks. In this study, we present SkySense, a generic billion-scale model, pre-trained on a curated multi-modal Remote Sensing Imagery (RSI) dataset with 21.5 million temporal sequences. SkySense incorporates a factorized mult",
    "arxiv_url": "https://arxiv.org/abs/2312.10115v2",
    "pdf_url": "https://arxiv.org/pdf/2312.10115v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2312.10115",
    "arxiv_authors": [
      "Xin Guo",
      "Jiangwei Lao",
      "Bo Dang",
      "Yingying Zhang",
      "Lei Yu",
      "Lixiang Ru",
      "Liheng Zhong",
      "Ziyuan Huang",
      "Kang Wu",
      "Dingxiang Hu",
      "Huimei He",
      "Jian Wang",
      "Jingdong Chen",
      "Ming Yang",
      "Yongjun Zhang",
      "Yansheng Li"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=SkySense%3A+A+Multi-Modal+Remote+Sensing+Foundation+Model+Towards+Universal+Interpretation+for+Earth+Observation+Imagery+Xin+Guo+Jiangwei+Lao+Bo+Dang+Yingying+Zhang+Lei+Yu",
    "gs_search_success": true,
    "gs_authors": [
      "Stk6JbgAAAAJ",
      "nOjOl4EAAAAJ",
      "A9D-disAAAAJ",
      "d2eWV9wAAAAJ",
      "alirF-oAAAAJ",
      "1aLVp4UAAAAJ",
      "AXlR4OwAAAAJ",
      "gz_hWPoAAAAJ",
      "y7wegKQAAAAJ"
    ],
    "citation_count": 254,
    "gs_author_count": 12
  },
  {
    "arxiv_id": "2306.06088",
    "title": "SENS: Part-Aware Sketch-based Implicit Neural Shape Modeling",
    "year": 2023,
    "published": "2023-06-09T17:50:53Z",
    "categories": [
      "cs.GR",
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "We present SENS, a novel method for generating and editing 3D models from hand-drawn sketches, including those of abstract nature. Our method allows users to quickly and easily sketch a shape, and then maps the sketch into the latent space of a part-aware neural implicit shape architecture. SENS analyzes the sketch and encodes its parts into ViT patch encoding, subsequently feeding them into a transformer decoder that converts them to shape embeddings suitable for editing 3D neural implicit shap",
    "arxiv_url": "https://arxiv.org/abs/2306.06088v2",
    "pdf_url": "https://arxiv.org/pdf/2306.06088v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2306.06088",
    "arxiv_authors": [
      "Alexandre Binninger",
      "Amir Hertz",
      "Olga Sorkine-Hornung",
      "Daniel Cohen-Or",
      "Raja Giryes"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=SENS%3A+Part-Aware+Sketch-based+Implicit+Neural+Shape+Modeling+Alexandre+Binninger+Amir+Hertz+Olga+Sorkine-Hornung+Daniel+Cohen-Or+Raja+Giryes",
    "gs_search_success": true,
    "gs_authors": [
      "mnk__kYAAAAJ",
      "fAxws1sAAAAJ",
      "wh0HJFoAAAAJ",
      "GBU568oAAAAJ",
      "9aQUYVQAAAAJ"
    ],
    "citation_count": 16,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2403.11978",
    "title": "Pedestrian Tracking with Monocular Camera using Unconstrained 3D Motion Model",
    "year": 2024,
    "published": "2024-03-18T17:13:18Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "A first-principle single-object model is proposed for pedestrian tracking. It is assumed that the extent of the moving object can be described via known statistics in 3D, such as pedestrian height. The proposed model thus need not constrain the object motion in 3D to a common ground plane, which is usual in 3D visual tracking applications. A nonlinear filter for this model is implemented using the unscented Kalman filter (UKF) and tested using the publicly available MOT-17 dataset. The proposed ",
    "arxiv_url": "https://arxiv.org/abs/2403.11978v2",
    "pdf_url": "https://arxiv.org/pdf/2403.11978v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2403.11978",
    "arxiv_authors": [
      "Jan Krejčí",
      "Oliver Kost",
      "Ondřej Straka",
      "Jindřich Duník"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Pedestrian+Tracking+with+Monocular+Camera+using+Unconstrained+3D+Motion+Model+Jan+Krej%C4%8D%C3%AD+Oliver+Kost+Ond%C5%99ej+Straka+Jind%C5%99ich+Dun%C3%ADk",
    "gs_search_success": true,
    "gs_authors": [
      "ssPjLSkAAAAJ",
      "rGUSyEcAAAAJ",
      "BtA_5acAAAAJ"
    ],
    "citation_count": 9,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2505.03702",
    "title": "Self-Supervised Learning for Robotic Leaf Manipulation: A Hybrid Geometric-Neural Approach",
    "year": 2025,
    "published": "2025-05-06T17:22:21Z",
    "categories": [
      "cs.RO",
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "Automating leaf manipulation in agricultural settings faces significant challenges, including the variability of plant morphologies and deformable leaves. We propose a novel hybrid geometric-neural approach for autonomous leaf grasping that combines traditional computer vision with neural networks through self-supervised learning. Our method integrates YOLOv8 for instance segmentation and RAFT-Stereo for 3D depth estimation to build rich leaf representations, which feed into both a geometric fea",
    "arxiv_url": "https://arxiv.org/abs/2505.03702v3",
    "pdf_url": "https://arxiv.org/pdf/2505.03702v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2505.03702",
    "arxiv_authors": [
      "Srecharan Selvam"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Self-Supervised+Learning+for+Robotic+Leaf+Manipulation%3A+A+Hybrid+Geometric-Neural+Approach+Srecharan+Selvam",
    "gs_search_success": true,
    "gs_authors": [],
    "citation_count": null,
    "gs_author_count": 1
  },
  {
    "arxiv_id": "2305.00718",
    "title": "Event Camera as Region Proposal Network",
    "year": 2023,
    "published": "2023-05-01T08:38:30Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "The human eye consists of two types of photoreceptors, rods and cones. Rods are responsible for monochrome vision, and cones for color vision. The number of rods is much higher than the cones, which means that most human vision processing is done in monochrome. An event camera reports the change in pixel intensity and is analogous to rods. Event and color cameras in computer vision are like rods and cones in human vision. Humans can notice objects moving in the peripheral vision (far right and l",
    "arxiv_url": "https://arxiv.org/abs/2305.00718v1",
    "pdf_url": "https://arxiv.org/pdf/2305.00718v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2305.00718",
    "arxiv_authors": [
      "Shrutarv Awasthi",
      "Anas Gouda",
      "Richard Julian Lodenkaemper",
      "Moritz Roidl"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Event+Camera+as+Region+Proposal+Network+Shrutarv+Awasthi+Anas+Gouda+Richard+Julian+Lodenkaemper+Moritz+Roidl",
    "gs_search_success": true,
    "gs_authors": [
      "7lUCJ7EAAAAJ",
      "nvxlFkUAAAAJ",
      "6eZOuRwAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2310.12790",
    "title": "Anomaly Heterogeneity Learning for Open-set Supervised Anomaly Detection",
    "year": 2023,
    "published": "2023-10-19T14:47:11Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Open-set supervised anomaly detection (OSAD) - a recently emerging anomaly detection area - aims at utilizing a few samples of anomaly classes seen during training to detect unseen anomalies (i.e., samples from open-set anomaly classes), while effectively identifying the seen anomalies. Benefiting from the prior knowledge illustrated by the seen anomalies, current OSAD methods can often largely reduce false positive errors. However, these methods are trained in a closed-set setting and treat the",
    "arxiv_url": "https://arxiv.org/abs/2310.12790v3",
    "pdf_url": "https://arxiv.org/pdf/2310.12790v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2310.12790",
    "arxiv_authors": [
      "Jiawen Zhu",
      "Choubo Ding",
      "Yu Tian",
      "Guansong Pang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Anomaly+Heterogeneity+Learning+for+Open-set+Supervised+Anomaly+Detection+Jiawen+Zhu+Choubo+Ding+Yu+Tian+Guansong+Pang",
    "gs_search_success": true,
    "gs_authors": [
      "1ZO7pHkAAAAJ",
      "knptLuEAAAAJ",
      "wroLWH0AAAAJ",
      "cU0UfhwAAAAJ"
    ],
    "citation_count": 44,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2403.13163",
    "title": "DeblurDiNAT: A Compact Model with Exceptional Generalization and Visual Fidelity on Unseen Domains",
    "year": 2024,
    "published": "2024-03-19T21:31:31Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Recent deblurring networks have effectively restored clear images from the blurred ones. However, they often struggle with generalization to unknown domains. Moreover, these models typically focus on distortion metrics such as PSNR and SSIM, neglecting the critical aspect of metrics aligned with human perception. To address these limitations, we propose DeblurDiNAT, a deblurring Transformer based on Dilated Neighborhood Attention. First, DeblurDiNAT employs an alternating dilation factor paradig",
    "arxiv_url": "https://arxiv.org/abs/2403.13163v5",
    "pdf_url": "https://arxiv.org/pdf/2403.13163v5",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2403.13163",
    "arxiv_authors": [
      "Hanzhou Liu",
      "Binghan Li",
      "Chengkai Liu",
      "Mi Lu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=DeblurDiNAT%3A+A+Compact+Model+with+Exceptional+Generalization+and+Visual+Fidelity+on+Unseen+Domains+Hanzhou+Liu+Binghan+Li+Chengkai+Liu+Mi+Lu",
    "gs_search_success": true,
    "gs_authors": [
      "crjEvpQAAAAJ",
      "Keab81kAAAAJ",
      "DYha9k0AAAAJ"
    ],
    "citation_count": 2,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2410.17247",
    "title": "PyramidDrop: Accelerating Your Large Vision-Language Models via Pyramid Visual Redundancy Reduction",
    "year": 2024,
    "published": "2024-10-22T17:59:53Z",
    "categories": [
      "cs.CV",
      "cs.CL"
    ],
    "abstract": "In large vision-language models (LVLMs), images serve as inputs that carry a wealth of information. As the idiom \"A picture is worth a thousand words\" implies, representing a single image in current LVLMs can require hundreds or even thousands of tokens. This results in significant computational costs, which grow quadratically as input image resolution increases, thereby severely impacting the efficiency of both training and inference. Previous approaches have attempted to reduce the number of i",
    "arxiv_url": "https://arxiv.org/abs/2410.17247v2",
    "pdf_url": "https://arxiv.org/pdf/2410.17247v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2410.17247",
    "arxiv_authors": [
      "Long Xing",
      "Qidong Huang",
      "Xiaoyi Dong",
      "Jiajie Lu",
      "Pan Zhang",
      "Yuhang Zang",
      "Yuhang Cao",
      "Conghui He",
      "Jiaqi Wang",
      "Feng Wu",
      "Dahua Lin"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=PyramidDrop%3A+Accelerating+Your+Large+Vision-Language+Models+via+Pyramid+Visual+Redundancy+Reduction+Long+Xing+Qidong+Huang+Xiaoyi+Dong+Jiajie+Lu+Pan+Zhang",
    "gs_search_success": false,
    "gs_authors": [],
    "citation_count": null,
    "gs_author_count": null,
    "error_type": "fetch_failed"
  },
  {
    "arxiv_id": "2411.08340",
    "title": "DyConfidMatch: Dynamic Thresholding and Re-sampling for 3D Semi-supervised Learning",
    "year": 2024,
    "published": "2024-11-13T05:09:28Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Semi-supervised learning (SSL) leverages limited labeled and abundant unlabeled data but often faces challenges with data imbalance, especially in 3D contexts. This study investigates class-level confidence as an indicator of learning status in 3D SSL, proposing a novel method that utilizes dynamic thresholding to better use unlabeled data, particularly from underrepresented classes. A re-sampling strategy is also introduced to mitigate bias towards well-represented classes, ensuring equitable c",
    "arxiv_url": "https://arxiv.org/abs/2411.08340v1",
    "pdf_url": "https://arxiv.org/pdf/2411.08340v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2411.08340",
    "arxiv_authors": [
      "Zhimin Chen",
      "Bing Li"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=DyConfidMatch%3A+Dynamic+Thresholding+and+Re-sampling+for+3D+Semi-supervised+Learning+Zhimin+Chen+Bing+Li",
    "gs_search_success": true,
    "gs_authors": [
      "yysOczkAAAAJ",
      "OIYNwLkAAAAJ"
    ],
    "citation_count": 3,
    "gs_author_count": 2
  },
  {
    "arxiv_id": "2410.11860",
    "title": "Comparing Zealous and Restrained AI Recommendations in a Real-World Human-AI Collaboration Task",
    "year": 2024,
    "published": "2024-10-06T23:19:19Z",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CV"
    ],
    "abstract": "When designing an AI-assisted decision-making system, there is often a tradeoff between precision and recall in the AI's recommendations. We argue that careful exploitation of this tradeoff can harness the complementary strengths in the human-AI collaboration to significantly improve team performance. We investigate a real-world video anonymization task for which recall is paramount and more costly to improve. We analyze the performance of 78 professional annotators working with a) no AI assista",
    "arxiv_url": "https://arxiv.org/abs/2410.11860v1",
    "pdf_url": "https://arxiv.org/pdf/2410.11860v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2410.11860",
    "arxiv_authors": [
      "Chengyuan Xu",
      "Kuo-Chin Lien",
      "Tobias Höllerer"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Comparing+Zealous+and+Restrained+AI+Recommendations+in+a+Real-World+Human-AI+Collaboration+Task+Chengyuan+Xu+Kuo-Chin+Lien+Tobias+H%C3%B6llerer",
    "gs_search_success": true,
    "gs_authors": [
      "hZ7hS1EAAAAJ",
      "008lr2cAAAAJ"
    ],
    "citation_count": 17,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2408.06190",
    "title": "FruitNeRF: A Unified Neural Radiance Field based Fruit Counting Framework",
    "year": 2024,
    "published": "2024-08-12T14:40:38Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "We introduce FruitNeRF, a unified novel fruit counting framework that leverages state-of-the-art view synthesis methods to count any fruit type directly in 3D. Our framework takes an unordered set of posed images captured by a monocular camera and segments fruit in each image. To make our system independent of the fruit type, we employ a foundation model that generates binary segmentation masks for any fruit. Utilizing both modalities, RGB and semantic, we train a semantic neural radiance field.",
    "arxiv_url": "https://arxiv.org/abs/2408.06190v2",
    "pdf_url": "https://arxiv.org/pdf/2408.06190v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2408.06190",
    "arxiv_authors": [
      "Lukas Meyer",
      "Andreas Gilson",
      "Ute Schmid",
      "Marc Stamminger"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=FruitNeRF%3A+A+Unified+Neural+Radiance+Field+based+Fruit+Counting+Framework+Lukas+Meyer+Andreas+Gilson+Ute+Schmid+Marc+Stamminger",
    "gs_search_success": true,
    "gs_authors": [
      "fJXIVKsAAAAJ",
      "sj3_63cAAAAJ",
      "cx4AaqoAAAAJ",
      "h84gW6QAAAAJ"
    ],
    "citation_count": 27,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2401.04720",
    "title": "Low-resource finetuning of foundation models beats state-of-the-art in histopathology",
    "year": 2024,
    "published": "2024-01-09T18:46:59Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "To handle the large scale of whole slide images in computational pathology, most approaches first tessellate the images into smaller patches, extract features from these patches, and finally aggregate the feature vectors with weakly-supervised learning. The performance of this workflow strongly depends on the quality of the extracted features. Recently, foundation models in computer vision showed that leveraging huge amounts of data through supervised or self-supervised learning improves feature",
    "arxiv_url": "https://arxiv.org/abs/2401.04720v1",
    "pdf_url": "https://arxiv.org/pdf/2401.04720v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2401.04720",
    "arxiv_authors": [
      "Benedikt Roth",
      "Valentin Koch",
      "Sophia J. Wagner",
      "Julia A. Schnabel",
      "Carsten Marr",
      "Tingying Peng"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Low-resource+finetuning+of+foundation+models+beats+state-of-the-art+in+histopathology+Benedikt+Roth+Valentin+Koch+Sophia+J.+Wagner+Julia+A.+Schnabel+Carsten+Marr",
    "gs_search_success": true,
    "gs_authors": [
      "Wg9zjqEAAAAJ",
      "gdWU8bgAAAAJ",
      "skcoUZMAAAAJ",
      "jUiKc6QAAAAJ",
      "FPykfZ0AAAAJ"
    ],
    "citation_count": 23,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2309.08372",
    "title": "Beyond Domain Gap: Exploiting Subjectivity in Sketch-Based Person Retrieval",
    "year": 2023,
    "published": "2023-09-15T12:59:01Z",
    "categories": [
      "cs.CV",
      "cs.MM"
    ],
    "abstract": "Person re-identification (re-ID) requires densely distributed cameras. In practice, the person of interest may not be captured by cameras and, therefore, needs to be retrieved using subjective information (e.g., sketches from witnesses). Previous research defines this case using the sketch as sketch re-identification (Sketch re-ID) and focuses on eliminating the domain gap. Actually, subjectivity is another significant challenge. We model and investigate it by posing a new dataset with multi-wit",
    "arxiv_url": "https://arxiv.org/abs/2309.08372v1",
    "pdf_url": "https://arxiv.org/pdf/2309.08372v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2309.08372",
    "arxiv_authors": [
      "Kejun Lin",
      "Zhixiang Wang",
      "Zheng Wang",
      "Yinqiang Zheng",
      "Shin'ichi Satoh"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Beyond+Domain+Gap%3A+Exploiting+Subjectivity+in+Sketch-Based+Person+Retrieval+Kejun+Lin+Zhixiang+Wang+Zheng+Wang+Yinqiang+Zheng+Shin%27ichi+Satoh",
    "gs_search_success": true,
    "gs_authors": [
      "-WHTbpUAAAAJ",
      "JD-5DKcAAAAJ",
      "yybzbxMAAAAJ",
      "7aEF5cQAAAAJ",
      "6AP8CisAAAAJ"
    ],
    "citation_count": 28,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2410.06905",
    "title": "Reliable Probabilistic Human Trajectory Prediction for Autonomous Applications",
    "year": 2024,
    "published": "2024-10-09T14:08:39Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Autonomous systems, like vehicles or robots, require reliable, accurate, fast, resource-efficient, scalable, and low-latency trajectory predictions to get initial knowledge about future locations and movements of surrounding objects for safe human-machine interaction. Furthermore, they need to know the uncertainty of the predictions for risk assessment to provide safe path planning. This paper presents a lightweight method to address these requirements, combining Long Short-Term Memory and Mixtu",
    "arxiv_url": "https://arxiv.org/abs/2410.06905v2",
    "pdf_url": "https://arxiv.org/pdf/2410.06905v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2410.06905",
    "arxiv_authors": [
      "Manuel Hetzel",
      "Hannes Reichert",
      "Konrad Doll",
      "Bernhard Sick"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Reliable+Probabilistic+Human+Trajectory+Prediction+for+Autonomous+Applications+Manuel+Hetzel+Hannes+Reichert+Konrad+Doll+Bernhard+Sick",
    "gs_search_success": true,
    "gs_authors": [
      "sGAKnroAAAAJ",
      "VCGlB18AAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2404.13565",
    "title": "Exploring Diverse Methods in Visual Question Answering",
    "year": 2024,
    "published": "2024-04-21T07:34:44Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "abstract": "This study explores innovative methods for improving Visual Question Answering (VQA) using Generative Adversarial Networks (GANs), autoencoders, and attention mechanisms. Leveraging a balanced VQA dataset, we investigate three distinct strategies. Firstly, GAN-based approaches aim to generate answer embeddings conditioned on image and question inputs, showing potential but struggling with more complex tasks. Secondly, autoencoder-based techniques focus on learning optimal embeddings for question",
    "arxiv_url": "https://arxiv.org/abs/2404.13565v3",
    "pdf_url": "https://arxiv.org/pdf/2404.13565v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2404.13565",
    "arxiv_authors": [
      "Panfeng Li",
      "Qikai Yang",
      "Xieming Geng",
      "Wenjing Zhou",
      "Zhicheng Ding",
      "Yi Nian"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Exploring+Diverse+Methods+in+Visual+Question+Answering+Panfeng+Li+Qikai+Yang+Xieming+Geng+Wenjing+Zhou+Zhicheng+Ding",
    "gs_search_success": true,
    "gs_authors": [
      "deqEXLgAAAAJ"
    ],
    "citation_count": 85,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2307.08308",
    "title": "A Novel Multi-Task Model Imitating Dermatologists for Accurate Differential Diagnosis of Skin Diseases in Clinical Images",
    "year": 2023,
    "published": "2023-07-17T08:05:30Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Skin diseases are among the most prevalent health issues, and accurate computer-aided diagnosis methods are of importance for both dermatologists and patients. However, most of the existing methods overlook the essential domain knowledge required for skin disease diagnosis. A novel multi-task model, namely DermImitFormer, is proposed to fill this gap by imitating dermatologists' diagnostic procedures and strategies. Through multi-task learning, the model simultaneously predicts body parts and le",
    "arxiv_url": "https://arxiv.org/abs/2307.08308v1",
    "pdf_url": "https://arxiv.org/pdf/2307.08308v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2307.08308",
    "arxiv_authors": [
      "Yan-Jie Zhou",
      "Wei Liu",
      "Yuan Gao",
      "Jing Xu",
      "Le Lu",
      "Yuping Duan",
      "Hao Cheng",
      "Na Jin",
      "Xiaoyong Man",
      "Shuang Zhao",
      "Yu Wang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=A+Novel+Multi-Task+Model+Imitating+Dermatologists+for+Accurate+Differential+Diagnosis+of+Skin+Diseases+in+Clinical+Images+Yan-Jie+Zhou+Wei+Liu+Yuan+Gao+Jing+Xu+Le+Lu",
    "gs_search_success": true,
    "gs_authors": [
      "pwETggMAAAAJ",
      "CAal4ZgAAAAJ",
      "kZn0f6gAAAAJ",
      "4R04g08AAAAJ",
      "lsm1L60AAAAJ",
      "1Tvfo7wAAAAJ"
    ],
    "citation_count": 10,
    "gs_author_count": 11
  },
  {
    "arxiv_id": "2406.17998",
    "title": "Changen2: Multi-Temporal Remote Sensing Generative Change Foundation Model",
    "year": 2024,
    "published": "2024-06-26T01:03:39Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Our understanding of the temporal dynamics of the Earth's surface has been advanced by deep vision models, which often require lots of labeled multi-temporal images for training. However, collecting, preprocessing, and annotating multi-temporal remote sensing images at scale is non-trivial since it is expensive and knowledge-intensive. In this paper, we present change data generators based on generative models, which are cheap and automatic, alleviating these data problems. Our main idea is to s",
    "arxiv_url": "https://arxiv.org/abs/2406.17998v1",
    "pdf_url": "https://arxiv.org/pdf/2406.17998v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2406.17998",
    "arxiv_authors": [
      "Zhuo Zheng",
      "Stefano Ermon",
      "Dongjun Kim",
      "Liangpei Zhang",
      "Yanfei Zhong"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Changen2%3A+Multi-Temporal+Remote+Sensing+Generative+Change+Foundation+Model+Zhuo+Zheng+Stefano+Ermon+Dongjun+Kim+Liangpei+Zhang+Yanfei+Zhong",
    "gs_search_success": true,
    "gs_authors": [
      "Fm7XZ5AAAAAJ",
      "zOeuKDgAAAAJ",
      "CREpn_AAAAAJ",
      "yFEl8hcAAAAJ",
      "ogXTOZ4AAAAJ"
    ],
    "citation_count": 54,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2504.21154",
    "title": "Emotion Recognition in Contemporary Dance Performances Using Laban Movement Analysis",
    "year": 2025,
    "published": "2025-04-29T20:17:27Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "This paper presents a novel framework for emotion recognition in contemporary dance by improving existing Laban Movement Analysis (LMA) feature descriptors and introducing robust, novel descriptors that capture both quantitative and qualitative aspects of the movement. Our approach extracts expressive characteristics from 3D keypoints data of professional dancers performing contemporary dance under various emotional states, and trains multiple classifiers, including Random Forests and Support Ve",
    "arxiv_url": "https://arxiv.org/abs/2504.21154v1",
    "pdf_url": "https://arxiv.org/pdf/2504.21154v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2504.21154",
    "arxiv_authors": [
      "Muhammad Turab",
      "Philippe Colantoni",
      "Damien Muselet",
      "Alain Tremeau"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Emotion+Recognition+in+Contemporary+Dance+Performances+Using+Laban+Movement+Analysis+Muhammad+Turab+Philippe+Colantoni+Damien+Muselet+Alain+Tremeau",
    "gs_search_success": true,
    "gs_authors": [
      "TyNSxjQAAAAJ",
      "LD8VtagAAAAJ",
      "XUE41swAAAAJ",
      "DN9PSmgAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2403.11582",
    "title": "OurDB: Ouroboric Domain Bridging for Multi-Target Domain Adaptive Semantic Segmentation",
    "year": 2024,
    "published": "2024-03-18T08:55:48Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Multi-target domain adaptation (MTDA) for semantic segmentation poses a significant challenge, as it involves multiple target domains with varying distributions. The goal of MTDA is to minimize the domain discrepancies among a single source and multi-target domains, aiming to train a single model that excels across all target domains. Previous MTDA approaches typically employ multiple teacher architectures, where each teacher specializes in one target domain to simplify the task. However, these ",
    "arxiv_url": "https://arxiv.org/abs/2403.11582v1",
    "pdf_url": "https://arxiv.org/pdf/2403.11582v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2403.11582",
    "arxiv_authors": [
      "Seungbeom Woo",
      "Geonwoo Baek",
      "Taehoon Kim",
      "Jaemin Na",
      "Joong-won Hwang",
      "Wonjun Hwang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=OurDB%3A+Ouroboric+Domain+Bridging+for+Multi-Target+Domain+Adaptive+Semantic+Segmentation+Seungbeom+Woo+Geonwoo+Baek+Taehoon+Kim+Jaemin+Na+Joong-won+Hwang",
    "gs_search_success": true,
    "gs_authors": [
      "-I8AfBAAAAAJ",
      "C1Jl61oAAAAJ",
      "ZWvngT8AAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2504.02151",
    "title": "Multivariate Temporal Regression at Scale: A Three-Pillar Framework Combining ML, XAI, and NLP",
    "year": 2025,
    "published": "2025-04-02T21:53:03Z",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "abstract": "This paper introduces a novel framework that accelerates the discovery of actionable relationships in high-dimensional temporal data by integrating machine learning (ML), explainable AI (XAI), and natural language processing (NLP) to enhance data quality and streamline workflows. Traditional methods often fail to recognize complex temporal relationships, leading to noisy, redundant, or biased datasets. Our approach combines ML-driven pruning to identify and mitigate low-quality samples, XAI-base",
    "arxiv_url": "https://arxiv.org/abs/2504.02151v2",
    "pdf_url": "https://arxiv.org/pdf/2504.02151v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2504.02151",
    "arxiv_authors": [
      "Jiztom Kavalakkatt Francis",
      "Matthew J Darr"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Multivariate+Temporal+Regression+at+Scale%3A+A+Three-Pillar+Framework+Combining+ML%2C+XAI%2C+and+NLP+Jiztom+Kavalakkatt+Francis+Matthew+J+Darr",
    "gs_search_success": true,
    "gs_authors": [
      "ZVmSUpYAAAAJ",
      "R5lpIu0AAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": null
  },
  {
    "arxiv_id": "2405.03613",
    "title": "Dual Relation Mining Network for Zero-Shot Learning",
    "year": 2024,
    "published": "2024-05-06T16:31:19Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Zero-shot learning (ZSL) aims to recognize novel classes through transferring shared semantic knowledge (e.g., attributes) from seen classes to unseen classes. Recently, attention-based methods have exhibited significant progress which align visual features and attributes via a spatial attention mechanism. However, these methods only explore visual-semantic relationship in the spatial dimension, which can lead to classification ambiguity when different attributes share similar attention regions,",
    "arxiv_url": "https://arxiv.org/abs/2405.03613v1",
    "pdf_url": "https://arxiv.org/pdf/2405.03613v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2405.03613",
    "arxiv_authors": [
      "Jinwei Han",
      "Yingguo Gao",
      "Zhiwen Lin",
      "Ke Yan",
      "Shouhong Ding",
      "Yuan Gao",
      "Gui-Song Xia"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Dual+Relation+Mining+Network+for+Zero-Shot+Learning+Jinwei+Han+Yingguo+Gao+Zhiwen+Lin+Ke+Yan+Shouhong+Ding",
    "gs_search_success": true,
    "gs_authors": [
      "sHrEsAwAAAAJ",
      "AAPYLWwAAAAJ",
      "vWstgn0AAAAJ",
      "yMpRezUAAAAJ",
      "SAUCVsEAAAAJ",
      "OGf40fkAAAAJ"
    ],
    "citation_count": 3,
    "gs_author_count": 7
  },
  {
    "arxiv_id": "2311.14749",
    "title": "Compositional Zero-shot Learning via Progressive Language-based Observations",
    "year": 2023,
    "published": "2023-11-23T10:14:23Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Compositional zero-shot learning aims to recognize unseen state-object compositions by leveraging known primitives (state and object) during training. However, effectively modeling interactions between primitives and generalizing knowledge to novel compositions remains a perennial challenge. There are two key factors: object-conditioned and state-conditioned variance, i.e., the appearance of states (or objects) can vary significantly when combined with different objects (or states). For instance",
    "arxiv_url": "https://arxiv.org/abs/2311.14749v2",
    "pdf_url": "https://arxiv.org/pdf/2311.14749v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2311.14749",
    "arxiv_authors": [
      "Lin Li",
      "Guikun Chen",
      "Zhen Wang",
      "Jun Xiao",
      "Long Chen"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Compositional+Zero-shot+Learning+via+Progressive+Language-based+Observations+Lin+Li+Guikun+Chen+Zhen+Wang+Jun+Xiao+Long+Chen",
    "gs_search_success": true,
    "gs_authors": [
      "eQ-G_bQAAAAJ",
      "-gtmMpIAAAAJ",
      "I1TOdpkAAAAJ",
      "fqOwFhQAAAAJ"
    ],
    "citation_count": 17,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2411.10800",
    "title": "Test-time Conditional Text-to-Image Synthesis Using Diffusion Models",
    "year": 2024,
    "published": "2024-11-16T13:32:18Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "We consider the problem of conditional text-to-image synthesis with diffusion models. Most recent works need to either finetune specific parts of the base diffusion model or introduce new trainable parameters, leading to deployment inflexibility due to the need for training. To address this gap in the current literature, we propose our method called TINTIN: Test-time Conditional Text-to-Image Synthesis using Diffusion Models which is a new training-free test-time only algorithm to condition text",
    "arxiv_url": "https://arxiv.org/abs/2411.10800v1",
    "pdf_url": "https://arxiv.org/pdf/2411.10800v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2411.10800",
    "arxiv_authors": [
      "Tripti Shukla",
      "Srikrishna Karanam",
      "Balaji Vasan Srinivasan"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Test-time+Conditional+Text-to-Image+Synthesis+Using+Diffusion+Models+Tripti+Shukla+Srikrishna+Karanam+Balaji+Vasan+Srinivasan",
    "gs_search_success": true,
    "gs_authors": [
      "2c_x00gAAAAJ",
      "PLtMXBMAAAAJ",
      "G2-skyUAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2312.00968",
    "title": "Omni-SMoLA: Boosting Generalist Multimodal Models with Soft Mixture of Low-rank Experts",
    "year": 2023,
    "published": "2023-12-01T23:04:27Z",
    "categories": [
      "cs.CV",
      "cs.CL"
    ],
    "abstract": "Large multi-modal models (LMMs) exhibit remarkable performance across numerous tasks. However, generalist LMMs often suffer from performance degradation when tuned over a large collection of tasks. Recent research suggests that Mixture of Experts (MoE) architectures are useful for instruction tuning, but for LMMs of parameter size around O(50-100B), the prohibitive cost of replicating and storing the expert models severely limits the number of experts we can use. We propose Omni-SMoLA, an archit",
    "arxiv_url": "https://arxiv.org/abs/2312.00968v2",
    "pdf_url": "https://arxiv.org/pdf/2312.00968v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2312.00968",
    "arxiv_authors": [
      "Jialin Wu",
      "Xia Hu",
      "Yaqing Wang",
      "Bo Pang",
      "Radu Soricut"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Omni-SMoLA%3A+Boosting+Generalist+Multimodal+Models+with+Soft+Mixture+of+Low-rank+Experts+Jialin+Wu+Xia+Hu+Yaqing+Wang+Bo+Pang+Radu+Soricut",
    "gs_search_success": true,
    "gs_authors": [
      "NAzD9mgAAAAJ",
      "M7EpKqsAAAAJ",
      "_Rfg2CAAAAAJ",
      "_f6aQBUAAAAJ",
      "1PT3EQoAAAAJ"
    ],
    "citation_count": 39,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2505.21041",
    "title": "CityGo: Lightweight Urban Modeling and Rendering with Proxy Buildings and Residual Gaussians",
    "year": 2025,
    "published": "2025-05-27T11:24:08Z",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "abstract": "Accurate and efficient modeling of large-scale urban scenes is critical for applications such as AR navigation, UAV based inspection, and smart city digital twins. While aerial imagery offers broad coverage and complements limitations of ground-based data, reconstructing city-scale environments from such views remains challenging due to occlusions, incomplete geometry, and high memory demands. Recent advances like 3D Gaussian Splatting (3DGS) improve scalability and visual quality but remain lim",
    "arxiv_url": "https://arxiv.org/abs/2505.21041v3",
    "pdf_url": "https://arxiv.org/pdf/2505.21041v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2505.21041",
    "arxiv_authors": [
      "Weihang Liu",
      "Yuhui Zhong",
      "Yuke Li",
      "Xi Chen",
      "Jiadi Cui",
      "Honglong Zhang",
      "Lan Xu",
      "Xin Lou",
      "Yujiao Shi",
      "Jingyi Yu",
      "Yingliang Zhang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=CityGo%3A+Lightweight+Urban+Modeling+and+Rendering+with+Proxy+Buildings+and+Residual+Gaussians+Weihang+Liu+Yuhui+Zhong+Yuke+Li+Xi+Chen+Jiadi+Cui",
    "gs_search_success": true,
    "gs_authors": [
      "V2JJolMAAAAJ",
      "aPS5pJkAAAAJ",
      "rVsRpZEAAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 11
  },
  {
    "arxiv_id": "2303.16975",
    "title": "EgoTV: Egocentric Task Verification from Natural Language Task Descriptions",
    "year": 2023,
    "published": "2023-03-29T19:16:49Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "To enable progress towards egocentric agents capable of understanding everyday tasks specified in natural language, we propose a benchmark and a synthetic dataset called Egocentric Task Verification (EgoTV). The goal in EgoTV is to verify the execution of tasks from egocentric videos based on the natural language description of these tasks. EgoTV contains pairs of videos and their task descriptions for multi-step tasks -- these tasks contain multiple sub-task decompositions, state changes, objec",
    "arxiv_url": "https://arxiv.org/abs/2303.16975v6",
    "pdf_url": "https://arxiv.org/pdf/2303.16975v6",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2303.16975",
    "arxiv_authors": [
      "Rishi Hazra",
      "Brian Chen",
      "Akshara Rai",
      "Nitin Kamra",
      "Ruta Desai"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=EgoTV%3A+Egocentric+Task+Verification+from+Natural+Language+Task+Descriptions+Rishi+Hazra+Brian+Chen+Akshara+Rai+Nitin+Kamra+Ruta+Desai",
    "gs_search_success": true,
    "gs_authors": [
      "aWN72pEAAAAJ",
      "Qn_rbIgAAAAJ",
      "H8FJlJoAAAAJ",
      "bwZFR4EAAAAJ",
      "7zfiaA8AAAAJ"
    ],
    "citation_count": 13,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2502.20698",
    "title": "Towards General Visual-Linguistic Face Forgery Detection(V2)",
    "year": 2025,
    "published": "2025-02-28T04:15:36Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Face manipulation techniques have achieved significant advances, presenting serious challenges to security and social trust. Recent works demonstrate that leveraging multimodal models can enhance the generalization and interpretability of face forgery detection. However, existing annotation approaches, whether through human labeling or direct Multimodal Large Language Model (MLLM) generation, often suffer from hallucination issues, leading to inaccurate text descriptions, especially for high-qua",
    "arxiv_url": "https://arxiv.org/abs/2502.20698v1",
    "pdf_url": "https://arxiv.org/pdf/2502.20698v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2502.20698",
    "arxiv_authors": [
      "Ke Sun",
      "Shen Chen",
      "Taiping Yao",
      "Ziyin Zhou",
      "Jiayi Ji",
      "Xiaoshuai Sun",
      "Chia-Wen Lin",
      "Rongrong Ji"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Towards+General+Visual-Linguistic+Face+Forgery+Detection%28V2%29+Ke+Sun+Shen+Chen+Taiping+Yao+Ziyin+Zhou+Jiayi+Ji",
    "gs_search_success": true,
    "gs_authors": [
      "lRSD7PQAAAAJ",
      "xMpvoLMAAAAJ",
      "KPMK3B4AAAAJ",
      "a0NUsDoAAAAJ",
      "xp_rICcAAAAJ",
      "qkpaPuAAAAAJ",
      "r9_7F_EAAAAJ",
      "fXN3dl0AAAAJ"
    ],
    "citation_count": 44,
    "gs_author_count": 8
  },
  {
    "arxiv_id": "2410.18057",
    "title": "CLEAR: Character Unlearning in Textual and Visual Modalities",
    "year": 2024,
    "published": "2024-10-23T17:30:50Z",
    "categories": [
      "cs.CV",
      "cs.CL"
    ],
    "abstract": "Machine Unlearning (MU) is critical for removing private or hazardous information from deep learning models. While MU has advanced significantly in unimodal (text or vision) settings, multimodal unlearning (MMU) remains underexplored due to the lack of open benchmarks for evaluating cross-modal data removal. To address this gap, we introduce CLEAR, the first open-source benchmark designed specifically for MMU. CLEAR contains 200 fictitious individuals and 3,700 images linked with corresponding q",
    "arxiv_url": "https://arxiv.org/abs/2410.18057v4",
    "pdf_url": "https://arxiv.org/pdf/2410.18057v4",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2410.18057",
    "arxiv_authors": [
      "Alexey Dontsov",
      "Dmitrii Korzh",
      "Alexey Zhavoronkin",
      "Boris Mikheev",
      "Denis Bobkov",
      "Aibek Alanov",
      "Oleg Y. Rogov",
      "Ivan Oseledets",
      "Elena Tutubalina"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=CLEAR%3A+Character+Unlearning+in+Textual+and+Visual+Modalities+Alexey+Dontsov+Dmitrii+Korzh+Alexey+Zhavoronkin+Boris+Mikheev+Denis+Bobkov",
    "gs_search_success": true,
    "gs_authors": [
      "bFHpdEcAAAAJ",
      "5kMqBQEAAAAJ",
      "fiw4fa4AAAAJ",
      "MXJTRGoAAAAJ",
      "gIx9BE0AAAAJ",
      "2SK4CMIAAAAJ"
    ],
    "citation_count": 19,
    "gs_author_count": 8
  },
  {
    "arxiv_id": "2501.08182",
    "title": "CG-MER: A Card Game-based Multimodal dataset for Emotion Recognition",
    "year": 2025,
    "published": "2025-01-14T15:08:56Z",
    "categories": [
      "cs.AI",
      "cs.CV",
      "cs.HC"
    ],
    "abstract": "The field of affective computing has seen significant advancements in exploring the relationship between emotions and emerging technologies. This paper presents a novel and valuable contribution to this field with the introduction of a comprehensive French multimodal dataset designed specifically for emotion recognition. The dataset encompasses three primary modalities: facial expressions, speech, and gestures, providing a holistic perspective on emotions. Moreover, the dataset has the potential",
    "arxiv_url": "https://arxiv.org/abs/2501.08182v1",
    "pdf_url": "https://arxiv.org/pdf/2501.08182v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2501.08182",
    "arxiv_authors": [
      "Nessrine Farhat",
      "Amine Bohi",
      "Leila Ben Letaifa",
      "Rim Slama"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=CG-MER%3A+A+Card+Game-based+Multimodal+dataset+for+Emotion+Recognition+Nessrine+Farhat+Amine+Bohi+Leila+Ben+Letaifa+Rim+Slama",
    "gs_search_success": false,
    "gs_authors": [],
    "citation_count": null,
    "gs_author_count": null,
    "error_type": "fetch_failed"
  },
  {
    "arxiv_id": "2307.10664",
    "title": "Lighting up NeRF via Unsupervised Decomposition and Enhancement",
    "year": 2023,
    "published": "2023-07-20T07:46:34Z",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "abstract": "Neural Radiance Field (NeRF) is a promising approach for synthesizing novel views, given a set of images and the corresponding camera poses of a scene. However, images photographed from a low-light scene can hardly be used to train a NeRF model to produce high-quality results, due to their low pixel intensities, heavy noise, and color distortion. Combining existing low-light image enhancement methods with NeRF methods also does not work well due to the view inconsistency caused by the individual",
    "arxiv_url": "https://arxiv.org/abs/2307.10664v1",
    "pdf_url": "https://arxiv.org/pdf/2307.10664v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2307.10664",
    "arxiv_authors": [
      "Haoyuan Wang",
      "Xiaogang Xu",
      "Ke Xu",
      "Rynson WH. Lau"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Lighting+up+NeRF+via+Unsupervised+Decomposition+and+Enhancement+Haoyuan+Wang+Xiaogang+Xu+Ke+Xu+Rynson+WH.+Lau",
    "gs_search_success": true,
    "gs_authors": [
      "R65xDQwAAAAJ",
      "KilQqKYAAAAJ",
      "AEdedn0AAAAJ",
      "2meBhbQAAAAJ"
    ],
    "citation_count": 59,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2503.02600",
    "title": "Resource-Efficient Affordance Grounding with Complementary Depth and Semantic Prompts",
    "year": 2025,
    "published": "2025-03-04T13:20:42Z",
    "categories": [
      "cs.CV",
      "cs.RO",
      "eess.IV"
    ],
    "abstract": "Affordance refers to the functional properties that an agent perceives and utilizes from its environment, and is key perceptual information required for robots to perform actions. This information is rich and multimodal in nature. Existing multimodal affordance methods face limitations in extracting useful information, mainly due to simple structural designs, basic fusion methods, and large model parameters, making it difficult to meet the performance requirements for practical deployment. To ad",
    "arxiv_url": "https://arxiv.org/abs/2503.02600v2",
    "pdf_url": "https://arxiv.org/pdf/2503.02600v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2503.02600",
    "arxiv_authors": [
      "Yizhou Huang",
      "Fan Yang",
      "Guoliang Zhu",
      "Gen Li",
      "Hao Shi",
      "Yukun Zuo",
      "Wenrui Chen",
      "Zhiyong Li",
      "Kailun Yang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Resource-Efficient+Affordance+Grounding+with+Complementary+Depth+and+Semantic+Prompts+Yizhou+Huang+Fan+Yang+Guoliang+Zhu+Gen+Li+Hao+Shi",
    "gs_search_success": true,
    "gs_authors": [
      "0EI9msQAAAAJ",
      "c3bvnWIAAAAJ",
      "gbuEGBQAAAAJ",
      "bG3YNRYAAAAJ",
      "Rfh4mm0AAAAJ",
      "pKFqWhgAAAAJ",
      "Ib6joREAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 9
  },
  {
    "arxiv_id": "2504.18215",
    "title": "Unify3D: An Augmented Holistic End-to-end Monocular 3D Human Reconstruction via Anatomy Shaping and Twins Negotiating",
    "year": 2025,
    "published": "2025-04-25T09:49:23Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Monocular 3D clothed human reconstruction aims to create a complete 3D avatar from a single image. To tackle the human geometry lacking in one RGB image, current methods typically resort to a preceding model for an explicit geometric representation. For the reconstruction itself, focus is on modeling both it and the input image. This routine is constrained by the preceding model, and overlooks the integrity of the reconstruction task. To address this, this paper introduces a novel paradigm that ",
    "arxiv_url": "https://arxiv.org/abs/2504.18215v2",
    "pdf_url": "https://arxiv.org/pdf/2504.18215v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2504.18215",
    "arxiv_authors": [
      "Nanjie Yao",
      "Gangjian Zhang",
      "Wenhao Shen",
      "Jian Shu",
      "Hao Wang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Unify3D%3A+An+Augmented+Holistic+End-to-end+Monocular+3D+Human+Reconstruction+via+Anatomy+Shaping+and+Twins+Negotiating+Nanjie+Yao+Gangjian+Zhang+Wenhao+Shen+Jian+Shu+Hao+Wang",
    "gs_search_success": true,
    "gs_authors": [
      "F0UvaMwAAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2403.19076",
    "title": "Tiny Machine Learning: Progress and Futures",
    "year": 2024,
    "published": "2024-03-28T00:34:56Z",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "abstract": "Tiny Machine Learning (TinyML) is a new frontier of machine learning. By squeezing deep learning models into billions of IoT devices and microcontrollers (MCUs), we expand the scope of AI applications and enable ubiquitous intelligence. However, TinyML is challenging due to hardware constraints: the tiny memory resource makes it difficult to hold deep learning models designed for cloud and mobile platforms. There is also limited compiler and inference engine support for bare-metal devices. There",
    "arxiv_url": "https://arxiv.org/abs/2403.19076v2",
    "pdf_url": "https://arxiv.org/pdf/2403.19076v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2403.19076",
    "arxiv_authors": [
      "Ji Lin",
      "Ligeng Zhu",
      "Wei-Ming Chen",
      "Wei-Chen Wang",
      "Song Han"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Tiny+Machine+Learning%3A+Progress+and+Futures+Ji+Lin+Ligeng+Zhu+Wei-Ming+Chen+Wei-Chen+Wang+Song+Han",
    "gs_search_success": true,
    "gs_authors": [
      "E0iCaa4AAAAJ",
      "6xFvyJwAAAAJ",
      "y0LVrtgAAAAJ",
      "eYrx3KAAAAAJ",
      "dVtzVVAAAAAJ"
    ],
    "citation_count": 339,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2404.10407",
    "title": "Comprehensive Survey of Model Compression and Speed up for Vision Transformers",
    "year": 2024,
    "published": "2024-04-16T09:19:11Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Vision Transformers (ViT) have marked a paradigm shift in computer vision, outperforming state-of-the-art models across diverse tasks. However, their practical deployment is hampered by high computational and memory demands. This study addresses the challenge by evaluating four primary model compression techniques: quantization, low-rank approximation, knowledge distillation, and pruning. We methodically analyze and compare the efficacy of these techniques and their combinations in optimizing Vi",
    "arxiv_url": "https://arxiv.org/abs/2404.10407v1",
    "pdf_url": "https://arxiv.org/pdf/2404.10407v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2404.10407",
    "arxiv_authors": [
      "Feiyang Chen",
      "Ziqian Luo",
      "Lisang Zhou",
      "Xueting Pan",
      "Ying Jiang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Comprehensive+Survey+of+Model+Compression+and+Speed+up+for+Vision+Transformers+Feiyang+Chen+Ziqian+Luo+Lisang+Zhou+Xueting+Pan+Ying+Jiang",
    "gs_search_success": true,
    "gs_authors": [
      "-3HMPOwAAAAJ",
      "yJgHVyYAAAAJ"
    ],
    "citation_count": 50,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2301.04626",
    "title": "Deep Axial Hypercomplex Networks",
    "year": 2023,
    "published": "2023-01-11T18:31:00Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Over the past decade, deep hypercomplex-inspired networks have enhanced feature extraction for image classification by enabling weight sharing across input channels. Recent works make it possible to improve representational capabilities by using hypercomplex-inspired networks which consume high computational costs. This paper reduces this cost by factorizing a quaternion 2D convolutional module into two consecutive vectormap 1D convolutional modules. Also, we use 5D parameterized hypercomplex mu",
    "arxiv_url": "https://arxiv.org/abs/2301.04626v1",
    "pdf_url": "https://arxiv.org/pdf/2301.04626v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2301.04626",
    "arxiv_authors": [
      "Nazmul Shahadat",
      "Anthony S. Maida"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Deep+Axial+Hypercomplex+Networks+Nazmul+Shahadat+Anthony+S.+Maida",
    "gs_search_success": true,
    "gs_authors": [
      "yCwFNtMAAAAJ",
      "aI7_PpEAAAAJ"
    ],
    "citation_count": 3,
    "gs_author_count": 2
  },
  {
    "arxiv_id": "2411.17305",
    "title": "in-Car Biometrics (iCarB) Datasets for Driver Recognition: Face, Fingerprint, and Voice",
    "year": 2024,
    "published": "2024-11-26T10:52:15Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "We present three biometric datasets (iCarB-Face, iCarB-Fingerprint, iCarB-Voice) containing face videos, fingerprint images, and voice samples, collected inside a car from 200 consenting volunteers. The data was acquired using a near-infrared camera, two fingerprint scanners, and two microphones, while the volunteers were seated in the driver's seat of the car. The data collection took place while the car was parked both indoors and outdoors, and different \"noises\" were added to simulate non-ide",
    "arxiv_url": "https://arxiv.org/abs/2411.17305v1",
    "pdf_url": "https://arxiv.org/pdf/2411.17305v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2411.17305",
    "arxiv_authors": [
      "Vedrana Krivokuca Hahn",
      "Jeremy Maceiras",
      "Alain Komaty",
      "Philip Abbet",
      "Sebastien Marcel"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=in-Car+Biometrics+%28iCarB%29+Datasets+for+Driver+Recognition%3A+Face%2C+Fingerprint%2C+and+Voice+Vedrana+Krivokuca+Hahn+Jeremy+Maceiras+Alain+Komaty+Philip+Abbet+Sebastien+Marcel",
    "gs_search_success": true,
    "gs_authors": [
      "aeIX3V0AAAAJ",
      "2cv7clrdUCsJ",
      "fEovmhkAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": null
  },
  {
    "arxiv_id": "2504.21340",
    "title": "Towards Improved Cervical Cancer Screening: Vision Transformer-Based Classification and Interpretability",
    "year": 2025,
    "published": "2025-04-30T05:59:56Z",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "We propose a novel approach to cervical cell image classification for cervical cancer screening using the EVA-02 transformer model. We developed a four-step pipeline: fine-tuning EVA-02, feature extraction, selecting important features through multiple machine learning models, and training a new artificial neural network with optional loss weighting for improved generalization. With this design, our best model achieved an F1-score of 0.85227, outperforming the baseline EVA-02 model (0.84878). We",
    "arxiv_url": "https://arxiv.org/abs/2504.21340v1",
    "pdf_url": "https://arxiv.org/pdf/2504.21340v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2504.21340",
    "arxiv_authors": [
      "Khoa Tuan Nguyen",
      "Ho-min Park",
      "Gaeun Oh",
      "Joris Vankerschaver",
      "Wesley De Neve"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Towards+Improved+Cervical+Cancer+Screening%3A+Vision+Transformer-Based+Classification+and+Interpretability+Khoa+Tuan+Nguyen+Ho-min+Park+Gaeun+Oh+Joris+Vankerschaver+Wesley+De+Neve",
    "gs_search_success": true,
    "gs_authors": [
      "7XpRM4cAAAAJ",
      "mFzW6LEAAAAJ",
      "bHjp9jEAAAAJ",
      "N5vV04AAAAAJ",
      "VpjWb7wAAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2412.14042",
    "title": "CAD-Recode: Reverse Engineering CAD Code from Point Clouds",
    "year": 2024,
    "published": "2024-12-18T16:55:42Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Computer-Aided Design (CAD) models are typically constructed by sequentially drawing parametric sketches and applying CAD operations to obtain a 3D model. The problem of 3D CAD reverse engineering consists of reconstructing the sketch and CAD operation sequences from 3D representations such as point clouds. In this paper, we address this challenge through novel contributions across three levels: CAD sequence representation, network design, and training dataset. In particular, we represent CAD sk",
    "arxiv_url": "https://arxiv.org/abs/2412.14042v2",
    "pdf_url": "https://arxiv.org/pdf/2412.14042v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2412.14042",
    "arxiv_authors": [
      "Danila Rukhovich",
      "Elona Dupont",
      "Dimitrios Mallis",
      "Kseniya Cherenkova",
      "Anis Kacem",
      "Djamila Aouada"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=CAD-Recode%3A+Reverse+Engineering+CAD+Code+from+Point+Clouds+Danila+Rukhovich+Elona+Dupont+Dimitrios+Mallis+Kseniya+Cherenkova+Anis+Kacem",
    "gs_search_success": true,
    "gs_authors": [
      "i9J6YFMAAAAJ",
      "VepvFBkAAAAJ",
      "WBmJVSkAAAAJ",
      "Osx2uh5eA2kC",
      "Gfc5ZXoAAAAJ",
      "K3EWusMAAAAJ"
    ],
    "citation_count": 24,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2310.06282",
    "title": "MuseChat: A Conversational Music Recommendation System for Videos",
    "year": 2023,
    "published": "2023-10-10T03:32:33Z",
    "categories": [
      "cs.LG",
      "cs.CV",
      "cs.IR"
    ],
    "abstract": "Music recommendation for videos attracts growing interest in multi-modal research. However, existing systems focus primarily on content compatibility, often ignoring the users' preferences. Their inability to interact with users for further refinements or to provide explanations leads to a less satisfying experience. We address these issues with MuseChat, a first-of-its-kind dialogue-based recommendation system that personalizes music suggestions for videos. Our system consists of two key functi",
    "arxiv_url": "https://arxiv.org/abs/2310.06282v4",
    "pdf_url": "https://arxiv.org/pdf/2310.06282v4",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2310.06282",
    "arxiv_authors": [
      "Zhikang Dong",
      "Bin Chen",
      "Xiulong Liu",
      "Pawel Polak",
      "Peng Zhang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=MuseChat%3A+A+Conversational+Music+Recommendation+System+for+Videos+Zhikang+Dong+Bin+Chen+Xiulong+Liu+Pawel+Polak+Peng+Zhang",
    "gs_search_success": true,
    "gs_authors": [
      "4Y8T3n0AAAAJ",
      "e5GPhrMAAAAJ",
      "W6UmxCIAAAAJ",
      "Ot5mZs0AAAAJ"
    ],
    "citation_count": 62,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2505.21754",
    "title": "Visual Loop Closure Detection Through Deep Graph Consensus",
    "year": 2025,
    "published": "2025-05-27T20:42:47Z",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "abstract": "Visual loop closure detection traditionally relies on place recognition methods to retrieve candidate loops that are validated using computationally expensive RANSAC-based geometric verification. As false positive loop closures significantly degrade downstream pose graph estimates, verifying a large number of candidates in online simultaneous localization and mapping scenarios is constrained by limited time and compute resources. While most deep loop closure detection approaches only operate on ",
    "arxiv_url": "https://arxiv.org/abs/2505.21754v1",
    "pdf_url": "https://arxiv.org/pdf/2505.21754v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2505.21754",
    "arxiv_authors": [
      "Martin Büchner",
      "Liza Dahiya",
      "Simon Dorer",
      "Vipul Ramtekkar",
      "Kenji Nishimiya",
      "Daniele Cattaneo",
      "Abhinav Valada"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Visual+Loop+Closure+Detection+Through+Deep+Graph+Consensus+Martin+B%C3%BCchner+Liza+Dahiya+Simon+Dorer+Vipul+Ramtekkar+Kenji+Nishimiya",
    "gs_search_success": true,
    "gs_authors": [
      "aZ43NwEAAAAJ",
      "RK_rxhYAAAAJ",
      "4Kif-mgAAAAJ",
      "Jwr5TRIAAAAJ",
      "6O_ECDkAAAAJ",
      "LcARjz0AAAAJ",
      "myrKT24AAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 7
  },
  {
    "arxiv_id": "2504.08603",
    "title": "FindAnything: Open-Vocabulary and Object-Centric Mapping for Robot Exploration in Any Environment",
    "year": 2025,
    "published": "2025-04-11T15:12:05Z",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV"
    ],
    "abstract": "Geometrically accurate and semantically expressive map representations have proven invaluable to facilitate robust and safe mobile robot navigation and task planning. Nevertheless, real-time, open-vocabulary semantic understanding of large-scale unknown environments is still an open problem. In this paper we present FindAnything, an open-world mapping and exploration framework that incorporates vision-language information into dense volumetric submaps. Thanks to the use of vision-language featur",
    "arxiv_url": "https://arxiv.org/abs/2504.08603v2",
    "pdf_url": "https://arxiv.org/pdf/2504.08603v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2504.08603",
    "arxiv_authors": [
      "Sebastián Barbas Laina",
      "Simon Boche",
      "Sotiris Papatheodorou",
      "Simon Schaefer",
      "Jaehyung Jung",
      "Stefan Leutenegger"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=FindAnything%3A+Open-Vocabulary+and+Object-Centric+Mapping+for+Robot+Exploration+in+Any+Environment+Sebasti%C3%A1n+Barbas+Laina+Simon+Boche+Sotiris+Papatheodorou+Simon+Schaefer+Jaehyung+Jung",
    "gs_search_success": true,
    "gs_authors": [
      "S_aTq3cAAAAJ",
      "8VVhlL0AAAAJ",
      "YxB2vHEAAAAJ"
    ],
    "citation_count": 8,
    "gs_author_count": null
  },
  {
    "arxiv_id": "2407.13778",
    "title": "Assessing the Potential of PlanetScope Satellite Imagery to Estimate Particulate Matter Oxidative Potential",
    "year": 2024,
    "published": "2024-07-01T22:23:19Z",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "Oxidative potential (OP), which measures particulate matter's (PM) capacity to induce oxidative stress in the lungs, is increasingly recognized as an indicator of PM toxicity. Since OP is not routinely monitored, it can be challenging to estimate exposure and health impacts. Remote sensing data are commonly used to estimate PM mass concentration, but have never been used to estimate OP. In this study, we evaluate the potential of satellite images to estimate OP as measured by acellular ascorbic ",
    "arxiv_url": "https://arxiv.org/abs/2407.13778v1",
    "pdf_url": "https://arxiv.org/pdf/2407.13778v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2407.13778",
    "arxiv_authors": [
      "Ian Hough",
      "Loïc Argentier",
      "Ziyang Jiang",
      "Tongshu Zheng",
      "Mike Bergin",
      "David Carlson",
      "Jean-Luc Jaffrezo",
      "Jocelyn Chanussot",
      "Gaëlle Uzu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Assessing+the+Potential+of+PlanetScope+Satellite+Imagery+to+Estimate+Particulate+Matter+Oxidative+Potential+Ian+Hough+Lo%C3%AFc+Argentier+Ziyang+Jiang+Tongshu+Zheng+Mike+Bergin",
    "gs_search_success": true,
    "gs_authors": [
      "6owK2OQAAAAJ",
      "AqhHdA8AAAAJ",
      "G73RRAQAAAAJ",
      "UIUFnVUAAAAJ",
      "50Ft04oAAAAJ",
      "R9FqK9cAAAAJ",
      "cuCwWe4AAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 8
  },
  {
    "arxiv_id": "2301.07870",
    "title": "Fast-BEV: Towards Real-time On-vehicle Bird's-Eye View Perception",
    "year": 2023,
    "published": "2023-01-19T03:58:48Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Recently, the pure camera-based Bird's-Eye-View (BEV) perception removes expensive Lidar sensors, making it a feasible solution for economical autonomous driving. However, most existing BEV solutions either suffer from modest performance or require considerable resources to execute on-vehicle inference. This paper proposes a simple yet effective framework, termed Fast-BEV, which is capable of performing real-time BEV perception on the on-vehicle chips. Towards this goal, we first empirically fin",
    "arxiv_url": "https://arxiv.org/abs/2301.07870v1",
    "pdf_url": "https://arxiv.org/pdf/2301.07870v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2301.07870",
    "arxiv_authors": [
      "Bin Huang",
      "Yangguang Li",
      "Enze Xie",
      "Feng Liang",
      "Luya Wang",
      "Mingzhu Shen",
      "Fenggang Liu",
      "Tianqi Wang",
      "Ping Luo",
      "Jing Shao"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Fast-BEV%3A+Towards+Real-time+On-vehicle+Bird%27s-Eye+View+Perception+Bin+Huang+Yangguang+Li+Enze+Xie+Feng+Liang+Luya+Wang",
    "gs_search_success": true,
    "gs_authors": [
      "RIfX6LkAAAAJ",
      "a7AMvgkAAAAJ",
      "o7vrw6IAAAAJ",
      "ecTFCUMAAAAJ",
      "42MVVPgAAAAJ",
      "oXnoc0AAAAAJ",
      "aXdjxb4AAAAJ",
      "D62c5woAAAAJ"
    ],
    "citation_count": 36,
    "gs_author_count": 10
  },
  {
    "arxiv_id": "2301.05994",
    "title": "Min-Max-Jump distance and its applications",
    "year": 2023,
    "published": "2023-01-15T00:55:40Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "We explore three applications of Min-Max-Jump distance (MMJ distance). MMJ-based K-means revises K-means with MMJ distance. MMJ-based Silhouette coefficient revises Silhouette coefficient with MMJ distance. We also tested the Clustering with Neural Network and Index (CNNI) model with MMJ-based Silhouette coefficient. In the last application, we tested using Min-Max-Jump distance for predicting labels of new points, after a clustering analysis of data. Result shows Min-Max-Jump distance achieves ",
    "arxiv_url": "https://arxiv.org/abs/2301.05994v6",
    "pdf_url": "https://arxiv.org/pdf/2301.05994v6",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2301.05994",
    "arxiv_authors": [
      "Gangli Liu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Min-Max-Jump+distance+and+its+applications+Gangli+Liu",
    "gs_search_success": false,
    "gs_authors": [],
    "citation_count": null,
    "gs_author_count": null,
    "error_type": "fetch_failed"
  },
  {
    "arxiv_id": "2305.10925",
    "title": "Unsupervised Hyperspectral Pansharpening via Low-rank Diffusion Model",
    "year": 2023,
    "published": "2023-05-18T12:38:29Z",
    "categories": [
      "cs.CV",
      "eess.IV"
    ],
    "abstract": "Hyperspectral pansharpening is a process of merging a high-resolution panchromatic (PAN) image and a low-resolution hyperspectral (LRHS) image to create a single high-resolution hyperspectral (HRHS) image. Existing Bayesian-based HS pansharpening methods require designing handcraft image prior to characterize the image features, and deep learning-based HS pansharpening methods usually require a large number of paired training data and suffer from poor generalization ability. To address these iss",
    "arxiv_url": "https://arxiv.org/abs/2305.10925v2",
    "pdf_url": "https://arxiv.org/pdf/2305.10925v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2305.10925",
    "arxiv_authors": [
      "Xiangyu Rui",
      "Xiangyong Cao",
      "Li Pang",
      "Zeyu Zhu",
      "Zongsheng Yue",
      "Deyu Meng"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Unsupervised+Hyperspectral+Pansharpening+via+Low-rank+Diffusion+Model+Xiangyu+Rui+Xiangyong+Cao+Li+Pang+Zeyu+Zhu+Zongsheng+Yue",
    "gs_search_success": true,
    "gs_authors": [
      "IePM9RsAAAAJ",
      "F554LkQAAAAJ",
      "an6w-64AAAAJ",
      "X3CisOwAAAAJ",
      "EPldY6EAAAAJ"
    ],
    "citation_count": 52,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2412.09428",
    "title": "Multimodal Music Generation with Explicit Bridges and Retrieval Augmentation",
    "year": 2024,
    "published": "2024-12-12T16:33:21Z",
    "categories": [
      "cs.CV",
      "cs.MM",
      "cs.SD",
      "eess.AS"
    ],
    "abstract": "Multimodal music generation aims to produce music from diverse input modalities, including text, videos, and images. Existing methods use a common embedding space for multimodal fusion. Despite their effectiveness in other modalities, their application in multimodal music generation faces challenges of data scarcity, weak cross-modal alignment, and limited controllability. This paper addresses these issues by using explicit bridges of text and music for multimodal alignment. We introduce a novel",
    "arxiv_url": "https://arxiv.org/abs/2412.09428v1",
    "pdf_url": "https://arxiv.org/pdf/2412.09428v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2412.09428",
    "arxiv_authors": [
      "Baisen Wang",
      "Le Zhuo",
      "Zhaokai Wang",
      "Chenxi Bao",
      "Wu Chengjing",
      "Xuecheng Nie",
      "Jiao Dai",
      "Jizhong Han",
      "Yue Liao",
      "Si Liu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Multimodal+Music+Generation+with+Explicit+Bridges+and+Retrieval+Augmentation+Baisen+Wang+Le+Zhuo+Zhaokai+Wang+Chenxi+Bao+Wu+Chengjing",
    "gs_search_success": true,
    "gs_authors": [
      "0b_BPiMAAAAJ",
      "h93ctSsAAAAJ",
      "mIt-3fEAAAAJ",
      "W0zVf-oAAAAJ",
      "xe2o29UAAAAJ",
      "25JjWb8AAAAJ",
      "NxNC8qgAAAAJ"
    ],
    "citation_count": 8,
    "gs_author_count": 10
  },
  {
    "arxiv_id": "2404.14435",
    "title": "Frenet-Serret Frame-based Decomposition for Part Segmentation of 3D Curvilinear Structures",
    "year": 2024,
    "published": "2024-04-19T16:40:24Z",
    "categories": [
      "cs.CV",
      "eess.IV"
    ],
    "abstract": "Accurately segmenting 3D curvilinear structures in medical imaging remains challenging due to their complex geometry and the scarcity of diverse, large-scale datasets for algorithm development and evaluation. In this paper, we use dendritic spine segmentation as a case study and address these challenges by introducing a novel Frenet--Serret Frame-based Decomposition, which decomposes 3D curvilinear structures into a globally \\( C^2 \\) continuous curve that captures the overall shape, and a cylin",
    "arxiv_url": "https://arxiv.org/abs/2404.14435v3",
    "pdf_url": "https://arxiv.org/pdf/2404.14435v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2404.14435",
    "arxiv_authors": [
      "Leslie Gu",
      "Jason Ken Adhinarta",
      "Mikhail Bessmeltsev",
      "Jiancheng Yang",
      "Yongjie Jessica Zhang",
      "Wenjie Yin",
      "Daniel Berger",
      "Jeff Lichtman",
      "Hanspeter Pfister",
      "Donglai Wei"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Frenet-Serret+Frame-based+Decomposition+for+Part+Segmentation+of+3D+Curvilinear+Structures+Leslie+Gu+Jason+Ken+Adhinarta+Mikhail+Bessmeltsev+Jiancheng+Yang+Yongjie+Jessica+Zhang",
    "gs_search_success": true,
    "gs_authors": [
      "EHkDyiUAAAAJ",
      "t4T8w8UAAAAJ",
      "EUzwk5cAAAAJ",
      "nwbMyO0AAAAJ",
      "peHbOtYAAAAJ",
      "dzHYBKcAAAAJ",
      "x5acEMEAAAAJ",
      "I2a9AJEAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 8
  },
  {
    "arxiv_id": "2503.21259",
    "title": "Reducing CT Metal Artifacts by Learning Latent Space Alignment with Gemstone Spectral Imaging Data",
    "year": 2025,
    "published": "2025-03-27T08:35:10Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Metal artifacts in CT slices have long posed challenges in medical diagnostics. These artifacts degrade image quality, resulting in suboptimal visualization and complicating the accurate interpretation of tissues adjacent to metal implants. To address these issues, we introduce the Latent Gemstone Spectral Imaging (GSI) Alignment Framework, which effectively reduces metal artifacts while avoiding the introduction of noise information. Our work is based on a key finding that even artifact-affecte",
    "arxiv_url": "https://arxiv.org/abs/2503.21259v1",
    "pdf_url": "https://arxiv.org/pdf/2503.21259v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2503.21259",
    "arxiv_authors": [
      "Wencheng Han",
      "Dongqian Guo",
      "Xiao Chen",
      "Pang Lyu",
      "Yi Jin",
      "Jianbing Shen"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Reducing+CT+Metal+Artifacts+by+Learning+Latent+Space+Alignment+with+Gemstone+Spectral+Imaging+Data+Wencheng+Han+Dongqian+Guo+Xiao+Chen+Pang+Lyu+Yi+Jin",
    "gs_search_success": true,
    "gs_authors": [
      "hGZueIUAAAAJ",
      "_Q3NTToAAAAJ",
      "fC226aMAAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2312.07661",
    "title": "CLIP as RNN: Segment Countless Visual Concepts without Training Endeavor",
    "year": 2023,
    "published": "2023-12-12T19:00:04Z",
    "categories": [
      "cs.CV",
      "cs.CL",
      "cs.LG",
      "cs.MM"
    ],
    "abstract": "Existing open-vocabulary image segmentation methods require a fine-tuning step on mask labels and/or image-text datasets. Mask labels are labor-intensive, which limits the number of categories in segmentation datasets. Consequently, the vocabulary capacity of pre-trained VLMs is severely reduced after fine-tuning. However, without fine-tuning, VLMs trained under weak image-text supervision tend to make suboptimal mask predictions. To alleviate these issues, we introduce a novel recurrent framewo",
    "arxiv_url": "https://arxiv.org/abs/2312.07661v3",
    "pdf_url": "https://arxiv.org/pdf/2312.07661v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2312.07661",
    "arxiv_authors": [
      "Shuyang Sun",
      "Runjia Li",
      "Philip Torr",
      "Xiuye Gu",
      "Siyang Li"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=CLIP+as+RNN%3A+Segment+Countless+Visual+Concepts+without+Training+Endeavor+Shuyang+Sun+Runjia+Li+Philip+Torr+Xiuye+Gu+Siyang+Li",
    "gs_search_success": true,
    "gs_authors": [
      "J32uBrUAAAAJ",
      "kPxa2w0AAAAJ",
      "PoAvGRMAAAAJ",
      "qCrypnoAAAAJ",
      "Zu3Rwn8AAAAJ"
    ],
    "citation_count": 53,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2503.19358",
    "title": "From Sparse to Dense: Camera Relocalization with Scene-Specific Detector from Feature Gaussian Splatting",
    "year": 2025,
    "published": "2025-03-25T05:18:19Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "This paper presents a novel camera relocalization method, STDLoc, which leverages Feature Gaussian as scene representation. STDLoc is a full relocalization pipeline that can achieve accurate relocalization without relying on any pose prior. Unlike previous coarse-to-fine localization methods that require image retrieval first and then feature matching, we propose a novel sparse-to-dense localization paradigm. Based on this scene representation, we introduce a novel matching-oriented Gaussian sam",
    "arxiv_url": "https://arxiv.org/abs/2503.19358v1",
    "pdf_url": "https://arxiv.org/pdf/2503.19358v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2503.19358",
    "arxiv_authors": [
      "Zhiwei Huang",
      "Hailin Yu",
      "Yichun Shentu",
      "Jin Yuan",
      "Guofeng Zhang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=From+Sparse+to+Dense%3A+Camera+Relocalization+with+Scene-Specific+Detector+from+Feature+Gaussian+Splatting+Zhiwei+Huang+Hailin+Yu+Yichun+Shentu+Jin+Yuan+Guofeng+Zhang",
    "gs_search_success": true,
    "gs_authors": [
      "F0xfpXAAAAAJ",
      "nlTOaxYAAAAJ"
    ],
    "citation_count": 9,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2412.05134",
    "title": "How to Squeeze An Explanation Out of Your Model",
    "year": 2024,
    "published": "2024-12-06T15:47:53Z",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "Deep learning models are widely used nowadays for their reliability in performing various tasks. However, they do not typically provide the reasoning behind their decision, which is a significant drawback, particularly for more sensitive areas such as biometrics, security and healthcare. The most commonly used approaches to provide interpretability create visual attention heatmaps of regions of interest on an image based on models gradient backpropagation. Although this is a viable approach, cur",
    "arxiv_url": "https://arxiv.org/abs/2412.05134v1",
    "pdf_url": "https://arxiv.org/pdf/2412.05134v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2412.05134",
    "arxiv_authors": [
      "Tiago Roxo",
      "Joana C. Costa",
      "Pedro R. M. Inácio",
      "Hugo Proença"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=How+to+Squeeze+An+Explanation+Out+of+Your+Model+Tiago+Roxo+Joana+C.+Costa+Pedro+R.+M.+In%C3%A1cio+Hugo+Proen%C3%A7a",
    "gs_search_success": true,
    "gs_authors": [
      "HhxSgvYAAAAJ",
      "5IINvyoAAAAJ",
      "FkTmrw4AAAAJ",
      "dJ2RA44AAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2409.02097",
    "title": "LinFusion: 1 GPU, 1 Minute, 16K Image",
    "year": 2024,
    "published": "2024-09-03T17:54:39Z",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "Modern diffusion models, particularly those utilizing a Transformer-based UNet for denoising, rely heavily on self-attention operations to manage complex spatial relationships, thus achieving impressive generation performance. However, this existing paradigm faces significant challenges in generating high-resolution visual content due to its quadratic time and memory complexity with respect to the number of spatial tokens. To address this limitation, we aim at a novel linear attention mechanism ",
    "arxiv_url": "https://arxiv.org/abs/2409.02097v3",
    "pdf_url": "https://arxiv.org/pdf/2409.02097v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2409.02097",
    "arxiv_authors": [
      "Songhua Liu",
      "Weihao Yu",
      "Zhenxiong Tan",
      "Xinchao Wang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=LinFusion%3A+1+GPU%2C+1+Minute%2C+16K+Image+Songhua+Liu+Weihao+Yu+Zhenxiong+Tan+Xinchao+Wang",
    "gs_search_success": true,
    "gs_authors": [
      "LYxjt1QAAAAJ",
      "w69Buq0AAAAJ",
      "AnYh2rAAAAAJ",
      "HP9Be6UAAAAJ"
    ],
    "citation_count": 29,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2405.21013",
    "title": "StrucTexTv3: An Efficient Vision-Language Model for Text-rich Image Perception, Comprehension, and Beyond",
    "year": 2024,
    "published": "2024-05-31T16:55:04Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Text-rich images have significant and extensive value, deeply integrated into various aspects of human life. Notably, both visual cues and linguistic symbols in text-rich images play crucial roles in information transmission but are accompanied by diverse challenges. Therefore, the efficient and effective understanding of text-rich images is a crucial litmus test for the capability of Vision-Language Models. We have crafted an efficient vision-language model, StrucTexTv3, tailored to tackle vari",
    "arxiv_url": "https://arxiv.org/abs/2405.21013v3",
    "pdf_url": "https://arxiv.org/pdf/2405.21013v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2405.21013",
    "arxiv_authors": [
      "Pengyuan Lyu",
      "Yulin Li",
      "Hao Zhou",
      "Weihong Ma",
      "Xingyu Wan",
      "Qunyi Xie",
      "Liang Wu",
      "Chengquan Zhang",
      "Kun Yao",
      "Errui Ding",
      "Jingdong Wang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=StrucTexTv3%3A+An+Efficient+Vision-Language+Model+for+Text-rich+Image+Perception%2C+Comprehension%2C+and+Beyond+Pengyuan+Lyu+Yulin+Li+Hao+Zhou+Weihong+Ma+Xingyu+Wan",
    "gs_search_success": true,
    "gs_authors": [
      "whvv9NgAAAAJ",
      "z5SPCmgAAAAJ",
      "xZ-0R3cAAAAJ",
      "luVjcWcAAAAJ",
      "1JkCa_AAAAAJ",
      "JZiUJj6_MJYC",
      "1wzEtxcAAAAJ"
    ],
    "citation_count": 5,
    "gs_author_count": 11
  },
  {
    "arxiv_id": "2406.04178",
    "title": "Encoding Semantic Priors into the Weights of Implicit Neural Representation",
    "year": 2024,
    "published": "2024-06-06T15:35:41Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Implicit neural representation (INR) has recently emerged as a promising paradigm for signal representations, which takes coordinates as inputs and generates corresponding signal values. Since these coordinates contain no semantic features, INR fails to take any semantic information into consideration. However, semantic information has been proven critical in many vision tasks, especially for visual signal representation. This paper proposes a reparameterization method termed as SPW, which encod",
    "arxiv_url": "https://arxiv.org/abs/2406.04178v1",
    "pdf_url": "https://arxiv.org/pdf/2406.04178v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2406.04178",
    "arxiv_authors": [
      "Zhicheng Cai",
      "Qiu Shen"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Encoding+Semantic+Priors+into+the+Weights+of+Implicit+Neural+Representation+Zhicheng+Cai+Qiu+Shen",
    "gs_search_success": true,
    "gs_authors": [
      "cFS40tMAAAAJ",
      "9wBz7NkAAAAJ"
    ],
    "citation_count": 2,
    "gs_author_count": 2
  },
  {
    "arxiv_id": "2404.12777",
    "title": "EfficientGS: Streamlining Gaussian Splatting for Large-Scale High-Resolution Scene Representation",
    "year": 2024,
    "published": "2024-04-19T10:32:30Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "In the domain of 3D scene representation, 3D Gaussian Splatting (3DGS) has emerged as a pivotal technology. However, its application to large-scale, high-resolution scenes (exceeding 4k$\\times$4k pixels) is hindered by the excessive computational requirements for managing a large number of Gaussians. Addressing this, we introduce 'EfficientGS', an advanced approach that optimizes 3DGS for high-resolution, large-scale scenes. We analyze the densification process in 3DGS and identify areas of Gaus",
    "arxiv_url": "https://arxiv.org/abs/2404.12777v1",
    "pdf_url": "https://arxiv.org/pdf/2404.12777v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2404.12777",
    "arxiv_authors": [
      "Wenkai Liu",
      "Tao Guan",
      "Bin Zhu",
      "Lili Ju",
      "Zikai Song",
      "Dan Li",
      "Yuesong Wang",
      "Wei Yang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=EfficientGS%3A+Streamlining+Gaussian+Splatting+for+Large-Scale+High-Resolution+Scene+Representation+Wenkai+Liu+Tao+Guan+Bin+Zhu+Lili+Ju+Zikai+Song",
    "gs_search_success": true,
    "gs_authors": [
      "1qnuOZsAAAAJ",
      "gVNLbIQAAAAJ",
      "C8pXaGkAAAAJ",
      "rhsNpmkAAAAJ",
      "g-TqdZQAAAAJ",
      "dCyytKYAAAAJ"
    ],
    "citation_count": 20,
    "gs_author_count": 8
  },
  {
    "arxiv_id": "2407.00252",
    "title": "Assistive Image Annotation Systems with Deep Learning and Natural Language Capabilities: A Review",
    "year": 2024,
    "published": "2024-06-28T22:56:17Z",
    "categories": [
      "cs.CV",
      "cs.ET"
    ],
    "abstract": "While supervised learning has achieved significant success in computer vision tasks, acquiring high-quality annotated data remains a bottleneck. This paper explores both scholarly and non-scholarly works in AI-assistive deep learning image annotation systems that provide textual suggestions, captions, or descriptions of the input image to the annotator. This potentially results in higher annotation efficiency and quality. Our exploration covers annotation for a range of computer vision tasks inc",
    "arxiv_url": "https://arxiv.org/abs/2407.00252v1",
    "pdf_url": "https://arxiv.org/pdf/2407.00252v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2407.00252",
    "arxiv_authors": [
      "Moseli Mots'oehli"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Assistive+Image+Annotation+Systems+with+Deep+Learning+and+Natural+Language+Capabilities%3A+A+Review+Moseli+Mots%27oehli",
    "gs_search_success": true,
    "gs_authors": [
      "kxa1cKgAAAAJ"
    ],
    "citation_count": 5,
    "gs_author_count": 1
  },
  {
    "arxiv_id": "2312.01886",
    "title": "InstructTA: Instruction-Tuned Targeted Attack for Large Vision-Language Models",
    "year": 2023,
    "published": "2023-12-04T13:40:05Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Large vision-language models (LVLMs) have demonstrated their incredible capability in image understanding and response generation. However, this rich visual interaction also makes LVLMs vulnerable to adversarial examples. In this paper, we formulate a novel and practical targeted attack scenario that the adversary can only know the vision encoder of the victim LVLM, without the knowledge of its prompts (which are often proprietary for service providers and not publicly available) and its underly",
    "arxiv_url": "https://arxiv.org/abs/2312.01886v3",
    "pdf_url": "https://arxiv.org/pdf/2312.01886v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2312.01886",
    "arxiv_authors": [
      "Xunguang Wang",
      "Zhenlan Ji",
      "Pingchuan Ma",
      "Zongjie Li",
      "Shuai Wang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=InstructTA%3A+Instruction-Tuned+Targeted+Attack+for+Large+Vision-Language+Models+Xunguang+Wang+Zhenlan+Ji+Pingchuan+Ma+Zongjie+Li+Shuai+Wang",
    "gs_search_success": true,
    "gs_authors": [
      "POlWWAsAAAAJ",
      "Sd8mmE0AAAAJ",
      "-VQxD1UAAAAJ",
      "xB42z10AAAAJ",
      "KNdj9HMAAAAJ"
    ],
    "citation_count": 33,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2303.03729",
    "title": "Learning Discriminative Representations for Skeleton Based Action Recognition",
    "year": 2023,
    "published": "2023-03-07T08:37:48Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Human action recognition aims at classifying the category of human action from a segment of a video. Recently, people have dived into designing GCN-based models to extract features from skeletons for performing this task, because skeleton representations are much more efficient and robust than other modalities such as RGB frames. However, when employing the skeleton data, some important clues like related items are also discarded. It results in some ambiguous actions that are hard to be distingu",
    "arxiv_url": "https://arxiv.org/abs/2303.03729v3",
    "pdf_url": "https://arxiv.org/pdf/2303.03729v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2303.03729",
    "arxiv_authors": [
      "Huanyu Zhou",
      "Qingjie Liu",
      "Yunhong Wang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Learning+Discriminative+Representations+for+Skeleton+Based+Action+Recognition+Huanyu+Zhou+Qingjie+Liu+Yunhong+Wang",
    "gs_search_success": true,
    "gs_authors": [
      "0ez7lA0AAAAJ",
      "HsLdRZYAAAAJ",
      "yaozss4AAAAJ"
    ],
    "citation_count": 213,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2308.04892",
    "title": "Transmission and Color-guided Network for Underwater Image Enhancement",
    "year": 2023,
    "published": "2023-08-09T11:43:54Z",
    "categories": [
      "cs.CV",
      "eess.IV"
    ],
    "abstract": "In recent years, with the continuous development of the marine industry, underwater image enhancement has attracted plenty of attention. Unfortunately, the propagation of light in water will be absorbed by water bodies and scattered by suspended particles, resulting in color deviation and low contrast. To solve these two problems, we propose an Adaptive Transmission and Dynamic Color guided network (named ATDCnet) for underwater image enhancement. In particular, to exploit the knowledge of physi",
    "arxiv_url": "https://arxiv.org/abs/2308.04892v1",
    "pdf_url": "https://arxiv.org/pdf/2308.04892v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2308.04892",
    "arxiv_authors": [
      "Pan Mu",
      "Jing Fang",
      "Haotian Qian",
      "Cong Bai"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Transmission+and+Color-guided+Network+for+Underwater+Image+Enhancement+Pan+Mu+Jing+Fang+Haotian+Qian+Cong+Bai",
    "gs_search_success": true,
    "gs_authors": [
      "XGZ4UZgAAAAJ",
      "fx_vH9wAAAAJ"
    ],
    "citation_count": 9,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2408.12282",
    "title": "Subsurface Scattering for 3D Gaussian Splatting",
    "year": 2024,
    "published": "2024-08-22T10:34:01Z",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "abstract": "3D reconstruction and relighting of objects made from scattering materials present a significant challenge due to the complex light transport beneath the surface. 3D Gaussian Splatting introduced high-quality novel view synthesis at real-time speeds. While 3D Gaussians efficiently approximate an object's surface, they fail to capture the volumetric properties of subsurface scattering. We propose a framework for optimizing an object's shape together with the radiance transfer field given multi-vi",
    "arxiv_url": "https://arxiv.org/abs/2408.12282v2",
    "pdf_url": "https://arxiv.org/pdf/2408.12282v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2408.12282",
    "arxiv_authors": [
      "Jan-Niklas Dihlmann",
      "Arjun Majumdar",
      "Andreas Engelhardt",
      "Raphael Braun",
      "Hendrik P. A. Lensch"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Subsurface+Scattering+for+3D+Gaussian+Splatting+Jan-Niklas+Dihlmann+Arjun+Majumdar+Andreas+Engelhardt+Raphael+Braun+Hendrik+P.+A.+Lensch",
    "gs_search_success": true,
    "gs_authors": [
      "eh2ePAUAAAAJ",
      "ZQUFcqAAAAAJ",
      "2R22h84AAAAJ",
      "Sp289eUAAAAJ"
    ],
    "citation_count": 8,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2305.00204",
    "title": "CARLA-BSP: a simulated dataset with pedestrians",
    "year": 2023,
    "published": "2023-04-29T09:10:32Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "We present a sample dataset featuring pedestrians generated using the ARCANE framework, a new framework for generating datasets in CARLA (0.9.13). We provide use cases for pedestrian detection, autoencoding, pose estimation, and pose lifting. We also showcase baseline results. For more information, visit https://project-arcane.eu/.",
    "arxiv_url": "https://arxiv.org/abs/2305.00204v1",
    "pdf_url": "https://arxiv.org/pdf/2305.00204v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2305.00204",
    "arxiv_authors": [
      "Maciej Wielgosz",
      "Antonio M. López",
      "Muhammad Naveed Riaz"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=CARLA-BSP%3A+a+simulated+dataset+with+pedestrians+Maciej+Wielgosz+Antonio+M.+L%C3%B3pez+Muhammad+Naveed+Riaz",
    "gs_search_success": true,
    "gs_authors": [
      "j563WT8AAAAJ",
      "3v_rzNYAAAAJ",
      "3LYW1zMAAAAJ"
    ],
    "citation_count": 2,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2411.04493",
    "title": "Synergy-Guided Regional Supervision of Pseudo Labels for Semi-Supervised Medical Image Segmentation",
    "year": 2024,
    "published": "2024-11-07T07:41:04Z",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "Semi-supervised learning has received considerable attention for its potential to leverage abundant unlabeled data to enhance model robustness. Pseudo labeling is a widely used strategy in semi supervised learning. However, existing methods often suffer from noise contamination, which can undermine model performance. To tackle this challenge, we introduce a novel Synergy-Guided Regional Supervision of Pseudo Labels (SGRS-Net) framework. Built upon the mean teacher network, we employ a Mix Augmen",
    "arxiv_url": "https://arxiv.org/abs/2411.04493v2",
    "pdf_url": "https://arxiv.org/pdf/2411.04493v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2411.04493",
    "arxiv_authors": [
      "Tao Wang",
      "Xinlin Zhang",
      "Yuanbin Chen",
      "Yuanbo Zhou",
      "Longxuan Zhao",
      "Tao Tan",
      "Tong Tong"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Synergy-Guided+Regional+Supervision+of+Pseudo+Labels+for+Semi-Supervised+Medical+Image+Segmentation+Tao+Wang+Xinlin+Zhang+Yuanbin+Chen+Yuanbo+Zhou+Longxuan+Zhao",
    "gs_search_success": true,
    "gs_authors": [
      "ZvgEfjEAAAAJ",
      "zgFZ93sAAAAJ",
      "hXK6E00AAAAJ",
      "vdNg9q0AAAAJ",
      "lLg3WRkAAAAJ",
      "De0cZdQAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 7
  },
  {
    "arxiv_id": "2302.09516",
    "title": "A Bibliography of Multiple Sclerosis Lesions Detection Methods using Brain MRIs",
    "year": 2023,
    "published": "2023-02-19T09:26:19Z",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "abstract": "Introduction: Multiple Sclerosis (MS) is a chronic disease that affects millions of people across the globe. MS can critically affect different organs of the central nervous system such as the eyes, the spinal cord, and the brain.   Background: To help physicians in diagnosing MS lesions, computer-aided methods are widely used. In this regard, a considerable research has been carried out in the area of automatic detection and segmentation of MS lesions in magnetic resonance images (MRIs).   Meth",
    "arxiv_url": "https://arxiv.org/abs/2302.09516v1",
    "pdf_url": "https://arxiv.org/pdf/2302.09516v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2302.09516",
    "arxiv_authors": [
      "Atif Shah",
      "Maged S. Al-Shaibani",
      "Moataz Ahmad",
      "Reem Bunyan"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=A+Bibliography+of+Multiple+Sclerosis+Lesions+Detection+Methods+using+Brain+MRIs+Atif+Shah+Maged+S.+Al-Shaibani+Moataz+Ahmad+Reem+Bunyan",
    "gs_search_success": true,
    "gs_authors": [
      "Uo91Tz0AAAAJ",
      "nkjzKEEAAAAJ",
      "Yx2mkN4AAAAJ"
    ],
    "citation_count": 6,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2307.12241",
    "title": "Explainable Depression Detection via Head Motion Patterns",
    "year": 2023,
    "published": "2023-07-23T06:39:51Z",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "While depression has been studied via multimodal non-verbal behavioural cues, head motion behaviour has not received much attention as a biomarker. This study demonstrates the utility of fundamental head-motion units, termed \\emph{kinemes}, for depression detection by adopting two distinct approaches, and employing distinctive features: (a) discovering kinemes from head motion data corresponding to both depressed patients and healthy controls, and (b) learning kineme patterns only from healthy c",
    "arxiv_url": "https://arxiv.org/abs/2307.12241v1",
    "pdf_url": "https://arxiv.org/pdf/2307.12241v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2307.12241",
    "arxiv_authors": [
      "Monika Gahalawat",
      "Raul Fernandez Rojas",
      "Tanaya Guha",
      "Ramanathan Subramanian",
      "Roland Goecke"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Explainable+Depression+Detection+via+Head+Motion+Patterns+Monika+Gahalawat+Raul+Fernandez+Rojas+Tanaya+Guha+Ramanathan+Subramanian+Roland+Goecke",
    "gs_search_success": true,
    "gs_authors": [
      "mUvcmRsAAAAJ",
      "xfYTKhAAAAAJ",
      "GljtiSUAAAAJ",
      "p-BUHOcAAAAJ",
      "Eg1VknAAAAAJ"
    ],
    "citation_count": 13,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2307.13720",
    "title": "Composite Diffusion | whole >= Σparts",
    "year": 2023,
    "published": "2023-07-25T17:58:43Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR",
      "cs.HC"
    ],
    "abstract": "For an artist or a graphic designer, the spatial layout of a scene is a critical design choice. However, existing text-to-image diffusion models provide limited support for incorporating spatial information. This paper introduces Composite Diffusion as a means for artists to generate high-quality images by composing from the sub-scenes. The artists can specify the arrangement of these sub-scenes through a flexible free-form segment layout. They can describe the content of each sub-scene primaril",
    "arxiv_url": "https://arxiv.org/abs/2307.13720v1",
    "pdf_url": "https://arxiv.org/pdf/2307.13720v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2307.13720",
    "arxiv_authors": [
      "Vikram Jamwal",
      "Ramaneswaran S"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Composite+Diffusion+%7C+whole+%3E%3D+%CE%A3parts+Vikram+Jamwal+Ramaneswaran+S",
    "gs_search_success": true,
    "gs_authors": [
      "LJzZEXUAAAAJ",
      "ug_alyQAAAAJ",
      "Gsw2iUEAAAAJ",
      "YIhHxbwAAAAJ",
      "JtjoOmEAAAAJ",
      "p3-nnLQAAAAJ",
      "9l_WJCsAAAAJ",
      "pF5p7_IAAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2409.11785",
    "title": "Distilling Channels for Efficient Deep Tracking",
    "year": 2024,
    "published": "2024-09-18T08:09:20Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Deep trackers have proven success in visual tracking. Typically, these trackers employ optimally pre-trained deep networks to represent all diverse objects with multi-channel features from some fixed layers. The deep networks employed are usually trained to extract rich knowledge from massive data used in object classification and so they are capable to represent generic objects very well. However, these networks are too complex to represent a specific moving object, leading to poor generalizati",
    "arxiv_url": "https://arxiv.org/abs/2409.11785v1",
    "pdf_url": "https://arxiv.org/pdf/2409.11785v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2409.11785",
    "arxiv_authors": [
      "Shiming Ge",
      "Zhao Luo",
      "Chunhui Zhang",
      "Yingying Hua",
      "Dacheng Tao"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Distilling+Channels+for+Efficient+Deep+Tracking+Shiming+Ge+Zhao+Luo+Chunhui+Zhang+Yingying+Hua+Dacheng+Tao",
    "gs_search_success": true,
    "gs_authors": [
      "RwlJNLcAAAAJ",
      "D_Isj3EAAAAJ",
      "T0p8ka0AAAAJ"
    ],
    "citation_count": 45,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2409.08143",
    "title": "Effective Segmentation of Post-Treatment Gliomas Using Simple Approaches: Artificial Sequence Generation and Ensemble Models",
    "year": 2024,
    "published": "2024-09-12T15:34:31Z",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "abstract": "Segmentation is a crucial task in the medical imaging field and is often an important primary step or even a prerequisite to the analysis of medical volumes. Yet treatments such as surgery complicate the accurate delineation of regions of interest. The BraTS Post-Treatment 2024 Challenge published the first public dataset for post-surgery glioma segmentation and addresses the aforementioned issue by fostering the development of automated segmentation tools for glioma in MRI data. In this effort,",
    "arxiv_url": "https://arxiv.org/abs/2409.08143v1",
    "pdf_url": "https://arxiv.org/pdf/2409.08143v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2409.08143",
    "arxiv_authors": [
      "Heejong Kim",
      "Leo Milecki",
      "Mina C Moghadam",
      "Fengbei Liu",
      "Minh Nguyen",
      "Eric Qiu",
      "Abhishek Thanki",
      "Mert R Sabuncu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Effective+Segmentation+of+Post-Treatment+Gliomas+Using+Simple+Approaches%3A+Artificial+Sequence+Generation+and+Ensemble+Models+Heejong+Kim+Leo+Milecki+Mina+C+Moghadam+Fengbei+Liu+Minh+Nguyen",
    "gs_search_success": true,
    "gs_authors": [
      "R8ko8mYAAAAJ",
      "bNzooxgAAAAJ",
      "oY_qRxMAAAAJ",
      "D3lZyGIAAAAJ",
      "PcuAQykAAAAJ",
      "AKkrECMAAAAJ",
      "Pig-I4QAAAAJ"
    ],
    "citation_count": 3,
    "gs_author_count": 8
  },
  {
    "arxiv_id": "2402.07059",
    "title": "Domain Adaptable Fine-Tune Distillation Framework For Advancing Farm Surveillance",
    "year": 2024,
    "published": "2024-02-10T22:20:37Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "In this study, we propose an automated framework for camel farm monitoring, introducing two key contributions: the Unified Auto-Annotation framework and the Fine-Tune Distillation framework. The Unified Auto-Annotation approach combines two models, GroundingDINO (GD), and Segment-Anything-Model (SAM), to automatically annotate raw datasets extracted from surveillance videos. Building upon this foundation, the Fine-Tune Distillation framework conducts fine-tuning of student models using the auto-",
    "arxiv_url": "https://arxiv.org/abs/2402.07059v1",
    "pdf_url": "https://arxiv.org/pdf/2402.07059v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2402.07059",
    "arxiv_authors": [
      "Raza Imam",
      "Muhammad Huzaifa",
      "Nabil Mansour",
      "Shaher Bano Mirza",
      "Fouad Lamghari"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Domain+Adaptable+Fine-Tune+Distillation+Framework+For+Advancing+Farm+Surveillance+Raza+Imam+Muhammad+Huzaifa+Nabil+Mansour+Shaher+Bano+Mirza+Fouad+Lamghari",
    "gs_search_success": true,
    "gs_authors": [
      "4O2P1lIAAAAJ",
      "iipzwX0AAAAJ",
      "67ARFXQAAAAJ",
      "fDtoo50AAAAJ",
      "oWd2RCsAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2411.11921",
    "title": "DeSiRe-GS: 4D Street Gaussians for Static-Dynamic Decomposition and Surface Reconstruction for Urban Driving Scenes",
    "year": 2024,
    "published": "2024-11-18T05:49:16Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "We present DeSiRe-GS, a self-supervised gaussian splatting representation, enabling effective static-dynamic decomposition and high-fidelity surface reconstruction in complex driving scenarios. Our approach employs a two-stage optimization pipeline of dynamic street Gaussians. In the first stage, we extract 2D motion masks based on the observation that 3D Gaussian Splatting inherently can reconstruct only the static regions in dynamic environments. These extracted 2D motion priors are then mappe",
    "arxiv_url": "https://arxiv.org/abs/2411.11921v2",
    "pdf_url": "https://arxiv.org/pdf/2411.11921v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2411.11921",
    "arxiv_authors": [
      "Chensheng Peng",
      "Chengwei Zhang",
      "Yixiao Wang",
      "Chenfeng Xu",
      "Yichen Xie",
      "Wenzhao Zheng",
      "Kurt Keutzer",
      "Masayoshi Tomizuka",
      "Wei Zhan"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=DeSiRe-GS%3A+4D+Street+Gaussians+for+Static-Dynamic+Decomposition+and+Surface+Reconstruction+for+Urban+Driving+Scenes+Chensheng+Peng+Chengwei+Zhang+Yixiao+Wang+Chenfeng+Xu+Yichen+Xie",
    "gs_search_success": true,
    "gs_authors": [
      "RpqvaTUAAAAJ",
      "LdK9scgAAAAJ",
      "ID9QePIAAAAJ",
      "nUIFbw8AAAAJ",
      "DbZxclcAAAAJ",
      "8m8taGEAAAAJ",
      "d9ANIlkAAAAJ",
      "SdX6DaEAAAAJ",
      "xVN3UxYAAAAJ"
    ],
    "citation_count": 20,
    "gs_author_count": 9
  },
  {
    "arxiv_id": "2305.02299",
    "title": "Dynamic Sparse Training with Structured Sparsity",
    "year": 2023,
    "published": "2023-05-03T17:48:55Z",
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "abstract": "Dynamic Sparse Training (DST) methods achieve state-of-the-art results in sparse neural network training, matching the generalization of dense models while enabling sparse training and inference. Although the resulting models are highly sparse and theoretically less computationally expensive, achieving speedups with unstructured sparsity on real-world hardware is challenging. In this work, we propose a sparse-to-sparse DST method, Structured RigL (SRigL), to learn a variant of fine-grained struc",
    "arxiv_url": "https://arxiv.org/abs/2305.02299v4",
    "pdf_url": "https://arxiv.org/pdf/2305.02299v4",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2305.02299",
    "arxiv_authors": [
      "Mike Lasby",
      "Anna Golubeva",
      "Utku Evci",
      "Mihai Nica",
      "Yani Ioannou"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Dynamic+Sparse+Training+with+Structured+Sparsity+Mike+Lasby+Anna+Golubeva+Utku+Evci+Mihai+Nica+Yani+Ioannou",
    "gs_search_success": true,
    "gs_authors": [
      "R9L0aqAAAAAJ",
      "8yGMMwcAAAAJ",
      "4odTWZMAAAAJ",
      "6iwjqpIAAAAJ",
      "Qy9yv44AAAAJ"
    ],
    "citation_count": 38,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2304.01804",
    "title": "Bridging the Gap between Model Explanations in Partially Annotated Multi-label Classification",
    "year": 2023,
    "published": "2023-04-04T14:00:59Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Due to the expensive costs of collecting labels in multi-label classification datasets, partially annotated multi-label classification has become an emerging field in computer vision. One baseline approach to this task is to assume unobserved labels as negative labels, but this assumption induces label noise as a form of false negative. To understand the negative impact caused by false negative labels, we study how these labels affect the model's explanation. We observe that the explanation of t",
    "arxiv_url": "https://arxiv.org/abs/2304.01804v1",
    "pdf_url": "https://arxiv.org/pdf/2304.01804v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2304.01804",
    "arxiv_authors": [
      "Youngwook Kim",
      "Jae Myung Kim",
      "Jieun Jeong",
      "Cordelia Schmid",
      "Zeynep Akata",
      "Jungwoo Lee"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Bridging+the+Gap+between+Model+Explanations+in+Partially+Annotated+Multi-label+Classification+Youngwook+Kim+Jae+Myung+Kim+Jieun+Jeong+Cordelia+Schmid+Zeynep+Akata",
    "gs_search_success": true,
    "gs_authors": [
      "eP6FHFAAAAAJ",
      "RWV9I_IAAAAJ",
      "jQl9RtkAAAAJ",
      "IvqCXP4AAAAJ"
    ],
    "citation_count": 41,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2412.13299",
    "title": "In-context learning for medical image segmentation",
    "year": 2024,
    "published": "2024-12-17T19:59:08Z",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "abstract": "Annotation of medical images, such as MRI and CT scans, is crucial for evaluating treatment efficacy and planning radiotherapy. However, the extensive workload of medical professionals limits their ability to annotate large image datasets, posing a bottleneck for AI applications in medical imaging. To address this, we propose In-context Cascade Segmentation (ICS), a novel method that minimizes annotation requirements while achieving high segmentation accuracy for sequential medical images. ICS b",
    "arxiv_url": "https://arxiv.org/abs/2412.13299v2",
    "pdf_url": "https://arxiv.org/pdf/2412.13299v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2412.13299",
    "arxiv_authors": [
      "Eichi Takaya",
      "Shinnosuke Yamamoto"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=In-context+learning+for+medical+image+segmentation+Eichi+Takaya+Shinnosuke+Yamamoto",
    "gs_search_success": true,
    "gs_authors": [
      "mDXxTtMAAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 2
  },
  {
    "arxiv_id": "2402.01171",
    "title": "AmbientCycleGAN for Establishing Interpretable Stochastic Object Models Based on Mathematical Phantoms and Medical Imaging Measurements",
    "year": 2024,
    "published": "2024-02-02T06:30:33Z",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "abstract": "Medical imaging systems that are designed for producing diagnostically informative images should be objectively assessed via task-based measures of image quality (IQ). Ideally, computation of task-based measures of IQ needs to account for all sources of randomness in the measurement data, including the variability in the ensemble of objects to be imaged. To address this need, stochastic object models (SOMs) that can generate an ensemble of synthesized objects or phantoms can be employed. Various",
    "arxiv_url": "https://arxiv.org/abs/2402.01171v1",
    "pdf_url": "https://arxiv.org/pdf/2402.01171v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2402.01171",
    "arxiv_authors": [
      "Xichen Xu",
      "Wentao Chen",
      "Weimin Zhou"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=AmbientCycleGAN+for+Establishing+Interpretable+Stochastic+Object+Models+Based+on+Mathematical+Phantoms+and+Medical+Imaging+Measurements+Xichen+Xu+Wentao+Chen+Weimin+Zhou",
    "gs_search_success": true,
    "gs_authors": [
      "FYs57jwAAAAJ",
      "kUswDmoAAAAJ"
    ],
    "citation_count": 2,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2308.00956",
    "title": "Curriculum Guided Domain Adaptation in the Dark",
    "year": 2023,
    "published": "2023-08-02T05:47:56Z",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "Addressing the rising concerns of privacy and security, domain adaptation in the dark aims to adapt a black-box source trained model to an unlabeled target domain without access to any source data or source model parameters. The need for domain adaptation of black-box predictors becomes even more pronounced to protect intellectual property as deep learning based solutions are becoming increasingly commercialized. Current methods distill noisy predictions on the target data obtained from the sour",
    "arxiv_url": "https://arxiv.org/abs/2308.00956v1",
    "pdf_url": "https://arxiv.org/pdf/2308.00956v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2308.00956",
    "arxiv_authors": [
      "Chowdhury Sadman Jahan",
      "Andreas Savakis"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Curriculum+Guided+Domain+Adaptation+in+the+Dark+Chowdhury+Sadman+Jahan+Andreas+Savakis",
    "gs_search_success": true,
    "gs_authors": [
      "ilA2UDwAAAAJ",
      "pqKohsIAAAAJ"
    ],
    "citation_count": 5,
    "gs_author_count": null
  },
  {
    "arxiv_id": "2502.17939",
    "title": "Deep-JGAC: End-to-End Deep Joint Geometry and Attribute Compression for Dense Colored Point Clouds",
    "year": 2025,
    "published": "2025-02-25T08:01:57Z",
    "categories": [
      "cs.MM",
      "cs.CV"
    ],
    "abstract": "Colored point cloud becomes a fundamental representation in the realm of 3D vision. Effective Point Cloud Compression (PCC) is urgently needed due to huge amount of data. In this paper, we propose an end-to-end Deep Joint Geometry and Attribute point cloud Compression (Deep-JGAC) framework for dense colored point clouds, which exploits the correlation between the geometry and attribute for high compression efficiency. Firstly, we propose a flexible Deep-JGAC framework, where the geometry and att",
    "arxiv_url": "https://arxiv.org/abs/2502.17939v1",
    "pdf_url": "https://arxiv.org/pdf/2502.17939v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2502.17939",
    "arxiv_authors": [
      "Yun Zhang",
      "Zixi Guo",
      "Linwei Zhu",
      "C. -C. Jay Kuo"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Deep-JGAC%3A+End-to-End+Deep+Joint+Geometry+and+Attribute+Compression+for+Dense+Colored+Point+Clouds+Yun+Zhang+Zixi+Guo+Linwei+Zhu+C.+-C.+Jay+Kuo",
    "gs_search_success": true,
    "gs_authors": [
      "81d60okAAAAJ",
      "tZp-uVoAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2308.14492",
    "title": "PointHPS: Cascaded 3D Human Pose and Shape Estimation from Point Clouds",
    "year": 2023,
    "published": "2023-08-28T11:10:14Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Human pose and shape estimation (HPS) has attracted increasing attention in recent years. While most existing studies focus on HPS from 2D images or videos with inherent depth ambiguity, there are surging need to investigate HPS from 3D point clouds as depth sensors have been frequently employed in commercial devices. However, real-world sensory 3D points are usually noisy and incomplete, and also human bodies could have different poses of high diversity. To tackle these challenges, we propose a",
    "arxiv_url": "https://arxiv.org/abs/2308.14492v1",
    "pdf_url": "https://arxiv.org/pdf/2308.14492v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2308.14492",
    "arxiv_authors": [
      "Zhongang Cai",
      "Liang Pan",
      "Chen Wei",
      "Wanqi Yin",
      "Fangzhou Hong",
      "Mingyuan Zhang",
      "Chen Change Loy",
      "Lei Yang",
      "Ziwei Liu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=PointHPS%3A+Cascaded+3D+Human+Pose+and+Shape+Estimation+from+Point+Clouds+Zhongang+Cai+Liang+Pan+Chen+Wei+Wanqi+Yin+Fangzhou+Hong",
    "gs_search_success": true,
    "gs_authors": [
      "zlIJwBEAAAAJ",
      "mhaiL5MAAAAJ",
      "lc45xlcAAAAJ",
      "2QLD4fAAAAAJ",
      "559LF80AAAAJ",
      "jZH2IPYAAAAJ",
      "WrDKqIAAAAAJ",
      "lSDISOcAAAAJ"
    ],
    "citation_count": 12,
    "gs_author_count": 9
  },
  {
    "arxiv_id": "2503.21779",
    "title": "X$^{2}$-Gaussian: 4D Radiative Gaussian Splatting for Continuous-time Tomographic Reconstruction",
    "year": 2025,
    "published": "2025-03-27T17:59:57Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Four-dimensional computed tomography (4D CT) reconstruction is crucial for capturing dynamic anatomical changes but faces inherent limitations from conventional phase-binning workflows. Current methods discretize temporal resolution into fixed phases with respiratory gating devices, introducing motion misalignment and restricting clinical practicality. In this paper, We propose X$^2$-Gaussian, a novel framework that enables continuous-time 4D-CT reconstruction by integrating dynamic radiative Ga",
    "arxiv_url": "https://arxiv.org/abs/2503.21779v2",
    "pdf_url": "https://arxiv.org/pdf/2503.21779v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2503.21779",
    "arxiv_authors": [
      "Weihao Yu",
      "Yuanhao Cai",
      "Ruyi Zha",
      "Zhiwen Fan",
      "Chenxin Li",
      "Yixuan Yuan"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=X%24%5E%7B2%7D%24-Gaussian%3A+4D+Radiative+Gaussian+Splatting+for+Continuous-time+Tomographic+Reconstruction+Weihao+Yu+Yuanhao+Cai+Ruyi+Zha+Zhiwen+Fan+Chenxin+Li",
    "gs_search_success": true,
    "gs_authors": [
      "fCzlLE4AAAAJ",
      "3YozQwcAAAAJ",
      "tdoBO3UAAAAJ",
      "_5W6W7oAAAAJ",
      "yfptgYMAAAAJ",
      "Aho5Jv8AAAAJ"
    ],
    "citation_count": 3,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2302.11795",
    "title": "Bridging Synthetic and Real Images: a Transferable and Multiple Consistency aided Fundus Image Enhancement Framework",
    "year": 2023,
    "published": "2023-02-23T06:16:15Z",
    "categories": [
      "eess.IV",
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "Deep learning based image enhancement models have largely improved the readability of fundus images in order to decrease the uncertainty of clinical observations and the risk of misdiagnosis. However, due to the difficulty of acquiring paired real fundus images at different qualities, most existing methods have to adopt synthetic image pairs as training data. The domain shift between the synthetic and the real images inevitably hinders the generalization of such models on clinical data. In this ",
    "arxiv_url": "https://arxiv.org/abs/2302.11795v1",
    "pdf_url": "https://arxiv.org/pdf/2302.11795v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2302.11795",
    "arxiv_authors": [
      "Erjian Guo",
      "Huazhu Fu",
      "Luping Zhou",
      "Dong Xu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Bridging+Synthetic+and+Real+Images%3A+a+Transferable+and+Multiple+Consistency+aided+Fundus+Image+Enhancement+Framework+Erjian+Guo+Huazhu+Fu+Luping+Zhou+Dong+Xu",
    "gs_search_success": true,
    "gs_authors": [
      "IO633wEAAAAJ",
      "SgofT2MAAAAJ",
      "7Hdu5k4AAAAJ",
      "jCvUBYMAAAAJ"
    ],
    "citation_count": 28,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2406.12424",
    "title": "Recognition of Dynamic Hand Gestures in Long Distance using a Web-Camera for Robot Guidance",
    "year": 2024,
    "published": "2024-06-18T09:17:28Z",
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "abstract": "Dynamic gestures enable the transfer of directive information to a robot. Moreover, the ability of a robot to recognize them from a long distance makes communication more effective and practical. However, current state-of-the-art models for dynamic gestures exhibit limitations in recognition distance, typically achieving effective performance only within a few meters. In this work, we propose a model for recognizing dynamic gestures from a long distance of up to 20 meters. The model integrates t",
    "arxiv_url": "https://arxiv.org/abs/2406.12424v1",
    "pdf_url": "https://arxiv.org/pdf/2406.12424v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2406.12424",
    "arxiv_authors": [
      "Eran Bamani Beeri",
      "Eden Nissinman",
      "Avishai Sintov"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Recognition+of+Dynamic+Hand+Gestures+in+Long+Distance+using+a+Web-Camera+for+Robot+Guidance+Eran+Bamani+Beeri+Eden+Nissinman+Avishai+Sintov",
    "gs_search_success": true,
    "gs_authors": [
      "T4t6T14AAAAJ",
      "l0y02JwAAAAJ"
    ],
    "citation_count": 2,
    "gs_author_count": null
  },
  {
    "arxiv_id": "2401.05964",
    "title": "An attempt to generate new bridge types from latent space of PixelCNN",
    "year": 2024,
    "published": "2024-01-11T15:06:25Z",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "abstract": "Try to generate new bridge types using generative artificial intelligence technology. Using symmetric structured image dataset of three-span beam bridge, arch bridge, cable-stayed bridge and suspension bridge , based on Python programming language, TensorFlow and Keras deep learning platform framework , PixelCNN is constructed and trained. The model can capture the statistical structure of the images and calculate the probability distribution of the next pixel when the previous pixels are given.",
    "arxiv_url": "https://arxiv.org/abs/2401.05964v1",
    "pdf_url": "https://arxiv.org/pdf/2401.05964v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2401.05964",
    "arxiv_authors": [
      "Hongjun Zhang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=An+attempt+to+generate+new+bridge+types+from+latent+space+of+PixelCNN+Hongjun+Zhang",
    "gs_search_success": true,
    "gs_authors": [],
    "citation_count": 4,
    "gs_author_count": 1
  },
  {
    "arxiv_id": "2305.05899",
    "title": "Mobile Image Restoration via Prior Quantization",
    "year": 2023,
    "published": "2023-05-10T05:05:58Z",
    "categories": [
      "cs.CV",
      "cs.MM",
      "eess.IV"
    ],
    "abstract": "In digital images, the performance of optical aberration is a multivariate degradation, where the spectral of the scene, the lens imperfections, and the field of view together contribute to the results. Besides eliminating it at the hardware level, the post-processing system, which utilizes various prior information, is significant for correction. However, due to the content differences among priors, the pipeline that aligns these factors shows limited efficiency and unoptimized restoration. Her",
    "arxiv_url": "https://arxiv.org/abs/2305.05899v1",
    "pdf_url": "https://arxiv.org/pdf/2305.05899v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2305.05899",
    "arxiv_authors": [
      "Shiqi Chen",
      "Jinwen Zhou",
      "Menghao Li",
      "Yueting Chen",
      "Tingting Jiang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Mobile+Image+Restoration+via+Prior+Quantization+Shiqi+Chen+Jinwen+Zhou+Menghao+Li+Yueting+Chen+Tingting+Jiang",
    "gs_search_success": true,
    "gs_authors": [
      "gS-0tfAAAAAJ",
      "gJCsz90AAAAJ",
      "s9HMveQAAAAJ"
    ],
    "citation_count": 13,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2403.07535",
    "title": "Adaptive Fusion of Single-View and Multi-View Depth for Autonomous Driving",
    "year": 2024,
    "published": "2024-03-12T11:18:35Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Multi-view depth estimation has achieved impressive performance over various benchmarks. However, almost all current multi-view systems rely on given ideal camera poses, which are unavailable in many real-world scenarios, such as autonomous driving. In this work, we propose a new robustness benchmark to evaluate the depth estimation system under various noisy pose settings. Surprisingly, we find current multi-view depth estimation methods or single-view and multi-view fusion methods will fail wh",
    "arxiv_url": "https://arxiv.org/abs/2403.07535v1",
    "pdf_url": "https://arxiv.org/pdf/2403.07535v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2403.07535",
    "arxiv_authors": [
      "JunDa Cheng",
      "Wei Yin",
      "Kaixuan Wang",
      "Xiaozhi Chen",
      "Shijie Wang",
      "Xin Yang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Adaptive+Fusion+of+Single-View+and+Multi-View+Depth+for+Autonomous+Driving+JunDa+Cheng+Wei+Yin+Kaixuan+Wang+Xiaozhi+Chen+Shijie+Wang",
    "gs_search_success": true,
    "gs_authors": [
      "DuAqyTwAAAAJ",
      "mSyd3BcAAAAJ",
      "_G_Tu9EAAAAJ",
      "FrQLlCYAAAAJ",
      "ZIf_rtcAAAAJ"
    ],
    "citation_count": 39,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2403.09939",
    "title": "Quantization Effects on Neural Networks Perception: How would quantization change the perceptual field of vision models?",
    "year": 2024,
    "published": "2024-03-15T00:43:03Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Neural network quantization is a critical technique for deploying models on resource-limited devices. Despite its widespread use, the impact of quantization on model perceptual fields, particularly in relation to class activation maps (CAMs), remains underexplored. This study investigates how quantization influences the spatial recognition abilities of vision models by examining the alignment between CAMs and visual salient objects maps across various architectures. Utilizing a dataset of 10,000",
    "arxiv_url": "https://arxiv.org/abs/2403.09939v2",
    "pdf_url": "https://arxiv.org/pdf/2403.09939v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2403.09939",
    "arxiv_authors": [
      "Mohamed Amine Kerkouri",
      "Marouane Tliba",
      "Aladine Chetouani",
      "Alessandro Bruno"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Quantization+Effects+on+Neural+Networks+Perception%3A+How+would+quantization+change+the+perceptual+field+of+vision+models%3F+Mohamed+Amine+Kerkouri+Marouane+Tliba+Aladine+Chetouani+Alessandro+Bruno",
    "gs_search_success": true,
    "gs_authors": [
      "kYKn0HgAAAAJ",
      "vpWieeAAAAAJ",
      "goUbn88AAAAJ"
    ],
    "citation_count": 3,
    "gs_author_count": null
  },
  {
    "arxiv_id": "2505.17581",
    "title": "MODEM: A Morton-Order Degradation Estimation Mechanism for Adverse Weather Image Recovery",
    "year": 2025,
    "published": "2025-05-23T07:43:14Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Restoring images degraded by adverse weather remains a significant challenge due to the highly non-uniform and spatially heterogeneous nature of weather-induced artifacts, e.g., fine-grained rain streaks versus widespread haze. Accurately estimating the underlying degradation can intuitively provide restoration models with more targeted and effective guidance, enabling adaptive processing strategies. To this end, we propose a Morton-Order Degradation Estimation Mechanism (MODEM) for adverse weat",
    "arxiv_url": "https://arxiv.org/abs/2505.17581v2",
    "pdf_url": "https://arxiv.org/pdf/2505.17581v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2505.17581",
    "arxiv_authors": [
      "Hainuo Wang",
      "Qiming Hu",
      "Xiaojie Guo"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=MODEM%3A+A+Morton-Order+Degradation+Estimation+Mechanism+for+Adverse+Weather+Image+Recovery+Hainuo+Wang+Qiming+Hu+Xiaojie+Guo",
    "gs_search_success": true,
    "gs_authors": [
      "Z2RcbF4AAAAJ",
      "RL7jPuQAAAAJ",
      "4zasPbwAAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2409.16143",
    "title": "Seeing Faces in Things: A Model and Dataset for Pareidolia",
    "year": 2024,
    "published": "2024-09-24T14:50:21Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.HC",
      "cs.IR",
      "cs.LG"
    ],
    "abstract": "The human visual system is well-tuned to detect faces of all shapes and sizes. While this brings obvious survival advantages, such as a better chance of spotting unknown predators in the bush, it also leads to spurious face detections. ``Face pareidolia'' describes the perception of face-like structure among otherwise random stimuli: seeing faces in coffee stains or clouds in the sky. In this paper, we study face pareidolia from a computer vision perspective. We present an image dataset of ``Fac",
    "arxiv_url": "https://arxiv.org/abs/2409.16143v1",
    "pdf_url": "https://arxiv.org/pdf/2409.16143v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2409.16143",
    "arxiv_authors": [
      "Mark Hamilton",
      "Simon Stent",
      "Vasha DuTell",
      "Anne Harrington",
      "Jennifer Corbett",
      "Ruth Rosenholtz",
      "William T. Freeman"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Seeing+Faces+in+Things%3A+A+Model+and+Dataset+for+Pareidolia+Mark+Hamilton+Simon+Stent+Vasha+DuTell+Anne+Harrington+Jennifer+Corbett",
    "gs_search_success": true,
    "gs_authors": [
      "BfE3-m0AAAAJ",
      "tvQjbgYAAAAJ",
      "7M9eSFMAAAAJ",
      "f3aij5UAAAAJ",
      "0zZnyMEAAAAJ",
      "kgZtMGsAAAAJ"
    ],
    "citation_count": 9,
    "gs_author_count": 7
  },
  {
    "arxiv_id": "2410.15615",
    "title": "Joint Top-Down and Bottom-Up Frameworks for 3D Visual Grounding",
    "year": 2024,
    "published": "2024-10-21T03:33:13Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "This paper tackles the challenging task of 3D visual grounding-locating a specific object in a 3D point cloud scene based on text descriptions. Existing methods fall into two categories: top-down and bottom-up methods. Top-down methods rely on a pre-trained 3D detector to generate and select the best bounding box, resulting in time-consuming processes. Bottom-up methods directly regress object bounding boxes with coarse-grained features, producing worse results. To combine their strengths while ",
    "arxiv_url": "https://arxiv.org/abs/2410.15615v1",
    "pdf_url": "https://arxiv.org/pdf/2410.15615v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2410.15615",
    "arxiv_authors": [
      "Yang Liu",
      "Daizong Liu",
      "Wei Hu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Joint+Top-Down+and+Bottom-Up+Frameworks+for+3D+Visual+Grounding+Yang+Liu+Daizong+Liu+Wei+Hu",
    "gs_search_success": true,
    "gs_authors": [
      "lUw7tVIAAAAJ",
      "5oFf8Q4AAAAJ"
    ],
    "citation_count": 7,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2402.17758",
    "title": "ADL4D: Towards A Contextually Rich Dataset for 4D Activities of Daily Living",
    "year": 2024,
    "published": "2024-02-27T18:51:52Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Hand-Object Interactions (HOIs) are conditioned on spatial and temporal contexts like surrounding objects, previous actions, and future intents (for example, grasping and handover actions vary greatly based on objects proximity and trajectory obstruction). However, existing datasets for 4D HOI (3D HOI over time) are limited to one subject interacting with one object only. This restricts the generalization of learning-based HOI methods trained on those datasets. We introduce ADL4D, a dataset of u",
    "arxiv_url": "https://arxiv.org/abs/2402.17758v1",
    "pdf_url": "https://arxiv.org/pdf/2402.17758v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2402.17758",
    "arxiv_authors": [
      "Marsil Zakour",
      "Partha Pratim Nath",
      "Ludwig Lohmer",
      "Emre Faik Gökçe",
      "Martin Piccolrovazzi",
      "Constantin Patsch",
      "Yuankai Wu",
      "Rahul Chaudhari",
      "Eckehard Steinbach"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=ADL4D%3A+Towards+A+Contextually+Rich+Dataset+for+4D+Activities+of+Daily+Living+Marsil+Zakour+Partha+Pratim+Nath+Ludwig+Lohmer+Emre+Faik+G%C3%B6k%C3%A7e+Martin+Piccolrovazzi",
    "gs_search_success": true,
    "gs_authors": [
      "18gEzFQAAAAJ",
      "qoSa_5gAAAAJ",
      "WC_55Y0AAAAJ",
      "YkXIFqsAAAAJ",
      "kO4cY9MAAAAJ"
    ],
    "citation_count": 4,
    "gs_author_count": 8
  },
  {
    "arxiv_id": "2402.14551",
    "title": "CLCE: An Approach to Refining Cross-Entropy and Contrastive Learning for Optimized Learning Fusion",
    "year": 2024,
    "published": "2024-02-22T13:45:01Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "abstract": "State-of-the-art pre-trained image models predominantly adopt a two-stage approach: initial unsupervised pre-training on large-scale datasets followed by task-specific fine-tuning using Cross-Entropy loss~(CE). However, it has been demonstrated that CE can compromise model generalization and stability. While recent works employing contrastive learning address some of these limitations by enhancing the quality of embeddings and producing better decision boundaries, they often overlook the importa",
    "arxiv_url": "https://arxiv.org/abs/2402.14551v2",
    "pdf_url": "https://arxiv.org/pdf/2402.14551v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2402.14551",
    "arxiv_authors": [
      "Zijun Long",
      "George Killick",
      "Lipeng Zhuang",
      "Gerardo Aragon-Camarasa",
      "Zaiqiao Meng",
      "Richard Mccreadie"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=CLCE%3A+An+Approach+to+Refining+Cross-Entropy+and+Contrastive+Learning+for+Optimized+Learning+Fusion+Zijun+Long+George+Killick+Lipeng+Zhuang+Gerardo+Aragon-Camarasa+Zaiqiao+Meng",
    "gs_search_success": true,
    "gs_authors": [
      "5jJKFVcAAAAJ",
      "FeBHPt0AAAAJ",
      "717cT8wAAAAJ",
      "lZQXAWwAAAAJ",
      "p8550tQAAAAJ",
      "mbSbOegAAAAJ"
    ],
    "citation_count": 7,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2304.04068",
    "title": "Word-level Persian Lipreading Dataset",
    "year": 2023,
    "published": "2023-04-08T17:00:35Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Lip-reading has made impressive progress in recent years, driven by advances in deep learning. Nonetheless, the prerequisite such advances is a suitable dataset. This paper provides a new in-the-wild dataset for Persian word-level lipreading containing 244,000 videos from approximately 1,800 speakers. We evaluated the state-of-the-art method in this field and used a novel approach for word-level lip-reading. In this method, we used the AV-HuBERT model for feature extraction and obtained signific",
    "arxiv_url": "https://arxiv.org/abs/2304.04068v1",
    "pdf_url": "https://arxiv.org/pdf/2304.04068v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2304.04068",
    "arxiv_authors": [
      "Javad Peymanfard",
      "Ali Lashini",
      "Samin Heydarian",
      "Hossein Zeinali",
      "Nasser Mozayani"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Word-level+Persian+Lipreading+Dataset+Javad+Peymanfard+Ali+Lashini+Samin+Heydarian+Hossein+Zeinali+Nasser+Mozayani",
    "gs_search_success": false,
    "gs_authors": [],
    "citation_count": null,
    "gs_author_count": null,
    "error_type": "fetch_failed"
  },
  {
    "arxiv_id": "2505.09943",
    "title": "CSPENet: Contour-Aware and Saliency Priors Embedding Network for Infrared Small Target Detection",
    "year": 2025,
    "published": "2025-05-15T03:56:36Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Infrared small target detection (ISTD) plays a critical role in a wide range of civilian and military applications. Existing methods suffer from deficiencies in the localization of dim targets and the perception of contour information under dense clutter environments, severely limiting their detection performance. To tackle these issues, we propose a contour-aware and saliency priors embedding network (CSPENet) for ISTD. We first design a surround-convergent prior extraction module (SCPEM) that ",
    "arxiv_url": "https://arxiv.org/abs/2505.09943v1",
    "pdf_url": "https://arxiv.org/pdf/2505.09943v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2505.09943",
    "arxiv_authors": [
      "Jiakun Deng",
      "Kexuan Li",
      "Xingye Cui",
      "Jiaxuan Li",
      "Chang Long",
      "Tian Pu",
      "Zhenming Peng"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=CSPENet%3A+Contour-Aware+and+Saliency+Priors+Embedding+Network+for+Infrared+Small+Target+Detection+Jiakun+Deng+Kexuan+Li+Xingye+Cui+Jiaxuan+Li+Chang+Long",
    "gs_search_success": true,
    "gs_authors": [
      "9Znj1YoAAAAJ",
      "k2GFFH0AAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 7
  },
  {
    "arxiv_id": "2501.09167",
    "title": "Embodied Scene Understanding for Vision Language Models via MetaVQA",
    "year": 2025,
    "published": "2025-01-15T21:36:19Z",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "abstract": "Vision Language Models (VLMs) demonstrate significant potential as embodied AI agents for various mobility applications. However, a standardized, closed-loop benchmark for evaluating their spatial reasoning and sequential decision-making capabilities is lacking. To address this, we present MetaVQA: a comprehensive benchmark designed to assess and enhance VLMs' understanding of spatial relationships and scene dynamics through Visual Question Answering (VQA) and closed-loop simulations. MetaVQA le",
    "arxiv_url": "https://arxiv.org/abs/2501.09167v1",
    "pdf_url": "https://arxiv.org/pdf/2501.09167v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2501.09167",
    "arxiv_authors": [
      "Weizhen Wang",
      "Chenda Duan",
      "Zhenghao Peng",
      "Yuxin Liu",
      "Bolei Zhou"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Embodied+Scene+Understanding+for+Vision+Language+Models+via+MetaVQA+Weizhen+Wang+Chenda+Duan+Zhenghao+Peng+Yuxin+Liu+Bolei+Zhou",
    "gs_search_success": true,
    "gs_authors": [
      "9D4aG8AAAAAJ",
      "DooYOyoAAAAJ",
      "ZQoOjaIAAAAJ",
      "JZ8ws6IAAAAJ",
      "BepoGRMAAAAJ"
    ],
    "citation_count": 10,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2404.09389",
    "title": "Masked and Shuffled Blind Spot Denoising for Real-World Images",
    "year": 2024,
    "published": "2024-04-15T00:19:47Z",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "We introduce a novel approach to single image denoising based on the Blind Spot Denoising principle, which we call MAsked and SHuffled Blind Spot Denoising (MASH). We focus on the case of correlated noise, which often plagues real images. MASH is the result of a careful analysis to determine the relationships between the level of blindness (masking) of the input and the (unknown) noise correlation. Moreover, we introduce a shuffling technique to weaken the local correlation of noise, which in tu",
    "arxiv_url": "https://arxiv.org/abs/2404.09389v1",
    "pdf_url": "https://arxiv.org/pdf/2404.09389v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2404.09389",
    "arxiv_authors": [
      "Hamadi Chihaoui",
      "Paolo Favaro"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Masked+and+Shuffled+Blind+Spot+Denoising+for+Real-World+Images+Hamadi+Chihaoui+Paolo+Favaro",
    "gs_search_success": true,
    "gs_authors": [
      "w_XDRRsAAAAJ"
    ],
    "citation_count": 22,
    "gs_author_count": null
  },
  {
    "arxiv_id": "2402.18172",
    "title": "NiteDR: Nighttime Image De-Raining with Cross-View Sensor Cooperative Learning for Dynamic Driving Scenes",
    "year": 2024,
    "published": "2024-02-28T09:02:33Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "In real-world environments, outdoor imaging systems are often affected by disturbances such as rain degradation. Especially, in nighttime driving scenes, insufficient and uneven lighting shrouds the scenes in darkness, resulting degradation of both the image quality and visibility. Particularly, in the field of autonomous driving, the visual perception ability of RGB sensors experiences a sharp decline in such harsh scenarios. Additionally, driving assistance systems suffer from reduced capabili",
    "arxiv_url": "https://arxiv.org/abs/2402.18172v2",
    "pdf_url": "https://arxiv.org/pdf/2402.18172v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2402.18172",
    "arxiv_authors": [
      "Cidan Shi",
      "Lihuang Fang",
      "Han Wu",
      "Xiaoyu Xian",
      "Yukai Shi",
      "Liang Lin"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=NiteDR%3A+Nighttime+Image+De-Raining+with+Cross-View+Sensor+Cooperative+Learning+for+Dynamic+Driving+Scenes+Cidan+Shi+Lihuang+Fang+Han+Wu+Xiaoyu+Xian+Yukai+Shi",
    "gs_search_success": true,
    "gs_authors": [
      "Nav8m8gAAAAJ",
      "z_tI-X4AAAAJ",
      "8crtYoIAAAAJ"
    ],
    "citation_count": 13,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2405.19861",
    "title": "Hierarchical Object-Centric Learning with Capsule Networks",
    "year": 2024,
    "published": "2024-05-30T09:10:33Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Capsule networks (CapsNets) were introduced to address convolutional neural networks limitations, learning object-centric representations that are more robust, pose-aware, and interpretable. They organize neurons into groups called capsules, where each capsule encodes the instantiation parameters of an object or one of its parts. Moreover, a routing algorithm connects capsules in different layers, thereby capturing hierarchical part-whole relationships in the data.   This thesis investigates the",
    "arxiv_url": "https://arxiv.org/abs/2405.19861v1",
    "pdf_url": "https://arxiv.org/pdf/2405.19861v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2405.19861",
    "arxiv_authors": [
      "Riccardo Renzulli"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Hierarchical+Object-Centric+Learning+with+Capsule+Networks+Riccardo+Renzulli",
    "gs_search_success": true,
    "gs_authors": [
      "JlAby_oAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 1
  },
  {
    "arxiv_id": "2501.14317",
    "title": "Nautilus: Locality-aware Autoencoder for Scalable Mesh Generation",
    "year": 2025,
    "published": "2025-01-24T08:22:02Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Triangle meshes are fundamental to 3D applications, enabling efficient modification and rasterization while maintaining compatibility with standard rendering pipelines. However, current automatic mesh generation methods typically rely on intermediate representations that lack the continuous surface quality inherent to meshes. Converting these representations into meshes produces dense, suboptimal outputs. Although recent autoregressive approaches demonstrate promise in directly modeling mesh ver",
    "arxiv_url": "https://arxiv.org/abs/2501.14317v5",
    "pdf_url": "https://arxiv.org/pdf/2501.14317v5",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2501.14317",
    "arxiv_authors": [
      "Yuxuan Wang",
      "Xuanyu Yi",
      "Haohan Weng",
      "Qingshan Xu",
      "Xiaokang Wei",
      "Xianghui Yang",
      "Chunchao Guo",
      "Long Chen",
      "Hanwang Zhang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Nautilus%3A+Locality-aware+Autoencoder+for+Scalable+Mesh+Generation+Yuxuan+Wang+Xuanyu+Yi+Haohan+Weng+Qingshan+Xu+Xiaokang+Wei",
    "gs_search_success": true,
    "gs_authors": [
      "e9EzzWAAAAAJ",
      "4pz1qQQAAAAJ",
      "V08HvHUAAAAJ",
      "-gtmMpIAAAAJ",
      "k96kDhsAAAAJ",
      "91i3wqgAAAAJ"
    ],
    "citation_count": 6,
    "gs_author_count": 9
  },
  {
    "arxiv_id": "2302.09461",
    "title": "Liveness score-based regression neural networks for face anti-spoofing",
    "year": 2023,
    "published": "2023-02-19T02:45:35Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Previous anti-spoofing methods have used either pseudo maps or user-defined labels, and the performance of each approach depends on the accuracy of the third party networks generating pseudo maps and the way in which the users define the labels. In this paper, we propose a liveness score-based regression network for overcoming the dependency on third party networks and users. First, we introduce a new labeling technique, called pseudo-discretized label encoding for generating discretized labels ",
    "arxiv_url": "https://arxiv.org/abs/2302.09461v2",
    "pdf_url": "https://arxiv.org/pdf/2302.09461v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2302.09461",
    "arxiv_authors": [
      "Youngjun Kwak",
      "Minyoung Jung",
      "Hunjae Yoo",
      "JinHo Shin",
      "Changick Kim"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Liveness+score-based+regression+neural+networks+for+face+anti-spoofing+Youngjun+Kwak+Minyoung+Jung+Hunjae+Yoo+JinHo+Shin+Changick+Kim",
    "gs_search_success": false,
    "gs_authors": [],
    "citation_count": null,
    "gs_author_count": null,
    "error_type": "fetch_failed"
  },
  {
    "arxiv_id": "2503.22359",
    "title": "Mitigating Knowledge Discrepancies among Multiple Datasets for Task-agnostic Unified Face Alignment",
    "year": 2025,
    "published": "2025-03-28T11:59:27Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Despite the similar structures of human faces, existing face alignment methods cannot learn unified knowledge from multiple datasets with different landmark annotations. The limited training samples in a single dataset commonly result in fragile robustness in this field. To mitigate knowledge discrepancies among different datasets and train a task-agnostic unified face alignment (TUFA) framework, this paper presents a strategy to unify knowledge from multiple datasets. Specifically, we calculate",
    "arxiv_url": "https://arxiv.org/abs/2503.22359v2",
    "pdf_url": "https://arxiv.org/pdf/2503.22359v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2503.22359",
    "arxiv_authors": [
      "Jiahao Xia",
      "Min Xu",
      "Wenjian Huang",
      "Jianguo Zhang",
      "Haimin Zhang",
      "Chunxia Xiao"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Mitigating+Knowledge+Discrepancies+among+Multiple+Datasets+for+Task-agnostic+Unified+Face+Alignment+Jiahao+Xia+Min+Xu+Wenjian+Huang+Jianguo+Zhang+Haimin+Zhang",
    "gs_search_success": true,
    "gs_authors": [
      "8frVIOAAAAAJ",
      "ypSmZtIAAAAJ",
      "Ac6VCMkAAAAJ",
      "flBRNP0AAAAJ",
      "NSu8LzMAAAAJ",
      "hHRl550AAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2506.11004",
    "title": "Developing a Dyslexia Indicator Using Eye Tracking",
    "year": 2025,
    "published": "2025-04-21T09:33:25Z",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.CV",
      "cs.HC"
    ],
    "abstract": "Dyslexia, affecting an estimated 10% to 20% of the global population, significantly impairs learning capabilities, highlighting the need for innovative and accessible diagnostic methods. This paper investigates the effectiveness of eye-tracking technology combined with machine learning algorithms as a cost-effective alternative for early dyslexia detection. By analyzing general eye movement patterns, including prolonged fixation durations and erratic saccades, we proposed an enhanced solution fo",
    "arxiv_url": "https://arxiv.org/abs/2506.11004v1",
    "pdf_url": "https://arxiv.org/pdf/2506.11004v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2506.11004",
    "arxiv_authors": [
      "Kevin Cogan",
      "Vuong M. Ngo",
      "Mark Roantree"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Developing+a+Dyslexia+Indicator+Using+Eye+Tracking+Kevin+Cogan+Vuong+M.+Ngo+Mark+Roantree",
    "gs_search_success": true,
    "gs_authors": [
      "_HPT1zgAAAAJ",
      "YVD0xOAAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2501.03471",
    "title": "Hyperbolic Binary Neural Network",
    "year": 2025,
    "published": "2025-01-07T02:15:58Z",
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "abstract": "Binary Neural Network (BNN) converts full-precision weights and activations into their extreme 1-bit counterparts, making it particularly suitable for deployment on lightweight mobile devices. While binary neural networks are typically formulated as a constrained optimization problem and optimized in the binarized space, general neural networks are formulated as an unconstrained optimization problem and optimized in the continuous space. This paper introduces the Hyperbolic Binary Neural Network",
    "arxiv_url": "https://arxiv.org/abs/2501.03471v1",
    "pdf_url": "https://arxiv.org/pdf/2501.03471v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2501.03471",
    "arxiv_authors": [
      "Jun Chen",
      "Jingyang Xiang",
      "Tianxin Huang",
      "Xiangrui Zhao",
      "Yong Liu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Hyperbolic+Binary+Neural+Network+Jun+Chen+Jingyang+Xiang+Tianxin+Huang+Xiangrui+Zhao+Yong+Liu",
    "gs_search_success": true,
    "gs_authors": [
      "qYcgBbEAAAAJ",
      "n1D1P8EAAAAJ",
      "Fg7WYfcAAAAJ",
      "OUtPkg0AAAAJ",
      "UaHKxKUAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2412.03878",
    "title": "DiffSign: AI-Assisted Generation of Customizable Sign Language Videos With Enhanced Realism",
    "year": 2024,
    "published": "2024-12-05T05:18:28Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "The proliferation of several streaming services in recent years has now made it possible for a diverse audience across the world to view the same media content, such as movies or TV shows. While translation and dubbing services are being added to make content accessible to the local audience, the support for making content accessible to people with different abilities, such as the Deaf and Hard of Hearing (DHH) community, is still lagging. Our goal is to make media content more accessible to the",
    "arxiv_url": "https://arxiv.org/abs/2412.03878v1",
    "pdf_url": "https://arxiv.org/pdf/2412.03878v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2412.03878",
    "arxiv_authors": [
      "Sudha Krishnamurthy",
      "Vimal Bhat",
      "Abhinav Jain"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=DiffSign%3A+AI-Assisted+Generation+of+Customizable+Sign+Language+Videos+With+Enhanced+Realism+Sudha+Krishnamurthy+Vimal+Bhat+Abhinav+Jain",
    "gs_search_success": true,
    "gs_authors": [
      "SemRGocAAAAJ",
      "hzOAwQoAAAAJ"
    ],
    "citation_count": 2,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2404.07217",
    "title": "Attention-aware Semantic Communications for Collaborative Inference",
    "year": 2024,
    "published": "2024-02-23T10:08:45Z",
    "categories": [
      "eess.SP",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "We propose a communication-efficient collaborative inference framework in the domain of edge inference, focusing on the efficient use of vision transformer (ViT) models. The partitioning strategy of conventional collaborative inference fails to reduce communication cost because of the inherent architecture of ViTs maintaining consistent layer dimensions across the entire transformer encoder. Therefore, instead of employing the partitioning strategy, our framework utilizes a lightweight ViT model",
    "arxiv_url": "https://arxiv.org/abs/2404.07217v2",
    "pdf_url": "https://arxiv.org/pdf/2404.07217v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2404.07217",
    "arxiv_authors": [
      "Jiwoong Im",
      "Nayoung Kwon",
      "Taewoo Park",
      "Jiheon Woo",
      "Jaeho Lee",
      "Yongjune Kim"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Attention-aware+Semantic+Communications+for+Collaborative+Inference+Jiwoong+Im+Nayoung+Kwon+Taewoo+Park+Jiheon+Woo+Jaeho+Lee",
    "gs_search_success": true,
    "gs_authors": [
      "t91zoQMAAAAJ",
      "cwMwJFkAAAAJ",
      "62pWEW0AAAAJ",
      "BnRA4MwAAAAJ",
      "WPKrXEoAAAAJ",
      "n_CPeokAAAAJ"
    ],
    "citation_count": 9,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2503.05333",
    "title": "PhysicsGen: Can Generative Models Learn from Images to Predict Complex Physical Relations?",
    "year": 2025,
    "published": "2025-03-07T11:19:13Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "The image-to-image translation abilities of generative learning models have recently made significant progress in the estimation of complex (steered) mappings between image distributions. While appearance based tasks like image in-painting or style transfer have been studied at length, we propose to investigate the potential of generative models in the context of physical simulations. Providing a dataset of 300k image-pairs and baseline evaluations for three different physical simulation tasks, ",
    "arxiv_url": "https://arxiv.org/abs/2503.05333v1",
    "pdf_url": "https://arxiv.org/pdf/2503.05333v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2503.05333",
    "arxiv_authors": [
      "Martin Spitznagel",
      "Jan Vaillant",
      "Janis Keuper"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=PhysicsGen%3A+Can+Generative+Models+Learn+from+Images+to+Predict+Complex+Physical+Relations%3F+Martin+Spitznagel+Jan+Vaillant+Janis+Keuper",
    "gs_search_success": true,
    "gs_authors": [
      "BUkDvU0AAAAJ"
    ],
    "citation_count": 3,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2504.12299",
    "title": "Adapting a World Model for Trajectory Following in a 3D Game",
    "year": 2025,
    "published": "2025-04-16T17:59:54Z",
    "categories": [
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "Imitation learning is a powerful tool for training agents by leveraging expert knowledge, and being able to replicate a given trajectory is an integral part of it. In complex environments, like modern 3D video games, distribution shift and stochasticity necessitate robust approaches beyond simple action replay. In this study, we apply Inverse Dynamics Models (IDM) with different encoders and policy heads to trajectory following in a modern 3D video game -- Bleeding Edge. Additionally, we investi",
    "arxiv_url": "https://arxiv.org/abs/2504.12299v1",
    "pdf_url": "https://arxiv.org/pdf/2504.12299v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2504.12299",
    "arxiv_authors": [
      "Marko Tot",
      "Shu Ishida",
      "Abdelhak Lemkhenter",
      "David Bignell",
      "Pallavi Choudhury",
      "Chris Lovett",
      "Luis França",
      "Matheus Ribeiro Furtado de Mendonça",
      "Tarun Gupta",
      "Darren Gehring",
      "Sam Devlin",
      "Sergio Valcarcel Macua",
      "Raluca Georgescu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Adapting+a+World+Model+for+Trajectory+Following+in+a+3D+Game+Marko+Tot+Shu+Ishida+Abdelhak+Lemkhenter+David+Bignell+Pallavi+Choudhury",
    "gs_search_success": true,
    "gs_authors": [
      "NEC8qU0AAAAJ",
      "4fgTaHgAAAAJ",
      "h_qfOBYAAAAJ"
    ],
    "citation_count": 5,
    "gs_author_count": 7
  },
  {
    "arxiv_id": "2401.08083",
    "title": "UV-SAM: Adapting Segment Anything Model for Urban Village Identification",
    "year": 2024,
    "published": "2024-01-16T03:21:42Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Urban villages, defined as informal residential areas in or around urban centers, are characterized by inadequate infrastructures and poor living conditions, closely related to the Sustainable Development Goals (SDGs) on poverty, adequate housing, and sustainable cities. Traditionally, governments heavily depend on field survey methods to monitor the urban villages, which however are time-consuming, labor-intensive, and possibly delayed. Thanks to widely available and timely updated satellite im",
    "arxiv_url": "https://arxiv.org/abs/2401.08083v2",
    "pdf_url": "https://arxiv.org/pdf/2401.08083v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2401.08083",
    "arxiv_authors": [
      "Xin Zhang",
      "Yu Liu",
      "Yuming Lin",
      "Qingmin Liao",
      "Yong Li"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=UV-SAM%3A+Adapting+Segment+Anything+Model+for+Urban+Village+Identification+Xin+Zhang+Yu+Liu+Yuming+Lin+Qingmin+Liao+Yong+Li",
    "gs_search_success": true,
    "gs_authors": [
      "AFQ0ii0AAAAJ",
      "ZXXlX3UAAAAJ",
      "kmgzPeQAAAAJ",
      "vL9MnfQAAAAJ"
    ],
    "citation_count": 78,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2401.09083",
    "title": "Remote Sensing ChatGPT: Solving Remote Sensing Tasks with ChatGPT and Visual Models",
    "year": 2024,
    "published": "2024-01-17T09:44:07Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Recently, the flourishing large language models(LLM), especially ChatGPT, have shown exceptional performance in language understanding, reasoning, and interaction, attracting users and researchers from multiple fields and domains. Although LLMs have shown great capacity to perform human-like task accomplishment in natural language and natural image, their potential in handling remote sensing interpretation tasks has not yet been fully explored. Moreover, the lack of automation in remote sensing ",
    "arxiv_url": "https://arxiv.org/abs/2401.09083v1",
    "pdf_url": "https://arxiv.org/pdf/2401.09083v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2401.09083",
    "arxiv_authors": [
      "Haonan Guo",
      "Xin Su",
      "Chen Wu",
      "Bo Du",
      "Liangpei Zhang",
      "Deren Li"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Remote+Sensing+ChatGPT%3A+Solving+Remote+Sensing+Tasks+with+ChatGPT+and+Visual+Models+Haonan+Guo+Xin+Su+Chen+Wu+Bo+Du+Liangpei+Zhang",
    "gs_search_success": true,
    "gs_authors": [
      "HvYxc84AAAAJ",
      "yFEl8hcAAAAJ",
      "aSqNc38AAAAJ",
      "DbTt_CcAAAAJ"
    ],
    "citation_count": 46,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2402.14741",
    "title": "Zero-Shot Pediatric Tuberculosis Detection in Chest X-Rays using Self-Supervised Learning",
    "year": 2024,
    "published": "2024-02-22T17:55:18Z",
    "categories": [
      "cs.CV",
      "eess.IV"
    ],
    "abstract": "Tuberculosis (TB) remains a significant global health challenge, with pediatric cases posing a major concern. The World Health Organization (WHO) advocates for chest X-rays (CXRs) for TB screening. However, visual interpretation by radiologists can be subjective, time-consuming and prone to error, especially in pediatric TB. Artificial intelligence (AI)-driven computer-aided detection (CAD) tools, especially those utilizing deep learning, show promise in enhancing lung disease detection. However",
    "arxiv_url": "https://arxiv.org/abs/2402.14741v1",
    "pdf_url": "https://arxiv.org/pdf/2402.14741v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2402.14741",
    "arxiv_authors": [
      "Daniel Capellán-Martín",
      "Abhijeet Parida",
      "Juan J. Gómez-Valverde",
      "Ramon Sanchez-Jacob",
      "Pooneh Roshanitabrizi",
      "Marius G. Linguraru",
      "María J. Ledesma-Carbayo",
      "Syed M. Anwar"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Zero-Shot+Pediatric+Tuberculosis+Detection+in+Chest+X-Rays+using+Self-Supervised+Learning+Daniel+Capell%C3%A1n-Mart%C3%ADn+Abhijeet+Parida+Juan+J.+G%C3%B3mez-Valverde+Ramon+Sanchez-Jacob+Pooneh+Roshanitabrizi",
    "gs_search_success": true,
    "gs_authors": [
      "s2MjcHIAAAAJ",
      "Z7db_TMAAAAJ",
      "ZR_GNNIAAAAJ",
      "sWYYUzkAAAAJ",
      "BylSAKUAAAAJ"
    ],
    "citation_count": 3,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2310.01799",
    "title": "SMRD: SURE-based Robust MRI Reconstruction with Diffusion Models",
    "year": 2023,
    "published": "2023-10-03T05:05:35Z",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "abstract": "Diffusion models have recently gained popularity for accelerated MRI reconstruction due to their high sample quality. They can effectively serve as rich data priors while incorporating the forward model flexibly at inference time, and they have been shown to be more robust than unrolled methods under distribution shifts. However, diffusion models require careful tuning of inference hyperparameters on a validation set and are still sensitive to distribution shifts during testing. To address these",
    "arxiv_url": "https://arxiv.org/abs/2310.01799v2",
    "pdf_url": "https://arxiv.org/pdf/2310.01799v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2310.01799",
    "arxiv_authors": [
      "Batu Ozturkler",
      "Chao Liu",
      "Benjamin Eckart",
      "Morteza Mardani",
      "Jiaming Song",
      "Jan Kautz"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=SMRD%3A+SURE-based+Robust+MRI+Reconstruction+with+Diffusion+Models+Batu+Ozturkler+Chao+Liu+Benjamin+Eckart+Morteza+Mardani+Jiaming+Song",
    "gs_search_success": true,
    "gs_authors": [
      "P9FclNEAAAAJ",
      "6dP660cAAAAJ",
      "8gAliWUAAAAJ",
      "9PRX6q8AAAAJ",
      "7tO7lZgAAAAJ",
      "pjcBeJYAAAAJ"
    ],
    "citation_count": 27,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2402.04031",
    "title": "Polyp-DDPM: Diffusion-Based Semantic Polyp Synthesis for Enhanced Segmentation",
    "year": 2024,
    "published": "2024-02-06T14:26:02Z",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "This study introduces Polyp-DDPM, a diffusion-based method for generating realistic images of polyps conditioned on masks, aimed at enhancing the segmentation of gastrointestinal (GI) tract polyps. Our approach addresses the challenges of data limitations, high annotation costs, and privacy concerns associated with medical images. By conditioning the diffusion model on segmentation masks-binary masks that represent abnormal areas-Polyp-DDPM outperforms state-of-the-art methods in terms of image ",
    "arxiv_url": "https://arxiv.org/abs/2402.04031v2",
    "pdf_url": "https://arxiv.org/pdf/2402.04031v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2402.04031",
    "arxiv_authors": [
      "Zolnamar Dorjsembe",
      "Hsing-Kuo Pao",
      "Furen Xiao"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Polyp-DDPM%3A+Diffusion-Based+Semantic+Polyp+Synthesis+for+Enhanced+Segmentation+Zolnamar+Dorjsembe+Hsing-Kuo+Pao+Furen+Xiao",
    "gs_search_success": true,
    "gs_authors": [
      "v0DNZWUAAAAJ",
      "xcMg8mkAAAAJ",
      "aoNKH0MAAAAJ"
    ],
    "citation_count": 20,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2406.10200",
    "title": "SSTFB: Leveraging self-supervised pretext learning and temporal self-attention with feature branching for real-time video polyp segmentation",
    "year": 2024,
    "published": "2024-06-14T17:33:11Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.MM"
    ],
    "abstract": "Polyps are early cancer indicators, so assessing occurrences of polyps and their removal is critical. They are observed through a colonoscopy screening procedure that generates a stream of video frames. Segmenting polyps in their natural video screening procedure has several challenges, such as the co-existence of imaging artefacts, motion blur, and floating debris. Most existing polyp segmentation algorithms are developed on curated still image datasets that do not represent real-world colonosc",
    "arxiv_url": "https://arxiv.org/abs/2406.10200v1",
    "pdf_url": "https://arxiv.org/pdf/2406.10200v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2406.10200",
    "arxiv_authors": [
      "Ziang Xu",
      "Jens Rittscher",
      "Sharib Ali"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=SSTFB%3A+Leveraging+self-supervised+pretext+learning+and+temporal+self-attention+with+feature+branching+for+real-time+video+polyp+segmentation+Ziang+Xu+Jens+Rittscher+Sharib+Ali",
    "gs_search_success": false,
    "gs_authors": [],
    "citation_count": null,
    "gs_author_count": null,
    "error_type": "fetch_failed"
  },
  {
    "arxiv_id": "2406.19225",
    "title": "ProtoGMM: Multi-prototype Gaussian-Mixture-based Domain Adaptation Model for Semantic Segmentation",
    "year": 2024,
    "published": "2024-06-27T14:50:50Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Domain adaptive semantic segmentation aims to generate accurate and dense predictions for an unlabeled target domain by leveraging a supervised model trained on a labeled source domain. The prevalent self-training approach involves retraining the dense discriminative classifier of $p(class|pixel feature)$ using the pseudo-labels from the target domain. While many methods focus on mitigating the issue of noisy pseudo-labels, they often overlook the underlying data distribution p(pixel feature|cla",
    "arxiv_url": "https://arxiv.org/abs/2406.19225v1",
    "pdf_url": "https://arxiv.org/pdf/2406.19225v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2406.19225",
    "arxiv_authors": [
      "Nazanin Moradinasab",
      "Laura S. Shankman",
      "Rebecca A. Deaton",
      "Gary K. Owens",
      "Donald E. Brown"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=ProtoGMM%3A+Multi-prototype+Gaussian-Mixture-based+Domain+Adaptation+Model+for+Semantic+Segmentation+Nazanin+Moradinasab+Laura+S.+Shankman+Rebecca+A.+Deaton+Gary+K.+Owens+Donald+E.+Brown",
    "gs_search_success": true,
    "gs_authors": [
      "oA8nMFQAAAAJ",
      "V9GCh-YAAAAJ",
      "AKjx2FMAAAAJ",
      "SSPvo1IAAAAJ"
    ],
    "citation_count": 4,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2410.17488",
    "title": "GenDP: 3D Semantic Fields for Category-Level Generalizable Diffusion Policy",
    "year": 2024,
    "published": "2024-10-23T00:51:47Z",
    "categories": [
      "cs.RO",
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "Diffusion-based policies have shown remarkable capability in executing complex robotic manipulation tasks but lack explicit characterization of geometry and semantics, which often limits their ability to generalize to unseen objects and layouts. To enhance the generalization capabilities of Diffusion Policy, we introduce a novel framework that incorporates explicit spatial and semantic information via 3D semantic fields. We generate 3D descriptor fields from multi-view RGBD observations with lar",
    "arxiv_url": "https://arxiv.org/abs/2410.17488v1",
    "pdf_url": "https://arxiv.org/pdf/2410.17488v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2410.17488",
    "arxiv_authors": [
      "Yixuan Wang",
      "Guang Yin",
      "Binghao Huang",
      "Tarik Kelestemur",
      "Jiuguang Wang",
      "Yunzhu Li"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=GenDP%3A+3D+Semantic+Fields+for+Category-Level+Generalizable+Diffusion+Policy+Yixuan+Wang+Guang+Yin+Binghao+Huang+Tarik+Kelestemur+Jiuguang+Wang",
    "gs_search_success": true,
    "gs_authors": [
      "svxVZpoAAAAJ",
      "WlA92lcAAAAJ",
      "nqoOetAAAAAJ",
      "oqXUt0AAAAAJ"
    ],
    "citation_count": 26,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2505.03562",
    "title": "Real-Time Person Image Synthesis Using a Flow Matching Model",
    "year": 2025,
    "published": "2025-05-06T14:13:44Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Pose-Guided Person Image Synthesis (PGPIS) generates realistic person images conditioned on a target pose and a source image. This task plays a key role in various real-world applications, such as sign language video generation, AR/VR, gaming, and live streaming. In these scenarios, real-time PGPIS is critical for providing immediate visual feedback and maintaining user immersion.However, achieving real-time performance remains a significant challenge due to the complexity of synthesizing high-f",
    "arxiv_url": "https://arxiv.org/abs/2505.03562v1",
    "pdf_url": "https://arxiv.org/pdf/2505.03562v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2505.03562",
    "arxiv_authors": [
      "Jiwoo Jeong",
      "Kirok Kim",
      "Wooju Kim",
      "Nam-Joon Kim"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Real-Time+Person+Image+Synthesis+Using+a+Flow+Matching+Model+Jiwoo+Jeong+Kirok+Kim+Wooju+Kim+Nam-Joon+Kim",
    "gs_search_success": true,
    "gs_authors": [
      "ucR8pK8AAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2412.11149",
    "title": "A Comprehensive Survey of Action Quality Assessment: Method and Benchmark",
    "year": 2024,
    "published": "2024-12-15T10:47:26Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Action Quality Assessment (AQA) quantitatively evaluates the quality of human actions, providing automated assessments that reduce biases in human judgment. Its applications span domains such as sports analysis, skill assessment, and medical care. Recent advances in AQA have introduced innovative methodologies, but similar methods often intertwine across different domains, highlighting the fragmented nature that hinders systematic reviews. In addition, the lack of a unified benchmark and limited",
    "arxiv_url": "https://arxiv.org/abs/2412.11149v1",
    "pdf_url": "https://arxiv.org/pdf/2412.11149v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2412.11149",
    "arxiv_authors": [
      "Kanglei Zhou",
      "Ruizhi Cai",
      "Liyuan Wang",
      "Hubert P. H. Shum",
      "Xiaohui Liang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=A+Comprehensive+Survey+of+Action+Quality+Assessment%3A+Method+and+Benchmark+Kanglei+Zhou+Ruizhi+Cai+Liyuan+Wang+Hubert+P.+H.+Shum+Xiaohui+Liang",
    "gs_search_success": true,
    "gs_authors": [
      "IGdJlqgAAAAJ",
      "pkPLCEYAAAAJ",
      "yG1FaioAAAAJ",
      "UAgdoY4AAAAJ"
    ],
    "citation_count": 13,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2311.10251",
    "title": "UniMOS: A Universal Framework For Multi-Organ Segmentation Over Label-Constrained Datasets",
    "year": 2023,
    "published": "2023-11-17T00:44:56Z",
    "categories": [
      "eess.IV",
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "Machine learning models for medical images can help physicians diagnose and manage diseases. However, due to the fact that medical image annotation requires a great deal of manpower and expertise, as well as the fact that clinical departments perform image annotation based on task orientation, there is the problem of having fewer medical image annotation data with more unlabeled data and having many datasets that annotate only a single organ. In this paper, we present UniMOS, the first universal",
    "arxiv_url": "https://arxiv.org/abs/2311.10251v2",
    "pdf_url": "https://arxiv.org/pdf/2311.10251v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2311.10251",
    "arxiv_authors": [
      "Can Li",
      "Sheng Shao",
      "Junyi Qu",
      "Shuchao Pang",
      "Mehmet A. Orgun"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=UniMOS%3A+A+Universal+Framework+For+Multi-Organ+Segmentation+Over+Label-Constrained+Datasets+Can+Li+Sheng+Shao+Junyi+Qu+Shuchao+Pang+Mehmet+A.+Orgun",
    "gs_search_success": true,
    "gs_authors": [
      "FpZlwKUAAAAJ",
      "xeDlacgAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2407.20962",
    "title": "MMTrail: A Multimodal Trailer Video Dataset with Language and Music Descriptions",
    "year": 2024,
    "published": "2024-07-30T16:43:24Z",
    "categories": [
      "cs.CV",
      "cs.MM",
      "cs.SD",
      "eess.AS"
    ],
    "abstract": "Massive multi-modality datasets play a significant role in facilitating the success of large video-language models. However, current video-language datasets primarily provide text descriptions for visual frames, considering audio to be weakly related information. They usually overlook exploring the potential of inherent audio-visual correlation, leading to monotonous annotation within each modality instead of comprehensive and precise descriptions. Such ignorance results in the difficulty of mul",
    "arxiv_url": "https://arxiv.org/abs/2407.20962v3",
    "pdf_url": "https://arxiv.org/pdf/2407.20962v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2407.20962",
    "arxiv_authors": [
      "Xiaowei Chi",
      "Yatian Wang",
      "Aosong Cheng",
      "Pengjun Fang",
      "Zeyue Tian",
      "Yingqing He",
      "Zhaoyang Liu",
      "Xingqun Qi",
      "Jiahao Pan",
      "Rongyu Zhang",
      "Mengfei Li",
      "Ruibin Yuan",
      "Yanbing Jiang",
      "Wei Xue",
      "Wenhan Luo",
      "Qifeng Chen",
      "Shanghang Zhang",
      "Qifeng Liu",
      "Yike Guo"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=MMTrail%3A+A+Multimodal+Trailer+Video+Dataset+with+Language+and+Music+Descriptions+Xiaowei+Chi+Yatian+Wang+Aosong+Cheng+Pengjun+Fang+Zeyue+Tian",
    "gs_search_success": true,
    "gs_authors": [
      "rRcc9eoAAAAJ",
      "B6y6yF0AAAAJ",
      "Wmb7IUEAAAAJ",
      "3tO41a8AAAAJ",
      "m39CfiwAAAAJ",
      "lQaJlDYAAAAJ",
      "R3pE-0EAAAAJ",
      "Vl1X_-sAAAAJ",
      "Qd_hX1cAAAAJ",
      "dghq4MQAAAAJ"
    ],
    "citation_count": 9,
    "gs_author_count": 12
  },
  {
    "arxiv_id": "2409.09731",
    "title": "Learning Two-factor Representation for Magnetic Resonance Image Super-resolution",
    "year": 2024,
    "published": "2024-09-15T13:32:24Z",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "abstract": "Magnetic Resonance Imaging (MRI) requires a trade-off between resolution, signal-to-noise ratio, and scan time, making high-resolution (HR) acquisition challenging. Therefore, super-resolution for MR image is a feasible solution. However, most existing methods face challenges in accurately learning a continuous volumetric representation from low-resolution image or require HR image for supervision. To solve these challenges, we propose a novel method for MR image super-resolution based on two-fa",
    "arxiv_url": "https://arxiv.org/abs/2409.09731v1",
    "pdf_url": "https://arxiv.org/pdf/2409.09731v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2409.09731",
    "arxiv_authors": [
      "Weifeng Wei",
      "Heng Chen",
      "Pengxiang Su"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Learning+Two-factor+Representation+for+Magnetic+Resonance+Image+Super-resolution+Weifeng+Wei+Heng+Chen+Pengxiang+Su",
    "gs_search_success": true,
    "gs_authors": [],
    "citation_count": 1,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2409.12418",
    "title": "Domain-stratified Training for Cross-organ and Cross-scanner Adenocarcinoma Segmentation in the COSAS 2024 Challenge",
    "year": 2024,
    "published": "2024-09-19T02:36:34Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "This manuscript presents an image segmentation algorithm developed for the Cross-Organ and Cross-Scanner Adenocarcinoma Segmentation (COSAS 2024) challenge. We adopted an organ-stratified and scanner-stratified approach to train multiple Upernet-based segmentation models and subsequently ensembled the results. Despite the challenges posed by the varying tumor characteristics across different organs and the differing imaging conditions of various scanners, our method achieved a final test score o",
    "arxiv_url": "https://arxiv.org/abs/2409.12418v1",
    "pdf_url": "https://arxiv.org/pdf/2409.12418v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2409.12418",
    "arxiv_authors": [
      "Huang Jiayan",
      "Ji Zheng",
      "Kuang Jinbo",
      "Xu Shuoyu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Domain-stratified+Training+for+Cross-organ+and+Cross-scanner+Adenocarcinoma+Segmentation+in+the+COSAS+2024+Challenge+Huang+Jiayan+Ji+Zheng+Kuang+Jinbo+Xu+Shuoyu",
    "gs_search_success": true,
    "gs_authors": [
      "QDnsWToAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2408.06721",
    "title": "Response Wide Shut: Surprising Observations in Basic Vision Language Model Capabilities",
    "year": 2024,
    "published": "2024-08-13T08:26:32Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Vision-Language Models (VLMs) have emerged as general purpose tools for addressing a variety of complex computer vision problems. Such models have been shown to be highly capable, but, at the same time, also lacking some basic visual understanding skills. In this paper, we set out to understand the limitations of SoTA VLMs on fundamental visual tasks: object classification, understanding spatial arrangement, and ability to delineate individual object instances (through counting), by constructing",
    "arxiv_url": "https://arxiv.org/abs/2408.06721v1",
    "pdf_url": "https://arxiv.org/pdf/2408.06721v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2408.06721",
    "arxiv_authors": [
      "Shivam Chandhok",
      "Wan-Cyuan Fan",
      "Leonid Sigal"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Response+Wide+Shut%3A+Surprising+Observations+in+Basic+Vision+Language+Model+Capabilities+Shivam+Chandhok+Wan-Cyuan+Fan+Leonid+Sigal",
    "gs_search_success": true,
    "gs_authors": [
      "7soDcboAAAAJ",
      "P2mG6rcAAAAJ",
      "bbe4ResAAAAJ",
      "EIPHoLEAAAAJ",
      "ZER2BeIAAAAJ"
    ],
    "citation_count": 8,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2405.05791",
    "title": "Sequential Amodal Segmentation via Cumulative Occlusion Learning",
    "year": 2024,
    "published": "2024-05-09T14:17:26Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "To fully understand the 3D context of a single image, a visual system must be able to segment both the visible and occluded regions of objects, while discerning their occlusion order. Ideally, the system should be able to handle any object and not be restricted to segmenting a limited set of object classes, especially in robotic applications. Addressing this need, we introduce a diffusion model with cumulative occlusion learning designed for sequential amodal segmentation of objects with uncerta",
    "arxiv_url": "https://arxiv.org/abs/2405.05791v1",
    "pdf_url": "https://arxiv.org/pdf/2405.05791v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2405.05791",
    "arxiv_authors": [
      "Jiayang Ao",
      "Qiuhong Ke",
      "Krista A. Ehinger"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Sequential+Amodal+Segmentation+via+Cumulative+Occlusion+Learning+Jiayang+Ao+Qiuhong+Ke+Krista+A.+Ehinger",
    "gs_search_success": true,
    "gs_authors": [
      "EdGfpdcAAAAJ",
      "Sq5U8MAAAAAJ",
      "84qxdhsAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2312.05605",
    "title": "TCNCA: Temporal Convolution Network with Chunked Attention for Scalable Sequence Processing",
    "year": 2023,
    "published": "2023-12-09T16:12:25Z",
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "abstract": "MEGA is a recent transformer-based architecture, which utilizes a linear recurrent operator whose parallel computation, based on the FFT, scales as $O(LlogL)$, with $L$ being the sequence length. We build upon their approach by replacing the linear recurrence with a special temporal convolutional network which permits larger receptive field size with shallower networks, and reduces the computational complexity to $O(L)$. The resulting model is called TCNCA, a Temporal Convolutional Network with ",
    "arxiv_url": "https://arxiv.org/abs/2312.05605v1",
    "pdf_url": "https://arxiv.org/pdf/2312.05605v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2312.05605",
    "arxiv_authors": [
      "Aleksandar Terzic",
      "Michael Hersche",
      "Geethan Karunaratne",
      "Luca Benini",
      "Abu Sebastian",
      "Abbas Rahimi"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=TCNCA%3A+Temporal+Convolution+Network+with+Chunked+Attention+for+Scalable+Sequence+Processing+Aleksandar+Terzic+Michael+Hersche+Geethan+Karunaratne+Luca+Benini+Abu+Sebastian",
    "gs_search_success": true,
    "gs_authors": [
      "CrCeq1QAAAAJ",
      "nivrGSAAAAAJ",
      "sUCQ7KMAAAAJ",
      "uhC6m3EAAAAJ",
      "8riq3sYAAAAJ",
      "yx0pEmYAAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2407.14979",
    "title": "RGB2Point: 3D Point Cloud Generation from Single RGB Images",
    "year": 2024,
    "published": "2024-07-20T21:06:33Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "We introduce RGB2Point, an unposed single-view RGB image to a 3D point cloud generation based on Transformer. RGB2Point takes an input image of an object and generates a dense 3D point cloud. Contrary to prior works based on CNN layers and diffusion denoising approaches, we use pre-trained Transformer layers that are fast and generate high-quality point clouds with consistent quality over available categories. Our generated point clouds demonstrate high quality on a real-world dataset, as eviden",
    "arxiv_url": "https://arxiv.org/abs/2407.14979v4",
    "pdf_url": "https://arxiv.org/pdf/2407.14979v4",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2407.14979",
    "arxiv_authors": [
      "Jae Joong Lee",
      "Bedrich Benes"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=RGB2Point%3A+3D+Point+Cloud+Generation+from+Single+RGB+Images+Jae+Joong+Lee+Bedrich+Benes",
    "gs_search_success": true,
    "gs_authors": [
      "2VvQrz0AAAAJ",
      "aYPLSVMAAAAJ"
    ],
    "citation_count": 9,
    "gs_author_count": 2
  },
  {
    "arxiv_id": "2411.00826",
    "title": "Uncertainty Quantification via Hölder Divergence for Multi-View Representation Learning",
    "year": 2024,
    "published": "2024-10-29T04:29:44Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "abstract": "Evidence-based deep learning represents a burgeoning paradigm for uncertainty estimation, offering reliable predictions with negligible extra computational overheads. Existing methods usually adopt Kullback-Leibler divergence to estimate the uncertainty of network predictions, ignoring domain gaps among various modalities. To tackle this issue, this paper introduces a novel algorithm based on Hölder Divergence (HD) to enhance the reliability of multi-view learning by addressing inherent uncertai",
    "arxiv_url": "https://arxiv.org/abs/2411.00826v2",
    "pdf_url": "https://arxiv.org/pdf/2411.00826v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2411.00826",
    "arxiv_authors": [
      "Yan Zhang",
      "Ming Li",
      "Chun Li",
      "Zhaoxia Liu",
      "Ye Zhang",
      "Fei Richard Yu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Uncertainty+Quantification+via+H%C3%B6lder+Divergence+for+Multi-View+Representation+Learning+Yan+Zhang+Ming+Li+Chun+Li+Zhaoxia+Liu+Ye+Zhang",
    "gs_search_success": true,
    "gs_authors": [
      "zuGMGBoAAAAJ",
      "2Rf9urMAAAAJ",
      "RLBmBdEAAAAJ",
      "2wySPkcAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2309.09502",
    "title": "RenderOcc: Vision-Centric 3D Occupancy Prediction with 2D Rendering Supervision",
    "year": 2023,
    "published": "2023-09-18T06:08:15Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "3D occupancy prediction holds significant promise in the fields of robot perception and autonomous driving, which quantifies 3D scenes into grid cells with semantic labels. Recent works mainly utilize complete occupancy labels in 3D voxel space for supervision. However, the expensive annotation process and sometimes ambiguous labels have severely constrained the usability and scalability of 3D occupancy models. To address this, we present RenderOcc, a novel paradigm for training 3D occupancy mod",
    "arxiv_url": "https://arxiv.org/abs/2309.09502v2",
    "pdf_url": "https://arxiv.org/pdf/2309.09502v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2309.09502",
    "arxiv_authors": [
      "Mingjie Pan",
      "Jiaming Liu",
      "Renrui Zhang",
      "Peixiang Huang",
      "Xiaoqi Li",
      "Bing Wang",
      "Hongwei Xie",
      "Li Liu",
      "Shanghang Zhang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=RenderOcc%3A+Vision-Centric+3D+Occupancy+Prediction+with+2D+Rendering+Supervision+Mingjie+Pan+Jiaming+Liu+Renrui+Zhang+Peixiang+Huang+Xiaoqi+Li",
    "gs_search_success": true,
    "gs_authors": [
      "SvVfZtUAAAAJ",
      "312BcwEAAAAJ",
      "vkQ5_LIAAAAJ",
      "kRvS9KAAAAAJ",
      "QdUeY3IAAAAJ",
      "zO5iemAAAAAJ",
      "YlL3xN4AAAAJ",
      "cPki5sUAAAAJ",
      "voqw10cAAAAJ"
    ],
    "citation_count": 130,
    "gs_author_count": 9
  },
  {
    "arxiv_id": "2305.14979",
    "title": "Assessment of the Reliablity of a Model's Decision by Generalizing Attribution to the Wavelet Domain",
    "year": 2023,
    "published": "2023-05-24T10:13:32Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "stat.ML"
    ],
    "abstract": "Neural networks have shown remarkable performance in computer vision, but their deployment in numerous scientific and technical fields is challenging due to their black-box nature. Scientists and practitioners need to evaluate the reliability of a decision, i.e., to know simultaneously if a model relies on the relevant features and whether these features are robust to image corruptions. Existing attribution methods aim to provide human-understandable explanations by highlighting important region",
    "arxiv_url": "https://arxiv.org/abs/2305.14979v5",
    "pdf_url": "https://arxiv.org/pdf/2305.14979v5",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2305.14979",
    "arxiv_authors": [
      "Gabriel Kasmi",
      "Laurent Dubus",
      "Yves-Marie Saint Drenan",
      "Philippe Blanc"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Assessment+of+the+Reliablity+of+a+Model%27s+Decision+by+Generalizing+Attribution+to+the+Wavelet+Domain+Gabriel+Kasmi+Laurent+Dubus+Yves-Marie+Saint+Drenan+Philippe+Blanc",
    "gs_search_success": true,
    "gs_authors": [
      "s7I9oVMAAAAJ",
      "WSZ-luEAAAAJ",
      "Rt14jucAAAAJ",
      "l0tNZQcAAAAJ"
    ],
    "citation_count": 5,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2405.07814",
    "title": "NutritionVerse-Direct: Exploring Deep Neural Networks for Multitask Nutrition Prediction from Food Images",
    "year": 2024,
    "published": "2024-05-13T14:56:55Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Many aging individuals encounter challenges in effectively tracking their dietary intake, exacerbating their susceptibility to nutrition-related health complications. Self-reporting methods are often inaccurate and suffer from substantial bias; however, leveraging intelligent prediction methods can automate and enhance precision in this process. Recent work has explored using computer vision prediction systems to predict nutritional information from food images. Still, these methods are often ta",
    "arxiv_url": "https://arxiv.org/abs/2405.07814v1",
    "pdf_url": "https://arxiv.org/pdf/2405.07814v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2405.07814",
    "arxiv_authors": [
      "Matthew Keller",
      "Chi-en Amy Tai",
      "Yuhao Chen",
      "Pengcheng Xi",
      "Alexander Wong"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=NutritionVerse-Direct%3A+Exploring+Deep+Neural+Networks+for+Multitask+Nutrition+Prediction+from+Food+Images+Matthew+Keller+Chi-en+Amy+Tai+Yuhao+Chen+Pengcheng+Xi+Alexander+Wong",
    "gs_search_success": false,
    "gs_authors": [],
    "citation_count": null,
    "gs_author_count": null,
    "error_type": "fetch_failed"
  },
  {
    "arxiv_id": "2412.00102",
    "title": "ElectroVizQA: How well do Multi-modal LLMs perform in Electronics Visual Question Answering?",
    "year": 2024,
    "published": "2024-11-27T20:25:07Z",
    "categories": [
      "cs.CV",
      "cs.CL",
      "cs.LG"
    ],
    "abstract": "Multi-modal Large Language Models (MLLMs) are gaining significant attention for their ability to process multi-modal data, providing enhanced contextual understanding of complex problems. MLLMs have demonstrated exceptional capabilities in tasks such as Visual Question Answering (VQA); however, they often struggle with fundamental engineering problems, and there is a scarcity of specialized datasets for training on topics like digital electronics. To address this gap, we propose a benchmark data",
    "arxiv_url": "https://arxiv.org/abs/2412.00102v2",
    "pdf_url": "https://arxiv.org/pdf/2412.00102v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2412.00102",
    "arxiv_authors": [
      "Pragati Shuddhodhan Meshram",
      "Swetha Karthikeyan",
      "Bhavya Bhavya",
      "Suma Bhat"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=ElectroVizQA%3A+How+well+do+Multi-modal+LLMs+perform+in+Electronics+Visual+Question+Answering%3F+Pragati+Shuddhodhan+Meshram+Swetha+Karthikeyan+Bhavya+Bhavya+Suma+Bhat",
    "gs_search_success": true,
    "gs_authors": [
      "YRT-D78AAAAJ",
      "Y47rHIoAAAAJ"
    ],
    "citation_count": 3,
    "gs_author_count": null
  },
  {
    "arxiv_id": "2303.13899",
    "title": "Robust Test-Time Adaptation in Dynamic Scenarios",
    "year": 2023,
    "published": "2023-03-24T10:19:14Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Test-time adaptation (TTA) intends to adapt the pretrained model to test distributions with only unlabeled test data streams. Most of the previous TTA methods have achieved great success on simple test data streams such as independently sampled data from single or multiple distributions. However, these attempts may fail in dynamic scenarios of real-world applications like autonomous driving, where the environments gradually change and the test data is sampled correlatively over time. In this wor",
    "arxiv_url": "https://arxiv.org/abs/2303.13899v1",
    "pdf_url": "https://arxiv.org/pdf/2303.13899v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2303.13899",
    "arxiv_authors": [
      "Longhui Yuan",
      "Binhui Xie",
      "Shuang Li"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Robust+Test-Time+Adaptation+in+Dynamic+Scenarios+Longhui+Yuan+Binhui+Xie+Shuang+Li",
    "gs_search_success": true,
    "gs_authors": [
      "aLx99vUAAAAJ",
      "cbVMMCwAAAAJ",
      "fVnEIZEAAAAJ"
    ],
    "citation_count": 218,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2303.04595",
    "title": "Structure-aware registration network for liver DCE-CT images",
    "year": 2023,
    "published": "2023-03-08T14:08:56Z",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "abstract": "Image registration of liver dynamic contrast-enhanced computed tomography (DCE-CT) is crucial for diagnosis and image-guided surgical planning of liver cancer. However, intensity variations due to the flow of contrast agents combined with complex spatial motion induced by respiration brings great challenge to existing intensity-based registration methods. To address these problems, we propose a novel structure-aware registration method by incorporating structural information of related organs wi",
    "arxiv_url": "https://arxiv.org/abs/2303.04595v1",
    "pdf_url": "https://arxiv.org/pdf/2303.04595v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2303.04595",
    "arxiv_authors": [
      "Peng Xue",
      "Jingyang Zhang",
      "Lei Ma",
      "Mianxin Liu",
      "Yuning Gu",
      "Jiawei Huang",
      "Feihong Liua",
      "Yongsheng Pan",
      "Xiaohuan Cao",
      "Dinggang Shen"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Structure-aware+registration+network+for+liver+DCE-CT+images+Peng+Xue+Jingyang+Zhang+Lei+Ma+Mianxin+Liu+Yuning+Gu",
    "gs_search_success": true,
    "gs_authors": [
      "sAoyYSUAAAAJ",
      "C-M2ufUAAAAJ",
      "8PSqvCAAAAAJ",
      "PqwTXvAAAAAJ",
      "ooqH5wcAAAAJ",
      "v6VYQC8AAAAJ",
      "LplimusAAAAJ",
      "E1na4Q4AAAAJ",
      "bI_qetYAAAAJ"
    ],
    "citation_count": 10,
    "gs_author_count": 10
  },
  {
    "arxiv_id": "2412.18859",
    "title": "Few-shot Metric Domain Adaptation: Practical Learning Strategies for an Automated Plant Disease Diagnosis",
    "year": 2024,
    "published": "2024-12-25T10:01:30Z",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "Numerous studies have explored image-based automated systems for plant disease diagnosis, demonstrating impressive diagnostic capabilities. However, recent large-scale analyses have revealed a critical limitation: that the diagnostic capability suffers significantly when validated on images captured in environments (domains) differing from those used during training. This shortfall stems from the inherently limited dataset size and the diverse manifestation of disease symptoms, combined with sub",
    "arxiv_url": "https://arxiv.org/abs/2412.18859v1",
    "pdf_url": "https://arxiv.org/pdf/2412.18859v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2412.18859",
    "arxiv_authors": [
      "Shoma Kudo",
      "Satoshi Kagiwada",
      "Hitoshi Iyatomi"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Few-shot+Metric+Domain+Adaptation%3A+Practical+Learning+Strategies+for+an+Automated+Plant+Disease+Diagnosis+Shoma+Kudo+Satoshi+Kagiwada+Hitoshi+Iyatomi",
    "gs_search_success": true,
    "gs_authors": [
      "ghyQxvIAAAAJ"
    ],
    "citation_count": 3,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2502.17832",
    "title": "MM-PoisonRAG: Disrupting Multimodal RAG with Local and Global Poisoning Attacks",
    "year": 2025,
    "published": "2025-02-25T04:23:59Z",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR",
      "cs.CV"
    ],
    "abstract": "Multimodal large language models with Retrieval Augmented Generation (RAG) have significantly advanced tasks such as multimodal question answering by grounding responses in external text and images. This grounding improves factuality, reduces hallucination, and extends reasoning beyond parametric knowledge. However, this reliance on external knowledge poses a critical yet underexplored safety risk: knowledge poisoning attacks, where adversaries deliberately inject adversarial multimodal content ",
    "arxiv_url": "https://arxiv.org/abs/2502.17832v3",
    "pdf_url": "https://arxiv.org/pdf/2502.17832v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2502.17832",
    "arxiv_authors": [
      "Hyeonjeong Ha",
      "Qiusi Zhan",
      "Jeonghwan Kim",
      "Dimitrios Bralios",
      "Saikrishna Sanniboina",
      "Nanyun Peng",
      "Kai-Wei Chang",
      "Daniel Kang",
      "Heng Ji"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=MM-PoisonRAG%3A+Disrupting+Multimodal+RAG+with+Local+and+Global+Poisoning+Attacks+Hyeonjeong+Ha+Qiusi+Zhan+Jeonghwan+Kim+Dimitrios+Bralios+Saikrishna+Sanniboina",
    "gs_search_success": true,
    "gs_authors": [
      "uZjpwGMAAAAJ",
      "CcnGNN8AAAAJ",
      "kpDJ2mgAAAAJ",
      "5Yfl8UsAAAAJ",
      "XaYJrgoAAAAJ",
      "fqDBtzYAAAAJ",
      "XxRXvX0AAAAJ",
      "CpMjT0YAAAAJ",
      "z7GCqT4AAAAJ"
    ],
    "citation_count": 4,
    "gs_author_count": 9
  },
  {
    "arxiv_id": "2503.13185",
    "title": "3DAxisPrompt: Promoting the 3D Grounding and Reasoning in GPT-4o",
    "year": 2025,
    "published": "2025-03-17T13:57:05Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Multimodal Large Language Models (MLLMs) exhibit impressive capabilities across a variety of tasks, especially when equipped with carefully designed visual prompts. However, existing studies primarily focus on logical reasoning and visual understanding, while the capability of MLLMs to operate effectively in 3D vision remains an ongoing area of exploration. In this paper, we introduce a novel visual prompting method, called 3DAxisPrompt, to elicit the 3D understanding capabilities of MLLMs in re",
    "arxiv_url": "https://arxiv.org/abs/2503.13185v1",
    "pdf_url": "https://arxiv.org/pdf/2503.13185v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2503.13185",
    "arxiv_authors": [
      "Dingning Liu",
      "Cheng Wang",
      "Peng Gao",
      "Renrui Zhang",
      "Xinzhu Ma",
      "Yuan Meng",
      "Zhihui Wang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=3DAxisPrompt%3A+Promoting+the+3D+Grounding+and+Reasoning+in+GPT-4o+Dingning+Liu+Cheng+Wang+Peng+Gao+Renrui+Zhang+Xinzhu+Ma",
    "gs_search_success": true,
    "gs_authors": [
      "7ubFBOYAAAAJ",
      "FbSpETgAAAAJ",
      "YlL3xN4AAAAJ",
      "a8KGGsAAAAAJ",
      "8PuKa_8AAAAJ"
    ],
    "citation_count": 5,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2310.08681",
    "title": "Fed-Safe: Securing Federated Learning in Healthcare Against Adversarial Attacks",
    "year": 2023,
    "published": "2023-10-12T19:33:53Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "This paper explores the security aspects of federated learning applications in medical image analysis. Current robustness-oriented methods like adversarial training, secure aggregation, and homomorphic encryption often risk privacy compromises. The central aim is to defend the network against potential privacy breaches while maintaining model robustness against adversarial manipulations. We show that incorporating distributed noise, grounded in the privacy guarantees in federated settings, enabl",
    "arxiv_url": "https://arxiv.org/abs/2310.08681v1",
    "pdf_url": "https://arxiv.org/pdf/2310.08681v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2310.08681",
    "arxiv_authors": [
      "Erfan Darzi",
      "Nanna M. Sijtsema",
      "P. M. A van Ooijen"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Fed-Safe%3A+Securing+Federated+Learning+in+Healthcare+Against+Adversarial+Attacks+Erfan+Darzi+Nanna+M.+Sijtsema+P.+M.+A+van+Ooijen",
    "gs_search_success": true,
    "gs_authors": [
      "xgggrLUAAAAJ",
      "cyIedloAAAAJ"
    ],
    "citation_count": 5,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2404.05258",
    "title": "Unsupervised Band Selection Using Fused HSI and LiDAR Attention Integrating With Autoencoder",
    "year": 2024,
    "published": "2024-04-08T07:47:28Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Band selection in hyperspectral imaging (HSI) is critical for optimising data processing and enhancing analytical accuracy. Traditional approaches have predominantly concentrated on analysing spectral and pixel characteristics within individual bands independently. These approaches overlook the potential benefits of integrating multiple data sources, such as Light Detection and Ranging (LiDAR), and is further challenged by the limited availability of labeled data in HSI processing, which represe",
    "arxiv_url": "https://arxiv.org/abs/2404.05258v1",
    "pdf_url": "https://arxiv.org/pdf/2404.05258v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2404.05258",
    "arxiv_authors": [
      "Judy X Yang",
      "Jun Zhou",
      "Jing Wang",
      "Hui Tian",
      "Alan Wee Chung Liew"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Unsupervised+Band+Selection+Using+Fused+HSI+and+LiDAR+Attention+Integrating+With+Autoencoder+Judy+X+Yang+Jun+Zhou+Jing+Wang+Hui+Tian+Alan+Wee+Chung+Liew",
    "gs_search_success": true,
    "gs_authors": [
      "CNgJ3LYAAAAJ",
      "3ljeojMAAAAJ",
      "6hOOxw0AAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2311.12077",
    "title": "Balancing Efficiency and Quality: MoEISR for Arbitrary-Scale Image Super-Resolution",
    "year": 2023,
    "published": "2023-11-20T05:34:36Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Arbitrary-scale image super-resolution employing implicit neural functions has gained significant attention lately due to its capability to upscale images across diverse scales utilizing only a single model. Nevertheless, these methodologies have imposed substantial computational demands as they involve querying every target pixel to a single resource-intensive decoder. In this paper, we introduce a novel and efficient framework, the Mixture-of-Experts Implicit Super-Resolution (MoEISR), which e",
    "arxiv_url": "https://arxiv.org/abs/2311.12077v2",
    "pdf_url": "https://arxiv.org/pdf/2311.12077v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2311.12077",
    "arxiv_authors": [
      "Young Jae Oh",
      "Jihun Kim",
      "Jihoon Nam",
      "Tae Hyun Kim"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Balancing+Efficiency+and+Quality%3A+MoEISR+for+Arbitrary-Scale+Image+Super-Resolution+Young+Jae+Oh+Jihun+Kim+Jihoon+Nam+Tae+Hyun+Kim",
    "gs_search_success": true,
    "gs_authors": [
      "7S6G5zUAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2303.08498",
    "title": "BEVHeight: A Robust Framework for Vision-based Roadside 3D Object Detection",
    "year": 2023,
    "published": "2023-03-15T10:18:53Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "While most recent autonomous driving system focuses on developing perception methods on ego-vehicle sensors, people tend to overlook an alternative approach to leverage intelligent roadside cameras to extend the perception ability beyond the visual range. We discover that the state-of-the-art vision-centric bird's eye view detection methods have inferior performances on roadside cameras. This is because these methods mainly focus on recovering the depth regarding the camera center, where the dep",
    "arxiv_url": "https://arxiv.org/abs/2303.08498v2",
    "pdf_url": "https://arxiv.org/pdf/2303.08498v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2303.08498",
    "arxiv_authors": [
      "Lei Yang",
      "Kaicheng Yu",
      "Tao Tang",
      "Jun Li",
      "Kun Yuan",
      "Li Wang",
      "Xinyu Zhang",
      "Peng Chen"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=BEVHeight%3A+A+Robust+Framework+for+Vision-based+Roadside+3D+Object+Detection+Lei+Yang+Kaicheng+Yu+Tao+Tang+Jun+Li+Kun+Yuan",
    "gs_search_success": true,
    "gs_authors": [
      "aMnHLz4AAAAJ",
      "EUnI2nMAAAAJ",
      "1ltylFwAAAAJ",
      "Jtmq_m0AAAAJ",
      "pmzKjcUAAAAJ",
      "0Q7pN4cAAAAJ",
      "9uHIfl4AAAAJ"
    ],
    "citation_count": 146,
    "gs_author_count": 8
  },
  {
    "arxiv_id": "2312.08730",
    "title": "Towards Robust and Expressive Whole-body Human Pose and Shape Estimation",
    "year": 2023,
    "published": "2023-12-14T08:17:42Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Whole-body pose and shape estimation aims to jointly predict different behaviors (e.g., pose, hand gesture, facial expression) of the entire human body from a monocular image. Existing methods often exhibit degraded performance under the complexity of in-the-wild scenarios. We argue that the accuracy and reliability of these models are significantly affected by the quality of the predicted \\textit{bounding box}, e.g., the scale and alignment of body parts. The natural discrepancy between the ide",
    "arxiv_url": "https://arxiv.org/abs/2312.08730v1",
    "pdf_url": "https://arxiv.org/pdf/2312.08730v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2312.08730",
    "arxiv_authors": [
      "Hui EnPang",
      "Zhongang Cai",
      "Lei Yang",
      "Qingyi Tao",
      "Zhonghua Wu",
      "Tianwei Zhang",
      "Ziwei Liu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Towards+Robust+and+Expressive+Whole-body+Human+Pose+and+Shape+Estimation+Hui+EnPang+Zhongang+Cai+Lei+Yang+Qingyi+Tao+Zhonghua+Wu",
    "gs_search_success": true,
    "gs_authors": [
      "fMXnSGMAAAAJ",
      "lc45xlcAAAAJ",
      "jZH2IPYAAAAJ",
      "9vpiYDIAAAAJ",
      "szblOQ4AAAAJ",
      "wMDgLCYAAAAJ",
      "WrDKqIAAAAAJ"
    ],
    "citation_count": 15,
    "gs_author_count": 7
  },
  {
    "arxiv_id": "2309.03891",
    "title": "ArtiGrasp: Physically Plausible Synthesis of Bi-Manual Dexterous Grasping and Articulation",
    "year": 2023,
    "published": "2023-09-07T17:53:20Z",
    "categories": [
      "cs.RO",
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "We present ArtiGrasp, a novel method to synthesize bi-manual hand-object interactions that include grasping and articulation. This task is challenging due to the diversity of the global wrist motions and the precise finger control that are necessary to articulate objects. ArtiGrasp leverages reinforcement learning and physics simulations to train a policy that controls the global and local hand pose. Our framework unifies grasping and articulation within a single policy guided by a single hand p",
    "arxiv_url": "https://arxiv.org/abs/2309.03891v2",
    "pdf_url": "https://arxiv.org/pdf/2309.03891v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2309.03891",
    "arxiv_authors": [
      "Hui Zhang",
      "Sammy Christen",
      "Zicong Fan",
      "Luocheng Zheng",
      "Jemin Hwangbo",
      "Jie Song",
      "Otmar Hilliges"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=ArtiGrasp%3A+Physically+Plausible+Synthesis+of+Bi-Manual+Dexterous+Grasping+and+Articulation+Hui+Zhang+Sammy+Christen+Zicong+Fan+Luocheng+Zheng+Jemin+Hwangbo",
    "gs_search_success": true,
    "gs_authors": [
      "xrcr7Z0AAAAJ",
      "kBN1B6YAAAAJ",
      "r1L_2qkAAAAJ",
      "Uam1ZB8AAAAJ",
      "jZXvcOsAAAAJ",
      "-epU9OsAAAAJ"
    ],
    "citation_count": 62,
    "gs_author_count": 7
  },
  {
    "arxiv_id": "2503.07772",
    "title": "Hallucinatory Image Tokens: A Training-free EAZY Approach on Detecting and Mitigating Object Hallucinations in LVLMs",
    "year": 2025,
    "published": "2025-03-10T18:53:39Z",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "Despite their remarkable potential, Large Vision-Language Models (LVLMs) still face challenges with object hallucination, a problem where their generated outputs mistakenly incorporate objects that do not actually exist. Although most works focus on addressing this issue within the language-model backbone, our work shifts the focus to the image input source, investigating how specific image tokens contribute to hallucinations. Our analysis reveals a striking finding: a small subset of image toke",
    "arxiv_url": "https://arxiv.org/abs/2503.07772v2",
    "pdf_url": "https://arxiv.org/pdf/2503.07772v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2503.07772",
    "arxiv_authors": [
      "Liwei Che",
      "Tony Qingze Liu",
      "Jing Jia",
      "Weiyi Qin",
      "Ruixiang Tang",
      "Vladimir Pavlovic"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Hallucinatory+Image+Tokens%3A+A+Training-free+EAZY+Approach+on+Detecting+and+Mitigating+Object+Hallucinations+in+LVLMs+Liwei+Che+Tony+Qingze+Liu+Jing+Jia+Weiyi+Qin+Ruixiang+Tang",
    "gs_search_success": true,
    "gs_authors": [
      "wUHM2GEAAAAJ",
      "qPmO1EIAAAAJ",
      "wrQ-S_IAAAAJ",
      "xZk3FfsAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2303.14977",
    "title": "A novel Multi to Single Module for small object detection",
    "year": 2023,
    "published": "2023-03-27T08:17:22Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Small object detection presents a significant challenge in computer vision and object detection. The performance of small object detectors is often compromised by a lack of pixels and less significant features. This issue stems from information misalignment caused by variations in feature scale and information loss during feature processing. In response to this challenge, this paper proposes a novel the Multi to Single Module (M2S), which enhances a specific layer through improving feature extra",
    "arxiv_url": "https://arxiv.org/abs/2303.14977v1",
    "pdf_url": "https://arxiv.org/pdf/2303.14977v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2303.14977",
    "arxiv_authors": [
      "Xiaohui Guo"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=A+novel+Multi+to+Single+Module+for+small+object+detection+Xiaohui+Guo",
    "gs_search_success": true,
    "gs_authors": [],
    "citation_count": 13,
    "gs_author_count": 1
  },
  {
    "arxiv_id": "2302.00953",
    "title": "Deep-Learning Tool for Early Identifying Non-Traumatic Intracranial Hemorrhage Etiology based on CT Scan",
    "year": 2023,
    "published": "2023-02-02T08:45:17Z",
    "categories": [
      "eess.IV",
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "Background: To develop an artificial intelligence system that can accurately identify acute non-traumatic intracranial hemorrhage (ICH) etiology based on non-contrast CT (NCCT) scans and investigate whether clinicians can benefit from it in a diagnostic setting. Materials and Methods: The deep learning model was developed with 1868 eligible NCCT scans with non-traumatic ICH collected between January 2011 and April 2018. We tested the model on two independent datasets (TT200 and SD 98) collected ",
    "arxiv_url": "https://arxiv.org/abs/2302.00953v1",
    "pdf_url": "https://arxiv.org/pdf/2302.00953v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2302.00953",
    "arxiv_authors": [
      "Meng Zhao",
      "Yifan Hu",
      "Ruixuan Jiang",
      "Yuanli Zhao",
      "Dong Zhang",
      "Yan Zhang",
      "Rong Wang",
      "Yong Cao",
      "Qian Zhang",
      "Yonggang Ma",
      "Jiaxi Li",
      "Shaochen Yu",
      "Wenjie Li",
      "Ran Zhang",
      "Yefeng Zheng",
      "Shuo Wang",
      "Jizong Zhao"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Deep-Learning+Tool+for+Early+Identifying+Non-Traumatic+Intracranial+Hemorrhage+Etiology+based+on+CT+Scan+Meng+Zhao+Yifan+Hu+Ruixuan+Jiang+Yuanli+Zhao+Dong+Zhang",
    "gs_search_success": true,
    "gs_authors": [
      "adHUrhkAAAAJ",
      "lWphcYIAAAAJ"
    ],
    "citation_count": 3,
    "gs_author_count": 11
  },
  {
    "arxiv_id": "2303.17225",
    "title": "FreeSeg: Unified, Universal and Open-Vocabulary Image Segmentation",
    "year": 2023,
    "published": "2023-03-30T08:42:49Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Recently, open-vocabulary learning has emerged to accomplish segmentation for arbitrary categories of text-based descriptions, which popularizes the segmentation system to more general-purpose application scenarios. However, existing methods devote to designing specialized architectures or parameters for specific segmentation tasks. These customized design paradigms lead to fragmentation between various segmentation tasks, thus hindering the uniformity of segmentation models. Hence in this paper",
    "arxiv_url": "https://arxiv.org/abs/2303.17225v1",
    "pdf_url": "https://arxiv.org/pdf/2303.17225v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2303.17225",
    "arxiv_authors": [
      "Jie Qin",
      "Jie Wu",
      "Pengxiang Yan",
      "Ming Li",
      "Ren Yuxi",
      "Xuefeng Xiao",
      "Yitong Wang",
      "Rui Wang",
      "Shilei Wen",
      "Xin Pan",
      "Xingang Wang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=FreeSeg%3A+Unified%2C+Universal+and+Open-Vocabulary+Image+Segmentation+Jie+Qin+Jie+Wu+Pengxiang+Yan+Ming+Li+Ren+Yuxi",
    "gs_search_success": true,
    "gs_authors": [
      "CVkM9TQAAAAJ",
      "zKtYrHYAAAAJ",
      "UCCiN_sAAAAJ",
      "MxvLqLcAAAAJ",
      "NfFTKfYAAAAJ",
      "BbfIggwAAAAJ",
      "F2h7bGwAAAAJ",
      "nGki_EEAAAAJ"
    ],
    "citation_count": 159,
    "gs_author_count": 11
  },
  {
    "arxiv_id": "2411.10004",
    "title": "EyeDiff: text-to-image diffusion model improves rare eye disease diagnosis",
    "year": 2024,
    "published": "2024-11-15T07:30:53Z",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "abstract": "The rising prevalence of vision-threatening retinal diseases poses a significant burden on the global healthcare systems. Deep learning (DL) offers a promising solution for automatic disease screening but demands substantial data. Collecting and labeling large volumes of ophthalmic images across various modalities encounters several real-world challenges, especially for rare diseases. Here, we introduce EyeDiff, a text-to-image model designed to generate multimodal ophthalmic images from natural",
    "arxiv_url": "https://arxiv.org/abs/2411.10004v1",
    "pdf_url": "https://arxiv.org/pdf/2411.10004v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2411.10004",
    "arxiv_authors": [
      "Ruoyu Chen",
      "Weiyi Zhang",
      "Bowen Liu",
      "Xiaolan Chen",
      "Pusheng Xu",
      "Shunming Liu",
      "Mingguang He",
      "Danli Shi"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=EyeDiff%3A+text-to-image+diffusion+model+improves+rare+eye+disease+diagnosis+Ruoyu+Chen+Weiyi+Zhang+Bowen+Liu+Xiaolan+Chen+Pusheng+Xu",
    "gs_search_success": true,
    "gs_authors": [
      "iA-PvEsAAAAJ",
      "HVdLUawAAAAJ",
      "uVVC9F8AAAAJ",
      "WgY19bYAAAAJ",
      "QyF6ivQAAAAJ"
    ],
    "citation_count": 12,
    "gs_author_count": 8
  },
  {
    "arxiv_id": "2312.08864",
    "title": "RankDVQA-mini: Knowledge Distillation-Driven Deep Video Quality Assessment",
    "year": 2023,
    "published": "2023-12-14T12:38:57Z",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "abstract": "Deep learning-based video quality assessment (deep VQA) has demonstrated significant potential in surpassing conventional metrics, with promising improvements in terms of correlation with human perception. However, the practical deployment of such deep VQA models is often limited due to their high computational complexity and large memory requirements. To address this issue, we aim to significantly reduce the model size and runtime of one of the state-of-the-art deep VQA methods, RankDVQA, by em",
    "arxiv_url": "https://arxiv.org/abs/2312.08864v2",
    "pdf_url": "https://arxiv.org/pdf/2312.08864v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2312.08864",
    "arxiv_authors": [
      "Chen Feng",
      "Duolikun Danier",
      "Haoran Wang",
      "Fan Zhang",
      "Benoit Vallade",
      "Alex Mackin",
      "David Bull"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=RankDVQA-mini%3A+Knowledge+Distillation-Driven+Deep+Video+Quality+Assessment+Chen+Feng+Duolikun+Danier+Haoran+Wang+Fan+Zhang+Benoit+Vallade",
    "gs_search_success": true,
    "gs_authors": [
      "sX9B2lUAAAAJ",
      "WraDXlkAAAAJ",
      "7uqUCRUAAAAJ",
      "BBujJNcAAAAJ",
      "kWY2QHoAAAAJ",
      "CD23RNgAAAAJ"
    ],
    "citation_count": 2,
    "gs_author_count": 7
  },
  {
    "arxiv_id": "2405.01503",
    "title": "PAM-UNet: Shifting Attention on Region of Interest in Medical Images",
    "year": 2024,
    "published": "2024-05-02T17:33:26Z",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "abstract": "Computer-aided segmentation methods can assist medical personnel in improving diagnostic outcomes. While recent advancements like UNet and its variants have shown promise, they face a critical challenge: balancing accuracy with computational efficiency. Shallow encoder architectures in UNets often struggle to capture crucial spatial features, leading in inaccurate and sparse segmentation. To address this limitation, we propose a novel \\underline{P}rogressive \\underline{A}ttention based \\underlin",
    "arxiv_url": "https://arxiv.org/abs/2405.01503v1",
    "pdf_url": "https://arxiv.org/pdf/2405.01503v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2405.01503",
    "arxiv_authors": [
      "Abhijit Das",
      "Debesh Jha",
      "Vandan Gorade",
      "Koushik Biswas",
      "Hongyi Pan",
      "Zheyuan Zhang",
      "Daniela P. Ladner",
      "Yury Velichko",
      "Amir Borhani",
      "Ulas Bagci"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=PAM-UNet%3A+Shifting+Attention+on+Region+of+Interest+in+Medical+Images+Abhijit+Das+Debesh+Jha+Vandan+Gorade+Koushik+Biswas+Hongyi+Pan",
    "gs_search_success": true,
    "gs_authors": [
      "U_vmD3sAAAAJ",
      "mMTyE68AAAAJ",
      "23I1CMYAAAAJ",
      "wNeoq5AAAAAJ",
      "gAiSKA8AAAAJ",
      "lHtpCNcAAAAJ",
      "OsJYrdEAAAAJ"
    ],
    "citation_count": 4,
    "gs_author_count": 9
  },
  {
    "arxiv_id": "2308.02784",
    "title": "Semi-supervised Contrastive Regression for Estimation of Eye Gaze",
    "year": 2023,
    "published": "2023-08-05T04:11:38Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.HC",
      "cs.LG"
    ],
    "abstract": "With the escalated demand of human-machine interfaces for intelligent systems, development of gaze controlled system have become a necessity. Gaze, being the non-intrusive form of human interaction, is one of the best suited approach. Appearance based deep learning models are the most widely used for gaze estimation. But the performance of these models is entirely influenced by the size of labeled gaze dataset and in effect affects generalization in performance. This paper aims to develop a semi",
    "arxiv_url": "https://arxiv.org/abs/2308.02784v1",
    "pdf_url": "https://arxiv.org/pdf/2308.02784v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2308.02784",
    "arxiv_authors": [
      "Somsukla Maiti",
      "Akshansh Gupta"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Semi-supervised+Contrastive+Regression+for+Estimation+of+Eye+Gaze+Somsukla+Maiti+Akshansh+Gupta",
    "gs_search_success": true,
    "gs_authors": [
      "T6HIeewAAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 2
  },
  {
    "arxiv_id": "2307.01003",
    "title": "Visual Instruction Tuning with Polite Flamingo",
    "year": 2023,
    "published": "2023-07-03T13:37:00Z",
    "categories": [
      "cs.CV",
      "cs.CL"
    ],
    "abstract": "Recent research has demonstrated that the multi-task fine-tuning of multi-modal Large Language Models (LLMs) using an assortment of annotated downstream vision-language datasets significantly enhances their performance. Yet, during this process, a side effect, which we termed as the \"multi-modal alignment tax\", surfaces. This side effect negatively impacts the model's ability to format responses appropriately -- for instance, its \"politeness\" -- due to the overly succinct and unformatted nature ",
    "arxiv_url": "https://arxiv.org/abs/2307.01003v2",
    "pdf_url": "https://arxiv.org/pdf/2307.01003v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2307.01003",
    "arxiv_authors": [
      "Delong Chen",
      "Jianfeng Liu",
      "Wenliang Dai",
      "Baoyuan Wang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Visual+Instruction+Tuning+with+Polite+Flamingo+Delong+Chen+Jianfeng+Liu+Wenliang+Dai+Baoyuan+Wang",
    "gs_search_success": true,
    "gs_authors": [
      "7PW095gAAAAJ",
      "OWa5rOEAAAAJ",
      "-_xy3jAAAAAJ"
    ],
    "citation_count": 59,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2312.00081",
    "title": "Synthesize, Diagnose, and Optimize: Towards Fine-Grained Vision-Language Understanding",
    "year": 2023,
    "published": "2023-11-30T03:20:37Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Vision language models (VLM) have demonstrated remarkable performance across various downstream tasks. However, understanding fine-grained visual-linguistic concepts, such as attributes and inter-object relationships, remains a significant challenge. While several benchmarks aim to evaluate VLMs in finer granularity, their primary focus remains on the linguistic aspect, neglecting the visual dimension. Here, we highlight the importance of evaluating VLMs from both a textual and visual perspectiv",
    "arxiv_url": "https://arxiv.org/abs/2312.00081v2",
    "pdf_url": "https://arxiv.org/pdf/2312.00081v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2312.00081",
    "arxiv_authors": [
      "Wujian Peng",
      "Sicheng Xie",
      "Zuyao You",
      "Shiyi Lan",
      "Zuxuan Wu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Synthesize%2C+Diagnose%2C+and+Optimize%3A+Towards+Fine-Grained+Vision-Language+Understanding+Wujian+Peng+Sicheng+Xie+Zuyao+You+Shiyi+Lan+Zuxuan+Wu",
    "gs_search_success": true,
    "gs_authors": [
      "X8Kh8uoAAAAJ",
      "7t12hVkAAAAJ",
      "GTuWk9YAAAAJ"
    ],
    "citation_count": 40,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2505.13817",
    "title": "InstanceBEV: Unifying Instance and BEV Representation for 3D Panoptic Segmentation",
    "year": 2025,
    "published": "2025-05-20T01:56:50Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "BEV-based 3D perception has emerged as a focal point of research in end-to-end autonomous driving. However, existing BEV approaches encounter significant challenges due to the large feature space, complicating efficient modeling and hindering effective integration of global attention mechanisms. We propose a novel modeling strategy, called InstanceBEV, that synergistically combines the strengths of both map-centric approaches and object-centric approaches. Our method effectively extracts instanc",
    "arxiv_url": "https://arxiv.org/abs/2505.13817v2",
    "pdf_url": "https://arxiv.org/pdf/2505.13817v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2505.13817",
    "arxiv_authors": [
      "Feng Li",
      "Zhaoyue Wang",
      "Enyuan Zhang",
      "Mohammad Masum Billah",
      "Yunduan Cui",
      "Kun Xu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=InstanceBEV%3A+Unifying+Instance+and+BEV+Representation+for+3D+Panoptic+Segmentation+Feng+Li+Zhaoyue+Wang+Enyuan+Zhang+Mohammad+Masum+Billah+Yunduan+Cui",
    "gs_search_success": true,
    "gs_authors": [
      "qUiCC-0AAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2412.06248",
    "title": "Rendering-Refined Stable Diffusion for Privacy Compliant Synthetic Data",
    "year": 2024,
    "published": "2024-12-09T06:47:29Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Growing privacy concerns and regulations like GDPR and CCPA necessitate pseudonymization techniques that protect identity in image datasets. However, retaining utility is also essential. Traditional methods like masking and blurring degrade quality and obscure critical context, especially in human-centric images. We introduce Rendering-Refined Stable Diffusion (RefSD), a pipeline that combines 3D-rendering with Stable Diffusion, enabling prompt-based control over human attributes while preservin",
    "arxiv_url": "https://arxiv.org/abs/2412.06248v1",
    "pdf_url": "https://arxiv.org/pdf/2412.06248v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2412.06248",
    "arxiv_authors": [
      "Kartik Patwari",
      "David Schneider",
      "Xiaoxiao Sun",
      "Chen-Nee Chuah",
      "Lingjuan Lyu",
      "Vivek Sharma"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Rendering-Refined+Stable+Diffusion+for+Privacy+Compliant+Synthetic+Data+Kartik+Patwari+David+Schneider+Xiaoxiao+Sun+Chen-Nee+Chuah+Lingjuan+Lyu",
    "gs_search_success": true,
    "gs_authors": [
      "0Om30ZUAAAAJ",
      "fNbVXwQAAAAJ",
      "T0t-GW4AAAAJ",
      "RZp_kd0AAAAJ",
      "bZNRLNAAAAAJ",
      "1oCrd64AAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2406.16231",
    "title": "Gradual Divergence for Seamless Adaptation: A Novel Domain Incremental Learning Method",
    "year": 2024,
    "published": "2024-06-23T22:05:52Z",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "abstract": "Domain incremental learning (DIL) poses a significant challenge in real-world scenarios, as models need to be sequentially trained on diverse domains over time, all the while avoiding catastrophic forgetting. Mitigating representation drift, which refers to the phenomenon of learned representations undergoing changes as the model adapts to new tasks, can help alleviate catastrophic forgetting. In this study, we propose a novel DIL method named DARE, featuring a three-stage training process: Dive",
    "arxiv_url": "https://arxiv.org/abs/2406.16231v1",
    "pdf_url": "https://arxiv.org/pdf/2406.16231v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2406.16231",
    "arxiv_authors": [
      "Kishaan Jeeveswaran",
      "Elahe Arani",
      "Bahram Zonooz"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Gradual+Divergence+for+Seamless+Adaptation%3A+A+Novel+Domain+Incremental+Learning+Method+Kishaan+Jeeveswaran+Elahe+Arani+Bahram+Zonooz",
    "gs_search_success": true,
    "gs_authors": [
      "JcqW3_QAAAAJ",
      "e_I_v6cAAAAJ",
      "FZmIlY8AAAAJ"
    ],
    "citation_count": 9,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2504.10834",
    "title": "LightFormer: A lightweight and efficient decoder for remote sensing image segmentation",
    "year": 2025,
    "published": "2025-04-15T03:25:39Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Deep learning techniques have achieved remarkable success in the semantic segmentation of remote sensing images and in land-use change detection. Nevertheless, their real-time deployment on edge platforms remains constrained by decoder complexity. Herein, we introduce LightFormer, a lightweight decoder for time-critical tasks that involve unstructured targets, such as disaster assessment, unmanned aerial vehicle search-and-rescue, and cultural heritage monitoring. LightFormer employs a feature-f",
    "arxiv_url": "https://arxiv.org/abs/2504.10834v1",
    "pdf_url": "https://arxiv.org/pdf/2504.10834v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2504.10834",
    "arxiv_authors": [
      "Sihang Chen",
      "Lijun Yun",
      "Ze Liu",
      "JianFeng Zhu",
      "Jie Chen",
      "Hui Wang",
      "Yueping Nie"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=LightFormer%3A+A+lightweight+and+efficient+decoder+for+remote+sensing+image+segmentation+Sihang+Chen+Lijun+Yun+Ze+Liu+JianFeng+Zhu+Jie+Chen",
    "gs_search_success": true,
    "gs_authors": [],
    "citation_count": 2,
    "gs_author_count": 7
  },
  {
    "arxiv_id": "2407.12697",
    "title": "Calibrated Diverse Ensemble Entropy Minimization for Robust Test-Time Adaptation in Prostate Cancer Detection",
    "year": 2024,
    "published": "2024-07-17T16:20:10Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "High resolution micro-ultrasound has demonstrated promise in real-time prostate cancer detection, with deep learning becoming a prominent tool for learning complex tissue properties reflected on ultrasound. However, a significant roadblock to real-world deployment remains, which prior works often overlook: model performance suffers when applied to data from different clinical centers due to variations in data distribution. This distribution shift significantly impacts the model's robustness, pos",
    "arxiv_url": "https://arxiv.org/abs/2407.12697v1",
    "pdf_url": "https://arxiv.org/pdf/2407.12697v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2407.12697",
    "arxiv_authors": [
      "Mahdi Gilany",
      "Mohamed Harmanani",
      "Paul Wilson",
      "Minh Nguyen Nhat To",
      "Amoon Jamzad",
      "Fahimeh Fooladgar",
      "Brian Wodlinger",
      "Purang Abolmaesumi",
      "Parvin Mousavi"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Calibrated+Diverse+Ensemble+Entropy+Minimization+for+Robust+Test-Time+Adaptation+in+Prostate+Cancer+Detection+Mahdi+Gilany+Mohamed+Harmanani+Paul+Wilson+Minh+Nguyen+Nhat+To+Amoon+Jamzad",
    "gs_search_success": true,
    "gs_authors": [
      "StQV08sAAAAJ",
      "njpAOrEAAAAJ",
      "kDY5yGQAAAAJ",
      "dNDG5CgAAAAJ",
      "UITl6ysAAAAJ",
      "-oSQmxwAAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 7
  },
  {
    "arxiv_id": "2410.05410",
    "title": "Enhanced Super-Resolution Training via Mimicked Alignment for Real-World Scenes",
    "year": 2024,
    "published": "2024-10-07T18:18:54Z",
    "categories": [
      "cs.CV",
      "eess.IV"
    ],
    "abstract": "Image super-resolution methods have made significant strides with deep learning techniques and ample training data. However, they face challenges due to inherent misalignment between low-resolution (LR) and high-resolution (HR) pairs in real-world datasets. In this study, we propose a novel plug-and-play module designed to mitigate these misalignment issues by aligning LR inputs with HR images during training. Specifically, our approach involves mimicking a novel LR sample that aligns with HR wh",
    "arxiv_url": "https://arxiv.org/abs/2410.05410v1",
    "pdf_url": "https://arxiv.org/pdf/2410.05410v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2410.05410",
    "arxiv_authors": [
      "Omar Elezabi",
      "Zongwei Wu",
      "Radu Timofte"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Enhanced+Super-Resolution+Training+via+Mimicked+Alignment+for+Real-World+Scenes+Omar+Elezabi+Zongwei+Wu+Radu+Timofte",
    "gs_search_success": true,
    "gs_authors": [
      "3QSALjX498QC",
      "8v3dYzEAAAAJ",
      "u3MwH5kAAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2303.14190",
    "title": "WildLight: In-the-wild Inverse Rendering with a Flashlight",
    "year": 2023,
    "published": "2023-03-24T17:59:56Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "This paper proposes a practical photometric solution for the challenging problem of in-the-wild inverse rendering under unknown ambient lighting. Our system recovers scene geometry and reflectance using only multi-view images captured by a smartphone. The key idea is to exploit smartphone's built-in flashlight as a minimally controlled light source, and decompose image intensities into two photometric components -- a static appearance corresponds to ambient flux, plus a dynamic reflection induce",
    "arxiv_url": "https://arxiv.org/abs/2303.14190v1",
    "pdf_url": "https://arxiv.org/pdf/2303.14190v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2303.14190",
    "arxiv_authors": [
      "Ziang Cheng",
      "Junxuan Li",
      "Hongdong Li"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=WildLight%3A+In-the-wild+Inverse+Rendering+with+a+Flashlight+Ziang+Cheng+Junxuan+Li+Hongdong+Li",
    "gs_search_success": true,
    "gs_authors": [
      "b2_zvDMAAAAJ",
      "Mq89JAcAAAAJ"
    ],
    "citation_count": 23,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2312.15133",
    "title": "Learning Continuous Implicit Field with Local Distance Indicator for Arbitrary-Scale Point Cloud Upsampling",
    "year": 2023,
    "published": "2023-12-23T01:52:14Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Point cloud upsampling aims to generate dense and uniformly distributed point sets from a sparse point cloud, which plays a critical role in 3D computer vision. Previous methods typically split a sparse point cloud into several local patches, upsample patch points, and merge all upsampled patches. However, these methods often produce holes, outliers or nonuniformity due to the splitting and merging process which does not maintain consistency among local patches. To address these issues, we propo",
    "arxiv_url": "https://arxiv.org/abs/2312.15133v1",
    "pdf_url": "https://arxiv.org/pdf/2312.15133v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2312.15133",
    "arxiv_authors": [
      "Shujuan Li",
      "Junsheng Zhou",
      "Baorui Ma",
      "Yu-Shen Liu",
      "Zhizhong Han"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Learning+Continuous+Implicit+Field+with+Local+Distance+Indicator+for+Arbitrary-Scale+Point+Cloud+Upsampling+Shujuan+Li+Junsheng+Zhou+Baorui+Ma+Yu-Shen+Liu+Zhizhong+Han",
    "gs_search_success": true,
    "gs_authors": [
      "RGNWczEAAAAJ",
      "Vo4ZHu0AAAAJ",
      "afPIrLYAAAAJ",
      "rmgXhrEAAAAJ",
      "wynhSuQAAAAJ"
    ],
    "citation_count": 34,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2311.10472",
    "title": "End-to-end autoencoding architecture for the simultaneous generation of medical images and corresponding segmentation masks",
    "year": 2023,
    "published": "2023-11-17T11:56:53Z",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "abstract": "Despite the increasing use of deep learning in medical image segmentation, acquiring sufficient training data remains a challenge in the medical field. In response, data augmentation techniques have been proposed; however, the generation of diverse and realistic medical images and their corresponding masks remains a difficult task, especially when working with insufficient training sets. To address these limitations, we present an end-to-end architecture based on the Hamiltonian Variational Auto",
    "arxiv_url": "https://arxiv.org/abs/2311.10472v1",
    "pdf_url": "https://arxiv.org/pdf/2311.10472v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2311.10472",
    "arxiv_authors": [
      "Aghiles Kebaili",
      "Jérôme Lapuyade-Lahorgue",
      "Pierre Vera",
      "Su Ruan"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=End-to-end+autoencoding+architecture+for+the+simultaneous+generation+of+medical+images+and+corresponding+segmentation+masks+Aghiles+Kebaili+J%C3%A9r%C3%B4me+Lapuyade-Lahorgue+Pierre+Vera+Su+Ruan",
    "gs_search_success": true,
    "gs_authors": [
      "Sp3Q6LQAAAAJ"
    ],
    "citation_count": 8,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2305.12844",
    "title": "An Optimized Ensemble Deep Learning Model For Brain Tumor Classification",
    "year": 2023,
    "published": "2023-05-22T09:08:59Z",
    "categories": [
      "eess.IV",
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "Brain tumors present a grave risk to human life, demanding precise and timely diagnosis for effective treatment. Inaccurate identification of brain tumors can significantly diminish life expectancy, underscoring the critical need for precise diagnostic methods. Manual identification of brain tumors within vast Magnetic Resonance Imaging (MRI) image datasets is arduous and time-consuming. Thus, the development of a reliable deep learning (DL) model is essential to enhance diagnostic accuracy and ",
    "arxiv_url": "https://arxiv.org/abs/2305.12844v3",
    "pdf_url": "https://arxiv.org/pdf/2305.12844v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2305.12844",
    "arxiv_authors": [
      "Md. Alamin Talukder",
      "Md. Manowarul Islam",
      "Md Ashraf Uddin"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=An+Optimized+Ensemble+Deep+Learning+Model+For+Brain+Tumor+Classification+Md.+Alamin+Talukder+Md.+Manowarul+Islam+Md+Ashraf+Uddin",
    "gs_search_success": true,
    "gs_authors": [
      "MBF7Y_4AAAAJ",
      "dmrKY2wAAAAJ",
      "ub1j2jYAAAAJ"
    ],
    "citation_count": 6,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2403.18116",
    "title": "QuakeSet: A Dataset and Low-Resource Models to Monitor Earthquakes through Sentinel-1",
    "year": 2024,
    "published": "2024-03-26T21:45:29Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Earthquake monitoring is necessary to promptly identify the affected areas, the severity of the events, and, finally, to estimate damages and plan the actions needed for the restoration process. The use of seismic stations to monitor the strength and origin of earthquakes is limited when dealing with remote areas (we cannot have global capillary coverage). Identification and analysis of all affected areas is mandatory to support areas not monitored by traditional stations. Using social media ima",
    "arxiv_url": "https://arxiv.org/abs/2403.18116v1",
    "pdf_url": "https://arxiv.org/pdf/2403.18116v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2403.18116",
    "arxiv_authors": [
      "Daniele Rege Cambrin",
      "Paolo Garza"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=QuakeSet%3A+A+Dataset+and+Low-Resource+Models+to+Monitor+Earthquakes+through+Sentinel-1+Daniele+Rege+Cambrin+Paolo+Garza",
    "gs_search_success": true,
    "gs_authors": [
      "w9dI8esAAAAJ",
      "7m0N4zUAAAAJ"
    ],
    "citation_count": 11,
    "gs_author_count": null
  },
  {
    "arxiv_id": "2407.12808",
    "title": "Towards Optimal Trade-offs in Knowledge Distillation for CNNs and Vision Transformers at the Edge",
    "year": 2024,
    "published": "2024-06-25T16:15:02Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "This paper discusses four facets of the Knowledge Distillation (KD) process for Convolutional Neural Networks (CNNs) and Vision Transformer (ViT) architectures, particularly when executed on edge devices with constrained processing capabilities. First, we conduct a comparative analysis of the KD process between CNNs and ViT architectures, aiming to elucidate the feasibility and efficacy of employing different architectural configurations for the teacher and student, while assessing their perform",
    "arxiv_url": "https://arxiv.org/abs/2407.12808v1",
    "pdf_url": "https://arxiv.org/pdf/2407.12808v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2407.12808",
    "arxiv_authors": [
      "John Violos",
      "Symeon Papadopoulos",
      "Ioannis Kompatsiaris"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Towards+Optimal+Trade-offs+in+Knowledge+Distillation+for+CNNs+and+Vision+Transformers+at+the+Edge+John+Violos+Symeon+Papadopoulos+Ioannis+Kompatsiaris",
    "gs_search_success": true,
    "gs_authors": [
      "65UYrBQAAAAJ",
      "Nr7smP8AAAAJ",
      "GuhyORoAAAAJ"
    ],
    "citation_count": 7,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2411.15539",
    "title": "Large Language Model with Region-guided Referring and Grounding for CT Report Generation",
    "year": 2024,
    "published": "2024-11-23T12:25:06Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Computed tomography (CT) report generation is crucial to assist radiologists in interpreting CT volumes, which can be time-consuming and labor-intensive. Existing methods primarily only consider the global features of the entire volume, making it struggle to focus on specific regions and potentially missing abnormalities. To address this issue, we propose Reg2RG, the first region-guided referring and grounding framework for CT report generation, which enhances diagnostic performance by focusing ",
    "arxiv_url": "https://arxiv.org/abs/2411.15539v2",
    "pdf_url": "https://arxiv.org/pdf/2411.15539v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2411.15539",
    "arxiv_authors": [
      "Zhixuan Chen",
      "Yequan Bie",
      "Haibo Jin",
      "Hao Chen"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Large+Language+Model+with+Region-guided+Referring+and+Grounding+for+CT+Report+Generation+Zhixuan+Chen+Yequan+Bie+Haibo+Jin+Hao+Chen",
    "gs_search_success": true,
    "gs_authors": [
      "Z_t5DjwAAAAJ",
      "7AMNEKAAAAAJ",
      "sptbC3EAAAAJ"
    ],
    "citation_count": 15,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2410.19378",
    "title": "Unified Cross-Modal Medical Image Synthesis with Hierarchical Mixture of Product-of-Experts",
    "year": 2024,
    "published": "2024-10-25T08:30:29Z",
    "categories": [
      "cs.CV",
      "cs.LG",
      "eess.IV"
    ],
    "abstract": "We propose a deep mixture of multimodal hierarchical variational auto-encoders called MMHVAE that synthesizes missing images from observed images in different modalities. MMHVAE's design focuses on tackling four challenges: (i) creating a complex latent representation of multimodal data to generate high-resolution images; (ii) encouraging the variational distributions to estimate the missing information needed for cross-modal image synthesis; (iii) learning to fuse multimodal information in the ",
    "arxiv_url": "https://arxiv.org/abs/2410.19378v3",
    "pdf_url": "https://arxiv.org/pdf/2410.19378v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2410.19378",
    "arxiv_authors": [
      "Reuben Dorent",
      "Nazim Haouchine",
      "Alexandra Golby",
      "Sarah Frisken",
      "Tina Kapur",
      "William Wells"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Unified+Cross-Modal+Medical+Image+Synthesis+with+Hierarchical+Mixture+of+Product-of-Experts+Reuben+Dorent+Nazim+Haouchine+Alexandra+Golby+Sarah+Frisken+Tina+Kapur",
    "gs_search_success": true,
    "gs_authors": [
      "PjpzomsAAAAJ",
      "3AFVSiYAAAAJ",
      "Cl3W1LEAAAAJ",
      "xdECLMkAAAAJ",
      "YeOfp-YAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2405.07194",
    "title": "Differentiable Model Scaling using Differentiable Topk",
    "year": 2024,
    "published": "2024-05-12T07:34:33Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Over the past few years, as large language models have ushered in an era of intelligence emergence, there has been an intensified focus on scaling networks. Currently, many network architectures are designed manually, often resulting in sub-optimal configurations. Although Neural Architecture Search (NAS) methods have been proposed to automate this process, they suffer from low search efficiency. This study introduces Differentiable Model Scaling (DMS), increasing the efficiency for searching op",
    "arxiv_url": "https://arxiv.org/abs/2405.07194v1",
    "pdf_url": "https://arxiv.org/pdf/2405.07194v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2405.07194",
    "arxiv_authors": [
      "Kai Liu",
      "Ruohui Wang",
      "Jianfei Gao",
      "Kai Chen"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Differentiable+Model+Scaling+using+Differentiable+Topk+Kai+Liu+Ruohui+Wang+Jianfei+Gao+Kai+Chen",
    "gs_search_success": true,
    "gs_authors": [
      "COUqsT4AAAAJ",
      "G-9-nIYAAAAJ",
      "b6NWTbAAAAAJ",
      "eGD0b7IAAAAJ"
    ],
    "citation_count": 5,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2309.05314",
    "title": "Semantic Latent Decomposition with Normalizing Flows for Face Editing",
    "year": 2023,
    "published": "2023-09-11T08:59:15Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Navigating in the latent space of StyleGAN has shown effectiveness for face editing. However, the resulting methods usually encounter challenges in complicated navigation due to the entanglement among different attributes in the latent space. To address this issue, this paper proposes a novel framework, termed SDFlow, with a semantic decomposition in original latent space using continuous conditional normalizing flows. Specifically, SDFlow decomposes the original latent code into different irrel",
    "arxiv_url": "https://arxiv.org/abs/2309.05314v1",
    "pdf_url": "https://arxiv.org/pdf/2309.05314v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2309.05314",
    "arxiv_authors": [
      "Binglei Li",
      "Zhizhong Huang",
      "Hongming Shan",
      "Junping Zhang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Semantic+Latent+Decomposition+with+Normalizing+Flows+for+Face+Editing+Binglei+Li+Zhizhong+Huang+Hongming+Shan+Junping+Zhang",
    "gs_search_success": true,
    "gs_authors": [
      "6Itl2tMAAAAJ",
      "Aib_NTYAAAAJ",
      "RYfSzKwAAAAJ"
    ],
    "citation_count": 5,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2404.05105",
    "title": "VMambaMorph: a Multi-Modality Deformable Image Registration Framework based on Visual State Space Model with Cross-Scan Module",
    "year": 2024,
    "published": "2024-04-07T23:10:26Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Image registration, a critical process in medical imaging, involves aligning different sets of medical imaging data into a single unified coordinate system. Deep learning networks, such as the Convolutional Neural Network (CNN)-based VoxelMorph, Vision Transformer (ViT)-based TransMorph, and State Space Model (SSM)-based MambaMorph, have demonstrated effective performance in this domain. The recent Visual State Space Model (VMamba), which incorporates a cross-scan module with SSM, has exhibited ",
    "arxiv_url": "https://arxiv.org/abs/2404.05105v2",
    "pdf_url": "https://arxiv.org/pdf/2404.05105v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2404.05105",
    "arxiv_authors": [
      "Ziyang Wang",
      "Jian-Qing Zheng",
      "Chao Ma",
      "Tao Guo"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=VMambaMorph%3A+a+Multi-Modality+Deformable+Image+Registration+Framework+based+on+Visual+State+Space+Model+with+Cross-Scan+Module+Ziyang+Wang+Jian-Qing+Zheng+Chao+Ma+Tao+Guo",
    "gs_search_success": true,
    "gs_authors": [
      "GWF20_wAAAAJ",
      "2bNsYR0AAAAJ"
    ],
    "citation_count": 14,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2406.01196",
    "title": "3D WholeBody Pose Estimation based on Semantic Graph Attention Network and Distance Information",
    "year": 2024,
    "published": "2024-06-03T10:59:00Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "In recent years, a plethora of diverse methods have been proposed for 3D pose estimation. Among these, self-attention mechanisms and graph convolutions have both been proven to be effective and practical methods. Recognizing the strengths of those two techniques, we have developed a novel Semantic Graph Attention Network which can benefit from the ability of self-attention to capture global context, while also utilizing the graph convolutions to handle the local connectivity and structural const",
    "arxiv_url": "https://arxiv.org/abs/2406.01196v1",
    "pdf_url": "https://arxiv.org/pdf/2406.01196v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2406.01196",
    "arxiv_authors": [
      "Sihan Wen",
      "Xiantan Zhu",
      "Zhiming Tan"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=3D+WholeBody+Pose+Estimation+based+on+Semantic+Graph+Attention+Network+and+Distance+Information+Sihan+Wen+Xiantan+Zhu+Zhiming+Tan",
    "gs_search_success": true,
    "gs_authors": [],
    "citation_count": 3,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2503.11115",
    "title": "Solution for 8th Competition on Affective & Behavior Analysis in-the-wild",
    "year": 2025,
    "published": "2025-03-14T06:26:55Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "In this report, we present our solution for the Action Unit (AU) Detection Challenge, in 8th Competition on Affective Behavior Analysis in-the-wild. In order to achieve robust and accurate classification of facial action unit in the wild environment, we introduce an innovative method that leverages audio-visual multimodal data. Our method employs ConvNeXt as the image encoder and uses Whisper to extract Mel spectrogram features. For these features, we utilize a Transformer encoder-based feature ",
    "arxiv_url": "https://arxiv.org/abs/2503.11115v1",
    "pdf_url": "https://arxiv.org/pdf/2503.11115v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2503.11115",
    "arxiv_authors": [
      "Jun Yu",
      "Yunxiang Zhang",
      "Xilong Lu",
      "Yang Zheng",
      "Yongqi Wang",
      "Lingsi Zhu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Solution+for+8th+Competition+on+Affective+%26+Behavior+Analysis+in-the-wild+Jun+Yu+Yunxiang+Zhang+Xilong+Lu+Yang+Zheng+Yongqi+Wang",
    "gs_search_success": true,
    "gs_authors": [],
    "citation_count": 2,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2501.02640",
    "title": "Multispectral Pedestrian Detection with Sparsely Annotated Label",
    "year": 2025,
    "published": "2025-01-05T20:05:10Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Although existing Sparsely Annotated Object Detection (SAOD) approches have made progress in handling sparsely annotated environments in multispectral domain, where only some pedestrians are annotated, they still have the following limitations: (i) they lack considerations for improving the quality of pseudo-labels for missing annotations, and (ii) they rely on fixed ground truth annotations, which leads to learning only a limited range of pedestrian visual appearances in the multispectral domai",
    "arxiv_url": "https://arxiv.org/abs/2501.02640v3",
    "pdf_url": "https://arxiv.org/pdf/2501.02640v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2501.02640",
    "arxiv_authors": [
      "Chan Lee",
      "Seungho Shin",
      "Gyeong-Moon Park",
      "Jung Uk Kim"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Multispectral+Pedestrian+Detection+with+Sparsely+Annotated+Label+Chan+Lee+Seungho+Shin+Gyeong-Moon+Park+Jung+Uk+Kim",
    "gs_search_success": true,
    "gs_authors": [
      "LNYugtYAAAAJ",
      "Sz6rfOMAAAAJ",
      "GxgmfkEAAAAJ",
      "JMZ80R8AAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2503.03987",
    "title": "RetinalGPT: A Retinal Clinical Preference Conversational Assistant Powered by Large Vision-Language Models",
    "year": 2025,
    "published": "2025-03-06T00:19:54Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "abstract": "Recently, Multimodal Large Language Models (MLLMs) have gained significant attention for their remarkable ability to process and analyze non-textual data, such as images, videos, and audio. Notably, several adaptations of general-domain MLLMs to the medical field have been explored, including LLaVA-Med. However, these medical adaptations remain insufficiently advanced in understanding and interpreting retinal images. In contrast, medical experts emphasize the importance of quantitative analyses ",
    "arxiv_url": "https://arxiv.org/abs/2503.03987v1",
    "pdf_url": "https://arxiv.org/pdf/2503.03987v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2503.03987",
    "arxiv_authors": [
      "Wenhui Zhu",
      "Xin Li",
      "Xiwen Chen",
      "Peijie Qiu",
      "Vamsi Krishna Vasa",
      "Xuanzhao Dong",
      "Yanxi Chen",
      "Natasha Lepore",
      "Oana Dumitrascu",
      "Yi Su",
      "Yalin Wang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=RetinalGPT%3A+A+Retinal+Clinical+Preference+Conversational+Assistant+Powered+by+Large+Vision-Language+Models+Wenhui+Zhu+Xin+Li+Xiwen+Chen+Peijie+Qiu+Vamsi+Krishna+Vasa",
    "gs_search_success": true,
    "gs_authors": [
      "7HLmlHMAAAAJ",
      "4-jHQiIAAAAJ",
      "vdZKSEIAAAAJ",
      "4evGH5wAAAAJ",
      "e8FLeP8AAAAJ",
      "Oxc6RG8AAAAJ",
      "Se8aIO4YIp8C",
      "9jKJmcgAAAAJ"
    ],
    "citation_count": 5,
    "gs_author_count": 10
  },
  {
    "arxiv_id": "2405.11476",
    "title": "NubbleDrop: A Simple Way to Improve Matching Strategy for Prompted One-Shot Segmentation",
    "year": 2024,
    "published": "2024-05-19T08:00:38Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Driven by large data trained segmentation models, such as SAM , research in one-shot segmentation has experienced significant advancements. Recent contributions like PerSAM and MATCHER , presented at ICLR 2024, utilize a similar approach by leveraging SAM with one or a few reference images to generate high quality segmentation masks for target images. Specifically, they utilize raw encoded features to compute cosine similarity between patches within reference and target images along the channel ",
    "arxiv_url": "https://arxiv.org/abs/2405.11476v1",
    "pdf_url": "https://arxiv.org/pdf/2405.11476v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2405.11476",
    "arxiv_authors": [
      "Zhiyu Xu",
      "Qingliang Chen"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=NubbleDrop%3A+A+Simple+Way+to+Improve+Matching+Strategy+for+Prompted+One-Shot+Segmentation+Zhiyu+Xu+Qingliang+Chen",
    "gs_search_success": true,
    "gs_authors": [
      "BnZzl70AAAAJ",
      "RiXuQboAAAAJ"
    ],
    "citation_count": 2,
    "gs_author_count": 2
  },
  {
    "arxiv_id": "2411.15540",
    "title": "Optical-Flow Guided Prompt Optimization for Coherent Video Generation",
    "year": 2024,
    "published": "2024-11-23T12:26:52Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "eess.IV"
    ],
    "abstract": "While text-to-video diffusion models have made significant strides, many still face challenges in generating videos with temporal consistency. Within diffusion frameworks, guidance techniques have proven effective in enhancing output quality during inference; however, applying these methods to video diffusion models introduces additional complexity of handling computations across entire sequences. To address this, we propose a novel framework called MotionPrompt that guides the video generation ",
    "arxiv_url": "https://arxiv.org/abs/2411.15540v2",
    "pdf_url": "https://arxiv.org/pdf/2411.15540v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2411.15540",
    "arxiv_authors": [
      "Hyelin Nam",
      "Jaemin Kim",
      "Dohun Lee",
      "Jong Chul Ye"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Optical-Flow+Guided+Prompt+Optimization+for+Coherent+Video+Generation+Hyelin+Nam+Jaemin+Kim+Dohun+Lee+Jong+Chul+Ye",
    "gs_search_success": true,
    "gs_authors": [
      "vnzsr1UAAAAJ",
      "QWRf4vgAAAAJ",
      "k6nzq08AAAAJ",
      "HNMjoNEAAAAJ"
    ],
    "citation_count": 6,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2301.02703",
    "title": "RUPNet: Residual upsampling network for real-time polyp segmentation",
    "year": 2023,
    "published": "2023-01-06T20:21:37Z",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "abstract": "Colorectal cancer is among the most prevalent cause of cancer-related mortality worldwide. Detection and removal of polyps at an early stage can help reduce mortality and even help in spreading over adjacent organs. Early polyp detection could save the lives of millions of patients over the world as well as reduce the clinical burden. However, the detection polyp rate varies significantly among endoscopists. There is numerous deep learning-based method proposed, however, most of the studies impr",
    "arxiv_url": "https://arxiv.org/abs/2301.02703v2",
    "pdf_url": "https://arxiv.org/pdf/2301.02703v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2301.02703",
    "arxiv_authors": [
      "Nikhil Kumar Tomar",
      "Ulas Bagci",
      "Debesh Jha"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=RUPNet%3A+Residual+upsampling+network+for+real-time+polyp+segmentation+Nikhil+Kumar+Tomar+Ulas+Bagci+Debesh+Jha",
    "gs_search_success": true,
    "gs_authors": [
      "Lkuik04AAAAJ",
      "mMTyE68AAAAJ",
      "9LUdPM4AAAAJ"
    ],
    "citation_count": 3,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2306.10159",
    "title": "Vision-Language Models can Identify Distracted Driver Behavior from Naturalistic Videos",
    "year": 2023,
    "published": "2023-06-16T20:02:51Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Recognizing the activities causing distraction in real-world driving scenarios is critical for ensuring the safety and reliability of both drivers and pedestrians on the roadways. Conventional computer vision techniques are typically data-intensive and require a large volume of annotated training data to detect and classify various distracted driving behaviors, thereby limiting their efficiency and scalability. We aim to develop a generalized framework that showcases robust performance with acce",
    "arxiv_url": "https://arxiv.org/abs/2306.10159v4",
    "pdf_url": "https://arxiv.org/pdf/2306.10159v4",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2306.10159",
    "arxiv_authors": [
      "Md Zahid Hasan",
      "Jiajing Chen",
      "Jiyang Wang",
      "Mohammed Shaiqur Rahman",
      "Ameya Joshi",
      "Senem Velipasalar",
      "Chinmay Hegde",
      "Anuj Sharma",
      "Soumik Sarkar"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Vision-Language+Models+can+Identify+Distracted+Driver+Behavior+from+Naturalistic+Videos+Md+Zahid+Hasan+Jiajing+Chen+Jiyang+Wang+Mohammed+Shaiqur+Rahman+Ameya+Joshi",
    "gs_search_success": true,
    "gs_authors": [
      "NQnok3sAAAAJ",
      "35r1NpQAAAAJ",
      "dd4Q1LgAAAAJ"
    ],
    "citation_count": 53,
    "gs_author_count": null
  },
  {
    "arxiv_id": "2405.15151",
    "title": "NeB-SLAM: Neural Blocks-based Salable RGB-D SLAM for Unknown Scenes",
    "year": 2024,
    "published": "2024-05-24T02:11:45Z",
    "categories": [
      "cs.CV",
      "cs.GR",
      "cs.RO"
    ],
    "abstract": "Neural implicit representations have recently demonstrated considerable potential in the field of visual simultaneous localization and mapping (SLAM). This is due to their inherent advantages, including low storage overhead and representation continuity. However, these methods necessitate the size of the scene as input, which is impractical for unknown scenes. Consequently, we propose NeB-SLAM, a neural block-based scalable RGB-D SLAM for unknown scenes. Specifically, we first propose a divide-a",
    "arxiv_url": "https://arxiv.org/abs/2405.15151v2",
    "pdf_url": "https://arxiv.org/pdf/2405.15151v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2405.15151",
    "arxiv_authors": [
      "Lizhi Bai",
      "Chunqi Tian",
      "Jun Yang",
      "Siyu Zhang",
      "Weijian Liang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=NeB-SLAM%3A+Neural+Blocks-based+Salable+RGB-D+SLAM+for+Unknown+Scenes+Lizhi+Bai+Chunqi+Tian+Jun+Yang+Siyu+Zhang+Weijian+Liang",
    "gs_search_success": true,
    "gs_authors": [
      "UJA0DmsAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2304.11196",
    "title": "Fast GraspNeXt: A Fast Self-Attention Neural Network Architecture for Multi-task Learning in Computer Vision Tasks for Robotic Grasping on the Edge",
    "year": 2023,
    "published": "2023-04-21T18:07:14Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "abstract": "Multi-task learning has shown considerable promise for improving the performance of deep learning-driven vision systems for the purpose of robotic grasping. However, high architectural and computational complexity can result in poor suitability for deployment on embedded devices that are typically leveraged in robotic arms for real-world manufacturing and warehouse environments. As such, the design of highly efficient multi-task deep neural network architectures tailored for computer vision task",
    "arxiv_url": "https://arxiv.org/abs/2304.11196v1",
    "pdf_url": "https://arxiv.org/pdf/2304.11196v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2304.11196",
    "arxiv_authors": [
      "Alexander Wong",
      "Yifan Wu",
      "Saad Abbasi",
      "Saeejith Nair",
      "Yuhao Chen",
      "Mohammad Javad Shafiee"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Fast+GraspNeXt%3A+A+Fast+Self-Attention+Neural+Network+Architecture+for+Multi-task+Learning+in+Computer+Vision+Tasks+for+Robotic+Grasping+on+the+Edge+Alexander+Wong+Yifan+Wu+Saad+Abbasi+Saeejith+Nair+Yuhao+Chen",
    "gs_search_success": true,
    "gs_authors": [
      "HkNlM6oAAAAJ",
      "c1h8n9sAAAAJ",
      "ZdqUd0AAAAAJ",
      "NwxXuCYAAAAJ",
      "i7h8-dYAAAAJ",
      "vEMBM2MAAAAJ"
    ],
    "citation_count": 15,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2411.17788",
    "title": "Geometric Point Attention Transformer for 3D Shape Reassembly",
    "year": 2024,
    "published": "2024-11-26T15:29:38Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "abstract": "Shape assembly, which aims to reassemble separate parts into a complete object, has gained significant interest in recent years. Existing methods primarily rely on networks to predict the poses of individual parts, but often fail to effectively capture the geometric interactions between the parts and their poses. In this paper, we present the Geometric Point Attention Transformer (GPAT), a network specifically designed to address the challenges of reasoning about geometric relationships. In the ",
    "arxiv_url": "https://arxiv.org/abs/2411.17788v2",
    "pdf_url": "https://arxiv.org/pdf/2411.17788v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2411.17788",
    "arxiv_authors": [
      "Jiahan Li",
      "Chaoran Cheng",
      "Jianzhu Ma",
      "Ge Liu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Geometric+Point+Attention+Transformer+for+3D+Shape+Reassembly+Jiahan+Li+Chaoran+Cheng+Jianzhu+Ma+Ge+Liu",
    "gs_search_success": true,
    "gs_authors": [
      "SrGZZ1wAAAAJ",
      "P6EahzcAAAAJ",
      "a3bwLKMAAAAJ",
      "AATzYuAAAAAJ"
    ],
    "citation_count": 2,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2403.14429",
    "title": "Style-Extracting Diffusion Models for Semi-Supervised Histopathology Segmentation",
    "year": 2024,
    "published": "2024-03-21T14:36:59Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "abstract": "Deep learning-based image generation has seen significant advancements with diffusion models, notably improving the quality of generated images. Despite these developments, generating images with unseen characteristics beneficial for downstream tasks has received limited attention. To bridge this gap, we propose Style-Extracting Diffusion Models, featuring two conditioning mechanisms. Specifically, we utilize 1) a style conditioning mechanism which allows to inject style information of previousl",
    "arxiv_url": "https://arxiv.org/abs/2403.14429v1",
    "pdf_url": "https://arxiv.org/pdf/2403.14429v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2403.14429",
    "arxiv_authors": [
      "Mathias Öttl",
      "Frauke Wilm",
      "Jana Steenpass",
      "Jingna Qiu",
      "Matthias Rübner",
      "Arndt Hartmann",
      "Matthias Beckmann",
      "Peter Fasching",
      "Andreas Maier",
      "Ramona Erber",
      "Bernhard Kainz",
      "Katharina Breininger"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Style-Extracting+Diffusion+Models+for+Semi-Supervised+Histopathology+Segmentation+Mathias+%C3%96ttl+Frauke+Wilm+Jana+Steenpass+Jingna+Qiu+Matthias+R%C3%BCbner",
    "gs_search_success": true,
    "gs_authors": [
      "G2vG2HwAAAAJ",
      "DOCBaYkAAAAJ",
      "nwdOfAMAAAAJ",
      "F9YYexgAAAAJ"
    ],
    "citation_count": 5,
    "gs_author_count": 8
  },
  {
    "arxiv_id": "2309.06747",
    "title": "Integrating GAN and Texture Synthesis for Enhanced Road Damage Detection",
    "year": 2023,
    "published": "2023-09-13T06:38:51Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "In the domain of traffic safety and road maintenance, precise detection of road damage is crucial for ensuring safe driving and prolonging road durability. However, current methods often fall short due to limited data. Prior attempts have used Generative Adversarial Networks to generate damage with diverse shapes and manually integrate it into appropriate positions. However, the problem has not been well explored and is faced with two challenges. First, they only enrich the location and shape of",
    "arxiv_url": "https://arxiv.org/abs/2309.06747v2",
    "pdf_url": "https://arxiv.org/pdf/2309.06747v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2309.06747",
    "arxiv_authors": [
      "Tengyang Chen",
      "Jiangtao Ren"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Integrating+GAN+and+Texture+Synthesis+for+Enhanced+Road+Damage+Detection+Tengyang+Chen+Jiangtao+Ren",
    "gs_search_success": true,
    "gs_authors": [
      "4nTlW_YAAAAJ"
    ],
    "citation_count": 11,
    "gs_author_count": 2
  },
  {
    "arxiv_id": "2412.15209",
    "title": "PRIMA: Multi-Image Vision-Language Models for Reasoning Segmentation",
    "year": 2024,
    "published": "2024-12-19T18:59:44Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "abstract": "Despite significant advancements in Large Vision-Language Models (LVLMs), existing pixel-grounding models operate on single-image settings, limiting their ability to perform detailed, fine-grained comparisons across multiple images. Conversely, current multi-image understanding models lack pixel-level grounding. Our work addresses this gap by introducing the task of multi-image pixel-grounded reasoning segmentation, and PRIMA, a novel LVLM that integrates pixel-level grounding with robust multi-",
    "arxiv_url": "https://arxiv.org/abs/2412.15209v1",
    "pdf_url": "https://arxiv.org/pdf/2412.15209v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2412.15209",
    "arxiv_authors": [
      "Muntasir Wahed",
      "Kiet A. Nguyen",
      "Adheesh Sunil Juvekar",
      "Xinzhuo Li",
      "Xiaona Zhou",
      "Vedant Shah",
      "Tianjiao Yu",
      "Pinar Yanardag",
      "Ismini Lourentzou"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=PRIMA%3A+Multi-Image+Vision-Language+Models+for+Reasoning+Segmentation+Muntasir+Wahed+Kiet+A.+Nguyen+Adheesh+Sunil+Juvekar+Xinzhuo+Li+Xiaona+Zhou",
    "gs_search_success": true,
    "gs_authors": [
      "dVDyCTIAAAAJ",
      "szV_wkoAAAAJ",
      "Lv_qA9gAAAAJ",
      "AmsXr58AAAAJ",
      "ItE5a5cAAAAJ",
      "qzczdd8AAAAJ",
      "CjxQvikAAAAJ",
      "TgdQrLwAAAAJ",
      "a1nxSRYAAAAJ"
    ],
    "citation_count": 4,
    "gs_author_count": 9
  },
  {
    "arxiv_id": "2502.20749",
    "title": "SemiSAM+: Rethinking Semi-Supervised Medical Image Segmentation in the Era of Foundation Models",
    "year": 2025,
    "published": "2025-02-28T05:54:41Z",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "abstract": "Deep learning-based medical image segmentation typically requires large amount of labeled data for training, making it less applicable in clinical settings due to high annotation cost. Semi-supervised learning (SSL) has emerged as an appealing strategy due to its less dependence on acquiring abundant annotations from experts compared to fully supervised methods. Beyond existing model-centric advancements of SSL by designing novel regularization strategies, we anticipate a paradigmatic shift due ",
    "arxiv_url": "https://arxiv.org/abs/2502.20749v1",
    "pdf_url": "https://arxiv.org/pdf/2502.20749v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2502.20749",
    "arxiv_authors": [
      "Yichi Zhang",
      "Bohao Lv",
      "Le Xue",
      "Wenbo Zhang",
      "Yuchen Liu",
      "Yu Fu",
      "Yuan Cheng",
      "Yuan Qi"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=SemiSAM%2B%3A+Rethinking+Semi-Supervised+Medical+Image+Segmentation+in+the+Era+of+Foundation+Models+Yichi+Zhang+Bohao+Lv+Le+Xue+Wenbo+Zhang+Yuchen+Liu",
    "gs_search_success": true,
    "gs_authors": [
      "0qlu9sIAAAAJ",
      "ThuuigQAAAAJ",
      "nVOvhVQAAAAJ",
      "PfmSvNMAAAAJ"
    ],
    "citation_count": 5,
    "gs_author_count": 8
  },
  {
    "arxiv_id": "2305.10126",
    "title": "Fusion-S2iGan: An Efficient and Effective Single-Stage Framework for Speech-to-Image Generation",
    "year": 2023,
    "published": "2023-05-17T11:12:07Z",
    "categories": [
      "cs.CV",
      "cs.MM"
    ],
    "abstract": "The goal of a speech-to-image transform is to produce a photo-realistic picture directly from a speech signal. Recently, various studies have focused on this task and have achieved promising performance. However, current speech-to-image approaches are based on a stacked modular framework that suffers from three vital issues: 1) Training separate networks is time-consuming as well as inefficient and the convergence of the final generative model strongly depends on the previous generators; 2) The ",
    "arxiv_url": "https://arxiv.org/abs/2305.10126v1",
    "pdf_url": "https://arxiv.org/pdf/2305.10126v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2305.10126",
    "arxiv_authors": [
      "Zhenxing Zhang",
      "Lambert Schomaker"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Fusion-S2iGan%3A+An+Efficient+and+Effective+Single-Stage+Framework+for+Speech-to-Image+Generation+Zhenxing+Zhang+Lambert+Schomaker",
    "gs_search_success": true,
    "gs_authors": [
      "omixoi0AAAAJ",
      "Pa5-t-QAAAAJ"
    ],
    "citation_count": 3,
    "gs_author_count": 1
  },
  {
    "arxiv_id": "2309.03100",
    "title": "FArMARe: a Furniture-Aware Multi-task methodology for Recommending Apartments based on the user interests",
    "year": 2023,
    "published": "2023-09-06T15:40:33Z",
    "categories": [
      "cs.CV",
      "cs.MM"
    ],
    "abstract": "Nowadays, many people frequently have to search for new accommodation options. Searching for a suitable apartment is a time-consuming process, especially because visiting them is often mandatory to assess the truthfulness of the advertisements found on the Web. While this process could be alleviated by visiting the apartments in the metaverse, the Web-based recommendation platforms are not suitable for the task. To address this shortcoming, in this paper, we define a new problem called text-to-a",
    "arxiv_url": "https://arxiv.org/abs/2309.03100v1",
    "pdf_url": "https://arxiv.org/pdf/2309.03100v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2309.03100",
    "arxiv_authors": [
      "Ali Abdari",
      "Alex Falcon",
      "Giuseppe Serra"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=FArMARe%3A+a+Furniture-Aware+Multi-task+methodology+for+Recommending+Apartments+based+on+the+user+interests+Ali+Abdari+Alex+Falcon+Giuseppe+Serra",
    "gs_search_success": true,
    "gs_authors": [
      "9qQUW_AAAAAJ",
      "sHPhexYAAAAJ",
      "xz1J-xQAAAAJ"
    ],
    "citation_count": 10,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2502.13754",
    "title": "Capturing Rich Behavior Representations: A Dynamic Action Semantic-Aware Graph Transformer for Video Captioning",
    "year": 2025,
    "published": "2025-02-19T14:16:47Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Existing video captioning methods merely provide shallow or simplistic representations of object behaviors, resulting in superficial and ambiguous descriptions. However, object behavior is dynamic and complex. To comprehensively capture the essence of object behavior, we propose a dynamic action semantic-aware graph transformer. Firstly, a multi-scale temporal modeling module is designed to flexibly learn long and short-term latent action features. It not only acquires latent action features acr",
    "arxiv_url": "https://arxiv.org/abs/2502.13754v1",
    "pdf_url": "https://arxiv.org/pdf/2502.13754v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2502.13754",
    "arxiv_authors": [
      "Caihua Liu",
      "Xu Li",
      "Wenjing Xue",
      "Wei Tang",
      "Xia Feng"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Capturing+Rich+Behavior+Representations%3A+A+Dynamic+Action+Semantic-Aware+Graph+Transformer+for+Video+Captioning+Caihua+Liu+Xu+Li+Wenjing+Xue+Wei+Tang+Xia+Feng",
    "gs_search_success": true,
    "gs_authors": [],
    "citation_count": null,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2308.02562",
    "title": "Beyond Images: Adaptive Fusion of Visual and Textual Data for Food Classification",
    "year": 2023,
    "published": "2023-08-03T04:03:46Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CY",
      "cs.LG"
    ],
    "abstract": "This study introduces a novel multimodal food recognition framework that effectively combines visual and textual modalities to enhance classification accuracy and robustness. The proposed approach employs a dynamic multimodal fusion strategy that adaptively integrates features from unimodal visual inputs and complementary textual metadata. This fusion mechanism is designed to maximize the use of informative content, while mitigating the adverse impact of missing or inconsistent modality data. Th",
    "arxiv_url": "https://arxiv.org/abs/2308.02562v4",
    "pdf_url": "https://arxiv.org/pdf/2308.02562v4",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2308.02562",
    "arxiv_authors": [
      "Prateek Mittal",
      "Puneet Goyal",
      "Joohi Chauhan"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Beyond+Images%3A+Adaptive+Fusion+of+Visual+and+Textual+Data+for+Food+Classification+Prateek+Mittal+Puneet+Goyal+Joohi+Chauhan",
    "gs_search_success": true,
    "gs_authors": [
      "xTKD8J4AAAAJ",
      "bzvUS-IAAAAJ",
      "WAdMipMAAAAJ",
      "_tANisQAAAAJ",
      "_OLkF6sAAAAJ",
      "qoG1VTgAAAAJ",
      "QU2O6JMAAAAJ",
      "s9q_hXoAAAAJ",
      "5lGnNQoAAAAJ",
      "Mt1kwb0AAAAJ",
      "VkREzegAAAAJ",
      "dc7EaVcAAAAJ"
    ],
    "citation_count": 2,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2505.16815",
    "title": "Image Quality Assessment for Embodied AI",
    "year": 2025,
    "published": "2025-05-22T15:51:07Z",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "abstract": "Embodied AI has developed rapidly in recent years, but it is still mainly deployed in laboratories, with various distortions in the Real-world limiting its application. Traditionally, Image Quality Assessment (IQA) methods are applied to predict human preferences for distorted images; however, there is no IQA method to assess the usability of an image in embodied tasks, namely, the perceptual quality for robots. To provide accurate and reliable quality indicators for future embodied scenarios, w",
    "arxiv_url": "https://arxiv.org/abs/2505.16815v2",
    "pdf_url": "https://arxiv.org/pdf/2505.16815v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2505.16815",
    "arxiv_authors": [
      "Chunyi Li",
      "Jiaohao Xiao",
      "Jianbo Zhang",
      "Farong Wen",
      "Zicheng Zhang",
      "Yuan Tian",
      "Xiangyang Zhu",
      "Xiaohong Liu",
      "Zhengxue Cheng",
      "Weisi Lin",
      "Guangtao Zhai"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Image+Quality+Assessment+for+Embodied+AI+Chunyi+Li+Jiaohao+Xiao+Jianbo+Zhang+Farong+Wen+Zicheng+Zhang",
    "gs_search_success": true,
    "gs_authors": [
      "QICTEckAAAAJ",
      "D_S41X4AAAAJ",
      "WosRriMAAAAJ",
      "Kzd0qtsAAAAJ",
      "k7YfbnEAAAAJ",
      "6Q1HTZgAAAAJ",
      "Tq2hoMQAAAAJ",
      "vtQjhcUAAAAJ",
      "E6zbSYgAAAAJ"
    ],
    "citation_count": 3,
    "gs_author_count": 11
  },
  {
    "arxiv_id": "2410.22952",
    "title": "Efficient Adaptation of Pre-trained Vision Transformer via Householder Transformation",
    "year": 2024,
    "published": "2024-10-30T12:08:30Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "A common strategy for Parameter-Efficient Fine-Tuning (PEFT) of pre-trained Vision Transformers (ViTs) involves adapting the model to downstream tasks by learning a low-rank adaptation matrix. This matrix is decomposed into a product of down-projection and up-projection matrices, with the bottleneck dimensionality being crucial for reducing the number of learnable parameters, as exemplified by prevalent methods like LoRA and Adapter. However, these low-rank strategies typically employ a fixed bo",
    "arxiv_url": "https://arxiv.org/abs/2410.22952v1",
    "pdf_url": "https://arxiv.org/pdf/2410.22952v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2410.22952",
    "arxiv_authors": [
      "Wei Dong",
      "Yuan Sun",
      "Yiting Yang",
      "Xing Zhang",
      "Zhijun Lin",
      "Qingsen Yan",
      "Haokui Zhang",
      "Peng Wang",
      "Yang Yang",
      "Hengtao Shen"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Efficient+Adaptation+of+Pre-trained+Vision+Transformer+via+Householder+Transformation+Wei+Dong+Yuan+Sun+Yiting+Yang+Xing+Zhang+Zhijun+Lin",
    "gs_search_success": true,
    "gs_authors": [
      "m3gPwCoAAAAJ",
      "H_15_ecAAAAJ",
      "tkTl3BMAAAAJ",
      "BSGy3foAAAAJ"
    ],
    "citation_count": 5,
    "gs_author_count": 10
  },
  {
    "arxiv_id": "2410.22655",
    "title": "FlowDCN: Exploring DCN-like Architectures for Fast Image Generation with Arbitrary Resolution",
    "year": 2024,
    "published": "2024-10-30T02:48:50Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Arbitrary-resolution image generation still remains a challenging task in AIGC, as it requires handling varying resolutions and aspect ratios while maintaining high visual quality. Existing transformer-based diffusion methods suffer from quadratic computation cost and limited resolution extrapolation capabilities, making them less effective for this task. In this paper, we propose FlowDCN, a purely convolution-based generative model with linear time and memory complexity, that can efficiently ge",
    "arxiv_url": "https://arxiv.org/abs/2410.22655v1",
    "pdf_url": "https://arxiv.org/pdf/2410.22655v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2410.22655",
    "arxiv_authors": [
      "Shuai Wang",
      "Zexian Li",
      "Tianhui Song",
      "Xubin Li",
      "Tiezheng Ge",
      "Bo Zheng",
      "Limin Wang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=FlowDCN%3A+Exploring+DCN-like+Architectures+for+Fast+Image+Generation+with+Arbitrary+Resolution+Shuai+Wang+Zexian+Li+Tianhui+Song+Xubin+Li+Tiezheng+Ge",
    "gs_search_success": true,
    "gs_authors": [
      "3gHhO9QAAAAJ",
      "8SabwVEAAAAJ",
      "t2V09QEAAAAJ",
      "us4prRUAAAAJ",
      "db5ZTlMAAAAJ"
    ],
    "citation_count": 11,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2403.08947",
    "title": "Robust COVID-19 Detection in CT Images with CLIP",
    "year": 2024,
    "published": "2024-03-13T20:26:50Z",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "abstract": "In the realm of medical imaging, particularly for COVID-19 detection, deep learning models face substantial challenges such as the necessity for extensive computational resources, the paucity of well-annotated datasets, and a significant amount of unlabeled data. In this work, we introduce the first lightweight detector designed to overcome these obstacles, leveraging a frozen CLIP image encoder and a trainable multilayer perception (MLP). Enhanced with Conditional Value at Risk (CVaR) for robus",
    "arxiv_url": "https://arxiv.org/abs/2403.08947v3",
    "pdf_url": "https://arxiv.org/pdf/2403.08947v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2403.08947",
    "arxiv_authors": [
      "Li Lin",
      "Yamini Sri Krubha",
      "Zhenhuan Yang",
      "Cheng Ren",
      "Thuc Duy Le",
      "Irene Amerini",
      "Xin Wang",
      "Shu Hu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Robust+COVID-19+Detection+in+CT+Images+with+CLIP+Li+Lin+Yamini+Sri+Krubha+Zhenhuan+Yang+Cheng+Ren+Thuc+Duy+Le",
    "gs_search_success": true,
    "gs_authors": [
      "q4qu28QAAAAJ",
      "qqx-4h4AAAAJ",
      "wMSCRxUAAAAJ",
      "4ZDhr6UAAAAJ",
      "Em_c6ocAAAAJ",
      "jUWx8fcAAAAJ",
      "HqZ6tL4AAAAJ"
    ],
    "citation_count": 14,
    "gs_author_count": 8
  },
  {
    "arxiv_id": "2505.24867",
    "title": "Time Blindness: Why Video-Language Models Can't See What Humans Can?",
    "year": 2025,
    "published": "2025-05-30T17:59:12Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Recent advances in vision-language models (VLMs) have made impressive strides in understanding spatio-temporal relationships in videos. However, when spatial information is obscured, these models struggle to capture purely temporal patterns. We introduce $\\textbf{SpookyBench}$, a benchmark where information is encoded solely in temporal sequences of noise-like frames, mirroring natural phenomena from biological signaling to covert communication. Interestingly, while humans can recognize shapes, ",
    "arxiv_url": "https://arxiv.org/abs/2505.24867v1",
    "pdf_url": "https://arxiv.org/pdf/2505.24867v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2505.24867",
    "arxiv_authors": [
      "Ujjwal Upadhyay",
      "Mukul Ranjan",
      "Zhiqiang Shen",
      "Mohamed Elhoseiny"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Time+Blindness%3A+Why+Video-Language+Models+Can%27t+See+What+Humans+Can%3F+Ujjwal+Upadhyay+Mukul+Ranjan+Zhiqiang+Shen+Mohamed+Elhoseiny",
    "gs_search_success": true,
    "gs_authors": [
      "fFBR0j0AAAAJ",
      "lvpaXdEAAAAJ",
      "DGr0fVoAAAAJ",
      "iRBUTOAAAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2412.20105",
    "title": "ST$^3$: Accelerating Multimodal Large Language Model by Spatial-Temporal Visual Token Trimming",
    "year": 2024,
    "published": "2024-12-28T10:17:29Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Multimodal large language models (MLLMs) enhance their perceptual capabilities by integrating visual and textual information. However, processing the massive number of visual tokens incurs a significant computational cost. Existing analysis of the MLLM attention mechanisms remains shallow, leading to coarse-grain token pruning strategies that fail to effectively balance speed and accuracy. In this paper, we conduct a comprehensive investigation of MLLM attention mechanisms with LLaVA. We find th",
    "arxiv_url": "https://arxiv.org/abs/2412.20105v1",
    "pdf_url": "https://arxiv.org/pdf/2412.20105v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2412.20105",
    "arxiv_authors": [
      "Jiedong Zhuang",
      "Lu Lu",
      "Ming Dai",
      "Rui Hu",
      "Jian Chen",
      "Qiang Liu",
      "Haoji Hu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=ST%24%5E3%24%3A+Accelerating+Multimodal+Large+Language+Model+by+Spatial-Temporal+Visual+Token+Trimming+Jiedong+Zhuang+Lu+Lu+Ming+Dai+Rui+Hu+Jian+Chen",
    "gs_search_success": true,
    "gs_authors": [
      "Zvqr25wAAAAJ",
      "AN_J0DYAAAAJ",
      "TPmDe4EAAAAJ",
      "s_ZB7xkAAAAJ",
      "aiy5G0oAAAAJ"
    ],
    "citation_count": 10,
    "gs_author_count": 7
  },
  {
    "arxiv_id": "2412.03259",
    "title": "GERD: Geometric event response data generation",
    "year": 2024,
    "published": "2024-12-04T11:59:36Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Event-based vision sensors are appealing because of their time resolution, higher dynamic range, and low-power consumption. They also provide data that is fundamentally different from conventional frame-based cameras: events are sparse, discrete, and require integration in time. Unlike conventional models grounded in established geometric and physical principles, event-based models lack comparable foundations. We introduce a method to generate event-based data under controlled transformations. S",
    "arxiv_url": "https://arxiv.org/abs/2412.03259v1",
    "pdf_url": "https://arxiv.org/pdf/2412.03259v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2412.03259",
    "arxiv_authors": [
      "Jens Egholm Pedersen",
      "Dimitris Korakovounis",
      "Jörg Conradt"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=GERD%3A+Geometric+event+response+data+generation+Jens+Egholm+Pedersen+Dimitris+Korakovounis+J%C3%B6rg+Conradt",
    "gs_search_success": true,
    "gs_authors": [
      "A4VwxccAAAAJ",
      "NGdMpTYAAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": null
  },
  {
    "arxiv_id": "2504.20378",
    "title": "Sparse2DGS: Geometry-Prioritized Gaussian Splatting for Surface Reconstruction from Sparse Views",
    "year": 2025,
    "published": "2025-04-29T02:47:02Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "We present a Gaussian Splatting method for surface reconstruction using sparse input views. Previous methods relying on dense views struggle with extremely sparse Structure-from-Motion points for initialization. While learning-based Multi-view Stereo (MVS) provides dense 3D points, directly combining it with Gaussian Splatting leads to suboptimal results due to the ill-posed nature of sparse-view geometric optimization. We propose Sparse2DGS, an MVS-initialized Gaussian Splatting pipeline for co",
    "arxiv_url": "https://arxiv.org/abs/2504.20378v1",
    "pdf_url": "https://arxiv.org/pdf/2504.20378v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2504.20378",
    "arxiv_authors": [
      "Jiang Wu",
      "Rui Li",
      "Yu Zhu",
      "Rong Guo",
      "Jinqiu Sun",
      "Yanning Zhang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Sparse2DGS%3A+Geometry-Prioritized+Gaussian+Splatting+for+Surface+Reconstruction+from+Sparse+Views+Jiang+Wu+Rui+Li+Yu+Zhu+Rong+Guo+Jinqiu+Sun",
    "gs_search_success": true,
    "gs_authors": [
      "CkVvikAAAAAJ"
    ],
    "citation_count": 10,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2306.03511",
    "title": "Curriculum-Based Augmented Fourier Domain Adaptation for Robust Medical Image Segmentation",
    "year": 2023,
    "published": "2023-06-06T08:56:58Z",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "abstract": "Accurate and robust medical image segmentation is fundamental and crucial for enhancing the autonomy of computer-aided diagnosis and intervention systems. Medical data collection normally involves different scanners, protocols, and populations, making domain adaptation (DA) a highly demanding research field to alleviate model degradation in the deployment site. To preserve the model performance across multiple testing domains, this work proposes the Curriculum-based Augmented Fourier Domain Adap",
    "arxiv_url": "https://arxiv.org/abs/2306.03511v1",
    "pdf_url": "https://arxiv.org/pdf/2306.03511v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2306.03511",
    "arxiv_authors": [
      "An Wang",
      "Mobarakol Islam",
      "Mengya Xu",
      "Hongliang Ren"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Curriculum-Based+Augmented+Fourier+Domain+Adaptation+for+Robust+Medical+Image+Segmentation+An+Wang+Mobarakol+Islam+Mengya+Xu+Hongliang+Ren",
    "gs_search_success": true,
    "gs_authors": [
      "rcF7N44AAAAJ",
      "Uq5qvyAAAAAJ",
      "QUwym50AAAAJ",
      "NYWnE6QAAAAJ"
    ],
    "citation_count": 25,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2304.02682",
    "title": "nD-PDPA: nDimensional Probability Density Profile Analysis",
    "year": 2023,
    "published": "2023-04-05T18:25:34Z",
    "categories": [
      "cs.CV",
      "q-bio.QM"
    ],
    "abstract": "Despite the recent advances in various Structural Genomics Projects, a large gap remains between the number of sequenced and structurally characterized proteins. Some reasons for this discrepancy include technical difficulties, labor, and the cost related to determining a structure by experimental methods such as NMR spectroscopy. Several computational methods have been developed to expand the applicability of NMR spectroscopy by addressing temporal and economical problems more efficiently. Whil",
    "arxiv_url": "https://arxiv.org/abs/2304.02682v1",
    "pdf_url": "https://arxiv.org/pdf/2304.02682v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2304.02682",
    "arxiv_authors": [
      "Arjang Fahim",
      "Stephanie Irausquin",
      "Homayoun Valafar"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=nD-PDPA%3A+nDimensional+Probability+Density+Profile+Analysis+Arjang+Fahim+Stephanie+Irausquin+Homayoun+Valafar",
    "gs_search_success": true,
    "gs_authors": [
      "bKXpCmoAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 1
  },
  {
    "arxiv_id": "2302.06098",
    "title": "Towards Local Visual Modeling for Image Captioning",
    "year": 2023,
    "published": "2023-02-13T04:42:00Z",
    "categories": [
      "cs.CV",
      "cs.MM"
    ],
    "abstract": "In this paper, we study the local visual modeling with grid features for image captioning, which is critical for generating accurate and detailed captions. To achieve this target, we propose a Locality-Sensitive Transformer Network (LSTNet) with two novel designs, namely Locality-Sensitive Attention (LSA) and Locality-Sensitive Fusion (LSF). LSA is deployed for the intra-layer interaction in Transformer via modeling the relationship between each grid and its neighbors. It reduces the difficulty ",
    "arxiv_url": "https://arxiv.org/abs/2302.06098v1",
    "pdf_url": "https://arxiv.org/pdf/2302.06098v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2302.06098",
    "arxiv_authors": [
      "Yiwei Ma",
      "Jiayi Ji",
      "Xiaoshuai Sun",
      "Yiyi Zhou",
      "Rongrong Ji"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Towards+Local+Visual+Modeling+for+Image+Captioning+Yiwei+Ma+Jiayi+Ji+Xiaoshuai+Sun+Yiyi+Zhou+Rongrong+Ji",
    "gs_search_success": true,
    "gs_authors": [
      "lRSD7PQAAAAJ",
      "w3_2ep0AAAAJ",
      "KPMK3B4AAAAJ",
      "xp_rICcAAAAJ",
      "KIDY5pUAAAAJ"
    ],
    "citation_count": 121,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2405.14633",
    "title": "Flatten Anything: Unsupervised Neural Surface Parameterization",
    "year": 2024,
    "published": "2024-05-23T14:39:52Z",
    "categories": [
      "cs.CV",
      "cs.CG"
    ],
    "abstract": "Surface parameterization plays an essential role in numerous computer graphics and geometry processing applications. Traditional parameterization approaches are designed for high-quality meshes laboriously created by specialized 3D modelers, thus unable to meet the processing demand for the current explosion of ordinary 3D data. Moreover, their working mechanisms are typically restricted to certain simple topologies, thus relying on cumbersome manual efforts (e.g., surface cutting, part segmenta",
    "arxiv_url": "https://arxiv.org/abs/2405.14633v2",
    "pdf_url": "https://arxiv.org/pdf/2405.14633v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2405.14633",
    "arxiv_authors": [
      "Qijian Zhang",
      "Junhui Hou",
      "Wenping Wang",
      "Ying He"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Flatten+Anything%3A+Unsupervised+Neural+Surface+Parameterization+Qijian+Zhang+Junhui+Hou+Wenping+Wang+Ying+He",
    "gs_search_success": true,
    "gs_authors": [
      "j6eefhwAAAAJ",
      "ISNmBxwAAAAJ",
      "4NIiTYgAAAAJ",
      "28shvv0AAAAJ"
    ],
    "citation_count": 12,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2312.12540",
    "title": "Lightning-Fast Image Inversion and Editing for Text-to-Image Diffusion Models",
    "year": 2023,
    "published": "2023-12-19T19:19:19Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Diffusion inversion is the problem of taking an image and a text prompt that describes it and finding a noise latent that would generate the exact same image. Most current deterministic inversion techniques operate by approximately solving an implicit equation and may converge slowly or yield poor reconstructed images. We formulate the problem by finding the roots of an implicit equation and devlop a method to solve it efficiently. Our solution is based on Newton-Raphson (NR), a well-known techn",
    "arxiv_url": "https://arxiv.org/abs/2312.12540v5",
    "pdf_url": "https://arxiv.org/pdf/2312.12540v5",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2312.12540",
    "arxiv_authors": [
      "Dvir Samuel",
      "Barak Meiri",
      "Haggai Maron",
      "Yoad Tewel",
      "Nir Darshan",
      "Shai Avidan",
      "Gal Chechik",
      "Rami Ben-Ari"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Lightning-Fast+Image+Inversion+and+Editing+for+Text-to-Image+Diffusion+Models+Dvir+Samuel+Barak+Meiri+Haggai+Maron+Yoad+Tewel+Nir+Darshan",
    "gs_search_success": true,
    "gs_authors": [
      "VSCSNhMAAAAJ",
      "4v8uJrIAAAAJ",
      "hpItE1QAAAAJ",
      "C4i_vUMAAAAJ",
      "_CWxQ1gAAAAJ",
      "Ado7zAYAAAAJ",
      "Wk2gAZUAAAAJ"
    ],
    "citation_count": 9,
    "gs_author_count": 8
  },
  {
    "arxiv_id": "2302.13891",
    "title": "Supervised Virtual-to-Real Domain Adaptation for Object Detection Task using YOLO",
    "year": 2023,
    "published": "2023-02-27T15:36:14Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Deep neural network shows excellent use in a lot of real-world tasks. One of the deep learning tasks is object detection. Well-annotated datasets will affect deep neural network accuracy. More data learned by deep neural networks will make the model more accurate. However, a well-annotated dataset is hard to find, especially in a specific domain. To overcome this, computer-generated data or virtual datasets are used. Researchers could generate many images with specific use cases also with its an",
    "arxiv_url": "https://arxiv.org/abs/2302.13891v1",
    "pdf_url": "https://arxiv.org/pdf/2302.13891v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2302.13891",
    "arxiv_authors": [
      "Akbar Satya Nugraha",
      "Yudistira Novanto",
      "Bayu Rahayudi"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Supervised+Virtual-to-Real+Domain+Adaptation+for+Object+Detection+Task+using+YOLO+Akbar+Satya+Nugraha+Yudistira+Novanto+Bayu+Rahayudi",
    "gs_search_success": true,
    "gs_authors": [
      "Lz5dgAgAAAAJ",
      "NxSJRlYAAAAJ"
    ],
    "citation_count": 9,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2503.18211",
    "title": "SimMotionEdit: Text-Based Human Motion Editing with Motion Similarity Prediction",
    "year": 2025,
    "published": "2025-03-23T21:29:37Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Text-based 3D human motion editing is a critical yet challenging task in computer vision and graphics. While training-free approaches have been explored, the recent release of the MotionFix dataset, which includes source-text-motion triplets, has opened new avenues for training, yielding promising results. However, existing methods struggle with precise control, often leading to misalignment between motion semantics and language instructions. In this paper, we introduce a related task, motion si",
    "arxiv_url": "https://arxiv.org/abs/2503.18211v2",
    "pdf_url": "https://arxiv.org/pdf/2503.18211v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2503.18211",
    "arxiv_authors": [
      "Zhengyuan Li",
      "Kai Cheng",
      "Anindita Ghosh",
      "Uttaran Bhattacharya",
      "Liangyan Gui",
      "Aniket Bera"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=SimMotionEdit%3A+Text-Based+Human+Motion+Editing+with+Motion+Similarity+Prediction+Zhengyuan+Li+Kai+Cheng+Anindita+Ghosh+Uttaran+Bhattacharya+Liangyan+Gui",
    "gs_search_success": true,
    "gs_authors": [
      "q3UdHk4AAAAJ",
      "uF17d-wAAAAJ",
      "3aE0r9QAAAAJ",
      "XsxguG8AAAAJ",
      "xx9nrfoAAAAJ",
      "HXN7DNoAAAAJ"
    ],
    "citation_count": 2,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2408.15077",
    "title": "MMASD+: A Novel Dataset for Privacy-Preserving Behavior Analysis of Children with Autism Spectrum Disorder",
    "year": 2024,
    "published": "2024-08-27T14:05:48Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "abstract": "Autism spectrum disorder (ASD) is characterized by significant challenges in social interaction and comprehending communication signals. Recently, therapeutic interventions for ASD have increasingly utilized Deep learning powered-computer vision techniques to monitor individual progress over time. These models are trained on private, non-public datasets from the autism community, creating challenges in comparing results across different models due to privacy-preserving data-sharing issues. This ",
    "arxiv_url": "https://arxiv.org/abs/2408.15077v2",
    "pdf_url": "https://arxiv.org/pdf/2408.15077v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2408.15077",
    "arxiv_authors": [
      "Pavan Uttej Ravva",
      "Behdokht Kiafar",
      "Pinar Kullu",
      "Jicheng Li",
      "Anjana Bhat",
      "Roghayeh Leila Barmaki"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=MMASD%2B%3A+A+Novel+Dataset+for+Privacy-Preserving+Behavior+Analysis+of+Children+with+Autism+Spectrum+Disorder+Pavan+Uttej+Ravva+Behdokht+Kiafar+Pinar+Kullu+Jicheng+Li+Anjana+Bhat",
    "gs_search_success": true,
    "gs_authors": [
      "g6FRh-sAAAAJ",
      "zmPRdkgAAAAJ",
      "PPrE9ioAAAAJ",
      "ICl-Tm0AAAAJ",
      "RcodyWEAAAAJ"
    ],
    "citation_count": 3,
    "gs_author_count": null
  },
  {
    "arxiv_id": "2408.00374",
    "title": "Conformal Trajectory Prediction with Multi-View Data Integration in Cooperative Driving",
    "year": 2024,
    "published": "2024-08-01T08:32:03Z",
    "categories": [
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "Current research on trajectory prediction primarily relies on data collected by onboard sensors of an ego vehicle. With the rapid advancement in connected technologies, such as vehicle-to-vehicle (V2V) and vehicle-to-infrastructure (V2I) communication, valuable information from alternate views becomes accessible via wireless networks. The integration of information from alternative views has the potential to overcome the inherent limitations associated with a single viewpoint, such as occlusions",
    "arxiv_url": "https://arxiv.org/abs/2408.00374v3",
    "pdf_url": "https://arxiv.org/pdf/2408.00374v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2408.00374",
    "arxiv_authors": [
      "Xi Chen",
      "Rahul Bhadani",
      "Larry Head"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Conformal+Trajectory+Prediction+with+Multi-View+Data+Integration+in+Cooperative+Driving+Xi+Chen+Rahul+Bhadani+Larry+Head",
    "gs_search_success": true,
    "gs_authors": [
      "hJVgftEAAAAJ",
      "leeo4RgAAAAJ"
    ],
    "citation_count": 5,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2503.06814",
    "title": "Unlocking Generalization for Robotics via Modularity and Scale",
    "year": 2025,
    "published": "2025-03-10T00:38:31Z",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "How can we build generalist robot systems? Scale may not be enough due to the significant multimodality of robotics tasks, lack of easily accessible data and the challenges of deploying on physical hardware. Meanwhile, most deployed robotic systems today are inherently modular and can leverage the independent generalization capabilities of each module to perform well. Therefore, this thesis seeks to tackle the task of building generalist robot agents by integrating these components into one: com",
    "arxiv_url": "https://arxiv.org/abs/2503.06814v1",
    "pdf_url": "https://arxiv.org/pdf/2503.06814v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2503.06814",
    "arxiv_authors": [
      "Murtaza Dalal"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Unlocking+Generalization+for+Robotics+via+Modularity+and+Scale+Murtaza+Dalal",
    "gs_search_success": true,
    "gs_authors": [],
    "citation_count": null,
    "gs_author_count": 1
  },
  {
    "arxiv_id": "2505.01644",
    "title": "A Dual-Task Synergy-Driven Generalization Framework for Pancreatic Cancer Segmentation in CT Scans",
    "year": 2025,
    "published": "2025-05-03T00:54:00Z",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "abstract": "Pancreatic cancer, characterized by its notable prevalence and mortality rates, demands accurate lesion delineation for effective diagnosis and therapeutic interventions. The generalizability of extant methods is frequently compromised due to the pronounced variability in imaging and the heterogeneous characteristics of pancreatic lesions, which may mimic normal tissues and exhibit significant inter-patient variability. Thus, we propose a generalization framework that synergizes pixel-level clas",
    "arxiv_url": "https://arxiv.org/abs/2505.01644v1",
    "pdf_url": "https://arxiv.org/pdf/2505.01644v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2505.01644",
    "arxiv_authors": [
      "Jun Li",
      "Yijue Zhang",
      "Haibo Shi",
      "Minhong Li",
      "Qiwei Li",
      "Xiaohua Qian"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=A+Dual-Task+Synergy-Driven+Generalization+Framework+for+Pancreatic+Cancer+Segmentation+in+CT+Scans+Jun+Li+Yijue+Zhang+Haibo+Shi+Minhong+Li+Qiwei+Li",
    "gs_search_success": true,
    "gs_authors": [
      "jlU5-ZUAAAAJ",
      "Q5tGl4kAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2404.03531",
    "title": "COMO: Compact Mapping and Odometry",
    "year": 2024,
    "published": "2024-04-04T15:35:43Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "We present COMO, a real-time monocular mapping and odometry system that encodes dense geometry via a compact set of 3D anchor points. Decoding anchor point projections into dense geometry via per-keyframe depth covariance functions guarantees that depth maps are joined together at visible anchor points. The representation enables joint optimization of camera poses and dense geometry, intrinsic 3D consistency, and efficient second-order inference. To maintain a compact yet expressive map, we intr",
    "arxiv_url": "https://arxiv.org/abs/2404.03531v2",
    "pdf_url": "https://arxiv.org/pdf/2404.03531v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2404.03531",
    "arxiv_authors": [
      "Eric Dexheimer",
      "Andrew J. Davison"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=COMO%3A+Compact+Mapping+and+Odometry+Eric+Dexheimer+Andrew+J.+Davison",
    "gs_search_success": true,
    "gs_authors": [
      "A0ae1agAAAAJ",
      "sMsaK0gAAAAJ"
    ],
    "citation_count": 5,
    "gs_author_count": 2
  },
  {
    "arxiv_id": "2303.13509",
    "title": "Position-Guided Point Cloud Panoptic Segmentation Transformer",
    "year": 2023,
    "published": "2023-03-23T17:59:02Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "DEtection TRansformer (DETR) started a trend that uses a group of learnable queries for unified visual perception. This work begins by applying this appealing paradigm to LiDAR-based point cloud segmentation and obtains a simple yet effective baseline. Although the naive adaptation obtains fair results, the instance segmentation performance is noticeably inferior to previous works. By diving into the details, we observe that instances in the sparse point clouds are relatively small to the whole ",
    "arxiv_url": "https://arxiv.org/abs/2303.13509v1",
    "pdf_url": "https://arxiv.org/pdf/2303.13509v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2303.13509",
    "arxiv_authors": [
      "Zeqi Xiao",
      "Wenwei Zhang",
      "Tai Wang",
      "Chen Change Loy",
      "Dahua Lin",
      "Jiangmiao Pang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Position-Guided+Point+Cloud+Panoptic+Segmentation+Transformer+Zeqi+Xiao+Wenwei+Zhang+Tai+Wang+Chen+Change+Loy+Dahua+Lin",
    "gs_search_success": true,
    "gs_authors": [
      "JmbbZWIAAAAJ",
      "559LF80AAAAJ",
      "ssSfKpAAAAAJ",
      "6sr_HqMAAAAJ",
      "QDXADSEAAAAJ",
      "GMzzRRUAAAAJ"
    ],
    "citation_count": 25,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2410.02006",
    "title": "An Architecture Built for Federated Learning: Addressing Data Heterogeneity through Adaptive Normalization-Free Feature Recalibration",
    "year": 2024,
    "published": "2024-10-02T20:16:56Z",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "abstract": "Federated learning is a decentralized collaborative training paradigm preserving stakeholders' data ownership while improving performance and generalization. However, statistical heterogeneity among client datasets degrades system performance. To address this issue, we propose Adaptive Normalization-free Feature Recalibration (ANFR), a model architecture-level approach that combines weight standardization and channel attention to combat heterogeneous data in FL. ANFR leverages weight standardiza",
    "arxiv_url": "https://arxiv.org/abs/2410.02006v2",
    "pdf_url": "https://arxiv.org/pdf/2410.02006v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2410.02006",
    "arxiv_authors": [
      "Vasilis Siomos",
      "Jonathan Passerat-Palmbach",
      "Giacomo Tarroni"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=An+Architecture+Built+for+Federated+Learning%3A+Addressing+Data+Heterogeneity+through+Adaptive+Normalization-Free+Feature+Recalibration+Vasilis+Siomos+Jonathan+Passerat-Palmbach+Giacomo+Tarroni",
    "gs_search_success": true,
    "gs_authors": [
      "6nlDNyoAAAAJ",
      "zUdoBe0AAAAJ",
      "EElOUT8AAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2402.05382",
    "title": "Task-customized Masked AutoEncoder via Mixture of Cluster-conditional Experts",
    "year": 2024,
    "published": "2024-02-08T03:46:32Z",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "Masked Autoencoder~(MAE) is a prevailing self-supervised learning method that achieves promising results in model pre-training. However, when the various downstream tasks have data distributions different from the pre-training data, the semantically irrelevant pre-training information might result in negative transfer, impeding MAE's scalability. To address this issue, we propose a novel MAE-based pre-training paradigm, Mixture of Cluster-conditional Experts (MoCE), which can be trained once but",
    "arxiv_url": "https://arxiv.org/abs/2402.05382v1",
    "pdf_url": "https://arxiv.org/pdf/2402.05382v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2402.05382",
    "arxiv_authors": [
      "Zhili Liu",
      "Kai Chen",
      "Jianhua Han",
      "Lanqing Hong",
      "Hang Xu",
      "Zhenguo Li",
      "James T. Kwok"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Task-customized+Masked+AutoEncoder+via+Mixture+of+Cluster-conditional+Experts+Zhili+Liu+Kai+Chen+Jianhua+Han+Lanqing+Hong+Hang+Xu",
    "gs_search_success": true,
    "gs_authors": [
      "J_8TX6sAAAAJ",
      "FdR09jsAAAAJ",
      "XboZC1AAAAAJ",
      "-oTraZ4AAAAJ",
      "2p7x6OUAAAAJ",
      "OEPMQEMAAAAJ",
      "3qBfyLIAAAAJ"
    ],
    "citation_count": 30,
    "gs_author_count": 7
  },
  {
    "arxiv_id": "2409.17134",
    "title": "Streaming Neural Images",
    "year": 2024,
    "published": "2024-09-25T17:51:20Z",
    "categories": [
      "cs.CV",
      "eess.IV"
    ],
    "abstract": "Implicit Neural Representations (INRs) are a novel paradigm for signal representation that have attracted considerable interest for image compression. INRs offer unprecedented advantages in signal resolution and memory efficiency, enabling new possibilities for compression techniques. However, the existing limitations of INRs for image compression have not been sufficiently addressed in the literature. In this work, we explore the critical yet overlooked limiting factors of INRs, such as computa",
    "arxiv_url": "https://arxiv.org/abs/2409.17134v2",
    "pdf_url": "https://arxiv.org/pdf/2409.17134v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2409.17134",
    "arxiv_authors": [
      "Marcos V. Conde",
      "Andy Bigos",
      "Radu Timofte"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Streaming+Neural+Images+Marcos+V.+Conde+Andy+Bigos+Radu+Timofte",
    "gs_search_success": true,
    "gs_authors": [
      "u3MwH5kAAAAJ",
      "NtB1kjYAAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2504.05137",
    "title": "BoxSeg: Quality-Aware and Peer-Assisted Learning for Box-supervised Instance Segmentation",
    "year": 2025,
    "published": "2025-04-07T14:42:33Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Box-supervised instance segmentation methods aim to achieve instance segmentation with only box annotations. Recent methods have demonstrated the effectiveness of acquiring high-quality pseudo masks under the teacher-student framework. Building upon this foundation, we propose a BoxSeg framework involving two novel and general modules named the Quality-Aware Module (QAM) and the Peer-assisted Copy-paste (PC). The QAM obtains high-quality pseudo masks and better measures the mask quality to help ",
    "arxiv_url": "https://arxiv.org/abs/2504.05137v1",
    "pdf_url": "https://arxiv.org/pdf/2504.05137v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2504.05137",
    "arxiv_authors": [
      "Jinxiang Lai",
      "Wenlong Wu",
      "Jiawei Zhan",
      "Jian Li",
      "Bin-Bin Gao",
      "Jun Liu",
      "Jie Zhang",
      "Song Guo"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=BoxSeg%3A+Quality-Aware+and+Peer-Assisted+Learning+for+Box-supervised+Instance+Segmentation+Jinxiang+Lai+Wenlong+Wu+Jiawei+Zhan+Jian+Li+Bin-Bin+Gao",
    "gs_search_success": true,
    "gs_authors": [
      "ACb5C40AAAAJ",
      "JIKuf4AAAAAJ",
      "v9qtGm0AAAAJ",
      "yYviZ-oAAAAJ",
      "6jAXwlwAAAAJ",
      "JRCNlI8AAAAJ",
      "Ib-sizwAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 8
  },
  {
    "arxiv_id": "2410.10914",
    "title": "Towards Better Multi-head Attention via Channel-wise Sample Permutation",
    "year": 2024,
    "published": "2024-10-14T06:28:40Z",
    "categories": [
      "cs.LG",
      "cs.CL",
      "cs.CV"
    ],
    "abstract": "Transformer plays a central role in many fundamental deep learning models, e.g., the ViT in computer vision and the BERT and GPT in natural language processing, whose effectiveness is mainly attributed to its multi-head attention (MHA) mechanism. In this study, we propose a simple and novel channel-wise sample permutation (CSP) operator, achieving a new structured MHA with fewer parameters and lower complexity. Given an input matrix, CSP circularly shifts the samples of different channels with v",
    "arxiv_url": "https://arxiv.org/abs/2410.10914v1",
    "pdf_url": "https://arxiv.org/pdf/2410.10914v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2410.10914",
    "arxiv_authors": [
      "Shen Yuan",
      "Hongteng Xu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Towards+Better+Multi-head+Attention+via+Channel-wise+Sample+Permutation+Shen+Yuan+Hongteng+Xu",
    "gs_search_success": true,
    "gs_authors": [
      "7gYVOO8AAAAJ"
    ],
    "citation_count": 2,
    "gs_author_count": 2
  },
  {
    "arxiv_id": "2404.14309",
    "title": "Towards Understanding the Robustness of Diffusion-Based Purification: A Stochastic Perspective",
    "year": 2024,
    "published": "2024-04-22T16:10:38Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Diffusion-Based Purification (DBP) has emerged as an effective defense mechanism against adversarial attacks. The success of DBP is often attributed to the forward diffusion process, which reduces the distribution gap between clean and adversarial images by adding Gaussian noise. While this explanation is theoretically sound, the exact role of this mechanism in enhancing robustness remains unclear. In this paper, through empirical analysis, we propose that the intrinsic stochasticity in the DBP ",
    "arxiv_url": "https://arxiv.org/abs/2404.14309v3",
    "pdf_url": "https://arxiv.org/pdf/2404.14309v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2404.14309",
    "arxiv_authors": [
      "Yiming Liu",
      "Kezhao Liu",
      "Yao Xiao",
      "Ziyi Dong",
      "Xiaogang Xu",
      "Pengxu Wei",
      "Liang Lin"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Towards+Understanding+the+Robustness+of+Diffusion-Based+Purification%3A+A+Stochastic+Perspective+Yiming+Liu+Kezhao+Liu+Yao+Xiao+Ziyi+Dong+Xiaogang+Xu",
    "gs_search_success": true,
    "gs_authors": [
      "ZsScBwMAAAAJ",
      "Nav8m8gAAAAJ",
      "D29MOC0AAAAJ",
      "NKKwkGsAAAAJ"
    ],
    "citation_count": 3,
    "gs_author_count": 7
  },
  {
    "arxiv_id": "2402.06982",
    "title": "Treatment-wise Glioblastoma Survival Inference with Multi-parametric Preoperative MRI",
    "year": 2024,
    "published": "2024-02-10T16:13:09Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "physics.med-ph"
    ],
    "abstract": "In this work, we aim to predict the survival time (ST) of glioblastoma (GBM) patients undergoing different treatments based on preoperative magnetic resonance (MR) scans. The personalized and precise treatment planning can be achieved by comparing the ST of different treatments. It is well established that both the current status of the patient (as represented by the MR scans) and the choice of treatment are the cause of ST. While previous related MR-based glioblastoma ST studies have focused on",
    "arxiv_url": "https://arxiv.org/abs/2402.06982v1",
    "pdf_url": "https://arxiv.org/pdf/2402.06982v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2402.06982",
    "arxiv_authors": [
      "Xiaofeng Liu",
      "Nadya Shusharina",
      "Helen A Shih",
      "C. -C. Jay Kuo",
      "Georges El Fakhri",
      "Jonghye Woo"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Treatment-wise+Glioblastoma+Survival+Inference+with+Multi-parametric+Preoperative+MRI+Xiaofeng+Liu+Nadya+Shusharina+Helen+A+Shih+C.+-C.+Jay+Kuo+Georges+El+Fakhri",
    "gs_search_success": true,
    "gs_authors": [
      "UKxQVssAAAAJ",
      "QIablCYAAAAJ",
      "yPlkD_QAAAAJ",
      "VighnTUAAAAJ",
      "81d60okAAAAJ",
      "lgLXQYkAAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2408.10246",
    "title": "VyAnG-Net: A Novel Multi-Modal Sarcasm Recognition Model by Uncovering Visual, Acoustic and Glossary Features",
    "year": 2024,
    "published": "2024-08-05T15:36:52Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "eess.AS"
    ],
    "abstract": "Various linguistic and non-linguistic clues, such as excessive emphasis on a word, a shift in the tone of voice, or an awkward expression, frequently convey sarcasm. The computer vision problem of sarcasm recognition in conversation aims to identify hidden sarcastic, criticizing, and metaphorical information embedded in everyday dialogue. Prior, sarcasm recognition has focused mainly on text. Still, it is critical to consider all textual information, audio stream, facial expression, and body pos",
    "arxiv_url": "https://arxiv.org/abs/2408.10246v1",
    "pdf_url": "https://arxiv.org/pdf/2408.10246v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2408.10246",
    "arxiv_authors": [
      "Ananya Pandey",
      "Dinesh Kumar Vishwakarma"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=VyAnG-Net%3A+A+Novel+Multi-Modal+Sarcasm+Recognition+Model+by+Uncovering+Visual%2C+Acoustic+and+Glossary+Features+Ananya+Pandey+Dinesh+Kumar+Vishwakarma",
    "gs_search_success": true,
    "gs_authors": [
      "TUsEwA8AAAAJ"
    ],
    "citation_count": 2,
    "gs_author_count": 2
  },
  {
    "arxiv_id": "2311.01905",
    "title": "From Chaos to Calibration: A Geometric Mutual Information Approach to Target-Free Camera LiDAR Extrinsic Calibration",
    "year": 2023,
    "published": "2023-11-03T13:30:31Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Sensor fusion is vital for the safe and robust operation of autonomous vehicles. Accurate extrinsic sensor to sensor calibration is necessary to accurately fuse multiple sensor's data in a common spatial reference frame. In this paper, we propose a target free extrinsic calibration algorithm that requires no ground truth training data, artificially constrained motion trajectories, hand engineered features or offline optimization and that is accurate, precise and extremely robust to initializatio",
    "arxiv_url": "https://arxiv.org/abs/2311.01905v1",
    "pdf_url": "https://arxiv.org/pdf/2311.01905v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2311.01905",
    "arxiv_authors": [
      "Jack Borer",
      "Jeremy Tschirner",
      "Florian Ölsner",
      "Stefan Milz"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=From+Chaos+to+Calibration%3A+A+Geometric+Mutual+Information+Approach+to+Target-Free+Camera+LiDAR+Extrinsic+Calibration+Jack+Borer+Jeremy+Tschirner+Florian+%C3%96lsner+Stefan+Milz",
    "gs_search_success": true,
    "gs_authors": [
      "mgHsw-IAAAAJ",
      "cYLLc-gAAAAJ"
    ],
    "citation_count": 7,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2308.07795",
    "title": "Learning to Identify Critical States for Reinforcement Learning from Videos",
    "year": 2023,
    "published": "2023-08-15T14:21:24Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Recent work on deep reinforcement learning (DRL) has pointed out that algorithmic information about good policies can be extracted from offline data which lack explicit information about executed actions. For example, videos of humans or robots may convey a lot of implicit information about rewarding action sequences, but a DRL machine that wants to profit from watching such videos must first learn by itself to identify and recognize relevant states/actions/rewards. Without relying on ground-tru",
    "arxiv_url": "https://arxiv.org/abs/2308.07795v1",
    "pdf_url": "https://arxiv.org/pdf/2308.07795v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2308.07795",
    "arxiv_authors": [
      "Haozhe Liu",
      "Mingchen Zhuge",
      "Bing Li",
      "Yuhui Wang",
      "Francesco Faccio",
      "Bernard Ghanem",
      "Jürgen Schmidhuber"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Learning+to+Identify+Critical+States+for+Reinforcement+Learning+from+Videos+Haozhe+Liu+Mingchen+Zhuge+Bing+Li+Yuhui+Wang+Francesco+Faccio",
    "gs_search_success": true,
    "gs_authors": [
      "Qnj6XlMAAAAJ",
      "rVsGTeEAAAAJ",
      "7rFn6IgAAAAJ",
      "gLnCTgIAAAAJ",
      "0z3DkrkAAAAJ",
      "QX51P54AAAAJ",
      "tddnkNYAAAAJ"
    ],
    "citation_count": 20,
    "gs_author_count": 7
  },
  {
    "arxiv_id": "2312.00694",
    "title": "Object Detector Differences when using Synthetic and Real Training Data",
    "year": 2023,
    "published": "2023-12-01T16:27:48Z",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "To train well-performing generalizing neural networks, sufficiently large and diverse datasets are needed. Collecting data while adhering to privacy legislation becomes increasingly difficult and annotating these large datasets is both a resource-heavy and time-consuming task. An approach to overcome these difficulties is to use synthetic data since it is inherently scalable and can be automatically annotated. However, how training on synthetic data affects the layers of a neural network is stil",
    "arxiv_url": "https://arxiv.org/abs/2312.00694v1",
    "pdf_url": "https://arxiv.org/pdf/2312.00694v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2312.00694",
    "arxiv_authors": [
      "Martin Georg Ljungqvist",
      "Otto Nordander",
      "Markus Skans",
      "Arvid Mildner",
      "Tony Liu",
      "Pierre Nugues"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Object+Detector+Differences+when+using+Synthetic+and+Real+Training+Data+Martin+Georg+Ljungqvist+Otto+Nordander+Markus+Skans+Arvid+Mildner+Tony+Liu",
    "gs_search_success": true,
    "gs_authors": [
      "yJ150ZgAAAAJ",
      "WaxK-y9Xx18C"
    ],
    "citation_count": 20,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2404.06704",
    "title": "Convolution-based Probability Gradient Loss for Semantic Segmentation",
    "year": 2024,
    "published": "2024-04-10T03:20:33Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "In this paper, we introduce a novel Convolution-based Probability Gradient (CPG) loss for semantic segmentation. It employs convolution kernels similar to the Sobel operator, capable of computing the gradient of pixel intensity in an image. This enables the computation of gradients for both ground-truth and predicted category-wise probabilities. It enhances network performance by maximizing the similarity between these two probability gradients. Moreover, to specifically enhance accuracy near th",
    "arxiv_url": "https://arxiv.org/abs/2404.06704v1",
    "pdf_url": "https://arxiv.org/pdf/2404.06704v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2404.06704",
    "arxiv_authors": [
      "Guohang Shan",
      "Shuangcheng Jia"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Convolution-based+Probability+Gradient+Loss+for+Semantic+Segmentation+Guohang+Shan+Shuangcheng+Jia",
    "gs_search_success": true,
    "gs_authors": [],
    "citation_count": 2,
    "gs_author_count": 2
  },
  {
    "arxiv_id": "2501.03637",
    "title": "Advancing the Understanding of Fine-Grained 3D Forest Structures using Digital Cousins and Simulation-to-Reality: Methods and Datasets",
    "year": 2025,
    "published": "2025-01-07T09:12:55Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Understanding and analyzing the spatial semantics and structure of forests is essential for accurate forest resource monitoring and ecosystem research. However, the lack of large-scale and annotated datasets has limited the widespread use of advanced intelligent techniques in this field. To address this challenge, a fully automated synthetic data generation and processing framework based on the concepts of Digital Cousins and Simulation-to-Reality (Sim2Real) is proposed, offering versatility and",
    "arxiv_url": "https://arxiv.org/abs/2501.03637v1",
    "pdf_url": "https://arxiv.org/pdf/2501.03637v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2501.03637",
    "arxiv_authors": [
      "Jing Liu",
      "Duanchu Wang",
      "Haoran Gong",
      "Chongyu Wang",
      "Jihua Zhu",
      "Di Wang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Advancing+the+Understanding+of+Fine-Grained+3D+Forest+Structures+using+Digital+Cousins+and+Simulation-to-Reality%3A+Methods+and+Datasets+Jing+Liu+Duanchu+Wang+Haoran+Gong+Chongyu+Wang+Jihua+Zhu",
    "gs_search_success": true,
    "gs_authors": [
      "yQSNFXAAAAAJ",
      "YDTW8wgAAAAJ",
      "EZtQGX4AAAAJ"
    ],
    "citation_count": 3,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2311.17776",
    "title": "One-Shot Open Affordance Learning with Foundation Models",
    "year": 2023,
    "published": "2023-11-29T16:23:06Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "We introduce One-shot Open Affordance Learning (OOAL), where a model is trained with just one example per base object category, but is expected to identify novel objects and affordances. While vision-language models excel at recognizing novel objects and scenes, they often struggle to understand finer levels of granularity such as affordances. To handle this issue, we conduct a comprehensive analysis of existing foundation models, to explore their inherent understanding of affordances and assess",
    "arxiv_url": "https://arxiv.org/abs/2311.17776v1",
    "pdf_url": "https://arxiv.org/pdf/2311.17776v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2311.17776",
    "arxiv_authors": [
      "Gen Li",
      "Deqing Sun",
      "Laura Sevilla-Lara",
      "Varun Jampani"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=One-Shot+Open+Affordance+Learning+with+Foundation+Models+Gen+Li+Deqing+Sun+Laura+Sevilla-Lara+Varun+Jampani",
    "gs_search_success": true,
    "gs_authors": [
      "bel5BBcAAAAJ",
      "t4rgICIAAAAJ",
      "Rfh4mm0AAAAJ",
      "1Cv6Sf4AAAAJ"
    ],
    "citation_count": 46,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2412.07984",
    "title": "Diffusion-Based Attention Warping for Consistent 3D Scene Editing",
    "year": 2024,
    "published": "2024-12-10T23:57:18Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "We present a novel method for 3D scene editing using diffusion models, designed to ensure view consistency and realism across perspectives. Our approach leverages attention features extracted from a single reference image to define the intended edits. These features are warped across multiple views by aligning them with scene geometry derived from Gaussian splatting depth estimates. Injecting these warped features into other viewpoints enables coherent propagation of edits, achieving high fideli",
    "arxiv_url": "https://arxiv.org/abs/2412.07984v1",
    "pdf_url": "https://arxiv.org/pdf/2412.07984v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2412.07984",
    "arxiv_authors": [
      "Eyal Gomel",
      "Lior Wolf"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Diffusion-Based+Attention+Warping+for+Consistent+3D+Scene+Editing+Eyal+Gomel+Lior+Wolf",
    "gs_search_success": true,
    "gs_authors": [
      "aQxJrDMAAAAJ",
      "UbFrXTsAAAAJ"
    ],
    "citation_count": 3,
    "gs_author_count": 2
  },
  {
    "arxiv_id": "2303.15893",
    "title": "VIVE3D: Viewpoint-Independent Video Editing using 3D-Aware GANs",
    "year": 2023,
    "published": "2023-03-28T11:15:57Z",
    "categories": [
      "cs.CV",
      "cs.GR",
      "cs.LG"
    ],
    "abstract": "We introduce VIVE3D, a novel approach that extends the capabilities of image-based 3D GANs to video editing and is able to represent the input video in an identity-preserving and temporally consistent way. We propose two new building blocks. First, we introduce a novel GAN inversion technique specifically tailored to 3D GANs by jointly embedding multiple frames and optimizing for the camera parameters. Second, besides traditional semantic face edits (e.g. for age and expression), we are the firs",
    "arxiv_url": "https://arxiv.org/abs/2303.15893v1",
    "pdf_url": "https://arxiv.org/pdf/2303.15893v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2303.15893",
    "arxiv_authors": [
      "Anna Frühstück",
      "Nikolaos Sarafianos",
      "Yuanlu Xu",
      "Peter Wonka",
      "Tony Tung"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=VIVE3D%3A+Viewpoint-Independent+Video+Editing+using+3D-Aware+GANs+Anna+Fr%C3%BChst%C3%BCck+Nikolaos+Sarafianos+Yuanlu+Xu+Peter+Wonka+Tony+Tung",
    "gs_search_success": true,
    "gs_authors": [
      "0EKXSXgAAAAJ",
      "O_TOBmAAAAAJ",
      "fvr-J3sAAAAJ",
      "d0hUiVIAAAAJ",
      "ni3EbYgAAAAJ"
    ],
    "citation_count": 27,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2301.00973",
    "title": "Detecting Severity of Diabetic Retinopathy from Fundus Images: A Transformer Network-based Review",
    "year": 2023,
    "published": "2023-01-03T07:05:38Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Diabetic Retinopathy (DR) is considered one of the significant concerns worldwide, primarily due to its impact on causing vision loss among most people with diabetes. The severity of DR is typically comprehended manually by ophthalmologists from fundus photography-based retina images. This paper deals with an automated understanding of the severity stages of DR. In the literature, researchers have focused on this automation using traditional machine learning-based algorithms and convolutional ar",
    "arxiv_url": "https://arxiv.org/abs/2301.00973v2",
    "pdf_url": "https://arxiv.org/pdf/2301.00973v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2301.00973",
    "arxiv_authors": [
      "Tejas Karkera",
      "Chandranath Adak",
      "Soumi Chattopadhyay",
      "Muhammad Saqib"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Detecting+Severity+of+Diabetic+Retinopathy+from+Fundus+Images%3A+A+Transformer+Network-based+Review+Tejas+Karkera+Chandranath+Adak+Soumi+Chattopadhyay+Muhammad+Saqib",
    "gs_search_success": true,
    "gs_authors": [
      "f49iYVQAAAAJ",
      "KvbLR3gAAAAJ",
      "BgU17cEAAAAJ"
    ],
    "citation_count": 18,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2311.02692",
    "title": "ChEF: A Comprehensive Evaluation Framework for Standardized Assessment of Multimodal Large Language Models",
    "year": 2023,
    "published": "2023-11-05T16:01:40Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Multimodal Large Language Models (MLLMs) have shown impressive abilities in interacting with visual content with myriad potential downstream tasks. However, even though a list of benchmarks has been proposed, the capabilities and limitations of MLLMs are still not comprehensively understood, due to a lack of a standardized and holistic evaluation framework. To this end, we present the first Comprehensive Evaluation Framework (ChEF) that can holistically profile each MLLM and fairly compare diffe",
    "arxiv_url": "https://arxiv.org/abs/2311.02692v1",
    "pdf_url": "https://arxiv.org/pdf/2311.02692v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2311.02692",
    "arxiv_authors": [
      "Zhelun Shi",
      "Zhipin Wang",
      "Hongxing Fan",
      "Zhenfei Yin",
      "Lu Sheng",
      "Yu Qiao",
      "Jing Shao"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=ChEF%3A+A+Comprehensive+Evaluation+Framework+for+Standardized+Assessment+of+Multimodal+Large+Language+Models+Zhelun+Shi+Zhipin+Wang+Hongxing+Fan+Zhenfei+Yin+Lu+Sheng",
    "gs_search_success": true,
    "gs_authors": [
      "gFtI-8QAAAAJ",
      "ngPR1dIAAAAJ",
      "EDLcoVkAAAAJ",
      "EYMVsHUAAAAJ",
      "_8lB7xcAAAAJ",
      "Wnk95ccAAAAJ"
    ],
    "citation_count": 13,
    "gs_author_count": 7
  },
  {
    "arxiv_id": "2405.02784",
    "title": "MR-Transformer: Vision Transformer for Total Knee Replacement Prediction Using Magnetic Resonance Imaging",
    "year": 2024,
    "published": "2024-05-05T01:59:11Z",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "abstract": "A transformer-based deep learning model, MR-Transformer, was developed for total knee replacement (TKR) prediction using magnetic resonance imaging (MRI). The model incorporates the ImageNet pre-training and captures three-dimensional (3D) spatial correlation from the MR images. The performance of the proposed model was compared to existing state-of-the-art deep learning models for knee injury diagnosis using MRI. Knee MR scans of four different tissue contrasts from the Osteoarthritis Initiativ",
    "arxiv_url": "https://arxiv.org/abs/2405.02784v1",
    "pdf_url": "https://arxiv.org/pdf/2405.02784v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2405.02784",
    "arxiv_authors": [
      "Chaojie Zhang",
      "Shengjia Chen",
      "Ozkan Cigdem",
      "Haresh Rengaraj Rajamohan",
      "Kyunghyun Cho",
      "Richard Kijowski",
      "Cem M. Deniz"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=MR-Transformer%3A+Vision+Transformer+for+Total+Knee+Replacement+Prediction+Using+Magnetic+Resonance+Imaging+Chaojie+Zhang+Shengjia+Chen+Ozkan+Cigdem+Haresh+Rengaraj+Rajamohan+Kyunghyun+Cho",
    "gs_search_success": true,
    "gs_authors": [
      "0RAmmIAAAAAJ",
      "sxiGwQoAAAAJ",
      "KUwdui4AAAAJ",
      "m4ZMfR4AAAAJ",
      "WxidlecAAAAJ"
    ],
    "citation_count": 3,
    "gs_author_count": 7
  },
  {
    "arxiv_id": "2503.15835",
    "title": "BARD-GS: Blur-Aware Reconstruction of Dynamic Scenes via Gaussian Splatting",
    "year": 2025,
    "published": "2025-03-20T04:23:52Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has shown remarkable potential for static scene reconstruction, and recent advancements have extended its application to dynamic scenes. However, the quality of reconstructions depends heavily on high-quality input images and precise camera poses, which are not that trivial to fulfill in real-world scenarios. Capturing dynamic scenes with handheld monocular cameras, for instance, typically involves simultaneous movement of both the camera and objects within a single ",
    "arxiv_url": "https://arxiv.org/abs/2503.15835v1",
    "pdf_url": "https://arxiv.org/pdf/2503.15835v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2503.15835",
    "arxiv_authors": [
      "Yiren Lu",
      "Yunlai Zhou",
      "Disheng Liu",
      "Tuo Liang",
      "Yu Yin"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=BARD-GS%3A+Blur-Aware+Reconstruction+of+Dynamic+Scenes+via+Gaussian+Splatting+Yiren+Lu+Yunlai+Zhou+Disheng+Liu+Tuo+Liang+Yu+Yin",
    "gs_search_success": true,
    "gs_authors": [
      "YBKC-AoAAAAJ",
      "ytck1QsAAAAJ",
      "8euSVtcAAAAJ",
      "pY0_YNcAAAAJ",
      "xlIBwREAAAAJ"
    ],
    "citation_count": 10,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2309.16668",
    "title": "RealFill: Reference-Driven Generation for Authentic Image Completion",
    "year": 2023,
    "published": "2023-09-28T17:59:29Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR",
      "cs.LG"
    ],
    "abstract": "Recent advances in generative imagery have brought forth outpainting and inpainting models that can produce high-quality, plausible image content in unknown regions. However, the content these models hallucinate is necessarily inauthentic, since they are unaware of the true scene. In this work, we propose RealFill, a novel generative approach for image completion that fills in missing regions of an image with the content that should have been there. RealFill is a generative inpainting model that",
    "arxiv_url": "https://arxiv.org/abs/2309.16668v2",
    "pdf_url": "https://arxiv.org/pdf/2309.16668v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2309.16668",
    "arxiv_authors": [
      "Luming Tang",
      "Nataniel Ruiz",
      "Qinghao Chu",
      "Yuanzhen Li",
      "Aleksander Holynski",
      "David E. Jacobs",
      "Bharath Hariharan",
      "Yael Pritch",
      "Neal Wadhwa",
      "Kfir Aberman",
      "Michael Rubinstein"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=RealFill%3A+Reference-Driven+Generation+for+Authentic+Image+Completion+Luming+Tang+Nataniel+Ruiz+Qinghao+Chu+Yuanzhen+Li+Aleksander+Holynski",
    "gs_search_success": true,
    "gs_authors": [
      "CiOmcSIAAAAJ",
      "ypBMJMgAAAAJ",
      "Zi5KiDsAAAAJ",
      "xLn7bRAAAAAJ",
      "0VQ1sjcAAAAJ",
      "Fm5mjCAAAAAJ",
      "k1eaag4AAAAJ",
      "TpglobcAAAAJ",
      "116n5vIAAAAJ"
    ],
    "citation_count": 67,
    "gs_author_count": 9
  },
  {
    "arxiv_id": "2409.09530",
    "title": "An Augmentation-based Model Re-adaptation Framework for Robust Image Segmentation",
    "year": 2024,
    "published": "2024-09-14T21:01:49Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Image segmentation is a crucial task in computer vision, with wide-ranging applications in industry. The Segment Anything Model (SAM) has recently attracted intensive attention; however, its application in industrial inspection, particularly for segmenting commercial anti-counterfeit codes, remains challenging. Unlike open-source datasets, industrial settings often face issues such as small sample sizes and complex textures. Additionally, computational cost is a key concern due to the varying nu",
    "arxiv_url": "https://arxiv.org/abs/2409.09530v1",
    "pdf_url": "https://arxiv.org/pdf/2409.09530v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2409.09530",
    "arxiv_authors": [
      "Zheming Zuo",
      "Joseph Smith",
      "Jonathan Stonehouse",
      "Boguslaw Obara"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=An+Augmentation-based+Model+Re-adaptation+Framework+for+Robust+Image+Segmentation+Zheming+Zuo+Joseph+Smith+Jonathan+Stonehouse+Boguslaw+Obara",
    "gs_search_success": true,
    "gs_authors": [
      "hb_vGvoAAAAJ",
      "lg9xfisAAAAJ",
      "jzpjf4UAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2405.06636",
    "title": "Federated Document Visual Question Answering: A Pilot Study",
    "year": 2024,
    "published": "2024-05-10T17:53:05Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "abstract": "An important handicap of document analysis research is that documents tend to be copyrighted or contain private information, which prohibits their open publication and the creation of centralised, large-scale document datasets. Instead, documents are scattered in private data silos, making extensive training over heterogeneous data a tedious task. In this work, we explore the use of a federated learning (FL) scheme as a way to train a shared model on decentralised private document data. We focus",
    "arxiv_url": "https://arxiv.org/abs/2405.06636v2",
    "pdf_url": "https://arxiv.org/pdf/2405.06636v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2405.06636",
    "arxiv_authors": [
      "Khanh Nguyen",
      "Dimosthenis Karatzas"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Federated+Document+Visual+Question+Answering%3A+A+Pilot+Study+Khanh+Nguyen+Dimosthenis+Karatzas",
    "gs_search_success": true,
    "gs_authors": [
      "FEf3gmEAAAAJ",
      "xASEtrUAAAAJ"
    ],
    "citation_count": 2,
    "gs_author_count": 2
  },
  {
    "arxiv_id": "2412.00692",
    "title": "MCBLT: Multi-Camera Multi-Object 3D Tracking in Long Videos",
    "year": 2024,
    "published": "2024-12-01T06:18:06Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Object perception from multi-view cameras is crucial for intelligent systems, particularly in indoor environments, e.g., warehouses, retail stores, and hospitals. Most traditional multi-target multi-camera (MTMC) detection and tracking methods rely on 2D object detection, single-view multi-object tracking (MOT), and cross-view re-identification (ReID) techniques, without properly handling important 3D information by multi-view image aggregation. In this paper, we propose a 3D object detection an",
    "arxiv_url": "https://arxiv.org/abs/2412.00692v3",
    "pdf_url": "https://arxiv.org/pdf/2412.00692v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2412.00692",
    "arxiv_authors": [
      "Yizhou Wang",
      "Tim Meinhardt",
      "Orcun Cetintas",
      "Cheng-Yen Yang",
      "Sameer Satish Pusegaonkar",
      "Benjamin Missaoui",
      "Sujit Biswas",
      "Zheng Tang",
      "Laura Leal-Taixé"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=MCBLT%3A+Multi-Camera+Multi-Object+3D+Tracking+in+Long+Videos+Yizhou+Wang+Tim+Meinhardt+Orcun+Cetintas+Cheng-Yen+Yang+Sameer+Satish+Pusegaonkar",
    "gs_search_success": true,
    "gs_authors": [
      "LZnH1HIAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2305.07024",
    "title": "SparseGNV: Generating Novel Views of Indoor Scenes with Sparse Input Views",
    "year": 2023,
    "published": "2023-05-11T17:58:37Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "We study to generate novel views of indoor scenes given sparse input views. The challenge is to achieve both photorealism and view consistency. We present SparseGNV: a learning framework that incorporates 3D structures and image generative models to generate novel views with three modules. The first module builds a neural point cloud as underlying geometry, providing contextual information and guidance for the target novel view. The second module utilizes a transformer-based network to map the s",
    "arxiv_url": "https://arxiv.org/abs/2305.07024v1",
    "pdf_url": "https://arxiv.org/pdf/2305.07024v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2305.07024",
    "arxiv_authors": [
      "Weihao Cheng",
      "Yan-Pei Cao",
      "Ying Shan"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=SparseGNV%3A+Generating+Novel+Views+of+Indoor+Scenes+with+Sparse+Input+Views+Weihao+Cheng+Yan-Pei+Cao+Ying+Shan",
    "gs_search_success": true,
    "gs_authors": [
      "gP-UxcoAAAAJ",
      "50194vkAAAAJ",
      "4oXBp9UAAAAJ"
    ],
    "citation_count": 3,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2504.13745",
    "title": "ESPLoRA: Enhanced Spatial Precision with Low-Rank Adaption in Text-to-Image Diffusion Models for High-Definition Synthesis",
    "year": 2025,
    "published": "2025-04-18T15:21:37Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Diffusion models have revolutionized text-to-image (T2I) synthesis, producing high-quality, photorealistic images. However, they still struggle to properly render the spatial relationships described in text prompts. To address the lack of spatial information in T2I generations, existing methods typically use external network conditioning and predefined layouts, resulting in higher computational costs and reduced flexibility. Our approach builds upon a curated dataset of spatially explicit prompt",
    "arxiv_url": "https://arxiv.org/abs/2504.13745v1",
    "pdf_url": "https://arxiv.org/pdf/2504.13745v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2504.13745",
    "arxiv_authors": [
      "Andrea Rigo",
      "Luca Stornaiuolo",
      "Mauro Martino",
      "Bruno Lepri",
      "Nicu Sebe"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=ESPLoRA%3A+Enhanced+Spatial+Precision+with+Low-Rank+Adaption+in+Text-to-Image+Diffusion+Models+for+High-Definition+Synthesis+Andrea+Rigo+Luca+Stornaiuolo+Mauro+Martino+Bruno+Lepri+Nicu+Sebe",
    "gs_search_success": true,
    "gs_authors": [
      "SuwIso8AAAAJ",
      "JfcopG0AAAAJ",
      "c9gkCgIAAAAJ",
      "stFCYOAAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2306.14169",
    "title": "A Web-based Mpox Skin Lesion Detection System Using State-of-the-art Deep Learning Models Considering Racial Diversity",
    "year": 2023,
    "published": "2023-06-25T08:23:44Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "The recent 'Mpox' outbreak, formerly known as 'Monkeypox', has become a significant public health concern and has spread to over 110 countries globally. The challenge of clinically diagnosing mpox early on is due, in part, to its similarity to other types of rashes. Computer-aided screening tools have been proven valuable in cases where Polymerase Chain Reaction (PCR) based diagnosis is not immediately available. Deep learning methods are powerful in learning complex data representations, but th",
    "arxiv_url": "https://arxiv.org/abs/2306.14169v1",
    "pdf_url": "https://arxiv.org/pdf/2306.14169v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2306.14169",
    "arxiv_authors": [
      "Shams Nafisa Ali",
      "Md. Tazuddin Ahmed",
      "Tasnim Jahan",
      "Joydip Paul",
      "S. M. Sakeef Sani",
      "Nawsabah Noor",
      "Anzirun Nahar Asma",
      "Taufiq Hasan"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=A+Web-based+Mpox+Skin+Lesion+Detection+System+Using+State-of-the-art+Deep+Learning+Models+Considering+Racial+Diversity+Shams+Nafisa+Ali+Md.+Tazuddin+Ahmed+Tasnim+Jahan+Joydip+Paul+S.+M.+Sakeef+Sani",
    "gs_search_success": true,
    "gs_authors": [
      "JqajiA4AAAAJ",
      "Utz82Y4AAAAJ",
      "q5Z1wEMAAAAJ",
      "sGKwpWsAAAAJ",
      "rrIwGEUAAAAJ"
    ],
    "citation_count": 74,
    "gs_author_count": 8
  },
  {
    "arxiv_id": "2411.10357",
    "title": "Interactive Image-Based Aphid Counting in Yellow Water Traps under Stirring Actions",
    "year": 2024,
    "published": "2024-11-15T17:06:50Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "The current vision-based aphid counting methods in water traps suffer from undercounts caused by occlusions and low visibility arising from dense aggregation of insects and other objects. To address this problem, we propose a novel aphid counting method through interactive stirring actions. We use interactive stirring to alter the distribution of aphids in the yellow water trap and capture a sequence of images which are then used for aphid detection and counting through an optimized small object",
    "arxiv_url": "https://arxiv.org/abs/2411.10357v1",
    "pdf_url": "https://arxiv.org/pdf/2411.10357v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2411.10357",
    "arxiv_authors": [
      "Xumin Gao",
      "Mark Stevens",
      "Grzegorz Cielniak"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Interactive+Image-Based+Aphid+Counting+in+Yellow+Water+Traps+under+Stirring+Actions+Xumin+Gao+Mark+Stevens+Grzegorz+Cielniak",
    "gs_search_success": true,
    "gs_authors": [
      "U9gKp38AAAAJ"
    ],
    "citation_count": 2,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2412.18911",
    "title": "Rethinking Token-wise Feature Caching: Accelerating Diffusion Transformers with Dual Feature Caching",
    "year": 2024,
    "published": "2024-12-25T14:00:14Z",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "abstract": "Diffusion Transformers (DiT) have become the dominant methods in image and video generation yet still suffer substantial computational costs. As an effective approach for DiT acceleration, feature caching methods are designed to cache the features of DiT in previous timesteps and reuse them in the next timesteps, allowing us to skip the computation in the next timesteps. Among them, token-wise feature caching has been introduced to perform different caching ratios for different tokens in DiTs, a",
    "arxiv_url": "https://arxiv.org/abs/2412.18911v2",
    "pdf_url": "https://arxiv.org/pdf/2412.18911v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2412.18911",
    "arxiv_authors": [
      "Chang Zou",
      "Evelyn Zhang",
      "Runlin Guo",
      "Haohang Xu",
      "Conghui He",
      "Xuming Hu",
      "Linfeng Zhang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Rethinking+Token-wise+Feature+Caching%3A+Accelerating+Diffusion+Transformers+with+Dual+Feature+Caching+Chang+Zou+Evelyn+Zhang+Runlin+Guo+Haohang+Xu+Conghui+He",
    "gs_search_success": true,
    "gs_authors": [
      "yWm0cJsAAAAJ",
      "dbBKbXoAAAAJ",
      "PopTv7kAAAAJ",
      "AK9VF30AAAAJ"
    ],
    "citation_count": 12,
    "gs_author_count": 7
  },
  {
    "arxiv_id": "2412.06082",
    "title": "Are foundation models for computer vision good conformal predictors?",
    "year": 2024,
    "published": "2024-12-08T22:05:38Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Recent advances in self-supervision and contrastive learning have brought the performance of foundation models to unprecedented levels in a variety of tasks. Fueled by this progress, these models are becoming the prevailing approach for a wide array of real-world vision problems, including risk-sensitive and high-stakes applications. However, ensuring safe deployment in these scenarios requires a more comprehensive understanding of their uncertainty modeling capabilities, which has been barely e",
    "arxiv_url": "https://arxiv.org/abs/2412.06082v2",
    "pdf_url": "https://arxiv.org/pdf/2412.06082v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2412.06082",
    "arxiv_authors": [
      "Leo Fillioux",
      "Julio Silva-Rodríguez",
      "Ismail Ben Ayed",
      "Paul-Henry Cournède",
      "Maria Vakalopoulou",
      "Stergios Christodoulidis",
      "Jose Dolz"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Are+foundation+models+for+computer+vision+good+conformal+predictors%3F+Leo+Fillioux+Julio+Silva-Rodr%C3%ADguez+Ismail+Ben+Ayed+Paul-Henry+Courn%C3%A8de+Maria+Vakalopoulou",
    "gs_search_success": true,
    "gs_authors": [
      "c0kBPnoAAAAJ",
      "LGr1sroAAAAJ",
      "1UMYgHMAAAAJ",
      "-h5w30sAAAAJ",
      "FKUHYqMAAAAJ",
      "29vyUccAAAAJ"
    ],
    "citation_count": 5,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2403.17937",
    "title": "Efficient Video Object Segmentation via Modulated Cross-Attention Memory",
    "year": 2024,
    "published": "2024-03-26T17:59:58Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Recently, transformer-based approaches have shown promising results for semi-supervised video object segmentation. However, these approaches typically struggle on long videos due to increased GPU memory demands, as they frequently expand the memory bank every few frames. We propose a transformer-based approach, named MAVOS, that introduces an optimized and dynamic long-term modulated cross-attention (MCA) memory to model temporal smoothness without requiring frequent memory expansion. The propos",
    "arxiv_url": "https://arxiv.org/abs/2403.17937v3",
    "pdf_url": "https://arxiv.org/pdf/2403.17937v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2403.17937",
    "arxiv_authors": [
      "Abdelrahman Shaker",
      "Syed Talal Wasim",
      "Martin Danelljan",
      "Salman Khan",
      "Ming-Hsuan Yang",
      "Fahad Shahbaz Khan"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Efficient+Video+Object+Segmentation+via+Modulated+Cross-Attention+Memory+Abdelrahman+Shaker+Syed+Talal+Wasim+Martin+Danelljan+Salman+Khan+Ming-Hsuan+Yang",
    "gs_search_success": true,
    "gs_authors": [
      "p9-ohHsAAAAJ",
      "eEz4Wu4AAAAJ",
      "uHySarAAAAAJ",
      "zvaeYnUAAAAJ",
      "NCSSpMkAAAAJ",
      "M59O9lkAAAAJ"
    ],
    "citation_count": 5,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2503.13617",
    "title": "Let Synthetic Data Shine: Domain Reassembly and Soft-Fusion for Single Domain Generalization",
    "year": 2025,
    "published": "2025-03-17T18:08:03Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Single Domain Generalization (SDG) aims to train models with consistent performance across diverse scenarios using data from a single source. While using latent diffusion models (LDMs) show promise in augmenting limited source data, we demonstrate that directly using synthetic data can be detrimental due to significant feature distribution discrepancies between synthetic and real target domains, leading to performance degradation. To address this issue, we propose Discriminative Domain Reassembl",
    "arxiv_url": "https://arxiv.org/abs/2503.13617v1",
    "pdf_url": "https://arxiv.org/pdf/2503.13617v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2503.13617",
    "arxiv_authors": [
      "Hao Li",
      "Yubin Xiao",
      "Ke Liang",
      "Mengzhu Wang",
      "Long Lan",
      "Kenli Li",
      "Xinwang Liu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Let+Synthetic+Data+Shine%3A+Domain+Reassembly+and+Soft-Fusion+for+Single+Domain+Generalization+Hao+Li+Yubin+Xiao+Ke+Liang+Mengzhu+Wang+Long+Lan",
    "gs_search_success": true,
    "gs_authors": [
      "xAk5P4AAAAAJ",
      "gwea2McAAAAJ",
      "2h-fI9wAAAAJ",
      "huVW6Y8AAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 7
  },
  {
    "arxiv_id": "2311.17396",
    "title": "Spectral and Polarization Vision: Spectro-polarimetric Real-world Dataset",
    "year": 2023,
    "published": "2023-11-29T06:53:23Z",
    "categories": [
      "cs.CV",
      "eess.IV"
    ],
    "abstract": "Image datasets are essential not only in validating existing methods in computer vision but also in developing new methods. Most existing image datasets focus on trichromatic intensity images to mimic human vision. However, polarization and spectrum, the wave properties of light that animals in harsh environments and with limited brain capacity often rely on, remain underrepresented in existing datasets. Although spectro-polarimetric datasets exist, these datasets have insufficient object divers",
    "arxiv_url": "https://arxiv.org/abs/2311.17396v2",
    "pdf_url": "https://arxiv.org/pdf/2311.17396v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2311.17396",
    "arxiv_authors": [
      "Yujin Jeon",
      "Eunsue Choi",
      "Youngchan Kim",
      "Yunseong Moon",
      "Khalid Omer",
      "Felix Heide",
      "Seung-Hwan Baek"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Spectral+and+Polarization+Vision%3A+Spectro-polarimetric+Real-world+Dataset+Yujin+Jeon+Eunsue+Choi+Youngchan+Kim+Yunseong+Moon+Khalid+Omer",
    "gs_search_success": true,
    "gs_authors": [
      "YwKp8dIAAAAJ",
      "BRwvHlIAAAAJ",
      "bOj5gQcAAAAJ",
      "M9ZnHHoAAAAJ",
      "gRqzSHsAAAAJ"
    ],
    "citation_count": 9,
    "gs_author_count": null
  },
  {
    "arxiv_id": "2505.09985",
    "title": "Ordered-subsets Multi-diffusion Model for Sparse-view CT Reconstruction",
    "year": 2025,
    "published": "2025-05-15T05:50:35Z",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "abstract": "Score-based diffusion models have shown significant promise in the field of sparse-view CT reconstruction. However, the projection dataset is large and riddled with redundancy. Consequently, applying the diffusion model to unprocessed data results in lower learning effectiveness and higher learning difficulty, frequently leading to reconstructed images that lack fine details. To address these issues, we propose the ordered-subsets multi-diffusion model (OSMM) for sparse-view CT reconstruction. T",
    "arxiv_url": "https://arxiv.org/abs/2505.09985v1",
    "pdf_url": "https://arxiv.org/pdf/2505.09985v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2505.09985",
    "arxiv_authors": [
      "Pengfei Yu",
      "Bin Huang",
      "Minghui Zhang",
      "Weiwen Wu",
      "Shaoyu Wang",
      "Qiegen Liu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Ordered-subsets+Multi-diffusion+Model+for+Sparse-view+CT+Reconstruction+Pengfei+Yu+Bin+Huang+Minghui+Zhang+Weiwen+Wu+Shaoyu+Wang",
    "gs_search_success": true,
    "gs_authors": [
      "T00zMvIAAAAJ",
      "T4o77REAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2404.09940",
    "title": "eMotion-GAN: A Motion-based GAN for Photorealistic and Facial Expression Preserving Frontal View Synthesis",
    "year": 2024,
    "published": "2024-04-15T17:08:53Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Many existing facial expression recognition (FER) systems encounter substantial performance degradation when faced with variations in head pose. Numerous frontalization methods have been proposed to enhance these systems' performance under such conditions. However, they often introduce undesirable deformations, rendering them less suitable for precise facial expression analysis. In this paper, we present eMotion-GAN, a novel deep learning approach designed for frontal view synthesis while preser",
    "arxiv_url": "https://arxiv.org/abs/2404.09940v1",
    "pdf_url": "https://arxiv.org/pdf/2404.09940v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2404.09940",
    "arxiv_authors": [
      "Omar Ikne",
      "Benjamin Allaert",
      "Ioan Marius Bilasco",
      "Hazem Wannous"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=eMotion-GAN%3A+A+Motion-based+GAN+for+Photorealistic+and+Facial+Expression+Preserving+Frontal+View+Synthesis+Omar+Ikne+Benjamin+Allaert+Ioan+Marius+Bilasco+Hazem+Wannous",
    "gs_search_success": true,
    "gs_authors": [
      "sbs8pg4AAAAJ",
      "jNBdRCsAAAAJ",
      "WsdQ9Z0AAAAJ",
      "ntkNl-8AAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2410.09902",
    "title": "Multi class activity classification in videos using Motion History Image generation",
    "year": 2024,
    "published": "2024-10-13T16:22:02Z",
    "categories": [
      "cs.CV",
      "cs.LG",
      "eess.IV"
    ],
    "abstract": "Human action recognition has been a topic of interest across multiple fields ranging from security to entertainment systems. Tracking the motion and identifying the action being performed on a real time basis is necessary for critical security systems. In entertainment, especially gaming, the need for immediate responses for actions and gestures are paramount for the success of that system. We show that Motion History image has been a well established framework to capture the temporal and activi",
    "arxiv_url": "https://arxiv.org/abs/2410.09902v1",
    "pdf_url": "https://arxiv.org/pdf/2410.09902v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2410.09902",
    "arxiv_authors": [
      "Senthilkumar Gopal"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Multi+class+activity+classification+in+videos+using+Motion+History+Image+generation+Senthilkumar+Gopal",
    "gs_search_success": true,
    "gs_authors": [
      "bs8WraEAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 1
  },
  {
    "arxiv_id": "2405.05145",
    "title": "Conformal Semantic Image Segmentation: Post-hoc Quantification of Predictive Uncertainty",
    "year": 2024,
    "published": "2024-04-16T15:51:39Z",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "We propose a post-hoc, computationally lightweight method to quantify predictive uncertainty in semantic image segmentation. Our approach uses conformal prediction to generate statistically valid prediction sets that are guaranteed to include the ground-truth segmentation mask at a predefined confidence level. We introduce a novel visualization technique of conformalized predictions based on heatmaps, and provide metrics to assess their empirical validity. We demonstrate the effectiveness of our",
    "arxiv_url": "https://arxiv.org/abs/2405.05145v1",
    "pdf_url": "https://arxiv.org/pdf/2405.05145v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2405.05145",
    "arxiv_authors": [
      "Luca Mossina",
      "Joseba Dalmau",
      "Léo andéol"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Conformal+Semantic+Image+Segmentation%3A+Post-hoc+Quantification+of+Predictive+Uncertainty+Luca+Mossina+Joseba+Dalmau+L%C3%A9o+and%C3%A9ol",
    "gs_search_success": true,
    "gs_authors": [
      "SoOpehAAAAAJ",
      "SCpz8XMAAAAJ"
    ],
    "citation_count": 23,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2308.04904",
    "title": "StableVQA: A Deep No-Reference Quality Assessment Model for Video Stability",
    "year": 2023,
    "published": "2023-08-09T12:04:36Z",
    "categories": [
      "cs.CV",
      "eess.IV"
    ],
    "abstract": "Video shakiness is an unpleasant distortion of User Generated Content (UGC) videos, which is usually caused by the unstable hold of cameras. In recent years, many video stabilization algorithms have been proposed, yet no specific and accurate metric enables comprehensively evaluating the stability of videos. Indeed, most existing quality assessment models evaluate video quality as a whole without specifically taking the subjective experience of video stability into consideration. Therefore, thes",
    "arxiv_url": "https://arxiv.org/abs/2308.04904v3",
    "pdf_url": "https://arxiv.org/pdf/2308.04904v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2308.04904",
    "arxiv_authors": [
      "Tengchuan Kou",
      "Xiaohong Liu",
      "Wei Sun",
      "Jun Jia",
      "Xiongkuo Min",
      "Guangtao Zhai",
      "Ning Liu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=StableVQA%3A+A+Deep+No-Reference+Quality+Assessment+Model+for+Video+Stability+Tengchuan+Kou+Xiaohong+Liu+Wei+Sun+Jun+Jia+Xiongkuo+Min",
    "gs_search_success": true,
    "gs_authors": [
      "nDlEBJ8AAAAJ",
      "1y69KtcAAAAJ",
      "Tq2hoMQAAAAJ",
      "91sjuWIAAAAJ",
      "E6zbSYgAAAAJ"
    ],
    "citation_count": 31,
    "gs_author_count": 7
  },
  {
    "arxiv_id": "2503.06271",
    "title": "SplatTalk: 3D VQA with Gaussian Splatting",
    "year": 2025,
    "published": "2025-03-08T16:31:48Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Language-guided 3D scene understanding is important for advancing applications in robotics, AR/VR, and human-computer interaction, enabling models to comprehend and interact with 3D environments through natural language. While 2D vision-language models (VLMs) have achieved remarkable success in 2D VQA tasks, progress in the 3D domain has been significantly slower due to the complexity of 3D data and the high cost of manual annotations. In this work, we introduce SplatTalk, a novel method that us",
    "arxiv_url": "https://arxiv.org/abs/2503.06271v2",
    "pdf_url": "https://arxiv.org/pdf/2503.06271v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2503.06271",
    "arxiv_authors": [
      "Anh Thai",
      "Songyou Peng",
      "Kyle Genova",
      "Leonidas Guibas",
      "Thomas Funkhouser"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=SplatTalk%3A+3D+VQA+with+Gaussian+Splatting+Anh+Thai+Songyou+Peng+Kyle+Genova+Leonidas+Guibas+Thomas+Funkhouser",
    "gs_search_success": true,
    "gs_authors": [
      "73HIeWcAAAAJ",
      "_pchxWQAAAAJ",
      "5JlEyTAAAAAJ",
      "eNypkO0AAAAJ",
      "BghVDhgAAAAJ"
    ],
    "citation_count": 9,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2309.16585",
    "title": "Text-to-3D using Gaussian Splatting",
    "year": 2023,
    "published": "2023-09-28T16:44:31Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Automatic text-to-3D generation that combines Score Distillation Sampling (SDS) with the optimization of volume rendering has achieved remarkable progress in synthesizing realistic 3D objects. Yet most existing text-to-3D methods by SDS and volume rendering suffer from inaccurate geometry, e.g., the Janus issue, since it is hard to explicitly integrate 3D priors into implicit 3D representations. Besides, it is usually time-consuming for them to generate elaborate 3D models with rich colors. In r",
    "arxiv_url": "https://arxiv.org/abs/2309.16585v4",
    "pdf_url": "https://arxiv.org/pdf/2309.16585v4",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2309.16585",
    "arxiv_authors": [
      "Zilong Chen",
      "Feng Wang",
      "Yikai Wang",
      "Huaping Liu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Text-to-3D+using+Gaussian+Splatting+Zilong+Chen+Feng+Wang+Yikai+Wang+Huaping+Liu",
    "gs_search_success": true,
    "gs_authors": [
      "MnW5aegAAAAJ",
      "bKG4Un8AAAAJ",
      "HXnkIkwAAAAJ",
      "2pbka1gAAAAJ"
    ],
    "citation_count": 330,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2503.11651",
    "title": "VGGT: Visual Geometry Grounded Transformer",
    "year": 2025,
    "published": "2025-03-14T17:59:47Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "We present VGGT, a feed-forward neural network that directly infers all key 3D attributes of a scene, including camera parameters, point maps, depth maps, and 3D point tracks, from one, a few, or hundreds of its views. This approach is a step forward in 3D computer vision, where models have typically been constrained to and specialized for single tasks. It is also simple and efficient, reconstructing images in under one second, and still outperforming alternatives that require post-processing wi",
    "arxiv_url": "https://arxiv.org/abs/2503.11651v1",
    "pdf_url": "https://arxiv.org/pdf/2503.11651v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2503.11651",
    "arxiv_authors": [
      "Jianyuan Wang",
      "Minghao Chen",
      "Nikita Karaev",
      "Andrea Vedaldi",
      "Christian Rupprecht",
      "David Novotny"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=VGGT%3A+Visual+Geometry+Grounded+Transformer+Jianyuan+Wang+Minghao+Chen+Nikita+Karaev+Andrea+Vedaldi+Christian+Rupprecht",
    "gs_search_success": true,
    "gs_authors": [
      "2glXz7cAAAAJ",
      "IrYlproAAAAJ",
      "DW-X-Y8AAAAJ",
      "2JxJjCoAAAAJ",
      "2wk2RdgAAAAJ",
      "bRT7t28AAAAJ"
    ],
    "citation_count": 365,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2406.17575",
    "title": "Toward Universal Medical Image Registration via Sharpness-Aware Meta-Continual Learning",
    "year": 2024,
    "published": "2024-06-25T14:15:48Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Current deep learning approaches in medical image registration usually face the challenges of distribution shift and data collection, hindering real-world deployment. In contrast, universal medical image registration aims to perform registration on a wide range of clinically relevant tasks simultaneously, thus having tremendous potential for clinical applications. In this paper, we present the first attempt to achieve the goal of universal 3D medical image registration in sequential learning sce",
    "arxiv_url": "https://arxiv.org/abs/2406.17575v1",
    "pdf_url": "https://arxiv.org/pdf/2406.17575v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2406.17575",
    "arxiv_authors": [
      "Bomin Wang",
      "Xinzhe Luo",
      "Xiahai Zhuang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Toward+Universal+Medical+Image+Registration+via+Sharpness-Aware+Meta-Continual+Learning+Bomin+Wang+Xinzhe+Luo+Xiahai+Zhuang",
    "gs_search_success": true,
    "gs_authors": [
      "XlavIMcAAAAJ",
      "l-oyIaAAAAAJ",
      "JAXdcAMAAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2410.11307",
    "title": "CONSULT: Contrastive Self-Supervised Learning for Few-shot Tumor Detection",
    "year": 2024,
    "published": "2024-10-15T06:09:28Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Artificial intelligence aids in brain tumor detection via MRI scans, enhancing the accuracy and reducing the workload of medical professionals. However, in scenarios with extremely limited medical images, traditional deep learning approaches tend to fail due to the absence of anomalous images. Anomaly detection also suffers from ineffective feature extraction due to vague training process. Our work introduces a novel two-stage anomaly detection algorithm called CONSULT (CONtrastive Self-sUpervis",
    "arxiv_url": "https://arxiv.org/abs/2410.11307v1",
    "pdf_url": "https://arxiv.org/pdf/2410.11307v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2410.11307",
    "arxiv_authors": [
      "Sin Chee Chin",
      "Xuan Zhang",
      "Lee Yeong Khang",
      "Wenming Yang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=CONSULT%3A+Contrastive+Self-Supervised+Learning+for+Few-shot+Tumor+Detection+Sin+Chee+Chin+Xuan+Zhang+Lee+Yeong+Khang+Wenming+Yang",
    "gs_search_success": true,
    "gs_authors": [
      "vsE4nKcAAAAJ",
      "WWLeQm4AAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": null
  },
  {
    "arxiv_id": "2502.13716",
    "title": "Event-Based Video Frame Interpolation With Cross-Modal Asymmetric Bidirectional Motion Fields",
    "year": 2025,
    "published": "2025-02-19T13:40:43Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Video Frame Interpolation (VFI) aims to generate intermediate video frames between consecutive input frames. Since the event cameras are bio-inspired sensors that only encode brightness changes with a micro-second temporal resolution, several works utilized the event camera to enhance the performance of VFI. However, existing methods estimate bidirectional inter-frame motion fields with only events or approximations, which can not consider the complex motion in real-world scenarios. In this pape",
    "arxiv_url": "https://arxiv.org/abs/2502.13716v1",
    "pdf_url": "https://arxiv.org/pdf/2502.13716v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2502.13716",
    "arxiv_authors": [
      "Taewoo Kim",
      "Yujeong Chae",
      "Hyun-Kurl Jang",
      "Kuk-Jin Yoon"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Event-Based+Video+Frame+Interpolation+With+Cross-Modal+Asymmetric+Bidirectional+Motion+Fields+Taewoo+Kim+Yujeong+Chae+Hyun-Kurl+Jang+Kuk-Jin+Yoon",
    "gs_search_success": false,
    "gs_authors": [],
    "citation_count": null,
    "gs_author_count": null,
    "error_type": "fetch_failed"
  },
  {
    "arxiv_id": "2309.12118",
    "title": "Vulnerability of 3D Face Recognition Systems to Morphing Attacks",
    "year": 2023,
    "published": "2023-09-21T14:36:10Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "In recent years face recognition systems have been brought to the mainstream due to development in hardware and software. Consistent efforts are being made to make them better and more secure. This has also brought developments in 3D face recognition systems at a rapid pace. These 3DFR systems are expected to overcome certain vulnerabilities of 2DFR systems. One such problem that the domain of 2DFR systems face is face image morphing. A substantial amount of research is being done for generation",
    "arxiv_url": "https://arxiv.org/abs/2309.12118v1",
    "pdf_url": "https://arxiv.org/pdf/2309.12118v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2309.12118",
    "arxiv_authors": [
      "Sanjeet Vardam",
      "Luuk Spreeuwers"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Vulnerability+of+3D+Face+Recognition+Systems+to+Morphing+Attacks+Sanjeet+Vardam+Luuk+Spreeuwers",
    "gs_search_success": true,
    "gs_authors": [
      "YrhxxlcAAAAJ"
    ],
    "citation_count": 2,
    "gs_author_count": 2
  },
  {
    "arxiv_id": "2403.19811",
    "title": "X-MIC: Cross-Modal Instance Conditioning for Egocentric Action Generalization",
    "year": 2024,
    "published": "2024-03-28T19:45:35Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Lately, there has been growing interest in adapting vision-language models (VLMs) to image and third-person video classification due to their success in zero-shot recognition. However, the adaptation of these models to egocentric videos has been largely unexplored. To address this gap, we propose a simple yet effective cross-modal adaptation framework, which we call X-MIC. Using a video adapter, our pipeline learns to align frozen text embeddings to each egocentric video directly in the shared e",
    "arxiv_url": "https://arxiv.org/abs/2403.19811v1",
    "pdf_url": "https://arxiv.org/pdf/2403.19811v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2403.19811",
    "arxiv_authors": [
      "Anna Kukleva",
      "Fadime Sener",
      "Edoardo Remelli",
      "Bugra Tekin",
      "Eric Sauser",
      "Bernt Schiele",
      "Shugao Ma"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=X-MIC%3A+Cross-Modal+Instance+Conditioning+for+Egocentric+Action+Generalization+Anna+Kukleva+Fadime+Sener+Edoardo+Remelli+Bugra+Tekin+Eric+Sauser",
    "gs_search_success": true,
    "gs_authors": [
      "yz2P_aUAAAAJ",
      "HDYUYvwAAAAJ",
      "SUd2LJUAAAAJ",
      "3fa02HAAAAAJ",
      "eLZ_clAAAAAJ",
      "z76PBfYAAAAJ",
      "-juoweoAAAAJ"
    ],
    "citation_count": 7,
    "gs_author_count": 7
  },
  {
    "arxiv_id": "2312.01128",
    "title": "SPEEDNet: Salient Pyramidal Enhancement Encoder-Decoder Network for Colonoscopy Images",
    "year": 2023,
    "published": "2023-12-02T13:03:08Z",
    "categories": [
      "eess.IV",
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "Accurate identification and precise delineation of regions of significance, such as tumors or lesions, is a pivotal goal in medical imaging analysis. This paper proposes SPEEDNet, a novel architecture for precisely segmenting lesions within colonoscopy images. SPEEDNet uses a novel block named Dilated-Involutional Pyramidal Convolution Fusion (DIPC). A DIPC block combines the dilated involution layers pairwise into a pyramidal structure to convert the feature maps into a compact space. This lowe",
    "arxiv_url": "https://arxiv.org/abs/2312.01128v1",
    "pdf_url": "https://arxiv.org/pdf/2312.01128v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2312.01128",
    "arxiv_authors": [
      "Tushir Sahu",
      "Vidhi Bhatt",
      "Sai Chandra Teja R",
      "Sparsh Mittal",
      "Nagesh Kumar S"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=SPEEDNet%3A+Salient+Pyramidal+Enhancement+Encoder-Decoder+Network+for+Colonoscopy+Images+Tushir+Sahu+Vidhi+Bhatt+Sai+Chandra+Teja+R+Sparsh+Mittal+Nagesh+Kumar+S",
    "gs_search_success": true,
    "gs_authors": [
      "aw8t8sgAAAAJ",
      "ZYpOpT4AAAAJ",
      "Hz44YrEAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2310.16459",
    "title": "DualMatch: Robust Semi-Supervised Learning with Dual-Level Interaction",
    "year": 2023,
    "published": "2023-10-25T08:34:05Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Semi-supervised learning provides an expressive framework for exploiting unlabeled data when labels are insufficient. Previous semi-supervised learning methods typically match model predictions of different data-augmented views in a single-level interaction manner, which highly relies on the quality of pseudo-labels and results in semi-supervised learning not robust. In this paper, we propose a novel SSL method called DualMatch, in which the class prediction jointly invokes feature embedding in ",
    "arxiv_url": "https://arxiv.org/abs/2310.16459v1",
    "pdf_url": "https://arxiv.org/pdf/2310.16459v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2310.16459",
    "arxiv_authors": [
      "Cong Wang",
      "Xiaofeng Cao",
      "Lanzhe Guo2",
      "Zenglin Shi"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=DualMatch%3A+Robust+Semi-Supervised+Learning+with+Dual-Level+Interaction+Cong+Wang+Xiaofeng+Cao+Lanzhe+Guo2+Zenglin+Shi",
    "gs_search_success": true,
    "gs_authors": [
      "gGK6E38AAAAJ",
      "dpunvqgAAAAJ",
      "cpIQPy8AAAAJ"
    ],
    "citation_count": 6,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2411.14883",
    "title": "Boundless Across Domains: A New Paradigm of Adaptive Feature and Cross-Attention for Domain Generalization in Medical Image Segmentation",
    "year": 2024,
    "published": "2024-11-22T12:06:24Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "abstract": "Domain-invariant representation learning is a powerful method for domain generalization. Previous approaches face challenges such as high computational demands, training instability, and limited effectiveness with high-dimensional data, potentially leading to the loss of valuable features. To address these issues, we hypothesize that an ideal generalized representation should exhibit similar pattern responses within the same channel across cross-domain images. Based on this hypothesis, we use de",
    "arxiv_url": "https://arxiv.org/abs/2411.14883v1",
    "pdf_url": "https://arxiv.org/pdf/2411.14883v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2411.14883",
    "arxiv_authors": [
      "Yuheng Xu",
      "Taiping Zhang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Boundless+Across+Domains%3A+A+New+Paradigm+of+Adaptive+Feature+and+Cross-Attention+for+Domain+Generalization+in+Medical+Image+Segmentation+Yuheng+Xu+Taiping+Zhang",
    "gs_search_success": true,
    "gs_authors": [],
    "citation_count": null,
    "gs_author_count": 2
  },
  {
    "arxiv_id": "2502.10674",
    "title": "Occlusion-aware Text-Image-Point Cloud Pretraining for Open-World 3D Object Recognition",
    "year": 2025,
    "published": "2025-02-15T04:58:21Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Recent open-world representation learning approaches have leveraged CLIP to enable zero-shot 3D object recognition. However, performance on real point clouds with occlusions still falls short due to unrealistic pretraining settings. Additionally, these methods incur high inference costs because they rely on Transformer's attention modules. In this paper, we make two contributions to address these limitations. First, we propose occlusion-aware text-image-point cloud pretraining to reduce the trai",
    "arxiv_url": "https://arxiv.org/abs/2502.10674v2",
    "pdf_url": "https://arxiv.org/pdf/2502.10674v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2502.10674",
    "arxiv_authors": [
      "Khanh Nguyen",
      "Ghulam Mubashar Hassan",
      "Ajmal Mian"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Occlusion-aware+Text-Image-Point+Cloud+Pretraining+for+Open-World+3D+Object+Recognition+Khanh+Nguyen+Ghulam+Mubashar+Hassan+Ajmal+Mian",
    "gs_search_success": true,
    "gs_authors": [
      "X589yaIAAAAJ",
      "LoOglv4AAAAJ",
      "GAeq5xYAAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2410.00337",
    "title": "SyntheOcc: Synthesize Geometric-Controlled Street View Images through 3D Semantic MPIs",
    "year": 2024,
    "published": "2024-10-01T02:29:24Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "The advancement of autonomous driving is increasingly reliant on high-quality annotated datasets, especially in the task of 3D occupancy prediction, where the occupancy labels require dense 3D annotation with significant human effort. In this paper, we propose SyntheOcc, which denotes a diffusion model that Synthesize photorealistic and geometric-controlled images by conditioning Occupancy labels in driving scenarios. This yields an unlimited amount of diverse, annotated, and controllable datase",
    "arxiv_url": "https://arxiv.org/abs/2410.00337v1",
    "pdf_url": "https://arxiv.org/pdf/2410.00337v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2410.00337",
    "arxiv_authors": [
      "Leheng Li",
      "Weichao Qiu",
      "Yingjie Cai",
      "Xu Yan",
      "Qing Lian",
      "Bingbing Liu",
      "Ying-Cong Chen"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=SyntheOcc%3A+Synthesize+Geometric-Controlled+Street+View+Images+through+3D+Semantic+MPIs+Leheng+Li+Weichao+Qiu+Yingjie+Cai+Xu+Yan+Qing+Lian",
    "gs_search_success": false,
    "gs_authors": [],
    "citation_count": null,
    "gs_author_count": null,
    "error_type": "fetch_failed"
  },
  {
    "arxiv_id": "2505.01799",
    "title": "AquaGS: Fast Underwater Scene Reconstruction with SfM-Free Gaussian Splatting",
    "year": 2025,
    "published": "2025-05-03T12:05:57Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Underwater scene reconstruction is a critical tech-nology for underwater operations, enabling the generation of 3D models from images captured by underwater platforms. However, the quality of underwater images is often degraded due to medium interference, which limits the effectiveness of Structure-from-Motion (SfM) pose estimation, leading to subsequent reconstruction failures. Additionally, SfM methods typically operate at slower speeds, further hindering their applicability in real-time scena",
    "arxiv_url": "https://arxiv.org/abs/2505.01799v1",
    "pdf_url": "https://arxiv.org/pdf/2505.01799v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2505.01799",
    "arxiv_authors": [
      "Junhao Shi",
      "Jisheng Xu",
      "Jianping He",
      "Zhiliang Lin"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=AquaGS%3A+Fast+Underwater+Scene+Reconstruction+with+SfM-Free+Gaussian+Splatting+Junhao+Shi+Jisheng+Xu+Jianping+He+Zhiliang+Lin",
    "gs_search_success": true,
    "gs_authors": [],
    "citation_count": null,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2410.02401",
    "title": "SynCo: Synthetic Hard Negatives for Contrastive Visual Representation Learning",
    "year": 2024,
    "published": "2024-10-03T11:29:09Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Contrastive learning has become a dominant approach in self-supervised visual representation learning, but efficiently leveraging hard negatives, which are samples closely resembling the anchor, remains challenging. We introduce SynCo (Synthetic negatives in Contrastive learning), a novel approach that improves model performance by generating synthetic hard negatives on the representation space. Building on the MoCo framework, SynCo introduces six strategies for creating diverse synthetic hard n",
    "arxiv_url": "https://arxiv.org/abs/2410.02401v7",
    "pdf_url": "https://arxiv.org/pdf/2410.02401v7",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2410.02401",
    "arxiv_authors": [
      "Nikolaos Giakoumoglou",
      "Tania Stathaki"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=SynCo%3A+Synthetic+Hard+Negatives+for+Contrastive+Visual+Representation+Learning+Nikolaos+Giakoumoglou+Tania+Stathaki",
    "gs_search_success": true,
    "gs_authors": [
      "sAB5gI8AAAAJ",
      "bDAQu54AAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 2
  },
  {
    "arxiv_id": "2502.06973",
    "title": "Indoor Heat Estimation from a Single Visible-Light Panorama",
    "year": 2025,
    "published": "2025-02-10T19:12:41Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "This paper introduces a novel image-based rendering technique for jointly estimating indoor lighting and thermal conditions from paired indoor-outdoor high dynamic range (HDR) panoramas. Our method uses the indoor panorama to estimate the 3D floor layout, while the corresponding outdoor panorama serves as an environment map to infer spatially-varying illumination and material properties. Assuming indoor surfaces are Lambertian and that all heat originates from outdoor visible light, we model the",
    "arxiv_url": "https://arxiv.org/abs/2502.06973v2",
    "pdf_url": "https://arxiv.org/pdf/2502.06973v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2502.06973",
    "arxiv_authors": [
      "Guanzhou Ji",
      "Sriram Narayanan",
      "Azadeh Sawyer",
      "Srinivasa Narasimhan"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Indoor+Heat+Estimation+from+a+Single+Visible-Light+Panorama+Guanzhou+Ji+Sriram+Narayanan+Azadeh+Sawyer+Srinivasa+Narasimhan",
    "gs_search_success": true,
    "gs_authors": [
      "MhYrLJAAAAAJ",
      "P0CpOFwAAAAJ",
      "QCJ0RpkAAAAJ",
      "IHqvM8gAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2505.06861",
    "title": "Efficient Robotic Policy Learning via Latent Space Backward Planning",
    "year": 2025,
    "published": "2025-05-11T06:13:51Z",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV"
    ],
    "abstract": "Current robotic planning methods often rely on predicting multi-frame images with full pixel details. While this fine-grained approach can serve as a generic world model, it introduces two significant challenges for downstream policy learning: substantial computational costs that hinder real-time deployment, and accumulated inaccuracies that can mislead action extraction. Planning with coarse-grained subgoals partially alleviates efficiency issues. However, their forward planning schemes can sti",
    "arxiv_url": "https://arxiv.org/abs/2505.06861v2",
    "pdf_url": "https://arxiv.org/pdf/2505.06861v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2505.06861",
    "arxiv_authors": [
      "Dongxiu Liu",
      "Haoyi Niu",
      "Zhihao Wang",
      "Jinliang Zheng",
      "Yinan Zheng",
      "Zhonghong Ou",
      "Jianming Hu",
      "Jianxiong Li",
      "Xianyuan Zhan"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Efficient+Robotic+Policy+Learning+via+Latent+Space+Backward+Planning+Dongxiu+Liu+Haoyi+Niu+Zhihao+Wang+Jinliang+Zheng+Yinan+Zheng",
    "gs_search_success": true,
    "gs_authors": [
      "TRLwpiUAAAAJ",
      "pDMnGloAAAAJ",
      "R-DSH0kAAAAJ",
      "OD-QFQQAAAAJ",
      "6zozRRYAAAAJ",
      "QbZB7QUAAAAJ",
      "3j5AHFsAAAAJ",
      "aYvRNFYAAAAJ",
      "mHXjEbQAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 9
  },
  {
    "arxiv_id": "2502.11756",
    "title": "On the Computation of the Fisher Information in Continual Learning",
    "year": 2025,
    "published": "2025-02-17T12:52:10Z",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "stat.ML"
    ],
    "abstract": "One of the most popular methods for continual learning with deep neural networks is Elastic Weight Consolidation (EWC), which involves computing the Fisher Information. The exact way in which the Fisher Information is computed is however rarely described, and multiple different implementations for it can be found online. This blog post discusses and empirically compares several often-used implementations, which highlights that many currently reported results for EWC could likely be improved by c",
    "arxiv_url": "https://arxiv.org/abs/2502.11756v1",
    "pdf_url": "https://arxiv.org/pdf/2502.11756v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2502.11756",
    "arxiv_authors": [
      "Gido M. van de Ven"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=On+the+Computation+of+the+Fisher+Information+in+Continual+Learning+Gido+M.+van+de+Ven",
    "gs_search_success": true,
    "gs_authors": [
      "3k0l15MAAAAJ"
    ],
    "citation_count": 6,
    "gs_author_count": 1
  },
  {
    "arxiv_id": "2312.12142",
    "title": "FontDiffuser: One-Shot Font Generation via Denoising Diffusion with Multi-Scale Content Aggregation and Style Contrastive Learning",
    "year": 2023,
    "published": "2023-12-19T13:23:20Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Automatic font generation is an imitation task, which aims to create a font library that mimics the style of reference images while preserving the content from source images. Although existing font generation methods have achieved satisfactory performance, they still struggle with complex characters and large style variations. To address these issues, we propose FontDiffuser, a diffusion-based image-to-image one-shot font generation method, which innovatively models the font imitation task as a ",
    "arxiv_url": "https://arxiv.org/abs/2312.12142v1",
    "pdf_url": "https://arxiv.org/pdf/2312.12142v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2312.12142",
    "arxiv_authors": [
      "Zhenhua Yang",
      "Dezhi Peng",
      "Yuxin Kong",
      "Yuyi Zhang",
      "Cong Yao",
      "Lianwen Jin"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=FontDiffuser%3A+One-Shot+Font+Generation+via+Denoising+Diffusion+with+Multi-Scale+Content+Aggregation+and+Style+Contrastive+Learning+Zhenhua+Yang+Dezhi+Peng+Yuxin+Kong+Yuyi+Zhang+Cong+Yao",
    "gs_search_success": true,
    "gs_authors": [
      "2ITs6lUAAAAJ",
      "6zNgcjAAAAAJ",
      "WMUStEUAAAAJ",
      "KBJ9ooAAAAAJ",
      "IpmnLFcAAAAJ"
    ],
    "citation_count": 77,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2305.05768",
    "title": "DifFIQA: Face Image Quality Assessment Using Denoising Diffusion Probabilistic Models",
    "year": 2023,
    "published": "2023-05-09T21:03:13Z",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "Modern face recognition (FR) models excel in constrained scenarios, but often suffer from decreased performance when deployed in unconstrained (real-world) environments due to uncertainties surrounding the quality of the captured facial data. Face image quality assessment (FIQA) techniques aim to mitigate these performance degradations by providing FR models with sample-quality predictions that can be used to reject low-quality samples and reduce false match errors. However, despite steady impro",
    "arxiv_url": "https://arxiv.org/abs/2305.05768v1",
    "pdf_url": "https://arxiv.org/pdf/2305.05768v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2305.05768",
    "arxiv_authors": [
      "Žiga Babnik",
      "Peter Peer",
      "Vitomir Štruc"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=DifFIQA%3A+Face+Image+Quality+Assessment+Using+Denoising+Diffusion+Probabilistic+Models+%C5%BDiga+Babnik+Peter+Peer+Vitomir+%C5%A0truc",
    "gs_search_success": true,
    "gs_authors": [
      "AVp-B84AAAAJ",
      "-h43hWoAAAAJ",
      "kr52cmAAAAAJ"
    ],
    "citation_count": 31,
    "gs_author_count": null
  },
  {
    "arxiv_id": "2410.11118",
    "title": "MoonMetaSync: Lunar Image Registration Analysis",
    "year": 2024,
    "published": "2024-10-14T22:05:48Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "math.AG"
    ],
    "abstract": "This paper compares scale-invariant (SIFT) and scale-variant (ORB) feature detection methods, alongside our novel feature detector, IntFeat, specifically applied to lunar imagery. We evaluate these methods using low (128x128) and high-resolution (1024x1024) lunar image patches, providing insights into their performance across scales in challenging extraterrestrial environments. IntFeat combines high-level features from SIFT and low-level features from ORB into a single vector space for robust lu",
    "arxiv_url": "https://arxiv.org/abs/2410.11118v1",
    "pdf_url": "https://arxiv.org/pdf/2410.11118v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2410.11118",
    "arxiv_authors": [
      "Ashutosh Kumar",
      "Sarthak Kaushal",
      "Shiv Vignesh Murthy"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=MoonMetaSync%3A+Lunar+Image+Registration+Analysis+Ashutosh+Kumar+Sarthak+Kaushal+Shiv+Vignesh+Murthy",
    "gs_search_success": true,
    "gs_authors": [
      "4iXjBjoAAAAJ",
      "RrBPZkIAAAAJ",
      "zyEE2RQAAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2505.18058",
    "title": "A Foundation Model Framework for Multi-View MRI Classification of Extramural Vascular Invasion and Mesorectal Fascia Invasion in Rectal Cancer",
    "year": 2025,
    "published": "2025-05-23T16:04:27Z",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "abstract": "Background: Accurate MRI-based identification of extramural vascular invasion (EVI) and mesorectal fascia invasion (MFI) is pivotal for risk-stratified management of rectal cancer, yet visual assessment is subjective and vulnerable to inter-institutional variability. Purpose: To develop and externally evaluate a multicenter, foundation-model-driven framework that automatically classifies EVI and MFI on axial and sagittal T2-weighted MRI. Methods: This retrospective study used 331 pre-treatment r",
    "arxiv_url": "https://arxiv.org/abs/2505.18058v1",
    "pdf_url": "https://arxiv.org/pdf/2505.18058v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2505.18058",
    "arxiv_authors": [
      "Yumeng Zhang",
      "Zohaib Salahuddin",
      "Danial Khan",
      "Shruti Atul Mali",
      "Henry C. Woodruff",
      "Sina Amirrajab",
      "Eduardo Ibor-Crespo",
      "Ana Jimenez-Pastor",
      "Luis Marti-Bonmati",
      "Philippe Lambin"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=A+Foundation+Model+Framework+for+Multi-View+MRI+Classification+of+Extramural+Vascular+Invasion+and+Mesorectal+Fascia+Invasion+in+Rectal+Cancer+Yumeng+Zhang+Zohaib+Salahuddin+Danial+Khan+Shruti+Atul+Mali+Henry+C.+Woodruff",
    "gs_search_success": true,
    "gs_authors": [
      "Y8b7DiwAAAAJ",
      "z6VfuPcAAAAJ",
      "m0TFXOoAAAAJ",
      "YisQqGIAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 7
  },
  {
    "arxiv_id": "2301.09121",
    "title": "Learning Open-vocabulary Semantic Segmentation Models From Natural Language Supervision",
    "year": 2023,
    "published": "2023-01-22T13:10:05Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "In this paper, we consider the problem of open-vocabulary semantic segmentation (OVS), which aims to segment objects of arbitrary classes instead of pre-defined, closed-set categories. The main contributions are as follows: First, we propose a transformer-based model for OVS, termed as OVSegmentor, which only exploits web-crawled image-text pairs for pre-training without using any mask annotations. OVSegmentor assembles the image pixels into a set of learnable group tokens via a slot-attention b",
    "arxiv_url": "https://arxiv.org/abs/2301.09121v2",
    "pdf_url": "https://arxiv.org/pdf/2301.09121v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2301.09121",
    "arxiv_authors": [
      "Jilan Xu",
      "Junlin Hou",
      "Yuejie Zhang",
      "Rui Feng",
      "Yi Wang",
      "Yu Qiao",
      "Weidi Xie"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Learning+Open-vocabulary+Semantic+Segmentation+Models+From+Natural+Language+Supervision+Jilan+Xu+Junlin+Hou+Yuejie+Zhang+Rui+Feng+Yi+Wang",
    "gs_search_success": true,
    "gs_authors": [
      "2bHYfQcAAAAJ",
      "Vtrqj4gAAAAJ",
      "mf2U64IAAAAJ",
      "Xm2M8UwAAAAJ",
      "wqwDjtYAAAAJ",
      "gFtI-8QAAAAJ"
    ],
    "citation_count": 138,
    "gs_author_count": 7
  },
  {
    "arxiv_id": "2302.06096",
    "title": "Dual-layer Image Compression via Adaptive Downsampling and Spatially Varying Upconversion",
    "year": 2023,
    "published": "2023-02-13T04:36:13Z",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "abstract": "Ultra high resolution (UHR) images are almost always downsampled to fit small displays of mobile end devices and upsampled to its original resolution when exhibited on very high-resolution displays. This observation motivates us on jointly optimizing operation pairs of downsampling and upsampling that are spatially adaptive to image contents for maximal rate-distortion performance. In this paper, we propose an adaptive downsampled dual-layer (ADDL) image compression system. In the ADDL compressi",
    "arxiv_url": "https://arxiv.org/abs/2302.06096v1",
    "pdf_url": "https://arxiv.org/pdf/2302.06096v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2302.06096",
    "arxiv_authors": [
      "Xi Zhang",
      "Xiaolin Wu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Dual-layer+Image+Compression+via+Adaptive+Downsampling+and+Spatially+Varying+Upconversion+Xi+Zhang+Xiaolin+Wu",
    "gs_search_success": true,
    "gs_authors": [
      "ZuQnEIgAAAAJ"
    ],
    "citation_count": 3,
    "gs_author_count": 2
  },
  {
    "arxiv_id": "2506.00596",
    "title": "Seg2Any: Open-set Segmentation-Mask-to-Image Generation with Precise Shape and Semantic Control",
    "year": 2025,
    "published": "2025-05-31T15:12:04Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Despite recent advances in diffusion models, top-tier text-to-image (T2I) models still struggle to achieve precise spatial layout control, i.e. accurately generating entities with specified attributes and locations. Segmentation-mask-to-image (S2I) generation has emerged as a promising solution by incorporating pixel-level spatial guidance and regional text prompts. However, existing S2I methods fail to simultaneously ensure semantic consistency and shape consistency. To address these challenges",
    "arxiv_url": "https://arxiv.org/abs/2506.00596v3",
    "pdf_url": "https://arxiv.org/pdf/2506.00596v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2506.00596",
    "arxiv_authors": [
      "Danfeng Li",
      "Hui Zhang",
      "Sheng Wang",
      "Jiacheng Li",
      "Zuxuan Wu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Seg2Any%3A+Open-set+Segmentation-Mask-to-Image+Generation+with+Precise+Shape+and+Semantic+Control+Danfeng+Li+Hui+Zhang+Sheng+Wang+Jiacheng+Li+Zuxuan+Wu",
    "gs_search_success": true,
    "gs_authors": [
      "PM0wt9wAAAAJ",
      "sqMq_vMAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2309.04028",
    "title": "Algebra and Geometry of Camera Resectioning",
    "year": 2023,
    "published": "2023-09-07T21:53:54Z",
    "categories": [
      "math.AG",
      "cs.CV",
      "math.AC"
    ],
    "abstract": "We study algebraic varieties associated with the camera resectioning problem. We characterize these resectioning varieties' multigraded vanishing ideals using Gröbner basis techniques. As an application, we derive and re-interpret celebrated results in geometric computer vision related to camera-point duality. We also clarify some relationships between the classical problems of optimal resectioning and triangulation, state a conjectural formula for the Euclidean distance degree of the resectioni",
    "arxiv_url": "https://arxiv.org/abs/2309.04028v1",
    "pdf_url": "https://arxiv.org/pdf/2309.04028v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2309.04028",
    "arxiv_authors": [
      "Erin Connelly",
      "Timothy Duff",
      "Jessie Loucks-Tavitas"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Algebra+and+Geometry+of+Camera+Resectioning+Erin+Connelly+Timothy+Duff+Jessie+Loucks-Tavitas",
    "gs_search_success": true,
    "gs_authors": [
      "oNcDH8MAAAAJ"
    ],
    "citation_count": 7,
    "gs_author_count": 2
  },
  {
    "arxiv_id": "2307.07341",
    "title": "PiTL: Cross-modal Retrieval with Weakly-supervised Vision-language Pre-training via Prompting",
    "year": 2023,
    "published": "2023-07-14T13:43:04Z",
    "categories": [
      "cs.IR",
      "cs.CV"
    ],
    "abstract": "Vision-language (VL) Pre-training (VLP) has shown to well generalize VL models over a wide range of VL downstream tasks, especially for cross-modal retrieval. However, it hinges on a huge amount of image-text pairs, which requires tedious and costly curation. On the contrary, weakly-supervised VLP (W-VLP) explores means with object tags generated by a pre-trained object detector (OD) from images. Yet, they still require paired information, i.e. images and object-level annotations, as supervision",
    "arxiv_url": "https://arxiv.org/abs/2307.07341v1",
    "pdf_url": "https://arxiv.org/pdf/2307.07341v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2307.07341",
    "arxiv_authors": [
      "Zixin Guo",
      "Tzu-Jui Julius Wang",
      "Selen Pehlivan",
      "Abduljalil Radman",
      "Jorma Laaksonen"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=PiTL%3A+Cross-modal+Retrieval+with+Weakly-supervised+Vision-language+Pre-training+via+Prompting+Zixin+Guo+Tzu-Jui+Julius+Wang+Selen+Pehlivan+Abduljalil+Radman+Jorma+Laaksonen",
    "gs_search_success": true,
    "gs_authors": [
      "XnrVKRoAAAAJ",
      "qQP6WXIAAAAJ",
      "sHmK0MUAAAAJ",
      "m-TZ68MAAAAJ"
    ],
    "citation_count": 5,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2303.13113",
    "title": "AdaCL:Adaptive Continual Learning",
    "year": 2023,
    "published": "2023-03-23T09:00:38Z",
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "abstract": "Class-Incremental Learning aims to update a deep classifier to learn new categories while maintaining or improving its accuracy on previously observed classes. Common methods to prevent forgetting previously learned classes include regularizing the neural network updates and storing exemplars in memory, which come with hyperparameters such as the learning rate, regularization strength, or the number of exemplars. However, these hyperparameters are usually only tuned at the start and then kept fi",
    "arxiv_url": "https://arxiv.org/abs/2303.13113v3",
    "pdf_url": "https://arxiv.org/pdf/2303.13113v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2303.13113",
    "arxiv_authors": [
      "Elif Ceren Gok Yildirim",
      "Murat Onur Yildirim",
      "Mert Kilickaya",
      "Joaquin Vanschoren"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=AdaCL%3AAdaptive+Continual+Learning+Elif+Ceren+Gok+Yildirim+Murat+Onur+Yildirim+Mert+Kilickaya+Joaquin+Vanschoren",
    "gs_search_success": true,
    "gs_authors": [
      "JGWGEIYAAAAJ",
      "MBMjO0sAAAAJ",
      "3NAjgx0AAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2403.13805",
    "title": "RAR: Retrieving And Ranking Augmented MLLMs for Visual Recognition",
    "year": 2024,
    "published": "2024-03-20T17:59:55Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "abstract": "CLIP (Contrastive Language-Image Pre-training) uses contrastive learning from noise image-text pairs to excel at recognizing a wide array of candidates, yet its focus on broad associations hinders the precision in distinguishing subtle differences among fine-grained items. Conversely, Multimodal Large Language Models (MLLMs) excel at classifying fine-grained categories, thanks to their substantial knowledge from pre-training on web-level corpora. However, the performance of MLLMs declines with a",
    "arxiv_url": "https://arxiv.org/abs/2403.13805v1",
    "pdf_url": "https://arxiv.org/pdf/2403.13805v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2403.13805",
    "arxiv_authors": [
      "Ziyu Liu",
      "Zeyi Sun",
      "Yuhang Zang",
      "Wei Li",
      "Pan Zhang",
      "Xiaoyi Dong",
      "Yuanjun Xiong",
      "Dahua Lin",
      "Jiaqi Wang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=RAR%3A+Retrieving+And+Ranking+Augmented+MLLMs+for+Visual+Recognition+Ziyu+Liu+Zeyi+Sun+Yuhang+Zang+Wei+Li+Pan+Zhang",
    "gs_search_success": true,
    "gs_authors": [
      "hUrX-LMAAAAJ",
      "u61Pe1QAAAAJ",
      "ojKsx6AAAAAJ",
      "RvGxDLUAAAAJ",
      "FscToE0AAAAJ",
      "hW23VKIAAAAJ",
      "GDvt570AAAAJ",
      "GMzzRRUAAAAJ",
      "41KAd6AAAAAJ"
    ],
    "citation_count": 23,
    "gs_author_count": 9
  },
  {
    "arxiv_id": "2503.07075",
    "title": "XR-VLM: Cross-Relationship Modeling with Multi-part Prompts and Visual Features for Fine-Grained Recognition",
    "year": 2025,
    "published": "2025-03-10T08:58:05Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Vision-Language Models (VLMs) have demonstrated impressive performance on various visual tasks, yet they still require adaptation on downstream tasks to achieve optimal performance. Recently, various adaptation technologies have been proposed, but we observe they often underperform in fine-grained visual recognition, which requires models to capture subtle yet discriminative features to distinguish similar sub-categories. Current adaptation methods typically rely on an alignment-based prediction",
    "arxiv_url": "https://arxiv.org/abs/2503.07075v1",
    "pdf_url": "https://arxiv.org/pdf/2503.07075v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2503.07075",
    "arxiv_authors": [
      "Chuanming Wang",
      "Henming Mao",
      "Huanhuan Zhang",
      "Huiyuan Fu",
      "Huadong Ma"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=XR-VLM%3A+Cross-Relationship+Modeling+with+Multi-part+Prompts+and+Visual+Features+for+Fine-Grained+Recognition+Chuanming+Wang+Henming+Mao+Huanhuan+Zhang+Huiyuan+Fu+Huadong+Ma",
    "gs_search_success": false,
    "gs_authors": [],
    "citation_count": null,
    "gs_author_count": null,
    "error_type": "fetch_failed"
  },
  {
    "arxiv_id": "2503.01612",
    "title": "Robust Palm-Vein Recognition Using the MMD Filter: Improving SIFT-Based Feature Matching",
    "year": 2025,
    "published": "2025-03-03T14:48:06Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "A major challenge with palm vein images is that slight movements of the fingers and thumb, or variations in hand posture, can stretch the skin in different areas and alter the vein patterns. This can result in an infinite number of variations in palm vein images for a given individual. This paper introduces a novel filtering technique for SIFT-based feature matching, known as the Mean and Median Distance (MMD) Filter. This method evaluates the differences in keypoint coordinates and computes the",
    "arxiv_url": "https://arxiv.org/abs/2503.01612v1",
    "pdf_url": "https://arxiv.org/pdf/2503.01612v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2503.01612",
    "arxiv_authors": [
      "Kaveen Perera",
      "Fouad Khelifi",
      "Ammar Belatreche"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Robust+Palm-Vein+Recognition+Using+the+MMD+Filter%3A+Improving+SIFT-Based+Feature+Matching+Kaveen+Perera+Fouad+Khelifi+Ammar+Belatreche",
    "gs_search_success": true,
    "gs_authors": [
      "DsJo3T8AAAAJ",
      "dDjIW94AAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2312.13114",
    "title": "Investigating Color Illusions from the Perspective of Computational Color Constancy",
    "year": 2023,
    "published": "2023-12-20T15:34:15Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Color constancy and color illusion perception are two phenomena occurring in the human visual system, which can help us reveal unknown mechanisms of human perception. For decades computer vision scientists have developed numerous color constancy methods, which estimate the reflectance of the surface by discounting the illuminant. However, color illusions have not been analyzed in detail in the field of computational color constancy, which we find surprising since the relationship they share is s",
    "arxiv_url": "https://arxiv.org/abs/2312.13114v1",
    "pdf_url": "https://arxiv.org/pdf/2312.13114v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2312.13114",
    "arxiv_authors": [
      "Oguzhan Ulucan",
      "Diclehan Ulucan",
      "Marc Ebner"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Investigating+Color+Illusions+from+the+Perspective+of+Computational+Color+Constancy+Oguzhan+Ulucan+Diclehan+Ulucan+Marc+Ebner",
    "gs_search_success": true,
    "gs_authors": [
      "JB3hc3cAAAAJ",
      "uA-5xdIAAAAJ",
      "GDBJBzMAAAAJ"
    ],
    "citation_count": 3,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2302.14807",
    "title": "DFR-FastMOT: Detection Failure Resistant Tracker for Fast Multi-Object Tracking Based on Sensor Fusion",
    "year": 2023,
    "published": "2023-02-28T17:57:06Z",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "abstract": "Persistent multi-object tracking (MOT) allows autonomous vehicles to navigate safely in highly dynamic environments. One of the well-known challenges in MOT is object occlusion when an object becomes unobservant for subsequent frames. The current MOT methods store objects information, like objects' trajectory, in internal memory to recover the objects after occlusions. However, they retain short-term memory to save computational time and avoid slowing down the MOT method. As a result, they lose ",
    "arxiv_url": "https://arxiv.org/abs/2302.14807v1",
    "pdf_url": "https://arxiv.org/pdf/2302.14807v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2302.14807",
    "arxiv_authors": [
      "Mohamed Nagy",
      "Majid Khonji",
      "Jorge Dias",
      "Sajid Javed"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=DFR-FastMOT%3A+Detection+Failure+Resistant+Tracker+for+Fast+Multi-Object+Tracking+Based+on+Sensor+Fusion+Mohamed+Nagy+Majid+Khonji+Jorge+Dias+Sajid+Javed",
    "gs_search_success": true,
    "gs_authors": [
      "6qvbEhUAAAAJ",
      "liGzaswAAAAJ",
      "IXmK0cwAAAAJ",
      "Z2AxaZYAAAAJ"
    ],
    "citation_count": 11,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2408.09426",
    "title": "A Robust Algorithm for Contactless Fingerprint Enhancement and Matching",
    "year": 2024,
    "published": "2024-08-18T10:01:42Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Compared to contact fingerprint images, contactless fingerprint images exhibit four distinct characteristics: (1) they contain less noise; (2) they have fewer discontinuities in ridge patterns; (3) the ridge-valley pattern is less distinct; and (4) they pose an interoperability problem, as they lack the elastic deformation caused by pressing the finger against the capture device. These properties present significant challenges for the enhancement of contactless fingerprint images. In this study,",
    "arxiv_url": "https://arxiv.org/abs/2408.09426v1",
    "pdf_url": "https://arxiv.org/pdf/2408.09426v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2408.09426",
    "arxiv_authors": [
      "Mahrukh Siddiqui",
      "Shahzaib Iqbal",
      "Bandar AlShammari",
      "Bandar Alhaqbani",
      "Tariq M. Khan",
      "Imran Razzak"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=A+Robust+Algorithm+for+Contactless+Fingerprint+Enhancement+and+Matching+Mahrukh+Siddiqui+Shahzaib+Iqbal+Bandar+AlShammari+Bandar+Alhaqbani+Tariq+M.+Khan",
    "gs_search_success": true,
    "gs_authors": [
      "GlXI4N8AAAAJ",
      "Q5NePJAAAAAJ",
      "YDx41x4AAAAJ"
    ],
    "citation_count": 3,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2409.04760",
    "title": "Training-Free Point Cloud Recognition Based on Geometric and Semantic Information Fusion",
    "year": 2024,
    "published": "2024-09-07T08:20:02Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "The trend of employing training-free methods for point cloud recognition is becoming increasingly popular due to its significant reduction in computational resources and time costs. However, existing approaches are limited as they typically extract either geometric or semantic features. To address this limitation, we are the first to propose a novel training-free method that integrates both geometric and semantic features. For the geometric branch, we adopt a non-parametric strategy to extract g",
    "arxiv_url": "https://arxiv.org/abs/2409.04760v5",
    "pdf_url": "https://arxiv.org/pdf/2409.04760v5",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2409.04760",
    "arxiv_authors": [
      "Yan Chen",
      "Di Huang",
      "Zhichao Liao",
      "Xi Cheng",
      "Xinghui Li",
      "Long Zeng"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Training-Free+Point+Cloud+Recognition+Based+on+Geometric+and+Semantic+Information+Fusion+Yan+Chen+Di+Huang+Zhichao+Liao+Xi+Cheng+Xinghui+Li",
    "gs_search_success": true,
    "gs_authors": [
      "72QbaQwAAAAJ",
      "4eRwbOEAAAAJ"
    ],
    "citation_count": 3,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2502.07556",
    "title": "SketchFlex: Facilitating Spatial-Semantic Coherence in Text-to-Image Generation with Region-Based Sketches",
    "year": 2025,
    "published": "2025-02-11T13:48:11Z",
    "categories": [
      "cs.HC",
      "cs.CV"
    ],
    "abstract": "Text-to-image models can generate visually appealing images from text descriptions. Efforts have been devoted to improving model controls with prompt tuning and spatial conditioning. However, our formative study highlights the challenges for non-expert users in crafting appropriate prompts and specifying fine-grained spatial conditions (e.g., depth or canny references) to generate semantically cohesive images, especially when multiple objects are involved. In response, we introduce SketchFlex, a",
    "arxiv_url": "https://arxiv.org/abs/2502.07556v1",
    "pdf_url": "https://arxiv.org/pdf/2502.07556v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2502.07556",
    "arxiv_authors": [
      "Haichuan Lin",
      "Yilin Ye",
      "Jiazhi Xia",
      "Wei Zeng"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=SketchFlex%3A+Facilitating+Spatial-Semantic+Coherence+in+Text-to-Image+Generation+with+Region-Based+Sketches+Haichuan+Lin+Yilin+Ye+Jiazhi+Xia+Wei+Zeng",
    "gs_search_success": true,
    "gs_authors": [
      "Sbeiir0AAAAJ",
      "w_63g8wAAAAJ",
      "kTZhR2EAAAAJ",
      "jwPgey4AAAAJ"
    ],
    "citation_count": 7,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2503.12797",
    "title": "DeepPerception: Advancing R1-like Cognitive Visual Perception in MLLMs for Knowledge-Intensive Visual Grounding",
    "year": 2025,
    "published": "2025-03-17T04:06:34Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "abstract": "Human experts excel at fine-grained visual discrimination by leveraging domain knowledge to refine perceptual features, a capability that remains underdeveloped in current Multimodal Large Language Models (MLLMs). Despite possessing vast expert-level knowledge, MLLMs struggle to integrate reasoning into visual perception, often generating direct responses without deeper analysis. To bridge this gap, we introduce knowledge-intensive visual grounding (KVG), a novel visual grounding task that requi",
    "arxiv_url": "https://arxiv.org/abs/2503.12797v2",
    "pdf_url": "https://arxiv.org/pdf/2503.12797v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2503.12797",
    "arxiv_authors": [
      "Xinyu Ma",
      "Ziyang Ding",
      "Zhicong Luo",
      "Chi Chen",
      "Zonghao Guo",
      "Derek F. Wong",
      "Xiaoyi Feng",
      "Maosong Sun"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=DeepPerception%3A+Advancing+R1-like+Cognitive+Visual+Perception+in+MLLMs+for+Knowledge-Intensive+Visual+Grounding+Xinyu+Ma+Ziyang+Ding+Zhicong+Luo+Chi+Chen+Zonghao+Guo",
    "gs_search_success": true,
    "gs_authors": [
      "SuFIjWQAAAAJ",
      "zIgT0HMAAAAJ",
      "KjQBe8oAAAAJ",
      "tlSFHT4AAAAJ",
      "h1I6LJcAAAAJ",
      "2jpvQ90AAAAJ"
    ],
    "citation_count": 10,
    "gs_author_count": 8
  },
  {
    "arxiv_id": "2406.01658",
    "title": "Proxy Denoising for Source-Free Domain Adaptation",
    "year": 2024,
    "published": "2024-06-03T17:36:36Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Source-Free Domain Adaptation (SFDA) aims to adapt a pre-trained source model to an unlabeled target domain with no access to the source data. Inspired by the success of large Vision-Language (ViL) models in many applications, the latest research has validated ViL's benefit for SFDA by using their predictions as pseudo supervision. However, we observe that ViL's supervision could be noisy and inaccurate at an unknown rate, introducing additional negative effects during adaption. To address this ",
    "arxiv_url": "https://arxiv.org/abs/2406.01658v3",
    "pdf_url": "https://arxiv.org/pdf/2406.01658v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2406.01658",
    "arxiv_authors": [
      "Song Tang",
      "Wenxin Su",
      "Yan Gan",
      "Mao Ye",
      "Jianwei Zhang",
      "Xiatian Zhu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Proxy+Denoising+for+Source-Free+Domain+Adaptation+Song+Tang+Wenxin+Su+Yan+Gan+Mao+Ye+Jianwei+Zhang",
    "gs_search_success": true,
    "gs_authors": [
      "BPQ9bSwAAAAJ",
      "ZbA-z1cAAAAJ",
      "UUbEzBYAAAAJ",
      "8uhkD9QAAAAJ"
    ],
    "citation_count": 9,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2501.01677",
    "title": "PG-SAG: Parallel Gaussian Splatting for Fine-Grained Large-Scale Urban Buildings Reconstruction via Semantic-Aware Grouping",
    "year": 2025,
    "published": "2025-01-03T07:40:16Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has emerged as a transformative method in the field of real-time novel synthesis. Based on 3DGS, recent advancements cope with large-scale scenes via spatial-based partition strategy to reduce video memory and optimization time costs. In this work, we introduce a parallel Gaussian splatting method, termed PG-SAG, which fully exploits semantic cues for both partitioning and Gaussian kernel optimization, enabling fine-grained building surface reconstruction of large-sc",
    "arxiv_url": "https://arxiv.org/abs/2501.01677v1",
    "pdf_url": "https://arxiv.org/pdf/2501.01677v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2501.01677",
    "arxiv_authors": [
      "Tengfei Wang",
      "Xin Wang",
      "Yongmao Hou",
      "Yiwei Xu",
      "Wendi Zhang",
      "Zongqian Zhan"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=PG-SAG%3A+Parallel+Gaussian+Splatting+for+Fine-Grained+Large-Scale+Urban+Buildings+Reconstruction+via+Semantic-Aware+Grouping+Tengfei+Wang+Xin+Wang+Yongmao+Hou+Yiwei+Xu+Wendi+Zhang",
    "gs_search_success": true,
    "gs_authors": [
      "pK2IPtsAAAAJ"
    ],
    "citation_count": 6,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2302.04858",
    "title": "Re-ViLM: Retrieval-Augmented Visual Language Model for Zero and Few-Shot Image Captioning",
    "year": 2023,
    "published": "2023-02-09T18:57:56Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.IR",
      "cs.LG"
    ],
    "abstract": "Augmenting pretrained language models (LMs) with a vision encoder (e.g., Flamingo) has obtained the state-of-the-art results in image-to-text generation. However, these models store all the knowledge within their parameters, thus often requiring enormous model parameters to model the abundant visual concepts and very rich textual descriptions. Additionally, they are inefficient in incorporating new data, requiring a computational-expensive fine-tuning process. In this work, we introduce a Retrie",
    "arxiv_url": "https://arxiv.org/abs/2302.04858v2",
    "pdf_url": "https://arxiv.org/pdf/2302.04858v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2302.04858",
    "arxiv_authors": [
      "Zhuolin Yang",
      "Wei Ping",
      "Zihan Liu",
      "Vijay Korthikanti",
      "Weili Nie",
      "De-An Huang",
      "Linxi Fan",
      "Zhiding Yu",
      "Shiyi Lan",
      "Bo Li",
      "Ming-Yu Liu",
      "Yuke Zhu",
      "Mohammad Shoeybi",
      "Bryan Catanzaro",
      "Chaowei Xiao",
      "Anima Anandkumar"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Re-ViLM%3A+Retrieval-Augmented+Visual+Language+Model+for+Zero+and+Few-Shot+Image+Captioning+Zhuolin+Yang+Wei+Ping+Zihan+Liu+Vijay+Korthikanti+Weili+Nie",
    "gs_search_success": true,
    "gs_authors": [
      "zW7BH7oAAAAJ",
      "62ElavIAAAAJ",
      "iwWl6AwAAAAJ",
      "jIUI6F4AAAAJ",
      "6gKEYRgAAAAJ",
      "HEY3UzgAAAAJ",
      "sljtWIUAAAAJ",
      "1VI_oYUAAAAJ",
      "BvSv-C0AAAAJ",
      "K8vJkTcAAAAJ",
      "LPabcsYAAAAJ"
    ],
    "citation_count": 56,
    "gs_author_count": 11
  },
  {
    "arxiv_id": "2305.10503",
    "title": "OR-NeRF: Object Removing from 3D Scenes Guided by Multiview Segmentation with Neural Radiance Fields",
    "year": 2023,
    "published": "2023-05-17T18:18:05Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "The emergence of Neural Radiance Fields (NeRF) for novel view synthesis has increased interest in 3D scene editing. An essential task in editing is removing objects from a scene while ensuring visual reasonability and multiview consistency. However, current methods face challenges such as time-consuming object labeling, limited capability to remove specific targets, and compromised rendering quality after removal. This paper proposes a novel object-removing pipeline, named OR-NeRF, that can remo",
    "arxiv_url": "https://arxiv.org/abs/2305.10503v3",
    "pdf_url": "https://arxiv.org/pdf/2305.10503v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2305.10503",
    "arxiv_authors": [
      "Youtan Yin",
      "Zhoujie Fu",
      "Fan Yang",
      "Guosheng Lin"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=OR-NeRF%3A+Object+Removing+from+3D+Scenes+Guided+by+Multiview+Segmentation+with+Neural+Radiance+Fields+Youtan+Yin+Zhoujie+Fu+Fan+Yang+Guosheng+Lin",
    "gs_search_success": true,
    "gs_authors": [
      "afDvaa8AAAAJ",
      "ZudEhvcAAAAJ",
      "6bdPeKcAAAAJ",
      "dSRAwwYAAAAJ"
    ],
    "citation_count": 38,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2404.03898",
    "title": "VoltaVision: A Transfer Learning model for electronic component classification",
    "year": 2024,
    "published": "2024-04-05T05:42:23Z",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "In this paper, we analyze the effectiveness of transfer learning on classifying electronic components. Transfer learning reuses pre-trained models to save time and resources in building a robust classifier rather than learning from scratch. Our work introduces a lightweight CNN, coined as VoltaVision, and compares its performance against more complex models. We test the hypothesis that transferring knowledge from a similar task to our target domain yields better results than state-of-the-art mod",
    "arxiv_url": "https://arxiv.org/abs/2404.03898v1",
    "pdf_url": "https://arxiv.org/pdf/2404.03898v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2404.03898",
    "arxiv_authors": [
      "Anas Mohammad Ishfaqul Muktadir Osmani",
      "Taimur Rahman",
      "Salekul Islam"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=VoltaVision%3A+A+Transfer+Learning+model+for+electronic+component+classification+Anas+Mohammad+Ishfaqul+Muktadir+Osmani+Taimur+Rahman+Salekul+Islam",
    "gs_search_success": true,
    "gs_authors": [
      "i_RXBsEAAAAJ",
      "CewWo2IAAAAJ",
      "crlPFpcAAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": null
  },
  {
    "arxiv_id": "2306.09077",
    "title": "Estimating Generic 3D Room Structures from 2D Annotations",
    "year": 2023,
    "published": "2023-06-15T12:10:27Z",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "abstract": "Indoor rooms are among the most common use cases in 3D scene understanding. Current state-of-the-art methods for this task are driven by large annotated datasets. Room layouts are especially important, consisting of structural elements in 3D, such as wall, floor, and ceiling. However, they are difficult to annotate, especially on pure RGB video. We propose a novel method to produce generic 3D room layouts just from 2D segmentation masks, which are easy to annotate for humans. Based on these 2D a",
    "arxiv_url": "https://arxiv.org/abs/2306.09077v2",
    "pdf_url": "https://arxiv.org/pdf/2306.09077v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2306.09077",
    "arxiv_authors": [
      "Denys Rozumnyi",
      "Stefan Popov",
      "Kevis-Kokitsi Maninis",
      "Matthias Nießner",
      "Vittorio Ferrari"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Estimating+Generic+3D+Room+Structures+from+2D+Annotations+Denys+Rozumnyi+Stefan+Popov+Kevis-Kokitsi+Maninis+Matthias+Nie%C3%9Fner+Vittorio+Ferrari",
    "gs_search_success": true,
    "gs_authors": [
      "4QvYJ00AAAAJ",
      "5KvD78sAAAAJ",
      "Lw_-pYsAAAAJ",
      "eUtEs6YAAAAJ",
      "Glq3dWkAAAAJ"
    ],
    "citation_count": 5,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2403.13338",
    "title": "Adaptive Critical Subgraph Mining for Cognitive Impairment Conversion Prediction with T1-MRI-based Brain Network",
    "year": 2024,
    "published": "2024-03-20T06:46:01Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Prediction the conversion to early-stage dementia is critical for mitigating its progression but remains challenging due to subtle cognitive impairments and structural brain changes. Traditional T1-weighted magnetic resonance imaging (T1-MRI) research focus on identifying brain atrophy regions but often fails to address the intricate connectivity between them. This limitation underscores the necessity of focuing on inter-regional connectivity for a comprehensive understand of the brain's complex",
    "arxiv_url": "https://arxiv.org/abs/2403.13338v2",
    "pdf_url": "https://arxiv.org/pdf/2403.13338v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2403.13338",
    "arxiv_authors": [
      "Yilin Leng",
      "Wenju Cui",
      "Bai Chen",
      "Xi Jiang",
      "Shuangqing Chen",
      "Jian Zheng"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Adaptive+Critical+Subgraph+Mining+for+Cognitive+Impairment+Conversion+Prediction+with+T1-MRI-based+Brain+Network+Yilin+Leng+Wenju+Cui+Bai+Chen+Xi+Jiang+Shuangqing+Chen",
    "gs_search_success": true,
    "gs_authors": [
      "qua5WK8AAAAJ",
      "KAWAvZQAAAAJ",
      "Cv3JHdYAAAAJ",
      "YvzaQ1YAAAAJ",
      "KAiDBPYAAAAJ"
    ],
    "citation_count": 2,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2301.00524",
    "title": "Learning Confident Classifiers in the Presence of Label Noise",
    "year": 2023,
    "published": "2023-01-02T04:27:25Z",
    "categories": [
      "cs.CV",
      "cs.HC",
      "cs.LG"
    ],
    "abstract": "The success of Deep Neural Network (DNN) models significantly depends on the quality of provided annotations. In medical image segmentation, for example, having multiple expert annotations for each data point is common to minimize subjective annotation bias. Then, the goal of estimation is to filter out the label noise and recover the ground-truth masks, which are not explicitly given. This paper proposes a probabilistic model for noisy observations that allows us to build a confident classifica",
    "arxiv_url": "https://arxiv.org/abs/2301.00524v3",
    "pdf_url": "https://arxiv.org/pdf/2301.00524v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2301.00524",
    "arxiv_authors": [
      "Asma Ahmed Hashmi",
      "Aigerim Zhumabayeva",
      "Nikita Kotelevskii",
      "Artem Agafonov",
      "Mohammad Yaqub",
      "Maxim Panov",
      "Martin Takáč"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Learning+Confident+Classifiers+in+the+Presence+of+Label+Noise+Asma+Ahmed+Hashmi+Aigerim+Zhumabayeva+Nikita+Kotelevskii+Artem+Agafonov+Mohammad+Yaqub",
    "gs_search_success": true,
    "gs_authors": [
      "ByRH2gQAAAAJ",
      "cr5pJdAAAAAJ",
      "9dfn5GkAAAAJ",
      "BqDhGJQAAAAJ",
      "qKQD-2cAAAAJ",
      "D9b8bXEAAAAJ"
    ],
    "citation_count": 4,
    "gs_author_count": 7
  },
  {
    "arxiv_id": "2309.14341",
    "title": "Extreme Parkour with Legged Robots",
    "year": 2023,
    "published": "2023-09-25T17:59:55Z",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV",
      "cs.LG",
      "eess.SY"
    ],
    "abstract": "Humans can perform parkour by traversing obstacles in a highly dynamic fashion requiring precise eye-muscle coordination and movement. Getting robots to do the same task requires overcoming similar challenges. Classically, this is done by independently engineering perception, actuation, and control systems to very low tolerances. This restricts them to tightly controlled settings such as a predetermined obstacle course in labs. In contrast, humans are able to learn parkour through practice witho",
    "arxiv_url": "https://arxiv.org/abs/2309.14341v1",
    "pdf_url": "https://arxiv.org/pdf/2309.14341v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2309.14341",
    "arxiv_authors": [
      "Xuxin Cheng",
      "Kexin Shi",
      "Ananye Agarwal",
      "Deepak Pathak"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Extreme+Parkour+with+Legged+Robots+Xuxin+Cheng+Kexin+Shi+Ananye+Agarwal+Deepak+Pathak",
    "gs_search_success": true,
    "gs_authors": [
      "oRhPc20AAAAJ",
      "AEsPCAUAAAAJ",
      "-E1AyF0AAAAJ",
      "Z8vhOxYAAAAJ"
    ],
    "citation_count": 279,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2412.19878",
    "title": "YOLO-MST: Multiscale deep learning method for infrared small target detection based on super-resolution and YOLO",
    "year": 2024,
    "published": "2024-12-27T18:43:56Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "With the advancement of aerospace technology and the increasing demands of military applications, the development of low false-alarm and high-precision infrared small target detection algorithms has emerged as a key focus of research globally. However, the traditional model-driven method is not robust enough when dealing with features such as noise, target size, and contrast. The existing deep-learning methods have limited ability to extract and fuse key features, and it is difficult to achieve ",
    "arxiv_url": "https://arxiv.org/abs/2412.19878v1",
    "pdf_url": "https://arxiv.org/pdf/2412.19878v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2412.19878",
    "arxiv_authors": [
      "Taoran Yue",
      "Xiaojin Lu",
      "Jiaxi Cai",
      "Yuanping Chen",
      "Shibing Chu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=YOLO-MST%3A+Multiscale+deep+learning+method+for+infrared+small+target+detection+based+on+super-resolution+and+YOLO+Taoran+Yue+Xiaojin+Lu+Jiaxi+Cai+Yuanping+Chen+Shibing+Chu",
    "gs_search_success": true,
    "gs_authors": [
      "2evllU0AAAAJ"
    ],
    "citation_count": 22,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2410.03790",
    "title": "Accelerating Deep Learning with Fixed Time Budget",
    "year": 2024,
    "published": "2024-10-03T21:18:04Z",
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "abstract": "The success of modern deep learning is attributed to two key elements: huge amounts of training data and large model sizes. Where a vast amount of data allows the model to learn more features, the large model architecture boosts the learning capability of the model. However, both these factors result in prolonged training time. In some practical applications such as edge-based learning and federated learning, limited-time budgets necessitate more efficient training methods. This paper proposes a",
    "arxiv_url": "https://arxiv.org/abs/2410.03790v1",
    "pdf_url": "https://arxiv.org/pdf/2410.03790v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2410.03790",
    "arxiv_authors": [
      "Muhammad Asif Khan",
      "Ridha Hamila",
      "Hamid Menouar"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Accelerating+Deep+Learning+with+Fixed+Time+Budget+Muhammad+Asif+Khan+Ridha+Hamila+Hamid+Menouar",
    "gs_search_success": true,
    "gs_authors": [
      "BgUrPm8AAAAJ",
      "7miMfTIAAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 2
  },
  {
    "arxiv_id": "2503.07942",
    "title": "STEAD: Spatio-Temporal Efficient Anomaly Detection for Time and Compute Sensitive Applications",
    "year": 2025,
    "published": "2025-03-11T00:48:32Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "This paper presents a new method for anomaly detection in automated systems with time and compute sensitive requirements, such as autonomous driving, with unparalleled efficiency. As systems like autonomous driving become increasingly popular, ensuring their safety has become more important than ever. Therefore, this paper focuses on how to quickly and effectively detect various anomalies in the aforementioned systems, with the goal of making them safer and more effective. Many detection systems",
    "arxiv_url": "https://arxiv.org/abs/2503.07942v1",
    "pdf_url": "https://arxiv.org/pdf/2503.07942v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2503.07942",
    "arxiv_authors": [
      "Andrew Gao",
      "Jun Liu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=STEAD%3A+Spatio-Temporal+Efficient+Anomaly+Detection+for+Time+and+Compute+Sensitive+Applications+Andrew+Gao+Jun+Liu",
    "gs_search_success": true,
    "gs_authors": [],
    "citation_count": 5,
    "gs_author_count": 1
  },
  {
    "arxiv_id": "2410.09421",
    "title": "VLFeedback: A Large-Scale AI Feedback Dataset for Large Vision-Language Models Alignment",
    "year": 2024,
    "published": "2024-10-12T07:56:47Z",
    "categories": [
      "cs.CV",
      "cs.CL"
    ],
    "abstract": "As large vision-language models (LVLMs) evolve rapidly, the demand for high-quality and diverse data to align these models becomes increasingly crucial. However, the creation of such data with human supervision proves costly and time-intensive. In this paper, we investigate the efficacy of AI feedback to scale supervision for aligning LVLMs. We introduce VLFeedback, the first large-scale vision-language feedback dataset, comprising over 82K multi-modal instructions and comprehensive rationales g",
    "arxiv_url": "https://arxiv.org/abs/2410.09421v2",
    "pdf_url": "https://arxiv.org/pdf/2410.09421v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2410.09421",
    "arxiv_authors": [
      "Lei Li",
      "Zhihui Xie",
      "Mukai Li",
      "Shunian Chen",
      "Peiyi Wang",
      "Liang Chen",
      "Yazheng Yang",
      "Benyou Wang",
      "Lingpeng Kong",
      "Qi Liu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=VLFeedback%3A+A+Large-Scale+AI+Feedback+Dataset+for+Large+Vision-Language+Models+Alignment+Lei+Li+Zhihui+Xie+Mukai+Li+Shunian+Chen+Peiyi+Wang",
    "gs_search_success": true,
    "gs_authors": [
      "lMKPaTYAAAAJ",
      "BizedOAAAAAJ",
      "K0uQ3ygAAAAJ",
      "7Oif9DMAAAAJ",
      "1B0l7U8AAAAJ",
      "f1hBi5wAAAAJ",
      "T0yyq-EAAAAJ",
      "Jk4vJU8AAAAJ",
      "Jml8NvkAAAAJ"
    ],
    "citation_count": 45,
    "gs_author_count": 10
  },
  {
    "arxiv_id": "2502.10801",
    "title": "FaceSwapGuard: Safeguarding Facial Privacy from DeepFake Threats through Identity Obfuscation",
    "year": 2025,
    "published": "2025-02-15T13:45:19Z",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CV"
    ],
    "abstract": "DeepFakes pose a significant threat to our society. One representative DeepFake application is face-swapping, which replaces the identity in a facial image with that of a victim. Although existing methods partially mitigate these risks by degrading the quality of swapped images, they often fail to disrupt the identity transformation effectively. To fill this gap, we propose FaceSwapGuard (FSG), a novel black-box defense mechanism against deepfake face-swapping threats. Specifically, FSG introduc",
    "arxiv_url": "https://arxiv.org/abs/2502.10801v1",
    "pdf_url": "https://arxiv.org/pdf/2502.10801v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2502.10801",
    "arxiv_authors": [
      "Li Wang",
      "Zheng Li",
      "Xuhong Zhang",
      "Shouling Ji",
      "Shanqing Guo"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=FaceSwapGuard%3A+Safeguarding+Facial+Privacy+from+DeepFake+Threats+through+Identity+Obfuscation+Li+Wang+Zheng+Li+Xuhong+Zhang+Shouling+Ji+Shanqing+Guo",
    "gs_search_success": true,
    "gs_authors": [
      "bWLpm3sAAAAJ",
      "5HoF_9oAAAAJ",
      "NTzYa3YAAAAJ",
      "zsoQa0cAAAAJ",
      "xEAaaGsAAAAJ"
    ],
    "citation_count": 2,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2306.01195",
    "title": "Consistency-guided Prompt Learning for Vision-Language Models",
    "year": 2023,
    "published": "2023-06-01T23:20:47Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "We propose Consistency-guided Prompt learning (CoPrompt), a new fine-tuning method for vision-language models. Our approach improves the generalization of large foundation models when fine-tuned on downstream tasks in a few-shot setting. The basic idea of CoPrompt is to enforce a consistency constraint in the prediction of the trainable and pre-trained models to prevent overfitting on the downstream task. Additionally, we introduce the following two components into our consistency constraint to ",
    "arxiv_url": "https://arxiv.org/abs/2306.01195v4",
    "pdf_url": "https://arxiv.org/pdf/2306.01195v4",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2306.01195",
    "arxiv_authors": [
      "Shuvendu Roy",
      "Ali Etemad"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Consistency-guided+Prompt+Learning+for+Vision-Language+Models+Shuvendu+Roy+Ali+Etemad",
    "gs_search_success": true,
    "gs_authors": [
      "UvOC8MkAAAAJ",
      "5-zu4ZsAAAAJ"
    ],
    "citation_count": 111,
    "gs_author_count": 2
  },
  {
    "arxiv_id": "2505.06210",
    "title": "Topo-VM-UNetV2: Encoding Topology into Vision Mamba UNet for Polyp Segmentation",
    "year": 2025,
    "published": "2025-05-09T17:41:13Z",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "abstract": "Convolutional neural network (CNN) and Transformer-based architectures are two dominant deep learning models for polyp segmentation. However, CNNs have limited capability for modeling long-range dependencies, while Transformers incur quadratic computational complexity. Recently, State Space Models such as Mamba have been recognized as a promising approach for polyp segmentation because they not only model long-range interactions effectively but also maintain linear computational complexity. Howe",
    "arxiv_url": "https://arxiv.org/abs/2505.06210v1",
    "pdf_url": "https://arxiv.org/pdf/2505.06210v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2505.06210",
    "arxiv_authors": [
      "Diego Adame",
      "Jose A. Nunez",
      "Fabian Vazquez",
      "Nayeli Gurrola",
      "Huimin Li",
      "Haoteng Tang",
      "Bin Fu",
      "Pengfei Gu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Topo-VM-UNetV2%3A+Encoding+Topology+into+Vision+Mamba+UNet+for+Polyp+Segmentation+Diego+Adame+Jose+A.+Nunez+Fabian+Vazquez+Nayeli+Gurrola+Huimin+Li",
    "gs_search_success": true,
    "gs_authors": [
      "K7Ltzt4AAAAJ",
      "cauZmtsAAAAJ",
      "h_7OO78AAAAJ",
      "P9q4gXYAAAAJ"
    ],
    "citation_count": 2,
    "gs_author_count": 8
  },
  {
    "arxiv_id": "2311.01448",
    "title": "UltraLiDAR: Learning Compact Representations for LiDAR Completion and Generation",
    "year": 2023,
    "published": "2023-11-02T17:57:03Z",
    "categories": [
      "cs.CV",
      "cs.LG",
      "cs.RO"
    ],
    "abstract": "LiDAR provides accurate geometric measurements of the 3D world. Unfortunately, dense LiDARs are very expensive and the point clouds captured by low-beam LiDAR are often sparse. To address these issues, we present UltraLiDAR, a data-driven framework for scene-level LiDAR completion, LiDAR generation, and LiDAR manipulation. The crux of UltraLiDAR is a compact, discrete representation that encodes the point cloud's geometric structure, is robust to noise, and is easy to manipulate. We show that by",
    "arxiv_url": "https://arxiv.org/abs/2311.01448v1",
    "pdf_url": "https://arxiv.org/pdf/2311.01448v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2311.01448",
    "arxiv_authors": [
      "Yuwen Xiong",
      "Wei-Chiu Ma",
      "Jingkang Wang",
      "Raquel Urtasun"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=UltraLiDAR%3A+Learning+Compact+Representations+for+LiDAR+Completion+and+Generation+Yuwen+Xiong+Wei-Chiu+Ma+Jingkang+Wang+Raquel+Urtasun",
    "gs_search_success": true,
    "gs_authors": [
      "7YALCcIAAAAJ",
      "jyxO2akAAAAJ",
      "c0BTYC4AAAAJ",
      "SVIdh6AAAAAJ"
    ],
    "citation_count": 61,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2303.12742",
    "title": "Empirical Assessment of End-to-End Iris Recognition System Capacity",
    "year": 2023,
    "published": "2023-03-20T14:49:10Z",
    "categories": [
      "cs.CV",
      "eess.IV"
    ],
    "abstract": "Iris is an established modality in biometric recognition applications including consumer electronics, e-commerce, border security, forensics, and de-duplication of identity at a national scale. In light of the expanding usage of biometric recognition, identity clash (when templates from two different people match) is an imperative factor of consideration for a system's deployment. This study explores system capacity estimation by empirically estimating the constrained capacity of an end-to-end i",
    "arxiv_url": "https://arxiv.org/abs/2303.12742v1",
    "pdf_url": "https://arxiv.org/pdf/2303.12742v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2303.12742",
    "arxiv_authors": [
      "Priyanka Das",
      "Richard Plesh",
      "Veeru Talreja",
      "Natalia Schmid",
      "Matthew Valenti",
      "Joseph Skufca",
      "Stephanie Schuckers"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Empirical+Assessment+of+End-to-End+Iris+Recognition+System+Capacity+Priyanka+Das+Richard+Plesh+Veeru+Talreja+Natalia+Schmid+Matthew+Valenti",
    "gs_search_success": true,
    "gs_authors": [
      "voWXQToAAAAJ",
      "3j0dkI4AAAAJ",
      "q1wXZdkAAAAJ",
      "EP5j9ssAAAAJ",
      "7SfJRIIAAAAJ"
    ],
    "citation_count": 9,
    "gs_author_count": 9
  },
  {
    "arxiv_id": "2309.00059",
    "title": "STint: Self-supervised Temporal Interpolation for Geospatial Data",
    "year": 2023,
    "published": "2023-08-31T18:04:50Z",
    "categories": [
      "cs.CV",
      "eess.IV"
    ],
    "abstract": "Supervised and unsupervised techniques have demonstrated the potential for temporal interpolation of video data. Nevertheless, most prevailing temporal interpolation techniques hinge on optical flow, which encodes the motion of pixels between video frames. On the other hand, geospatial data exhibits lower temporal resolution while encompassing a spectrum of movements and deformations that challenge several assumptions inherent to optical flow. In this work, we propose an unsupervised temporal in",
    "arxiv_url": "https://arxiv.org/abs/2309.00059v1",
    "pdf_url": "https://arxiv.org/pdf/2309.00059v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2309.00059",
    "arxiv_authors": [
      "Nidhin Harilal",
      "Bri-Mathias Hodge",
      "Aneesh Subramanian",
      "Claire Monteleoni"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=STint%3A+Self-supervised+Temporal+Interpolation+for+Geospatial+Data+Nidhin+Harilal+Bri-Mathias+Hodge+Aneesh+Subramanian+Claire+Monteleoni",
    "gs_search_success": true,
    "gs_authors": [
      "Twbq4AEAAAAJ",
      "a5ZHSRsAAAAJ",
      "GpwXhsoAAAAJ",
      "FqNPXeoAAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2404.06406",
    "title": "Emergent Dynamics in Neural Cellular Automata",
    "year": 2024,
    "published": "2024-04-09T15:54:03Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Neural Cellular Automata (NCA) models are trainable variations of traditional Cellular Automata (CA). Emergent motion in the patterns created by NCA has been successfully applied to synthesize dynamic textures. However, the conditions required for an NCA to display dynamic patterns remain unexplored. Here, we investigate the relationship between the NCA architecture and the emergent dynamics of the trained models. Specifically, we vary the number of channels in the cell state and the number of h",
    "arxiv_url": "https://arxiv.org/abs/2404.06406v3",
    "pdf_url": "https://arxiv.org/pdf/2404.06406v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2404.06406",
    "arxiv_authors": [
      "Yitao Xu",
      "Ehsan Pajouheshgar",
      "Sabine Süsstrunk"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Emergent+Dynamics+in+Neural+Cellular+Automata+Yitao+Xu+Ehsan+Pajouheshgar+Sabine+S%C3%BCsstrunk",
    "gs_search_success": true,
    "gs_authors": [
      "bddQ6pQAAAAJ",
      "EX3OYP4AAAAJ",
      "jvCvNPQAAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2410.03645",
    "title": "GenSim2: Scaling Robot Data Generation with Multi-modal and Reasoning LLMs",
    "year": 2024,
    "published": "2024-10-04T17:51:33Z",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "Robotic simulation today remains challenging to scale up due to the human efforts required to create diverse simulation tasks and scenes. Simulation-trained policies also face scalability issues as many sim-to-real methods focus on a single task. To address these challenges, this work proposes GenSim2, a scalable framework that leverages coding LLMs with multi-modal and reasoning capabilities for complex and realistic simulation task creation, including long-horizon tasks with articulated object",
    "arxiv_url": "https://arxiv.org/abs/2410.03645v1",
    "pdf_url": "https://arxiv.org/pdf/2410.03645v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2410.03645",
    "arxiv_authors": [
      "Pu Hua",
      "Minghuan Liu",
      "Annabella Macaluso",
      "Yunfeng Lin",
      "Weinan Zhang",
      "Huazhe Xu",
      "Lirui Wang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=GenSim2%3A+Scaling+Robot+Data+Generation+with+Multi-modal+and+Reasoning+LLMs+Pu+Hua+Minghuan+Liu+Annabella+Macaluso+Yunfeng+Lin+Weinan+Zhang",
    "gs_search_success": true,
    "gs_authors": [
      "Qzss0GEAAAAJ",
      "EM9YhH0AAAAJ",
      "0ar3JNYAAAAJ",
      "O_4qYW4AAAAJ",
      "t9HPFawAAAAJ",
      "yqqESloAAAAJ",
      "zj5l2ZkAAAAJ"
    ],
    "citation_count": 28,
    "gs_author_count": 7
  },
  {
    "arxiv_id": "2503.04496",
    "title": "Learning Object Placement Programs for Indoor Scene Synthesis with Iterative Self Training",
    "year": 2025,
    "published": "2025-03-06T14:44:25Z",
    "categories": [
      "cs.GR",
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "Data driven and autoregressive indoor scene synthesis systems generate indoor scenes automatically by suggesting and then placing objects one at a time. Empirical observations show that current systems tend to produce incomplete next object location distributions. We introduce a system which addresses this problem. We design a Domain Specific Language (DSL) that specifies functional constraints. Programs from our language take as input a partial scene and object to place. Upon execution they pre",
    "arxiv_url": "https://arxiv.org/abs/2503.04496v1",
    "pdf_url": "https://arxiv.org/pdf/2503.04496v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2503.04496",
    "arxiv_authors": [
      "Adrian Chang",
      "Kai Wang",
      "Yuanbo Li",
      "Manolis Savva",
      "Angel X. Chang",
      "Daniel Ritchie"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Learning+Object+Placement+Programs+for+Indoor+Scene+Synthesis+with+Iterative+Self+Training+Adrian+Chang+Kai+Wang+Yuanbo+Li+Manolis+Savva+Angel+X.+Chang",
    "gs_search_success": true,
    "gs_authors": [
      "0RiypNsAAAAJ",
      "4D2vsdYAAAAJ",
      "8gfs8XIAAAAJ",
      "s_1CcsUAAAAJ",
      "juiUbFEAAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2408.10360",
    "title": "HaSPeR: An Image Repository for Hand Shadow Puppet Recognition",
    "year": 2024,
    "published": "2024-08-19T18:56:24Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Hand shadow puppetry, also known as shadowgraphy or ombromanie, is a form of theatrical art and storytelling where hand shadows are projected onto flat surfaces to create illusions of living creatures. The skilled performers create these silhouettes by hand positioning, finger movements, and dexterous gestures to resemble shadows of animals and objects. Due to the lack of practitioners and a seismic shift in people's entertainment standards, this art form is on the verge of extinction. To facili",
    "arxiv_url": "https://arxiv.org/abs/2408.10360v7",
    "pdf_url": "https://arxiv.org/pdf/2408.10360v7",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2408.10360",
    "arxiv_authors": [
      "Syed Rifat Raiyan",
      "Zibran Zarif Amio",
      "Sabbir Ahmed"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=HaSPeR%3A+An+Image+Repository+for+Hand+Shadow+Puppet+Recognition+Syed+Rifat+Raiyan+Zibran+Zarif+Amio+Sabbir+Ahmed",
    "gs_search_success": true,
    "gs_authors": [
      "4L_7vaoAAAAJ",
      "l_Dk4ZoAAAAJ",
      "IIGJYZgAAAAJ"
    ],
    "citation_count": 4,
    "gs_author_count": null
  },
  {
    "arxiv_id": "2408.09194",
    "title": "DRL-Based Resource Allocation for Motion Blur Resistant Federated Self-Supervised Learning in IoV",
    "year": 2024,
    "published": "2024-08-17T13:12:04Z",
    "categories": [
      "cs.CV",
      "cs.LG",
      "cs.NI"
    ],
    "abstract": "In the Internet of Vehicles (IoV), Federated Learning (FL) provides a privacy-preserving solution by aggregating local models without sharing data. Traditional supervised learning requires image data with labels, but data labeling involves significant manual effort. Federated Self-Supervised Learning (FSSL) utilizes Self-Supervised Learning (SSL) for local training in FL, eliminating the need for labels while protecting privacy. Compared to other SSL methods, Momentum Contrast (MoCo) reduces the",
    "arxiv_url": "https://arxiv.org/abs/2408.09194v2",
    "pdf_url": "https://arxiv.org/pdf/2408.09194v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2408.09194",
    "arxiv_authors": [
      "Xueying Gu",
      "Qiong Wu",
      "Pingyi Fan",
      "Qiang Fan",
      "Nan Cheng",
      "Wen Chen",
      "Khaled B. Letaief"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=DRL-Based+Resource+Allocation+for+Motion+Blur+Resistant+Federated+Self-Supervised+Learning+in+IoV+Xueying+Gu+Qiong+Wu+Pingyi+Fan+Qiang+Fan+Nan+Cheng",
    "gs_search_success": true,
    "gs_authors": [
      "Cxm51twAAAAJ",
      "-OCiYbYAAAAJ",
      "cBBPDKAAAAAJ",
      "IKstIhAAAAAJ",
      "gy98KEAAAAAJ",
      "6WLhtHgAAAAJ"
    ],
    "citation_count": 16,
    "gs_author_count": 7
  },
  {
    "arxiv_id": "2311.14275",
    "title": "Cooperative Dual Attention for Audio-Visual Speech Enhancement with Facial Cues",
    "year": 2023,
    "published": "2023-11-24T04:30:31Z",
    "categories": [
      "cs.CV",
      "cs.SD",
      "eess.AS"
    ],
    "abstract": "In this work, we focus on leveraging facial cues beyond the lip region for robust Audio-Visual Speech Enhancement (AVSE). The facial region, encompassing the lip region, reflects additional speech-related attributes such as gender, skin color, nationality, etc., which contribute to the effectiveness of AVSE. However, static and dynamic speech-unrelated attributes also exist, causing appearance changes during speech. To address these challenges, we propose a Dual Attention Cooperative Framework, ",
    "arxiv_url": "https://arxiv.org/abs/2311.14275v1",
    "pdf_url": "https://arxiv.org/pdf/2311.14275v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2311.14275",
    "arxiv_authors": [
      "Feixiang Wang",
      "Shuang Yang",
      "Shiguang Shan",
      "Xilin Chen"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Cooperative+Dual+Attention+for+Audio-Visual+Speech+Enhancement+with+Facial+Cues+Feixiang+Wang+Shuang+Yang+Shiguang+Shan+Xilin+Chen",
    "gs_search_success": true,
    "gs_authors": [
      "8wizL74AAAAJ",
      "Vkzd7MIAAAAJ"
    ],
    "citation_count": 2,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2312.03030",
    "title": "Generating Visually Realistic Adversarial Patch",
    "year": 2023,
    "published": "2023-12-05T11:07:39Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Deep neural networks (DNNs) are vulnerable to various types of adversarial examples, bringing huge threats to security-critical applications. Among these, adversarial patches have drawn increasing attention due to their good applicability to fool DNNs in the physical world. However, existing works often generate patches with meaningless noise or patterns, making it conspicuous to humans. To address this issue, we explore how to generate visually realistic adversarial patches to fool DNNs. Firstl",
    "arxiv_url": "https://arxiv.org/abs/2312.03030v1",
    "pdf_url": "https://arxiv.org/pdf/2312.03030v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2312.03030",
    "arxiv_authors": [
      "Xiaosen Wang",
      "Kunyu Wang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Generating+Visually+Realistic+Adversarial+Patch+Xiaosen+Wang+Kunyu+Wang",
    "gs_search_success": true,
    "gs_authors": [
      "sVeDOcsAAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 2
  },
  {
    "arxiv_id": "2412.08771",
    "title": "LLaVA-Zip: Adaptive Visual Token Compression with Intrinsic Image Information",
    "year": 2024,
    "published": "2024-12-11T20:46:06Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "abstract": "Multi-modal large language models (MLLMs) utilizing instruction-following data, such as LLaVA, have achieved great progress in the industry. A major limitation in these models is that visual tokens consume a substantial portion of the maximum token limit in large language models (LLMs), leading to increased computational demands and decreased performance when prompts include multiple images or videos. Industry solutions often mitigate this issue by increasing computational power, but this approa",
    "arxiv_url": "https://arxiv.org/abs/2412.08771v1",
    "pdf_url": "https://arxiv.org/pdf/2412.08771v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2412.08771",
    "arxiv_authors": [
      "Ke Wang",
      "Hong Xuan"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=LLaVA-Zip%3A+Adaptive+Visual+Token+Compression+with+Intrinsic+Image+Information+Ke+Wang+Hong+Xuan",
    "gs_search_success": true,
    "gs_authors": [
      "BhuVOcoAAAAJ"
    ],
    "citation_count": 2,
    "gs_author_count": 2
  },
  {
    "arxiv_id": "2306.04889",
    "title": "ShaDDR: Interactive Example-Based Geometry and Texture Generation via 3D Shape Detailization and Differentiable Rendering",
    "year": 2023,
    "published": "2023-06-08T02:35:30Z",
    "categories": [
      "cs.CV",
      "cs.GR",
      "cs.LG"
    ],
    "abstract": "We present ShaDDR, an example-based deep generative neural network which produces a high-resolution textured 3D shape through geometry detailization and conditional texture generation applied to an input coarse voxel shape. Trained on a small set of detailed and textured exemplar shapes, our method learns to detailize the geometry via multi-resolution voxel upsampling and generate textures on voxel surfaces via differentiable rendering against exemplar texture images from a few views. The genera",
    "arxiv_url": "https://arxiv.org/abs/2306.04889v2",
    "pdf_url": "https://arxiv.org/pdf/2306.04889v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2306.04889",
    "arxiv_authors": [
      "Qimin Chen",
      "Zhiqin Chen",
      "Hang Zhou",
      "Hao Zhang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=ShaDDR%3A+Interactive+Example-Based+Geometry+and+Texture+Generation+via+3D+Shape+Detailization+and+Differentiable+Rendering+Qimin+Chen+Zhiqin+Chen+Hang+Zhou+Hao+Zhang",
    "gs_search_success": true,
    "gs_authors": [
      "YrQxT8cAAAAJ",
      "N5MghGIAAAAJ",
      "ytK2uIoAAAAJ",
      "osTl-5IAAAAJ"
    ],
    "citation_count": 17,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2405.00354",
    "title": "CrossMatch: Enhance Semi-Supervised Medical Image Segmentation with Perturbation Strategies and Knowledge Distillation",
    "year": 2024,
    "published": "2024-05-01T07:16:03Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Semi-supervised learning for medical image segmentation presents a unique challenge of efficiently using limited labeled data while leveraging abundant unlabeled data. Despite advancements, existing methods often do not fully exploit the potential of the unlabeled data for enhancing model robustness and accuracy. In this paper, we introduce CrossMatch, a novel framework that integrates knowledge distillation with dual perturbation strategies-image-level and feature-level-to improve the model's l",
    "arxiv_url": "https://arxiv.org/abs/2405.00354v2",
    "pdf_url": "https://arxiv.org/pdf/2405.00354v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2405.00354",
    "arxiv_authors": [
      "Bin Zhao",
      "Chunshi Wang",
      "Shuxue Ding"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=CrossMatch%3A+Enhance+Semi-Supervised+Medical+Image+Segmentation+with+Perturbation+Strategies+and+Knowledge+Distillation+Bin+Zhao+Chunshi+Wang+Shuxue+Ding",
    "gs_search_success": true,
    "gs_authors": [
      "8D8CMiEAAAAJ",
      "LNxEN9cAAAAJ",
      "DQB0hqwAAAAJ"
    ],
    "citation_count": 16,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2504.06815",
    "title": "SVG-IR: Spatially-Varying Gaussian Splatting for Inverse Rendering",
    "year": 2025,
    "published": "2025-04-09T12:11:58Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Reconstructing 3D assets from images, known as inverse rendering (IR), remains a challenging task due to its ill-posed nature. 3D Gaussian Splatting (3DGS) has demonstrated impressive capabilities for novel view synthesis (NVS) tasks. Methods apply it to relighting by separating radiance into BRDF parameters and lighting, yet produce inferior relighting quality with artifacts and unnatural indirect illumination due to the limited capability of each Gaussian, which has constant material parameter",
    "arxiv_url": "https://arxiv.org/abs/2504.06815v1",
    "pdf_url": "https://arxiv.org/pdf/2504.06815v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2504.06815",
    "arxiv_authors": [
      "Hanxiao Sun",
      "YuPeng Gao",
      "Jin Xie",
      "Jian Yang",
      "Beibei Wang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=SVG-IR%3A+Spatially-Varying+Gaussian+Splatting+for+Inverse+Rendering+Hanxiao+Sun+YuPeng+Gao+Jin+Xie+Jian+Yang+Beibei+Wang",
    "gs_search_success": true,
    "gs_authors": [
      "bF4JBKYAAAAJ"
    ],
    "citation_count": 2,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2303.10571",
    "title": "Reinforcement Learning Friendly Vision-Language Model for Minecraft",
    "year": 2023,
    "published": "2023-03-19T05:20:52Z",
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "abstract": "One of the essential missions in the AI research community is to build an autonomous embodied agent that can achieve high-level performance across a wide spectrum of tasks. However, acquiring or manually designing rewards for all open-ended tasks is unrealistic. In this paper, we propose a novel cross-modal contrastive learning framework architecture, CLIP4MC, aiming to learn a reinforcement learning (RL) friendly vision-language model (VLM) that serves as an intrinsic reward function for open-e",
    "arxiv_url": "https://arxiv.org/abs/2303.10571v2",
    "pdf_url": "https://arxiv.org/pdf/2303.10571v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2303.10571",
    "arxiv_authors": [
      "Haobin Jiang",
      "Junpeng Yue",
      "Hao Luo",
      "Ziluo Ding",
      "Zongqing Lu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Reinforcement+Learning+Friendly+Vision-Language+Model+for+Minecraft+Haobin+Jiang+Junpeng+Yue+Hao+Luo+Ziluo+Ding+Zongqing+Lu",
    "gs_search_success": true,
    "gs_authors": [
      "4jL2iaUAAAAJ",
      "TwuNaTYAAAAJ",
      "o0NO50kAAAAJ",
      "5Oc2LAEAAAAJ",
      "k3IFtTYAAAAJ"
    ],
    "citation_count": 26,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2504.16940",
    "title": "Better artificial intelligence does not mean better models of biology",
    "year": 2025,
    "published": "2025-04-08T00:36:29Z",
    "categories": [
      "q-bio.NC",
      "cs.AI",
      "cs.CV"
    ],
    "abstract": "Deep neural networks (DNNs) once showed increasing alignment with primate perception and neural responses as they improved on vision benchmarks, raising hopes that advances in AI would yield better models of biological vision. However, we show across three benchmarks that this alignment is now plateauing - and in some cases worsening - as DNNs scale to human or superhuman accuracy. This divergence may reflect the adoption of visual strategies that differ from those used by primates. These findin",
    "arxiv_url": "https://arxiv.org/abs/2504.16940v3",
    "pdf_url": "https://arxiv.org/pdf/2504.16940v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2504.16940",
    "arxiv_authors": [
      "Drew Linsley",
      "Pinyuan Feng",
      "Thomas Serre"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Better+artificial+intelligence+does+not+mean+better+models+of+biology+Drew+Linsley+Pinyuan+Feng+Thomas+Serre",
    "gs_search_success": true,
    "gs_authors": [
      "kZlPW4wAAAAJ",
      "cXZlAuQAAAAJ",
      "XT9SDFEAAAAJ"
    ],
    "citation_count": 3,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2410.03816",
    "title": "Modeling and Analysis of Spatial and Temporal Land Clutter Statistics in SAR Imaging Based on MSTAR Data",
    "year": 2024,
    "published": "2024-10-04T18:47:49Z",
    "categories": [
      "cs.CV",
      "eess.SP",
      "stat.AP"
    ],
    "abstract": "The statistical analysis of land clutter for Synthetic Aperture Radar (SAR) imaging has become an increasingly important subject for research and investigation. It is also absolutely necessary for designing robust algorithms capable of performing the task of target detection in the background clutter. Any attempt to extract the energy of the desired targets from the land clutter requires complete knowledge of the statistical properties of the background clutter. In this paper, the spatial as wel",
    "arxiv_url": "https://arxiv.org/abs/2410.03816v2",
    "pdf_url": "https://arxiv.org/pdf/2410.03816v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2410.03816",
    "arxiv_authors": [
      "Shahrokh Hamidi"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Modeling+and+Analysis+of+Spatial+and+Temporal+Land+Clutter+Statistics+in+SAR+Imaging+Based+on+MSTAR+Data+Shahrokh+Hamidi",
    "gs_search_success": true,
    "gs_authors": [
      "fPFBTC0AAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 1
  },
  {
    "arxiv_id": "2307.08930",
    "title": "Unsupervised Deep Graph Matching Based on Cycle Consistency",
    "year": 2023,
    "published": "2023-07-18T02:35:01Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "We contribute to the sparsely populated area of unsupervised deep graph matching with application to keypoint matching in images. Contrary to the standard \\emph{supervised} approach, our method does not require ground truth correspondences between keypoint pairs. Instead, it is self-supervised by enforcing consistency of matchings between images of the same object category. As the matching and the consistency loss are discrete, their derivatives cannot be straightforwardly used for learning. We ",
    "arxiv_url": "https://arxiv.org/abs/2307.08930v5",
    "pdf_url": "https://arxiv.org/pdf/2307.08930v5",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2307.08930",
    "arxiv_authors": [
      "Siddharth Tourani",
      "Carsten Rother",
      "Muhammad Haris Khan",
      "Bogdan Savchynskyy"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Unsupervised+Deep+Graph+Matching+Based+on+Cycle+Consistency+Siddharth+Tourani+Carsten+Rother+Muhammad+Haris+Khan+Bogdan+Savchynskyy",
    "gs_search_success": true,
    "gs_authors": [
      "ZgERfFwAAAAJ",
      "N_YNMIMAAAAJ",
      "nvycljAAAAAJ"
    ],
    "citation_count": 4,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2311.13603",
    "title": "Cross-layer scheme for low latency multiple description video streaming over Vehicular Ad-hoc NETworks (VANETs)",
    "year": 2023,
    "published": "2023-11-05T14:34:58Z",
    "categories": [
      "cs.CV",
      "cs.MM",
      "cs.NI",
      "eess.IV"
    ],
    "abstract": "There is nowadays a growing demand in vehicular communications for real-time applications requiring video assistance. The new state-of-the-art high-efficiency video coding (HEVC) standard is very promising for real-time video streaming. It offers high coding efficiency, as well as dedicated low delay coding structures. Among these, the all intra (AI) coding structure guarantees minimal coding time at the expense of higher video bitrates, which therefore penalizes transmission performances. In th",
    "arxiv_url": "https://arxiv.org/abs/2311.13603v1",
    "pdf_url": "https://arxiv.org/pdf/2311.13603v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2311.13603",
    "arxiv_authors": [
      "Mohamed Aymen Labiod",
      "Mohamed Gharbi",
      "Francois-Xavier Coudoux",
      "Patrick Corlay",
      "Noureddine Doghmane"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Cross-layer+scheme+for+low+latency+multiple+description+video+streaming+over+Vehicular+Ad-hoc+NETworks+%28VANETs%29+Mohamed+Aymen+Labiod+Mohamed+Gharbi+Francois-Xavier+Coudoux+Patrick+Corlay+Noureddine+Doghmane",
    "gs_search_success": true,
    "gs_authors": [
      "bR4D9PUAAAAJ",
      "xykCeIcAAAAJ",
      "w1Z9B6sAAAAJ"
    ],
    "citation_count": 22,
    "gs_author_count": null
  },
  {
    "arxiv_id": "2502.13935",
    "title": "Foundations of a Developmental Design Paradigm for Integrated Continual Learning, Deliberative Behavior, and Comprehensibility",
    "year": 2025,
    "published": "2025-02-19T18:18:27Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "abstract": "Inherent limitations of contemporary machine learning systems in crucial areas -- importantly in continual learning, information reuse, comprehensibility, and integration with deliberate behavior -- are receiving increasing attention. To address these challenges, we introduce a system design, fueled by a novel learning approach conceptually grounded in principles of evolutionary developmental biology, that overcomes key limitations of current methods. Our design comprises three core components: ",
    "arxiv_url": "https://arxiv.org/abs/2502.13935v2",
    "pdf_url": "https://arxiv.org/pdf/2502.13935v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2502.13935",
    "arxiv_authors": [
      "Zeki Doruk Erden",
      "Boi Faltings"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Foundations+of+a+Developmental+Design+Paradigm+for+Integrated+Continual+Learning%2C+Deliberative+Behavior%2C+and+Comprehensibility+Zeki+Doruk+Erden+Boi+Faltings",
    "gs_search_success": true,
    "gs_authors": [
      "dJg1TUEAAAAJ",
      "IbVrW6EAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 2
  },
  {
    "arxiv_id": "2504.20178",
    "title": "A Transformer-based Multimodal Fusion Model for Efficient Crowd Counting Using Visual and Wireless Signals",
    "year": 2025,
    "published": "2025-04-28T18:26:28Z",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "Current crowd-counting models often rely on single-modal inputs, such as visual images or wireless signal data, which can result in significant information loss and suboptimal recognition performance. To address these shortcomings, we propose TransFusion, a novel multimodal fusion-based crowd-counting model that integrates Channel State Information (CSI) with image data. By leveraging the powerful capabilities of Transformer networks, TransFusion effectively combines these two distinct data moda",
    "arxiv_url": "https://arxiv.org/abs/2504.20178v1",
    "pdf_url": "https://arxiv.org/pdf/2504.20178v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2504.20178",
    "arxiv_authors": [
      "Zhe Cui",
      "Yuli Li",
      "Le-Nam Tran"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=A+Transformer-based+Multimodal+Fusion+Model+for+Efficient+Crowd+Counting+Using+Visual+and+Wireless+Signals+Zhe+Cui+Yuli+Li+Le-Nam+Tran",
    "gs_search_success": true,
    "gs_authors": [
      "Y3mPMEwAAAAJ",
      "iVZZ2SAAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2409.06104",
    "title": "LSE-NeRF: Learning Sensor Modeling Errors for Deblured Neural Radiance Fields with RGB-Event Stereo",
    "year": 2024,
    "published": "2024-09-09T23:11:46Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "We present a method for reconstructing a clear Neural Radiance Field (NeRF) even with fast camera motions. To address blur artifacts, we leverage both (blurry) RGB images and event camera data captured in a binocular configuration. Importantly, when reconstructing our clear NeRF, we consider the camera modeling imperfections that arise from the simple pinhole camera model as learned embeddings for each camera measurement, and further learn a mapper that connects event camera measurements with RG",
    "arxiv_url": "https://arxiv.org/abs/2409.06104v1",
    "pdf_url": "https://arxiv.org/pdf/2409.06104v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2409.06104",
    "arxiv_authors": [
      "Wei Zhi Tang",
      "Daniel Rebain",
      "Kostantinos G. Derpanis",
      "Kwang Moo Yi"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=LSE-NeRF%3A+Learning+Sensor+Modeling+Errors+for+Deblured+Neural+Radiance+Fields+with+RGB-Event+Stereo+Wei+Zhi+Tang+Daniel+Rebain+Kostantinos+G.+Derpanis+Kwang+Moo+Yi",
    "gs_search_success": true,
    "gs_authors": [
      "pr6rIJEAAAAJ",
      "h-qFKrQAAAAJ",
      "3Br8x_gAAAAJ"
    ],
    "citation_count": 8,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2404.14507",
    "title": "Align Your Steps: Optimizing Sampling Schedules in Diffusion Models",
    "year": 2024,
    "published": "2024-04-22T18:18:41Z",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "Diffusion models (DMs) have established themselves as the state-of-the-art generative modeling approach in the visual domain and beyond. A crucial drawback of DMs is their slow sampling speed, relying on many sequential function evaluations through large neural networks. Sampling from DMs can be seen as solving a differential equation through a discretized set of noise levels known as the sampling schedule. While past works primarily focused on deriving efficient solvers, little attention has be",
    "arxiv_url": "https://arxiv.org/abs/2404.14507v1",
    "pdf_url": "https://arxiv.org/pdf/2404.14507v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2404.14507",
    "arxiv_authors": [
      "Amirmojtaba Sabour",
      "Sanja Fidler",
      "Karsten Kreis"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Align+Your+Steps%3A+Optimizing+Sampling+Schedules+in+Diffusion+Models+Amirmojtaba+Sabour+Sanja+Fidler+Karsten+Kreis",
    "gs_search_success": true,
    "gs_authors": [
      "rFd-DiAAAAAJ",
      "pUEBuscAAAAJ",
      "CUlqK5EAAAAJ"
    ],
    "citation_count": 50,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2403.18821",
    "title": "Real Acoustic Fields: An Audio-Visual Room Acoustics Dataset and Benchmark",
    "year": 2024,
    "published": "2024-03-27T17:59:56Z",
    "categories": [
      "cs.SD",
      "cs.CV",
      "cs.MM",
      "eess.AS"
    ],
    "abstract": "We present a new dataset called Real Acoustic Fields (RAF) that captures real acoustic room data from multiple modalities. The dataset includes high-quality and densely captured room impulse response data paired with multi-view images, and precise 6DoF pose tracking data for sound emitters and listeners in the rooms. We used this dataset to evaluate existing methods for novel-view acoustic synthesis and impulse response generation which previously relied on synthetic data. In our evaluation, we ",
    "arxiv_url": "https://arxiv.org/abs/2403.18821v1",
    "pdf_url": "https://arxiv.org/pdf/2403.18821v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2403.18821",
    "arxiv_authors": [
      "Ziyang Chen",
      "Israel D. Gebru",
      "Christian Richardt",
      "Anurag Kumar",
      "William Laney",
      "Andrew Owens",
      "Alexander Richard"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Real+Acoustic+Fields%3A+An+Audio-Visual+Room+Acoustics+Dataset+and+Benchmark+Ziyang+Chen+Israel+D.+Gebru+Christian+Richardt+Anurag+Kumar+William+Laney",
    "gs_search_success": true,
    "gs_authors": [
      "PbsR83sAAAAJ",
      "5RFgT84AAAAJ",
      "AZH_wV0AAAAJ",
      "9hX-JksAAAAJ",
      "73DTbNAAAAAJ",
      "HH5cCX0AAAAJ"
    ],
    "citation_count": 34,
    "gs_author_count": 7
  },
  {
    "arxiv_id": "2312.15916",
    "title": "Monocular 3D Hand Mesh Recovery via Dual Noise Estimation",
    "year": 2023,
    "published": "2023-12-26T07:21:01Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Current parametric models have made notable progress in 3D hand pose and shape estimation. However, due to the fixed hand topology and complex hand poses, current models are hard to generate meshes that are aligned with the image well. To tackle this issue, we introduce a dual noise estimation method in this paper. Given a single-view image as input, we first adopt a baseline parametric regressor to obtain the coarse hand meshes. We assume the mesh vertices and their image-plane projections are ",
    "arxiv_url": "https://arxiv.org/abs/2312.15916v1",
    "pdf_url": "https://arxiv.org/pdf/2312.15916v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2312.15916",
    "arxiv_authors": [
      "Hanhui Li",
      "Xiaojian Lin",
      "Xuan Huang",
      "Zejun Yang",
      "Zhisheng Wang",
      "Xiaodan Liang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Monocular+3D+Hand+Mesh+Recovery+via+Dual+Noise+Estimation+Hanhui+Li+Xiaojian+Lin+Xuan+Huang+Zejun+Yang+Zhisheng+Wang",
    "gs_search_success": true,
    "gs_authors": [
      "liHuqdgAAAAJ",
      "XrK2HNcAAAAJ",
      "voxznZAAAAAJ",
      "aKLA6owAAAAJ"
    ],
    "citation_count": 2,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2401.12176",
    "title": "Broiler-Net: A Deep Convolutional Framework for Broiler Behavior Analysis in Poultry Houses",
    "year": 2024,
    "published": "2024-01-22T18:09:15Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Detecting anomalies in poultry houses is crucial for maintaining optimal chicken health conditions, minimizing economic losses and bolstering profitability. This paper presents a novel real-time framework for analyzing chicken behavior in cage-free poultry houses to detect abnormal behaviors. Specifically, two significant abnormalities, namely inactive broiler and huddling behavior, are investigated in this study. The proposed framework comprises three key steps: (1) chicken detection utilizing ",
    "arxiv_url": "https://arxiv.org/abs/2401.12176v1",
    "pdf_url": "https://arxiv.org/pdf/2401.12176v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2401.12176",
    "arxiv_authors": [
      "Tahereh Zarrat Ehsan",
      "Seyed Mehdi Mohtavipour"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Broiler-Net%3A+A+Deep+Convolutional+Framework+for+Broiler+Behavior+Analysis+in+Poultry+Houses+Tahereh+Zarrat+Ehsan+Seyed+Mehdi+Mohtavipour",
    "gs_search_success": true,
    "gs_authors": [
      "fKBl-m8AAAAJ",
      "YG3LknEAAAAJ"
    ],
    "citation_count": 6,
    "gs_author_count": null
  },
  {
    "arxiv_id": "2502.14129",
    "title": "GlossGau: Efficient Inverse Rendering for Glossy Surface with Anisotropic Spherical Gaussian",
    "year": 2025,
    "published": "2025-02-19T22:20:57Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "The reconstruction of 3D objects from calibrated photographs represents a fundamental yet intricate challenge in the domains of computer graphics and vision. Although neural reconstruction approaches based on Neural Radiance Fields (NeRF) have shown remarkable capabilities, their processing costs remain substantial. Recently, the advent of 3D Gaussian Splatting (3D-GS) largely improves the training efficiency and facilitates to generate realistic rendering in real-time. However, due to the limit",
    "arxiv_url": "https://arxiv.org/abs/2502.14129v1",
    "pdf_url": "https://arxiv.org/pdf/2502.14129v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2502.14129",
    "arxiv_authors": [
      "Bang Du",
      "Runfa Blark Li",
      "Chen Du",
      "Truong Nguyen"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=GlossGau%3A+Efficient+Inverse+Rendering+for+Glossy+Surface+with+Anisotropic+Spherical+Gaussian+Bang+Du+Runfa+Blark+Li+Chen+Du+Truong+Nguyen",
    "gs_search_success": true,
    "gs_authors": [
      "vntCBHQAAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2503.09449",
    "title": "Fast computation of the TGOSPA metric for multiple target tracking via unbalanced optimal transport",
    "year": 2025,
    "published": "2025-03-12T14:51:03Z",
    "categories": [
      "math.OC",
      "cs.CV",
      "eess.SY"
    ],
    "abstract": "In multiple target tracking, it is important to be able to evaluate the performance of different tracking algorithms. The trajectory generalized optimal sub-pattern assignment metric (TGOSPA) is a recently proposed metric for such evaluations. The TGOSPA metric is computed as the solution to an optimization problem, but for large tracking scenarios, solving this problem becomes computationally demanding. In this paper, we present an approximation algorithm for evaluating the TGOSPA metric, based",
    "arxiv_url": "https://arxiv.org/abs/2503.09449v2",
    "pdf_url": "https://arxiv.org/pdf/2503.09449v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2503.09449",
    "arxiv_authors": [
      "Viktor Nevelius Wernholm",
      "Alfred Wärnsäter",
      "Axel Ringh"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Fast+computation+of+the+TGOSPA+metric+for+multiple+target+tracking+via+unbalanced+optimal+transport+Viktor+Nevelius+Wernholm+Alfred+W%C3%A4rns%C3%A4ter+Axel+Ringh",
    "gs_search_success": true,
    "gs_authors": [
      "5_PtHCMAAAAJ"
    ],
    "citation_count": 2,
    "gs_author_count": null
  },
  {
    "arxiv_id": "2312.15897",
    "title": "Recursive Distillation for Open-Set Distributed Robot Localization",
    "year": 2023,
    "published": "2023-12-26T06:20:55Z",
    "categories": [
      "cs.RO",
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "A typical assumption in state-of-the-art self-localization models is that an annotated training dataset is available for the target workspace. However, this is not necessarily true when a robot travels around the general open world. This work introduces a novel training scheme for open-world distributed robot systems. In our scheme, a robot (``student\") can ask the other robots it meets at unfamiliar places (``teachers\") for guidance. Specifically, a pseudo-training dataset is reconstructed from",
    "arxiv_url": "https://arxiv.org/abs/2312.15897v2",
    "pdf_url": "https://arxiv.org/pdf/2312.15897v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2312.15897",
    "arxiv_authors": [
      "Kenta Tsukahara",
      "Kanji Tanaka"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Recursive+Distillation+for+Open-Set+Distributed+Robot+Localization+Kenta+Tsukahara+Kanji+Tanaka",
    "gs_search_success": true,
    "gs_authors": [],
    "citation_count": null,
    "gs_author_count": 2
  },
  {
    "arxiv_id": "2406.10638",
    "title": "Unveiling the Ignorance of MLLMs: Seeing Clearly, Answering Incorrectly",
    "year": 2024,
    "published": "2024-06-15T13:58:26Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Multimodal Large Language Models (MLLMs) have displayed remarkable performance in multi-modal tasks, particularly in visual comprehension. However, we reveal that MLLMs often generate incorrect answers even when they understand the visual content. To this end, we manually construct a benchmark with 12 categories and design evaluation metrics that assess the degree of error in MLLM responses even when the visual content is seemingly understood. Based on this benchmark, we test 15 leading MLLMs an",
    "arxiv_url": "https://arxiv.org/abs/2406.10638v3",
    "pdf_url": "https://arxiv.org/pdf/2406.10638v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2406.10638",
    "arxiv_authors": [
      "Yexin Liu",
      "Zhengyang Liang",
      "Yueze Wang",
      "Xianfeng Wu",
      "Feilong Tang",
      "Muyang He",
      "Jian Li",
      "Zheng Liu",
      "Harry Yang",
      "Sernam Lim",
      "Bo Zhao"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Unveiling+the+Ignorance+of+MLLMs%3A+Seeing+Clearly%2C+Answering+Incorrectly+Yexin+Liu+Zhengyang+Liang+Yueze+Wang+Xianfeng+Wu+Feilong+Tang",
    "gs_search_success": true,
    "gs_authors": [
      "ga2MKaMAAAAJ",
      "Q0Xn7i4AAAAJ",
      "ACb5C40AAAAJ",
      "9IC8FBQAAAAJ",
      "dBUiUBMAAAAJ",
      "jpIFgToAAAAJ",
      "Y8zBpcoAAAAJ",
      "C9B5JKYAAAAJ",
      "HX0BfLYAAAAJ"
    ],
    "citation_count": 7,
    "gs_author_count": 11
  },
  {
    "arxiv_id": "2311.14906",
    "title": "AutoEval-Video: An Automatic Benchmark for Assessing Large Vision Language Models in Open-Ended Video Question Answering",
    "year": 2023,
    "published": "2023-11-25T02:46:12Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "We propose a novel and challenging benchmark, AutoEval-Video, to comprehensively evaluate large vision-language models in open-ended video question answering. The comprehensiveness of AutoEval-Video is demonstrated in two aspects: 1) AutoEval-Video constructs open-ended video-questions across 9 skill dimensions, addressing capabilities of perception, comprehension, and generation. 2) AutoEval-Video contains newly collected videos that cover over 40 distinct themes. To efficiently evaluate respon",
    "arxiv_url": "https://arxiv.org/abs/2311.14906v2",
    "pdf_url": "https://arxiv.org/pdf/2311.14906v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2311.14906",
    "arxiv_authors": [
      "Xiuyuan Chen",
      "Yuan Lin",
      "Yuchen Zhang",
      "Weiran Huang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=AutoEval-Video%3A+An+Automatic+Benchmark+for+Assessing+Large+Vision+Language+Models+in+Open-Ended+Video+Question+Answering+Xiuyuan+Chen+Yuan+Lin+Yuchen+Zhang+Weiran+Huang",
    "gs_search_success": true,
    "gs_authors": [
      "vUPLTvoAAAAJ",
      "AjJ2rf8AAAAJ",
      "wD4HrKkAAAAJ"
    ],
    "citation_count": 42,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2309.06680",
    "title": "STUPD: A Synthetic Dataset for Spatial and Temporal Relation Reasoning",
    "year": 2023,
    "published": "2023-09-13T02:35:59Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Understanding relations between objects is crucial for understanding the semantics of a visual scene. It is also an essential step in order to bridge visual and language models. However, current state-of-the-art computer vision models still lack the ability to perform spatial reasoning well. Existing datasets mostly cover a relatively small number of spatial relations, all of which are static relations that do not intrinsically involve motion. In this paper, we propose the Spatial and Temporal U",
    "arxiv_url": "https://arxiv.org/abs/2309.06680v3",
    "pdf_url": "https://arxiv.org/pdf/2309.06680v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2309.06680",
    "arxiv_authors": [
      "Palaash Agrawal",
      "Haidi Azaman",
      "Cheston Tan"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=STUPD%3A+A+Synthetic+Dataset+for+Spatial+and+Temporal+Relation+Reasoning+Palaash+Agrawal+Haidi+Azaman+Cheston+Tan",
    "gs_search_success": true,
    "gs_authors": [
      "Up0UYEYAAAAJ",
      "nvebgE0AAAAJ",
      "rIrM-oEAAAAJ"
    ],
    "citation_count": 6,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2407.01959",
    "title": "FlowTrack: Point-level Flow Network for 3D Single Object Tracking",
    "year": 2024,
    "published": "2024-07-02T05:31:34Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "3D single object tracking (SOT) is a crucial task in fields of mobile robotics and autonomous driving. Traditional motion-based approaches achieve target tracking by estimating the relative movement of target between two consecutive frames. However, they usually overlook local motion information of the target and fail to exploit historical frame information effectively. To overcome the above limitations, we propose a point-level flow method with multi-frame information for 3D SOT task, called Fl",
    "arxiv_url": "https://arxiv.org/abs/2407.01959v1",
    "pdf_url": "https://arxiv.org/pdf/2407.01959v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2407.01959",
    "arxiv_authors": [
      "Shuo Li",
      "Yubo Cui",
      "Zhiheng Li",
      "Zheng Fang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=FlowTrack%3A+Point-level+Flow+Network+for+3D+Single+Object+Tracking+Shuo+Li+Yubo+Cui+Zhiheng+Li+Zheng+Fang",
    "gs_search_success": true,
    "gs_authors": [
      "PJ9x-6AAAAAJ"
    ],
    "citation_count": 3,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2311.16637",
    "title": "Parallax-Tolerant Image Stitching with Epipolar Displacement Field",
    "year": 2023,
    "published": "2023-11-28T09:44:01Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Image stitching with parallax is still a challenging task. Existing methods often struggle to maintain both the local and global structures of the image while reducing alignment artifacts and warping distortions. In this paper, we propose a novel approach that utilizes epipolar geometry to establish a warping technique based on the epipolar displacement field. Initially, the warping rule for pixels in the epipolar geometry is established through the infinite homography. Subsequently, the epipola",
    "arxiv_url": "https://arxiv.org/abs/2311.16637v2",
    "pdf_url": "https://arxiv.org/pdf/2311.16637v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2311.16637",
    "arxiv_authors": [
      "Jian Yu",
      "Feipeng Da"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Parallax-Tolerant+Image+Stitching+with+Epipolar+Displacement+Field+Jian+Yu+Feipeng+Da",
    "gs_search_success": true,
    "gs_authors": [
      "1GKjCB8AAAAJ"
    ],
    "citation_count": 2,
    "gs_author_count": 2
  },
  {
    "arxiv_id": "2405.10030",
    "title": "RSDehamba: Lightweight Vision Mamba for Remote Sensing Satellite Image Dehazing",
    "year": 2024,
    "published": "2024-05-16T12:12:07Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Remote sensing image dehazing (RSID) aims to remove nonuniform and physically irregular haze factors for high-quality image restoration. The emergence of CNNs and Transformers has taken extraordinary strides in the RSID arena. However, these methods often struggle to demonstrate the balance of adequate long-range dependency modeling and maintaining computational efficiency. To this end, we propose the first lightweight network on the mamba-based model called RSDhamba in the field of RSID. Greatl",
    "arxiv_url": "https://arxiv.org/abs/2405.10030v1",
    "pdf_url": "https://arxiv.org/pdf/2405.10030v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2405.10030",
    "arxiv_authors": [
      "Huiling Zhou",
      "Xianhao Wu",
      "Hongming Chen",
      "Xiang Chen",
      "Xin He"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=RSDehamba%3A+Lightweight+Vision+Mamba+for+Remote+Sensing+Satellite+Image+Dehazing+Huiling+Zhou+Xianhao+Wu+Hongming+Chen+Xiang+Chen+Xin+He",
    "gs_search_success": true,
    "gs_authors": [
      "C9LyxWEAAAAJ",
      "2DBuqfkAAAAJ"
    ],
    "citation_count": 32,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2407.18128",
    "title": "Estimating Earthquake Magnitude in Sentinel-1 Imagery via Ranking",
    "year": 2024,
    "published": "2024-07-25T15:35:44Z",
    "categories": [
      "cs.CV",
      "eess.IV"
    ],
    "abstract": "Earthquakes are commonly estimated using physical seismic stations, however, due to the installation requirements and costs of these stations, global coverage quickly becomes impractical. An efficient and lower-cost alternative is to develop machine learning models to globally monitor earth observation data to pinpoint regions impacted by these natural disasters. However, due to the small amount of historically recorded earthquakes, this becomes a low-data regime problem requiring algorithmic im",
    "arxiv_url": "https://arxiv.org/abs/2407.18128v2",
    "pdf_url": "https://arxiv.org/pdf/2407.18128v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2407.18128",
    "arxiv_authors": [
      "Daniele Rege Cambrin",
      "Isaac Corley",
      "Paolo Garza",
      "Peyman Najafirad"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Estimating+Earthquake+Magnitude+in+Sentinel-1+Imagery+via+Ranking+Daniele+Rege+Cambrin+Isaac+Corley+Paolo+Garza+Peyman+Najafirad",
    "gs_search_success": true,
    "gs_authors": [
      "w9dI8esAAAAJ",
      "Xw0xO3UAAAAJ",
      "uoCn8c8AAAAJ",
      "7m0N4zUAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": null
  },
  {
    "arxiv_id": "2311.15463",
    "title": "Where to Begin? From Random to Foundation Model Instructed Initialization in Federated Learning for Medical Image Segmentation",
    "year": 2023,
    "published": "2023-11-27T00:29:10Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "In medical image analysis, Federated Learning (FL) stands out as a key technology that enables privacy-preserved, decentralized data processing, crucial for handling sensitive medical data. Currently, most FL models employ random initialization, which has been proven effective in various instances. However, given the unique challenges posed by non-IID (independently and identically distributed) data in FL, we propose a novel perspective: exploring the impact of using the foundation model with en",
    "arxiv_url": "https://arxiv.org/abs/2311.15463v1",
    "pdf_url": "https://arxiv.org/pdf/2311.15463v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2311.15463",
    "arxiv_authors": [
      "Ming Li",
      "Guang Yang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Where+to+Begin%3F+From+Random+to+Foundation+Model+Instructed+Initialization+in+Federated+Learning+for+Medical+Image+Segmentation+Ming+Li+Guang+Yang",
    "gs_search_success": true,
    "gs_authors": [
      "0gNvQVcAAAAJ",
      "ZfzEFpsAAAAJ"
    ],
    "citation_count": 4,
    "gs_author_count": 2
  },
  {
    "arxiv_id": "2409.15341",
    "title": "StructuReiser: A Structure-preserving Video Stylization Method",
    "year": 2024,
    "published": "2024-09-09T21:09:47Z",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "abstract": "We introduce StructuReiser, a novel video-to-video translation method that transforms input videos into stylized sequences using a set of user-provided keyframes. Unlike existing approaches, StructuReiser maintains strict adherence to the structural elements of the target video, preserving the original identity while seamlessly applying the desired stylistic transformations. This enables a level of control and consistency that was previously unattainable with traditional text-driven or keyframe-",
    "arxiv_url": "https://arxiv.org/abs/2409.15341v2",
    "pdf_url": "https://arxiv.org/pdf/2409.15341v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2409.15341",
    "arxiv_authors": [
      "Radim Spetlik",
      "David Futschik",
      "Daniel Sykora"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=StructuReiser%3A+A+Structure-preserving+Video+Stylization+Method+Radim+Spetlik+David+Futschik+Daniel+Sykora",
    "gs_search_success": true,
    "gs_authors": [
      "sH-zNWkAAAAJ",
      "rs0a3IQAAAAJ",
      "ozNFrecAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2504.17945",
    "title": "Spectral Bias Correction in PINNs for Myocardial Image Registration of Pathological Data",
    "year": 2025,
    "published": "2025-04-24T21:18:11Z",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "abstract": "Accurate myocardial image registration is essential for cardiac strain analysis and disease diagnosis. However, spectral bias in neural networks impedes modeling high-frequency deformations, producing inaccurate, biomechanically implausible results, particularly in pathological data. This paper addresses spectral bias in physics-informed neural networks (PINNs) by integrating Fourier Feature mappings and introducing modulation strategies into a PINN framework. Experiments on two distinct dataset",
    "arxiv_url": "https://arxiv.org/abs/2504.17945v1",
    "pdf_url": "https://arxiv.org/pdf/2504.17945v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2504.17945",
    "arxiv_authors": [
      "Bastien C. Baluyot",
      "Marta Varela",
      "Chen Qin"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Spectral+Bias+Correction+in+PINNs+for+Myocardial+Image+Registration+of+Pathological+Data+Bastien+C.+Baluyot+Marta+Varela+Chen+Qin",
    "gs_search_success": false,
    "gs_authors": [],
    "citation_count": null,
    "gs_author_count": null,
    "error_type": "fetch_failed"
  },
  {
    "arxiv_id": "2307.09302",
    "title": "Conformal prediction under ambiguous ground truth",
    "year": 2023,
    "published": "2023-07-18T14:40:48Z",
    "categories": [
      "cs.LG",
      "cs.CV",
      "stat.ME",
      "stat.ML"
    ],
    "abstract": "Conformal Prediction (CP) allows to perform rigorous uncertainty quantification by constructing a prediction set $C(X)$ satisfying $\\mathbb{P}(Y \\in C(X))\\geq 1-α$ for a user-chosen $α\\in [0,1]$ by relying on calibration data $(X_1,Y_1),...,(X_n,Y_n)$ from $\\mathbb{P}=\\mathbb{P}^{X} \\otimes \\mathbb{P}^{Y|X}$. It is typically implicitly assumed that $\\mathbb{P}^{Y|X}$ is the \"true\" posterior label distribution. However, in many real-world scenarios, the labels $Y_1,...,Y_n$ are obtained by aggreg",
    "arxiv_url": "https://arxiv.org/abs/2307.09302v2",
    "pdf_url": "https://arxiv.org/pdf/2307.09302v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2307.09302",
    "arxiv_authors": [
      "David Stutz",
      "Abhijit Guha Roy",
      "Tatiana Matejovicova",
      "Patricia Strachan",
      "Ali Taylan Cemgil",
      "Arnaud Doucet"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Conformal+prediction+under+ambiguous+ground+truth+David+Stutz+Abhijit+Guha+Roy+Tatiana+Matejovicova+Patricia+Strachan+Ali+Taylan+Cemgil",
    "gs_search_success": true,
    "gs_authors": [
      "X3ZFZ7AAAAAJ",
      "W4SZGV8AAAAJ",
      "TxEy3cwAAAAJ",
      "d9i26joAAAAJ",
      "r2ulM_sAAAAJ"
    ],
    "citation_count": 29,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2303.08670",
    "title": "Deep Visual Forced Alignment: Learning to Align Transcription with Talking Face Video",
    "year": 2023,
    "published": "2023-02-27T02:59:50Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.SD",
      "eess.AS"
    ],
    "abstract": "Forced alignment refers to a technology that time-aligns a given transcription with a corresponding speech. However, as the forced alignment technologies have developed using speech audio, they might fail in alignment when the input speech audio is noise-corrupted or is not accessible. We focus on that there is another component that the speech can be inferred from, the speech video (i.e., talking face video). Since the drawbacks of audio-based forced alignment can be complemented using the visu",
    "arxiv_url": "https://arxiv.org/abs/2303.08670v1",
    "pdf_url": "https://arxiv.org/pdf/2303.08670v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2303.08670",
    "arxiv_authors": [
      "Minsu Kim",
      "Chae Won Kim",
      "Yong Man Ro"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Deep+Visual+Forced+Alignment%3A+Learning+to+Align+Transcription+with+Talking+Face+Video+Minsu+Kim+Chae+Won+Kim+Yong+Man+Ro",
    "gs_search_success": true,
    "gs_authors": [
      "TXB0FyoAAAAJ",
      "IPzfF7cAAAAJ",
      "_eh1uCMAAAAJ"
    ],
    "citation_count": 6,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2405.14171",
    "title": "Multi-view Remote Sensing Image Segmentation With SAM priors",
    "year": 2024,
    "published": "2024-05-23T04:57:41Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Multi-view segmentation in Remote Sensing (RS) seeks to segment images from diverse perspectives within a scene. Recent methods leverage 3D information extracted from an Implicit Neural Field (INF), bolstering result consistency across multiple views while using limited accounts of labels (even within 3-5 labels) to streamline labor. Nonetheless, achieving superior performance within the constraints of limited-view labels remains challenging due to inadequate scene-wide supervision and insuffici",
    "arxiv_url": "https://arxiv.org/abs/2405.14171v1",
    "pdf_url": "https://arxiv.org/pdf/2405.14171v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2405.14171",
    "arxiv_authors": [
      "Zipeng Qi",
      "Chenyang Liu",
      "Zili Liu",
      "Hao Chen",
      "Yongchang Wu",
      "Zhengxia Zou",
      "Zhenwei Sh"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Multi-view+Remote+Sensing+Image+Segmentation+With+SAM+priors+Zipeng+Qi+Chenyang+Liu+Zili+Liu+Hao+Chen+Yongchang+Wu",
    "gs_search_success": true,
    "gs_authors": [
      "BEDNoZIAAAAJ",
      "kNhFWQIAAAAJ",
      "KMEs58kAAAAJ",
      "jBnA45cAAAAJ",
      "6d0Pic4AAAAJ",
      "DzwoyZsAAAAJ"
    ],
    "citation_count": 9,
    "gs_author_count": 7
  },
  {
    "arxiv_id": "2404.00252",
    "title": "Learned Scanpaths Aid Blind Panoramic Video Quality Assessment",
    "year": 2024,
    "published": "2024-03-30T05:42:17Z",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "abstract": "Panoramic videos have the advantage of providing an immersive and interactive viewing experience. Nevertheless, their spherical nature gives rise to various and uncertain user viewing behaviors, which poses significant challenges for panoramic video quality assessment (PVQA). In this work, we propose an end-to-end optimized, blind PVQA method with explicit modeling of user viewing patterns through visual scanpaths. Our method consists of two modules: a scanpath generator and a quality assessor. ",
    "arxiv_url": "https://arxiv.org/abs/2404.00252v2",
    "pdf_url": "https://arxiv.org/pdf/2404.00252v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2404.00252",
    "arxiv_authors": [
      "Kanglong Fan",
      "Wen Wen",
      "Mu Li",
      "Yifan Peng",
      "Kede Ma"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Learned+Scanpaths+Aid+Blind+Panoramic+Video+Quality+Assessment+Kanglong+Fan+Wen+Wen+Mu+Li+Yifan+Peng+Kede+Ma",
    "gs_search_success": true,
    "gs_authors": [
      "qKfskogAAAAJ",
      "wYqdizQAAAAJ",
      "sfzOyFoAAAAJ",
      "UMveGGwAAAAJ",
      "H-62ab0AAAAJ"
    ],
    "citation_count": 5,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2408.17095",
    "title": "RISSOLE: Parameter-efficient Diffusion Models via Block-wise Generation and Retrieval-Guidance",
    "year": 2024,
    "published": "2024-08-30T08:26:55Z",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "Diffusion-based models demonstrate impressive generation capabilities. However, they also have a massive number of parameters, resulting in enormous model sizes, thus making them unsuitable for deployment on resource-constraint devices. Block-wise generation can be a promising alternative for designing compact-sized (parameter-efficient) deep generative models since the model can generate one block at a time instead of generating the whole image at once. However, block-wise generation is also co",
    "arxiv_url": "https://arxiv.org/abs/2408.17095v2",
    "pdf_url": "https://arxiv.org/pdf/2408.17095v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2408.17095",
    "arxiv_authors": [
      "Avideep Mukherjee",
      "Soumya Banerjee",
      "Piyush Rai",
      "Vinay P. Namboodiri"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=RISSOLE%3A+Parameter-efficient+Diffusion+Models+via+Block-wise+Generation+and+Retrieval-Guidance+Avideep+Mukherjee+Soumya+Banerjee+Piyush+Rai+Vinay+P.+Namboodiri",
    "gs_search_success": true,
    "gs_authors": [
      "kvAHwoYAAAAJ",
      "D50grEgAAAAJ",
      "p7BoMpMAAAAJ",
      "JyHi9OoAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2305.10217",
    "title": "Deep Learning Applications Based on WISE Infrared Data: Classification of Stars, Galaxies and Quasars",
    "year": 2023,
    "published": "2023-05-17T13:50:33Z",
    "categories": [
      "astro-ph.IM",
      "cs.CV"
    ],
    "abstract": "The Wide-field Infrared Survey Explorer (WISE) has detected hundreds of millions of sources over the entire sky. However, classifying them reliably is a great challenge due to degeneracies in WISE multicolor space and low detection levels in its two longest-wavelength bandpasses. In this paper, the deep learning classification network, IICnet (Infrared Image Classification network), is designed to classify sources from WISE images to achieve a more accurate classification goal. IICnet shows good",
    "arxiv_url": "https://arxiv.org/abs/2305.10217v1",
    "pdf_url": "https://arxiv.org/pdf/2305.10217v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2305.10217",
    "arxiv_authors": [
      "Guiyu Zhao",
      "Bo Qiu",
      "A-Li Luo",
      "Xiaoyu Guo",
      "Lin Yao",
      "Kun Wang",
      "Yuanbo Liu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Deep+Learning+Applications+Based+on+WISE+Infrared+Data%3A+Classification+of+Stars%2C+Galaxies+and+Quasars+Guiyu+Zhao+Bo+Qiu+A-Li+Luo+Xiaoyu+Guo+Lin+Yao",
    "gs_search_success": true,
    "gs_authors": [
      "bunftYsAAAAJ",
      "fnpaWMAAAAAJ"
    ],
    "citation_count": 3,
    "gs_author_count": 7
  },
  {
    "arxiv_id": "2312.00761",
    "title": "Deep Unlearning: Fast and Efficient Gradient-free Approach to Class Forgetting",
    "year": 2023,
    "published": "2023-12-01T18:29:08Z",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "stat.ML"
    ],
    "abstract": "Machine unlearning is a prominent and challenging field, driven by regulatory demands for user data deletion and heightened privacy awareness. Existing approaches involve retraining model or multiple finetuning steps for each deletion request, often constrained by computational limits and restricted data access. In this work, we introduce a novel class unlearning algorithm designed to strategically eliminate specific classes from the learned model. Our algorithm first estimates the Retain and th",
    "arxiv_url": "https://arxiv.org/abs/2312.00761v4",
    "pdf_url": "https://arxiv.org/pdf/2312.00761v4",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2312.00761",
    "arxiv_authors": [
      "Sangamesh Kodge",
      "Gobinda Saha",
      "Kaushik Roy"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Deep+Unlearning%3A+Fast+and+Efficient+Gradient-free+Approach+to+Class+Forgetting+Sangamesh+Kodge+Gobinda+Saha+Kaushik+Roy",
    "gs_search_success": true,
    "gs_authors": [
      "Y7I-7EsAAAAJ",
      "to4P8KgAAAAJ",
      "Qyet-JUAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2406.01210",
    "title": "GeminiFusion: Efficient Pixel-wise Multimodal Fusion for Vision Transformer",
    "year": 2024,
    "published": "2024-06-03T11:24:15Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Cross-modal transformers have demonstrated superiority in various vision tasks by effectively integrating different modalities. This paper first critiques prior token exchange methods which replace less informative tokens with inter-modal features, and demonstrate exchange based methods underperform cross-attention mechanisms, while the computational demand of the latter inevitably restricts its use with longer sequences. To surmount the computational challenges, we propose GeminiFusion, a pixel",
    "arxiv_url": "https://arxiv.org/abs/2406.01210v2",
    "pdf_url": "https://arxiv.org/pdf/2406.01210v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2406.01210",
    "arxiv_authors": [
      "Ding Jia",
      "Jianyuan Guo",
      "Kai Han",
      "Han Wu",
      "Chao Zhang",
      "Chang Xu",
      "Xinghao Chen"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=GeminiFusion%3A+Efficient+Pixel-wise+Multimodal+Fusion+for+Vision+Transformer+Ding+Jia+Jianyuan+Guo+Kai+Han+Han+Wu+Chao+Zhang",
    "gs_search_success": true,
    "gs_authors": [
      "UnAbd4gAAAAJ",
      "46ZKFNUAAAAJ",
      "NeCCx-kAAAAJ",
      "k8XBjmIAAAAJ",
      "tuGWUVIAAAAJ",
      "vThoBVcAAAAJ",
      "N4F_3eoAAAAJ"
    ],
    "citation_count": 54,
    "gs_author_count": 7
  },
  {
    "arxiv_id": "2311.16581",
    "title": "GeoScaler: Geometry and Rendering-Aware Downsampling of 3D Mesh Textures",
    "year": 2023,
    "published": "2023-11-28T07:55:25Z",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "abstract": "High-resolution texture maps are necessary for representing real-world objects accurately with 3D meshes. The large sizes of textures can bottleneck the real-time rendering of high-quality virtual 3D scenes on devices having low computational budgets and limited memory. Downsampling the texture maps directly addresses the issue, albeit at the cost of visual fidelity. Traditionally, downsampling of texture maps is performed using methods like bicubic interpolation and the Lanczos algorithm. These",
    "arxiv_url": "https://arxiv.org/abs/2311.16581v2",
    "pdf_url": "https://arxiv.org/pdf/2311.16581v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2311.16581",
    "arxiv_authors": [
      "Sai Karthikey Pentapati",
      "Anshul Rai",
      "Arkady Ten",
      "Chaitanya Atluru",
      "Alan Bovik"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=GeoScaler%3A+Geometry+and+Rendering-Aware+Downsampling+of+3D+Mesh+Textures+Sai+Karthikey+Pentapati+Anshul+Rai+Arkady+Ten+Chaitanya+Atluru+Alan+Bovik",
    "gs_search_success": false,
    "gs_authors": [],
    "citation_count": null,
    "gs_author_count": null,
    "error_type": "fetch_failed"
  },
  {
    "arxiv_id": "2307.05663",
    "title": "Objaverse-XL: A Universe of 10M+ 3D Objects",
    "year": 2023,
    "published": "2023-07-11T17:57:40Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Natural language processing and 2D vision models have attained remarkable proficiency on many tasks primarily by escalating the scale of training data. However, 3D vision tasks have not seen the same progress, in part due to the challenges of acquiring high-quality 3D data. In this work, we present Objaverse-XL, a dataset of over 10 million 3D objects. Our dataset comprises deduplicated 3D objects from a diverse set of sources, including manually designed objects, photogrammetry scans of landmar",
    "arxiv_url": "https://arxiv.org/abs/2307.05663v1",
    "pdf_url": "https://arxiv.org/pdf/2307.05663v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2307.05663",
    "arxiv_authors": [
      "Matt Deitke",
      "Ruoshi Liu",
      "Matthew Wallingford",
      "Huong Ngo",
      "Oscar Michel",
      "Aditya Kusupati",
      "Alan Fan",
      "Christian Laforte",
      "Vikram Voleti",
      "Samir Yitzhak Gadre",
      "Eli VanderBilt",
      "Aniruddha Kembhavi",
      "Carl Vondrick",
      "Georgia Gkioxari",
      "Kiana Ehsani",
      "Ludwig Schmidt",
      "Ali Farhadi"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Objaverse-XL%3A+A+Universe+of+10M%2B+3D+Objects+Matt+Deitke+Ruoshi+Liu+Matthew+Wallingford+Huong+Ngo+Oscar+Michel",
    "gs_search_success": true,
    "gs_authors": [
      "vSPbuZQAAAAJ",
      "k4VxCcYAAAAJ",
      "qULx8g8AAAAJ",
      "PPCRqZUAAAAJ",
      "zczXAk8AAAAJ",
      "whX3iA8AAAAJ",
      "fZ3EWukAAAAJ",
      "suAawHYAAAAJ"
    ],
    "citation_count": 623,
    "gs_author_count": 9
  },
  {
    "arxiv_id": "2309.13956",
    "title": "In-Domain GAN Inversion for Faithful Reconstruction and Editability",
    "year": 2023,
    "published": "2023-09-25T08:42:06Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Generative Adversarial Networks (GANs) have significantly advanced image synthesis through mapping randomly sampled latent codes to high-fidelity synthesized images. However, applying well-trained GANs to real image editing remains challenging. A common solution is to find an approximate latent code that can adequately recover the input image to edit, which is also known as GAN inversion. To invert a GAN model, prior works typically focus on reconstructing the target image at the pixel level, ye",
    "arxiv_url": "https://arxiv.org/abs/2309.13956v1",
    "pdf_url": "https://arxiv.org/pdf/2309.13956v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2309.13956",
    "arxiv_authors": [
      "Jiapeng Zhu",
      "Yujun Shen",
      "Yinghao Xu",
      "Deli Zhao",
      "Qifeng Chen",
      "Bolei Zhou"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=In-Domain+GAN+Inversion+for+Faithful+Reconstruction+and+Editability+Jiapeng+Zhu+Yujun+Shen+Yinghao+Xu+Deli+Zhao+Qifeng+Chen",
    "gs_search_success": true,
    "gs_authors": [
      "9D4aG8AAAAAJ",
      "-ACBm-gAAAAJ",
      "lLMX9hcAAAAJ",
      "7LhjCn0AAAAJ",
      "I_7PXKEAAAAJ",
      "u76xfogAAAAJ"
    ],
    "citation_count": 12,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2503.12834",
    "title": "PASTA: Part-Aware Sketch-to-3D Shape Generation with Text-Aligned Prior",
    "year": 2025,
    "published": "2025-03-17T05:31:09Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "A fundamental challenge in conditional 3D shape generation is to minimize the information loss and maximize the intention of user input. Existing approaches have predominantly focused on two types of isolated conditional signals, i.e., user sketches and text descriptions, each of which does not offer flexible control of the generated shape. In this paper, we introduce PASTA, the flexible approach that seamlessly integrates a user sketch and a text description for 3D shape generation. The key ide",
    "arxiv_url": "https://arxiv.org/abs/2503.12834v1",
    "pdf_url": "https://arxiv.org/pdf/2503.12834v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2503.12834",
    "arxiv_authors": [
      "Seunggwan Lee",
      "Hwanhee Jung",
      "Byoungsoo Koh",
      "Qixing Huang",
      "Sangho Yoon",
      "Sangpil Kim"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=PASTA%3A+Part-Aware+Sketch-to-3D+Shape+Generation+with+Text-Aligned+Prior+Seunggwan+Lee+Hwanhee+Jung+Byoungsoo+Koh+Qixing+Huang+Sangho+Yoon",
    "gs_search_success": true,
    "gs_authors": [
      "mzH6yYgAAAAJ",
      "cOln5T0AAAAJ",
      "pamL_rIAAAAJ",
      "Qq_qUZ4AAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2504.01028",
    "title": "Improving Applicability of Deep Learning based Token Classification models during Training",
    "year": 2025,
    "published": "2025-03-28T17:01:19Z",
    "categories": [
      "cs.CV",
      "cs.CL",
      "cs.IR"
    ],
    "abstract": "This paper shows that further evaluation metrics during model training are needed to decide about its applicability in inference. As an example, a LayoutLM-based model is trained for token classification in documents. The documents are German receipts. We show that conventional classification metrics, represented by the F1-Score in our experiments, are insufficient for evaluating the applicability of machine learning models in practice. To address this problem, we introduce a novel metric, Docum",
    "arxiv_url": "https://arxiv.org/abs/2504.01028v1",
    "pdf_url": "https://arxiv.org/pdf/2504.01028v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2504.01028",
    "arxiv_authors": [
      "Anket Mehra",
      "Malte Prieß",
      "Marian Himstedt"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Improving+Applicability+of+Deep+Learning+based+Token+Classification+models+during+Training+Anket+Mehra+Malte+Prie%C3%9F+Marian+Himstedt",
    "gs_search_success": true,
    "gs_authors": [
      "JlEfY2IAAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2410.17772",
    "title": "Scaling Robot Policy Learning via Zero-Shot Labeling with Foundation Models",
    "year": 2024,
    "published": "2024-10-23T11:19:48Z",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "A central challenge towards developing robots that can relate human language to their perception and actions is the scarcity of natural language annotations in diverse robot datasets. Moreover, robot policies that follow natural language instructions are typically trained on either templated language or expensive human-labeled instructions, hindering their scalability. To this end, we introduce NILS: Natural language Instruction Labeling for Scalability. NILS automatically labels uncurated, long",
    "arxiv_url": "https://arxiv.org/abs/2410.17772v2",
    "pdf_url": "https://arxiv.org/pdf/2410.17772v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2410.17772",
    "arxiv_authors": [
      "Nils Blank",
      "Moritz Reuss",
      "Marcel Rühle",
      "Ömer Erdinç Yağmurlu",
      "Fabian Wenzel",
      "Oier Mees",
      "Rudolf Lioutikov"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Scaling+Robot+Policy+Learning+via+Zero-Shot+Labeling+with+Foundation+Models+Nils+Blank+Moritz+Reuss+Marcel+R%C3%BChle+%C3%96mer+Erdin%C3%A7+Ya%C4%9Fmurlu+Fabian+Wenzel",
    "gs_search_success": true,
    "gs_authors": [
      "sgsLkM0AAAAJ",
      "DUd7J0YAAAAJ",
      "hvjV43MAAAAJ",
      "I_Mxp5cAAAAJ",
      "NLuzkPIAAAAJ"
    ],
    "citation_count": 18,
    "gs_author_count": 7
  },
  {
    "arxiv_id": "2305.17559",
    "title": "Pruning at Initialization -- A Sketching Perspective",
    "year": 2023,
    "published": "2023-05-27T19:22:25Z",
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "abstract": "The lottery ticket hypothesis (LTH) has increased attention to pruning neural networks at initialization. We study this problem in the linear setting. We show that finding a sparse mask at initialization is equivalent to the sketching problem introduced for efficient matrix multiplication. This gives us tools to analyze the LTH problem and gain insights into it. Specifically, using the mask found at initialization, we bound the approximation error of the pruned linear model at the end of trainin",
    "arxiv_url": "https://arxiv.org/abs/2305.17559v2",
    "pdf_url": "https://arxiv.org/pdf/2305.17559v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2305.17559",
    "arxiv_authors": [
      "Noga Bar",
      "Raja Giryes"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Pruning+at+Initialization+--+A+Sketching+Perspective+Noga+Bar+Raja+Giryes",
    "gs_search_success": true,
    "gs_authors": [
      "9aQUYVQAAAAJ",
      "sHC73-MAAAAJ"
    ],
    "citation_count": 2,
    "gs_author_count": 2
  },
  {
    "arxiv_id": "2310.02528",
    "title": "On the Cognition of Visual Question Answering Models and Human Intelligence: A Comparative Study",
    "year": 2023,
    "published": "2023-10-04T02:06:48Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Visual Question Answering (VQA) is a challenging task that requires cross-modal understanding and reasoning of visual image and natural language question. To inspect the association of VQA models to human cognition, we designed a survey to record human thinking process and analyzed VQA models by comparing the outputs and attention maps with those of humans. We found that although the VQA models resemble human cognition in architecture and performs similarly with human on the recognition-level, t",
    "arxiv_url": "https://arxiv.org/abs/2310.02528v1",
    "pdf_url": "https://arxiv.org/pdf/2310.02528v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2310.02528",
    "arxiv_authors": [
      "Liben Chen",
      "Long Chen",
      "Tian Ellison-Chen",
      "Zhuoyuan Xu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=On+the+Cognition+of+Visual+Question+Answering+Models+and+Human+Intelligence%3A+A+Comparative+Study+Liben+Chen+Long+Chen+Tian+Ellison-Chen+Zhuoyuan+Xu",
    "gs_search_success": true,
    "gs_authors": [
      "UB7WWCwAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2305.00627",
    "title": "CNN-based fully automatic mitral valve extraction using CT images and existence probability maps",
    "year": 2023,
    "published": "2023-05-01T02:20:29Z",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "abstract": "Accurate extraction of mitral valve shape from clinical tomographic images acquired in patients has proven useful for planning surgical and interventional mitral valve treatments. However, manual extraction of the mitral valve shape is laborious, and the existing automatic extraction methods have not been sufficiently accurate. In this paper, we propose a fully automated method of extracting mitral valve shape from computed tomography (CT) images for the all phases of the cardiac cycle. This met",
    "arxiv_url": "https://arxiv.org/abs/2305.00627v2",
    "pdf_url": "https://arxiv.org/pdf/2305.00627v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2305.00627",
    "arxiv_authors": [
      "Yukiteru Masuda",
      "Ryo Ishikawa",
      "Toru Tanaka",
      "Gakuto Aoyama",
      "Keitaro Kawashima",
      "James V. Chapman",
      "Masahiko Asami",
      "Michael Huy Cuong Pham",
      "Klaus Fuglsang Kofoed",
      "Takuya Sakaguchi",
      "Kiyohide Satoh"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=CNN-based+fully+automatic+mitral+valve+extraction+using+CT+images+and+existence+probability+maps+Yukiteru+Masuda+Ryo+Ishikawa+Toru+Tanaka+Gakuto+Aoyama+Keitaro+Kawashima",
    "gs_search_success": true,
    "gs_authors": [],
    "citation_count": 9,
    "gs_author_count": 7
  },
  {
    "arxiv_id": "2311.16483",
    "title": "ChartLlama: A Multimodal LLM for Chart Understanding and Generation",
    "year": 2023,
    "published": "2023-11-27T15:20:23Z",
    "categories": [
      "cs.CV",
      "cs.CL"
    ],
    "abstract": "Multi-modal large language models have demonstrated impressive performances on most vision-language tasks. However, the model generally lacks the understanding capabilities for specific domain data, particularly when it comes to interpreting chart figures. This is mainly due to the lack of relevant multi-modal instruction tuning datasets. In this article, we create a high-quality instruction-tuning dataset leveraging GPT-4. We develop a multi-step data generation process in which different steps",
    "arxiv_url": "https://arxiv.org/abs/2311.16483v1",
    "pdf_url": "https://arxiv.org/pdf/2311.16483v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2311.16483",
    "arxiv_authors": [
      "Yucheng Han",
      "Chi Zhang",
      "Xin Chen",
      "Xu Yang",
      "Zhibin Wang",
      "Gang Yu",
      "Bin Fu",
      "Hanwang Zhang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=ChartLlama%3A+A+Multimodal+LLM+for+Chart+Understanding+and+Generation+Yucheng+Han+Chi+Zhang+Xin+Chen+Xu+Yang+Zhibin+Wang",
    "gs_search_success": true,
    "gs_authors": [
      "bvPDWO4AAAAJ",
      "7qeAJZ4AAAAJ",
      "LbwqJBQAAAAJ",
      "BJdigYsAAAAJ",
      "YG0DFyYAAAAJ",
      "SqdxMH0AAAAJ"
    ],
    "citation_count": 199,
    "gs_author_count": 8
  },
  {
    "arxiv_id": "2504.11470",
    "title": "SO-DETR: Leveraging Dual-Domain Features and Knowledge Distillation for Small Object Detection",
    "year": 2025,
    "published": "2025-04-11T13:47:37Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Detection Transformer-based methods have achieved significant advancements in general object detection. However, challenges remain in effectively detecting small objects. One key difficulty is that existing encoders struggle to efficiently fuse low-level features. Additionally, the query selection strategies are not effectively tailored for small objects. To address these challenges, this paper proposes an efficient model, Small Object Detection Transformer (SO-DETR). The model comprises three k",
    "arxiv_url": "https://arxiv.org/abs/2504.11470v1",
    "pdf_url": "https://arxiv.org/pdf/2504.11470v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2504.11470",
    "arxiv_authors": [
      "Huaxiang Zhang",
      "Hao Zhang",
      "Aoran Mei",
      "Zhongxue Gan",
      "Guo-Niu Zhu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=SO-DETR%3A+Leveraging+Dual-Domain+Features+and+Knowledge+Distillation+for+Small+Object+Detection+Huaxiang+Zhang+Hao+Zhang+Aoran+Mei+Zhongxue+Gan+Guo-Niu+Zhu",
    "gs_search_success": true,
    "gs_authors": [
      "5b4lCXkAAAAJ",
      "L0r5eC8AAAAJ"
    ],
    "citation_count": 8,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2401.13627",
    "title": "Scaling Up to Excellence: Practicing Model Scaling for Photo-Realistic Image Restoration In the Wild",
    "year": 2024,
    "published": "2024-01-24T17:58:07Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "We introduce SUPIR (Scaling-UP Image Restoration), a groundbreaking image restoration method that harnesses generative prior and the power of model scaling up. Leveraging multi-modal techniques and advanced generative prior, SUPIR marks a significant advance in intelligent and realistic image restoration. As a pivotal catalyst within SUPIR, model scaling dramatically enhances its capabilities and demonstrates new potential for image restoration. We collect a dataset comprising 20 million high-re",
    "arxiv_url": "https://arxiv.org/abs/2401.13627v2",
    "pdf_url": "https://arxiv.org/pdf/2401.13627v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2401.13627",
    "arxiv_authors": [
      "Fanghua Yu",
      "Jinjin Gu",
      "Zheyuan Li",
      "Jinfan Hu",
      "Xiangtao Kong",
      "Xintao Wang",
      "Jingwen He",
      "Yu Qiao",
      "Chao Dong"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Scaling+Up+to+Excellence%3A+Practicing+Model+Scaling+for+Photo-Realistic+Image+Restoration+In+the+Wild+Fanghua+Yu+Jinjin+Gu+Zheyuan+Li+Jinfan+Hu+Xiangtao+Kong",
    "gs_search_success": true,
    "gs_authors": [
      "cYPIz6EAAAAJ",
      "FQgZpQoAAAAJ",
      "hT-EiJEAAAAJ",
      "lueNzSgAAAAJ",
      "GUxrycUAAAAJ",
      "OSDCB0UAAAAJ",
      "uMQ-G-QAAAAJ",
      "gFtI-8QAAAAJ"
    ],
    "citation_count": 99,
    "gs_author_count": 9
  },
  {
    "arxiv_id": "2410.08218",
    "title": "A Visual-Analytical Approach for Automatic Detection of Cyclonic Events in Satellite Observations",
    "year": 2024,
    "published": "2024-09-25T14:52:04Z",
    "categories": [
      "eess.IV",
      "cs.CV",
      "cs.LG",
      "physics.ao-ph"
    ],
    "abstract": "Estimating the location and intensity of tropical cyclones holds crucial significance for predicting catastrophic weather events. In this study, we approach this task as a detection and regression challenge, specifically over the North Indian Ocean (NIO) region where best tracks location and wind speed information serve as the labels. The current process for cyclone detection and intensity estimation involves physics-based simulation studies which are time-consuming, only using image features wi",
    "arxiv_url": "https://arxiv.org/abs/2410.08218v1",
    "pdf_url": "https://arxiv.org/pdf/2410.08218v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2410.08218",
    "arxiv_authors": [
      "Akash Agrawal",
      "Mayesh Mohapatra",
      "Abhinav Raja",
      "Paritosh Tiwari",
      "Vishwajeet Pattanaik",
      "Neeru Jaiswal",
      "Arpit Agarwal",
      "Punit Rathore"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=A+Visual-Analytical+Approach+for+Automatic+Detection+of+Cyclonic+Events+in+Satellite+Observations+Akash+Agrawal+Mayesh+Mohapatra+Abhinav+Raja+Paritosh+Tiwari+Vishwajeet+Pattanaik",
    "gs_search_success": true,
    "gs_authors": [
      "IXu29XIAAAAJ",
      "J5OFv5EAAAAJ",
      "2IRfuAcAAAAJ",
      "2clQgooAAAAJ",
      "HlDP3CcAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 8
  },
  {
    "arxiv_id": "2407.03919",
    "title": "MedRAT: Unpaired Medical Report Generation via Auxiliary Tasks",
    "year": 2024,
    "published": "2024-07-04T13:31:47Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Medical report generation from X-ray images is a challenging task, particularly in an unpaired setting where paired image-report data is unavailable for training. To address this challenge, we propose a novel model that leverages the available information in two distinct datasets, one comprising reports and the other consisting of images. The core idea of our model revolves around the notion that combining auto-encoding report generation with multi-modal (report-image) alignment can offer a solu",
    "arxiv_url": "https://arxiv.org/abs/2407.03919v2",
    "pdf_url": "https://arxiv.org/pdf/2407.03919v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2407.03919",
    "arxiv_authors": [
      "Elad Hirsch",
      "Gefen Dawidowicz",
      "Ayellet Tal"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=MedRAT%3A+Unpaired+Medical+Report+Generation+via+Auxiliary+Tasks+Elad+Hirsch+Gefen+Dawidowicz+Ayellet+Tal",
    "gs_search_success": true,
    "gs_authors": [
      "eFGgX-QAAAAJ",
      "nssQHPIAAAAJ",
      "L_V2Q9wAAAAJ"
    ],
    "citation_count": 7,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2310.18933",
    "title": "Label Poisoning is All You Need",
    "year": 2023,
    "published": "2023-10-29T08:03:45Z",
    "categories": [
      "cs.LG",
      "cs.CR",
      "cs.CV"
    ],
    "abstract": "In a backdoor attack, an adversary injects corrupted data into a model's training dataset in order to gain control over its predictions on images with a specific attacker-defined trigger. A typical corrupted training example requires altering both the image, by applying the trigger, and the label. Models trained on clean images, therefore, were considered safe from backdoor attacks. However, in some common machine learning scenarios, the training labels are provided by potentially malicious thir",
    "arxiv_url": "https://arxiv.org/abs/2310.18933v1",
    "pdf_url": "https://arxiv.org/pdf/2310.18933v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2310.18933",
    "arxiv_authors": [
      "Rishi D. Jha",
      "Jonathan Hayase",
      "Sewoong Oh"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Label+Poisoning+is+All+You+Need+Rishi+D.+Jha+Jonathan+Hayase+Sewoong+Oh",
    "gs_search_success": true,
    "gs_authors": [
      "Zw-l1d8AAAAJ",
      "v8oRh6YAAAAJ",
      "55TAOdgAAAAJ"
    ],
    "citation_count": 61,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2505.20834",
    "title": "Fully Spiking Neural Networks for Unified Frame-Event Object Tracking",
    "year": 2025,
    "published": "2025-05-27T07:53:50Z",
    "categories": [
      "cs.CV",
      "cs.NE"
    ],
    "abstract": "The integration of image and event streams offers a promising approach for achieving robust visual object tracking in complex environments. However, current fusion methods achieve high performance at the cost of significant computational overhead and struggle to efficiently extract the sparse, asynchronous information from event streams, failing to leverage the energy-efficient advantages of event-driven spiking paradigms. To address this challenge, we propose the first fully Spiking Frame-Event",
    "arxiv_url": "https://arxiv.org/abs/2505.20834v2",
    "pdf_url": "https://arxiv.org/pdf/2505.20834v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2505.20834",
    "arxiv_authors": [
      "Jingjun Yang",
      "Liangwei Fan",
      "Jinpu Zhang",
      "Xiangkai Lian",
      "Hui Shen",
      "Dewen Hu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Fully+Spiking+Neural+Networks+for+Unified+Frame-Event+Object+Tracking+Jingjun+Yang+Liangwei+Fan+Jinpu+Zhang+Xiangkai+Lian+Hui+Shen",
    "gs_search_success": true,
    "gs_authors": [
      "jhUjJ5sAAAAJ",
      "SddRsowAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2505.05467",
    "title": "StreamBridge: Turning Your Offline Video Large Language Model into a Proactive Streaming Assistant",
    "year": 2025,
    "published": "2025-05-08T17:57:40Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "abstract": "We present StreamBridge, a simple yet effective framework that seamlessly transforms offline Video-LLMs into streaming-capable models. It addresses two fundamental challenges in adapting existing models into online scenarios: (1) limited capability for multi-turn real-time understanding, and (2) lack of proactive response mechanisms. Specifically, StreamBridge incorporates (1) a memory buffer combined with a round-decayed compression strategy, supporting long-context multi-turn interactions, and",
    "arxiv_url": "https://arxiv.org/abs/2505.05467v2",
    "pdf_url": "https://arxiv.org/pdf/2505.05467v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2505.05467",
    "arxiv_authors": [
      "Haibo Wang",
      "Bo Feng",
      "Zhengfeng Lai",
      "Mingze Xu",
      "Shiyu Li",
      "Weifeng Ge",
      "Afshin Dehghan",
      "Meng Cao",
      "Ping Huang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=StreamBridge%3A+Turning+Your+Offline+Video+Large+Language+Model+into+a+Proactive+Streaming+Assistant+Haibo+Wang+Bo+Feng+Zhengfeng+Lai+Mingze+Xu+Shiyu+Li",
    "gs_search_success": true,
    "gs_authors": [
      "wcX-UW4AAAAJ",
      "kF5Gci4AAAAJ",
      "vURT920AAAAJ",
      "yVz1dzwAAAAJ",
      "KNcECJQAAAAJ",
      "s_Ws1uYAAAAJ"
    ],
    "citation_count": 4,
    "gs_author_count": 9
  },
  {
    "arxiv_id": "2305.11125",
    "title": "Skin Lesion Diagnosis Using Convolutional Neural Networks",
    "year": 2023,
    "published": "2023-05-18T17:15:08Z",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "abstract": "Cancerous skin lesions are one of the most common malignancies detected in humans, and if not detected at an early stage, they can lead to death. Therefore, it is crucial to have access to accurate results early on to optimize the chances of survival. Unfortunately, accurate results are typically obtained by highly trained dermatologists, who may not be accessible to many people, particularly in low-income and middle-income countries. Artificial Intelligence (AI) appears to be a potential soluti",
    "arxiv_url": "https://arxiv.org/abs/2305.11125v1",
    "pdf_url": "https://arxiv.org/pdf/2305.11125v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2305.11125",
    "arxiv_authors": [
      "Daniel Alonso Villanueva Nunez",
      "Yongmin Li"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Skin+Lesion+Diagnosis+Using+Convolutional+Neural+Networks+Daniel+Alonso+Villanueva+Nunez+Yongmin+Li",
    "gs_search_success": true,
    "gs_authors": [
      "R5dedx0AAAAJ"
    ],
    "citation_count": 4,
    "gs_author_count": null
  },
  {
    "arxiv_id": "2408.05211",
    "title": "VITA: Towards Open-Source Interactive Omni Multimodal LLM",
    "year": 2024,
    "published": "2024-08-09T17:59:49Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "abstract": "The remarkable multimodal capabilities and interactive experience of GPT-4o underscore their necessity in practical applications, yet open-source models rarely excel in both areas. In this paper, we introduce VITA, the first-ever open-source Multimodal Large Language Model (MLLM) adept at simultaneous processing and analysis of Video, Image, Text, and Audio modalities, and meanwhile has an advanced multimodal interactive experience. Starting from Mixtral 8x7B as a language foundation, we expand ",
    "arxiv_url": "https://arxiv.org/abs/2408.05211v3",
    "pdf_url": "https://arxiv.org/pdf/2408.05211v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2408.05211",
    "arxiv_authors": [
      "Chaoyou Fu",
      "Haojia Lin",
      "Zuwei Long",
      "Yunhang Shen",
      "Yuhang Dai",
      "Meng Zhao",
      "Yi-Fan Zhang",
      "Shaoqi Dong",
      "Yangze Li",
      "Xiong Wang",
      "Haoyu Cao",
      "Di Yin",
      "Long Ma",
      "Xiawu Zheng",
      "Rongrong Ji",
      "Yunsheng Wu",
      "Ran He",
      "Caifeng Shan",
      "Xing Sun"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=VITA%3A+Towards+Open-Source+Interactive+Omni+Multimodal+LLM+Chaoyou+Fu+Haojia+Lin+Zuwei+Long+Yunhang+Shen+Yuhang+Dai",
    "gs_search_success": true,
    "gs_authors": [
      "4A1xYQwAAAAJ",
      "wY7HV2gAAAAJ",
      "lBVUDu0AAAAJ",
      "LV8ejn8AAAAJ",
      "e2wkaxUAAAAJ",
      "ghz3qWQAAAAJ",
      "29teR74AAAAJ",
      "lUnt8X4AAAAJ"
    ],
    "citation_count": 140,
    "gs_author_count": 12
  },
  {
    "arxiv_id": "2505.13617",
    "title": "Direction-Aware Neural Acoustic Fields for Few-Shot Interpolation of Ambisonic Impulse Responses",
    "year": 2025,
    "published": "2025-05-19T18:01:53Z",
    "categories": [
      "eess.AS",
      "cs.AI",
      "cs.CV",
      "cs.LG",
      "cs.SD"
    ],
    "abstract": "The characteristics of a sound field are intrinsically linked to the geometric and spatial properties of the environment surrounding a sound source and a listener. The physics of sound propagation is captured in a time-domain signal known as a room impulse response (RIR). Prior work using neural fields (NFs) has allowed learning spatially-continuous representations of RIRs from finite RIR measurements. However, previous NF-based methods have focused on monaural omnidirectional or at most binaura",
    "arxiv_url": "https://arxiv.org/abs/2505.13617v1",
    "pdf_url": "https://arxiv.org/pdf/2505.13617v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2505.13617",
    "arxiv_authors": [
      "Christopher Ick",
      "Gordon Wichern",
      "Yoshiki Masuyama",
      "François Germain",
      "Jonathan Le Roux"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Direction-Aware+Neural+Acoustic+Fields+for+Few-Shot+Interpolation+of+Ambisonic+Impulse+Responses+Christopher+Ick+Gordon+Wichern+Yoshiki+Masuyama+Fran%C3%A7ois+Germain+Jonathan+Le+Roux",
    "gs_search_success": true,
    "gs_authors": [
      "IM_ioMYAAAAJ",
      "JVkh89YAAAAJ",
      "uMNdvZEAAAAJ",
      "aUpxty8AAAAJ",
      "ykLTvcYAAAAJ"
    ],
    "citation_count": 4,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2406.09315",
    "title": "Vertical LoRA: Dense Expectation-Maximization Interpretation of Transformers",
    "year": 2024,
    "published": "2024-06-13T16:51:33Z",
    "categories": [
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "In this paper, we show how Transformers can be interpreted as dense Expectation-Maximization algorithms performed on Bayesian Nets. Based on the above interpretation, we propose a new model design paradigm, namely Vertical LoRA (VLoRA), which reduces the parameter count dramatically while preserving performance. In VLoRA, a model consists of layers, each of which recursively learns an increment based on the previous layer. We then apply LoRA decomposition to the increments. VLoRA works on the ba",
    "arxiv_url": "https://arxiv.org/abs/2406.09315v1",
    "pdf_url": "https://arxiv.org/pdf/2406.09315v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2406.09315",
    "arxiv_authors": [
      "Zhuolin Fu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Vertical+LoRA%3A+Dense+Expectation-Maximization+Interpretation+of+Transformers+Zhuolin+Fu",
    "gs_search_success": true,
    "gs_authors": [],
    "citation_count": 1,
    "gs_author_count": 1
  },
  {
    "arxiv_id": "2312.03013",
    "title": "Breast Ultrasound Report Generation using LangChain",
    "year": 2023,
    "published": "2023-12-05T00:28:26Z",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "Breast ultrasound (BUS) is a critical diagnostic tool in the field of breast imaging, aiding in the early detection and characterization of breast abnormalities. Interpreting breast ultrasound images commonly involves creating comprehensive medical reports, containing vital information to promptly assess the patient's condition. However, the ultrasound imaging system necessitates capturing multiple images of various parts to compile a single report, presenting a time-consuming challenge. To addr",
    "arxiv_url": "https://arxiv.org/abs/2312.03013v1",
    "pdf_url": "https://arxiv.org/pdf/2312.03013v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2312.03013",
    "arxiv_authors": [
      "Jaeyoung Huh",
      "Hyun Jeong Park",
      "Jong Chul Ye"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Breast+Ultrasound+Report+Generation+using+LangChain+Jaeyoung+Huh+Hyun+Jeong+Park+Jong+Chul+Ye",
    "gs_search_success": true,
    "gs_authors": [
      "HNMjoNEAAAAJ",
      "jT1cYdQAAAAJ"
    ],
    "citation_count": 11,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2305.14334",
    "title": "Diffusion Hyperfeatures: Searching Through Time and Space for Semantic Correspondence",
    "year": 2023,
    "published": "2023-05-23T17:58:05Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Diffusion models have been shown to be capable of generating high-quality images, suggesting that they could contain meaningful internal representations. Unfortunately, the feature maps that encode a diffusion model's internal information are spread not only over layers of the network, but also over diffusion timesteps, making it challenging to extract useful descriptors. We propose Diffusion Hyperfeatures, a framework for consolidating multi-scale and multi-timestep feature maps into per-pixel ",
    "arxiv_url": "https://arxiv.org/abs/2305.14334v2",
    "pdf_url": "https://arxiv.org/pdf/2305.14334v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2305.14334",
    "arxiv_authors": [
      "Grace Luo",
      "Lisa Dunlap",
      "Dong Huk Park",
      "Aleksander Holynski",
      "Trevor Darrell"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Diffusion+Hyperfeatures%3A+Searching+Through+Time+and+Space+for+Semantic+Correspondence+Grace+Luo+Lisa+Dunlap+Dong+Huk+Park+Aleksander+Holynski+Trevor+Darrell",
    "gs_search_success": true,
    "gs_authors": [
      "ypBMJMgAAAAJ",
      "_kJ-zUYAAAAJ",
      "bh-uRFMAAAAJ",
      "j88h_roAAAAJ",
      "Vy16O5UAAAAJ"
    ],
    "citation_count": 201,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2503.20209",
    "title": "BEAR: A Video Dataset For Fine-grained Behaviors Recognition Oriented with Action and Environment Factors",
    "year": 2025,
    "published": "2025-03-26T04:06:20Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Behavior recognition is an important task in video representation learning. An essential aspect pertains to effective feature learning conducive to behavior recognition. Recently, researchers have started to study fine-grained behavior recognition, which provides similar behaviors and encourages the model to concern with more details of behaviors with effective features for distinction. However, previous fine-grained behaviors limited themselves to controlling partial information to be similar, ",
    "arxiv_url": "https://arxiv.org/abs/2503.20209v1",
    "pdf_url": "https://arxiv.org/pdf/2503.20209v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2503.20209",
    "arxiv_authors": [
      "Chengyang Hu",
      "Yuduo Chen",
      "Lizhuang Ma"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=BEAR%3A+A+Video+Dataset+For+Fine-grained+Behaviors+Recognition+Oriented+with+Action+and+Environment+Factors+Chengyang+Hu+Yuduo+Chen+Lizhuang+Ma",
    "gs_search_success": true,
    "gs_authors": [
      "uRtPRPAAAAAJ",
      "yd58y_0AAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2312.11206",
    "title": "QDA$^2$: A principled approach to automatically annotating charge stability diagrams",
    "year": 2023,
    "published": "2023-12-18T13:52:18Z",
    "categories": [
      "cond-mat.mes-hall",
      "cs.CV",
      "quant-ph"
    ],
    "abstract": "Gate-defined semiconductor quantum dot (QD) arrays are a promising platform for quantum computing. However, presently, the large configuration spaces and inherent noise make tuning of QD devices a nontrivial task and with the increasing number of QD qubits, the human-driven experimental control becomes unfeasible. Recently, researchers working with QD systems have begun putting considerable effort into automating device control, with a particular focus on machine-learning-driven methods. Yet, th",
    "arxiv_url": "https://arxiv.org/abs/2312.11206v1",
    "pdf_url": "https://arxiv.org/pdf/2312.11206v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2312.11206",
    "arxiv_authors": [
      "Brian Weber",
      "Justyna P. Zwolak"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=QDA%24%5E2%24%3A+A+principled+approach+to+automatically+annotating+charge+stability+diagrams+Brian+Weber+Justyna+P.+Zwolak",
    "gs_search_success": true,
    "gs_authors": [
      "xnocV3AAAAAJ"
    ],
    "citation_count": 3,
    "gs_author_count": 2
  },
  {
    "arxiv_id": "2406.11262",
    "title": "Generative Visual Instruction Tuning",
    "year": 2024,
    "published": "2024-06-17T07:06:58Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "We propose to use automatically generated instruction-following data to improve the zero-shot capabilities of a large multimodal model with additional support for generative and image editing tasks. We achieve this by curating a new multimodal instruction-following set using GPT-4V and existing datasets for image generation and editing. Using this instruction set and the existing LLaVA-Finetune instruction set for visual understanding tasks, we produce GenLLaVA, a Generative Large Language and V",
    "arxiv_url": "https://arxiv.org/abs/2406.11262v2",
    "pdf_url": "https://arxiv.org/pdf/2406.11262v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2406.11262",
    "arxiv_authors": [
      "Jefferson Hernandez",
      "Ruben Villegas",
      "Vicente Ordonez"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Generative+Visual+Instruction+Tuning+Jefferson+Hernandez+Ruben+Villegas+Vicente+Ordonez",
    "gs_search_success": true,
    "gs_authors": [
      "TtA_j4YAAAAJ",
      "uGDQoU0AAAAJ",
      "Xk6-2C0AAAAJ"
    ],
    "citation_count": 3,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2407.07789",
    "title": "Raising the Ceiling: Conflict-Free Local Feature Matching with Dynamic View Switching",
    "year": 2024,
    "published": "2024-07-10T16:06:32Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Current feature matching methods prioritize improving modeling capabilities to better align outputs with ground-truth matches, which are the theoretical upper bound on matching results, metaphorically depicted as the \"ceiling\". However, these enhancements fail to address the underlying issues that directly hinder ground-truth matches, including the scarcity of matchable points in small scale images, matching conflicts in dense methods, and the keypoint-repeatability reliance in sparse methods. W",
    "arxiv_url": "https://arxiv.org/abs/2407.07789v2",
    "pdf_url": "https://arxiv.org/pdf/2407.07789v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2407.07789",
    "arxiv_authors": [
      "Xiaoyong Lu",
      "Songlin Du"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Raising+the+Ceiling%3A+Conflict-Free+Local+Feature+Matching+with+Dynamic+View+Switching+Xiaoyong+Lu+Songlin+Du",
    "gs_search_success": true,
    "gs_authors": [
      "U6eyWMkAAAAJ"
    ],
    "citation_count": 10,
    "gs_author_count": 2
  },
  {
    "arxiv_id": "2307.02971",
    "title": "On the Cultural Gap in Text-to-Image Generation",
    "year": 2023,
    "published": "2023-07-06T13:17:55Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "abstract": "One challenge in text-to-image (T2I) generation is the inadvertent reflection of culture gaps present in the training data, which signifies the disparity in generated image quality when the cultural elements of the input text are rarely collected in the training set. Although various T2I models have shown impressive but arbitrary examples, there is no benchmark to systematically evaluate a T2I model's ability to generate cross-cultural images. To bridge the gap, we propose a Challenging Cross-Cu",
    "arxiv_url": "https://arxiv.org/abs/2307.02971v1",
    "pdf_url": "https://arxiv.org/pdf/2307.02971v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2307.02971",
    "arxiv_authors": [
      "Bingshuai Liu",
      "Longyue Wang",
      "Chenyang Lyu",
      "Yong Zhang",
      "Jinsong Su",
      "Shuming Shi",
      "Zhaopeng Tu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=On+the+Cultural+Gap+in+Text-to-Image+Generation+Bingshuai+Liu+Longyue+Wang+Chenyang+Lyu+Yong+Zhang+Jinsong+Su",
    "gs_search_success": true,
    "gs_authors": [
      "r1ctChkAAAAJ",
      "0n7cAw0AAAAJ",
      "IvE2zRgAAAAJ",
      "Mfe0St8AAAAJ",
      "Lg31AKMAAAAJ",
      "w6qCk3sAAAAJ"
    ],
    "citation_count": 27,
    "gs_author_count": 7
  },
  {
    "arxiv_id": "2401.15721",
    "title": "A Study of Acquisition Functions for Medical Imaging Deep Active Learning",
    "year": 2024,
    "published": "2024-01-28T18:09:02Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.HC",
      "cs.LG"
    ],
    "abstract": "The Deep Learning revolution has enabled groundbreaking achievements in recent years. From breast cancer detection to protein folding, deep learning algorithms have been at the core of very important advancements. However, these modern advancements are becoming more and more data-hungry, especially on labeled data whose availability is scarce: this is even more prevalent in the medical context. In this work, we show how active learning could be very effective in data scarcity situations, where o",
    "arxiv_url": "https://arxiv.org/abs/2401.15721v2",
    "pdf_url": "https://arxiv.org/pdf/2401.15721v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2401.15721",
    "arxiv_authors": [
      "Bonaventure F. P. Dossou"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=A+Study+of+Acquisition+Functions+for+Medical+Imaging+Deep+Active+Learning+Bonaventure+F.+P.+Dossou",
    "gs_search_success": true,
    "gs_authors": [
      "2J581k0AAAAJ"
    ],
    "citation_count": 5,
    "gs_author_count": 1
  },
  {
    "arxiv_id": "2502.15488",
    "title": "FQ-PETR: Fully Quantized Position Embedding Transformation for Multi-View 3D Object Detection",
    "year": 2025,
    "published": "2025-02-21T14:26:23Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Camera-based multi-view 3D detection is crucial for autonomous driving. PETR and its variants (PETRs) excel in benchmarks but face deployment challenges due to high computational cost and memory footprint. Quantization is an effective technique for compressing deep neural networks by reducing the bit width of weights and activations. However, directly applying existing quantization methods to PETRs leads to severe accuracy degradation. This issue primarily arises from two key challenges: (1) sig",
    "arxiv_url": "https://arxiv.org/abs/2502.15488v4",
    "pdf_url": "https://arxiv.org/pdf/2502.15488v4",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2502.15488",
    "arxiv_authors": [
      "Jiangyong Yu",
      "Changyong Shu",
      "Sifan Zhou",
      "Zichen Yu",
      "Xing Hu",
      "Yan Chen",
      "Dawei Yang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=FQ-PETR%3A+Fully+Quantized+Position+Embedding+Transformation+for+Multi-View+3D+Object+Detection+Jiangyong+Yu+Changyong+Shu+Sifan+Zhou+Zichen+Yu+Xing+Hu",
    "gs_search_success": true,
    "gs_authors": [
      "aJIJUoAAAAAJ",
      "j2ANma0AAAAJ",
      "43VmtLIAAAAJ",
      "kSdqoi0AAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 7
  },
  {
    "arxiv_id": "2407.17193",
    "title": "Unpaired Photo-realistic Image Deraining with Energy-informed Diffusion Model",
    "year": 2024,
    "published": "2024-07-24T11:51:47Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Existing unpaired image deraining approaches face challenges in accurately capture the distinguishing characteristics between the rainy and clean domains, resulting in residual degradation and color distortion within the reconstructed images. To this end, we propose an energy-informed diffusion model for unpaired photo-realistic image deraining (UPID-EDM). Initially, we delve into the intricate visual-language priors embedded within the contrastive language-image pre-training model (CLIP), and d",
    "arxiv_url": "https://arxiv.org/abs/2407.17193v1",
    "pdf_url": "https://arxiv.org/pdf/2407.17193v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2407.17193",
    "arxiv_authors": [
      "Yuanbo Wen",
      "Tao Gao",
      "Ting Chen"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Unpaired+Photo-realistic+Image+Deraining+with+Energy-informed+Diffusion+Model+Yuanbo+Wen+Tao+Gao+Ting+Chen",
    "gs_search_success": true,
    "gs_authors": [
      "8PK8tSEAAAAJ",
      "n43ejvQAAAAJ"
    ],
    "citation_count": 10,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2411.17189",
    "title": "PhysMotion: Physics-Grounded Dynamics From a Single Image",
    "year": 2024,
    "published": "2024-11-26T07:59:11Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "We introduce PhysMotion, a novel framework that leverages principled physics-based simulations to guide intermediate 3D representations generated from a single image and input conditions (e.g., applied force and torque), producing high-quality, physically plausible video generation. By utilizing continuum mechanics-based simulations as a prior knowledge, our approach addresses the limitations of traditional data-driven generative models and result in more consistent physically plausible motions.",
    "arxiv_url": "https://arxiv.org/abs/2411.17189v2",
    "pdf_url": "https://arxiv.org/pdf/2411.17189v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2411.17189",
    "arxiv_authors": [
      "Xiyang Tan",
      "Ying Jiang",
      "Xuan Li",
      "Zeshun Zong",
      "Tianyi Xie",
      "Yin Yang",
      "Chenfanfu Jiang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=PhysMotion%3A+Physics-Grounded+Dynamics+From+a+Single+Image+Xiyang+Tan+Ying+Jiang+Xuan+Li+Zeshun+Zong+Tianyi+Xie",
    "gs_search_success": true,
    "gs_authors": [
      "jTivVMEAAAAJ",
      "hKhtnMkAAAAJ",
      "xAI6xm4AAAAJ",
      "xxKErSYAAAAJ",
      "YUvQ6FkAAAAJ",
      "oRt4qcgAAAAJ",
      "-z2_nggAAAAJ"
    ],
    "citation_count": 21,
    "gs_author_count": 7
  },
  {
    "arxiv_id": "2308.09107",
    "title": "Hyperbolic Face Anti-Spoofing",
    "year": 2023,
    "published": "2023-08-17T17:18:21Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Learning generalized face anti-spoofing (FAS) models against presentation attacks is essential for the security of face recognition systems. Previous FAS methods usually encourage models to extract discriminative features, of which the distances within the same class (bonafide or attack) are pushed close while those between bonafide and attack are pulled away. However, these methods are designed based on Euclidean distance, which lacks generalization ability for unseen attack detection due to po",
    "arxiv_url": "https://arxiv.org/abs/2308.09107v1",
    "pdf_url": "https://arxiv.org/pdf/2308.09107v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2308.09107",
    "arxiv_authors": [
      "Shuangpeng Han",
      "Rizhao Cai",
      "Yawen Cui",
      "Zitong Yu",
      "Yongjian Hu",
      "Alex Kot"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Hyperbolic+Face+Anti-Spoofing+Shuangpeng+Han+Rizhao+Cai+Yawen+Cui+Zitong+Yu+Yongjian+Hu",
    "gs_search_success": true,
    "gs_authors": [
      "ziHejLwAAAAJ",
      "Er0gOskAAAAJ",
      "3f-kR_4AAAAJ",
      "UGZXLxIAAAAJ"
    ],
    "citation_count": 10,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2310.15099",
    "title": "Dual-path convolutional neural network using micro-FTIR imaging to predict breast cancer subtypes and biomarkers levels: estrogen receptor, progesterone receptor, HER2 and Ki67",
    "year": 2023,
    "published": "2023-10-23T17:05:53Z",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "abstract": "Breast cancer molecular subtypes classification plays an import role to sort patients with divergent prognosis. The biomarkers used are Estrogen Receptor (ER), Progesterone Receptor (PR), HER2, and Ki67. Based on these biomarkers expression levels, subtypes are classified as Luminal A (LA), Luminal B (LB), HER2 subtype, and Triple-Negative Breast Cancer (TNBC). Immunohistochemistry is used to classify subtypes, although interlaboratory and interobserver variations can affect its accuracy, beside",
    "arxiv_url": "https://arxiv.org/abs/2310.15099v1",
    "pdf_url": "https://arxiv.org/pdf/2310.15099v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2310.15099",
    "arxiv_authors": [
      "Matheus del-Valle",
      "Emerson Soares Bernardes",
      "Denise Maria Zezell"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Dual-path+convolutional+neural+network+using+micro-FTIR+imaging+to+predict+breast+cancer+subtypes+and+biomarkers+levels%3A+estrogen+receptor%2C+progesterone+receptor%2C+HER2+and+Ki67+Matheus+del-Valle+Emerson+Soares+Bernardes+Denise+Maria+Zezell",
    "gs_search_success": true,
    "gs_authors": [
      "T-7bPc0AAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2502.01943",
    "title": "DAMA: Data- and Model-aware Alignment of Multi-modal LLMs",
    "year": 2025,
    "published": "2025-02-04T02:30:36Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Direct Preference Optimization (DPO) has shown effectiveness in aligning multi-modal large language models (MLLM) with human preferences. However, existing methods exhibit an imbalanced responsiveness to the data of varying hardness, tending to overfit on the easy-to-distinguish data while underfitting on the hard-to-distinguish data. In this paper, we propose Data- and Model-aware DPO (DAMA) to dynamically adjust the optimization process from two key aspects: (1) a data-aware strategy that inco",
    "arxiv_url": "https://arxiv.org/abs/2502.01943v2",
    "pdf_url": "https://arxiv.org/pdf/2502.01943v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2502.01943",
    "arxiv_authors": [
      "Jinda Lu",
      "Junkang Wu",
      "Jinghan Li",
      "Xiaojun Jia",
      "Shuo Wang",
      "YiFan Zhang",
      "Junfeng Fang",
      "Xiang Wang",
      "Xiangnan He"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=DAMA%3A+Data-+and+Model-aware+Alignment+of+Multi-modal+LLMs+Jinda+Lu+Junkang+Wu+Jinghan+Li+Xiaojun+Jia+Shuo+Wang",
    "gs_search_success": true,
    "gs_authors": [
      "retTWnEAAAAJ",
      "qTE3BacAAAAJ",
      "deBwV5oAAAAJ",
      "X45Go24AAAAJ",
      "ZBfFoEMAAAAJ",
      "HdhaQB0AAAAJ",
      "lUnt8X4AAAAJ",
      "IW_HhNwAAAAJ"
    ],
    "citation_count": 8,
    "gs_author_count": 9
  },
  {
    "arxiv_id": "2409.06183",
    "title": "EDADepth: Enhanced Data Augmentation for Monocular Depth Estimation",
    "year": 2024,
    "published": "2024-09-10T03:25:24Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Due to their text-to-image synthesis feature, diffusion models have recently seen a rise in visual perception tasks, such as depth estimation. The lack of good-quality datasets makes the extraction of a fine-grain semantic context challenging for the diffusion models. The semantic context with fewer details further worsens the process of creating effective text embeddings that will be used as input for diffusion models. In this paper, we propose a novel EDADepth, an enhanced data augmentation me",
    "arxiv_url": "https://arxiv.org/abs/2409.06183v2",
    "pdf_url": "https://arxiv.org/pdf/2409.06183v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2409.06183",
    "arxiv_authors": [
      "Nischal Khanal",
      "Shivanand Venkanna Sheshappanavar"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=EDADepth%3A+Enhanced+Data+Augmentation+for+Monocular+Depth+Estimation+Nischal+Khanal+Shivanand+Venkanna+Sheshappanavar",
    "gs_search_success": true,
    "gs_authors": [
      "I1ajnioAAAAJ",
      "gi3KmZkAAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 2
  },
  {
    "arxiv_id": "2312.17133",
    "title": "ARTrackV2: Prompting Autoregressive Tracker Where to Look and How to Describe",
    "year": 2023,
    "published": "2023-12-28T17:08:11Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "We present ARTrackV2, which integrates two pivotal aspects of tracking: determining where to look (localization) and how to describe (appearance analysis) the target object across video frames. Building on the foundation of its predecessor, ARTrackV2 extends the concept by introducing a unified generative framework to \"read out\" object's trajectory and \"retell\" its appearance in an autoregressive manner. This approach fosters a time-continuous methodology that models the joint evolution of motio",
    "arxiv_url": "https://arxiv.org/abs/2312.17133v3",
    "pdf_url": "https://arxiv.org/pdf/2312.17133v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2312.17133",
    "arxiv_authors": [
      "Yifan Bai",
      "Zeyang Zhao",
      "Yihong Gong",
      "Xing Wei"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=ARTrackV2%3A+Prompting+Autoregressive+Tracker+Where+to+Look+and+How+to+Describe+Yifan+Bai+Zeyang+Zhao+Yihong+Gong+Xing+Wei",
    "gs_search_success": true,
    "gs_authors": [
      "KNyC5EUAAAAJ",
      "x2xdU7gAAAAJ",
      "jlDQUOkAAAAJ",
      "JdQnaesAAAAJ"
    ],
    "citation_count": 117,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2505.04850",
    "title": "ORXE: Orchestrating Experts for Dynamically Configurable Efficiency",
    "year": 2025,
    "published": "2025-05-07T23:16:56Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "This paper presents ORXE, a modular and adaptable framework for achieving real-time configurable efficiency in AI models. By leveraging a collection of pre-trained experts with diverse computational costs and performance levels, ORXE dynamically adjusts inference pathways based on the complexity of input samples. Unlike conventional approaches that require complex metamodel training, ORXE achieves high efficiency and flexibility without complicating the development process. The proposed system u",
    "arxiv_url": "https://arxiv.org/abs/2505.04850v1",
    "pdf_url": "https://arxiv.org/pdf/2505.04850v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2505.04850",
    "arxiv_authors": [
      "Qingyuan Wang",
      "Guoxin Wang",
      "Barry Cardiff",
      "Deepu John"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=ORXE%3A+Orchestrating+Experts+for+Dynamically+Configurable+Efficiency+Qingyuan+Wang+Guoxin+Wang+Barry+Cardiff+Deepu+John",
    "gs_search_success": true,
    "gs_authors": [
      "-G-F-m4AAAAJ",
      "yo7t7CIAAAAJ",
      "id0RowUAAAAJ",
      "QnOwZmoAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2312.04547",
    "title": "Digital Life Project: Autonomous 3D Characters with Social Intelligence",
    "year": 2023,
    "published": "2023-12-07T18:58:59Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR",
      "cs.HC"
    ],
    "abstract": "In this work, we present Digital Life Project, a framework utilizing language as the universal medium to build autonomous 3D characters, who are capable of engaging in social interactions and expressing with articulated body motions, thereby simulating life in a digital environment. Our framework comprises two primary components: 1) SocioMind: a meticulously crafted digital brain that models personalities with systematic few-shot exemplars, incorporates a reflection process based on psychology p",
    "arxiv_url": "https://arxiv.org/abs/2312.04547v1",
    "pdf_url": "https://arxiv.org/pdf/2312.04547v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2312.04547",
    "arxiv_authors": [
      "Zhongang Cai",
      "Jianping Jiang",
      "Zhongfei Qing",
      "Xinying Guo",
      "Mingyuan Zhang",
      "Zhengyu Lin",
      "Haiyi Mei",
      "Chen Wei",
      "Ruisi Wang",
      "Wanqi Yin",
      "Xiangyu Fan",
      "Han Du",
      "Liang Pan",
      "Peng Gao",
      "Zhitao Yang",
      "Yang Gao",
      "Jiaqi Li",
      "Tianxiang Ren",
      "Yukun Wei",
      "Xiaogang Wang",
      "Chen Change Loy",
      "Lei Yang",
      "Ziwei Liu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Digital+Life+Project%3A+Autonomous+3D+Characters+with+Social+Intelligence+Zhongang+Cai+Jianping+Jiang+Zhongfei+Qing+Xinying+Guo+Mingyuan+Zhang",
    "gs_search_success": true,
    "gs_authors": [
      "zlIJwBEAAAAJ",
      "WLu-UdoAAAAJ",
      "2QLD4fAAAAAJ",
      "TOZ9wR4AAAAJ",
      "v8aU6LQAAAAJ",
      "XbFxzicAAAAJ",
      "4r07QUQAAAAJ",
      "TEticdEAAAAJ",
      "WrDKqIAAAAAJ",
      "lSDISOcAAAAJ"
    ],
    "citation_count": 56,
    "gs_author_count": 12
  },
  {
    "arxiv_id": "2303.09608",
    "title": "VEIL: Vetting Extracted Image Labels from In-the-Wild Captions for Weakly-Supervised Object Detection",
    "year": 2023,
    "published": "2023-03-16T19:28:37Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "The use of large-scale vision-language datasets is limited for object detection due to the negative impact of label noise on localization. Prior methods have shown how such large-scale datasets can be used for pretraining, which can provide initial signal for localization, but is insufficient without clean bounding-box data for at least some categories. We propose a technique to \"vet\" labels extracted from noisy captions, and use them for weakly-supervised object detection (WSOD), without any bo",
    "arxiv_url": "https://arxiv.org/abs/2303.09608v3",
    "pdf_url": "https://arxiv.org/pdf/2303.09608v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2303.09608",
    "arxiv_authors": [
      "Arushi Rai",
      "Adriana Kovashka"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=VEIL%3A+Vetting+Extracted+Image+Labels+from+In-the-Wild+Captions+for+Weakly-Supervised+Object+Detection+Arushi+Rai+Adriana+Kovashka",
    "gs_search_success": true,
    "gs_authors": [
      "Dl949GoAAAAJ",
      "VcU4TUAAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 2
  },
  {
    "arxiv_id": "2408.00210",
    "title": "A Prior Embedding-Driven Architecture for Long Distance Blind Iris Recognition",
    "year": 2024,
    "published": "2024-08-01T00:40:17Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Blind iris images, which result from unknown degradation during the process of iris recognition at long distances, often lead to decreased iris recognition rates. Currently, little existing literature offers a solution to this problem. In response, we propose a prior embedding-driven architecture for long distance blind iris recognition. We first proposed a blind iris image restoration network called Iris-PPRGAN. To effectively restore the texture of the blind iris, Iris-PPRGAN includes a Genera",
    "arxiv_url": "https://arxiv.org/abs/2408.00210v1",
    "pdf_url": "https://arxiv.org/pdf/2408.00210v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2408.00210",
    "arxiv_authors": [
      "Qi Xiong",
      "Xinman Zhang",
      "Jun Shen"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=A+Prior+Embedding-Driven+Architecture+for+Long+Distance+Blind+Iris+Recognition+Qi+Xiong+Xinman+Zhang+Jun+Shen",
    "gs_search_success": true,
    "gs_authors": [
      "Bf6gvGkAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2506.00667",
    "title": "Scene Detection Policies and Keyframe Extraction Strategies for Large-Scale Video Analysis",
    "year": 2025,
    "published": "2025-05-31T18:37:21Z",
    "categories": [
      "cs.CV",
      "cs.MM"
    ],
    "abstract": "Robust scene segmentation and keyframe extraction are essential preprocessing steps in video understanding pipelines, supporting tasks such as indexing, summarization, and semantic retrieval. However, existing methods often lack generalizability across diverse video types and durations. We present a unified, adaptive framework for automatic scene detection and keyframe selection that handles formats ranging from short-form media to long-form films, archival content, and surveillance footage. Our",
    "arxiv_url": "https://arxiv.org/abs/2506.00667v1",
    "pdf_url": "https://arxiv.org/pdf/2506.00667v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2506.00667",
    "arxiv_authors": [
      "Vasilii Korolkov"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Scene+Detection+Policies+and+Keyframe+Extraction+Strategies+for+Large-Scale+Video+Analysis+Vasilii+Korolkov",
    "gs_search_success": true,
    "gs_authors": [],
    "citation_count": 1,
    "gs_author_count": 1
  },
  {
    "arxiv_id": "2409.00343",
    "title": "EgoHDM: An Online Egocentric-Inertial Human Motion Capture, Localization, and Dense Mapping System",
    "year": 2024,
    "published": "2024-08-31T04:19:02Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "We present EgoHDM, an online egocentric-inertial human motion capture (mocap), localization, and dense mapping system. Our system uses 6 inertial measurement units (IMUs) and a commodity head-mounted RGB camera. EgoHDM is the first human mocap system that offers dense scene mapping in near real-time. Further, it is fast and robust to initialize and fully closes the loop between physically plausible map-aware global human motion estimation and mocap-aware 3D scene reconstruction. Our key idea is ",
    "arxiv_url": "https://arxiv.org/abs/2409.00343v2",
    "pdf_url": "https://arxiv.org/pdf/2409.00343v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2409.00343",
    "arxiv_authors": [
      "Bonan Liu",
      "Handi Yin",
      "Manuel Kaufmann",
      "Jinhao He",
      "Sammy Christen",
      "Jie Song",
      "Pan Hui"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=EgoHDM%3A+An+Online+Egocentric-Inertial+Human+Motion+Capture%2C+Localization%2C+and+Dense+Mapping+System+Bonan+Liu+Handi+Yin+Manuel+Kaufmann+Jinhao+He+Sammy+Christen",
    "gs_search_success": true,
    "gs_authors": [
      "7rpD7dAAAAAJ",
      "qhw2qgQAAAAJ",
      "r1L_2qkAAAAJ",
      "RU8gNcgAAAAJ",
      "ml0zo88AAAAJ",
      "ipSS2ToAAAAJ",
      "4hMhIecAAAAJ",
      "BBujJNcAAAAJ",
      "i_YtM78AAAAJ",
      "DYXnRWEAAAAJ",
      "25nKBNAAAAAJ",
      "0P_684sAAAAJ",
      "NR4TTRgAAAAJ",
      "NrF9a0MAAAAJ"
    ],
    "citation_count": 6,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2407.10536",
    "title": "An experimental evaluation of Siamese Neural Networks for robot localization using omnidirectional imaging in indoor environments",
    "year": 2024,
    "published": "2024-07-15T08:44:37Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "The objective of this paper is to address the localization problem using omnidirectional images captured by a catadioptric vision system mounted on the robot. For this purpose, we explore the potential of Siamese Neural Networks for modeling indoor environments using panoramic images as the unique source of information. Siamese Neural Networks are characterized by their ability to generate a similarity function between two input data, in this case, between two panoramic images. In this study, Si",
    "arxiv_url": "https://arxiv.org/abs/2407.10536v1",
    "pdf_url": "https://arxiv.org/pdf/2407.10536v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2407.10536",
    "arxiv_authors": [
      "J. J. Cabrera",
      "V. Román",
      "A. Gil",
      "O. Reinoso",
      "L. Payá"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=An+experimental+evaluation+of+Siamese+Neural+Networks+for+robot+localization+using+omnidirectional+imaging+in+indoor+environments+J.+J.+Cabrera+V.+Rom%C3%A1n+A.+Gil+O.+Reinoso+L.+Pay%C3%A1",
    "gs_search_success": true,
    "gs_authors": [
      "kUAmbQMAAAAJ",
      "pFlS_NQAAAAJ",
      "WtDfnrUAAAAJ"
    ],
    "citation_count": 8,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2503.01645",
    "title": "DesignDiffusion: High-Quality Text-to-Design Image Generation with Diffusion Models",
    "year": 2025,
    "published": "2025-03-03T15:22:57Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "In this paper, we present DesignDiffusion, a simple yet effective framework for the novel task of synthesizing design images from textual descriptions. A primary challenge lies in generating accurate and style-consistent textual and visual content. Existing works in a related task of visual text generation often focus on generating text within given specific regions, which limits the creativity of generation models, resulting in style or color inconsistencies between textual and visual elements ",
    "arxiv_url": "https://arxiv.org/abs/2503.01645v1",
    "pdf_url": "https://arxiv.org/pdf/2503.01645v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2503.01645",
    "arxiv_authors": [
      "Zhendong Wang",
      "Jianmin Bao",
      "Shuyang Gu",
      "Dong Chen",
      "Wengang Zhou",
      "Houqiang Li"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=DesignDiffusion%3A+High-Quality+Text-to-Design+Image+Generation+with+Diffusion+Models+Zhendong+Wang+Jianmin+Bao+Shuyang+Gu+Dong+Chen+Wengang+Zhou",
    "gs_search_success": true,
    "gs_authors": [
      "hjwvkYUAAAAJ",
      "-NnIabUAAAAJ",
      "Ya5VDjQAAAAJ",
      "_fKSYOwAAAAJ"
    ],
    "citation_count": 14,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2402.11836",
    "title": "DIO: Dataset of 3D Mesh Models of Indoor Objects for Robotics and Computer Vision Applications",
    "year": 2024,
    "published": "2024-02-19T04:58:40Z",
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "abstract": "The creation of accurate virtual models of real-world objects is imperative to robotic simulations and applications such as computer vision, artificial intelligence, and machine learning. This paper documents the different methods employed for generating a database of mesh models of real-world objects. These methods address the tedious and time-intensive process of manually generating the models using CAD software. Essentially, DSLR/phone cameras were employed to acquire images of target objects",
    "arxiv_url": "https://arxiv.org/abs/2402.11836v1",
    "pdf_url": "https://arxiv.org/pdf/2402.11836v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2402.11836",
    "arxiv_authors": [
      "Nillan Nimal",
      "Wenbin Li",
      "Ronald Clark",
      "Sajad Saeedi"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=DIO%3A+Dataset+of+3D+Mesh+Models+of+Indoor+Objects+for+Robotics+and+Computer+Vision+Applications+Nillan+Nimal+Wenbin+Li+Ronald+Clark+Sajad+Saeedi",
    "gs_search_success": true,
    "gs_authors": [
      "JvInxPIAAAAJ",
      "hynjzMkAAAAJ",
      "0bzqo0YAAAAJ",
      "Z0Lj36MAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2411.10224",
    "title": "EVOKE: Elevating Chest X-ray Report Generation via Multi-View Contrastive Learning and Patient-Specific Knowledge",
    "year": 2024,
    "published": "2024-11-15T14:38:13Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Radiology reports are crucial for planning treatment strategies and facilitating effective doctor-patient communication. However, the manual creation of these reports places a significant burden on radiologists. While automatic radiology report generation presents a promising solution, existing methods often rely on single-view radiographs, which constrain diagnostic accuracy. To address this challenge, we propose \\textbf{EVOKE}, a novel chest X-ray report generation framework that incorporates ",
    "arxiv_url": "https://arxiv.org/abs/2411.10224v2",
    "pdf_url": "https://arxiv.org/pdf/2411.10224v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2411.10224",
    "arxiv_authors": [
      "Qiguang Miao",
      "Kang Liu",
      "Zhuoqi Ma",
      "Yunan Li",
      "Xiaolu Kang",
      "Ruixuan Liu",
      "Tianyi Liu",
      "Kun Xie",
      "Zhicheng Jiao"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=EVOKE%3A+Elevating+Chest+X-ray+Report+Generation+via+Multi-View+Contrastive+Learning+and+Patient-Specific+Knowledge+Qiguang+Miao+Kang+Liu+Zhuoqi+Ma+Yunan+Li+Xiaolu+Kang",
    "gs_search_success": true,
    "gs_authors": [
      "Gp4H63sAAAAJ",
      "ZaeZfN8AAAAJ",
      "2TQfvt8AAAAJ",
      "lMcUyKAAAAAJ",
      "lQoZLmkAAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 9
  },
  {
    "arxiv_id": "2306.13990",
    "title": "Cross-Validation Is All You Need: A Statistical Approach To Label Noise Estimation",
    "year": 2023,
    "published": "2023-06-24T14:50:20Z",
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "abstract": "Machine learning models experience deteriorated performance when trained in the presence of noisy labels. This is particularly problematic for medical tasks, such as survival prediction, which typically face high label noise complexity with few clear-cut solutions. Inspired by the large fluctuations across folds in the cross-validation performance of survival analyses, we design Monte-Carlo experiments to show that such fluctuation could be caused by label noise. We propose two novel and straigh",
    "arxiv_url": "https://arxiv.org/abs/2306.13990v2",
    "pdf_url": "https://arxiv.org/pdf/2306.13990v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2306.13990",
    "arxiv_authors": [
      "Jianan Chen",
      "Vishwesh Ramanathan",
      "Tony Xu",
      "Anne L. Martel"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Cross-Validation+Is+All+You+Need%3A+A+Statistical+Approach+To+Label+Noise+Estimation+Jianan+Chen+Vishwesh+Ramanathan+Tony+Xu+Anne+L.+Martel",
    "gs_search_success": true,
    "gs_authors": [
      "3vwB4oEAAAAJ",
      "y7u4Ea8AAAAJ",
      "dSkS3EEAAAAJ",
      "B9tp488AAAAJ"
    ],
    "citation_count": 4,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2412.07518",
    "title": "Hallucination Elimination and Semantic Enhancement Framework for Vision-Language Models in Traffic Scenarios",
    "year": 2024,
    "published": "2024-12-10T13:56:49Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Large vision-language models (LVLMs) have demonstrated remarkable capabilities in multimodal understanding and generation tasks. However, these models occasionally generate hallucinatory texts, resulting in descriptions that seem reasonable but do not correspond to the image. This phenomenon can lead to wrong driving decisions of the autonomous driving system. To address this challenge, this paper proposes HCOENet, a plug-and-play chain-of-thought correction method designed to eliminate object h",
    "arxiv_url": "https://arxiv.org/abs/2412.07518v1",
    "pdf_url": "https://arxiv.org/pdf/2412.07518v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2412.07518",
    "arxiv_authors": [
      "Jiaqi Fan",
      "Jianhua Wu",
      "Hongqing Chu",
      "Quanbo Ge",
      "Bingzhao Gao"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Hallucination+Elimination+and+Semantic+Enhancement+Framework+for+Vision-Language+Models+in+Traffic+Scenarios+Jiaqi+Fan+Jianhua+Wu+Hongqing+Chu+Quanbo+Ge+Bingzhao+Gao",
    "gs_search_success": true,
    "gs_authors": [
      "B2tCkGgAAAAJ",
      "GvK2l7sAAAAJ",
      "mR2ai0sAAAAJ"
    ],
    "citation_count": 6,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2409.20340",
    "title": "Enhancing GANs with Contrastive Learning-Based Multistage Progressive Finetuning SNN and RL-Based External Optimization",
    "year": 2024,
    "published": "2024-09-30T14:39:56Z",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "Generative Adversarial Networks (GANs) have been at the forefront of image synthesis, especially in medical fields like histopathology, where they help address challenges such as data scarcity, patient privacy, and class imbalance. However, several inherent and domain-specific issues remain. For GANs, training instability, mode collapse, and insufficient feedback from binary classification can undermine performance. These challenges are particularly pronounced with high-resolution histopathology",
    "arxiv_url": "https://arxiv.org/abs/2409.20340v3",
    "pdf_url": "https://arxiv.org/pdf/2409.20340v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2409.20340",
    "arxiv_authors": [
      "Osama Mustafa"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Enhancing+GANs+with+Contrastive+Learning-Based+Multistage+Progressive+Finetuning+SNN+and+RL-Based+External+Optimization+Osama+Mustafa",
    "gs_search_success": true,
    "gs_authors": [
      "IRjZqNIAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 1
  },
  {
    "arxiv_id": "2308.16461",
    "title": "Domain Adaptive Synapse Detection with Weak Point Annotations",
    "year": 2023,
    "published": "2023-08-31T05:05:53Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "The development of learning-based methods has greatly improved the detection of synapses from electron microscopy (EM) images. However, training a model for each dataset is time-consuming and requires extensive annotations. Additionally, it is difficult to apply a learned model to data from different brain regions due to variations in data distributions. In this paper, we present AdaSyn, a two-stage segmentation-based framework for domain adaptive synapse detection with weak point annotations. I",
    "arxiv_url": "https://arxiv.org/abs/2308.16461v1",
    "pdf_url": "https://arxiv.org/pdf/2308.16461v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2308.16461",
    "arxiv_authors": [
      "Qi Chen",
      "Wei Huang",
      "Yueyi Zhang",
      "Zhiwei Xiong"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Domain+Adaptive+Synapse+Detection+with+Weak+Point+Annotations+Qi+Chen+Wei+Huang+Yueyi+Zhang+Zhiwei+Xiong",
    "gs_search_success": true,
    "gs_authors": [
      "LatWlFAAAAAJ",
      "C4zmoy4AAAAJ",
      "Snl0HPEAAAAJ",
      "4Q5gs2MAAAAJ"
    ],
    "citation_count": 2,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2404.13013",
    "title": "Groma: Localized Visual Tokenization for Grounding Multimodal Large Language Models",
    "year": 2024,
    "published": "2024-04-19T17:22:51Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "abstract": "We introduce Groma, a Multimodal Large Language Model (MLLM) with grounded and fine-grained visual perception ability. Beyond holistic image understanding, Groma is adept at region-level tasks such as region captioning and visual grounding. Such capabilities are built upon a localized visual tokenization mechanism, where an image input is decomposed into regions of interest and subsequently encoded into region tokens. By integrating region tokens into user instructions and model responses, we se",
    "arxiv_url": "https://arxiv.org/abs/2404.13013v1",
    "pdf_url": "https://arxiv.org/pdf/2404.13013v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2404.13013",
    "arxiv_authors": [
      "Chuofan Ma",
      "Yi Jiang",
      "Jiannan Wu",
      "Zehuan Yuan",
      "Xiaojuan Qi"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Groma%3A+Localized+Visual+Tokenization+for+Grounding+Multimodal+Large+Language+Models+Chuofan+Ma+Yi+Jiang+Jiannan+Wu+Zehuan+Yuan+Xiaojuan+Qi",
    "gs_search_success": true,
    "gs_authors": [
      "1euA66EAAAAJ",
      "FqMOHnEAAAAJ",
      "bGn0uacAAAAJ",
      "6dikuoYAAAAJ",
      "hgKtgWAAAAAJ"
    ],
    "citation_count": 112,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2311.11427",
    "title": "Appearance Codes using Joint Embedding Learning of Multiple Modalities",
    "year": 2023,
    "published": "2023-11-19T21:24:34Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "The use of appearance codes in recent work on generative modeling has enabled novel view renders with variable appearance and illumination, such as day-time and night-time renders of a scene. A major limitation of this technique is the need to re-train new appearance codes for every scene on inference, so in this work we address this problem proposing a framework that learns a joint embedding space for the appearance and structure of the scene by enforcing a contrastive loss constraint between d",
    "arxiv_url": "https://arxiv.org/abs/2311.11427v1",
    "pdf_url": "https://arxiv.org/pdf/2311.11427v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2311.11427",
    "arxiv_authors": [
      "Alex Zhang",
      "Evan Dogariu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Appearance+Codes+using+Joint+Embedding+Learning+of+Multiple+Modalities+Alex+Zhang+Evan+Dogariu",
    "gs_search_success": true,
    "gs_authors": [
      "jCDOPGMAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 2
  },
  {
    "arxiv_id": "2501.06770",
    "title": "SuperNeRF-GAN: A Universal 3D-Consistent Super-Resolution Framework for Efficient and Enhanced 3D-Aware Image Synthesis",
    "year": 2025,
    "published": "2025-01-12T10:31:33Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Neural volume rendering techniques, such as NeRF, have revolutionized 3D-aware image synthesis by enabling the generation of images of a single scene or object from various camera poses. However, the high computational cost of NeRF presents challenges for synthesizing high-resolution (HR) images. Most existing methods address this issue by leveraging 2D super-resolution, which compromise 3D-consistency. Other methods propose radiance manifolds or two-stage generation to achieve 3D-consistent HR ",
    "arxiv_url": "https://arxiv.org/abs/2501.06770v2",
    "pdf_url": "https://arxiv.org/pdf/2501.06770v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2501.06770",
    "arxiv_authors": [
      "Peng Zheng",
      "Linzhi Huang",
      "Yizhou Yu",
      "Yi Chang",
      "Yilin Wang",
      "Rui Ma"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=SuperNeRF-GAN%3A+A+Universal+3D-Consistent+Super-Resolution+Framework+for+Efficient+and+Enhanced+3D-Aware+Image+Synthesis+Peng+Zheng+Linzhi+Huang+Yizhou+Yu+Yi+Chang+Yilin+Wang",
    "gs_search_success": true,
    "gs_authors": [
      "drEkR50AAAAJ",
      "e38fTZQAAAAJ",
      "YERRAEAAAAAJ",
      "gU429joAAAAJ"
    ],
    "citation_count": 2,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2501.07104",
    "title": "RMAvatar: Photorealistic Human Avatar Reconstruction from Monocular Video Based on Rectified Mesh-embedded Gaussians",
    "year": 2025,
    "published": "2025-01-13T07:32:44Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "We introduce RMAvatar, a novel human avatar representation with Gaussian splatting embedded on mesh to learn clothed avatar from a monocular video. We utilize the explicit mesh geometry to represent motion and shape of a virtual human and implicit appearance rendering with Gaussian Splatting. Our method consists of two main modules: Gaussian initialization module and Gaussian rectification module. We embed Gaussians into triangular faces and control their motion through the mesh, which ensures l",
    "arxiv_url": "https://arxiv.org/abs/2501.07104v1",
    "pdf_url": "https://arxiv.org/pdf/2501.07104v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2501.07104",
    "arxiv_authors": [
      "Sen Peng",
      "Weixing Xie",
      "Zilong Wang",
      "Xiaohu Guo",
      "Zhonggui Chen",
      "Baorong Yang",
      "Xiao Dong"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=RMAvatar%3A+Photorealistic+Human+Avatar+Reconstruction+from+Monocular+Video+Based+on+Rectified+Mesh-embedded+Gaussians+Sen+Peng+Weixing+Xie+Zilong+Wang+Xiaohu+Guo+Zhonggui+Chen",
    "gs_search_success": true,
    "gs_authors": [
      "Y4SdsvsAAAAJ",
      "CCOcQQYAAAAJ",
      "uv-Mbh8AAAAJ"
    ],
    "citation_count": 3,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2409.02914",
    "title": "Can LVLMs Obtain a Driver's License? A Benchmark Towards Reliable AGI for Autonomous Driving",
    "year": 2024,
    "published": "2024-09-04T17:52:43Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Large Vision-Language Models (LVLMs) have recently garnered significant attention, with many efforts aimed at harnessing their general knowledge to enhance the interpretability and robustness of autonomous driving models. However, LVLMs typically rely on large, general-purpose datasets and lack the specialized expertise required for professional and safe driving. Existing vision-language driving datasets focus primarily on scene understanding and decision-making, without providing explicit guida",
    "arxiv_url": "https://arxiv.org/abs/2409.02914v1",
    "pdf_url": "https://arxiv.org/pdf/2409.02914v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2409.02914",
    "arxiv_authors": [
      "Yuhang Lu",
      "Yichen Yao",
      "Jiadong Tu",
      "Jiangnan Shao",
      "Yuexin Ma",
      "Xinge Zhu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Can+LVLMs+Obtain+a+Driver%27s+License%3F+A+Benchmark+Towards+Reliable+AGI+for+Autonomous+Driving+Yuhang+Lu+Yichen+Yao+Jiadong+Tu+Jiangnan+Shao+Yuexin+Ma",
    "gs_search_success": true,
    "gs_authors": [
      "BnIM0U4AAAAJ",
      "DIsP7rUAAAAJ",
      "yHAcRooAAAAJ",
      "HiwDOksAAAAJ",
      "R3UZRMEAAAAJ"
    ],
    "citation_count": 13,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2405.10951",
    "title": "Block Selective Reprogramming for On-device Training of Vision Transformers",
    "year": 2024,
    "published": "2024-03-25T08:41:01Z",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "The ubiquity of vision transformers (ViTs) for various edge applications, including personalized learning, has created the demand for on-device fine-tuning. However, training with the limited memory and computation power of edge devices remains a significant challenge. In particular, the memory required for training is much higher than that needed for inference, primarily due to the need to store activations across all layers in order to compute the gradients needed for weight updates. Previous ",
    "arxiv_url": "https://arxiv.org/abs/2405.10951v1",
    "pdf_url": "https://arxiv.org/pdf/2405.10951v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2405.10951",
    "arxiv_authors": [
      "Sreetama Sarkar",
      "Souvik Kundu",
      "Kai Zheng",
      "Peter A. Beerel"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Block+Selective+Reprogramming+for+On-device+Training+of+Vision+Transformers+Sreetama+Sarkar+Souvik+Kundu+Kai+Zheng+Peter+A.+Beerel",
    "gs_search_success": true,
    "gs_authors": [
      "JSdH7PsAAAAJ",
      "BHZqjebF-IIC",
      "5G5bBKEAAAAJ"
    ],
    "citation_count": 6,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2505.11034",
    "title": "CleanPatrick: A Benchmark for Image Data Cleaning",
    "year": 2025,
    "published": "2025-05-16T09:29:41Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "abstract": "Robust machine learning depends on clean data, yet current image data cleaning benchmarks rely on synthetic noise or narrow human studies, limiting comparison and real-world relevance. We introduce CleanPatrick, the first large-scale benchmark for data cleaning in the image domain, built upon the publicly available Fitzpatrick17k dermatology dataset. We collect 496,377 binary annotations from 933 medical crowd workers, identify off-topic samples (4%), near-duplicates (21%), and label errors (22%",
    "arxiv_url": "https://arxiv.org/abs/2505.11034v1",
    "pdf_url": "https://arxiv.org/pdf/2505.11034v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2505.11034",
    "arxiv_authors": [
      "Fabian Gröger",
      "Simone Lionetti",
      "Philippe Gottfrois",
      "Alvaro Gonzalez-Jimenez",
      "Ludovic Amruthalingam",
      "Elisabeth Victoria Goessinger",
      "Hanna Lindemann",
      "Marie Bargiela",
      "Marie Hofbauer",
      "Omar Badri",
      "Philipp Tschandl",
      "Arash Koochek",
      "Matthew Groh",
      "Alexander A. Navarini",
      "Marc Pouly"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=CleanPatrick%3A+A+Benchmark+for+Image+Data+Cleaning+Fabian+Gr%C3%B6ger+Simone+Lionetti+Philippe+Gottfrois+Alvaro+Gonzalez-Jimenez+Ludovic+Amruthalingam",
    "gs_search_success": true,
    "gs_authors": [
      "uwyr5k0AAAAJ",
      "LbtKzVgAAAAJ",
      "74ixG6IAAAAJ",
      "3GMhoS8AAAAJ",
      "WCvkBwkAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2306.16019",
    "title": "Fast Recognition of birds in offshore wind farms based on an improved deep learning model",
    "year": 2023,
    "published": "2023-06-28T08:47:04Z",
    "categories": [
      "cs.CV",
      "eess.IV"
    ],
    "abstract": "The safety of wind turbines is a prerequisite for the stable operation of offshore wind farms. However, bird damage poses a direct threat to the safe operation of wind turbines and wind turbine blades. In addition, millions of birds are killed by wind turbines every year. In order to protect the ecological environment and maintain the safe operation of offshore wind turbines, and to address the problem of the low detection capability of current target detection algorithms in low-light environmen",
    "arxiv_url": "https://arxiv.org/abs/2306.16019v1",
    "pdf_url": "https://arxiv.org/pdf/2306.16019v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2306.16019",
    "arxiv_authors": [
      "Yantong Liu",
      "Xingke Li",
      "Jong-Chan Lee"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Fast+Recognition+of+birds+in+offshore+wind+farms+based+on+an+improved+deep+learning+model+Yantong+Liu+Xingke+Li+Jong-Chan+Lee",
    "gs_search_success": true,
    "gs_authors": [
      "V-0ZPuMAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2407.18821",
    "title": "Deep Companion Learning: Enhancing Generalization Through Historical Consistency",
    "year": 2024,
    "published": "2024-07-26T15:31:13Z",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "We propose Deep Companion Learning (DCL), a novel training method for Deep Neural Networks (DNNs) that enhances generalization by penalizing inconsistent model predictions compared to its historical performance. To achieve this, we train a deep-companion model (DCM), by using previous versions of the model to provide forecasts on new inputs. This companion model deciphers a meaningful latent semantic structure within the data, thereby providing targeted supervision that encourages the primary mo",
    "arxiv_url": "https://arxiv.org/abs/2407.18821v1",
    "pdf_url": "https://arxiv.org/pdf/2407.18821v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2407.18821",
    "arxiv_authors": [
      "Ruizhao Zhu",
      "Venkatesh Saligrama"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Deep+Companion+Learning%3A+Enhancing+Generalization+Through+Historical+Consistency+Ruizhao+Zhu+Venkatesh+Saligrama",
    "gs_search_success": true,
    "gs_authors": [
      "S4z3uzMAAAAJ",
      "kAepw1UAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 2
  },
  {
    "arxiv_id": "2401.01201",
    "title": "Whole-examination AI estimation of fetal biometrics from 20-week ultrasound scans",
    "year": 2024,
    "published": "2024-01-02T13:04:41Z",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "The current approach to fetal anomaly screening is based on biometric measurements derived from individually selected ultrasound images. In this paper, we introduce a paradigm shift that attains human-level performance in biometric measurement by aggregating automatically extracted biometrics from every frame across an entire scan, with no need for operator intervention. We use a convolutional neural network to classify each frame of an ultrasound video recording. We then measure fetal biometric",
    "arxiv_url": "https://arxiv.org/abs/2401.01201v1",
    "pdf_url": "https://arxiv.org/pdf/2401.01201v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2401.01201",
    "arxiv_authors": [
      "Lorenzo Venturini",
      "Samuel Budd",
      "Alfonso Farruggia",
      "Robert Wright",
      "Jacqueline Matthew",
      "Thomas G. Day",
      "Bernhard Kainz",
      "Reza Razavi",
      "Jo V. Hajnal"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Whole-examination+AI+estimation+of+fetal+biometrics+from+20-week+ultrasound+scans+Lorenzo+Venturini+Samuel+Budd+Alfonso+Farruggia+Robert+Wright+Jacqueline+Matthew",
    "gs_search_success": true,
    "gs_authors": [
      "FQIRuLUAAAAJ",
      "DmDaHS4AAAAJ",
      "-EIo9DwAAAAJ",
      "UFGS69kAAAAJ",
      "WEg0Rq0AAAAJ",
      "Igxq-YEAAAAJ",
      "eB0qZcEAAAAJ",
      "izlZ7qAAAAAJ",
      "r5_sKhMAAAAJ"
    ],
    "citation_count": 9,
    "gs_author_count": 9
  },
  {
    "arxiv_id": "2312.10237",
    "title": "A Distributed Privacy Preserving Model for the Detection of Alzheimer's Disease",
    "year": 2023,
    "published": "2023-12-15T22:09:04Z",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "cs.DC"
    ],
    "abstract": "In the era of rapidly advancing medical technologies, the segmentation of medical data has become inevitable, necessitating the development of privacy preserving machine learning algorithms that can train on distributed data. Consolidating sensitive medical data is not always an option particularly due to the stringent privacy regulations imposed by the Health Insurance Portability and Accountability Act (HIPAA). In this paper, I introduce a HIPAA compliant framework that can train from distribu",
    "arxiv_url": "https://arxiv.org/abs/2312.10237v5",
    "pdf_url": "https://arxiv.org/pdf/2312.10237v5",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2312.10237",
    "arxiv_authors": [
      "Paul K. Mandal"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=A+Distributed+Privacy+Preserving+Model+for+the+Detection+of+Alzheimer%27s+Disease+Paul+K.+Mandal",
    "gs_search_success": true,
    "gs_authors": [
      "LKW8FS4AAAAJ"
    ],
    "citation_count": 2,
    "gs_author_count": 1
  },
  {
    "arxiv_id": "2411.01019",
    "title": "A lightweight Convolutional Neural Network based on U shape structure and Attention Mechanism for Anterior Mediastinum Segmentation",
    "year": 2024,
    "published": "2024-11-01T20:41:01Z",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "abstract": "To automatically detect Anterior Mediastinum Lesions (AMLs) in the Anterior Mediastinum (AM), the primary requirement will be an automatic segmentation model specifically designed for the AM. The prevalence of AML is extremely low, making it challenging to conduct screening research similar to lung cancer screening. Retrospectively reviewing chest CT scans over a specific period to investigate the prevalence of AML requires substantial time. Therefore, developing an Artificial Intelligence (AI) ",
    "arxiv_url": "https://arxiv.org/abs/2411.01019v1",
    "pdf_url": "https://arxiv.org/pdf/2411.01019v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2411.01019",
    "arxiv_authors": [
      "Sina Soleimani-Fard",
      "Won Gi Jeong",
      "Francis Ferri Ripalda",
      "Hasti Sasani",
      "Younhee Choi",
      "S Deiva",
      "Gong Yong Jin",
      "Seok-bum Ko"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=A+lightweight+Convolutional+Neural+Network+based+on+U+shape+structure+and+Attention+Mechanism+for+Anterior+Mediastinum+Segmentation+Sina+Soleimani-Fard+Won+Gi+Jeong+Francis+Ferri+Ripalda+Hasti+Sasani+Younhee+Choi",
    "gs_search_success": true,
    "gs_authors": [
      "urw1yA4AAAAJ",
      "aIAvCvcAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 7
  },
  {
    "arxiv_id": "2303.11389",
    "title": "Creating Ensembles of Classifiers through UMDA for Aerial Scene Classification",
    "year": 2023,
    "published": "2023-03-20T18:49:39Z",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "Aerial scene classification, which aims to semantically label remote sensing images in a set of predefined classes (e.g., agricultural, beach, and harbor), is a very challenging task in remote sensing due to high intra-class variability and the different scales and orientations of the objects present in the dataset images. In remote sensing area, the use of CNN architectures as an alternative solution is also a reality for scene classification tasks. Generally, these CNNs are used to perform the",
    "arxiv_url": "https://arxiv.org/abs/2303.11389v2",
    "pdf_url": "https://arxiv.org/pdf/2303.11389v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2303.11389",
    "arxiv_authors": [
      "Fabio A. Faria",
      "Luiz H. Buris",
      "Luis A. M. Pereira",
      "Fábio A. M. Cappabianco"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Creating+Ensembles+of+Classifiers+through+UMDA+for+Aerial+Scene+Classification+Fabio+A.+Faria+Luiz+H.+Buris+Luis+A.+M.+Pereira+F%C3%A1bio+A.+M.+Cappabianco",
    "gs_search_success": true,
    "gs_authors": [
      "qmH9VEEAAAAJ",
      "cYSV0s0AAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2504.12676",
    "title": "Accurate Tracking of Arabidopsis Root Cortex Cell Nuclei in 3D Time-Lapse Microscopy Images Based on Genetic Algorithm",
    "year": 2025,
    "published": "2025-04-17T06:07:17Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Arabidopsis is a widely used model plant to gain basic knowledge on plant physiology and development. Live imaging is an important technique to visualize and quantify elemental processes in plant development. To uncover novel theories underlying plant growth and cell division, accurate cell tracking on live imaging is of utmost importance. The commonly used cell tracking software, TrackMate, adopts tracking-by-detection fashion, which applies Laplacian of Gaussian (LoG) for blob detection, and L",
    "arxiv_url": "https://arxiv.org/abs/2504.12676v1",
    "pdf_url": "https://arxiv.org/pdf/2504.12676v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2504.12676",
    "arxiv_authors": [
      "Yu Song",
      "Tatsuaki Goh",
      "Yinhao Li",
      "Jiahua Dong",
      "Shunsuke Miyashima",
      "Yutaro Iwamoto",
      "Yohei Kondo",
      "Keiji Nakajima",
      "Yen-wei Chen"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Accurate+Tracking+of+Arabidopsis+Root+Cortex+Cell+Nuclei+in+3D+Time-Lapse+Microscopy+Images+Based+on+Genetic+Algorithm+Yu+Song+Tatsuaki+Goh+Yinhao+Li+Jiahua+Dong+Shunsuke+Miyashima",
    "gs_search_success": true,
    "gs_authors": [
      "NHGzGq0AAAAJ",
      "SqldJlAAAAAJ",
      "NUu-LmcAAAAJ",
      "ZI0PvoQAAAAJ",
      "LHNDm0kAAAAJ",
      "FPLjP_kAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 9
  },
  {
    "arxiv_id": "2306.09718",
    "title": "Label-noise-tolerant medical image classification via self-attention and self-supervised learning",
    "year": 2023,
    "published": "2023-06-16T09:37:16Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Deep neural networks (DNNs) have been widely applied in medical image classification and achieve remarkable classification performance. These achievements heavily depend on large-scale accurately annotated training data. However, label noise is inevitably introduced in the medical image annotation, as the labeling process heavily relies on the expertise and experience of annotators. Meanwhile, DNNs suffer from overfitting noisy labels, degrading the performance of models. Therefore, in this work",
    "arxiv_url": "https://arxiv.org/abs/2306.09718v1",
    "pdf_url": "https://arxiv.org/pdf/2306.09718v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2306.09718",
    "arxiv_authors": [
      "Hongyang Jiang",
      "Mengdi Gao",
      "Yan Hu",
      "Qiushi Ren",
      "Zhaoheng Xie",
      "Jiang Liu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Label-noise-tolerant+medical+image+classification+via+self-attention+and+self-supervised+learning+Hongyang+Jiang+Mengdi+Gao+Yan+Hu+Qiushi+Ren+Zhaoheng+Xie",
    "gs_search_success": true,
    "gs_authors": [
      "QxWIB_QAAAAJ",
      "7cOdUFgAAAAJ",
      "rYLooucAAAAJ",
      "NHt3fUcAAAAJ"
    ],
    "citation_count": 6,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2401.07358",
    "title": "Harnessing Machine Learning for Discerning AI-Generated Synthetic Images",
    "year": 2024,
    "published": "2024-01-14T20:00:37Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "In the realm of digital media, the advent of AI-generated synthetic images has introduced significant challenges in distinguishing between real and fabricated visual content. These images, often indistinguishable from authentic ones, pose a threat to the credibility of digital media, with potential implications for disinformation and fraud. Our research addresses this challenge by employing machine learning techniques to discern between AI-generated and genuine images. Central to our approach is",
    "arxiv_url": "https://arxiv.org/abs/2401.07358v2",
    "pdf_url": "https://arxiv.org/pdf/2401.07358v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2401.07358",
    "arxiv_authors": [
      "Yuyang Wang",
      "Yizhi Hao",
      "Amando Xu Cong"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Harnessing+Machine+Learning+for+Discerning+AI-Generated+Synthetic+Images+Yuyang+Wang+Yizhi+Hao+Amando+Xu+Cong",
    "gs_search_success": true,
    "gs_authors": [
      "QoIq-08AAAAJ"
    ],
    "citation_count": 16,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2505.23743",
    "title": "DarkDiff: Advancing Low-Light Raw Enhancement by Retasking Diffusion Models for Camera ISP",
    "year": 2025,
    "published": "2025-05-29T17:58:48Z",
    "categories": [
      "cs.CV",
      "eess.IV"
    ],
    "abstract": "High-quality photography in extreme low-light conditions is challenging but impactful for digital cameras. With advanced computing hardware, traditional camera image signal processor (ISP) algorithms are gradually being replaced by efficient deep networks that enhance noisy raw images more intelligently. However, existing regression-based models often minimize pixel errors and result in oversmoothing of low-light photos or deep shadows. Recent work has attempted to address this limitation by tra",
    "arxiv_url": "https://arxiv.org/abs/2505.23743v1",
    "pdf_url": "https://arxiv.org/pdf/2505.23743v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2505.23743",
    "arxiv_authors": [
      "Amber Yijia Zheng",
      "Yu Zhang",
      "Jun Hu",
      "Raymond A. Yeh",
      "Chen Chen"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=DarkDiff%3A+Advancing+Low-Light+Raw+Enhancement+by+Retasking+Diffusion+Models+for+Camera+ISP+Amber+Yijia+Zheng+Yu+Zhang+Jun+Hu+Raymond+A.+Yeh+Chen+Chen",
    "gs_search_success": true,
    "gs_authors": [
      "SZQIVG0AAAAJ",
      "I22Gt-0AAAAJ",
      "7HDE1ZwAAAAJ",
      "X-VK_5EAAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": null
  },
  {
    "arxiv_id": "2410.11492",
    "title": "NavTopo: Leveraging Topological Maps For Autonomous Navigation Of a Mobile Robot",
    "year": 2024,
    "published": "2024-10-15T10:54:49Z",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV"
    ],
    "abstract": "Autonomous navigation of a mobile robot is a challenging task which requires ability of mapping, localization, path planning and path following. Conventional mapping methods build a dense metric map like an occupancy grid, which is affected by odometry error accumulation and consumes a lot of memory and computations in large environments. Another approach to mapping is the usage of topological properties, e.g. adjacency of locations in the environment. Topological maps are less prone to odometry",
    "arxiv_url": "https://arxiv.org/abs/2410.11492v1",
    "pdf_url": "https://arxiv.org/pdf/2410.11492v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2410.11492",
    "arxiv_authors": [
      "Kirill Muravyev",
      "Konstantin Yakovlev"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=NavTopo%3A+Leveraging+Topological+Maps+For+Autonomous+Navigation+Of+a+Mobile+Robot+Kirill+Muravyev+Konstantin+Yakovlev",
    "gs_search_success": true,
    "gs_authors": [
      "rlF2jXUAAAAJ",
      "Tw0A27kAAAAJ"
    ],
    "citation_count": 3,
    "gs_author_count": 2
  },
  {
    "arxiv_id": "2405.13285",
    "title": "Enhancing Active Learning for Sentinel 2 Imagery through Contrastive Learning and Uncertainty Estimation",
    "year": 2024,
    "published": "2024-05-22T01:54:51Z",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "In this paper, we introduce a novel method designed to enhance label efficiency in satellite imagery analysis by integrating semi-supervised learning (SSL) with active learning strategies. Our approach utilizes contrastive learning together with uncertainty estimations via Monte Carlo Dropout (MC Dropout), with a particular focus on Sentinel-2 imagery analyzed using the Eurosat dataset. We explore the effectiveness of our method in scenarios featuring both balanced and unbalanced class distribut",
    "arxiv_url": "https://arxiv.org/abs/2405.13285v2",
    "pdf_url": "https://arxiv.org/pdf/2405.13285v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2405.13285",
    "arxiv_authors": [
      "David Pogorzelski",
      "Peter Arlinghaus",
      "Wenyan Zhang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Enhancing+Active+Learning+for+Sentinel+2+Imagery+through+Contrastive+Learning+and+Uncertainty+Estimation+David+Pogorzelski+Peter+Arlinghaus+Wenyan+Zhang",
    "gs_search_success": true,
    "gs_authors": [
      "N9jPHQYAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2503.10195",
    "title": "ST-FlowNet: An Efficient Spiking Neural Network for Event-Based Optical Flow Estimation",
    "year": 2025,
    "published": "2025-03-13T09:28:42Z",
    "categories": [
      "cs.CV",
      "cs.NE",
      "q-bio.NC"
    ],
    "abstract": "Spiking Neural Networks (SNNs) have emerged as a promising tool for event-based optical flow estimation tasks due to their ability to leverage spatio-temporal information and low-power capabilities. However, the performance of SNN models is often constrained, limiting their application in real-world scenarios. In this work, we address this gap by proposing a novel neural network architecture, ST-FlowNet, specifically tailored for optical flow estimation from event-based data. The ST-FlowNet arch",
    "arxiv_url": "https://arxiv.org/abs/2503.10195v2",
    "pdf_url": "https://arxiv.org/pdf/2503.10195v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2503.10195",
    "arxiv_authors": [
      "Hongze Sun",
      "Jun Wang",
      "Wuque Cai",
      "Duo Chen",
      "Qianqian Liao",
      "Jiayi He",
      "Yan Cui",
      "Dezhong Yao",
      "Daqing Guo"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=ST-FlowNet%3A+An+Efficient+Spiking+Neural+Network+for+Event-Based+Optical+Flow+Estimation+Hongze+Sun+Jun+Wang+Wuque+Cai+Duo+Chen+Qianqian+Liao",
    "gs_search_success": true,
    "gs_authors": [
      "Bk8zcvsAAAAJ",
      "wotvHt4AAAAJ",
      "r3XU9PEAAAAJ",
      "ClUoWqsAAAAJ",
      "RNa8D1sAAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 9
  },
  {
    "arxiv_id": "2501.02872",
    "title": "Two-Dimensional Unknown View Tomography from Unknown Angle Distributions",
    "year": 2025,
    "published": "2025-01-06T09:27:08Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "This study presents a technique for 2D tomography under unknown viewing angles when the distribution of the viewing angles is also unknown. Unknown view tomography (UVT) is a problem encountered in cryo-electron microscopy and in the geometric calibration of CT systems. There exists a moderate-sized literature on the 2D UVT problem, but most existing 2D UVT algorithms assume knowledge of the angle distribution which is not available usually. Our proposed methodology formulates the problem as an ",
    "arxiv_url": "https://arxiv.org/abs/2501.02872v1",
    "pdf_url": "https://arxiv.org/pdf/2501.02872v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2501.02872",
    "arxiv_authors": [
      "Kaishva Chintan Shah",
      "Karthik S. Gurumoorthy",
      "Ajit Rajwade"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Two-Dimensional+Unknown+View+Tomography+from+Unknown+Angle+Distributions+Kaishva+Chintan+Shah+Karthik+S.+Gurumoorthy+Ajit+Rajwade",
    "gs_search_success": true,
    "gs_authors": [
      "6BysJLEAAAAJ",
      "aHprC8MAAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": null
  },
  {
    "arxiv_id": "2403.15770",
    "title": "Graph Image Prior for Unsupervised Dynamic Cardiac Cine MRI Reconstruction",
    "year": 2024,
    "published": "2024-03-23T08:57:46Z",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "abstract": "The inductive bias of the convolutional neural network (CNN) can be a strong prior for image restoration, which is known as the Deep Image Prior (DIP). Recently, DIP is utilized in unsupervised dynamic MRI reconstruction, which adopts a generative model from the latent space to the image space. However, existing methods usually use a pyramid-shaped CNN generator shared by all frames, embedding the temporal modeling within the latent space, which may hamper the model expression capability. In thi",
    "arxiv_url": "https://arxiv.org/abs/2403.15770v3",
    "pdf_url": "https://arxiv.org/pdf/2403.15770v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2403.15770",
    "arxiv_authors": [
      "Zhongsen Li",
      "Wenxuan Chen",
      "Shuai Wang",
      "Chuyu Liu",
      "Qing Zou",
      "Rui Li"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Graph+Image+Prior+for+Unsupervised+Dynamic+Cardiac+Cine+MRI+Reconstruction+Zhongsen+Li+Wenxuan+Chen+Shuai+Wang+Chuyu+Liu+Qing+Zou",
    "gs_search_success": true,
    "gs_authors": [
      "0mXK3C4AAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2303.10542",
    "title": "Wheat Head Counting by Estimating a Density Map with Convolutional Neural Networks",
    "year": 2023,
    "published": "2023-03-19T02:45:53Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Wheat is one of the most significant crop species with an annual worldwide grain production of 700 million tonnes. Assessing the production of wheat spikes can help us measure the grain production. Thus, detecting and characterizing spikes from images of wheat fields is an essential component in a wheat breeding process. In this study, we propose three wheat head counting networks (WHCNet\\_1, WHCNet\\_2 and WHCNet\\_3) to accurately estimate the wheat head count from an individual image and constr",
    "arxiv_url": "https://arxiv.org/abs/2303.10542v1",
    "pdf_url": "https://arxiv.org/pdf/2303.10542v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2303.10542",
    "arxiv_authors": [
      "Hongyu Guo"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Wheat+Head+Counting+by+Estimating+a+Density+Map+with+Convolutional+Neural+Networks+Hongyu+Guo",
    "gs_search_success": true,
    "gs_authors": [],
    "citation_count": 2,
    "gs_author_count": 1
  },
  {
    "arxiv_id": "2311.13993",
    "title": "EIGEN: Expert-Informed Joint Learning Aggregation for High-Fidelity Information Extraction from Document Images",
    "year": 2023,
    "published": "2023-11-23T13:20:42Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Information Extraction (IE) from document images is challenging due to the high variability of layout formats. Deep models such as LayoutLM and BROS have been proposed to address this problem and have shown promising results. However, they still require a large amount of field-level annotations for training these models. Other approaches using rule-based methods have also been proposed based on the understanding of the layout and semantics of a form such as geometric position, or type of the fie",
    "arxiv_url": "https://arxiv.org/abs/2311.13993v1",
    "pdf_url": "https://arxiv.org/pdf/2311.13993v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2311.13993",
    "arxiv_authors": [
      "Abhishek Singh",
      "Venkatapathy Subramanian",
      "Ayush Maheshwari",
      "Pradeep Narayan",
      "Devi Prasad Shetty",
      "Ganesh Ramakrishnan"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=EIGEN%3A+Expert-Informed+Joint+Learning+Aggregation+for+High-Fidelity+Information+Extraction+from+Document+Images+Abhishek+Singh+Venkatapathy+Subramanian+Ayush+Maheshwari+Pradeep+Narayan+Devi+Prasad+Shetty",
    "gs_search_success": true,
    "gs_authors": [
      "7E4Vjm0AAAAJ",
      "9pyqZXMAAAAJ",
      "W1ZpREMAAAAJ",
      "wwtbXH0AAAAJ"
    ],
    "citation_count": 2,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2409.03600",
    "title": "TCDiff: Triple Condition Diffusion Model with 3D Constraints for Stylizing Synthetic Faces",
    "year": 2024,
    "published": "2024-09-05T14:59:41Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "A robust face recognition model must be trained using datasets that include a large number of subjects and numerous samples per subject under varying conditions (such as pose, expression, age, noise, and occlusion). Due to ethical and privacy concerns, large-scale real face datasets have been discontinued, such as MS1MV3, and synthetic face generators have been proposed, utilizing GANs and Diffusion Models, such as SYNFace, SFace, DigiFace-1M, IDiff-Face, DCFace, and GANDiffFace, aiming to suppl",
    "arxiv_url": "https://arxiv.org/abs/2409.03600v1",
    "pdf_url": "https://arxiv.org/pdf/2409.03600v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2409.03600",
    "arxiv_authors": [
      "Bernardo Biesseck",
      "Pedro Vidal",
      "Luiz Coelho",
      "Roger Granada",
      "David Menotti|"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=TCDiff%3A+Triple+Condition+Diffusion+Model+with+3D+Constraints+for+Stylizing+Synthetic+Faces+Bernardo+Biesseck+Pedro+Vidal+Luiz+Coelho+Roger+Granada+David+Menotti%7C",
    "gs_search_success": true,
    "gs_authors": [
      "Ipu5_-gAAAAJ",
      "CMM8xHUAAAAJ",
      "VopZ0M8AAAAJ",
      "kuk1sSgAAAAJ",
      "vctCBA4AAAAJ"
    ],
    "citation_count": 2,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2504.01008",
    "title": "IntrinsiX: High-Quality PBR Generation using Image Priors",
    "year": 2025,
    "published": "2025-04-01T17:47:48Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "We introduce IntrinsiX, a novel method that generates high-quality intrinsic images from text description. In contrast to existing text-to-image models whose outputs contain baked-in scene lighting, our approach predicts physically-based rendering (PBR) maps. This enables the generated outputs to be used for content creation scenarios in core graphics applications that facilitate re-lighting, editing, and texture generation tasks. In order to train our generator, we exploit strong image priors, ",
    "arxiv_url": "https://arxiv.org/abs/2504.01008v2",
    "pdf_url": "https://arxiv.org/pdf/2504.01008v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2504.01008",
    "arxiv_authors": [
      "Peter Kocsis",
      "Lukas Höllein",
      "Matthias Nießner"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=IntrinsiX%3A+High-Quality+PBR+Generation+using+Image+Priors+Peter+Kocsis+Lukas+H%C3%B6llein+Matthias+Nie%C3%9Fner",
    "gs_search_success": true,
    "gs_authors": [
      "pv8a-lsAAAAJ",
      "eUtEs6YAAAAJ",
      "Te0MGV4AAAAJ"
    ],
    "citation_count": 3,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2503.14701",
    "title": "ARC-Calib: Autonomous Markerless Camera-to-Robot Calibration via Exploratory Robot Motions",
    "year": 2025,
    "published": "2025-03-18T20:03:32Z",
    "categories": [
      "cs.RO",
      "cs.CV"
    ],
    "abstract": "Camera-to-robot (also known as eye-to-hand) calibration is a critical component of vision-based robot manipulation. Traditional marker-based methods often require human intervention for system setup. Furthermore, existing autonomous markerless calibration methods typically rely on pre-trained robot tracking models that impede their application on edge devices and require fine-tuning for novel robot embodiments. To address these limitations, this paper proposes a model-based markerless camera-to-",
    "arxiv_url": "https://arxiv.org/abs/2503.14701v1",
    "pdf_url": "https://arxiv.org/pdf/2503.14701v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2503.14701",
    "arxiv_authors": [
      "Podshara Chanrungmaneekul",
      "Yiting Chen",
      "Joshua T. Grace",
      "Aaron M. Dollar",
      "Kaiyu Hang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=ARC-Calib%3A+Autonomous+Markerless+Camera-to-Robot+Calibration+via+Exploratory+Robot+Motions+Podshara+Chanrungmaneekul+Yiting+Chen+Joshua+T.+Grace+Aaron+M.+Dollar+Kaiyu+Hang",
    "gs_search_success": true,
    "gs_authors": [
      "GrgH1lQAAAAJ",
      "jFekO3IAAAAJ",
      "iz-6HCEAAAAJ",
      "UA5y84MAAAAJ",
      "Hx2howwAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2407.05610",
    "title": "Described Spatial-Temporal Video Detection",
    "year": 2024,
    "published": "2024-07-08T04:54:39Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Detecting visual content on language expression has become an emerging topic in the community. However, in the video domain, the existing setting, i.e., spatial-temporal video grounding (STVG), is formulated to only detect one pre-existing object in each frame, ignoring the fact that language descriptions can involve none or multiple entities within a video. In this work, we advance the STVG to a more practical setting called described spatial-temporal video detection (DSTVD) by overcoming the a",
    "arxiv_url": "https://arxiv.org/abs/2407.05610v1",
    "pdf_url": "https://arxiv.org/pdf/2407.05610v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2407.05610",
    "arxiv_authors": [
      "Wei Ji",
      "Xiangyan Liu",
      "Yingfei Sun",
      "Jiajun Deng",
      "You Qin",
      "Ammar Nuwanna",
      "Mengyao Qiu",
      "Lina Wei",
      "Roger Zimmermann"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Described+Spatial-Temporal+Video+Detection+Wei+Ji+Xiangyan+Liu+Yingfei+Sun+Jiajun+Deng+You+Qin",
    "gs_search_success": true,
    "gs_authors": [
      "mna2LlkAAAAJ",
      "IDREwXEAAAAJ",
      "qC39v8kAAAAJ",
      "Oy-s998AAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 9
  },
  {
    "arxiv_id": "2504.04495",
    "title": "AVadCLIP: Audio-Visual Collaboration for Robust Video Anomaly Detection",
    "year": 2025,
    "published": "2025-04-06T13:59:16Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "With the increasing adoption of video anomaly detection in intelligent surveillance domains, conventional visual-based detection approaches often struggle with information insufficiency and high false-positive rates in complex environments. To address these limitations, we present a novel weakly supervised framework that leverages audio-visual collaboration for robust video anomaly detection. Capitalizing on the exceptional cross-modal representation learning capabilities of Contrastive Language",
    "arxiv_url": "https://arxiv.org/abs/2504.04495v2",
    "pdf_url": "https://arxiv.org/pdf/2504.04495v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2504.04495",
    "arxiv_authors": [
      "Peng Wu",
      "Wanshun Su",
      "Guansong Pang",
      "Yujia Sun",
      "Qingsen Yan",
      "Peng Wang",
      "Yanning Zhang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=AVadCLIP%3A+Audio-Visual+Collaboration+for+Robust+Video+Anomaly+Detection+Peng+Wu+Wanshun+Su+Guansong+Pang+Yujia+Sun+Qingsen+Yan",
    "gs_search_success": true,
    "gs_authors": [
      "aPLp7pAAAAAJ",
      "QkNqUH4AAAAJ",
      "1ZO7pHkAAAAJ",
      "BSGy3foAAAAJ"
    ],
    "citation_count": 4,
    "gs_author_count": 7
  },
  {
    "arxiv_id": "2406.13815",
    "title": "IG-CFAT: An Improved GAN-Based Framework for Effectively Exploiting Transformers in Real-World Image Super-Resolution",
    "year": 2024,
    "published": "2024-06-19T20:21:26Z",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "abstract": "In the field of single image super-resolution (SISR), transformer-based models, have demonstrated significant advancements. However, the potential and efficiency of these models in applied fields such as real-world image super-resolution have been less noticed and there are substantial opportunities for improvement. Recently, composite fusion attention transformer (CFAT), outperformed previous state-of-the-art (SOTA) models in classic image super-resolution. In this paper, we propose a novel GAN",
    "arxiv_url": "https://arxiv.org/abs/2406.13815v4",
    "pdf_url": "https://arxiv.org/pdf/2406.13815v4",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2406.13815",
    "arxiv_authors": [
      "Alireza Aghelan",
      "Ali Amiryan",
      "Abolfazl Zarghani",
      "Modjtaba Rouhani"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=IG-CFAT%3A+An+Improved+GAN-Based+Framework+for+Effectively+Exploiting+Transformers+in+Real-World+Image+Super-Resolution+Alireza+Aghelan+Ali+Amiryan+Abolfazl+Zarghani+Modjtaba+Rouhani",
    "gs_search_success": true,
    "gs_authors": [
      "siQM3_kAAAAJ",
      "nPuuMikAAAAJ",
      "cz3oBQIAAAAJ"
    ],
    "citation_count": 2,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2403.08433",
    "title": "An Empirical Study of Parameter Efficient Fine-tuning on Vision-Language Pre-train Model",
    "year": 2024,
    "published": "2024-03-13T11:33:38Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Recent studies applied Parameter Efficient Fine-Tuning techniques (PEFTs) to efficiently narrow the performance gap between pre-training and downstream. There are two important factors for various PEFTs, namely, the accessible data size and fine-tunable parameter size. A natural expectation for PEFTs is that the performance of various PEFTs is positively related to the data size and fine-tunable parameter size. However, according to the evaluation of five PEFTs on two downstream vision-language ",
    "arxiv_url": "https://arxiv.org/abs/2403.08433v2",
    "pdf_url": "https://arxiv.org/pdf/2403.08433v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2403.08433",
    "arxiv_authors": [
      "Yuxin Tian",
      "Mouxing Yang",
      "Yunfan Li",
      "Dayiheng Liu",
      "Xingzhang Ren",
      "Xi Peng",
      "Jiancheng Lv"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=An+Empirical+Study+of+Parameter+Efficient+Fine-tuning+on+Vision-Language+Pre-train+Model+Yuxin+Tian+Mouxing+Yang+Yunfan+Li+Dayiheng+Liu+Xingzhang+Ren",
    "gs_search_success": true,
    "gs_authors": [
      "n36mg0QAAAAJ",
      "JmXIt5oAAAAJ",
      "IaRmgrEAAAAJ",
      "pPLQrX4AAAAJ",
      "0TCaWKwAAAAJ",
      "bw9FOHAAAAAJ",
      "3YzSsyIAAAAJ"
    ],
    "citation_count": 2,
    "gs_author_count": 7
  },
  {
    "arxiv_id": "2411.11706",
    "title": "MC-LLaVA: Multi-Concept Personalized Vision-Language Model",
    "year": 2024,
    "published": "2024-11-18T16:33:52Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Current vision-language models (VLMs) show exceptional abilities across diverse tasks, such as visual question answering. To enhance user experience, recent studies investigate VLM personalization to understand user-provided concepts. However, they mainly focus on single-concept personalization, neglecting the existence and interplay of multiple concepts, which limits real-world applicability. This paper proposes the first multi-concept personalization paradigm, MC-LLaVA. Specifically, MC-LLaVA ",
    "arxiv_url": "https://arxiv.org/abs/2411.11706v3",
    "pdf_url": "https://arxiv.org/pdf/2411.11706v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2411.11706",
    "arxiv_authors": [
      "Ruichuan An",
      "Sihan Yang",
      "Ming Lu",
      "Renrui Zhang",
      "Kai Zeng",
      "Yulin Luo",
      "Jiajun Cao",
      "Hao Liang",
      "Ying Chen",
      "Qi She",
      "Shanghang Zhang",
      "Wentao Zhang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=MC-LLaVA%3A+Multi-Concept+Personalized+Vision-Language+Model+Ruichuan+An+Sihan+Yang+Ming+Lu+Renrui+Zhang+Kai+Zeng",
    "gs_search_success": true,
    "gs_authors": [
      "SgeV4NkAAAAJ",
      "R5iSLPQAAAAJ",
      "BfG6jnoAAAAJ",
      "HgapY3sAAAAJ",
      "iHoGTt4AAAAJ",
      "YlL3xN4AAAAJ",
      "voqw10cAAAAJ",
      "femNsd0AAAAJ"
    ],
    "citation_count": 37,
    "gs_author_count": 11
  },
  {
    "arxiv_id": "2504.07745",
    "title": "SF2T: Self-supervised Fragment Finetuning of Video-LLMs for Fine-Grained Understanding",
    "year": 2025,
    "published": "2025-04-10T13:40:34Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Video-based Large Language Models (Video-LLMs) have witnessed substantial advancements in recent years, propelled by the advancement in multi-modal LLMs. Although these models have demonstrated proficiency in providing the overall description of videos, they struggle with fine-grained understanding, particularly in aspects such as visual dynamics and video details inquiries. To tackle these shortcomings, we find that fine-tuning Video-LLMs on self-supervised fragment tasks, greatly improve their",
    "arxiv_url": "https://arxiv.org/abs/2504.07745v1",
    "pdf_url": "https://arxiv.org/pdf/2504.07745v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2504.07745",
    "arxiv_authors": [
      "Yangliu Hu",
      "Zikai Song",
      "Na Feng",
      "Yawei Luo",
      "Junqing Yu",
      "Yi-Ping Phoebe Chen",
      "Wei Yang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=SF2T%3A+Self-supervised+Fragment+Finetuning+of+Video-LLMs+for+Fine-Grained+Understanding+Yangliu+Hu+Zikai+Song+Na+Feng+Yawei+Luo+Junqing+Yu",
    "gs_search_success": true,
    "gs_authors": [
      "1qnuOZsAAAAJ",
      "pnVwaGsAAAAJ",
      "Vt5edEkAAAAJ",
      "_UjqBfcAAAAJ",
      "rhsNpmkAAAAJ"
    ],
    "citation_count": 6,
    "gs_author_count": 7
  },
  {
    "arxiv_id": "2303.14526",
    "title": "Selective Structured State-Spaces for Long-Form Video Understanding",
    "year": 2023,
    "published": "2023-03-25T17:47:12Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Effective modeling of complex spatiotemporal dependencies in long-form videos remains an open problem. The recently proposed Structured State-Space Sequence (S4) model with its linear complexity offers a promising direction in this space. However, we demonstrate that treating all image-tokens equally as done by S4 model can adversely affect its efficiency and accuracy. To address this limitation, we present a novel Selective S4 (i.e., S5) model that employs a lightweight mask generator to adapti",
    "arxiv_url": "https://arxiv.org/abs/2303.14526v1",
    "pdf_url": "https://arxiv.org/pdf/2303.14526v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2303.14526",
    "arxiv_authors": [
      "Jue Wang",
      "Wentao Zhu",
      "Pichao Wang",
      "Xiang Yu",
      "Linda Liu",
      "Mohamed Omar",
      "Raffay Hamid"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Selective+Structured+State-Spaces+for+Long-Form+Video+Understanding+Jue+Wang+Wentao+Zhu+Pichao+Wang+Xiang+Yu+Linda+Liu",
    "gs_search_success": true,
    "gs_authors": [
      "Lt945BwAAAAJ",
      "4OqjHMgAAAAJ",
      "QozdnnoAAAAJ",
      "2hjYfqIAAAAJ",
      "gdD2E9QAAAAJ"
    ],
    "citation_count": 192,
    "gs_author_count": 7
  },
  {
    "arxiv_id": "2408.01701",
    "title": "Signal-SGN: A Spiking Graph Convolutional Network for Skeletal Action Recognition via Learning Temporal-Frequency Dynamics",
    "year": 2024,
    "published": "2024-08-03T07:47:16Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "For multimodal skeleton-based action recognition, Graph Convolutional Networks (GCNs) are effective models. Still, their reliance on floating-point computations leads to high energy consumption, limiting their applicability in battery-powered devices. While energy-efficient, Spiking Neural Networks (SNNs) struggle to model skeleton dynamics, leading to suboptimal solutions. We propose Signal-SGN (Spiking Graph Convolutional Network), which utilizes the temporal dimension of skeleton sequences as",
    "arxiv_url": "https://arxiv.org/abs/2408.01701v6",
    "pdf_url": "https://arxiv.org/pdf/2408.01701v6",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2408.01701",
    "arxiv_authors": [
      "Naichuan Zheng",
      "Yuchen Du",
      "Hailun Xia",
      "Zeyu Liang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Signal-SGN%3A+A+Spiking+Graph+Convolutional+Network+for+Skeletal+Action+Recognition+via+Learning+Temporal-Frequency+Dynamics+Naichuan+Zheng+Yuchen+Du+Hailun+Xia+Zeyu+Liang",
    "gs_search_success": true,
    "gs_authors": [
      "u_ZyTLsAAAAJ"
    ],
    "citation_count": 2,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2403.10064",
    "title": "Progressive Divide-and-Conquer via Subsampling Decomposition for Accelerated MRI",
    "year": 2024,
    "published": "2024-03-15T07:14:01Z",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "abstract": "Deep unfolding networks (DUN) have emerged as a popular iterative framework for accelerated magnetic resonance imaging (MRI) reconstruction. However, conventional DUN aims to reconstruct all the missing information within the entire null space in each iteration. Thus it could be challenging when dealing with highly ill-posed degradation, usually leading to unsatisfactory reconstruction. In this work, we propose a Progressive Divide-And-Conquer (PDAC) strategy, aiming to break down the subsamplin",
    "arxiv_url": "https://arxiv.org/abs/2403.10064v1",
    "pdf_url": "https://arxiv.org/pdf/2403.10064v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2403.10064",
    "arxiv_authors": [
      "Chong Wang",
      "Lanqing Guo",
      "Yufei Wang",
      "Hao Cheng",
      "Yi Yu",
      "Bihan Wen"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Progressive+Divide-and-Conquer+via+Subsampling+Decomposition+for+Accelerated+MRI+Chong+Wang+Lanqing+Guo+Yufei+Wang+Hao+Cheng+Yi+Yu",
    "gs_search_success": true,
    "gs_authors": [
      "jLd1l_sAAAAJ",
      "ypkClpwAAAAJ",
      "QlZK_hQAAAAJ",
      "8rkIFHcAAAAJ",
      "X_z2xDEAAAAJ",
      "7Bv4y4YAAAAJ"
    ],
    "citation_count": 20,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2307.03212",
    "title": "Attentive Graph Enhanced Region Representation Learning",
    "year": 2023,
    "published": "2023-07-06T16:38:43Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CY",
      "cs.LG"
    ],
    "abstract": "Representing urban regions accurately and comprehensively is essential for various urban planning and analysis tasks. Recently, with the expansion of the city, modeling long-range spatial dependencies with multiple data sources plays an important role in urban region representation. In this paper, we propose the Attentive Graph Enhanced Region Representation Learning (ATGRL) model, which aims to capture comprehensive dependencies from multiple graphs and learn rich semantic representations of ur",
    "arxiv_url": "https://arxiv.org/abs/2307.03212v3",
    "pdf_url": "https://arxiv.org/pdf/2307.03212v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2307.03212",
    "arxiv_authors": [
      "Weiliang Chen",
      "Qianqian Ren",
      "Jinbao Li"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Attentive+Graph+Enhanced+Region+Representation+Learning+Weiliang+Chen+Qianqian+Ren+Jinbao+Li",
    "gs_search_success": true,
    "gs_authors": [],
    "citation_count": null,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2504.10278",
    "title": "DiffMOD: Progressive Diffusion Point Denoising for Moving Object Detection in Remote Sensing",
    "year": 2025,
    "published": "2025-04-14T14:44:52Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Moving object detection (MOD) in remote sensing is significantly challenged by low resolution, extremely small object sizes, and complex noise interference. Current deep learning-based MOD methods rely on probability density estimation, which restricts flexible information interaction between objects and across temporal frames. To flexibly capture high-order inter-object and temporal relationships, we propose a point-based MOD in remote sensing. Inspired by diffusion models, the network optimiza",
    "arxiv_url": "https://arxiv.org/abs/2504.10278v1",
    "pdf_url": "https://arxiv.org/pdf/2504.10278v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2504.10278",
    "arxiv_authors": [
      "Jinyue Zhang",
      "Xiangrong Zhang",
      "Zhongjian Huang",
      "Tianyang Zhang",
      "Yifei Jiang",
      "Licheng Jiao"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=DiffMOD%3A+Progressive+Diffusion+Point+Denoising+for+Moving+Object+Detection+in+Remote+Sensing+Jinyue+Zhang+Xiangrong+Zhang+Zhongjian+Huang+Tianyang+Zhang+Yifei+Jiang",
    "gs_search_success": false,
    "gs_authors": [],
    "citation_count": null,
    "gs_author_count": null,
    "error_type": "fetch_failed"
  },
  {
    "arxiv_id": "2308.08431",
    "title": "Integrating Visual and Semantic Similarity Using Hierarchies for Image Retrieval",
    "year": 2023,
    "published": "2023-08-16T15:23:14Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Most of the research in content-based image retrieval (CBIR) focus on developing robust feature representations that can effectively retrieve instances from a database of images that are visually similar to a query. However, the retrieved images sometimes contain results that are not semantically related to the query. To address this, we propose a method for CBIR that captures both visual and semantic similarity using a visual hierarchy. The hierarchy is constructed by merging classes with overl",
    "arxiv_url": "https://arxiv.org/abs/2308.08431v1",
    "pdf_url": "https://arxiv.org/pdf/2308.08431v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2308.08431",
    "arxiv_authors": [
      "Aishwarya Venkataramanan",
      "Martin Laviale",
      "Cédric Pradalier"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Integrating+Visual+and+Semantic+Similarity+Using+Hierarchies+for+Image+Retrieval+Aishwarya+Venkataramanan+Martin+Laviale+C%C3%A9dric+Pradalier",
    "gs_search_success": true,
    "gs_authors": [
      "IqlI43cAAAAJ",
      "xedn-i8AAAAJ",
      "4_1DZoYAAAAJ"
    ],
    "citation_count": 4,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2503.17728",
    "title": "DynASyn: Multi-Subject Personalization Enabling Dynamic Action Synthesis",
    "year": 2025,
    "published": "2025-03-22T10:56:35Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Recent advances in text-to-image diffusion models spurred research on personalization, i.e., a customized image synthesis, of subjects within reference images. Although existing personalization methods are able to alter the subjects' positions or to personalize multiple subjects simultaneously, they often struggle to modify the behaviors of subjects or their dynamic interactions. The difficulty is attributable to overfitting to reference images, which worsens if only a single reference image is ",
    "arxiv_url": "https://arxiv.org/abs/2503.17728v1",
    "pdf_url": "https://arxiv.org/pdf/2503.17728v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2503.17728",
    "arxiv_authors": [
      "Yongjin Choi",
      "Chanhun Park",
      "Seung Jun Baek"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=DynASyn%3A+Multi-Subject+Personalization+Enabling+Dynamic+Action+Synthesis+Yongjin+Choi+Chanhun+Park+Seung+Jun+Baek",
    "gs_search_success": true,
    "gs_authors": [
      "lcd-oTsAAAAJ",
      "F6NzxRIAAAAJ"
    ],
    "citation_count": 2,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2403.14729",
    "title": "Auto-Train-Once: Controller Network Guided Automatic Network Pruning from Scratch",
    "year": 2024,
    "published": "2024-03-21T02:33:37Z",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "Current techniques for deep neural network (DNN) pruning often involve intricate multi-step processes that require domain-specific expertise, making their widespread adoption challenging. To address the limitation, the Only-Train-Once (OTO) and OTOv2 are proposed to eliminate the need for additional fine-tuning steps by directly training and compressing a general DNN from scratch. Nevertheless, the static design of optimizers (in OTO) can lead to convergence issues of local optima. In this paper",
    "arxiv_url": "https://arxiv.org/abs/2403.14729v1",
    "pdf_url": "https://arxiv.org/pdf/2403.14729v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2403.14729",
    "arxiv_authors": [
      "Xidong Wu",
      "Shangqian Gao",
      "Zeyu Zhang",
      "Zhenzhen Li",
      "Runxue Bao",
      "Yanfu Zhang",
      "Xiaoqian Wang",
      "Heng Huang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Auto-Train-Once%3A+Controller+Network+Guided+Automatic+Network+Pruning+from+Scratch+Xidong+Wu+Shangqian+Gao+Zeyu+Zhang+Zhenzhen+Li+Runxue+Bao",
    "gs_search_success": true,
    "gs_authors": [
      "rj21L7sAAAAJ",
      "ZPBYEngAAAAJ",
      "k_fCQhkAAAAJ",
      "4OqLaDwAAAAJ",
      "9mNI83oAAAAJ",
      "I3tc214AAAAJ",
      "jlcUOeoAAAAJ",
      "6LYI6uUAAAAJ"
    ],
    "citation_count": 41,
    "gs_author_count": 8
  },
  {
    "arxiv_id": "2410.10412",
    "title": "4DStyleGaussian: Zero-shot 4D Style Transfer with Gaussian Splatting",
    "year": 2024,
    "published": "2024-10-14T12:03:00Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "3D neural style transfer has gained significant attention for its potential to provide user-friendly stylization with spatial consistency. However, existing 3D style transfer methods often fall short in terms of inference efficiency, generalization ability, and struggle to handle dynamic scenes with temporal consistency. In this paper, we introduce 4DStyleGaussian, a novel 4D style transfer framework designed to achieve real-time stylization of arbitrary style references while maintaining reason",
    "arxiv_url": "https://arxiv.org/abs/2410.10412v1",
    "pdf_url": "https://arxiv.org/pdf/2410.10412v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2410.10412",
    "arxiv_authors": [
      "Wanlin Liang",
      "Hongbin Xu",
      "Weitao Chen",
      "Feng Xiao",
      "Wenxiong Kang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=4DStyleGaussian%3A+Zero-shot+4D+Style+Transfer+with+Gaussian+Splatting+Wanlin+Liang+Hongbin+Xu+Weitao+Chen+Feng+Xiao+Wenxiong+Kang",
    "gs_search_success": true,
    "gs_authors": [
      "LMt_WpcAAAAJ",
      "mRC_emoAAAAJ",
      "meU7EOAAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2408.04426",
    "title": "A Review of 3D Reconstruction Techniques for Deformable Tissues in Robotic Surgery",
    "year": 2024,
    "published": "2024-08-08T12:51:23Z",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "abstract": "As a crucial and intricate task in robotic minimally invasive surgery, reconstructing surgical scenes using stereo or monocular endoscopic video holds immense potential for clinical applications. NeRF-based techniques have recently garnered attention for the ability to reconstruct scenes implicitly. On the other hand, Gaussian splatting-based 3D-GS represents scenes explicitly using 3D Gaussians and projects them onto a 2D plane as a replacement for the complex volume rendering in NeRF. However,",
    "arxiv_url": "https://arxiv.org/abs/2408.04426v1",
    "pdf_url": "https://arxiv.org/pdf/2408.04426v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2408.04426",
    "arxiv_authors": [
      "Mengya Xu",
      "Ziqi Guo",
      "An Wang",
      "Long Bai",
      "Hongliang Ren"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=A+Review+of+3D+Reconstruction+Techniques+for+Deformable+Tissues+in+Robotic+Surgery+Mengya+Xu+Ziqi+Guo+An+Wang+Long+Bai+Hongliang+Ren",
    "gs_search_success": true,
    "gs_authors": [
      "rcF7N44AAAAJ",
      "Uq5qvyAAAAAJ",
      "v-fw-98AAAAJ",
      "NYWnE6QAAAAJ"
    ],
    "citation_count": 6,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2311.08949",
    "title": "Automated Volume Corrected Mitotic Index Calculation Through Annotation-Free Deep Learning using Immunohistochemistry as Reference Standard",
    "year": 2023,
    "published": "2023-11-15T13:35:40Z",
    "categories": [
      "eess.IV",
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "The volume-corrected mitotic index (M/V-Index) was shown to provide prognostic value in invasive breast carcinomas. However, despite its prognostic significance, it is not established as the standard method for assessing aggressive biological behaviour, due to the high additional workload associated with determining the epithelial proportion. In this work, we show that using a deep learning pipeline solely trained with an annotation-free, immunohistochemistry-based approach, provides accurate es",
    "arxiv_url": "https://arxiv.org/abs/2311.08949v1",
    "pdf_url": "https://arxiv.org/pdf/2311.08949v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2311.08949",
    "arxiv_authors": [
      "Jonas Ammeling",
      "Moritz Hecker",
      "Jonathan Ganz",
      "Taryn A. Donovan",
      "Christof A. Bertram",
      "Katharina Breininger",
      "Marc Aubreville"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Automated+Volume+Corrected+Mitotic+Index+Calculation+Through+Annotation-Free+Deep+Learning+using+Immunohistochemistry+as+Reference+Standard+Jonas+Ammeling+Moritz+Hecker+Jonathan+Ganz+Taryn+A.+Donovan+Christof+A.+Bertram",
    "gs_search_success": true,
    "gs_authors": [
      "gdVTIP4AAAAJ",
      "6iPoAXAAAAAJ",
      "ed-CTtgAAAAJ",
      "fIiy9V4AAAAJ",
      "YJrvIbsAAAAJ",
      "BehA6akAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 7
  },
  {
    "arxiv_id": "2403.04880",
    "title": "An Item is Worth a Prompt: Versatile Image Editing with Disentangled Control",
    "year": 2024,
    "published": "2024-03-07T20:06:29Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Building on the success of text-to-image diffusion models (DPMs), image editing is an important application to enable human interaction with AI-generated content. Among various editing methods, editing within the prompt space gains more attention due to its capacity and simplicity of controlling semantics. However, since diffusion models are commonly pretrained on descriptive text captions, direct editing of words in text prompts usually leads to completely different generated images, violating ",
    "arxiv_url": "https://arxiv.org/abs/2403.04880v4",
    "pdf_url": "https://arxiv.org/pdf/2403.04880v4",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2403.04880",
    "arxiv_authors": [
      "Aosong Feng",
      "Weikang Qiu",
      "Jinbin Bai",
      "Xiao Zhang",
      "Zhen Dong",
      "Kaicheng Zhou",
      "Rex Ying",
      "Leandros Tassiulas"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=An+Item+is+Worth+a+Prompt%3A+Versatile+Image+Editing+with+Disentangled+Control+Aosong+Feng+Weikang+Qiu+Jinbin+Bai+Xiao+Zhang+Zhen+Dong",
    "gs_search_success": true,
    "gs_authors": [
      "czxMUzcAAAAJ",
      "9qtgcZ8AAAAJ",
      "bqvp0PYAAAAJ",
      "hFhhrmgAAAAJ",
      "PAfNNrYAAAAJ",
      "6fqNXooAAAAJ",
      "OLRjhHAAAAAJ"
    ],
    "citation_count": 9,
    "gs_author_count": 8
  },
  {
    "arxiv_id": "2406.17323",
    "title": "XAMI -- A Benchmark Dataset for Artefact Detection in XMM-Newton Optical Images",
    "year": 2024,
    "published": "2024-06-25T07:14:15Z",
    "categories": [
      "cs.CV",
      "astro-ph.IM",
      "cs.LG"
    ],
    "abstract": "Reflected or scattered light produce artefacts in astronomical observations that can negatively impact the scientific study. Hence, automated detection of these artefacts is highly beneficial, especially with the increasing amounts of data gathered. Machine learning methods are well-suited to this problem, but currently there is a lack of annotated data to train such approaches to detect artefacts in astronomical observations. In this work, we present a dataset of images from the XMM-Newton spac",
    "arxiv_url": "https://arxiv.org/abs/2406.17323v3",
    "pdf_url": "https://arxiv.org/pdf/2406.17323v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2406.17323",
    "arxiv_authors": [
      "Elisabeta-Iulia Dima",
      "Pablo Gómez",
      "Sandor Kruk",
      "Peter Kretschmar",
      "Simon Rosen",
      "Călin-Adrian Popa"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=XAMI+--+A+Benchmark+Dataset+for+Artefact+Detection+in+XMM-Newton+Optical+Images+Elisabeta-Iulia+Dima+Pablo+G%C3%B3mez+Sandor+Kruk+Peter+Kretschmar+Simon+Rosen",
    "gs_search_success": true,
    "gs_authors": [
      "MJZYn2IAAAAJ",
      "3QasQr4AAAAJ",
      "0cTFIAQAAAAJ",
      "anjmlbwAAAAJ",
      "U6prQIkAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2311.18243",
    "title": "DKiS: Decay weight invertible image steganography with private key",
    "year": 2023,
    "published": "2023-11-30T04:21:10Z",
    "categories": [
      "cs.MM",
      "cs.CR",
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "Image steganography, defined as the practice of concealing information within another image, traditionally encounters security challenges when its methods become publicly known or are under attack. To address this, a novel private key-based image steganography technique has been introduced. This approach ensures the security of the hidden information, as access requires a corresponding private key, regardless of the public knowledge of the steganography method. Experimental evidence has been pre",
    "arxiv_url": "https://arxiv.org/abs/2311.18243v2",
    "pdf_url": "https://arxiv.org/pdf/2311.18243v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2311.18243",
    "arxiv_authors": [
      "Hang Yang",
      "Yitian Xu",
      "Xuhua Liu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=DKiS%3A+Decay+weight+invertible+image+steganography+with+private+key+Hang+Yang+Yitian+Xu+Xuhua+Liu",
    "gs_search_success": true,
    "gs_authors": [
      "usu8YBAAAAAJ"
    ],
    "citation_count": 7,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2505.12266",
    "title": "PMQ-VE: Progressive Multi-Frame Quantization for Video Enhancement",
    "year": 2025,
    "published": "2025-05-18T07:10:40Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Multi-frame video enhancement tasks aim to improve the spatial and temporal resolution and quality of video sequences by leveraging temporal information from multiple frames, which are widely used in streaming video processing, surveillance, and generation. Although numerous Transformer-based enhancement methods have achieved impressive performance, their computational and memory demands hinder deployment on edge devices. Quantization offers a practical solution by reducing the bit-width of weig",
    "arxiv_url": "https://arxiv.org/abs/2505.12266v2",
    "pdf_url": "https://arxiv.org/pdf/2505.12266v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2505.12266",
    "arxiv_authors": [
      "ZhanFeng Feng",
      "Long Peng",
      "Xin Di",
      "Yong Guo",
      "Wenbo Li",
      "Yulun Zhang",
      "Renjing Pei",
      "Yang Wang",
      "Yang Cao",
      "Zheng-Jun Zha"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=PMQ-VE%3A+Progressive+Multi-Frame+Quantization+for+Video+Enhancement+ZhanFeng+Feng+Long+Peng+Xin+Di+Yong+Guo+Wenbo+Li",
    "gs_search_success": true,
    "gs_authors": [
      "gDnBC1gAAAAJ",
      "7TFKCpQAAAAJ",
      "VmLZOc0AAAAJ",
      "2lzOXwQAAAAJ",
      "foGn_TIAAAAJ",
      "K7rTHNcAAAAJ"
    ],
    "citation_count": 10,
    "gs_author_count": 10
  },
  {
    "arxiv_id": "2310.14489",
    "title": "MSFormer: A Skeleton-multiview Fusion Method For Tooth Instance Segmentation",
    "year": 2023,
    "published": "2023-10-23T01:46:22Z",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "abstract": "Recently, deep learning-based tooth segmentation methods have been limited by the expensive and time-consuming processes of data collection and labeling. Achieving high-precision segmentation with limited datasets is critical. A viable solution to this entails fine-tuning pre-trained multiview-based models, thereby enhancing performance with limited data. However, relying solely on two-dimensional (2D) images for three-dimensional (3D) tooth segmentation can produce suboptimal outcomes because o",
    "arxiv_url": "https://arxiv.org/abs/2310.14489v1",
    "pdf_url": "https://arxiv.org/pdf/2310.14489v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2310.14489",
    "arxiv_authors": [
      "Yuan Li",
      "Huan Liu",
      "Yubo Tao",
      "Xiangyang He",
      "Haifeng Li",
      "Xiaohu Guo",
      "Hai Lin"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=MSFormer%3A+A+Skeleton-multiview+Fusion+Method+For+Tooth+Instance+Segmentation+Yuan+Li+Huan+Liu+Yubo+Tao+Xiangyang+He+Haifeng+Li",
    "gs_search_success": true,
    "gs_authors": [
      "6ai51UkAAAAJ",
      "Y4SdsvsAAAAJ",
      "Pl8UV9wAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 7
  },
  {
    "arxiv_id": "2307.08351",
    "title": "Neural Modulation Fields for Conditional Cone Beam Neural Tomography",
    "year": 2023,
    "published": "2023-07-17T09:41:01Z",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "abstract": "Conventional Computed Tomography (CT) methods require large numbers of noise-free projections for accurate density reconstructions, limiting their applicability to the more complex class of Cone Beam Geometry CT (CBCT) reconstruction. Recently, deep learning methods have been proposed to overcome these limitations, with methods based on neural fields (NF) showing strong performance, by approximating the reconstructed density through a continuous-in-space coordinate based neural network. Our focu",
    "arxiv_url": "https://arxiv.org/abs/2307.08351v1",
    "pdf_url": "https://arxiv.org/pdf/2307.08351v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2307.08351",
    "arxiv_authors": [
      "Samuele Papa",
      "David M. Knigge",
      "Riccardo Valperga",
      "Nikita Moriakov",
      "Miltos Kofinas",
      "Jan-Jakob Sonke",
      "Efstratios Gavves"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Neural+Modulation+Fields+for+Conditional+Cone+Beam+Neural+Tomography+Samuele+Papa+David+M.+Knigge+Riccardo+Valperga+Nikita+Moriakov+Miltos+Kofinas",
    "gs_search_success": true,
    "gs_authors": [
      "Ur5BV8MAAAAJ",
      "0c2MtX4AAAAJ",
      "IK64D1wAAAAJ",
      "QqfCvsgAAAAJ",
      "iyW85DwAAAAJ",
      "Csnj-pQAAAAJ",
      "iTVHO5oAAAAJ"
    ],
    "citation_count": 5,
    "gs_author_count": 7
  },
  {
    "arxiv_id": "2310.17167",
    "title": "Improving Denoising Diffusion Models via Simultaneous Estimation of Image and Noise",
    "year": 2023,
    "published": "2023-10-26T05:43:07Z",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "abstract": "This paper introduces two key contributions aimed at improving the speed and quality of images generated through inverse diffusion processes. The first contribution involves reparameterizing the diffusion process in terms of the angle on a quarter-circular arc between the image and noise, specifically setting the conventional $\\displaystyle \\sqrt{\\barα}=\\cos(η)$. This reparameterization eliminates two singularities and allows for the expression of diffusion evolution as a well-behaved ordinary d",
    "arxiv_url": "https://arxiv.org/abs/2310.17167v1",
    "pdf_url": "https://arxiv.org/pdf/2310.17167v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2310.17167",
    "arxiv_authors": [
      "Zhenkai Zhang",
      "Krista A. Ehinger",
      "Tom Drummond"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Improving+Denoising+Diffusion+Models+via+Simultaneous+Estimation+of+Image+and+Noise+Zhenkai+Zhang+Krista+A.+Ehinger+Tom+Drummond",
    "gs_search_success": true,
    "gs_authors": [
      "6sWGL5wAAAAJ",
      "EdGfpdcAAAAJ",
      "4u4zWfEAAAAJ"
    ],
    "citation_count": 4,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2411.16216",
    "title": "SMGDiff: Soccer Motion Generation using diffusion probabilistic models",
    "year": 2024,
    "published": "2024-11-25T09:25:53Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Soccer is a globally renowned sport with significant applications in video games and VR/AR. However, generating realistic soccer motions remains challenging due to the intricate interactions between the human player and the ball. In this paper, we introduce SMGDiff, a novel two-stage framework for generating real-time and user-controllable soccer motions. Our key idea is to integrate real-time character control with a powerful diffusion-based generative model, ensuring high-quality and diverse o",
    "arxiv_url": "https://arxiv.org/abs/2411.16216v1",
    "pdf_url": "https://arxiv.org/pdf/2411.16216v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2411.16216",
    "arxiv_authors": [
      "Hongdi Yang",
      "Chengyang Li",
      "Zhenxuan Wu",
      "Gaozheng Li",
      "Jingya Wang",
      "Jingyi Yu",
      "Zhuo Su",
      "Lan Xu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=SMGDiff%3A+Soccer+Motion+Generation+using+diffusion+probabilistic+models+Hongdi+Yang+Chengyang+Li+Zhenxuan+Wu+Gaozheng+Li+Jingya+Wang",
    "gs_search_success": true,
    "gs_authors": [
      "vmvJV_IAAAAJ",
      "aPS5pJkAAAAJ",
      "shdnxr8AAAAJ",
      "iaqDkqMAAAAJ"
    ],
    "citation_count": 4,
    "gs_author_count": 8
  },
  {
    "arxiv_id": "2503.06621",
    "title": "Dynamic Updates for Language Adaptation in Visual-Language Tracking",
    "year": 2025,
    "published": "2025-03-09T13:47:19Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "The consistency between the semantic information provided by the multi-modal reference and the tracked object is crucial for visual-language (VL) tracking. However, existing VL tracking frameworks rely on static multi-modal references to locate dynamic objects, which can lead to semantic discrepancies and reduce the robustness of the tracker. To address this issue, we propose a novel vision-language tracking framework, named DUTrack, which captures the latest state of the target by dynamically u",
    "arxiv_url": "https://arxiv.org/abs/2503.06621v1",
    "pdf_url": "https://arxiv.org/pdf/2503.06621v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2503.06621",
    "arxiv_authors": [
      "Xiaohai Li",
      "Bineng Zhong",
      "Qihua Liang",
      "Zhiyi Mo",
      "Jian Nong",
      "Shuxiang Song"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Dynamic+Updates+for+Language+Adaptation+in+Visual-Language+Tracking+Xiaohai+Li+Bineng+Zhong+Qihua+Liang+Zhiyi+Mo+Jian+Nong",
    "gs_search_success": true,
    "gs_authors": [
      "hvRBydsAAAAJ",
      "YeYPQWoAAAAJ"
    ],
    "citation_count": 7,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2501.17131",
    "title": "Scenario Understanding of Traffic Scenes Through Large Visual Language Models",
    "year": 2025,
    "published": "2025-01-28T18:23:12Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Deep learning models for autonomous driving, encompassing perception, planning, and control, depend on vast datasets to achieve their high performance. However, their generalization often suffers due to domain-specific data distributions, making an effective scene-based categorization of samples necessary to improve their reliability across diverse domains. Manual captioning, though valuable, is both labor-intensive and time-consuming, creating a bottleneck in the data annotation process. Large ",
    "arxiv_url": "https://arxiv.org/abs/2501.17131v2",
    "pdf_url": "https://arxiv.org/pdf/2501.17131v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2501.17131",
    "arxiv_authors": [
      "Esteban Rivera",
      "Jannik Lübberstedt",
      "Nico Uhlemann",
      "Markus Lienkamp"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Scenario+Understanding+of+Traffic+Scenes+Through+Large+Visual+Language+Models+Esteban+Rivera+Jannik+L%C3%BCbberstedt+Nico+Uhlemann+Markus+Lienkamp",
    "gs_search_success": true,
    "gs_authors": [
      "iqsHA9gAAAAJ",
      "W1CvMSUAAAAJ"
    ],
    "citation_count": 7,
    "gs_author_count": null
  },
  {
    "arxiv_id": "2308.06015",
    "title": "Enhancing Generalization of Universal Adversarial Perturbation through Gradient Aggregation",
    "year": 2023,
    "published": "2023-08-11T08:44:58Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Deep neural networks are vulnerable to universal adversarial perturbation (UAP), an instance-agnostic perturbation capable of fooling the target model for most samples. Compared to instance-specific adversarial examples, UAP is more challenging as it needs to generalize across various samples and models. In this paper, we examine the serious dilemma of UAP generation methods from a generalization perspective -- the gradient vanishing problem using small-batch stochastic gradient optimization and",
    "arxiv_url": "https://arxiv.org/abs/2308.06015v1",
    "pdf_url": "https://arxiv.org/pdf/2308.06015v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2308.06015",
    "arxiv_authors": [
      "Xuannan Liu",
      "Yaoyao Zhong",
      "Yuhang Zhang",
      "Lixiong Qin",
      "Weihong Deng"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Enhancing+Generalization+of+Universal+Adversarial+Perturbation+through+Gradient+Aggregation+Xuannan+Liu+Yaoyao+Zhong+Yuhang+Zhang+Lixiong+Qin+Weihong+Deng",
    "gs_search_success": true,
    "gs_authors": [
      "8ixNofEAAAAJ",
      "J4TYDDcAAAAJ",
      "1rhBlUEAAAAJ",
      "DF_m60EAAAAJ"
    ],
    "citation_count": 42,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2302.14533",
    "title": "DEff-GAN: Diverse Attribute Transfer for Few-Shot Image Synthesis",
    "year": 2023,
    "published": "2023-02-28T12:43:52Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Requirements of large amounts of data is a difficulty in training many GANs. Data efficient GANs involve fitting a generators continuous target distribution with a limited discrete set of data samples, which is a difficult task. Single image methods have focused on modeling the internal distribution of a single image and generating its samples. While single image methods can synthesize image samples with diversity, they do not model multiple images or capture the inherent relationship possible b",
    "arxiv_url": "https://arxiv.org/abs/2302.14533v1",
    "pdf_url": "https://arxiv.org/pdf/2302.14533v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2302.14533",
    "arxiv_authors": [
      "Rajiv Kumar",
      "G. Sivakumar"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=DEff-GAN%3A+Diverse+Attribute+Transfer+for+Few-Shot+Image+Synthesis+Rajiv+Kumar+G.+Sivakumar",
    "gs_search_success": true,
    "gs_authors": [
      "bAQjMRsAAAAJ",
      "CXkZvSMAAAAJ"
    ],
    "citation_count": 2,
    "gs_author_count": 2
  },
  {
    "arxiv_id": "2412.00065",
    "title": "DYRECT Computed Tomography: DYnamic Reconstruction of Events on a Continuous Timescale",
    "year": 2024,
    "published": "2024-11-15T14:21:46Z",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "abstract": "Time-resolved high-resolution X-ray Computed Tomography (4D $μ$CT) is an imaging technique that offers insight into the evolution of dynamic processes inside materials that are opaque to visible light. Conventional tomographic reconstruction techniques are based on recording a sequence of 3D images that represent the sample state at different moments in time. This frame-based approach limits the temporal resolution compared to dynamic radiography experiments due to the time needed to make CT sca",
    "arxiv_url": "https://arxiv.org/abs/2412.00065v1",
    "pdf_url": "https://arxiv.org/pdf/2412.00065v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2412.00065",
    "arxiv_authors": [
      "Wannes Goethals",
      "Tom Bultreys",
      "Steffen Berg",
      "Matthieu N. Boone",
      "Jan Aelterman"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=DYRECT+Computed+Tomography%3A+DYnamic+Reconstruction+of+Events+on+a+Continuous+Timescale+Wannes+Goethals+Tom+Bultreys+Steffen+Berg+Matthieu+N.+Boone+Jan+Aelterman",
    "gs_search_success": true,
    "gs_authors": [
      "PkaHAnwAAAAJ",
      "sK-sq1MAAAAJ",
      "8lFSCh0AAAAJ",
      "jOfwQ5AAAAAJ",
      "-fqmKYgAAAAJ"
    ],
    "citation_count": 7,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2303.15223",
    "title": "How far generated data can impact Neural Networks performance?",
    "year": 2023,
    "published": "2023-03-27T14:02:43Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "The success of deep learning models depends on the size and quality of the dataset to solve certain tasks. Here, we explore how far generated data can aid real data in improving the performance of Neural Networks. In this work, we consider facial expression recognition since it requires challenging local data generation at the level of local regions such as mouth, eyebrows, etc, rather than simple augmentation. Generative Adversarial Networks (GANs) provide an alternative method for generating s",
    "arxiv_url": "https://arxiv.org/abs/2303.15223v1",
    "pdf_url": "https://arxiv.org/pdf/2303.15223v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2303.15223",
    "arxiv_authors": [
      "Sayeh Gholipour Picha",
      "Dawood AL Chanti",
      "Alice Caplier"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=How+far+generated+data+can+impact+Neural+Networks+performance%3F+Sayeh+Gholipour+Picha+Dawood+AL+Chanti+Alice+Caplier",
    "gs_search_success": true,
    "gs_authors": [
      "8J9vHC8AAAAJ",
      "osXo54QAAAAJ",
      "vq2KiEYAAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": null
  },
  {
    "arxiv_id": "2310.05306",
    "title": "Progressive Neural Compression for Adaptive Image Offloading under Timing Constraints",
    "year": 2023,
    "published": "2023-10-08T22:58:31Z",
    "categories": [
      "cs.DC",
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "IoT devices are increasingly the source of data for machine learning (ML) applications running on edge servers. Data transmissions from devices to servers are often over local wireless networks whose bandwidth is not just limited but, more importantly, variable. Furthermore, in cyber-physical systems interacting with the physical environment, image offloading is also commonly subject to timing constraints. It is, therefore, important to develop an adaptive approach that maximizes the inference p",
    "arxiv_url": "https://arxiv.org/abs/2310.05306v1",
    "pdf_url": "https://arxiv.org/pdf/2310.05306v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2310.05306",
    "arxiv_authors": [
      "Ruiqi Wang",
      "Hanyang Liu",
      "Jiaming Qiu",
      "Moran Xu",
      "Roch Guerin",
      "Chenyang Lu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Progressive+Neural+Compression+for+Adaptive+Image+Offloading+under+Timing+Constraints+Ruiqi+Wang+Hanyang+Liu+Jiaming+Qiu+Moran+Xu+Roch+Guerin",
    "gs_search_success": true,
    "gs_authors": [
      "ArKSyCkAAAAJ",
      "eIkRVZIAAAAJ",
      "tCq7Wx0AAAAJ",
      "54w2_AcAAAAJ",
      "Pz5A2ZkAAAAJ",
      "gFElGZoAAAAJ"
    ],
    "citation_count": 10,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2308.08529",
    "title": "Diagnosing Human-object Interaction Detectors",
    "year": 2023,
    "published": "2023-08-16T17:39:15Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "We have witnessed significant progress in human-object interaction (HOI) detection. The reliance on mAP (mean Average Precision) scores as a summary metric, however, does not provide sufficient insight into the nuances of model performance (e.g., why one model is better than another), which can hinder further innovation in this field. To address this issue, in this paper, we introduce a diagnosis toolbox to provide detailed quantitative break-down analysis of HOI detection models, inspired by th",
    "arxiv_url": "https://arxiv.org/abs/2308.08529v2",
    "pdf_url": "https://arxiv.org/pdf/2308.08529v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2308.08529",
    "arxiv_authors": [
      "Fangrui Zhu",
      "Yiming Xie",
      "Weidi Xie",
      "Huaizu Jiang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Diagnosing+Human-object+Interaction+Detectors+Fangrui+Zhu+Yiming+Xie+Weidi+Xie+Huaizu+Jiang",
    "gs_search_success": true,
    "gs_authors": [
      "0hHqYoAAAAAJ",
      "Vtrqj4gAAAAJ",
      "Xq0De8EAAAAJ",
      "86y0joUAAAAJ"
    ],
    "citation_count": 11,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2301.06193",
    "title": "RedBit: An End-to-End Flexible Framework for Evaluating the Accuracy of Quantized CNNs",
    "year": 2023,
    "published": "2023-01-15T21:27:35Z",
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "abstract": "In recent years, Convolutional Neural Networks (CNNs) have become the standard class of deep neural network for image processing, classification and segmentation tasks. However, the large strides in accuracy obtained by CNNs have been derived from increasing the complexity of network topologies, which incurs sizeable performance and energy penalties in the training and inference of CNNs. Many recent works have validated the effectiveness of parameter quantization, which consists in reducing the ",
    "arxiv_url": "https://arxiv.org/abs/2301.06193v1",
    "pdf_url": "https://arxiv.org/pdf/2301.06193v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2301.06193",
    "arxiv_authors": [
      "André Santos",
      "João Dinis Ferreira",
      "Onur Mutlu",
      "Gabriel Falcao"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=RedBit%3A+An+End-to-End+Flexible+Framework+for+Evaluating+the+Accuracy+of+Quantized+CNNs+Andr%C3%A9+Santos+Jo%C3%A3o+Dinis+Ferreira+Onur+Mutlu+Gabriel+Falcao",
    "gs_search_success": true,
    "gs_authors": [
      "KxvMVlAAAAAJ",
      "A-rywd8AAAAJ",
      "7XyGUGkAAAAJ"
    ],
    "citation_count": 6,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2306.00971",
    "title": "ViCo: Plug-and-play Visual Condition for Personalized Text-to-image Generation",
    "year": 2023,
    "published": "2023-06-01T17:58:44Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Personalized text-to-image generation using diffusion models has recently emerged and garnered significant interest. This task learns a novel concept (e.g., a unique toy), illustrated in a handful of images, into a generative model that captures fine visual details and generates photorealistic images based on textual embeddings. In this paper, we present ViCo, a novel lightweight plug-and-play method that seamlessly integrates visual condition into personalized text-to-image generation. ViCo sta",
    "arxiv_url": "https://arxiv.org/abs/2306.00971v2",
    "pdf_url": "https://arxiv.org/pdf/2306.00971v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2306.00971",
    "arxiv_authors": [
      "Shaozhe Hao",
      "Kai Han",
      "Shihao Zhao",
      "Kwan-Yee K. Wong"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=ViCo%3A+Plug-and-play+Visual+Condition+for+Personalized+Text-to-image+Generation+Shaozhe+Hao+Kai+Han+Shihao+Zhao+Kwan-Yee+K.+Wong",
    "gs_search_success": true,
    "gs_authors": [
      "Dxpvy4EAAAAJ",
      "tG8S_vMAAAAJ",
      "dNQiLDQAAAAJ",
      "72uQORoAAAAJ"
    ],
    "citation_count": 64,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2407.00614",
    "title": "Learning Granularity-Aware Affordances from Human-Object Interaction for Tool-Based Functional Dexterous Grasping",
    "year": 2024,
    "published": "2024-06-30T07:42:57Z",
    "categories": [
      "cs.RO",
      "cs.CV",
      "eess.IV"
    ],
    "abstract": "To enable robots to use tools, the initial step is teaching robots to employ dexterous gestures for touching specific areas precisely where tasks are performed. Affordance features of objects serve as a bridge in the functional interaction between agents and objects. However, leveraging these affordance cues to help robots achieve functional tool grasping remains unresolved. To address this, we propose a granularity-aware affordance feature extraction method for locating functional affordance ar",
    "arxiv_url": "https://arxiv.org/abs/2407.00614v2",
    "pdf_url": "https://arxiv.org/pdf/2407.00614v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2407.00614",
    "arxiv_authors": [
      "Fan Yang",
      "Wenrui Chen",
      "Kailun Yang",
      "Haoran Lin",
      "Dongsheng Luo",
      "Conghui Tang",
      "Zhiyong Li",
      "Yaonan Wang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Learning+Granularity-Aware+Affordances+from+Human-Object+Interaction+for+Tool-Based+Functional+Dexterous+Grasping+Fan+Yang+Wenrui+Chen+Kailun+Yang+Haoran+Lin+Dongsheng+Luo",
    "gs_search_success": true,
    "gs_authors": [
      "c3bvnWIAAAAJ",
      "bG3YNRYAAAAJ",
      "pKFqWhgAAAAJ",
      "f035wg8AAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 8
  },
  {
    "arxiv_id": "2412.08643",
    "title": "GPD-1: Generative Pre-training for Driving",
    "year": 2024,
    "published": "2024-12-11T18:59:51Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "cs.RO"
    ],
    "abstract": "Modeling the evolutions of driving scenarios is important for the evaluation and decision-making of autonomous driving systems. Most existing methods focus on one aspect of scene evolution such as map generation, motion prediction, and trajectory planning. In this paper, we propose a unified Generative Pre-training for Driving (GPD-1) model to accomplish all these tasks altogether without additional fine-tuning. We represent each scene with ego, agent, and map tokens and formulate autonomous dri",
    "arxiv_url": "https://arxiv.org/abs/2412.08643v1",
    "pdf_url": "https://arxiv.org/pdf/2412.08643v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2412.08643",
    "arxiv_authors": [
      "Zixun Xie",
      "Sicheng Zuo",
      "Wenzhao Zheng",
      "Yunpeng Zhang",
      "Dalong Du",
      "Jie Zhou",
      "Jiwen Lu",
      "Shanghang Zhang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=GPD-1%3A+Generative+Pre-training+for+Driving+Zixun+Xie+Sicheng+Zuo+Wenzhao+Zheng+Yunpeng+Zhang+Dalong+Du",
    "gs_search_success": true,
    "gs_authors": [
      "UgadGL8AAAAJ",
      "LdK9scgAAAAJ",
      "TN8uDQoAAAAJ",
      "voqw10cAAAAJ",
      "11kh6C4AAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 8
  },
  {
    "arxiv_id": "2403.13258",
    "title": "SAMCT: Segment Any CT Allowing Labor-Free Task-Indicator Prompts",
    "year": 2024,
    "published": "2024-03-20T02:39:15Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Segment anything model (SAM), a foundation model with superior versatility and generalization across diverse segmentation tasks, has attracted widespread attention in medical imaging. However, it has been proved that SAM would encounter severe performance degradation due to the lack of medical knowledge in training and local feature encoding. Though several SAM-based models have been proposed for tuning SAM in medical imaging, they still suffer from insufficient feature extraction and highly rel",
    "arxiv_url": "https://arxiv.org/abs/2403.13258v1",
    "pdf_url": "https://arxiv.org/pdf/2403.13258v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2403.13258",
    "arxiv_authors": [
      "Xian Lin",
      "Yangyang Xiang",
      "Zhehao Wang",
      "Kwang-Ting Cheng",
      "Zengqiang Yan",
      "Li Yu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=SAMCT%3A+Segment+Any+CT+Allowing+Labor-Free+Task-Indicator+Prompts+Xian+Lin+Yangyang+Xiang+Zhehao+Wang+Kwang-Ting+Cheng+Zengqiang+Yan",
    "gs_search_success": true,
    "gs_authors": [
      "6VxEC3sAAAAJ",
      "zZx9OlkAAAAJ",
      "8srWYE8AAAAJ",
      "j0-TqGAAAAAJ",
      "-SgpaF8AAAAJ",
      "HtmUkQ8AAAAJ"
    ],
    "citation_count": 7,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2406.18279",
    "title": "Improving EO Foundation Models with Confidence Assessment for enhanced Semantic segmentation",
    "year": 2024,
    "published": "2024-06-26T12:05:49Z",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "Confidence assessments of semantic segmentation algorithms are important. Ideally, deep learning models should have the ability to predict in advance whether their output is likely to be incorrect. Assessing the confidence levels of model predictions in Earth Observation (EO) classification is essential, as it can enhance semantic segmentation performance and help prevent further exploitation of the results in case of erroneous prediction. The model we developed, Confidence Assessment for enhanc",
    "arxiv_url": "https://arxiv.org/abs/2406.18279v2",
    "pdf_url": "https://arxiv.org/pdf/2406.18279v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2406.18279",
    "arxiv_authors": [
      "Nikolaos Dionelis",
      "Nicolas Longepe"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Improving+EO+Foundation+Models+with+Confidence+Assessment+for+enhanced+Semantic+segmentation+Nikolaos+Dionelis+Nicolas+Longepe",
    "gs_search_success": true,
    "gs_authors": [
      "YVVkIX8AAAAJ",
      "2UweGHoAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 2
  },
  {
    "arxiv_id": "2503.01655",
    "title": "Can Optical Denoising Clean Sonar Images? A Benchmark and Fusion Approach",
    "year": 2025,
    "published": "2025-03-03T15:30:39Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Object detection in sonar images is crucial for underwater robotics applications including autonomous navigation and resource exploration. However, complex noise patterns inherent in sonar imagery, particularly speckle, reverberation, and non-Gaussian noise, significantly degrade detection accuracy. While denoising techniques have achieved remarkable success in optical imaging, their applicability to sonar data remains underexplored. This study presents the first systematic evaluation of nine st",
    "arxiv_url": "https://arxiv.org/abs/2503.01655v2",
    "pdf_url": "https://arxiv.org/pdf/2503.01655v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2503.01655",
    "arxiv_authors": [
      "Ziyu Wang",
      "Tao Xue",
      "Jingyuan Li",
      "Haibin Zhang",
      "Zhiqiang Xu",
      "Gaofei Xu",
      "Zhen Wang",
      "Yanbin Wang",
      "Zhiquan Liu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Can+Optical+Denoising+Clean+Sonar+Images%3F+A+Benchmark+and+Fusion+Approach+Ziyu+Wang+Tao+Xue+Jingyuan+Li+Haibin+Zhang+Zhiqiang+Xu",
    "gs_search_success": true,
    "gs_authors": [],
    "citation_count": 1,
    "gs_author_count": 6
  },
  {
    "arxiv_id": "2409.13084",
    "title": "Real-time estimation of overt attention from dynamic features of the face using deep-learning",
    "year": 2024,
    "published": "2024-09-19T20:49:39Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Students often drift in and out of focus during class. Effective teachers recognize this and re-engage them when necessary. With the shift to remote learning, teachers have lost the visual feedback needed to adapt to varying student engagement. We propose using readily available front-facing video to infer attention levels based on movements of the eyes, head, and face. We train a deep learning model to predict a measure of attention based on overt eye movements. Specifically, we measure Inter-S",
    "arxiv_url": "https://arxiv.org/abs/2409.13084v2",
    "pdf_url": "https://arxiv.org/pdf/2409.13084v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2409.13084",
    "arxiv_authors": [
      "Aimar Silvan Ortubay",
      "Lucas C. Parra",
      "Jens Madsen"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Real-time+estimation+of+overt+attention+from+dynamic+features+of+the+face+using+deep-learning+Aimar+Silvan+Ortubay+Lucas+C.+Parra+Jens+Madsen",
    "gs_search_success": true,
    "gs_authors": [
      "CGS12EUAAAAJ",
      "-dqyhv4AAAAJ",
      "-4BM5pwAAAAJ"
    ],
    "citation_count": 3,
    "gs_author_count": null
  },
  {
    "arxiv_id": "2406.11641",
    "title": "YOLO-FEDER FusionNet: A Novel Deep Learning Architecture for Drone Detection",
    "year": 2024,
    "published": "2024-06-17T15:25:31Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Predominant methods for image-based drone detection frequently rely on employing generic object detection algorithms like YOLOv5. While proficient in identifying drones against homogeneous backgrounds, these algorithms often struggle in complex, highly textured environments. In such scenarios, drones seamlessly integrate into the background, creating camouflage effects that adversely affect the detection quality. To address this issue, we introduce a novel deep learning architecture called YOLO-",
    "arxiv_url": "https://arxiv.org/abs/2406.11641v1",
    "pdf_url": "https://arxiv.org/pdf/2406.11641v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2406.11641",
    "arxiv_authors": [
      "Tamara R. Lenhard",
      "Andreas Weinmann",
      "Stefan Jäger",
      "Tobias Koch"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=YOLO-FEDER+FusionNet%3A+A+Novel+Deep+Learning+Architecture+for+Drone+Detection+Tamara+R.+Lenhard+Andreas+Weinmann+Stefan+J%C3%A4ger+Tobias+Koch",
    "gs_search_success": true,
    "gs_authors": [
      "xByiKDYAAAAJ",
      "UmCfkcQAAAAJ",
      "VdzvXOoAAAAJ"
    ],
    "citation_count": 8,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2503.14779",
    "title": "Involution and BSConv Multi-Depth Distillation Network for Lightweight Image Super-Resolution",
    "year": 2025,
    "published": "2025-03-18T23:10:08Z",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "abstract": "Single-image super-resolution (SISR) is a fundamental problem in computer vision that aims to reconstruct high-resolution (HR) images from low-resolution (LR) inputs. Although convolutional neural networks (CNNs) have achieved substantial advancements, deeper architectures often introduce excessive parameters, higher memory usage, and computational cost, limiting their applicability on resource-constrained devices. Recent research has thus focused on lightweight architectures that preserve accur",
    "arxiv_url": "https://arxiv.org/abs/2503.14779v2",
    "pdf_url": "https://arxiv.org/pdf/2503.14779v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2503.14779",
    "arxiv_authors": [
      "Akram Khatami-Rizi",
      "Ahmad Mahmoudi-Aznaveh"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Involution+and+BSConv+Multi-Depth+Distillation+Network+for+Lightweight+Image+Super-Resolution+Akram+Khatami-Rizi+Ahmad+Mahmoudi-Aznaveh",
    "gs_search_success": true,
    "gs_authors": [
      "jL2Y2CAAAAAJ",
      "HlT_NN4AAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 2
  },
  {
    "arxiv_id": "2406.09484",
    "title": "Is Diffusion Model Safe? Severe Data Leakage via Gradient-Guided Diffusion Model",
    "year": 2024,
    "published": "2024-06-13T14:41:47Z",
    "categories": [
      "cs.CV",
      "cs.CR"
    ],
    "abstract": "Gradient leakage has been identified as a potential source of privacy breaches in modern image processing systems, where the adversary can completely reconstruct the training images from leaked gradients. However, existing methods are restricted to reconstructing low-resolution images where data leakage risks of image processing systems are not sufficiently explored. In this paper, by exploiting diffusion models, we propose an innovative gradient-guided fine-tuning method and introduce a new rec",
    "arxiv_url": "https://arxiv.org/abs/2406.09484v1",
    "pdf_url": "https://arxiv.org/pdf/2406.09484v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2406.09484",
    "arxiv_authors": [
      "Jiayang Meng",
      "Tao Huang",
      "Hong Chen",
      "Cuiping Li"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Is+Diffusion+Model+Safe%3F+Severe+Data+Leakage+via+Gradient-Guided+Diffusion+Model+Jiayang+Meng+Tao+Huang+Hong+Chen+Cuiping+Li",
    "gs_search_success": true,
    "gs_authors": [
      "GJmm9F8AAAAJ"
    ],
    "citation_count": 2,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2303.11630",
    "title": "BoxSnake: Polygonal Instance Segmentation with Box Supervision",
    "year": 2023,
    "published": "2023-03-21T06:54:18Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Box-supervised instance segmentation has gained much attention as it requires only simple box annotations instead of costly mask or polygon annotations. However, existing box-supervised instance segmentation models mainly focus on mask-based frameworks. We propose a new end-to-end training technique, termed BoxSnake, to achieve effective polygonal instance segmentation using only box annotations for the first time. Our method consists of two loss functions: (1) a point-based unary loss that cons",
    "arxiv_url": "https://arxiv.org/abs/2303.11630v3",
    "pdf_url": "https://arxiv.org/pdf/2303.11630v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2303.11630",
    "arxiv_authors": [
      "Rui Yang",
      "Lin Song",
      "Yixiao Ge",
      "Xiu Li"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=BoxSnake%3A+Polygonal+Instance+Segmentation+with+Box+Supervision+Rui+Yang+Lin+Song+Yixiao+Ge+Xiu+Li",
    "gs_search_success": true,
    "gs_authors": [
      "TtU74NAAAAAJ",
      "Bi7e6bIAAAAJ",
      "6Ra2TgQAAAAJ",
      "Xrh1OIUAAAAJ"
    ],
    "citation_count": 39,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2505.24667",
    "title": "Decoupled Competitive Framework for Semi-supervised Medical Image Segmentation",
    "year": 2025,
    "published": "2025-05-30T14:56:00Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Confronting the critical challenge of insufficiently annotated samples in medical domain, semi-supervised medical image segmentation (SSMIS) emerges as a promising solution. Specifically, most methodologies following the Mean Teacher (MT) or Dual Students (DS) architecture have achieved commendable results. However, to date, these approaches face a performance bottleneck due to two inherent limitations, \\textit{e.g.}, the over-coupling problem within MT structure owing to the employment of expon",
    "arxiv_url": "https://arxiv.org/abs/2505.24667v1",
    "pdf_url": "https://arxiv.org/pdf/2505.24667v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2505.24667",
    "arxiv_authors": [
      "Jiahe Chen",
      "Jiahe Ying",
      "Shen Wang",
      "Jianwei Zheng"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Decoupled+Competitive+Framework+for+Semi-supervised+Medical+Image+Segmentation+Jiahe+Chen+Jiahe+Ying+Shen+Wang+Jianwei+Zheng",
    "gs_search_success": true,
    "gs_authors": [],
    "citation_count": 1,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2401.14248",
    "title": "On generalisability of segment anything model for nuclear instance segmentation in histology images",
    "year": 2024,
    "published": "2024-01-25T15:39:37Z",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "abstract": "Pre-trained on a large and diverse dataset, the segment anything model (SAM) is the first promptable foundation model in computer vision aiming at object segmentation tasks. In this work, we evaluate SAM for the task of nuclear instance segmentation performance with zero-shot learning and finetuning. We compare SAM with other representative methods in nuclear instance segmentation, especially in the context of model generalisability. To achieve automatic nuclear instance segmentation, we propose",
    "arxiv_url": "https://arxiv.org/abs/2401.14248v1",
    "pdf_url": "https://arxiv.org/pdf/2401.14248v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2401.14248",
    "arxiv_authors": [
      "Kesi Xu",
      "Lea Goetz",
      "Nasir Rajpoot"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=On+generalisability+of+segment+anything+model+for+nuclear+instance+segmentation+in+histology+images+Kesi+Xu+Lea+Goetz+Nasir+Rajpoot",
    "gs_search_success": true,
    "gs_authors": [
      "BSDXhwgAAAAJ",
      "kLGJ-eUAAAAJ",
      "Mw5XKu8AAAAJ"
    ],
    "citation_count": 3,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2503.21242",
    "title": "PLAIN: Scalable Estimation Architecture for Integrated Sensing and Communication",
    "year": 2025,
    "published": "2025-03-27T08:04:46Z",
    "categories": [
      "eess.SP",
      "cs.CV"
    ],
    "abstract": "Integrated sensing and communication (ISAC) is envisioned be to one of the paradigms upon which next-generation mobile networks will be built, extending localization and tracking capabilities, as well as giving birth to environment-aware wireless access. A key aspect of sensing integration is parameter estimation, which involves extracting information about the surrounding environment, such as the direction, distance, and velocity of various objects within. This is typically of a high-dimensiona",
    "arxiv_url": "https://arxiv.org/abs/2503.21242v1",
    "pdf_url": "https://arxiv.org/pdf/2503.21242v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2503.21242",
    "arxiv_authors": [
      "Bashar Tahir",
      "Philipp Svoboda",
      "Markus Rupp"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=PLAIN%3A+Scalable+Estimation+Architecture+for+Integrated+Sensing+and+Communication+Bashar+Tahir+Philipp+Svoboda+Markus+Rupp",
    "gs_search_success": true,
    "gs_authors": [
      "ffE1-yQAAAAJ",
      "wHEcfFgAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2412.05980",
    "title": "Anti-Reference: Universal and Immediate Defense Against Reference-Based Generation",
    "year": 2024,
    "published": "2024-12-08T16:04:45Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Diffusion models have revolutionized generative modeling with their exceptional ability to produce high-fidelity images. However, misuse of such potent tools can lead to the creation of fake news or disturbing content targeting individuals, resulting in significant social harm. In this paper, we introduce Anti-Reference, a novel method that protects images from the threats posed by reference-based generation techniques by adding imperceptible adversarial noise to the images. We propose a unified",
    "arxiv_url": "https://arxiv.org/abs/2412.05980v1",
    "pdf_url": "https://arxiv.org/pdf/2412.05980v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2412.05980",
    "arxiv_authors": [
      "Yiren Song",
      "Shengtao Lou",
      "Xiaokang Liu",
      "Hai Ci",
      "Pei Yang",
      "Jiaming Liu",
      "Mike Zheng Shou"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Anti-Reference%3A+Universal+and+Immediate+Defense+Against+Reference-Based+Generation+Yiren+Song+Shengtao+Lou+Xiaokang+Liu+Hai+Ci+Pei+Yang",
    "gs_search_success": true,
    "gs_authors": [
      "eBvav_0AAAAJ",
      "h1-3lSoAAAAJ",
      "L2YS0jgAAAAJ",
      "GMrjppAAAAAJ",
      "dAEHm8AAAAAJ"
    ],
    "citation_count": 13,
    "gs_author_count": 7
  },
  {
    "arxiv_id": "2407.05683",
    "title": "RadiomicsFill-Mammo: Synthetic Mammogram Mass Manipulation with Radiomics Features",
    "year": 2024,
    "published": "2024-07-08T07:33:52Z",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "abstract": "Motivated by the question, \"Can we generate tumors with desired attributes?'' this study leverages radiomics features to explore the feasibility of generating synthetic tumor images. Characterized by its low-dimensional yet biologically meaningful markers, radiomics bridges the gap between complex medical imaging data and actionable clinical insights. We present RadiomicsFill-Mammo, the first of the RadiomicsFill series, an innovative technique that generates realistic mammogram mass images mirr",
    "arxiv_url": "https://arxiv.org/abs/2407.05683v2",
    "pdf_url": "https://arxiv.org/pdf/2407.05683v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2407.05683",
    "arxiv_authors": [
      "Inye Na",
      "Jonghun Kim",
      "Eun Sook Ko",
      "Hyunjin Park"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=RadiomicsFill-Mammo%3A+Synthetic+Mammogram+Mass+Manipulation+with+Radiomics+Features+Inye+Na+Jonghun+Kim+Eun+Sook+Ko+Hyunjin+Park",
    "gs_search_success": true,
    "gs_authors": [
      "wQp97-EAAAAJ",
      "Pt2N0C8AAAAJ",
      "NeEUJFsAAAAJ"
    ],
    "citation_count": 6,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2406.05075",
    "title": "Diving Deep into the Motion Representation of Video-Text Models",
    "year": 2024,
    "published": "2024-06-07T16:46:10Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Videos are more informative than images because they capture the dynamics of the scene. By representing motion in videos, we can capture dynamic activities. In this work, we introduce GPT-4 generated motion descriptions that capture fine-grained motion descriptions of activities and apply them to three action datasets. We evaluated several video-text models on the task of retrieval of motion descriptions. We found that they fall far behind human expert performance on two action datasets, raising",
    "arxiv_url": "https://arxiv.org/abs/2406.05075v1",
    "pdf_url": "https://arxiv.org/pdf/2406.05075v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2406.05075",
    "arxiv_authors": [
      "Chinmaya Devaraj",
      "Cornelia Fermuller",
      "Yiannis Aloimonos"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Diving+Deep+into+the+Motion+Representation+of+Video-Text+Models+Chinmaya+Devaraj+Cornelia+Fermuller+Yiannis+Aloimonos",
    "gs_search_success": true,
    "gs_authors": [
      "7QmEsOwAAAAJ",
      "yOY8fyQAAAAJ",
      "0gEOJSEAAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 2
  },
  {
    "arxiv_id": "2412.14158",
    "title": "AKiRa: Augmentation Kit on Rays for optical video generation",
    "year": 2024,
    "published": "2024-12-18T18:53:22Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.MM"
    ],
    "abstract": "Recent advances in text-conditioned video diffusion have greatly improved video quality. However, these methods offer limited or sometimes no control to users on camera aspects, including dynamic camera motion, zoom, distorted lens and focus shifts. These motion and optical aspects are crucial for adding controllability and cinematic elements to generation frameworks, ultimately resulting in visual content that draws focus, enhances mood, and guides emotions according to filmmakers' controls. In",
    "arxiv_url": "https://arxiv.org/abs/2412.14158v2",
    "pdf_url": "https://arxiv.org/pdf/2412.14158v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2412.14158",
    "arxiv_authors": [
      "Xi Wang",
      "Robin Courant",
      "Marc Christie",
      "Vicky Kalogeiton"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=AKiRa%3A+Augmentation+Kit+on+Rays+for+optical+video+generation+Xi+Wang+Robin+Courant+Marc+Christie+Vicky+Kalogeiton",
    "gs_search_success": true,
    "gs_authors": [
      "gIRvhKkAAAAJ",
      "YvIJwRAAAAAJ",
      "UPPES_cAAAAJ",
      "qF59g4QAAAAJ"
    ],
    "citation_count": 6,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2411.11912",
    "title": "F$^3$OCUS -- Federated Finetuning of Vision-Language Foundation Models with Optimal Client Layer Updating Strategy via Multi-objective Meta-Heuristics",
    "year": 2024,
    "published": "2024-11-17T21:54:57Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "abstract": "Effective training of large Vision-Language Models (VLMs) on resource-constrained client devices in Federated Learning (FL) requires the usage of parameter-efficient fine-tuning (PEFT) strategies. To this end, we demonstrate the impact of two factors \\textit{viz.}, client-specific layer importance score that selects the most important VLM layers for fine-tuning and inter-client layer diversity score that encourages diverse layer selection across clients for optimal VLM layer selection. We first ",
    "arxiv_url": "https://arxiv.org/abs/2411.11912v2",
    "pdf_url": "https://arxiv.org/pdf/2411.11912v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2411.11912",
    "arxiv_authors": [
      "Pramit Saha",
      "Felix Wagner",
      "Divyanshu Mishra",
      "Can Peng",
      "Anshul Thakur",
      "David Clifton",
      "Konstantinos Kamnitsas",
      "J. Alison Noble"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=F%24%5E3%24OCUS+--+Federated+Finetuning+of+Vision-Language+Foundation+Models+with+Optimal+Client+Layer+Updating+Strategy+via+Multi-objective+Meta-Heuristics+Pramit+Saha+Felix+Wagner+Divyanshu+Mishra+Can+Peng+Anshul+Thakur",
    "gs_search_success": true,
    "gs_authors": [
      "VC3rTrAAAAAJ",
      "b0tmmYMAAAAJ",
      "e1QWXJIAAAAJ",
      "rm7rbz8AAAAJ",
      "yB-oPPoAAAAJ",
      "HmzYOu4AAAAJ",
      "mFN2KJ4AAAAJ",
      "Sb7d-rsAAAAJ"
    ],
    "citation_count": 5,
    "gs_author_count": 8
  },
  {
    "arxiv_id": "2307.08763",
    "title": "Video-Mined Task Graphs for Keystep Recognition in Instructional Videos",
    "year": 2023,
    "published": "2023-07-17T18:19:36Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Procedural activity understanding requires perceiving human actions in terms of a broader task, where multiple keysteps are performed in sequence across a long video to reach a final goal state -- such as the steps of a recipe or a DIY fix-it task. Prior work largely treats keystep recognition in isolation of this broader structure, or else rigidly confines keysteps to align with a predefined sequential script. We propose discovering a task graph automatically from how-to videos to represent pro",
    "arxiv_url": "https://arxiv.org/abs/2307.08763v2",
    "pdf_url": "https://arxiv.org/pdf/2307.08763v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2307.08763",
    "arxiv_authors": [
      "Kumar Ashutosh",
      "Santhosh Kumar Ramakrishnan",
      "Triantafyllos Afouras",
      "Kristen Grauman"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Video-Mined+Task+Graphs+for+Keystep+Recognition+in+Instructional+Videos+Kumar+Ashutosh+Santhosh+Kumar+Ramakrishnan+Triantafyllos+Afouras+Kristen+Grauman",
    "gs_search_success": true,
    "gs_authors": [
      "zr9B1YgAAAAJ",
      "Jp6Mz1sAAAAJ",
      "TkBHFfgAAAAJ",
      "GDqE4f8AAAAJ"
    ],
    "citation_count": 40,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2504.14626",
    "title": "MSAD-Net: Multiscale and Spatial Attention-based Dense Network for Lung Cancer Classification",
    "year": 2025,
    "published": "2025-04-20T14:07:21Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Lung cancer, a severe form of malignant tumor that originates in the tissues of the lungs, can be fatal if not detected in its early stages. It ranks among the top causes of cancer-related mortality worldwide. Detecting lung cancer manually using chest X-Ray image or Computational Tomography (CT) scans image poses significant challenges for radiologists. Hence, there is a need for automatic diagnosis system of lung cancers from radiology images. With the recent emergence of deep learning, partic",
    "arxiv_url": "https://arxiv.org/abs/2504.14626v1",
    "pdf_url": "https://arxiv.org/pdf/2504.14626v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2504.14626",
    "arxiv_authors": [
      "Santanu Roy",
      "Shweta Singh",
      "Palak Sahu",
      "Ashvath Suresh",
      "Debashish Das"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=MSAD-Net%3A+Multiscale+and+Spatial+Attention-based+Dense+Network+for+Lung+Cancer+Classification+Santanu+Roy+Shweta+Singh+Palak+Sahu+Ashvath+Suresh+Debashish+Das",
    "gs_search_success": true,
    "gs_authors": [
      "ZD9xQ5MAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2411.02793",
    "title": "Toward Robust Incomplete Multimodal Sentiment Analysis via Hierarchical Representation Learning",
    "year": 2024,
    "published": "2024-11-05T04:04:41Z",
    "categories": [
      "cs.CL",
      "cs.CV"
    ],
    "abstract": "Multimodal Sentiment Analysis (MSA) is an important research area that aims to understand and recognize human sentiment through multiple modalities. The complementary information provided by multimodal fusion promotes better sentiment analysis compared to utilizing only a single modality. Nevertheless, in real-world applications, many unavoidable factors may lead to situations of uncertain modality missing, thus hindering the effectiveness of multimodal modeling and degrading the model's perform",
    "arxiv_url": "https://arxiv.org/abs/2411.02793v1",
    "pdf_url": "https://arxiv.org/pdf/2411.02793v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2411.02793",
    "arxiv_authors": [
      "Mingcheng Li",
      "Dingkang Yang",
      "Yang Liu",
      "Shunli Wang",
      "Jiawei Chen",
      "Shuaibing Wang",
      "Jinjie Wei",
      "Yue Jiang",
      "Qingyao Xu",
      "Xiaolu Hou",
      "Mingyang Sun",
      "Ziyun Qian",
      "Dongliang Kou",
      "Lihua Zhang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Toward+Robust+Incomplete+Multimodal+Sentiment+Analysis+via+Hierarchical+Representation+Learning+Mingcheng+Li+Dingkang+Yang+Yang+Liu+Shunli+Wang+Jiawei+Chen",
    "gs_search_success": true,
    "gs_authors": [
      "4-_udwwAAAAJ",
      "VDNoOWcAAAAJ",
      "AH4WWu4AAAAJ",
      "i-Xg2Y8AAAAJ",
      "jvlDhkcAAAAJ",
      "ksiUbVkAAAAJ",
      "kA7yH9QAAAAJ"
    ],
    "citation_count": 13,
    "gs_author_count": 12
  },
  {
    "arxiv_id": "2306.00834",
    "title": "Deformable Convolutions and LSTM-based Flexible Event Frame Fusion Network for Motion Deblurring",
    "year": 2023,
    "published": "2023-06-01T15:57:12Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "abstract": "Event cameras differ from conventional RGB cameras in that they produce asynchronous data sequences. While RGB cameras capture every frame at a fixed rate, event cameras only capture changes in the scene, resulting in sparse and asynchronous data output. Despite the fact that event data carries useful information that can be utilized in motion deblurring of RGB cameras, integrating event and image information remains a challenge. Recent state-of-the-art CNN-based deblurring solutions produce mul",
    "arxiv_url": "https://arxiv.org/abs/2306.00834v1",
    "pdf_url": "https://arxiv.org/pdf/2306.00834v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2306.00834",
    "arxiv_authors": [
      "Dan Yang",
      "Mehmet Yamac"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Deformable+Convolutions+and+LSTM-based+Flexible+Event+Frame+Fusion+Network+for+Motion+Deblurring+Dan+Yang+Mehmet+Yamac",
    "gs_search_success": true,
    "gs_authors": [
      "iLDEE9sAAAAJ",
      "ANxNCj0AAAAJ"
    ],
    "citation_count": 6,
    "gs_author_count": 2
  },
  {
    "arxiv_id": "2304.10701",
    "title": "GMValuator: Similarity-based Data Valuation for Generative Models",
    "year": 2023,
    "published": "2023-04-21T02:02:02Z",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "Data valuation plays a crucial role in machine learning. Existing data valuation methods, mainly focused on discriminative models, overlook generative models that have gained attention recently. In generative models, data valuation measures the impact of training data on generated datasets. Very few existing attempts at data valuation methods designed for deep generative models either concentrate on specific models or lack robustness in their outcomes. Moreover, efficiency still reveals vulnerab",
    "arxiv_url": "https://arxiv.org/abs/2304.10701v9",
    "pdf_url": "https://arxiv.org/pdf/2304.10701v9",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2304.10701",
    "arxiv_authors": [
      "Jiaxi Yang",
      "Wenglong Deng",
      "Benlin Liu",
      "Yangsibo Huang",
      "James Zou",
      "Xiaoxiao Li"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=GMValuator%3A+Similarity-based+Data+Valuation+for+Generative+Models+Jiaxi+Yang+Wenglong+Deng+Benlin+Liu+Yangsibo+Huang+James+Zou",
    "gs_search_success": false,
    "gs_authors": [],
    "citation_count": null,
    "gs_author_count": null,
    "error_type": "fetch_failed"
  },
  {
    "arxiv_id": "2304.04978",
    "title": "StageInteractor: Query-based Object Detector with Cross-stage Interaction",
    "year": 2023,
    "published": "2023-04-11T04:50:13Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Previous object detectors make predictions based on dense grid points or numerous preset anchors. Most of these detectors are trained with one-to-many label assignment strategies. On the contrary, recent query-based object detectors depend on a sparse set of learnable queries and a series of decoder layers. The one-to-one label assignment is independently applied on each layer for the deep supervision during training. Despite the great success of query-based object detection, however, this one-t",
    "arxiv_url": "https://arxiv.org/abs/2304.04978v2",
    "pdf_url": "https://arxiv.org/pdf/2304.04978v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2304.04978",
    "arxiv_authors": [
      "Yao Teng",
      "Haisong Liu",
      "Sheng Guo",
      "Limin Wang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=StageInteractor%3A+Query-based+Object+Detector+with+Cross-stage+Interaction+Yao+Teng+Haisong+Liu+Sheng+Guo+Limin+Wang",
    "gs_search_success": true,
    "gs_authors": [
      "Z9yWFA0AAAAJ",
      "mbpgOmEAAAAJ",
      "HEuN8PcAAAAJ",
      "eLIsViIAAAAJ"
    ],
    "citation_count": 14,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2505.19692",
    "title": "DriveCamSim: Generalizable Camera Simulation via Explicit Camera Modeling for Autonomous Driving",
    "year": 2025,
    "published": "2025-05-26T08:50:15Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Camera sensor simulation serves as a critical role for autonomous driving (AD), e.g. evaluating vision-based AD algorithms. While existing approaches have leveraged generative models for controllable image/video generation, they remain constrained to generating multi-view video sequences with fixed camera viewpoints and video frequency, significantly limiting their downstream applications. To address this, we present a generalizable camera simulation framework DriveCamSim, whose core innovation ",
    "arxiv_url": "https://arxiv.org/abs/2505.19692v1",
    "pdf_url": "https://arxiv.org/pdf/2505.19692v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2505.19692",
    "arxiv_authors": [
      "Wenchao Sun",
      "Xuewu Lin",
      "Keyu Chen",
      "Zixiang Pei",
      "Yining Shi",
      "Chuang Zhang",
      "Sifa Zheng"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=DriveCamSim%3A+Generalizable+Camera+Simulation+via+Explicit+Camera+Modeling+for+Autonomous+Driving+Wenchao+Sun+Xuewu+Lin+Keyu+Chen+Zixiang+Pei+Yining+Shi",
    "gs_search_success": true,
    "gs_authors": [
      "pfXQwcQAAAAJ",
      "m_bC1VAAAAAJ",
      "yd-sMoQAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 7
  },
  {
    "arxiv_id": "2404.01249",
    "title": "FireANTs: Adaptive Riemannian Optimization for Multi-Scale Diffeomorphic Matching",
    "year": 2024,
    "published": "2024-04-01T17:12:47Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "The paper proposes FireANTs, a multi-scale Adaptive Riemannian Optimization algorithm for dense diffeomorphic image matching. Existing state-of-the-art methods for diffeomorphic image matching are slow due to inefficient implementations and slow convergence due to the ill-conditioned nature of the optimization problem. Deep learning methods offer fast inference but require extensive training time, substantial inference memory, and fail to generalize across long-tailed distributions or diverse im",
    "arxiv_url": "https://arxiv.org/abs/2404.01249v4",
    "pdf_url": "https://arxiv.org/pdf/2404.01249v4",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2404.01249",
    "arxiv_authors": [
      "Rohit Jena",
      "Pratik Chaudhari",
      "James C. Gee"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=FireANTs%3A+Adaptive+Riemannian+Optimization+for+Multi-Scale+Diffeomorphic+Matching+Rohit+Jena+Pratik+Chaudhari+James+C.+Gee",
    "gs_search_success": true,
    "gs_authors": [
      "c_z5hWEAAAAJ",
      "kZQQFE4AAAAJ",
      "MIjWr_8AAAAJ"
    ],
    "citation_count": 6,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2304.02152",
    "title": "Can Adversarial Networks Make Uninformative Colonoscopy Video Frames Clinically Informative?",
    "year": 2023,
    "published": "2023-04-04T22:58:28Z",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "abstract": "Various artifacts, such as ghost colors, interlacing, and motion blur, hinder diagnosing colorectal cancer (CRC) from videos acquired during colonoscopy. The frames containing these artifacts are called uninformative frames and are present in large proportions in colonoscopy videos. To alleviate the impact of artifacts, we propose an adversarial network based framework to convert uninformative frames to clinically relevant frames. We examine the effectiveness of the proposed approach by evaluati",
    "arxiv_url": "https://arxiv.org/abs/2304.02152v1",
    "pdf_url": "https://arxiv.org/pdf/2304.02152v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2304.02152",
    "arxiv_authors": [
      "Vanshali Sharma",
      "M. K. Bhuyan",
      "Pradip K. Das"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Can+Adversarial+Networks+Make+Uninformative+Colonoscopy+Video+Frames+Clinically+Informative%3F+Vanshali+Sharma+M.+K.+Bhuyan+Pradip+K.+Das",
    "gs_search_success": true,
    "gs_authors": [
      "uR1apqsAAAAJ",
      "o4yGgBQAAAAJ",
      "dLGInJUAAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2311.17096",
    "title": "Robust Transductive Few-shot Learning via Joint Message Passing and Prototype-based Soft-label Propagation",
    "year": 2023,
    "published": "2023-11-28T06:44:27Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Few-shot learning (FSL) aims to develop a learning model with the ability to generalize to new classes using a few support samples. For transductive FSL tasks, prototype learning and label propagation methods are commonly employed. Prototype methods generally first learn the representative prototypes from the support set and then determine the labels of queries based on the metric between query samples and prototypes. Label propagation methods try to propagate the labels of support samples on th",
    "arxiv_url": "https://arxiv.org/abs/2311.17096v1",
    "pdf_url": "https://arxiv.org/pdf/2311.17096v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2311.17096",
    "arxiv_authors": [
      "Jiahui Wang",
      "Qin Xu",
      "Bo Jiang",
      "Bin Luo"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Robust+Transductive+Few-shot+Learning+via+Joint+Message+Passing+and+Prototype-based+Soft-label+Propagation+Jiahui+Wang+Qin+Xu+Bo+Jiang+Bin+Luo",
    "gs_search_success": true,
    "gs_authors": [
      "0qaDapcAAAAJ",
      "zRehfjgAAAAJ"
    ],
    "citation_count": 1,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2503.00641",
    "title": "How to Probe: Simple Yet Effective Techniques for Improving Post-hoc Explanations",
    "year": 2025,
    "published": "2025-03-01T22:25:11Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Post-hoc importance attribution methods are a popular tool for \"explaining\" Deep Neural Networks (DNNs) and are inherently based on the assumption that the explanations can be applied independently of how the models were trained. Contrarily, in this work we bring forward empirical evidence that challenges this very notion. Surprisingly, we discover a strong dependency on and demonstrate that the training details of a pre-trained model's classification layer (less than 10 percent of model paramet",
    "arxiv_url": "https://arxiv.org/abs/2503.00641v1",
    "pdf_url": "https://arxiv.org/pdf/2503.00641v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2503.00641",
    "arxiv_authors": [
      "Siddhartha Gairola",
      "Moritz Böhle",
      "Francesco Locatello",
      "Bernt Schiele"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=How+to+Probe%3A+Simple+Yet+Effective+Techniques+for+Improving+Post-hoc+Explanations+Siddhartha+Gairola+Moritz+B%C3%B6hle+Francesco+Locatello+Bernt+Schiele",
    "gs_search_success": true,
    "gs_authors": [
      "4tInxbgAAAAJ",
      "z76PBfYAAAAJ",
      "Kn3znAMAAAAJ",
      "wQanfTIAAAAJ"
    ],
    "citation_count": 2,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2503.04344",
    "title": "LEDiT: Your Length-Extrapolatable Diffusion Transformer without Positional Encoding",
    "year": 2025,
    "published": "2025-03-06T11:41:36Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Diffusion transformers (DiTs) struggle to generate images at resolutions higher than their training resolutions. The primary obstacle is that the explicit positional encodings(PE), such as RoPE, need extrapolating to unseen positions which degrades performance when the inference resolution differs from training. In this paper, We propose a Length-Extrapolatable Diffusion Transformer~(LEDiT) to overcome this limitation. LEDiT needs no explicit PEs, thereby avoiding PE extrapolation. The key innov",
    "arxiv_url": "https://arxiv.org/abs/2503.04344v3",
    "pdf_url": "https://arxiv.org/pdf/2503.04344v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2503.04344",
    "arxiv_authors": [
      "Shen Zhang",
      "Siyuan Liang",
      "Yaning Tan",
      "Zhaowei Chen",
      "Linze Li",
      "Ge Wu",
      "Yuhao Chen",
      "Shuheng Li",
      "Zhenyu Zhao",
      "Caihua Chen",
      "Jiajun Liang",
      "Yao Tang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=LEDiT%3A+Your+Length-Extrapolatable+Diffusion+Transformer+without+Positional+Encoding+Shen+Zhang+Siyuan+Liang+Yaning+Tan+Zhaowei+Chen+Linze+Li",
    "gs_search_success": true,
    "gs_authors": [
      "QFowS4cAAAAJ",
      "0xP6bxcAAAAJ",
      "xNxlvjEAAAAJ",
      "fI8eEDoAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 12
  },
  {
    "arxiv_id": "2304.07039",
    "title": "Learning Semantic-Aware Knowledge Guidance for Low-Light Image Enhancement",
    "year": 2023,
    "published": "2023-04-14T10:22:28Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Low-light image enhancement (LLIE) investigates how to improve illumination and produce normal-light images. The majority of existing methods improve low-light images via a global and uniform manner, without taking into account the semantic information of different regions. Without semantic priors, a network may easily deviate from a region's original color. To address this issue, we propose a novel semantic-aware knowledge-guided framework (SKF) that can assist a low-light enhancement model in ",
    "arxiv_url": "https://arxiv.org/abs/2304.07039v1",
    "pdf_url": "https://arxiv.org/pdf/2304.07039v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2304.07039",
    "arxiv_authors": [
      "Yuhui Wu",
      "Chen Pan",
      "Guoqing Wang",
      "Yang Yang",
      "Jiwei Wei",
      "Chongyi Li",
      "Heng Tao Shen"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Learning+Semantic-Aware+Knowledge+Guidance+for+Low-Light+Image+Enhancement+Yuhui+Wu+Chen+Pan+Guoqing+Wang+Yang+Yang+Jiwei+Wei",
    "gs_search_success": true,
    "gs_authors": [
      "Qt4WlSkAAAAJ",
      "V08v5OEAAAAJ",
      "1_I0P-AAAAAJ",
      "krryaDkAAAAJ",
      "2Jmbr6AAAAAJ"
    ],
    "citation_count": 232,
    "gs_author_count": 7
  },
  {
    "arxiv_id": "2502.08988",
    "title": "Latents of latents to delineate pixels: hybrid Matryoshka autoencoder-to-U-Net pairing for segmenting large medical images in GPU-poor and low-data regimes",
    "year": 2025,
    "published": "2025-02-13T05:51:41Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Medical images are often high-resolution and lose important detail if downsampled, making pixel-level methods such as semantic segmentation much less efficient if performed on a low-dimensional image. We propose a low-rank Matryoshka projection and a hybrid segmenting architecture that preserves important information while retaining sufficient pixel geometry for pixel-level tasks. We design the Matryoshka Autoencoder (MatAE-U-Net) which combines the hierarchical encoding of the Matryoshka Autoen",
    "arxiv_url": "https://arxiv.org/abs/2502.08988v1",
    "pdf_url": "https://arxiv.org/pdf/2502.08988v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2502.08988",
    "arxiv_authors": [
      "Tahir Syed",
      "Ariba Khan",
      "Sawera Hanif"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Latents+of+latents+to+delineate+pixels%3A+hybrid+Matryoshka+autoencoder-to-U-Net+pairing+for+segmenting+large+medical+images+in+GPU-poor+and+low-data+regimes+Tahir+Syed+Ariba+Khan+Sawera+Hanif",
    "gs_search_success": true,
    "gs_authors": [
      "Y_kxezwAAAAJ"
    ],
    "citation_count": null,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2412.10510",
    "title": "DEFAME: Dynamic Evidence-based FAct-checking with Multimodal Experts",
    "year": 2024,
    "published": "2024-12-13T19:11:18Z",
    "categories": [
      "cs.CV",
      "cs.CL"
    ],
    "abstract": "The proliferation of disinformation demands reliable and scalable fact-checking solutions. We present Dynamic Evidence-based FAct-checking with Multimodal Experts (DEFAME), a modular, zero-shot MLLM pipeline for open-domain, text-image claim verification. DEFAME operates in a six-stage process, dynamically selecting the tools and search depth to extract and evaluate textual and visual evidence. Unlike prior approaches that are text-only, lack explainability, or rely solely on parametric knowledg",
    "arxiv_url": "https://arxiv.org/abs/2412.10510v4",
    "pdf_url": "https://arxiv.org/pdf/2412.10510v4",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2412.10510",
    "arxiv_authors": [
      "Tobias Braun",
      "Mark Rothermel",
      "Marcus Rohrbach",
      "Anna Rohrbach"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=DEFAME%3A+Dynamic+Evidence-based+FAct-checking+with+Multimodal+Experts+Tobias+Braun+Mark+Rothermel+Marcus+Rohrbach+Anna+Rohrbach",
    "gs_search_success": true,
    "gs_authors": [
      "KVXJjTEAAAAJ",
      "GHpxNQIAAAAJ",
      "wqVWJNIAAAAJ",
      "3kDtybgAAAAJ"
    ],
    "citation_count": 21,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2502.00402",
    "title": "Enhancing Highway Safety: Accident Detection on the A9 Test Stretch Using Roadside Sensors",
    "year": 2025,
    "published": "2025-02-01T11:34:16Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Road traffic injuries are the leading cause of death for people aged 5-29, resulting in about 1.19 million deaths each year. To reduce these fatalities, it is essential to address human errors like speeding, drunk driving, and distractions. Additionally, faster accident detection and quicker medical response can help save lives. We propose an accident detection framework that combines a rule-based approach with a learning-based one. We introduce a dataset of real-world highway accidents featurin",
    "arxiv_url": "https://arxiv.org/abs/2502.00402v1",
    "pdf_url": "https://arxiv.org/pdf/2502.00402v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2502.00402",
    "arxiv_authors": [
      "Walter Zimmer",
      "Ross Greer",
      "Xingcheng Zhou",
      "Rui Song",
      "Marc Pavel",
      "Daniel Lehmberg",
      "Ahmed Ghita",
      "Akshay Gopalkrishnan",
      "Mohan Trivedi",
      "Alois Knoll"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Enhancing+Highway+Safety%3A+Accident+Detection+on+the+A9+Test+Stretch+Using+Roadside+Sensors+Walter+Zimmer+Ross+Greer+Xingcheng+Zhou+Rui+Song+Marc+Pavel",
    "gs_search_success": true,
    "gs_authors": [
      "YStWLX8AAAAJ",
      "ZAX3UCwAAAAJ",
      "FPYXLpQAAAAJ",
      "DJCj4I4AAAAJ",
      "HnoGwlYAAAAJ",
      "9IupKeQAAAAJ",
      "ZPIJIbgAAAAJ",
      "YT_LKykAAAAJ"
    ],
    "citation_count": 2,
    "gs_author_count": 8
  },
  {
    "arxiv_id": "2403.04965",
    "title": "StereoDiffusion: Training-Free Stereo Image Generation Using Latent Diffusion Models",
    "year": 2024,
    "published": "2024-03-08T00:30:25Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "The demand for stereo images increases as manufacturers launch more XR devices. To meet this demand, we introduce StereoDiffusion, a method that, unlike traditional inpainting pipelines, is trainning free, remarkably straightforward to use, and it seamlessly integrates into the original Stable Diffusion model. Our method modifies the latent variable to provide an end-to-end, lightweight capability for fast generation of stereo image pairs, without the need for fine-tuning model weights or any po",
    "arxiv_url": "https://arxiv.org/abs/2403.04965v2",
    "pdf_url": "https://arxiv.org/pdf/2403.04965v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2403.04965",
    "arxiv_authors": [
      "Lezhong Wang",
      "Jeppe Revall Frisvad",
      "Mark Bo Jensen",
      "Siavash Arjomand Bigdeli"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=StereoDiffusion%3A+Training-Free+Stereo+Image+Generation+Using+Latent+Diffusion+Models+Lezhong+Wang+Jeppe+Revall+Frisvad+Mark+Bo+Jensen+Siavash+Arjomand+Bigdeli",
    "gs_search_success": true,
    "gs_authors": [
      "kY4fkdYAAAAJ",
      "NhEK9voAAAAJ",
      "TzYqX0wAAAAJ",
      "xgiDEY4AAAAJ"
    ],
    "citation_count": 21,
    "gs_author_count": 4
  },
  {
    "arxiv_id": "2410.19863",
    "title": "Breaking the Illusion: Real-world Challenges for Adversarial Patches in Object Detection",
    "year": 2024,
    "published": "2024-10-23T11:16:11Z",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "Adversarial attacks pose a significant threat to the robustness and reliability of machine learning systems, particularly in computer vision applications. This study investigates the performance of adversarial patches for the YOLO object detection network in the physical world. Two attacks were tested: a patch designed to be placed anywhere within the scene - global patch, and another patch intended to partially overlap with specific object targeted for removal from detection - local patch. Vari",
    "arxiv_url": "https://arxiv.org/abs/2410.19863v2",
    "pdf_url": "https://arxiv.org/pdf/2410.19863v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2410.19863",
    "arxiv_authors": [
      "Jakob Shack",
      "Katarina Petrovic",
      "Olga Saukh"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Breaking+the+Illusion%3A+Real-world+Challenges+for+Adversarial+Patches+in+Object+Detection+Jakob+Shack+Katarina+Petrovic+Olga+Saukh",
    "gs_search_success": true,
    "gs_authors": [
      "f-MDKlYAAAAJ"
    ],
    "citation_count": 4,
    "gs_author_count": 3
  },
  {
    "arxiv_id": "2304.11379",
    "title": "LiDAR2Map: In Defense of LiDAR-Based Semantic Map Construction Using Online Camera Distillation",
    "year": 2023,
    "published": "2023-04-22T12:05:29Z",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "abstract": "Semantic map construction under bird's-eye view (BEV) plays an essential role in autonomous driving. In contrast to camera image, LiDAR provides the accurate 3D observations to project the captured 3D features onto BEV space inherently. However, the vanilla LiDAR-based BEV feature often contains many indefinite noises, where the spatial features have little texture and semantic cues. In this paper, we propose an effective LiDAR-based method to build semantic map. Specifically, we introduce a BEV",
    "arxiv_url": "https://arxiv.org/abs/2304.11379v2",
    "pdf_url": "https://arxiv.org/pdf/2304.11379v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2304.11379",
    "arxiv_authors": [
      "Song Wang",
      "Wentong Li",
      "Wenyu Liu",
      "Xiaolu Liu",
      "Jianke Zhu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=LiDAR2Map%3A+In+Defense+of+LiDAR-Based+Semantic+Map+Construction+Using+Online+Camera+Distillation+Song+Wang+Wentong+Li+Wenyu+Liu+Xiaolu+Liu+Jianke+Zhu",
    "gs_search_success": true,
    "gs_authors": [
      "SmHr4W4AAAAJ",
      "Jj0jbL8AAAAJ",
      "SC-WmzwAAAAJ",
      "2ihtou4AAAAJ",
      "MJjM6BcAAAAJ"
    ],
    "citation_count": 30,
    "gs_author_count": 5
  },
  {
    "arxiv_id": "2404.18926",
    "title": "Point Cloud Models Improve Visual Robustness in Robotic Learners",
    "year": 2024,
    "published": "2024-04-29T17:59:11Z",
    "categories": [
      "cs.RO",
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "Visual control policies can encounter significant performance degradation when visual conditions like lighting or camera position differ from those seen during training -- often exhibiting sharp declines in capability even for minor differences. In this work, we examine robustness to a suite of these types of visual changes for RGB-D and point cloud based visual control policies. To perform these experiments on both model-free and model-based reinforcement learners, we introduce a novel Point Cl",
    "arxiv_url": "https://arxiv.org/abs/2404.18926v1",
    "pdf_url": "https://arxiv.org/pdf/2404.18926v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2404.18926",
    "arxiv_authors": [
      "Skand Peri",
      "Iain Lee",
      "Chanho Kim",
      "Li Fuxin",
      "Tucker Hermans",
      "Stefan Lee"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Point+Cloud+Models+Improve+Visual+Robustness+in+Robotic+Learners+Skand+Peri+Iain+Lee+Chanho+Kim+Li+Fuxin+Tucker+Hermans",
    "gs_search_success": true,
    "gs_authors": [
      "G5_VFfkAAAAJ",
      "xARSfT4AAAAJ",
      "snDpfA0AAAAJ",
      "AaY4U-wAAAAJ",
      "C7PpyhYAAAAJ"
    ],
    "citation_count": 10,
    "gs_author_count": null
  },
  {
    "arxiv_id": "2311.11512",
    "title": "Seeing through the Mask: Multi-task Generative Mask Decoupling Face Recognition",
    "year": 2023,
    "published": "2023-11-20T03:23:03Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "The outbreak of COVID-19 pandemic make people wear masks more frequently than ever. Current general face recognition system suffers from serious performance degradation,when encountering occluded scenes. The potential reason is that face features are corrupted by occlusions on key facial regions. To tackle this problem, previous works either extract identity-related embeddings on feature level by additional mask prediction, or restore the occluded facial part by generative models. However, the f",
    "arxiv_url": "https://arxiv.org/abs/2311.11512v1",
    "pdf_url": "https://arxiv.org/pdf/2311.11512v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2311.11512",
    "arxiv_authors": [
      "Zhaohui Wang",
      "Sufang Zhang",
      "Jianteng Peng",
      "Xinyi Wang",
      "Yandong Guo"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Seeing+through+the+Mask%3A+Multi-task+Generative+Mask+Decoupling+Face+Recognition+Zhaohui+Wang+Sufang+Zhang+Jianteng+Peng+Xinyi+Wang+Yandong+Guo",
    "gs_search_success": true,
    "gs_authors": [],
    "citation_count": null,
    "gs_author_count": 5
  }
]
