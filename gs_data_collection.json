[
  {
    "arxiv_id": "2503.13184",
    "title": "Triad: Empowering LMM-based Anomaly Detection with Vision Expert-guided Visual Tokenizer and Manufacturing Process",
    "year": 2025,
    "published": "2025-03-17T13:56:57Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Although recent methods have tried to introduce large multimodal models (LMMs) into industrial anomaly detection (IAD), their generalization in the IAD field is far inferior to that for general purposes. We summarize the main reasons for this gap into two aspects. On one hand, general-purpose LMMs lack cognition of defects in the visual modality, thereby failing to sufficiently focus on defect areas. Therefore, we propose to modify the AnyRes structure of the LLaVA model, providing the potential",
    "arxiv_url": "https://arxiv.org/abs/2503.13184v2",
    "pdf_url": "https://arxiv.org/pdf/2503.13184v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2503.13184",
    "arxiv_authors": [
      "Yuanze Li",
      "Shihao Yuan",
      "Haolin Wang",
      "Qizhang Li",
      "Ming Liu",
      "Chen Xu",
      "Guangming Shi",
      "Wangmeng Zuo"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Triad%3A+Empowering+LMM-based+Anomaly+Detection+with+Vision+Expert-guided+Visual+Tokenizer+and+Manufacturing+Process+Yuanze+Li+Shihao+Yuan+Haolin+Wang+Qizhang+Li+Ming+Liu",
    "gs_search_success": true,
    "gs_authors": [
      "CdV5LfQAAAAJ",
      "rUOpCEYAAAAJ"
    ],
    "citation_count": 3
  },
  {
    "arxiv_id": "2503.06382",
    "title": "X-LRM: X-ray Large Reconstruction Model for Extremely Sparse-View Computed Tomography Recovery in One Second",
    "year": 2025,
    "published": "2025-03-09T01:39:59Z",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "abstract": "Sparse-view 3D CT reconstruction aims to recover volumetric structures from a limited number of 2D X-ray projections. Existing feedforward methods are constrained by the limited capacity of CNN-based architectures and the scarcity of large-scale training datasets. In this paper, we propose an X-ray Large Reconstruction Model (X-LRM) for extremely sparse-view (<10 views) CT reconstruction. X-LRM consists of two key components: X-former and X-triplane. Our X-former can handle an arbitrary number o",
    "arxiv_url": "https://arxiv.org/abs/2503.06382v1",
    "pdf_url": "https://arxiv.org/pdf/2503.06382v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2503.06382",
    "arxiv_authors": [
      "Guofeng Zhang",
      "Ruyi Zha",
      "Hao He",
      "Yixun Liang",
      "Alan Yuille",
      "Hongdong Li",
      "Yuanhao Cai"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=X-LRM%3A+X-ray+Large+Reconstruction+Model+for+Extremely+Sparse-View+Computed+Tomography+Recovery+in+One+Second+Guofeng+Zhang+Ruyi+Zha+Hao+He+Yixun+Liang+Alan+Yuille",
    "gs_search_success": true,
    "gs_authors": [
      "rmX3lvEAAAAJ",
      "vl0mzhEAAAAJ",
      "lOEX3aUAAAAJ",
      "_5W6W7oAAAAJ",
      "FJ-huxgAAAAJ",
      "3YozQwcAAAAJ"
    ],
    "citation_count": 3
  },
  {
    "arxiv_id": "2501.03729",
    "title": "Realistic Test-Time Adaptation of Vision-Language Models",
    "year": 2025,
    "published": "2025-01-07T12:17:25Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "The zero-shot capabilities of Vision-Language Models (VLMs) have been widely leveraged to improve predictive performance. However, previous works on transductive or test-time adaptation (TTA) often make strong assumptions about the data distribution, such as the presence of all classes. Our work challenges these favorable deployment scenarios, and introduces a more realistic evaluation framework, including: (i) a variable number of effective classes for adaptation within a single batch, and (ii)",
    "arxiv_url": "https://arxiv.org/abs/2501.03729v1",
    "pdf_url": "https://arxiv.org/pdf/2501.03729v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2501.03729",
    "arxiv_authors": [
      "Maxime Zanella",
      "Clément Fuchs",
      "Christophe De Vleeschouwer",
      "Ismail Ben Ayed"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Realistic+Test-Time+Adaptation+of+Vision-Language+Models+Maxime+Zanella+Cl%C3%A9ment+Fuchs+Christophe+De+Vleeschouwer+Ismail+Ben+Ayed",
    "gs_search_success": true,
    "gs_authors": [
      "xb3Zc3cAAAAJ",
      "FIoE9YIAAAAJ",
      "ZXWUJ4QAAAAJ",
      "29vyUccAAAAJ"
    ],
    "citation_count": 7
  },
  {
    "arxiv_id": "2403.10452",
    "title": "Robust Shape Fitting for 3D Scene Abstraction",
    "year": 2024,
    "published": "2024-03-15T16:37:43Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Humans perceive and construct the world as an arrangement of simple parametric models. In particular, we can often describe man-made environments using volumetric primitives such as cuboids or cylinders. Inferring these primitives is important for attaining high-level, abstract scene descriptions. Previous approaches for primitive-based abstraction estimate shape parameters directly and are only able to reproduce simple objects. In contrast, we propose a robust estimator for primitive fitting, w",
    "arxiv_url": "https://arxiv.org/abs/2403.10452v1",
    "pdf_url": "https://arxiv.org/pdf/2403.10452v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2403.10452",
    "arxiv_authors": [
      "Florian Kluger",
      "Eric Brachmann",
      "Michael Ying Yang",
      "Bodo Rosenhahn"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Robust+Shape+Fitting+for+3D+Scene+Abstraction+Florian+Kluger+Eric+Brachmann+Michael+Ying+Yang+Bodo+Rosenhahn",
    "gs_search_success": true,
    "gs_authors": [
      "qq3TxtcAAAAJ",
      "cAIshsYAAAAJ",
      "lgGmYBoAAAAJ",
      "KNIzPH8AAAAJ"
    ],
    "citation_count": 6
  },
  {
    "arxiv_id": "2501.01101",
    "title": "Deformable Gaussian Splatting for Efficient and High-Fidelity Reconstruction of Surgical Scenes",
    "year": 2025,
    "published": "2025-01-02T06:50:25Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Efficient and high-fidelity reconstruction of deformable surgical scenes is a critical yet challenging task. Building on recent advancements in 3D Gaussian splatting, current methods have seen significant improvements in both reconstruction quality and rendering speed. However, two major limitations remain: (1) difficulty in handling irreversible dynamic changes, such as tissue shearing, which are common in surgical scenes; and (2) the lack of hierarchical modeling for surgical scene deformation",
    "arxiv_url": "https://arxiv.org/abs/2501.01101v1",
    "pdf_url": "https://arxiv.org/pdf/2501.01101v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2501.01101",
    "arxiv_authors": [
      "Jiwei Shan",
      "Zeyu Cai",
      "Cheng-Tai Hsieh",
      "Shing Shin Cheng",
      "Hesheng Wang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Deformable+Gaussian+Splatting+for+Efficient+and+High-Fidelity+Reconstruction+of+Surgical+Scenes+Jiwei+Shan+Zeyu+Cai+Cheng-Tai+Hsieh+Shing+Shin+Cheng+Hesheng+Wang",
    "gs_search_success": true,
    "gs_authors": [
      "8VYntc0AAAAJ",
      "F-ucRegAAAAJ",
      "s4AaiW4AAAAJ",
      "q6AY9XsAAAAJ"
    ],
    "citation_count": 7
  },
  {
    "arxiv_id": "2410.05301",
    "title": "Diffusion-based Unsupervised Audio-visual Speech Enhancement",
    "year": 2024,
    "published": "2024-10-04T12:22:54Z",
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.CV",
      "cs.LG",
      "eess.AS",
      "eess.SP"
    ],
    "abstract": "This paper proposes a new unsupervised audio-visual speech enhancement (AVSE) approach that combines a diffusion-based audio-visual speech generative model with a non-negative matrix factorization (NMF) noise model. First, the diffusion model is pre-trained on clean speech conditioned on corresponding video data to simulate the speech generative distribution. This pre-trained model is then paired with the NMF-based noise model to estimate clean speech iteratively. Specifically, a diffusion-based",
    "arxiv_url": "https://arxiv.org/abs/2410.05301v2",
    "pdf_url": "https://arxiv.org/pdf/2410.05301v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2410.05301",
    "arxiv_authors": [
      "Jean-Eudes Ayilo",
      "Mostafa Sadeghi",
      "Romain Serizel",
      "Xavier Alameda-Pineda"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Diffusion-based+Unsupervised+Audio-visual+Speech+Enhancement+Jean-Eudes+Ayilo+Mostafa+Sadeghi+Romain+Serizel+Xavier+Alameda-Pineda",
    "gs_search_success": true,
    "gs_authors": [
      "_PXk20cAAAAJ",
      "ukI2bz8AAAAJ",
      "gSQ5xpcAAAAJ"
    ],
    "citation_count": 6
  },
  {
    "arxiv_id": "2406.13006",
    "title": "Weighted Sum of Segmented Correlation: An Efficient Method for Spectra Matching in Hyperspectral Images",
    "year": 2024,
    "published": "2024-06-18T18:51:00Z",
    "categories": [
      "cs.CV",
      "cs.ET",
      "eess.IV"
    ],
    "abstract": "Matching a target spectrum with known spectra in a spectral library is a common method for material identification in hyperspectral imaging research. Hyperspectral spectra exhibit precise absorption features across different wavelength segments, and the unique shapes and positions of these absorptions create distinct spectral signatures for each material, aiding in their identification. Therefore, only the specific positions can be considered for material identification. This study introduces th",
    "arxiv_url": "https://arxiv.org/abs/2406.13006v1",
    "pdf_url": "https://arxiv.org/pdf/2406.13006v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2406.13006",
    "arxiv_authors": [
      "Sampriti Soor",
      "Priyanka Kumari",
      "B. S. Daya Sagar",
      "Amba Shetty"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Weighted+Sum+of+Segmented+Correlation%3A+An+Efficient+Method+for+Spectra+Matching+in+Hyperspectral+Images+Sampriti+Soor+Priyanka+Kumari+B.+S.+Daya+Sagar+Amba+Shetty",
    "gs_search_success": true,
    "gs_authors": [
      "epOfTAYAAAAJ",
      "MDqXWJAAAAAJ",
      "2_rp5LIAAAAJ",
      "kd6hGOkAAAAJ"
    ],
    "citation_count": 1
  },
  {
    "arxiv_id": "2504.14717",
    "title": "TAPIP3D: Tracking Any Point in Persistent 3D Geometry",
    "year": 2025,
    "published": "2025-04-20T19:09:43Z",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "We introduce TAPIP3D, a novel approach for long-term 3D point tracking in monocular RGB and RGB-D videos. TAPIP3D represents videos as camera-stabilized spatio-temporal feature clouds, leveraging depth and camera motion information to lift 2D video features into a 3D world space where camera movement is effectively canceled out. Within this stabilized 3D representation, TAPIP3D iteratively refines multi-frame motion estimates, enabling robust point tracking over long time horizons. To handle the",
    "arxiv_url": "https://arxiv.org/abs/2504.14717v3",
    "pdf_url": "https://arxiv.org/pdf/2504.14717v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2504.14717",
    "arxiv_authors": [
      "Bowei Zhang",
      "Lei Ke",
      "Adam W. Harley",
      "Katerina Fragkiadaki"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=TAPIP3D%3A+Tracking+Any+Point+in+Persistent+3D+Geometry+Bowei+Zhang+Lei+Ke+Adam+W.+Harley+Katerina+Fragkiadaki",
    "gs_search_success": true,
    "gs_authors": [
      "tYH72AYAAAAJ",
      "FWp7728AAAAJ",
      "OB6vAtkAAAAJ",
      "WseeNrUAAAAJ"
    ],
    "citation_count": 17
  },
  {
    "arxiv_id": "2407.04819",
    "title": "RPN: Reconciled Polynomial Network Towards Unifying PGMs, Kernel SVMs, MLP and KAN",
    "year": 2024,
    "published": "2024-07-05T19:00:18Z",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "cs.IT",
      "stat.ML"
    ],
    "abstract": "In this paper, we will introduce a novel deep model named Reconciled Polynomial Network (RPN) for deep function learning. RPN has a very general architecture and can be used to build models with various complexities, capacities, and levels of completeness, which all contribute to the correctness of these models. As indicated in the subtitle, RPN can also serve as the backbone to unify different base models into one canonical representation. This includes non-deep models, like probabilistic graph",
    "arxiv_url": "https://arxiv.org/abs/2407.04819v1",
    "pdf_url": "https://arxiv.org/pdf/2407.04819v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2407.04819",
    "arxiv_authors": [
      "Jiawei Zhang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=RPN%3A+Reconciled+Polynomial+Network+Towards+Unifying+PGMs%2C+Kernel+SVMs%2C+MLP+and+KAN+Jiawei+Zhang",
    "gs_search_success": true,
    "gs_authors": [
      "7AkZSJsAAAAJ"
    ],
    "citation_count": 7
  },
  {
    "arxiv_id": "2501.10144",
    "title": "A Vision-Language Framework for Multispectral Scene Representation Using Language-Grounded Features",
    "year": 2025,
    "published": "2025-01-17T12:12:33Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Scene understanding in remote sensing often faces challenges in generating accurate representations for complex environments such as various land use areas or coastal regions, which may also include snow, clouds, or haze. To address this, we present a vision-language framework named Spectral LLaVA, which integrates multispectral data with vision-language alignment techniques to enhance scene representation and description. Using the BigEarthNet v2 dataset from Sentinel-2, we establish a baseline",
    "arxiv_url": "https://arxiv.org/abs/2501.10144v1",
    "pdf_url": "https://arxiv.org/pdf/2501.10144v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2501.10144",
    "arxiv_authors": [
      "Enes Karanfil",
      "Nevrez Imamoglu",
      "Erkut Erdem",
      "Aykut Erdem"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=A+Vision-Language+Framework+for+Multispectral+Scene+Representation+Using+Language-Grounded+Features+Enes+Karanfil+Nevrez+Imamoglu+Erkut+Erdem+Aykut+Erdem",
    "gs_search_success": true,
    "gs_authors": [
      "-xA1_OAAAAAJ",
      "VJgx61MAAAAJ",
      "eALwl74AAAAJ",
      "J7UN2eoAAAAJ"
    ],
    "citation_count": 2
  },
  {
    "arxiv_id": "2407.11566",
    "title": "TGIF: Text-Guided Inpainting Forgery Dataset",
    "year": 2024,
    "published": "2024-07-16T10:19:14Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CR",
      "cs.MM"
    ],
    "abstract": "Digital image manipulation has become increasingly accessible and realistic with the advent of generative AI technologies. Recent developments allow for text-guided inpainting, making sophisticated image edits possible with minimal effort. This poses new challenges for digital media forensics. For example, diffusion model-based approaches could either splice the inpainted region into the original image, or regenerate the entire image. In the latter case, traditional image forgery localization (I",
    "arxiv_url": "https://arxiv.org/abs/2407.11566v2",
    "pdf_url": "https://arxiv.org/pdf/2407.11566v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2407.11566",
    "arxiv_authors": [
      "Hannes Mareen",
      "Dimitrios Karageorgiou",
      "Glenn Van Wallendael",
      "Peter Lambert",
      "Symeon Papadopoulos"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=TGIF%3A+Text-Guided+Inpainting+Forgery+Dataset+Hannes+Mareen+Dimitrios+Karageorgiou+Glenn+Van+Wallendael+Peter+Lambert+Symeon+Papadopoulos",
    "gs_search_success": true,
    "gs_authors": [
      "SNmMEJEAAAAJ",
      "GuhyORoAAAAJ",
      "0Y690dMAAAAJ",
      "kFxdDCkAAAAJ",
      "CaDakRwAAAAJ"
    ],
    "citation_count": 13
  },
  {
    "arxiv_id": "2411.09037",
    "title": "Pay Attention to the Keys: Visual Piano Transcription Using Transformers",
    "year": 2024,
    "published": "2024-11-13T21:31:12Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Visual piano transcription (VPT) is the task of obtaining a symbolic representation of a piano performance from visual information alone (e.g., from a top-down video of the piano keyboard). In this work we propose a VPT system based on the vision transformer (ViT), which surpasses previous methods based on convolutional neural networks (CNNs). Our system is trained on the newly introduced R3 dataset, consisting of ca.~31 hours of synchronized video and MIDI recordings of piano performances. We a",
    "arxiv_url": "https://arxiv.org/abs/2411.09037v2",
    "pdf_url": "https://arxiv.org/pdf/2411.09037v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2411.09037",
    "arxiv_authors": [
      "Uros Zivanovic",
      "Ivan Pilkov",
      "Carlos Eduardo Cancino-Chacón"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Pay+Attention+to+the+Keys%3A+Visual+Piano+Transcription+Using+Transformers+Uros+Zivanovic+Ivan+Pilkov+Carlos+Eduardo+Cancino-Chac%C3%B3n",
    "gs_search_success": false,
    "gs_authors": [],
    "citation_count": null,
    "error_type": "fetch_failed"
  },
  {
    "arxiv_id": "2501.05236",
    "title": "Automated external cervical resorption segmentation in cone-beam CT using local texture features",
    "year": 2025,
    "published": "2025-01-09T13:43:01Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "External cervical resorption (ECR) is a resorptive process affecting teeth. While in some patients, active resorption ceases and gets replaced by osseous tissue, in other cases, the resorption progresses and ultimately results in tooth loss. For proper ECR assessment, cone-beam computed tomography (CBCT) is the recommended imaging modality, enabling a 3-D characterization of these lesions. While it is possible to manually identify and measure ECR resorption in CBCT scans, this process can be tim",
    "arxiv_url": "https://arxiv.org/abs/2501.05236v1",
    "pdf_url": "https://arxiv.org/pdf/2501.05236v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2501.05236",
    "arxiv_authors": [
      "Sadhana Ravikumar",
      "Asma A. Khan",
      "Matthew C. Davis",
      "Beatriz Paniagua"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Automated+external+cervical+resorption+segmentation+in+cone-beam+CT+using+local+texture+features+Sadhana+Ravikumar+Asma+A.+Khan+Matthew+C.+Davis+Beatriz+Paniagua",
    "gs_search_success": true,
    "gs_authors": [
      "N3etvb4AAAAJ",
      "TxK5raEAAAAJ",
      "PRVRWcwAAAAJ"
    ],
    "citation_count": 1
  },
  {
    "arxiv_id": "2310.06945",
    "title": "End-to-end Evaluation of Practical Video Analytics Systems for Face Detection and Recognition",
    "year": 2023,
    "published": "2023-10-10T19:06:10Z",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "abstract": "Practical video analytics systems that are deployed in bandwidth constrained environments like autonomous vehicles perform computer vision tasks such as face detection and recognition. In an end-to-end face analytics system, inputs are first compressed using popular video codecs like HEVC and then passed onto modules that perform face detection, alignment, and recognition sequentially. Typically, the modules of these systems are evaluated independently using task-specific imbalanced datasets tha",
    "arxiv_url": "https://arxiv.org/abs/2310.06945v1",
    "pdf_url": "https://arxiv.org/pdf/2310.06945v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2310.06945",
    "arxiv_authors": [
      "Praneet Singh",
      "Edward J. Delp",
      "Amy R. Reibman"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=End-to-end+Evaluation+of+Practical+Video+Analytics+Systems+for+Face+Detection+and+Recognition+Praneet+Singh+Edward+J.+Delp+Amy+R.+Reibman",
    "gs_search_success": true,
    "gs_authors": [
      "TUX6cg8AAAAJ",
      "owurYNEAAAAJ",
      "qxs5pc4AAAAJ"
    ],
    "citation_count": 5
  },
  {
    "arxiv_id": "2407.20917",
    "title": "How to Choose a Reinforcement-Learning Algorithm",
    "year": 2024,
    "published": "2024-07-30T15:54:18Z",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "stat.ML"
    ],
    "abstract": "The field of reinforcement learning offers a large variety of concepts and methods to tackle sequential decision-making problems. This variety has become so large that choosing an algorithm for a task at hand can be challenging. In this work, we streamline the process of choosing reinforcement-learning algorithms and action-distribution families. We provide a structured overview of existing methods and their properties, as well as guidelines for when to choose which methods. An interactive versi",
    "arxiv_url": "https://arxiv.org/abs/2407.20917v1",
    "pdf_url": "https://arxiv.org/pdf/2407.20917v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2407.20917",
    "arxiv_authors": [
      "Fabian Bongratz",
      "Vladimir Golkov",
      "Lukas Mautner",
      "Luca Della Libera",
      "Frederik Heetmeyer",
      "Felix Czaja",
      "Julian Rodemann",
      "Daniel Cremers"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=How+to+Choose+a+Reinforcement-Learning+Algorithm+Fabian+Bongratz+Vladimir+Golkov+Lukas+Mautner+Luca+Della+Libera+Frederik+Heetmeyer",
    "gs_search_success": true,
    "gs_authors": [
      "fLjta2AAAAAJ",
      "k-vHBED9_CgC",
      "hIcjYocAAAAJ",
      "mylZOd4AAAAJ",
      "aQzZhi8AAAAJ",
      "rXQPW74AAAAJ"
    ],
    "citation_count": 2
  },
  {
    "arxiv_id": "2404.05069",
    "title": "AirShot: Efficient Few-Shot Detection for Autonomous Exploration",
    "year": 2024,
    "published": "2024-04-07T20:39:31Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Few-shot object detection has drawn increasing attention in the field of robotic exploration, where robots are required to find unseen objects with a few online provided examples. Despite recent efforts have been made to yield online processing capabilities, slow inference speeds of low-powered robots fail to meet the demands of real-time detection-making them impractical for autonomous exploration. Existing methods still face performance and efficiency challenges, mainly due to unreliable featu",
    "arxiv_url": "https://arxiv.org/abs/2404.05069v1",
    "pdf_url": "https://arxiv.org/pdf/2404.05069v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2404.05069",
    "arxiv_authors": [
      "Zihan Wang",
      "Bowen Li",
      "Chen Wang",
      "Sebastian Scherer"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=AirShot%3A+Efficient+Few-Shot+Detection+for+Autonomous+Exploration+Zihan+Wang+Bowen+Li+Chen+Wang+Sebastian+Scherer",
    "gs_search_success": true,
    "gs_authors": [
      "VsCA3oQAAAAJ",
      "gxoPfIYAAAAJ",
      "XIAMHVMAAAAJ",
      "vZfmKl4AAAAJ"
    ],
    "citation_count": 14
  },
  {
    "arxiv_id": "2309.05793",
    "title": "PhotoVerse: Tuning-Free Image Customization with Text-to-Image Diffusion Models",
    "year": 2023,
    "published": "2023-09-11T19:59:43Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Personalized text-to-image generation has emerged as a powerful and sought-after tool, empowering users to create customized images based on their specific concepts and prompts. However, existing approaches to personalization encounter multiple challenges, including long tuning times, large storage requirements, the necessity for multiple input images per identity, and limitations in preserving identity and editability. To address these obstacles, we present PhotoVerse, an innovative methodology",
    "arxiv_url": "https://arxiv.org/abs/2309.05793v1",
    "pdf_url": "https://arxiv.org/pdf/2309.05793v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2309.05793",
    "arxiv_authors": [
      "Li Chen",
      "Mengyi Zhao",
      "Yiheng Liu",
      "Mingxu Ding",
      "Yangyang Song",
      "Shizun Wang",
      "Xu Wang",
      "Hao Yang",
      "Jing Liu",
      "Kang Du",
      "Min Zheng"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=PhotoVerse%3A+Tuning-Free+Image+Customization+with+Text-to-Image+Diffusion+Models+Li+Chen+Mengyi+Zhao+Yiheng+Liu+Mingxu+Ding+Yangyang+Song",
    "gs_search_success": true,
    "gs_authors": [
      "vJXUa7MAAAAJ",
      "vlgvrD0AAAAJ",
      "n9rw7ZgAAAAJ",
      "fv8F6CEAAAAJ",
      "HxpOiHoAAAAJ",
      "cFS823sAAAAJ"
    ],
    "citation_count": 69
  },
  {
    "arxiv_id": "2501.06215",
    "title": "Fitting Different Interactive Information: Joint Classification of Emotion and Intention",
    "year": 2025,
    "published": "2025-01-05T05:23:27Z",
    "categories": [
      "cs.CV",
      "cs.CL",
      "cs.LG",
      "cs.MM",
      "eess.AS"
    ],
    "abstract": "This paper is the first-place solution for ICASSP MEIJU@2025 Track I, which focuses on low-resource multimodal emotion and intention recognition. How to effectively utilize a large amount of unlabeled data, while ensuring the mutual promotion of different difficulty levels tasks in the interaction stage, these two points become the key to the competition. In this paper, pseudo-label labeling is carried out on the model trained with labeled data, and samples with high confidence and their labels ",
    "arxiv_url": "https://arxiv.org/abs/2501.06215v1",
    "pdf_url": "https://arxiv.org/pdf/2501.06215v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2501.06215",
    "arxiv_authors": [
      "Xinger Li",
      "Zhiqiang Zhong",
      "Bo Huang",
      "Yang Yang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Fitting+Different+Interactive+Information%3A+Joint+Classification+of+Emotion+and+Intention+Xinger+Li+Zhiqiang+Zhong+Bo+Huang+Yang+Yang",
    "gs_search_success": true,
    "gs_authors": [
      "_6NJip0AAAAJ"
    ],
    "citation_count": null
  },
  {
    "arxiv_id": "2406.01059",
    "title": "VIP: Versatile Image Outpainting Empowered by Multimodal Large Language Model",
    "year": 2024,
    "published": "2024-06-03T07:14:19Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "In this paper, we focus on resolving the problem of image outpainting, which aims to extrapolate the surrounding parts given the center contents of an image. Although recent works have achieved promising performance, the lack of versatility and customization hinders their practical applications in broader scenarios. Therefore, this work presents a novel image outpainting framework that is capable of customizing the results according to the requirement of users. First of all, we take advantage of",
    "arxiv_url": "https://arxiv.org/abs/2406.01059v3",
    "pdf_url": "https://arxiv.org/pdf/2406.01059v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2406.01059",
    "arxiv_authors": [
      "Jinze Yang",
      "Haoran Wang",
      "Zining Zhu",
      "Chenglong Liu",
      "Meng Wymond Wu",
      "Mingming Sun"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=VIP%3A+Versatile+Image+Outpainting+Empowered+by+Multimodal+Large+Language+Model+Jinze+Yang+Haoran+Wang+Zining+Zhu+Chenglong+Liu+Meng+Wymond+Wu",
    "gs_search_success": true,
    "gs_authors": [
      "xfnL2IEAAAAJ",
      "_PfM-AUAAAAJ",
      "kUqmflsAAAAJ"
    ],
    "citation_count": 8
  },
  {
    "arxiv_id": "2412.03200",
    "title": "Fab-ME: A Vision State-Space and Attention-Enhanced Framework for Fabric Defect Detection",
    "year": 2024,
    "published": "2024-12-04T10:40:17Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Effective defect detection is critical for ensuring the quality, functionality, and economic value of textile products. However, existing methods face challenges in achieving high accuracy, real-time performance, and efficient global information extraction. To address these issues, we propose Fab-ME, an advanced framework based on YOLOv8s, specifically designed for the accurate detection of 20 fabric defect types. Our contributions include the introduction of the cross-stage partial bottleneck w",
    "arxiv_url": "https://arxiv.org/abs/2412.03200v2",
    "pdf_url": "https://arxiv.org/pdf/2412.03200v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2412.03200",
    "arxiv_authors": [
      "Shuai Wang",
      "Huiyan Kong",
      "Baotian Li",
      "Fa Zheng"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Fab-ME%3A+A+Vision+State-Space+and+Attention-Enhanced+Framework+for+Fabric+Defect+Detection+Shuai+Wang+Huiyan+Kong+Baotian+Li+Fa+Zheng",
    "gs_search_success": true,
    "gs_authors": [],
    "citation_count": 2
  },
  {
    "arxiv_id": "2402.14371",
    "title": "HR-APR: APR-agnostic Framework with Uncertainty Estimation and Hierarchical Refinement for Camera Relocalisation",
    "year": 2024,
    "published": "2024-02-22T08:21:46Z",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "abstract": "Absolute Pose Regressors (APRs) directly estimate camera poses from monocular images, but their accuracy is unstable for different queries. Uncertainty-aware APRs provide uncertainty information on the estimated pose, alleviating the impact of these unreliable predictions. However, existing uncertainty modelling techniques are often coupled with a specific APR architecture, resulting in suboptimal performance compared to state-of-the-art (SOTA) APR methods. This work introduces a novel APR-agnos",
    "arxiv_url": "https://arxiv.org/abs/2402.14371v2",
    "pdf_url": "https://arxiv.org/pdf/2402.14371v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2402.14371",
    "arxiv_authors": [
      "Changkun Liu",
      "Shuai Chen",
      "Yukun Zhao",
      "Huajian Huang",
      "Victor Prisacariu",
      "Tristan Braud"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=HR-APR%3A+APR-agnostic+Framework+with+Uncertainty+Estimation+and+Hierarchical+Refinement+for+Camera+Relocalisation+Changkun+Liu+Shuai+Chen+Yukun+Zhao+Huajian+Huang+Victor+Prisacariu",
    "gs_search_success": true,
    "gs_authors": [
      "NcLael4AAAAJ",
      "c0xTh_YAAAAJ",
      "25npC_YAAAAJ",
      "rOhG9NoAAAAJ",
      "ZOZtoQUAAAAJ",
      "GmWA-LoAAAAJ"
    ],
    "citation_count": 14
  },
  {
    "arxiv_id": "2403.17808",
    "title": "Annotated Biomedical Video Generation using Denoising Diffusion Probabilistic Models and Flow Fields",
    "year": 2024,
    "published": "2024-03-26T15:45:29Z",
    "categories": [
      "eess.IV",
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "The segmentation and tracking of living cells play a vital role within the biomedical domain, particularly in cancer research, drug development, and developmental biology. These are usually tedious and time-consuming tasks that are traditionally done by biomedical experts. Recently, to automatize these processes, deep learning based segmentation and tracking methods have been proposed. These methods require large-scale datasets and their full potential is constrained by the scarcity of annotated",
    "arxiv_url": "https://arxiv.org/abs/2403.17808v1",
    "pdf_url": "https://arxiv.org/pdf/2403.17808v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2403.17808",
    "arxiv_authors": [
      "Rüveyda Yilmaz",
      "Dennis Eschweiler",
      "Johannes Stegmaier"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Annotated+Biomedical+Video+Generation+using+Denoising+Diffusion+Probabilistic+Models+and+Flow+Fields+R%C3%BCveyda+Yilmaz+Dennis+Eschweiler+Johannes+Stegmaier",
    "gs_search_success": true,
    "gs_authors": [
      "JV57X-wAAAAJ",
      "gwf8A9YAAAAJ",
      "L1Pa16AAAAAJ"
    ],
    "citation_count": 6
  },
  {
    "arxiv_id": "2409.14984",
    "title": "SocialCircle+: Learning the Angle-based Conditioned Interaction Representation for Pedestrian Trajectory Prediction",
    "year": 2024,
    "published": "2024-09-23T13:02:12Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Trajectory prediction is a crucial aspect of understanding human behaviors. Researchers have made efforts to represent socially interactive behaviors among pedestrians and utilize various networks to enhance prediction capability. Unfortunately, they still face challenges not only in fully explaining and measuring how these interactive behaviors work to modify trajectories but also in modeling pedestrians' preferences to plan or participate in social interactions in response to the changeable ph",
    "arxiv_url": "https://arxiv.org/abs/2409.14984v1",
    "pdf_url": "https://arxiv.org/pdf/2409.14984v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2409.14984",
    "arxiv_authors": [
      "Conghao Wong",
      "Beihao Xia",
      "Ziqian Zou",
      "Xinge You"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=SocialCircle%2B%3A+Learning+the+Angle-based+Conditioned+Interaction+Representation+for+Pedestrian+Trajectory+Prediction+Conghao+Wong+Beihao+Xia+Ziqian+Zou+Xinge+You",
    "gs_search_success": true,
    "gs_authors": [
      "v7bRZX8AAAAJ",
      "P-WtHKgAAAAJ",
      "Lx_iqH8AAAAJ",
      "cA_6-EwAAAAJ"
    ],
    "citation_count": 6
  },
  {
    "arxiv_id": "2412.03407",
    "title": "Skel3D: Skeleton Guided Novel View Synthesis",
    "year": 2024,
    "published": "2024-12-04T15:45:20Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "In this paper, we present an approach for monocular open-set novel view synthesis (NVS) that leverages object skeletons to guide the underlying diffusion model. Building upon a baseline that utilizes a pre-trained 2D image generator, our method takes advantage of the Objaverse dataset, which includes animated objects with bone structures. By introducing a skeleton guide layer following the existing ray conditioning normalization (RCN) layer, our approach enhances pose accuracy and multi-view con",
    "arxiv_url": "https://arxiv.org/abs/2412.03407v1",
    "pdf_url": "https://arxiv.org/pdf/2412.03407v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2412.03407",
    "arxiv_authors": [
      "Aron Fóthi",
      "Bence Fazekas",
      "Natabara Máté Gyöngyössy",
      "Kristian Fenech"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Skel3D%3A+Skeleton+Guided+Novel+View+Synthesis+Aron+F%C3%B3thi+Bence+Fazekas+Natabara+M%C3%A1t%C3%A9+Gy%C3%B6ngy%C3%B6ssy+Kristian+Fenech",
    "gs_search_success": true,
    "gs_authors": [
      "-drqZsoAAAAJ",
      "2UlkRh0AAAAJ",
      "xpQLT_oAAAAJ"
    ],
    "citation_count": null
  },
  {
    "arxiv_id": "2307.00038",
    "title": "Training-free Object Counting with Prompts",
    "year": 2023,
    "published": "2023-06-30T13:26:30Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "This paper tackles the problem of object counting in images. Existing approaches rely on extensive training data with point annotations for each object, making data collection labor-intensive and time-consuming. To overcome this, we propose a training-free object counter that treats the counting task as a segmentation problem. Our approach leverages the Segment Anything Model (SAM), known for its high-quality masks and zero-shot segmentation capability. However, the vanilla mask generation metho",
    "arxiv_url": "https://arxiv.org/abs/2307.00038v2",
    "pdf_url": "https://arxiv.org/pdf/2307.00038v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2307.00038",
    "arxiv_authors": [
      "Zenglin Shi",
      "Ying Sun",
      "Mengmi Zhang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Training-free+Object+Counting+with+Prompts+Zenglin+Shi+Ying+Sun+Mengmi+Zhang",
    "gs_search_success": true,
    "gs_authors": [
      "G2sVOhcAAAAJ",
      "gGK6E38AAAAJ",
      "AJReaQ8AAAAJ"
    ],
    "citation_count": 45
  },
  {
    "arxiv_id": "2401.04750",
    "title": "DedustNet: A Frequency-dominated Swin Transformer-based Wavelet Network for Agricultural Dust Removal",
    "year": 2024,
    "published": "2024-01-09T13:40:01Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "While dust significantly affects the environmental perception of automated agricultural machines, the existing deep learning-based methods for dust removal require further research and improvement in this area to improve the performance and reliability of automated agricultural machines in agriculture. We propose an end-to-end trainable learning network (DedustNet) to solve the real-world agricultural dust removal task. To our knowledge, DedustNet is the first time Swin Transformer-based units h",
    "arxiv_url": "https://arxiv.org/abs/2401.04750v1",
    "pdf_url": "https://arxiv.org/pdf/2401.04750v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2401.04750",
    "arxiv_authors": [
      "Shengli Zhang",
      "Zhiyong Tao",
      "Sen Lin"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=DedustNet%3A+A+Frequency-dominated+Swin+Transformer-based+Wavelet+Network+for+Agricultural+Dust+Removal+Shengli+Zhang+Zhiyong+Tao+Sen+Lin",
    "gs_search_success": true,
    "gs_authors": [
      "R71r-pwAAAAJ",
      "qCY7Af8AAAAJ"
    ],
    "citation_count": 1
  },
  {
    "arxiv_id": "2309.11382",
    "title": "Discuss Before Moving: Visual Language Navigation via Multi-expert Discussions",
    "year": 2023,
    "published": "2023-09-20T15:04:49Z",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CL",
      "cs.CV"
    ],
    "abstract": "Visual language navigation (VLN) is an embodied task demanding a wide range of skills encompassing understanding, perception, and planning. For such a multifaceted challenge, previous VLN methods totally rely on one model's own thinking to make predictions within one round. However, existing models, even the most advanced large language model GPT4, still struggle with dealing with multiple tasks by single-round self-thinking. In this work, drawing inspiration from the expert consultation meeting",
    "arxiv_url": "https://arxiv.org/abs/2309.11382v1",
    "pdf_url": "https://arxiv.org/pdf/2309.11382v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2309.11382",
    "arxiv_authors": [
      "Yuxing Long",
      "Xiaoqi Li",
      "Wenzhe Cai",
      "Hao Dong"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Discuss+Before+Moving%3A+Visual+Language+Navigation+via+Multi-expert+Discussions+Yuxing+Long+Xiaoqi+Li+Wenzhe+Cai+Hao+Dong",
    "gs_search_success": true,
    "gs_authors": [
      "UqQ41BIAAAAJ",
      "NHQcCyAAAAAJ",
      "vkQ5_LIAAAAJ",
      "xLFL4sMAAAAJ"
    ],
    "citation_count": 112
  },
  {
    "arxiv_id": "2309.15048",
    "title": "Class Incremental Learning via Likelihood Ratio Based Task Prediction",
    "year": 2023,
    "published": "2023-09-26T16:25:57Z",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "abstract": "Class incremental learning (CIL) is a challenging setting of continual learning, which learns a series of tasks sequentially. Each task consists of a set of unique classes. The key feature of CIL is that no task identifier (or task-id) is provided at test time. Predicting the task-id for each test sample is a challenging problem. An emerging theory-guided approach (called TIL+OOD) is to train a task-specific model for each task in a shared network for all tasks based on a task-incremental learni",
    "arxiv_url": "https://arxiv.org/abs/2309.15048v4",
    "pdf_url": "https://arxiv.org/pdf/2309.15048v4",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2309.15048",
    "arxiv_authors": [
      "Haowei Lin",
      "Yijia Shao",
      "Weinan Qian",
      "Ningxin Pan",
      "Yiduo Guo",
      "Bing Liu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Class+Incremental+Learning+via+Likelihood+Ratio+Based+Task+Prediction+Haowei+Lin+Yijia+Shao+Weinan+Qian+Ningxin+Pan+Yiduo+Guo",
    "gs_search_success": true,
    "gs_authors": [
      "ov-Cb2kAAAAJ",
      "Ng-DmJgAAAAJ",
      "Kt1bjZoAAAAJ",
      "H0zcQh4AAAAJ"
    ],
    "citation_count": 23
  },
  {
    "arxiv_id": "2310.19820",
    "title": "NetDistiller: Empowering Tiny Deep Learning via In-Situ Distillation",
    "year": 2023,
    "published": "2023-10-24T04:27:51Z",
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "abstract": "Boosting the task accuracy of tiny neural networks (TNNs) has become a fundamental challenge for enabling the deployments of TNNs on edge devices which are constrained by strict limitations in terms of memory, computation, bandwidth, and power supply. To this end, we propose a framework called NetDistiller to boost the achievable accuracy of TNNs by treating them as sub-networks of a weight-sharing teacher constructed by expanding the number of channels of the TNN. Specifically, the target TNN m",
    "arxiv_url": "https://arxiv.org/abs/2310.19820v1",
    "pdf_url": "https://arxiv.org/pdf/2310.19820v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2310.19820",
    "arxiv_authors": [
      "Shunyao Zhang",
      "Yonggan Fu",
      "Shang Wu",
      "Jyotikrishna Dass",
      "Haoran You",
      "Yingyan",
      "Lin"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=NetDistiller%3A+Empowering+Tiny+Deep+Learning+via+In-Situ+Distillation+Shunyao+Zhang+Yonggan+Fu+Shang+Wu+Jyotikrishna+Dass+Haoran+You",
    "gs_search_success": true,
    "gs_authors": [
      "xIU4snQAAAAJ",
      "fBp8EWgAAAAJ",
      "YJWCW8gAAAAJ",
      "dobDj4AAAAAJ",
      "dio8IesAAAAJ",
      "pt3GfXcAAAAJ"
    ],
    "citation_count": null
  },
  {
    "arxiv_id": "2408.14789",
    "title": "Revisiting Surgical Instrument Segmentation Without Human Intervention: A Graph Partitioning View",
    "year": 2024,
    "published": "2024-08-27T05:31:30Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Surgical instrument segmentation (SIS) on endoscopic images stands as a long-standing and essential task in the context of computer-assisted interventions for boosting minimally invasive surgery. Given the recent surge of deep learning methodologies and their data-hungry nature, training a neural predictive model based on massive expert-curated annotations has been dominating and served as an off-the-shelf approach in the field, which could, however, impose prohibitive burden to clinicians for p",
    "arxiv_url": "https://arxiv.org/abs/2408.14789v3",
    "pdf_url": "https://arxiv.org/pdf/2408.14789v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2408.14789",
    "arxiv_authors": [
      "Mingyu Sheng",
      "Jianan Fan",
      "Dongnan Liu",
      "Ron Kikinis",
      "Weidong Cai"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Revisiting+Surgical+Instrument+Segmentation+Without+Human+Intervention%3A+A+Graph+Partitioning+View+Mingyu+Sheng+Jianan+Fan+Dongnan+Liu+Ron+Kikinis+Weidong+Cai",
    "gs_search_success": true,
    "gs_authors": [
      "n01L0mEAAAAJ",
      "P7MIBuMAAAAJ",
      "N8qTc2AAAAAJ",
      "JZzb8XUAAAAJ"
    ],
    "citation_count": 3
  },
  {
    "arxiv_id": "2401.17231",
    "title": "Achieving More Human Brain-Like Vision via Human EEG Representational Alignment",
    "year": 2024,
    "published": "2024-01-30T18:18:41Z",
    "categories": [
      "cs.CV",
      "cs.LG",
      "cs.NE",
      "q-bio.NC"
    ],
    "abstract": "Despite advancements in artificial intelligence, object recognition models still lag behind in emulating visual information processing in human brains. Recent studies have highlighted the potential of using neural data to mimic brain processing; however, these often rely on invasive neural recordings from non-human subjects, leaving a critical gap in understanding human visual perception. Addressing this gap, we present, 'Re(presentational)Al(ignment)net', a vision model aligned with human brain",
    "arxiv_url": "https://arxiv.org/abs/2401.17231v3",
    "pdf_url": "https://arxiv.org/pdf/2401.17231v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2401.17231",
    "arxiv_authors": [
      "Zitong Lu",
      "Yile Wang",
      "Julie D. Golomb"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Achieving+More+Human+Brain-Like+Vision+via+Human+EEG+Representational+Alignment+Zitong+Lu+Yile+Wang+Julie+D.+Golomb",
    "gs_search_success": true,
    "gs_authors": [
      "bE5VCKsAAAAJ",
      "bgnVj8wAAAAJ",
      "h1VR9HAAAAAJ"
    ],
    "citation_count": 8
  },
  {
    "arxiv_id": "2305.00970",
    "title": "ArK: Augmented Reality with Knowledge Interactive Emergent Ability",
    "year": 2023,
    "published": "2023-05-01T17:57:01Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Despite the growing adoption of mixed reality and interactive AI agents, it remains challenging for these systems to generate high quality 2D/3D scenes in unseen environments. The common practice requires deploying an AI agent to collect large amounts of data for model training for every new task. This process is costly, or even impossible, for many domains. In this study, we develop an infinite agent that learns to transfer knowledge memory from general foundation models (e.g. GPT4, DALLE) to n",
    "arxiv_url": "https://arxiv.org/abs/2305.00970v1",
    "pdf_url": "https://arxiv.org/pdf/2305.00970v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2305.00970",
    "arxiv_authors": [
      "Qiuyuan Huang",
      "Jae Sung Park",
      "Abhinav Gupta",
      "Paul Bennett",
      "Ran Gong",
      "Subhojit Som",
      "Baolin Peng",
      "Owais Khan Mohammed",
      "Chris Pal",
      "Yejin Choi",
      "Jianfeng Gao"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=ArK%3A+Augmented+Reality+with+Knowledge+Interactive+Emergent+Ability+Qiuyuan+Huang+Jae+Sung+Park+Abhinav+Gupta+Paul+Bennett+Ran+Gong",
    "gs_search_success": true,
    "gs_authors": [
      "u1CNjgwAAAAJ",
      "AIncPrIAAAAJ",
      "n1gxPekAAAAJ",
      "hD2WqqcAAAAJ",
      "jAaCd7YAAAAJ",
      "1ScWJOoAAAAJ",
      "U7Mmyc8AAAAJ"
    ],
    "citation_count": 5
  },
  {
    "arxiv_id": "2403.17869",
    "title": "To Supervise or Not to Supervise: Understanding and Addressing the Key Challenges of Point Cloud Transfer Learning",
    "year": 2024,
    "published": "2024-03-26T16:57:33Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Transfer learning has long been a key factor in the advancement of many fields including 2D image analysis. Unfortunately, its applicability in 3D data processing has been relatively limited. While several approaches for point cloud transfer learning have been proposed in recent literature, with contrastive learning gaining particular prominence, most existing methods in this domain have only been studied and evaluated in limited scenarios. Most importantly, there is currently a lack of principl",
    "arxiv_url": "https://arxiv.org/abs/2403.17869v2",
    "pdf_url": "https://arxiv.org/pdf/2403.17869v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2403.17869",
    "arxiv_authors": [
      "Souhail Hadgi",
      "Lei Li",
      "Maks Ovsjanikov"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=To+Supervise+or+Not+to+Supervise%3A+Understanding+and+Addressing+the+Key+Challenges+of+Point+Cloud+Transfer+Learning+Souhail+Hadgi+Lei+Li+Maks+Ovsjanikov",
    "gs_search_success": true,
    "gs_authors": [
      "CxK2IjYAAAAJ",
      "0IsSPNEAAAAJ",
      "1B0l7U8AAAAJ"
    ],
    "citation_count": null
  },
  {
    "arxiv_id": "2308.05128",
    "title": "High-Level Parallelism and Nested Features for Dynamic Inference Cost and Top-Down Attention",
    "year": 2023,
    "published": "2023-08-09T08:49:29Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "This paper introduces a novel network topology that seamlessly integrates dynamic inference cost with a top-down attention mechanism, addressing two significant gaps in traditional deep learning models. Drawing inspiration from human perception, we combine sequential processing of generic low-level features with parallelism and nesting of high-level features. This design not only reflects a finding from recent neuroscience research regarding - spatially and contextually distinct neural activatio",
    "arxiv_url": "https://arxiv.org/abs/2308.05128v3",
    "pdf_url": "https://arxiv.org/pdf/2308.05128v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2308.05128",
    "arxiv_authors": [
      "André Peter Kelm",
      "Niels Hannemann",
      "Bruno Heberle",
      "Lucas Schmidt",
      "Tim Rolff",
      "Christian Wilms",
      "Ehsan Yaghoubi",
      "Simone Frintrop"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=High-Level+Parallelism+and+Nested+Features+for+Dynamic+Inference+Cost+and+Top-Down+Attention+Andr%C3%A9+Peter+Kelm+Niels+Hannemann+Bruno+Heberle+Lucas+Schmidt+Tim+Rolff",
    "gs_search_success": true,
    "gs_authors": [
      "NynHirYAAAAJ",
      "klbfxswAAAAJ",
      "AjkH8tYAAAAJ"
    ],
    "citation_count": null
  },
  {
    "arxiv_id": "2412.17523",
    "title": "Constructing Fair Latent Space for Intersection of Fairness and Explainability",
    "year": 2024,
    "published": "2024-12-23T12:47:04Z",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "abstract": "As the use of machine learning models has increased, numerous studies have aimed to enhance fairness. However, research on the intersection of fairness and explainability remains insufficient, leading to potential issues in gaining the trust of actual users. Here, we propose a novel module that constructs a fair latent space, enabling faithful explanation while ensuring fairness. The fair latent space is constructed by disentangling and redistributing labels and sensitive attributes, allowing th",
    "arxiv_url": "https://arxiv.org/abs/2412.17523v2",
    "pdf_url": "https://arxiv.org/pdf/2412.17523v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2412.17523",
    "arxiv_authors": [
      "Hyungjun Joo",
      "Hyeonggeun Han",
      "Sehwan Kim",
      "Sangwoo Hong",
      "Jungwoo Lee"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Constructing+Fair+Latent+Space+for+Intersection+of+Fairness+and+Explainability+Hyungjun+Joo+Hyeonggeun+Han+Sehwan+Kim+Sangwoo+Hong+Jungwoo+Lee",
    "gs_search_success": true,
    "gs_authors": [
      "B6LcRbYAAAAJ",
      "zYn8YZ8AAAAJ",
      "IZ8-3PgAAAAJ"
    ],
    "citation_count": 2
  },
  {
    "arxiv_id": "2309.15426",
    "title": "NeuRBF: A Neural Fields Representation with Adaptive Radial Basis Functions",
    "year": 2023,
    "published": "2023-09-27T06:32:05Z",
    "categories": [
      "cs.CV",
      "cs.GR",
      "cs.LG"
    ],
    "abstract": "We present a novel type of neural fields that uses general radial bases for signal representation. State-of-the-art neural fields typically rely on grid-based representations for storing local neural features and N-dimensional linear kernels for interpolating features at continuous query points. The spatial positions of their neural features are fixed on grid nodes and cannot well adapt to target signals. Our method instead builds upon general radial bases with flexible kernel position and shape",
    "arxiv_url": "https://arxiv.org/abs/2309.15426v1",
    "pdf_url": "https://arxiv.org/pdf/2309.15426v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2309.15426",
    "arxiv_authors": [
      "Zhang Chen",
      "Zhong Li",
      "Liangchen Song",
      "Lele Chen",
      "Jingyi Yu",
      "Junsong Yuan",
      "Yi Xu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=NeuRBF%3A+A+Neural+Fields+Representation+with+Adaptive+Radial+Basis+Functions+Zhang+Chen+Zhong+Li+Liangchen+Song+Lele+Chen+Jingyi+Yu",
    "gs_search_success": true,
    "gs_authors": [
      "fJ7seq0AAAAJ",
      "Kl4T9FYAAAAJ",
      "ldanjkUAAAAJ",
      "4MIbSrAAAAAJ",
      "C-wK73YAAAAJ",
      "H71yt54AAAAJ"
    ],
    "citation_count": 105
  },
  {
    "arxiv_id": "2402.02936",
    "title": "Panoramic Image Inpainting With Gated Convolution And Contextual Reconstruction Loss",
    "year": 2024,
    "published": "2024-02-05T11:58:08Z",
    "categories": [
      "eess.IV",
      "cs.CV",
      "cs.LG",
      "cs.MM"
    ],
    "abstract": "Deep learning-based methods have demonstrated encouraging results in tackling the task of panoramic image inpainting. However, it is challenging for existing methods to distinguish valid pixels from invalid pixels and find suitable references for corrupted areas, thus leading to artifacts in the inpainted results. In response to these challenges, we propose a panoramic image inpainting framework that consists of a Face Generator, a Cube Generator, a side branch, and two discriminators. We use th",
    "arxiv_url": "https://arxiv.org/abs/2402.02936v1",
    "pdf_url": "https://arxiv.org/pdf/2402.02936v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2402.02936",
    "arxiv_authors": [
      "Li Yu",
      "Yanjun Gao",
      "Farhad Pakdaman",
      "Moncef Gabbouj"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Panoramic+Image+Inpainting+With+Gated+Convolution+And+Contextual+Reconstruction+Loss+Li+Yu+Yanjun+Gao+Farhad+Pakdaman+Moncef+Gabbouj",
    "gs_search_success": true,
    "gs_authors": [
      "HSzWxk4AAAAJ",
      "cHukfSUAAAAJ",
      "RVluyTgAAAAJ"
    ],
    "citation_count": 6
  },
  {
    "arxiv_id": "2307.07928",
    "title": "Reinforced Disentanglement for Face Swapping without Skip Connection",
    "year": 2023,
    "published": "2023-07-16T02:44:19Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "The SOTA face swap models still suffer the problem of either target identity (i.e., shape) being leaked or the target non-identity attributes (i.e., background, hair) failing to be fully preserved in the final results. We show that this insufficient disentanglement is caused by two flawed designs that were commonly adopted in prior models: (1) counting on only one compressed encoder to represent both the semantic-level non-identity facial attributes(i.e., pose) and the pixel-level non-facial reg",
    "arxiv_url": "https://arxiv.org/abs/2307.07928v4",
    "pdf_url": "https://arxiv.org/pdf/2307.07928v4",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2307.07928",
    "arxiv_authors": [
      "Xiaohang Ren",
      "Xingyu Chen",
      "Pengfei Yao",
      "Heung-Yeung Shum",
      "Baoyuan Wang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Reinforced+Disentanglement+for+Face+Swapping+without+Skip+Connection+Xiaohang+Ren+Xingyu+Chen+Pengfei+Yao+Heung-Yeung+Shum+Baoyuan+Wang",
    "gs_search_success": true,
    "gs_authors": [
      "9akH-n8AAAAJ",
      "YurWtIEAAAAJ",
      "OWa5rOEAAAAJ"
    ],
    "citation_count": 18
  },
  {
    "arxiv_id": "2311.02480",
    "title": "A Strictly Bounded Deep Network for Unpaired Cyclic Translation of Medical Images",
    "year": 2023,
    "published": "2023-11-04T18:43:31Z",
    "categories": [
      "eess.IV",
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "Medical image translation is an ill-posed problem. Unlike existing paired unbounded unidirectional translation networks, in this paper, we consider unpaired medical images and provide a strictly bounded network that yields a stable bidirectional translation. We propose a patch-level concatenated cyclic conditional generative adversarial network (pCCGAN) embedded with adaptive dictionary learning. It consists of two cyclically connected CGANs of 47 layers each; where both generators (each of 32 l",
    "arxiv_url": "https://arxiv.org/abs/2311.02480v1",
    "pdf_url": "https://arxiv.org/pdf/2311.02480v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2311.02480",
    "arxiv_authors": [
      "Swati Rai",
      "Jignesh S. Bhatt",
      "Sarat Kumar Patra"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=A+Strictly+Bounded+Deep+Network+for+Unpaired+Cyclic+Translation+of+Medical+Images+Swati+Rai+Jignesh+S.+Bhatt+Sarat+Kumar+Patra",
    "gs_search_success": true,
    "gs_authors": [
      "Ro5gYAwAAAAJ",
      "clxd-nsAAAAJ",
      "6vFfeb4AAAAJ"
    ],
    "citation_count": 4
  },
  {
    "arxiv_id": "2401.16456",
    "title": "SHViT: Single-Head Vision Transformer with Memory Efficient Macro Design",
    "year": 2024,
    "published": "2024-01-29T09:12:23Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Recently, efficient Vision Transformers have shown great performance with low latency on resource-constrained devices. Conventionally, they use 4x4 patch embeddings and a 4-stage structure at the macro level, while utilizing sophisticated attention with multi-head configuration at the micro level. This paper aims to address computational redundancy at all design levels in a memory-efficient manner. We discover that using larger-stride patchify stem not only reduces memory access costs but also a",
    "arxiv_url": "https://arxiv.org/abs/2401.16456v2",
    "pdf_url": "https://arxiv.org/pdf/2401.16456v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2401.16456",
    "arxiv_authors": [
      "Seokju Yun",
      "Youngmin Ro"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=SHViT%3A+Single-Head+Vision+Transformer+with+Memory+Efficient+Macro+Design+Seokju+Yun+Youngmin+Ro",
    "gs_search_success": true,
    "gs_authors": [
      "-2MnHEIAAAAJ",
      "f5zk6PsAAAAJ"
    ],
    "citation_count": 112
  },
  {
    "arxiv_id": "2407.11383",
    "title": "TM-PATHVQA:90000+ Textless Multilingual Questions for Medical Visual Question Answering",
    "year": 2024,
    "published": "2024-07-16T04:54:45Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "In healthcare and medical diagnostics, Visual Question Answering (VQA) mayemergeasapivotal tool in scenarios where analysis of intricate medical images becomes critical for accurate diagnoses. Current text-based VQA systems limit their utility in scenarios where hands-free interaction and accessibility are crucial while performing tasks. A speech-based VQA system may provide a better means of interaction where information can be accessed while performing tasks simultaneously. To this end, this w",
    "arxiv_url": "https://arxiv.org/abs/2407.11383v1",
    "pdf_url": "https://arxiv.org/pdf/2407.11383v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2407.11383",
    "arxiv_authors": [
      "Tonmoy Rajkhowa",
      "Amartya Roy Chowdhury",
      "Sankalp Nagaonkar",
      "Achyut Mani Tripathi"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=TM-PATHVQA%3A90000%2B+Textless+Multilingual+Questions+for+Medical+Visual+Question+Answering+Tonmoy+Rajkhowa+Amartya+Roy+Chowdhury+Sankalp+Nagaonkar+Achyut+Mani+Tripathi",
    "gs_search_success": true,
    "gs_authors": [
      "daIIpSUAAAAJ",
      "sCr-DQgAAAAJ",
      "R2iStHMAAAAJ"
    ],
    "citation_count": 1
  },
  {
    "arxiv_id": "2312.05288",
    "title": "MotionCrafter: One-Shot Motion Customization of Diffusion Models",
    "year": 2023,
    "published": "2023-12-08T16:31:04Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "The essence of a video lies in its dynamic motions, including character actions, object movements, and camera movements. While text-to-video generative diffusion models have recently advanced in creating diverse contents, controlling specific motions through text prompts remains a significant challenge. A primary issue is the coupling of appearance and motion, often leading to overfitting on appearance. To tackle this challenge, we introduce MotionCrafter, a novel one-shot instance-guided motion",
    "arxiv_url": "https://arxiv.org/abs/2312.05288v2",
    "pdf_url": "https://arxiv.org/pdf/2312.05288v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2312.05288",
    "arxiv_authors": [
      "Yuxin Zhang",
      "Fan Tang",
      "Nisha Huang",
      "Haibin Huang",
      "Chongyang Ma",
      "Weiming Dong",
      "Changsheng Xu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=MotionCrafter%3A+One-Shot+Motion+Customization+of+Diffusion+Models+Yuxin+Zhang+Fan+Tang+Nisha+Huang+Haibin+Huang+Chongyang+Ma",
    "gs_search_success": true,
    "gs_authors": [
      "YDl1M80AAAAJ",
      "wTmPkSsAAAAJ",
      "l-ZQfpQAAAAJ",
      "hI9NRDkAAAAJ",
      "8VD0_DkAAAAJ",
      "WKGx4k8AAAAJ",
      "PdKElfwAAAAJ"
    ],
    "citation_count": 25
  },
  {
    "arxiv_id": "2306.16142",
    "title": "Neural directional distance field object representation for uni-directional path-traced rendering",
    "year": 2023,
    "published": "2023-06-28T12:16:38Z",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "abstract": "Faster rendering of synthetic images is a core problem in the field of computer graphics. Rendering algorithms, such as path-tracing is dependent on parameters like size of the image, number of light bounces, number of samples per pixel, all of which, are fixed if one wants to obtain a image of a desired quality. It is also dependent on the size and complexity of the scene being rendered. One of the largest bottleneck in rendering, particularly when the scene is very large, is querying for objec",
    "arxiv_url": "https://arxiv.org/abs/2306.16142v1",
    "pdf_url": "https://arxiv.org/pdf/2306.16142v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2306.16142",
    "arxiv_authors": [
      "Annada Prasad Behera",
      "Subhankar Mishra"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Neural+directional+distance+field+object+representation+for+uni-directional+path-traced+rendering+Annada+Prasad+Behera+Subhankar+Mishra",
    "gs_search_success": true,
    "gs_authors": [
      "FICz_ukAAAAJ"
    ],
    "citation_count": 3
  },
  {
    "arxiv_id": "2303.15735",
    "title": "Improving the Transferability of Adversarial Samples by Path-Augmented Method",
    "year": 2023,
    "published": "2023-03-28T05:14:04Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Deep neural networks have achieved unprecedented success on diverse vision tasks. However, they are vulnerable to adversarial noise that is imperceptible to humans. This phenomenon negatively affects their deployment in real-world scenarios, especially security-related ones. To evaluate the robustness of a target model in practice, transfer-based attacks craft adversarial samples with a local model and have attracted increasing attention from researchers due to their high efficiency. The state-o",
    "arxiv_url": "https://arxiv.org/abs/2303.15735v1",
    "pdf_url": "https://arxiv.org/pdf/2303.15735v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2303.15735",
    "arxiv_authors": [
      "Jianping Zhang",
      "Jen-tse Huang",
      "Wenxuan Wang",
      "Yichen Li",
      "Weibin Wu",
      "Xiaosen Wang",
      "Yuxin Su",
      "Michael R. Lyu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Improving+the+Transferability+of+Adversarial+Samples+by+Path-Augmented+Method+Jianping+Zhang+Jen-tse+Huang+Wenxuan+Wang+Yichen+Li+Weibin+Wu",
    "gs_search_success": true,
    "gs_authors": [
      "2o7L9GwAAAAJ",
      "uQnBgK0AAAAJ",
      "XBzDTAQAAAAJ",
      "4v5x0bUAAAAJ",
      "ckycnWkAAAAJ",
      "IrNImLsAAAAJ",
      "sVeDOcsAAAAJ",
      "6mtEjCEAAAAJ"
    ],
    "citation_count": 76
  },
  {
    "arxiv_id": "2401.05224",
    "title": "Do Vision and Language Encoders Represent the World Similarly?",
    "year": 2024,
    "published": "2024-01-10T15:51:39Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "abstract": "Aligned text-image encoders such as CLIP have become the de facto model for vision-language tasks. Furthermore, modality-specific encoders achieve impressive performances in their respective domains. This raises a central question: does an alignment exist between uni-modal vision and language encoders since they fundamentally represent the same physical world? Analyzing the latent spaces structure of vision and language models on image-caption benchmarks using the Centered Kernel Alignment (CKA)",
    "arxiv_url": "https://arxiv.org/abs/2401.05224v2",
    "pdf_url": "https://arxiv.org/pdf/2401.05224v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2401.05224",
    "arxiv_authors": [
      "Mayug Maniparambil",
      "Raiymbek Akshulakov",
      "Yasser Abdelaziz Dahou Djilali",
      "Sanath Narayan",
      "Mohamed El Amine Seddik",
      "Karttikeya Mangalam",
      "Noel E. O'Connor"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Do+Vision+and+Language+Encoders+Represent+the+World+Similarly%3F+Mayug+Maniparambil+Raiymbek+Akshulakov+Yasser+Abdelaziz+Dahou+Djilali+Sanath+Narayan+Mohamed+El+Amine+Seddik",
    "gs_search_success": true,
    "gs_authors": [
      "2l1fWEoAAAAJ",
      "Bx7EFGoAAAAJ",
      "tCHi_7IAAAAJ",
      "f02jV7oAAAAJ",
      "85Hxd24AAAAJ"
    ],
    "citation_count": 35
  },
  {
    "arxiv_id": "2501.01723",
    "title": "IGAF: Incremental Guided Attention Fusion for Depth Super-Resolution",
    "year": 2025,
    "published": "2025-01-03T09:27:51Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Accurate depth estimation is crucial for many fields, including robotics, navigation, and medical imaging. However, conventional depth sensors often produce low-resolution (LR) depth maps, making detailed scene perception challenging. To address this, enhancing LR depth maps to high-resolution (HR) ones has become essential, guided by HR-structured inputs like RGB or grayscale images. We propose a novel sensor fusion methodology for guided depth super-resolution (GDSR), a technique that combines",
    "arxiv_url": "https://arxiv.org/abs/2501.01723v1",
    "pdf_url": "https://arxiv.org/pdf/2501.01723v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2501.01723",
    "arxiv_authors": [
      "Athanasios Tragakis",
      "Chaitanya Kaul",
      "Kevin J. Mitchell",
      "Hang Dai",
      "Roderick Murray-Smith",
      "Daniele Faccio"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=IGAF%3A+Incremental+Guided+Attention+Fusion+for+Depth+Super-Resolution+Athanasios+Tragakis+Chaitanya+Kaul+Kevin+J.+Mitchell+Hang+Dai+Roderick+Murray-Smith",
    "gs_search_success": true,
    "gs_authors": [
      "_acS9zAAAAAJ",
      "6yvjpQQAAAAJ",
      "MsPIYAoAAAAJ",
      "eA8DbtoAAAAJ",
      "laX7LzQAAAAJ",
      "GAGMBAwAAAAJ"
    ],
    "citation_count": 1
  },
  {
    "arxiv_id": "2304.02255",
    "title": "Topology-Guided Multi-Class Cell Context Generation for Digital Pathology",
    "year": 2023,
    "published": "2023-04-05T07:01:34Z",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "abstract": "In digital pathology, the spatial context of cells is important for cell classification, cancer diagnosis and prognosis. To model such complex cell context, however, is challenging. Cells form different mixtures, lineages, clusters and holes. To model such structural patterns in a learnable fashion, we introduce several mathematical tools from spatial statistics and topological data analysis. We incorporate such structural descriptors into a deep generative model as both conditional inputs and a",
    "arxiv_url": "https://arxiv.org/abs/2304.02255v1",
    "pdf_url": "https://arxiv.org/pdf/2304.02255v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2304.02255",
    "arxiv_authors": [
      "Shahira Abousamra",
      "Rajarsi Gupta",
      "Tahsin Kurc",
      "Dimitris Samaras",
      "Joel Saltz",
      "Chao Chen"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Topology-Guided+Multi-Class+Cell+Context+Generation+for+Digital+Pathology+Shahira+Abousamra+Rajarsi+Gupta+Tahsin+Kurc+Dimitris+Samaras+Joel+Saltz",
    "gs_search_success": true,
    "gs_authors": [
      "LcEPA3cAAAAJ",
      "J-iIIFAAAAAJ",
      "_0dkufgAAAAJ",
      "QuEQAlMAAAAJ",
      "BxbKTYkAAAAJ"
    ],
    "citation_count": 20
  },
  {
    "arxiv_id": "2505.01113",
    "title": "NeuroLoc: Encoding Navigation Cells for 6-DOF Camera Localization",
    "year": 2025,
    "published": "2025-05-02T08:47:31Z",
    "categories": [
      "cs.RO",
      "cs.CV",
      "cs.NE"
    ],
    "abstract": "Recently, camera localization has been widely adopted in autonomous robotic navigation due to its efficiency and convenience. However, autonomous navigation in unknown environments often suffers from scene ambiguity, environmental disturbances, and dynamic object transformation in camera localization. To address this problem, inspired by the biological brain navigation mechanism (such as grid cells, place cells, and head direction cells), we propose a novel neurobiological camera location method",
    "arxiv_url": "https://arxiv.org/abs/2505.01113v1",
    "pdf_url": "https://arxiv.org/pdf/2505.01113v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2505.01113",
    "arxiv_authors": [
      "Xun Li",
      "Jian Yang",
      "Fenli Jia",
      "Muyu Wang",
      "Qi Wu",
      "Jun Wu",
      "Jinpeng Mi",
      "Jilin Hu",
      "Peidong Liang",
      "Xuan Tang",
      "Ke Li",
      "Xiong You",
      "Xian Wei"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=NeuroLoc%3A+Encoding+Navigation+Cells+for+6-DOF+Camera+Localization+Xun+Li+Jian+Yang+Fenli+Jia+Muyu+Wang+Qi+Wu",
    "gs_search_success": true,
    "gs_authors": [
      "o3m_s5YAAAAJ",
      "mFj-I10AAAAJ"
    ],
    "citation_count": null
  },
  {
    "arxiv_id": "2304.11450",
    "title": "Dilated-UNet: A Fast and Accurate Medical Image Segmentation Approach using a Dilated Transformer and U-Net Architecture",
    "year": 2023,
    "published": "2023-04-22T17:20:13Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Medical image segmentation is crucial for the development of computer-aided diagnostic and therapeutic systems, but still faces numerous difficulties. In recent years, the commonly used encoder-decoder architecture based on CNNs has been applied effectively in medical image segmentation, but has limitations in terms of learning global context and spatial relationships. Some researchers have attempted to incorporate transformers into both the decoder and encoder components, with promising results",
    "arxiv_url": "https://arxiv.org/abs/2304.11450v1",
    "pdf_url": "https://arxiv.org/pdf/2304.11450v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2304.11450",
    "arxiv_authors": [
      "Davoud Saadati",
      "Omid Nejati Manzari",
      "Sattar Mirzakuchaki"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Dilated-UNet%3A+A+Fast+and+Accurate+Medical+Image+Segmentation+Approach+using+a+Dilated+Transformer+and+U-Net+Architecture+Davoud+Saadati+Omid+Nejati+Manzari+Sattar+Mirzakuchaki",
    "gs_search_success": true,
    "gs_authors": [
      "UvZmQzIAAAAJ"
    ],
    "citation_count": 27
  },
  {
    "arxiv_id": "2308.09242",
    "title": "ASAG: Building Strong One-Decoder-Layer Sparse Detectors via Adaptive Sparse Anchor Generation",
    "year": 2023,
    "published": "2023-08-18T02:06:49Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Recent sparse detectors with multiple, e.g. six, decoder layers achieve promising performance but much inference time due to complex heads. Previous works have explored using dense priors as initialization and built one-decoder-layer detectors. Although they gain remarkable acceleration, their performance still lags behind their six-decoder-layer counterparts by a large margin. In this work, we aim to bridge this performance gap while retaining fast speed. We find that the architecture discrepan",
    "arxiv_url": "https://arxiv.org/abs/2308.09242v1",
    "pdf_url": "https://arxiv.org/pdf/2308.09242v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2308.09242",
    "arxiv_authors": [
      "Shenghao Fu",
      "Junkai Yan",
      "Yipeng Gao",
      "Xiaohua Xie",
      "Wei-Shi Zheng"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=ASAG%3A+Building+Strong+One-Decoder-Layer+Sparse+Detectors+via+Adaptive+Sparse+Anchor+Generation+Shenghao+Fu+Junkai+Yan+Yipeng+Gao+Xiaohua+Xie+Wei-Shi+Zheng",
    "gs_search_success": true,
    "gs_authors": [
      "m8I1ELMAAAAJ",
      "5YZ3kvoAAAAJ",
      "QMm29SwAAAAJ",
      "AwqDDGoAAAAJ",
      "G16eJVMAAAAJ"
    ],
    "citation_count": 9
  },
  {
    "arxiv_id": "2502.09980",
    "title": "V2V-LLM: Vehicle-to-Vehicle Cooperative Autonomous Driving with Multi-Modal Large Language Models",
    "year": 2025,
    "published": "2025-02-14T08:05:41Z",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "abstract": "Current autonomous driving vehicles rely mainly on their individual sensors to understand surrounding scenes and plan for future trajectories, which can be unreliable when the sensors are malfunctioning or occluded. To address this problem, cooperative perception methods via vehicle-to-vehicle (V2V) communication have been proposed, but they have tended to focus on perception tasks like detection or tracking. How those approaches contribute to overall cooperative planning performance is still un",
    "arxiv_url": "https://arxiv.org/abs/2502.09980v3",
    "pdf_url": "https://arxiv.org/pdf/2502.09980v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2502.09980",
    "arxiv_authors": [
      "Hsu-kuang Chiu",
      "Ryo Hachiuma",
      "Chien-Yi Wang",
      "Stephen F. Smith",
      "Yu-Chiang Frank Wang",
      "Min-Hung Chen"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=V2V-LLM%3A+Vehicle-to-Vehicle+Cooperative+Autonomous+Driving+with+Multi-Modal+Large+Language+Models+Hsu-kuang+Chiu+Ryo+Hachiuma+Chien-Yi+Wang+Stephen+F.+Smith+Yu-Chiang+Frank+Wang",
    "gs_search_success": true,
    "gs_authors": [
      "fCFwERYAAAAJ",
      "ovzuxi8AAAAJ",
      "F_c5sHkAAAAJ",
      "05LW2DcAAAAJ",
      "W1KPqGEAAAAJ"
    ],
    "citation_count": 21
  },
  {
    "arxiv_id": "2502.04640",
    "title": "Building Rome with Convex Optimization",
    "year": 2025,
    "published": "2025-02-07T03:53:46Z",
    "categories": [
      "cs.RO",
      "cs.CV",
      "math.OC"
    ],
    "abstract": "Global bundle adjustment is made easy by depth prediction and convex optimization. We (i) propose a scaled bundle adjustment (SBA) formulation that lifts 2D keypoint measurements to 3D with learned depth, (ii) design an empirically tight convex semidfinite program (SDP) relaxation that solves SBA to certfiable global optimality, (iii) solve the SDP relaxations at extreme scale with Burer-Monteiro factorization and a CUDA-based trust-region Riemannian optimizer (dubbed XM), (iv) build a structure",
    "arxiv_url": "https://arxiv.org/abs/2502.04640v4",
    "pdf_url": "https://arxiv.org/pdf/2502.04640v4",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2502.04640",
    "arxiv_authors": [
      "Haoyu Han",
      "Heng Yang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Building+Rome+with+Convex+Optimization+Haoyu+Han+Heng+Yang",
    "gs_search_success": true,
    "gs_authors": [
      "GuKEDfixZqsC"
    ],
    "citation_count": 6
  },
  {
    "arxiv_id": "2406.19635",
    "title": "Model Predictive Simulation Using Structured Graphical Models and Transformers",
    "year": 2024,
    "published": "2024-06-28T03:46:53Z",
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "abstract": "We propose an approach to simulating trajectories of multiple interacting agents (road users) based on transformers and probabilistic graphical models (PGMs), and apply it to the Waymo SimAgents challenge. The transformer baseline is based on the MTR model, which predicts multiple future trajectories conditioned on the past trajectories and static road layout features. We then improve upon these generated trajectories using a PGM, which contains factors which encode prior knowledge, such as a pr",
    "arxiv_url": "https://arxiv.org/abs/2406.19635v1",
    "pdf_url": "https://arxiv.org/pdf/2406.19635v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2406.19635",
    "arxiv_authors": [
      "Xinghua Lou",
      "Meet Dave",
      "Shrinu Kushagra",
      "Miguel Lazaro-Gredilla",
      "Kevin Murphy"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Model+Predictive+Simulation+Using+Structured+Graphical+Models+and+Transformers+Xinghua+Lou+Meet+Dave+Shrinu+Kushagra+Miguel+Lazaro-Gredilla+Kevin+Murphy",
    "gs_search_success": true,
    "gs_authors": [
      "tqPmopoAAAAJ",
      "8RYloKYAAAAJ",
      "SFjDQk8AAAAJ",
      "MxxZkEcAAAAJ",
      "5o4N31oAAAAJ"
    ],
    "citation_count": 2
  },
  {
    "arxiv_id": "2406.19736",
    "title": "MM-Instruct: Generated Visual Instructions for Large Multimodal Model Alignment",
    "year": 2024,
    "published": "2024-06-28T08:25:27Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "abstract": "This paper introduces MM-Instruct, a large-scale dataset of diverse and high-quality visual instruction data designed to enhance the instruction-following capabilities of large multimodal models (LMMs). While existing visual instruction datasets often focus on question-answering, they struggle to generalize to broader application scenarios such as creative writing, summarization, or image analysis. To address these limitations, we propose a novel approach to constructing MM-Instruct that leverag",
    "arxiv_url": "https://arxiv.org/abs/2406.19736v1",
    "pdf_url": "https://arxiv.org/pdf/2406.19736v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2406.19736",
    "arxiv_authors": [
      "Jihao Liu",
      "Xin Huang",
      "Jinliang Zheng",
      "Boxiao Liu",
      "Jia Wang",
      "Osamu Yoshie",
      "Yu Liu",
      "Hongsheng Li"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=MM-Instruct%3A+Generated+Visual+Instructions+for+Large+Multimodal+Model+Alignment+Jihao+Liu+Xin+Huang+Jinliang+Zheng+Boxiao+Liu+Jia+Wang",
    "gs_search_success": true,
    "gs_authors": [
      "gU5abfcAAAAJ",
      "YLA5LwEAAAAJ",
      "PP1HyToAAAAJ",
      "3j5AHFsAAAAJ",
      "VGJ1rRAAAAAJ"
    ],
    "citation_count": 5
  },
  {
    "arxiv_id": "2412.13569",
    "title": "Multi-View Pedestrian Occupancy Prediction with a Novel Synthetic Dataset",
    "year": 2024,
    "published": "2024-12-18T07:35:42Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "We address an advanced challenge of predicting pedestrian occupancy as an extension of multi-view pedestrian detection in urban traffic. To support this, we have created a new synthetic dataset called MVP-Occ, designed for dense pedestrian scenarios in large-scale scenes. Our dataset provides detailed representations of pedestrians using voxel structures, accompanied by rich semantic scene understanding labels, facilitating visual navigation and insights into pedestrian spatial information. Furt",
    "arxiv_url": "https://arxiv.org/abs/2412.13569v1",
    "pdf_url": "https://arxiv.org/pdf/2412.13569v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2412.13569",
    "arxiv_authors": [
      "Sithu Aung",
      "Min-Cheol Sagong",
      "Junghyun Cho"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Multi-View+Pedestrian+Occupancy+Prediction+with+a+Novel+Synthetic+Dataset+Sithu+Aung+Min-Cheol+Sagong+Junghyun+Cho",
    "gs_search_success": true,
    "gs_authors": [
      "pEx-E2wAAAAJ",
      "ifybhH8AAAAJ",
      "Yj1hv8YAAAAJ"
    ],
    "citation_count": 2
  },
  {
    "arxiv_id": "2310.05699",
    "title": "Uni3DETR: Unified 3D Detection Transformer",
    "year": 2023,
    "published": "2023-10-09T13:20:20Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Existing point cloud based 3D detectors are designed for the particular scene, either indoor or outdoor ones. Because of the substantial differences in object distribution and point density within point clouds collected from various environments, coupled with the intricate nature of 3D metrics, there is still a lack of a unified network architecture that can accommodate diverse scenes. In this paper, we propose Uni3DETR, a unified 3D detector that addresses indoor and outdoor 3D detection within",
    "arxiv_url": "https://arxiv.org/abs/2310.05699v1",
    "pdf_url": "https://arxiv.org/pdf/2310.05699v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2310.05699",
    "arxiv_authors": [
      "Zhenyu Wang",
      "Yali Li",
      "Xi Chen",
      "Hengshuang Zhao",
      "Shengjin Wang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Uni3DETR%3A+Unified+3D+Detection+Transformer+Zhenyu+Wang+Yali+Li+Xi+Chen+Hengshuang+Zhao+Shengjin+Wang",
    "gs_search_success": true,
    "gs_authors": [
      "x_-kOjoAAAAJ",
      "INISnXkAAAAJ",
      "RgzLZZsAAAAJ",
      "4uE10I0AAAAJ"
    ],
    "citation_count": 41
  },
  {
    "arxiv_id": "2304.07522",
    "title": "ID2image: Leakage of non-ID information into face descriptors and inversion from descriptors to images",
    "year": 2023,
    "published": "2023-04-15T10:11:22Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Embedding a face image to a descriptor vector using a deep CNN is a widely used technique in face recognition. Via several possible training strategies, such embeddings are supposed to capture only identity information. Information about the environment (such as background and lighting) or changeable aspects of the face (such as pose, expression, presence of glasses, hat etc.) should be discarded since they are not useful for recognition. In this paper, we present a surprising result that this i",
    "arxiv_url": "https://arxiv.org/abs/2304.07522v1",
    "pdf_url": "https://arxiv.org/pdf/2304.07522v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2304.07522",
    "arxiv_authors": [
      "Mingrui Li",
      "William A. P. Smith",
      "Patrik Huber"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=ID2image%3A+Leakage+of+non-ID+information+into+face+descriptors+and+inversion+from+descriptors+to+images+Mingrui+Li+William+A.+P.+Smith+Patrik+Huber",
    "gs_search_success": true,
    "gs_authors": [
      "o2z0h6MAAAAJ",
      "nb_I6yUAAAAJ"
    ],
    "citation_count": 4
  },
  {
    "arxiv_id": "2302.10915",
    "title": "Conformers are All You Need for Visual Speech Recognition",
    "year": 2023,
    "published": "2023-02-17T01:31:55Z",
    "categories": [
      "cs.LG",
      "cs.CL",
      "cs.CV",
      "cs.SD",
      "eess.AS"
    ],
    "abstract": "Visual speech recognition models extract visual features in a hierarchical manner. At the lower level, there is a visual front-end with a limited temporal receptive field that processes the raw pixels depicting the lips or faces. At the higher level, there is an encoder that attends to the embeddings produced by the front-end over a large temporal receptive field. Previous work has focused on improving the visual front-end of the model to extract more useful features for speech recognition. Surp",
    "arxiv_url": "https://arxiv.org/abs/2302.10915v2",
    "pdf_url": "https://arxiv.org/pdf/2302.10915v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2302.10915",
    "arxiv_authors": [
      "Oscar Chang",
      "Hank Liao",
      "Dmitriy Serdyuk",
      "Ankit Shah",
      "Olivier Siohan"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Conformers+are+All+You+Need+for+Visual+Speech+Recognition+Oscar+Chang+Hank+Liao+Dmitriy+Serdyuk+Ankit+Shah+Olivier+Siohan",
    "gs_search_success": false,
    "gs_authors": [],
    "citation_count": null,
    "error_type": "fetch_failed"
  },
  {
    "arxiv_id": "2403.10722",
    "title": "Cannabis Seed Variant Detection using Faster R-CNN",
    "year": 2024,
    "published": "2024-03-15T22:49:47Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Analyzing and detecting cannabis seed variants is crucial for the agriculture industry. It enables precision breeding, allowing cultivators to selectively enhance desirable traits. Accurate identification of seed variants also ensures regulatory compliance, facilitating the cultivation of specific cannabis strains with defined characteristics, ultimately improving agricultural productivity and meeting diverse market demands. This paper presents a study on cannabis seed variant detection by emplo",
    "arxiv_url": "https://arxiv.org/abs/2403.10722v1",
    "pdf_url": "https://arxiv.org/pdf/2403.10722v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2403.10722",
    "arxiv_authors": [
      "Toqi Tahamid Sarker",
      "Taminul Islam",
      "Khaled R Ahmed"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Cannabis+Seed+Variant+Detection+using+Faster+R-CNN+Toqi+Tahamid+Sarker+Taminul+Islam+Khaled+R+Ahmed",
    "gs_search_success": true,
    "gs_authors": [
      "i1SmuwYAAAAJ",
      "FYKqgh4AAAAJ",
      "Kgo_S9sAAAAJ"
    ],
    "citation_count": 7
  },
  {
    "arxiv_id": "2308.09380",
    "title": "Deciphering knee osteoarthritis diagnostic features with explainable artificial intelligence: A systematic review",
    "year": 2023,
    "published": "2023-08-18T08:23:47Z",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "abstract": "Existing artificial intelligence (AI) models for diagnosing knee osteoarthritis (OA) have faced criticism for their lack of transparency and interpretability, despite achieving medical-expert-like performance. This opacity makes them challenging to trust in clinical practice. Recently, explainable artificial intelligence (XAI) has emerged as a specialized technique that can provide confidence in the model's prediction by revealing how the prediction is derived, thus promoting the use of AI syste",
    "arxiv_url": "https://arxiv.org/abs/2308.09380v1",
    "pdf_url": "https://arxiv.org/pdf/2308.09380v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2308.09380",
    "arxiv_authors": [
      "Yun Xin Teoh",
      "Alice Othmani",
      "Siew Li Goh",
      "Juliana Usman",
      "Khin Wee Lai"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Deciphering+knee+osteoarthritis+diagnostic+features+with+explainable+artificial+intelligence%3A+A+systematic+review+Yun+Xin+Teoh+Alice+Othmani+Siew+Li+Goh+Juliana+Usman+Khin+Wee+Lai",
    "gs_search_success": true,
    "gs_authors": [
      "axupJGAAAAAJ",
      "vzmTHjsAAAAJ",
      "UX_D-VgAAAAJ",
      "MuTNEIUAAAAJ"
    ],
    "citation_count": 15
  },
  {
    "arxiv_id": "2407.04245",
    "title": "Every Pixel Has its Moments: Ultra-High-Resolution Unpaired Image-to-Image Translation via Dense Normalization",
    "year": 2024,
    "published": "2024-07-05T04:14:50Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Recent advancements in ultra-high-resolution unpaired image-to-image translation have aimed to mitigate the constraints imposed by limited GPU memory through patch-wise inference. Nonetheless, existing methods often compromise between the reduction of noticeable tiling artifacts and the preservation of color and hue contrast, attributed to the reliance on global image- or patch-level statistics in the instance normalization layers. In this study, we introduce a Dense Normalization (DN) layer des",
    "arxiv_url": "https://arxiv.org/abs/2407.04245v1",
    "pdf_url": "https://arxiv.org/pdf/2407.04245v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2407.04245",
    "arxiv_authors": [
      "Ming-Yang Ho",
      "Che-Ming Wu",
      "Min-Sheng Wu",
      "Yufeng Jane Tseng"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Every+Pixel+Has+its+Moments%3A+Ultra-High-Resolution+Unpaired+Image-to-Image+Translation+via+Dense+Normalization+Ming-Yang+Ho+Che-Ming+Wu+Min-Sheng+Wu+Yufeng+Jane+Tseng",
    "gs_search_success": true,
    "gs_authors": [
      "NRmTj0sAAAAJ",
      "QOHrBUYAAAAJ",
      "CmvyshAAAAAJ",
      "PFh1h-kAAAAJ"
    ],
    "citation_count": 9
  },
  {
    "arxiv_id": "2310.16590",
    "title": "$\\mathbb{VD}$-$\\mathbb{GR}$: Boosting $\\mathbb{V}$isual $\\mathbb{D}$ialog with Cascaded Spatial-Temporal Multi-Modal $\\mathbb{GR}$aphs",
    "year": 2023,
    "published": "2023-10-25T12:25:53Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "We propose $\\mathbb{VD}$-$\\mathbb{GR}$ - a novel visual dialog model that combines pre-trained language models (LMs) with graph neural networks (GNNs). Prior works mainly focused on one class of models at the expense of the other, thus missing out on the opportunity of combining their respective benefits. At the core of $\\mathbb{VD}$-$\\mathbb{GR}$ is a novel integration mechanism that alternates between spatial-temporal multi-modal GNNs and BERT layers, and that covers three distinct contributio",
    "arxiv_url": "https://arxiv.org/abs/2310.16590v1",
    "pdf_url": "https://arxiv.org/pdf/2310.16590v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2310.16590",
    "arxiv_authors": [
      "Adnen Abdessaied",
      "Lei Shi",
      "Andreas Bulling"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=%24%5Cmathbb%7BVD%7D%24-%24%5Cmathbb%7BGR%7D%24%3A+Boosting+%24%5Cmathbb%7BV%7D%24isual+%24%5Cmathbb%7BD%7D%24ialog+with+Cascaded+Spatial-Temporal+Multi-Modal+%24%5Cmathbb%7BGR%7D%24aphs+Adnen+Abdessaied+Lei+Shi+Andreas+Bulling",
    "gs_search_success": true,
    "gs_authors": [
      "nsuJs6QAAAAJ",
      "GRF1XoMAAAAJ",
      "QURZIzUAAAAJ"
    ],
    "citation_count": null
  },
  {
    "arxiv_id": "2503.00897",
    "title": "A Simple and Effective Reinforcement Learning Method for Text-to-Image Diffusion Fine-tuning",
    "year": 2025,
    "published": "2025-03-02T13:43:53Z",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "abstract": "Reinforcement learning (RL)-based fine-tuning has emerged as a powerful approach for aligning diffusion models with black-box objectives. Proximal policy optimization (PPO) is the most popular choice of method for policy optimization. While effective in terms of performance, PPO is highly sensitive to hyper-parameters and involves substantial computational overhead. REINFORCE, on the other hand, mitigates some computational complexities such as high memory overhead and sensitive hyper-parameter ",
    "arxiv_url": "https://arxiv.org/abs/2503.00897v6",
    "pdf_url": "https://arxiv.org/pdf/2503.00897v6",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2503.00897",
    "arxiv_authors": [
      "Shashank Gupta",
      "Chaitanya Ahuja",
      "Tsung-Yu Lin",
      "Sreya Dutta Roy",
      "Harrie Oosterhuis",
      "Maarten de Rijke",
      "Satya Narayan Shukla"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=A+Simple+and+Effective+Reinforcement+Learning+Method+for+Text-to-Image+Diffusion+Fine-tuning+Shashank+Gupta+Chaitanya+Ahuja+Tsung-Yu+Lin+Sreya+Dutta+Roy+Harrie+Oosterhuis",
    "gs_search_success": true,
    "gs_authors": [
      "AVDkgFIAAAAJ",
      "ahFtDysAAAAJ",
      "CX8zqPoAAAAJ",
      "e9JynrAAAAAJ",
      "UvTcU-IAAAAJ",
      "l1tsmesAAAAJ"
    ],
    "citation_count": 12
  },
  {
    "arxiv_id": "2309.00696",
    "title": "AAN: Attributes-Aware Network for Temporal Action Detection",
    "year": 2023,
    "published": "2023-09-01T18:35:31Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "The challenge of long-term video understanding remains constrained by the efficient extraction of object semantics and the modelling of their relationships for downstream tasks. Although the CLIP visual features exhibit discriminative properties for various vision tasks, particularly in object encoding, they are suboptimal for long-term video understanding. To address this issue, we present the Attributes-Aware Network (AAN), which consists of two key components: the Attributes Extractor and a G",
    "arxiv_url": "https://arxiv.org/abs/2309.00696v1",
    "pdf_url": "https://arxiv.org/pdf/2309.00696v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2309.00696",
    "arxiv_authors": [
      "Rui Dai",
      "Srijan Das",
      "Michael S. Ryoo",
      "Francois Bremond"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=AAN%3A+Attributes-Aware+Network+for+Temporal+Action+Detection+Rui+Dai+Srijan+Das+Michael+S.+Ryoo+Francois+Bremond",
    "gs_search_success": true,
    "gs_authors": [
      "vcw0TJIAAAAJ",
      "h-oGBzsAAAAJ",
      "ZDTF5AEAAAAJ",
      "V1s0tTwAAAAJ"
    ],
    "citation_count": 4
  },
  {
    "arxiv_id": "2407.16309",
    "title": "A new visual quality metric for Evaluating the performance of multidimensional projections",
    "year": 2024,
    "published": "2024-07-23T09:02:46Z",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "abstract": "Multidimensional projections (MP) are among the most essential approaches in the visual analysis of multidimensional data. It transforms multidimensional data into two-dimensional representations that may be shown as scatter plots while preserving their similarity with the original data. Human visual perception is frequently used to evaluate the quality of MP. In this work, we propose to study and improve on a well-known map called Local Affine Multidimensional Projection (LAMP), which takes a m",
    "arxiv_url": "https://arxiv.org/abs/2407.16309v1",
    "pdf_url": "https://arxiv.org/pdf/2407.16309v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2407.16309",
    "arxiv_authors": [
      "Maniru Ibrahim",
      "Thales Vieira"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=A+new+visual+quality+metric+for+Evaluating+the+performance+of+multidimensional+projections+Maniru+Ibrahim+Thales+Vieira",
    "gs_search_success": true,
    "gs_authors": [
      "meFmL2sAAAAJ",
      "Qz6kX58AAAAJ"
    ],
    "citation_count": null
  },
  {
    "arxiv_id": "2307.07663",
    "title": "INVE: Interactive Neural Video Editing",
    "year": 2023,
    "published": "2023-07-15T00:02:41Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "We present Interactive Neural Video Editing (INVE), a real-time video editing solution, which can assist the video editing process by consistently propagating sparse frame edits to the entire video clip. Our method is inspired by the recent work on Layered Neural Atlas (LNA). LNA, however, suffers from two major drawbacks: (1) the method is too slow for interactive editing, and (2) it offers insufficient support for some editing use cases, including direct frame editing and rigid texture trackin",
    "arxiv_url": "https://arxiv.org/abs/2307.07663v1",
    "pdf_url": "https://arxiv.org/pdf/2307.07663v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2307.07663",
    "arxiv_authors": [
      "Jiahui Huang",
      "Leonid Sigal",
      "Kwang Moo Yi",
      "Oliver Wang",
      "Joon-Young Lee"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=INVE%3A+Interactive+Neural+Video+Editing+Jiahui+Huang+Leonid+Sigal+Kwang+Moo+Yi+Oliver+Wang+Joon-Young+Lee",
    "gs_search_success": true,
    "gs_authors": [
      "qWU4y1wAAAAJ",
      "pr6rIJEAAAAJ",
      "6ajse1YAAAAJ",
      "0B8uuBkAAAAJ",
      "P2mG6rcAAAAJ"
    ],
    "citation_count": 19
  },
  {
    "arxiv_id": "2309.09256",
    "title": "LiDAR Data Synthesis with Denoising Diffusion Probabilistic Models",
    "year": 2023,
    "published": "2023-09-17T12:26:57Z",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "abstract": "Generative modeling of 3D LiDAR data is an emerging task with promising applications for autonomous mobile robots, such as scalable simulation, scene manipulation, and sparse-to-dense completion of LiDAR point clouds. While existing approaches have demonstrated the feasibility of image-based LiDAR data generation using deep generative models, they still struggle with fidelity and training stability. In this work, we present R2DM, a novel generative model for LiDAR data that can generate diverse ",
    "arxiv_url": "https://arxiv.org/abs/2309.09256v2",
    "pdf_url": "https://arxiv.org/pdf/2309.09256v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2309.09256",
    "arxiv_authors": [
      "Kazuto Nakashima",
      "Ryo Kurazume"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=LiDAR+Data+Synthesis+with+Denoising+Diffusion+Probabilistic+Models+Kazuto+Nakashima+Ryo+Kurazume",
    "gs_search_success": true,
    "gs_authors": [
      "VnDvavYAAAAJ",
      "PGhVyVcAAAAJ"
    ],
    "citation_count": 53
  },
  {
    "arxiv_id": "2503.21721",
    "title": "Evaluating Text-to-Image and Text-to-Video Synthesis with a Conditional Fréchet Distance",
    "year": 2025,
    "published": "2025-03-27T17:35:14Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Evaluating text-to-image and text-to-video models is challenging due to a fundamental disconnect: established metrics fail to jointly measure visual quality and semantic alignment with text, leading to a poor correlation with human judgments. To address this critical issue, we propose cFreD, a general metric based on a Conditional Fréchet Distance that unifies the assessment of visual fidelity and text-prompt consistency into a single score. Existing metrics such as Fréchet Inception Distance (F",
    "arxiv_url": "https://arxiv.org/abs/2503.21721v2",
    "pdf_url": "https://arxiv.org/pdf/2503.21721v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2503.21721",
    "arxiv_authors": [
      "Jaywon Koo",
      "Jefferson Hernandez",
      "Moayed Haji-Ali",
      "Ziyan Yang",
      "Vicente Ordonez"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Evaluating+Text-to-Image+and+Text-to-Video+Synthesis+with+a+Conditional+Fr%C3%A9chet+Distance+Jaywon+Koo+Jefferson+Hernandez+Moayed+Haji-Ali+Ziyan+Yang+Vicente+Ordonez",
    "gs_search_success": true,
    "gs_authors": [
      "TtA_j4YAAAAJ",
      "amZrKVQAAAAJ",
      "nJya9woAAAAJ",
      "Ck-mSEwAAAAJ",
      "Xk6-2C0AAAAJ"
    ],
    "citation_count": null
  },
  {
    "arxiv_id": "2405.10305",
    "title": "4D Panoptic Scene Graph Generation",
    "year": 2024,
    "published": "2024-05-16T17:56:55Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "We are living in a three-dimensional space while moving forward through a fourth dimension: time. To allow artificial intelligence to develop a comprehensive understanding of such a 4D environment, we introduce 4D Panoptic Scene Graph (PSG-4D), a new representation that bridges the raw visual data perceived in a dynamic 4D world and high-level visual understanding. Specifically, PSG-4D abstracts rich 4D sensory data into nodes, which represent entities with precise location and status informatio",
    "arxiv_url": "https://arxiv.org/abs/2405.10305v1",
    "pdf_url": "https://arxiv.org/pdf/2405.10305v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2405.10305",
    "arxiv_authors": [
      "Jingkang Yang",
      "Jun Cen",
      "Wenxuan Peng",
      "Shuai Liu",
      "Fangzhou Hong",
      "Xiangtai Li",
      "Kaiyang Zhou",
      "Qifeng Chen",
      "Ziwei Liu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=4D+Panoptic+Scene+Graph+Generation+Jingkang+Yang+Jun+Cen+Wenxuan+Peng+Shuai+Liu+Fangzhou+Hong",
    "gs_search_success": true,
    "gs_authors": [
      "7SKAhBwAAAAJ",
      "mhaiL5MAAAAJ",
      "lLMX9hcAAAAJ",
      "W9190BQAAAAJ",
      "lc45xlcAAAAJ",
      "I_B6IHIAAAAJ",
      "S-YjbUYAAAAJ",
      "gRIejugAAAAJ",
      "FL3ReD0AAAAJ"
    ],
    "citation_count": 29
  },
  {
    "arxiv_id": "2505.04758",
    "title": "Lightweight RGB-D Salient Object Detection from a Speed-Accuracy Tradeoff Perspective",
    "year": 2025,
    "published": "2025-05-07T19:37:20Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Current RGB-D methods usually leverage large-scale backbones to improve accuracy but sacrifice efficiency. Meanwhile, several existing lightweight methods are difficult to achieve high-precision performance. To balance the efficiency and performance, we propose a Speed-Accuracy Tradeoff Network (SATNet) for Lightweight RGB-D SOD from three fundamental perspectives: depth quality, modality fusion, and feature representation. Concerning depth quality, we introduce the Depth Anything Model to gener",
    "arxiv_url": "https://arxiv.org/abs/2505.04758v1",
    "pdf_url": "https://arxiv.org/pdf/2505.04758v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2505.04758",
    "arxiv_authors": [
      "Songsong Duan",
      "Xi Yang",
      "Nannan Wang",
      "Xinbo Gao"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Lightweight+RGB-D+Salient+Object+Detection+from+a+Speed-Accuracy+Tradeoff+Perspective+Songsong+Duan+Xi+Yang+Nannan+Wang+Xinbo+Gao",
    "gs_search_success": true,
    "gs_authors": [
      "VZVTOOIAAAAJ",
      "W5c-LSYAAAAJ",
      "SRBn7oUAAAAJ"
    ],
    "citation_count": 7
  },
  {
    "arxiv_id": "2407.21640",
    "title": "MSA$^2$Net: Multi-scale Adaptive Attention-guided Network for Medical Image Segmentation",
    "year": 2024,
    "published": "2024-07-31T14:41:10Z",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "abstract": "Medical image segmentation involves identifying and separating object instances in a medical image to delineate various tissues and structures, a task complicated by the significant variations in size, shape, and density of these features. Convolutional neural networks (CNNs) have traditionally been used for this task but have limitations in capturing long-range dependencies. Transformers, equipped with self-attention mechanisms, aim to address this problem. However, in medical image segmentatio",
    "arxiv_url": "https://arxiv.org/abs/2407.21640v3",
    "pdf_url": "https://arxiv.org/pdf/2407.21640v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2407.21640",
    "arxiv_authors": [
      "Sina Ghorbani Kolahi",
      "Seyed Kamal Chaharsooghi",
      "Toktam Khatibi",
      "Afshin Bozorgpour",
      "Reza Azad",
      "Moein Heidari",
      "Ilker Hacihaliloglu",
      "Dorit Merhof"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=MSA%24%5E2%24Net%3A+Multi-scale+Adaptive+Attention-guided+Network+for+Medical+Image+Segmentation+Sina+Ghorbani+Kolahi+Seyed+Kamal+Chaharsooghi+Toktam+Khatibi+Afshin+Bozorgpour+Reza+Azad",
    "gs_search_success": true,
    "gs_authors": [
      "Lxl6D2gAAAAJ",
      "IitYW5AAAAAJ"
    ],
    "citation_count": 25
  },
  {
    "arxiv_id": "2410.03143",
    "title": "ECHOPulse: ECG controlled echocardio-grams video generation",
    "year": 2024,
    "published": "2024-10-04T04:49:56Z",
    "categories": [
      "eess.IV",
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "Echocardiography (ECHO) is essential for cardiac assessments, but its video quality and interpretation heavily relies on manual expertise, leading to inconsistent results from clinical and portable devices. ECHO video generation offers a solution by improving automated monitoring through synthetic data and generating high-quality videos from routine health data. However, existing models often face high computational costs, slow inference, and rely on complex conditional prompts that require expe",
    "arxiv_url": "https://arxiv.org/abs/2410.03143v2",
    "pdf_url": "https://arxiv.org/pdf/2410.03143v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2410.03143",
    "arxiv_authors": [
      "Yiwei Li",
      "Sekeun Kim",
      "Zihao Wu",
      "Hanqi Jiang",
      "Yi Pan",
      "Pengfei Jin",
      "Sifan Song",
      "Yucheng Shi",
      "Tianming Liu",
      "Quanzheng Li",
      "Xiang Li"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=ECHOPulse%3A+ECG+controlled+echocardio-grams+video+generation+Yiwei+Li+Sekeun+Kim+Zihao+Wu+Hanqi+Jiang+Yi+Pan",
    "gs_search_success": true,
    "gs_authors": [
      "RMvoE4sAAAAJ",
      "MjkwwiQAAAAJ",
      "rIFRHvIAAAAJ",
      "4S5I1TwAAAAJ",
      "MHq2z7oAAAAJ",
      "A-SP7VYAAAAJ",
      "92RPXm0AAAAJ",
      "sfEPWiAAAAAJ",
      "jENRqN8AAAAJ",
      "adC1a0IAAAAJ",
      "tU6pacYAAAAJ"
    ],
    "citation_count": 6
  },
  {
    "arxiv_id": "2402.18925",
    "title": "PCDepth: Pattern-based Complementary Learning for Monocular Depth Estimation by Best of Both Worlds",
    "year": 2024,
    "published": "2024-02-29T07:31:59Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Event cameras can record scene dynamics with high temporal resolution, providing rich scene details for monocular depth estimation (MDE) even at low-level illumination. Therefore, existing complementary learning approaches for MDE fuse intensity information from images and scene details from event data for better scene understanding. However, most methods directly fuse two modalities at pixel level, ignoring that the attractive complementarity mainly impacts high-level patterns that only occupy ",
    "arxiv_url": "https://arxiv.org/abs/2402.18925v1",
    "pdf_url": "https://arxiv.org/pdf/2402.18925v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2402.18925",
    "arxiv_authors": [
      "Haotian Liu",
      "Sanqing Qu",
      "Fan Lu",
      "Zongtao Bu",
      "Florian Roehrbein",
      "Alois Knoll",
      "Guang Chen"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=PCDepth%3A+Pattern-based+Complementary+Learning+for+Monocular+Depth+Estimation+by+Best+of+Both+Worlds+Haotian+Liu+Sanqing+Qu+Fan+Lu+Zongtao+Bu+Florian+Roehrbein",
    "gs_search_success": true,
    "gs_authors": [
      "DyEUPFUAAAAJ",
      "kBhIyv4AAAAJ",
      "-CA8QgwAAAAJ",
      "IEOJBbAAAAAJ",
      "fbAyN08AAAAJ",
      "pZk-LBIAAAAJ"
    ],
    "citation_count": 8
  },
  {
    "arxiv_id": "2411.10180",
    "title": "CART: Compositional Auto-Regressive Transformer for Image Generation",
    "year": 2024,
    "published": "2024-11-15T13:29:44Z",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "We propose a novel Auto-Regressive (AR) image generation approach that models images as hierarchical compositions of interpretable visual layers. While AR models have achieved transformative success in language modeling, replicating this success in vision tasks remains challenging due to inherent spatial dependencies in images. Addressing the unique challenges of vision tasks, our method (CART) adds image details iteratively via semantically meaningful decompositions. We demonstrate the flexibil",
    "arxiv_url": "https://arxiv.org/abs/2411.10180v3",
    "pdf_url": "https://arxiv.org/pdf/2411.10180v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2411.10180",
    "arxiv_authors": [
      "Siddharth Roheda",
      "Rohit Chowdhury",
      "Aniruddha Bala",
      "Rohan Jaiswal"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=CART%3A+Compositional+Auto-Regressive+Transformer+for+Image+Generation+Siddharth+Roheda+Rohit+Chowdhury+Aniruddha+Bala+Rohan+Jaiswal",
    "gs_search_success": true,
    "gs_authors": [
      "oajO0OMAAAAJ",
      "cmCuIeYAAAAJ"
    ],
    "citation_count": 1
  },
  {
    "arxiv_id": "2407.10559",
    "title": "LIP-CAR: contrast agent reduction by a deep learned inverse problem",
    "year": 2024,
    "published": "2024-07-15T09:16:54Z",
    "categories": [
      "cs.CV",
      "eess.IV",
      "math.NA"
    ],
    "abstract": "The adoption of contrast agents in medical imaging protocols is crucial for accurate and timely diagnosis. While highly effective and characterized by an excellent safety profile, the use of contrast agents has its limitation, including rare risk of allergic reactions, potential environmental impact and economic burdens on patients and healthcare systems. In this work, we address the contrast agent reduction (CAR) problem, which involves reducing the administered dosage of contrast agent while p",
    "arxiv_url": "https://arxiv.org/abs/2407.10559v1",
    "pdf_url": "https://arxiv.org/pdf/2407.10559v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2407.10559",
    "arxiv_authors": [
      "Davide Bianchi",
      "Sonia Colombo Serra",
      "Davide Evangelista",
      "Pengpeng Luo",
      "Elena Morotti",
      "Giovanni Valbusa"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=LIP-CAR%3A+contrast+agent+reduction+by+a+deep+learned+inverse+problem+Davide+Bianchi+Sonia+Colombo+Serra+Davide+Evangelista+Pengpeng+Luo+Elena+Morotti",
    "gs_search_success": true,
    "gs_authors": [
      "s1GjMQMAAAAJ",
      "Ji7V1qEAAAAJ"
    ],
    "citation_count": null
  },
  {
    "arxiv_id": "2304.10054",
    "title": "Complex Mixer for MedMNIST Classification Decathlon",
    "year": 2023,
    "published": "2023-04-20T02:34:36Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "With the development of the medical image field, researchers seek to develop a class of datasets to block the need for medical knowledge, such as \\text{MedMNIST} (v2). MedMNIST (v2) includes a large number of small-sized (28 $\\times$ 28 or 28 $\\times$ 28 $\\times$ 28) medical samples and the corresponding expert annotations (class label). The existing baseline model (Google AutoML Vision, ResNet-50+3D) can reach an average accuracy of over 70\\% on MedMNIST (v2) datasets, which is comparable to th",
    "arxiv_url": "https://arxiv.org/abs/2304.10054v1",
    "pdf_url": "https://arxiv.org/pdf/2304.10054v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2304.10054",
    "arxiv_authors": [
      "Zhuoran Zheng",
      "Xiuyi Jia"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Complex+Mixer+for+MedMNIST+Classification+Decathlon+Zhuoran+Zheng+Xiuyi+Jia",
    "gs_search_success": true,
    "gs_authors": [
      "pXzPL-sAAAAJ",
      "sEFaytgAAAAJ"
    ],
    "citation_count": 9
  },
  {
    "arxiv_id": "2309.02435",
    "title": "Efficient RL via Disentangled Environment and Agent Representations",
    "year": 2023,
    "published": "2023-09-05T17:59:45Z",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "cs.NE",
      "cs.RO"
    ],
    "abstract": "Agents that are aware of the separation between themselves and their environments can leverage this understanding to form effective representations of visual input. We propose an approach for learning such structured representations for RL algorithms, using visual knowledge of the agent, such as its shape or mask, which is often inexpensive to obtain. This is incorporated into the RL objective using a simple auxiliary loss. We show that our method, Structured Environment-Agent Representations, o",
    "arxiv_url": "https://arxiv.org/abs/2309.02435v1",
    "pdf_url": "https://arxiv.org/pdf/2309.02435v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2309.02435",
    "arxiv_authors": [
      "Kevin Gmelin",
      "Shikhar Bahl",
      "Russell Mendonca",
      "Deepak Pathak"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Efficient+RL+via+Disentangled+Environment+and+Agent+Representations+Kevin+Gmelin+Shikhar+Bahl+Russell+Mendonca+Deepak+Pathak",
    "gs_search_success": true,
    "gs_authors": [
      "Uly5spMAAAAJ",
      "AEsPCAUAAAAJ",
      "bdHgGgEAAAAJ"
    ],
    "citation_count": 15
  },
  {
    "arxiv_id": "2307.16256",
    "title": "3D Medical Image Segmentation with Sparse Annotation via Cross-Teaching between 3D and 2D Networks",
    "year": 2023,
    "published": "2023-07-30T15:26:17Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Medical image segmentation typically necessitates a large and precisely annotated dataset. However, obtaining pixel-wise annotation is a labor-intensive task that requires significant effort from domain experts, making it challenging to obtain in practical clinical scenarios. In such situations, reducing the amount of annotation required is a more practical approach. One feasible direction is sparse annotation, which involves annotating only a few slices, and has several advantages over traditio",
    "arxiv_url": "https://arxiv.org/abs/2307.16256v1",
    "pdf_url": "https://arxiv.org/pdf/2307.16256v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2307.16256",
    "arxiv_authors": [
      "Heng Cai",
      "Lei Qi",
      "Qian Yu",
      "Yinghuan Shi",
      "Yang Gao"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=3D+Medical+Image+Segmentation+with+Sparse+Annotation+via+Cross-Teaching+between+3D+and+2D+Networks+Heng+Cai+Lei+Qi+Qian+Yu+Yinghuan+Shi+Yang+Gao",
    "gs_search_success": true,
    "gs_authors": [
      "wlm5blIAAAAJ",
      "k0jjQo8AAAAJ",
      "m6BKDUMAAAAJ",
      "7mm8iZwAAAAJ"
    ],
    "citation_count": 25
  },
  {
    "arxiv_id": "2304.04653",
    "title": "Do We Train on Test Data? The Impact of Near-Duplicates on License Plate Recognition",
    "year": 2023,
    "published": "2023-04-10T15:24:29Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "This work draws attention to the large fraction of near-duplicates in the training and test sets of datasets widely adopted in License Plate Recognition (LPR) research. These duplicates refer to images that, although different, show the same license plate. Our experiments, conducted on the two most popular datasets in the field, show a substantial decrease in recognition rate when six well-known models are trained and tested under fair splits, that is, in the absence of duplicates in the trainin",
    "arxiv_url": "https://arxiv.org/abs/2304.04653v2",
    "pdf_url": "https://arxiv.org/pdf/2304.04653v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2304.04653",
    "arxiv_authors": [
      "Rayson Laroca",
      "Valter Estevam",
      "Alceu S. Britto",
      "Rodrigo Minetto",
      "David Menotti"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Do+We+Train+on+Test+Data%3F+The+Impact+of+Near-Duplicates+on+License+Plate+Recognition+Rayson+Laroca+Valter+Estevam+Alceu+S.+Britto+Rodrigo+Minetto+David+Menotti",
    "gs_search_success": true,
    "gs_authors": [
      "DSZaSPcAAAAJ",
      "Ipu5_-gAAAAJ",
      "pRK8DCUAAAAJ",
      "9jt-hIMAAAAJ",
      "ntyfuEcAAAAJ"
    ],
    "citation_count": 21
  },
  {
    "arxiv_id": "2305.09211",
    "title": "CB-HVTNet: A channel-boosted hybrid vision transformer network for lymphocyte assessment in histopathological images",
    "year": 2023,
    "published": "2023-05-16T06:40:04Z",
    "categories": [
      "eess.IV",
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "Transformers, due to their ability to learn long range dependencies, have overcome the shortcomings of convolutional neural networks (CNNs) for global perspective learning. Therefore, they have gained the focus of researchers for several vision related tasks including medical diagnosis. However, their multi-head attention module only captures global level feature representations, which is insufficient for medical images. To address this issue, we propose a Channel Boosted Hybrid Vision Transform",
    "arxiv_url": "https://arxiv.org/abs/2305.09211v3",
    "pdf_url": "https://arxiv.org/pdf/2305.09211v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2305.09211",
    "arxiv_authors": [
      "Momina Liaqat Ali",
      "Zunaira Rauf",
      "Asifullah Khan",
      "Anabia Sohail",
      "Rafi Ullah",
      "Jeonghwan Gwak"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=CB-HVTNet%3A+A+channel-boosted+hybrid+vision+transformer+network+for+lymphocyte+assessment+in+histopathological+images+Momina+Liaqat+Ali+Zunaira+Rauf+Asifullah+Khan+Anabia+Sohail+Rafi+Ullah",
    "gs_search_success": true,
    "gs_authors": [
      "d_vkseAAAAAJ",
      "FXrC-T4AAAAJ",
      "C8uhO88AAAAJ",
      "f2QH4MsAAAAJ",
      "S4f2ALkAAAAJ",
      "-mx1qCAAAAAJ"
    ],
    "citation_count": 8
  },
  {
    "arxiv_id": "2412.00121",
    "title": "Hybrid Discriminative Attribute-Object Embedding Network for Compositional Zero-Shot Learning",
    "year": 2024,
    "published": "2024-11-28T09:50:25Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Compositional Zero-Shot Learning (CZSL) recognizes new combinations by learning from known attribute-object pairs. However, the main challenge of this task lies in the complex interactions between attributes and object visual representations, which lead to significant differences in images. In addition, the long-tail label distribution in the real world makes the recognition task more complicated. To address these problems, we propose a novel method, named Hybrid Discriminative Attribute-Object ",
    "arxiv_url": "https://arxiv.org/abs/2412.00121v1",
    "pdf_url": "https://arxiv.org/pdf/2412.00121v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2412.00121",
    "arxiv_authors": [
      "Yang Liu",
      "Xinshuo Wang",
      "Jiale Du",
      "Xinbo Gao",
      "Jungong Han"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Hybrid+Discriminative+Attribute-Object+Embedding+Network+for+Compositional+Zero-Shot+Learning+Yang+Liu+Xinshuo+Wang+Jiale+Du+Xinbo+Gao+Jungong+Han",
    "gs_search_success": true,
    "gs_authors": [
      "VZVTOOIAAAAJ",
      "hHK9Ik0AAAAJ",
      "hNi1gxAAAAAJ"
    ],
    "citation_count": null
  },
  {
    "arxiv_id": "2412.16937",
    "title": "PINN-EMFNet: PINN-based and Enhanced Multi-Scale Feature Fusion Network for Breast Ultrasound Images Segmentation",
    "year": 2024,
    "published": "2024-12-22T09:16:00Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "With the rapid development of deep learning and computer vision technologies, medical image segmentation plays a crucial role in the early diagnosis of breast cancer. However, due to the characteristics of breast ultrasound images, such as low contrast, speckle noise, and the highly diverse morphology of tumors, existing segmentation methods exhibit significant limitations in terms of accuracy and robustness. To address these challenges, this study proposes a PINN-based and Enhanced Multi-Scale ",
    "arxiv_url": "https://arxiv.org/abs/2412.16937v1",
    "pdf_url": "https://arxiv.org/pdf/2412.16937v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2412.16937",
    "arxiv_authors": [
      "Jiajun Ding",
      "Beiyao Zhu",
      "Wenjie Wang",
      "Shurong Zhang",
      "Dian Zhua",
      "Zhao Liua"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=PINN-EMFNet%3A+PINN-based+and+Enhanced+Multi-Scale+Feature+Fusion+Network+for+Breast+Ultrasound+Images+Segmentation+Jiajun+Ding+Beiyao+Zhu+Wenjie+Wang+Shurong+Zhang+Dian+Zhua",
    "gs_search_success": true,
    "gs_authors": [],
    "citation_count": 3
  },
  {
    "arxiv_id": "2305.14188",
    "title": "The Best Defense is a Good Offense: Adversarial Augmentation against Adversarial Attacks",
    "year": 2023,
    "published": "2023-05-23T16:07:58Z",
    "categories": [
      "cs.LG",
      "cs.CR",
      "cs.CV"
    ],
    "abstract": "Many defenses against adversarial attacks (\\eg robust classifiers, randomization, or image purification) use countermeasures put to work only after the attack has been crafted. We adopt a different perspective to introduce $A^5$ (Adversarial Augmentation Against Adversarial Attacks), a novel framework including the first certified preemptive defense against adversarial attacks. The main idea is to craft a defensive perturbation to guarantee that any attack (up to a given magnitude) towards the i",
    "arxiv_url": "https://arxiv.org/abs/2305.14188v1",
    "pdf_url": "https://arxiv.org/pdf/2305.14188v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2305.14188",
    "arxiv_authors": [
      "Iuri Frosio",
      "Jan Kautz"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=The+Best+Defense+is+a+Good+Offense%3A+Adversarial+Augmentation+against+Adversarial+Attacks+Iuri+Frosio+Jan+Kautz",
    "gs_search_success": true,
    "gs_authors": [
      "PCJJ8LkAAAAJ",
      "P9FclNEAAAAJ"
    ],
    "citation_count": 31
  },
  {
    "arxiv_id": "2405.01483",
    "title": "MANTIS: Interleaved Multi-Image Instruction Tuning",
    "year": 2024,
    "published": "2024-05-02T17:14:57Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "abstract": "Large multimodal models (LMMs) have shown great results in single-image vision language tasks. However, their abilities to solve multi-image visual language tasks is yet to be improved. The existing LMMs like OpenFlamingo, Emu2, and Idefics gain their multi-image ability through pre-training on hundreds of millions of noisy interleaved image-text data from the web, which is neither efficient nor effective. In this paper, we aim to build strong multi-image LMMs via instruction tuning with academi",
    "arxiv_url": "https://arxiv.org/abs/2405.01483v3",
    "pdf_url": "https://arxiv.org/pdf/2405.01483v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2405.01483",
    "arxiv_authors": [
      "Dongfu Jiang",
      "Xuan He",
      "Huaye Zeng",
      "Cong Wei",
      "Max Ku",
      "Qian Liu",
      "Wenhu Chen"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=MANTIS%3A+Interleaved+Multi-Image+Instruction+Tuning+Dongfu+Jiang+Xuan+He+Huaye+Zeng+Cong+Wei+Max+Ku",
    "gs_search_success": true,
    "gs_authors": [
      "bcbeUo0AAAAJ",
      "U8ShbhUAAAAJ",
      "I7WSsuMAAAAJ",
      "y1d5C5YAAAAJ",
      "G0_RcM4AAAAJ",
      "oCFgVhUAAAAJ",
      "kciKEPUAAAAJ"
    ],
    "citation_count": 202
  },
  {
    "arxiv_id": "2407.14418",
    "title": "Improving classification of road surface conditions via road area extraction and contrastive learning",
    "year": 2024,
    "published": "2024-07-19T15:43:16Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Maintaining roads is crucial to economic growth and citizen well-being because roads are a vital means of transportation. In various countries, the inspection of road surfaces is still done manually, however, to automate it, research interest is now focused on detecting the road surface defects via the visual data. While, previous research has been focused on deep learning methods which tend to process the entire image and leads to heavy computational cost. In this study, we focus our attention ",
    "arxiv_url": "https://arxiv.org/abs/2407.14418v1",
    "pdf_url": "https://arxiv.org/pdf/2407.14418v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2407.14418",
    "arxiv_authors": [
      "Linh Trinh",
      "Ali Anwar",
      "Siegfried Mercelis"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Improving+classification+of+road+surface+conditions+via+road+area+extraction+and+contrastive+learning+Linh+Trinh+Ali+Anwar+Siegfried+Mercelis",
    "gs_search_success": true,
    "gs_authors": [
      "8bC9-DYAAAAJ",
      "7dUIGzkAAAAJ",
      "FtcBjx8AAAAJ"
    ],
    "citation_count": 2
  },
  {
    "arxiv_id": "2505.22673",
    "title": "Physiology-Informed Generative Multi-Task Network for Contrast-Free CT Perfusion",
    "year": 2025,
    "published": "2025-05-12T22:58:55Z",
    "categories": [
      "q-bio.TO",
      "cs.AI",
      "cs.CV"
    ],
    "abstract": "Perfusion imaging is extensively utilized to assess hemodynamic status and tissue perfusion in various organs. Computed tomography perfusion (CTP) imaging plays a key role in the early assessment and planning of stroke treatment. While CTP provides essential perfusion parameters to identify abnormal blood flow in the brain, the use of contrast agents in CTP can lead to allergic reactions and adverse side effects, along with costing USD 4.9 billion worldwide in 2022. To address these challenges, ",
    "arxiv_url": "https://arxiv.org/abs/2505.22673v1",
    "pdf_url": "https://arxiv.org/pdf/2505.22673v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2505.22673",
    "arxiv_authors": [
      "Wasif Khan",
      "Kyle B. See",
      "Simon Kato",
      "Ziqian Huang",
      "Amy Lazarte",
      "Kyle Douglas",
      "Xiangyang Lou",
      "Teng J. Peng",
      "Dhanashree Rajderkar",
      "John Rees",
      "Pina Sanelli",
      "Amita Singh",
      "Ibrahim Tuna",
      "Christina A. Wilson",
      "Ruogu Fang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Physiology-Informed+Generative+Multi-Task+Network+for+Contrast-Free+CT+Perfusion+Wasif+Khan+Kyle+B.+See+Simon+Kato+Ziqian+Huang+Amy+Lazarte",
    "gs_search_success": true,
    "gs_authors": [
      "QwJDNZYAAAAJ",
      "9CmjlcMAAAAJ",
      "h7g65d4AAAAJ"
    ],
    "citation_count": null
  },
  {
    "arxiv_id": "2503.19283",
    "title": "ISPDiffuser: Learning RAW-to-sRGB Mappings with Texture-Aware Diffusion Models and Histogram-Guided Color Consistency",
    "year": 2025,
    "published": "2025-03-25T02:29:39Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "RAW-to-sRGB mapping, or the simulation of the traditional camera image signal processor (ISP), aims to generate DSLR-quality sRGB images from raw data captured by smartphone sensors. Despite achieving comparable results to sophisticated handcrafted camera ISP solutions, existing learning-based methods still struggle with detail disparity and color distortion. In this paper, we present ISPDiffuser, a diffusion-based decoupled framework that separates the RAW-to-sRGB mapping into detail reconstruc",
    "arxiv_url": "https://arxiv.org/abs/2503.19283v1",
    "pdf_url": "https://arxiv.org/pdf/2503.19283v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2503.19283",
    "arxiv_authors": [
      "Yang Ren",
      "Hai Jiang",
      "Menglong Yang",
      "Wei Li",
      "Shuaicheng Liu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=ISPDiffuser%3A+Learning+RAW-to-sRGB+Mappings+with+Texture-Aware+Diffusion+Models+and+Histogram-Guided+Color+Consistency+Yang+Ren+Hai+Jiang+Menglong+Yang+Wei+Li+Shuaicheng+Liu",
    "gs_search_success": true,
    "gs_authors": [
      "GxgVfyVaOEwC",
      "ahVKn4AAAAAJ",
      "1DP9DAUAAAAJ",
      "ZKBt-wgAAAAJ"
    ],
    "citation_count": 2
  },
  {
    "arxiv_id": "2308.00799",
    "title": "Body Knowledge and Uncertainty Modeling for Monocular 3D Human Body Reconstruction",
    "year": 2023,
    "published": "2023-08-01T19:29:10Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "While 3D body reconstruction methods have made remarkable progress recently, it remains difficult to acquire the sufficiently accurate and numerous 3D supervisions required for training. In this paper, we propose \\textbf{KNOWN}, a framework that effectively utilizes body \\textbf{KNOW}ledge and u\\textbf{N}certainty modeling to compensate for insufficient 3D supervisions. KNOWN exploits a comprehensive set of generic body constraints derived from well-established body knowledge. These generic cons",
    "arxiv_url": "https://arxiv.org/abs/2308.00799v1",
    "pdf_url": "https://arxiv.org/pdf/2308.00799v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2308.00799",
    "arxiv_authors": [
      "Yufei Zhang",
      "Hanjing Wang",
      "Jeffrey O. Kephart",
      "Qiang Ji"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Body+Knowledge+and+Uncertainty+Modeling+for+Monocular+3D+Human+Body+Reconstruction+Yufei+Zhang+Hanjing+Wang+Jeffrey+O.+Kephart+Qiang+Ji",
    "gs_search_success": false,
    "gs_authors": [],
    "citation_count": null,
    "error_type": "fetch_failed"
  },
  {
    "arxiv_id": "2502.09026",
    "title": "Billet Number Recognition Based on Test-Time Adaptation",
    "year": 2025,
    "published": "2025-02-13T07:31:03Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "During the steel billet production process, it is essential to recognize machine-printed or manually written billet numbers on moving billets in real-time. To address the issue of low recognition accuracy for existing scene text recognition methods, caused by factors such as image distortions and distribution differences between training and test data, we propose a billet number recognition method that integrates test-time adaptation with prior knowledge. First, we introduce a test-time adaptati",
    "arxiv_url": "https://arxiv.org/abs/2502.09026v1",
    "pdf_url": "https://arxiv.org/pdf/2502.09026v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2502.09026",
    "arxiv_authors": [
      "Yuan Wei",
      "Xiuzhuang Zhou"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Billet+Number+Recognition+Based+on+Test-Time+Adaptation+Yuan+Wei+Xiuzhuang+Zhou",
    "gs_search_success": true,
    "gs_authors": [],
    "citation_count": null
  },
  {
    "arxiv_id": "2307.00717",
    "title": "SSC3OD: Sparsely Supervised Collaborative 3D Object Detection from LiDAR Point Clouds",
    "year": 2023,
    "published": "2023-07-03T02:42:14Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Collaborative 3D object detection, with its improved interaction advantage among multiple agents, has been widely explored in autonomous driving. However, existing collaborative 3D object detectors in a fully supervised paradigm heavily rely on large-scale annotated 3D bounding boxes, which is labor-intensive and time-consuming. To tackle this issue, we propose a sparsely supervised collaborative 3D object detection framework SSC3OD, which only requires each agent to randomly label one object in",
    "arxiv_url": "https://arxiv.org/abs/2307.00717v1",
    "pdf_url": "https://arxiv.org/pdf/2307.00717v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2307.00717",
    "arxiv_authors": [
      "Yushan Han",
      "Hui Zhang",
      "Honglei Zhang",
      "Yidong Li"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=SSC3OD%3A+Sparsely+Supervised+Collaborative+3D+Object+Detection+from+LiDAR+Point+Clouds+Yushan+Han+Hui+Zhang+Honglei+Zhang+Yidong+Li",
    "gs_search_success": true,
    "gs_authors": [
      "X91w9HIAAAAJ",
      "L5o3ERIAAAAJ",
      "TLXKZswAAAAJ"
    ],
    "citation_count": 4
  },
  {
    "arxiv_id": "2311.11013",
    "title": "Implicit Event-RGBD Neural SLAM",
    "year": 2023,
    "published": "2023-11-18T08:48:58Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Implicit neural SLAM has achieved remarkable progress recently. Nevertheless, existing methods face significant challenges in non-ideal scenarios, such as motion blur or lighting variation, which often leads to issues like convergence failures, localization drifts, and distorted mapping. To address these challenges, we propose EN-SLAM, the first event-RGBD implicit neural SLAM framework, which effectively leverages the high rate and high dynamic range advantages of event data for tracking and ma",
    "arxiv_url": "https://arxiv.org/abs/2311.11013v3",
    "pdf_url": "https://arxiv.org/pdf/2311.11013v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2311.11013",
    "arxiv_authors": [
      "Delin Qu",
      "Chi Yan",
      "Dong Wang",
      "Jie Yin",
      "Dan Xu",
      "Bin Zhao",
      "Xuelong Li"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Implicit+Event-RGBD+Neural+SLAM+Delin+Qu+Chi+Yan+Dong+Wang+Jie+Yin+Dan+Xu",
    "gs_search_success": true,
    "gs_authors": [
      "ahUibskAAAAJ",
      "OuSPv-AAAAAJ",
      "8IFwRIQAAAAJ",
      "YCGcVJ0AAAAJ",
      "dasL9V4AAAAJ",
      "DQB0hqwAAAAJ",
      "Y8LVRYIAAAAJ",
      "zgiFoOwAAAAJ"
    ],
    "citation_count": 27
  },
  {
    "arxiv_id": "2502.07160",
    "title": "HDCompression: Hybrid-Diffusion Image Compression for Ultra-Low Bitrates",
    "year": 2025,
    "published": "2025-02-11T00:56:44Z",
    "categories": [
      "cs.CV",
      "cs.MM"
    ],
    "abstract": "Image compression under ultra-low bitrates remains challenging for both conventional learned image compression (LIC) and generative vector-quantized (VQ) modeling. Conventional LIC suffers from severe artifacts due to heavy quantization, while generative VQ modeling gives poor fidelity due to the mismatch between learned generative priors and specific inputs. In this work, we propose Hybrid-Diffusion Image Compression (HDCompression), a dual-stream framework that utilizes both generative VQ-mode",
    "arxiv_url": "https://arxiv.org/abs/2502.07160v3",
    "pdf_url": "https://arxiv.org/pdf/2502.07160v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2502.07160",
    "arxiv_authors": [
      "Lei Lu",
      "Yize Li",
      "Yanzhi Wang",
      "Wei Wang",
      "Wei Jiang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=HDCompression%3A+Hybrid-Diffusion+Image+Compression+for+Ultra-Low+Bitrates+Lei+Lu+Yize+Li+Yanzhi+Wang+Wei+Wang+Wei+Jiang",
    "gs_search_success": true,
    "gs_authors": [
      "aLii7l0AAAAJ",
      "c8koDJgAAAAJ",
      "a7akgIEAAAAJ",
      "_wzQ1EgAAAAJ",
      "rxyMOeAAAAAJ"
    ],
    "citation_count": 3
  },
  {
    "arxiv_id": "2406.05821",
    "title": "F-LMM: Grounding Frozen Large Multimodal Models",
    "year": 2024,
    "published": "2024-06-09T15:14:26Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Endowing Large Multimodal Models (LMMs) with visual grounding capability can significantly enhance AIs' understanding of the visual world and their interaction with humans. However, existing methods typically fine-tune the parameters of LMMs to learn additional segmentation tokens and overfit grounding and segmentation datasets. Such a design would inevitably cause a catastrophic diminution in the indispensable conversational capability of general AI assistants. In this paper, we comprehensively",
    "arxiv_url": "https://arxiv.org/abs/2406.05821v3",
    "pdf_url": "https://arxiv.org/pdf/2406.05821v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2406.05821",
    "arxiv_authors": [
      "Size Wu",
      "Sheng Jin",
      "Wenwei Zhang",
      "Lumin Xu",
      "Wentao Liu",
      "Wei Li",
      "Chen Change Loy"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=F-LMM%3A+Grounding+Frozen+Large+Multimodal+Models+Size+Wu+Sheng+Jin+Wenwei+Zhang+Lumin+Xu+Wentao+Liu",
    "gs_search_success": true,
    "gs_authors": [
      "41KAd6AAAAAJ",
      "wrNd--oAAAAJ",
      "KZn9NWEAAAAJ",
      "559LF80AAAAJ",
      "dkXuy54AAAAJ",
      "y2S02IcAAAAJ",
      "QDXADSEAAAAJ"
    ],
    "citation_count": 25
  },
  {
    "arxiv_id": "2504.09852",
    "title": "GFT: Gradient Focal Transformer",
    "year": 2025,
    "published": "2025-04-14T03:49:06Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Fine-Grained Image Classification (FGIC) remains a complex task in computer vision, as it requires models to distinguish between categories with subtle localized visual differences. Well-studied CNN-based models, while strong in local feature extraction, often fail to capture the global context required for fine-grained recognition, while more recent ViT-backboned models address FGIC with attention-driven mechanisms but lack the ability to adaptively focus on truly discriminative regions. TransF",
    "arxiv_url": "https://arxiv.org/abs/2504.09852v1",
    "pdf_url": "https://arxiv.org/pdf/2504.09852v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2504.09852",
    "arxiv_authors": [
      "Boris Kriuk",
      "Simranjit Kaur Gill",
      "Shoaib Aslam",
      "Amir Fakhrutdinov"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=GFT%3A+Gradient+Focal+Transformer+Boris+Kriuk+Simranjit+Kaur+Gill+Shoaib+Aslam+Amir+Fakhrutdinov",
    "gs_search_success": true,
    "gs_authors": [
      "24vkt1oAAAAJ",
      "KaV-T2gAAAAJ"
    ],
    "citation_count": null
  },
  {
    "arxiv_id": "2302.03689",
    "title": "PartitionVAE -- a human-interpretable VAE",
    "year": 2023,
    "published": "2023-02-04T05:22:19Z",
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "abstract": "VAEs, or variational autoencoders, are autoencoders that explicitly learn the distribution of the input image space rather than assuming no prior information about the distribution. This allows it to classify similar samples close to each other in the latent space's distribution. VAEs classically assume the latent space is normally distributed, though many distribution priors work, and they encode this assumption through a K-L divergence term in the loss function. While VAEs learn the distributi",
    "arxiv_url": "https://arxiv.org/abs/2302.03689v1",
    "pdf_url": "https://arxiv.org/pdf/2302.03689v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2302.03689",
    "arxiv_authors": [
      "Fareed Sheriff",
      "Sameer Pai"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=PartitionVAE+--+a+human-interpretable+VAE+Fareed+Sheriff+Sameer+Pai",
    "gs_search_success": true,
    "gs_authors": [],
    "citation_count": 1
  },
  {
    "arxiv_id": "2307.10768",
    "title": "Decoding the Enigma: Benchmarking Humans and AIs on the Many Facets of Working Memory",
    "year": 2023,
    "published": "2023-07-20T10:57:02Z",
    "categories": [
      "q-bio.NC",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "Working memory (WM), a fundamental cognitive process facilitating the temporary storage, integration, manipulation, and retrieval of information, plays a vital role in reasoning and decision-making tasks. Robust benchmark datasets that capture the multifaceted nature of WM are crucial for the effective development and evaluation of AI WM models. Here, we introduce a comprehensive Working Memory (WorM) benchmark dataset for this purpose. WorM comprises 10 tasks and a total of 1 million trials, as",
    "arxiv_url": "https://arxiv.org/abs/2307.10768v2",
    "pdf_url": "https://arxiv.org/pdf/2307.10768v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2307.10768",
    "arxiv_authors": [
      "Ankur Sikarwar",
      "Mengmi Zhang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Decoding+the+Enigma%3A+Benchmarking+Humans+and+AIs+on+the+Many+Facets+of+Working+Memory+Ankur+Sikarwar+Mengmi+Zhang",
    "gs_search_success": true,
    "gs_authors": [
      "G2sVOhcAAAAJ",
      "eWDiT_wAAAAJ"
    ],
    "citation_count": 4
  },
  {
    "arxiv_id": "2503.24382",
    "title": "Free360: Layered Gaussian Splatting for Unbounded 360-Degree View Synthesis from Extremely Sparse and Unposed Views",
    "year": 2025,
    "published": "2025-03-31T17:59:25Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Neural rendering has demonstrated remarkable success in high-quality 3D neural reconstruction and novel view synthesis with dense input views and accurate poses. However, applying it to extremely sparse, unposed views in unbounded 360° scenes remains a challenging problem. In this paper, we propose a novel neural rendering framework to accomplish the unposed and extremely sparse-view 3D reconstruction in unbounded 360° scenes. To resolve the spatial ambiguity inherent in unbounded scenes with sp",
    "arxiv_url": "https://arxiv.org/abs/2503.24382v1",
    "pdf_url": "https://arxiv.org/pdf/2503.24382v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2503.24382",
    "arxiv_authors": [
      "Chong Bao",
      "Xiyu Zhang",
      "Zehao Yu",
      "Jiale Shi",
      "Guofeng Zhang",
      "Songyou Peng",
      "Zhaopeng Cui"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Free360%3A+Layered+Gaussian+Splatting+for+Unbounded+360-Degree+View+Synthesis+from+Extremely+Sparse+and+Unposed+Views+Chong+Bao+Xiyu+Zhang+Zehao+Yu+Jiale+Shi+Guofeng+Zhang",
    "gs_search_success": true,
    "gs_authors": [
      "4daSiAwAAAAJ",
      "eNypkO0AAAAJ",
      "Z8MwnzsAAAAJ",
      "HRHCYq0AAAAJ",
      "F0xfpXAAAAAJ",
      "vwIRwDUAAAAJ",
      "vvW-UNMAAAAJ"
    ],
    "citation_count": 6
  },
  {
    "arxiv_id": "2310.03456",
    "title": "Multi-Resolution Audio-Visual Feature Fusion for Temporal Action Localization",
    "year": 2023,
    "published": "2023-10-05T10:54:33Z",
    "categories": [
      "cs.CV",
      "cs.LG",
      "cs.MM"
    ],
    "abstract": "Temporal Action Localization (TAL) aims to identify actions' start, end, and class labels in untrimmed videos. While recent advancements using transformer networks and Feature Pyramid Networks (FPN) have enhanced visual feature recognition in TAL tasks, less progress has been made in the integration of audio features into such frameworks. This paper introduces the Multi-Resolution Audio-Visual Feature Fusion (MRAV-FF), an innovative method to merge audio-visual data across different temporal res",
    "arxiv_url": "https://arxiv.org/abs/2310.03456v1",
    "pdf_url": "https://arxiv.org/pdf/2310.03456v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2310.03456",
    "arxiv_authors": [
      "Edward Fish",
      "Jon Weinbren",
      "Andrew Gilbert"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Multi-Resolution+Audio-Visual+Feature+Fusion+for+Temporal+Action+Localization+Edward+Fish+Jon+Weinbren+Andrew+Gilbert",
    "gs_search_success": true,
    "gs_authors": [
      "oLUdxEUAAAAJ",
      "G0YGBekAAAAJ"
    ],
    "citation_count": 2
  },
  {
    "arxiv_id": "2304.00414",
    "title": "Learning Dynamic Style Kernels for Artistic Style Transfer",
    "year": 2023,
    "published": "2023-04-02T00:26:43Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Arbitrary style transfer has been demonstrated to be efficient in artistic image generation. Previous methods either globally modulate the content feature ignoring local details, or overly focus on the local structure details leading to style leakage. In contrast to the literature, we propose a new scheme \\textit{``style kernel\"} that learns {\\em spatially adaptive kernels} for per-pixel stylization, where the convolutional kernels are dynamically generated from the global style-content aligned ",
    "arxiv_url": "https://arxiv.org/abs/2304.00414v2",
    "pdf_url": "https://arxiv.org/pdf/2304.00414v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2304.00414",
    "arxiv_authors": [
      "Wenju Xu",
      "Chengjiang Long",
      "Yongwei Nie"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Learning+Dynamic+Style+Kernels+for+Artistic+Style+Transfer+Wenju+Xu+Chengjiang+Long+Yongwei+Nie",
    "gs_search_success": true,
    "gs_authors": [
      "jVlU_oQAAAAJ",
      "k0XkeiAAAAAJ"
    ],
    "citation_count": 36
  },
  {
    "arxiv_id": "2505.06898",
    "title": "Multi-Modal Explainable Medical AI Assistant for Trustworthy Human-AI Collaboration",
    "year": 2025,
    "published": "2025-05-11T08:32:01Z",
    "categories": [
      "cs.CV",
      "cs.CL"
    ],
    "abstract": "Generalist Medical AI (GMAI) systems have demonstrated expert-level performance in biomedical perception tasks, yet their clinical utility remains limited by inadequate multi-modal explainability and suboptimal prognostic capabilities. Here, we present XMedGPT, a clinician-centric, multi-modal AI assistant that integrates textual and visual interpretability to support transparent and trustworthy medical decision-making. XMedGPT not only produces accurate diagnostic and descriptive outputs, but a",
    "arxiv_url": "https://arxiv.org/abs/2505.06898v1",
    "pdf_url": "https://arxiv.org/pdf/2505.06898v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2505.06898",
    "arxiv_authors": [
      "Honglong Yang",
      "Shanshan Song",
      "Yi Qin",
      "Lehan Wang",
      "Haonan Wang",
      "Xinpeng Ding",
      "Qixiang Zhang",
      "Bodong Du",
      "Xiaomeng Li"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Multi-Modal+Explainable+Medical+AI+Assistant+for+Trustworthy+Human-AI+Collaboration+Honglong+Yang+Shanshan+Song+Yi+Qin+Lehan+Wang+Haonan+Wang",
    "gs_search_success": true,
    "gs_authors": [
      "3BPUjoQAAAAJ",
      "qq-o-JoAAAAJ",
      "FzR7uSsAAAAJ",
      "KDNRnW0AAAAJ",
      "uVTzPpoAAAAJ",
      "oIcu4mgAAAAJ",
      "KqePyFoAAAAJ",
      "EoNWyTcAAAAJ"
    ],
    "citation_count": 3
  },
  {
    "arxiv_id": "2308.06701",
    "title": "Camouflaged Image Synthesis Is All You Need to Boost Camouflaged Detection",
    "year": 2023,
    "published": "2023-08-13T06:55:05Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "abstract": "Camouflaged objects that blend into natural scenes pose significant challenges for deep-learning models to detect and synthesize. While camouflaged object detection is a crucial task in computer vision with diverse real-world applications, this research topic has been constrained by limited data availability. We propose a framework for synthesizing camouflage data to enhance the detection of camouflaged objects in natural scenes. Our approach employs a generative model to produce realistic camou",
    "arxiv_url": "https://arxiv.org/abs/2308.06701v2",
    "pdf_url": "https://arxiv.org/pdf/2308.06701v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2308.06701",
    "arxiv_authors": [
      "Haichao Zhang",
      "Can Qin",
      "Yu Yin",
      "Yun Fu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Camouflaged+Image+Synthesis+Is+All+You+Need+to+Boost+Camouflaged+Detection+Haichao+Zhang+Can+Qin+Yu+Yin+Yun+Fu",
    "gs_search_success": true,
    "gs_authors": [
      "h-JEcQ8AAAAJ",
      "QQoemgQAAAAJ",
      "QCik-YcAAAAJ",
      "pY0_YNcAAAAJ"
    ],
    "citation_count": 6
  },
  {
    "arxiv_id": "2407.01014",
    "title": "An Expectation-Maximization Algorithm for Training Clean Diffusion Models from Corrupted Observations",
    "year": 2024,
    "published": "2024-07-01T07:00:17Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Diffusion models excel in solving imaging inverse problems due to their ability to model complex image priors. However, their reliance on large, clean datasets for training limits their practical use where clean data is scarce. In this paper, we propose EMDiffusion, an expectation-maximization (EM) approach to train diffusion models from corrupted observations. Our method alternates between reconstructing clean images from corrupted data using a known diffusion model (E-step) and refining diffus",
    "arxiv_url": "https://arxiv.org/abs/2407.01014v1",
    "pdf_url": "https://arxiv.org/pdf/2407.01014v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2407.01014",
    "arxiv_authors": [
      "Weimin Bai",
      "Yifei Wang",
      "Wenzheng Chen",
      "He Sun"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=An+Expectation-Maximization+Algorithm+for+Training+Clean+Diffusion+Models+from+Corrupted+Observations+Weimin+Bai+Yifei+Wang+Wenzheng+Chen+He+Sun",
    "gs_search_success": true,
    "gs_authors": [
      "c9V5HkYAAAAJ",
      "M6bEs8IAAAAJ",
      "KzhR_TsAAAAJ"
    ],
    "citation_count": 21
  },
  {
    "arxiv_id": "2505.24361",
    "title": "Revisiting Cross-Modal Knowledge Distillation: A Disentanglement Approach for RGBD Semantic Segmentation",
    "year": 2025,
    "published": "2025-05-30T08:53:35Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Multi-modal RGB and Depth (RGBD) data are predominant in many domains such as robotics, autonomous driving and remote sensing. The combination of these multi-modal data enhances environmental perception by providing 3D spatial context, which is absent in standard RGB images. Although RGBD multi-modal data can be available to train computer vision models, accessing all sensor modalities during the inference stage may be infeasible due to sensor failures or resource constraints, leading to a misma",
    "arxiv_url": "https://arxiv.org/abs/2505.24361v1",
    "pdf_url": "https://arxiv.org/pdf/2505.24361v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2505.24361",
    "arxiv_authors": [
      "Roger Ferrod",
      "Cássio F. Dantas",
      "Luigi Di Caro",
      "Dino Ienco"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Revisiting+Cross-Modal+Knowledge+Distillation%3A+A+Disentanglement+Approach+for+RGBD+Semantic+Segmentation+Roger+Ferrod+C%C3%A1ssio+F.+Dantas+Luigi+Di+Caro+Dino+Ienco",
    "gs_search_success": true,
    "gs_authors": [
      "YgcZQpgAAAAJ",
      "MS7sDJgAAAAJ",
      "XQAvqi4AAAAJ",
      "C8zfH3kAAAAJ"
    ],
    "citation_count": null
  },
  {
    "arxiv_id": "2305.15001",
    "title": "Contrastive Training of Complex-Valued Autoencoders for Object Discovery",
    "year": 2023,
    "published": "2023-05-24T10:37:43Z",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "abstract": "Current state-of-the-art object-centric models use slots and attention-based routing for binding. However, this class of models has several conceptual limitations: the number of slots is hardwired; all slots have equal capacity; training has high computational cost; there are no object-level relational factors within slots. Synchrony-based models in principle can address these limitations by using complex-valued activations which store binding information in their phase components. However, work",
    "arxiv_url": "https://arxiv.org/abs/2305.15001v3",
    "pdf_url": "https://arxiv.org/pdf/2305.15001v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2305.15001",
    "arxiv_authors": [
      "Aleksandar Stanić",
      "Anand Gopalakrishnan",
      "Kazuki Irie",
      "Jürgen Schmidhuber"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Contrastive+Training+of+Complex-Valued+Autoencoders+for+Object+Discovery+Aleksandar+Stani%C4%87+Anand+Gopalakrishnan+Kazuki+Irie+J%C3%BCrgen+Schmidhuber",
    "gs_search_success": true,
    "gs_authors": [
      "gLnCTgIAAAAJ",
      "tx0opKcAAAAJ",
      "-gZ-BdwAAAAJ",
      "SsbgJ1UAAAAJ"
    ],
    "citation_count": 20
  },
  {
    "arxiv_id": "2307.00485",
    "title": "TopicFM+: Boosting Accuracy and Efficiency of Topic-Assisted Feature Matching",
    "year": 2023,
    "published": "2023-07-02T06:14:07Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "This study tackles the challenge of image matching in difficult scenarios, such as scenes with significant variations or limited texture, with a strong emphasis on computational efficiency. Previous studies have attempted to address this challenge by encoding global scene contexts using Transformers. However, these approaches suffer from high computational costs and may not capture sufficient high-level contextual information, such as structural shapes or semantic instances. Consequently, the en",
    "arxiv_url": "https://arxiv.org/abs/2307.00485v1",
    "pdf_url": "https://arxiv.org/pdf/2307.00485v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2307.00485",
    "arxiv_authors": [
      "Khang Truong Giang",
      "Soohwan Song",
      "Sungho Jo"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=TopicFM%2B%3A+Boosting+Accuracy+and+Efficiency+of+Topic-Assisted+Feature+Matching+Khang+Truong+Giang+Soohwan+Song+Sungho+Jo",
    "gs_search_success": true,
    "gs_authors": [
      "Wy4ytA0AAAAJ",
      "mEvl5GsAAAAJ"
    ],
    "citation_count": 14
  },
  {
    "arxiv_id": "2505.16334",
    "title": "Panoptic Captioning: An Equivalence Bridge for Image and Text",
    "year": 2025,
    "published": "2025-05-22T07:44:10Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "This work introduces panoptic captioning, a novel task striving to seek the minimum text equivalent of images, which has broad potential applications. We take the first step towards panoptic captioning by formulating it as a task of generating a comprehensive textual description for an image, which encapsulates all entities, their respective locations and attributes, relationships among entities, as well as global image state. Through an extensive evaluation, our work reveals that state-of-the-a",
    "arxiv_url": "https://arxiv.org/abs/2505.16334v3",
    "pdf_url": "https://arxiv.org/pdf/2505.16334v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2505.16334",
    "arxiv_authors": [
      "Kun-Yu Lin",
      "Hongjun Wang",
      "Weining Ren",
      "Kai Han"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Panoptic+Captioning%3A+An+Equivalence+Bridge+for+Image+and+Text+Kun-Yu+Lin+Hongjun+Wang+Weining+Ren+Kai+Han",
    "gs_search_success": false,
    "gs_authors": [],
    "citation_count": null,
    "error_type": "fetch_failed"
  },
  {
    "arxiv_id": "2406.01187",
    "title": "Patch-Based Encoder-Decoder Architecture for Automatic Transmitted Light to Fluorescence Imaging Transition: Contribution to the LightMyCells Challenge",
    "year": 2024,
    "published": "2024-06-03T10:49:34Z",
    "categories": [
      "eess.IV",
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "Automatic prediction of fluorescently labeled organelles from label-free transmitted light input images is an important, yet difficult task. The traditional way to obtain fluorescence images is related to performing biochemical labeling which is time-consuming and costly. Therefore, an automatic algorithm to perform the task based on the label-free transmitted light microscopy could be strongly beneficial. The importance of the task motivated researchers from the France-BioImaging to organize th",
    "arxiv_url": "https://arxiv.org/abs/2406.01187v1",
    "pdf_url": "https://arxiv.org/pdf/2406.01187v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2406.01187",
    "arxiv_authors": [
      "Marek Wodzinski",
      "Henning Müller"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Patch-Based+Encoder-Decoder+Architecture+for+Automatic+Transmitted+Light+to+Fluorescence+Imaging+Transition%3A+Contribution+to+the+LightMyCells+Challenge+Marek+Wodzinski+Henning+M%C3%BCller",
    "gs_search_success": true,
    "gs_authors": [
      "YvLZ5rsAAAAJ",
      "UEZ9RlUAAAAJ"
    ],
    "citation_count": 1
  },
  {
    "arxiv_id": "2411.09968",
    "title": "Seeing Clearly by Layer Two: Enhancing Attention Heads to Alleviate Hallucination in LVLMs",
    "year": 2024,
    "published": "2024-11-15T05:51:29Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "The hallucination problem in multimodal large language models (MLLMs) remains a common issue. Although image tokens occupy a majority of the input sequence of MLLMs, there is limited research to explore the relationship between image tokens and hallucinations. In this paper, we analyze the distribution of attention scores for image tokens across each layer and head of the model, revealing an intriguing and common phenomenon: most hallucinations are closely linked to the pattern of attention sink",
    "arxiv_url": "https://arxiv.org/abs/2411.09968v1",
    "pdf_url": "https://arxiv.org/pdf/2411.09968v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2411.09968",
    "arxiv_authors": [
      "Xiaofeng Zhang",
      "Yihao Quan",
      "Chaochen Gu",
      "Chen Shen",
      "Xiaosong Yuan",
      "Shaotian Yan",
      "Hao Cheng",
      "Kaijie Wu",
      "Jieping Ye"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Seeing+Clearly+by+Layer+Two%3A+Enhancing+Attention+Heads+to+Alleviate+Hallucination+in+LVLMs+Xiaofeng+Zhang+Yihao+Quan+Chaochen+Gu+Chen+Shen+Xiaosong+Yuan",
    "gs_search_success": true,
    "gs_authors": [
      "T9AzhwcAAAAJ",
      "Y6Z5xQQAAAAJ",
      "-Fg_EuEAAAAJ",
      "agZ9flIAAAAJ",
      "sBhbb2wAAAAJ",
      "Rm69hkYAAAAJ",
      "b6vn1uMAAAAJ"
    ],
    "citation_count": 32
  },
  {
    "arxiv_id": "2311.05410",
    "title": "Linear Gaussian Bounding Box Representation and Ring-Shaped Rotated Convolution for Oriented Object Detection",
    "year": 2023,
    "published": "2023-11-09T14:45:22Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "In oriented object detection, current representations of oriented bounding boxes (OBBs) often suffer from boundary discontinuity problem. Methods of designing continuous regression losses do not essentially solve this problem. Although Gaussian bounding box (GBB) representation avoids this problem, directly regressing GBB is susceptible to numerical instability. We propose linear GBB (LGBB), a novel OBB representation. By linearly transforming the elements of GBB, LGBB avoids the boundary discon",
    "arxiv_url": "https://arxiv.org/abs/2311.05410v2",
    "pdf_url": "https://arxiv.org/pdf/2311.05410v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2311.05410",
    "arxiv_authors": [
      "Zhen Zhou",
      "Yunkai Ma",
      "Junfeng Fan",
      "Zhaoyang Liu",
      "Fengshui Jing",
      "Min Tan"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Linear+Gaussian+Bounding+Box+Representation+and+Ring-Shaped+Rotated+Convolution+for+Oriented+Object+Detection+Zhen+Zhou+Yunkai+Ma+Junfeng+Fan+Zhaoyang+Liu+Fengshui+Jing",
    "gs_search_success": true,
    "gs_authors": [
      "jxxktc4AAAAJ",
      "lqht97EAAAAJ",
      "AOiXnrwAAAAJ",
      "NHitQ88AAAAJ"
    ],
    "citation_count": 10
  },
  {
    "arxiv_id": "2303.12533",
    "title": "Pixel-wise Agricultural Image Time Series Classification: Comparisons and a Deformable Prototype-based Approach",
    "year": 2023,
    "published": "2023-03-22T13:06:39Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Improvements in Earth observation by satellites allow for imagery of ever higher temporal and spatial resolution. Leveraging this data for agricultural monitoring is key for addressing environmental and economic challenges. Current methods for crop segmentation using temporal data either rely on annotated data or are heavily engineered to compensate the lack of supervision. In this paper, we present and compare datasets and methods for both supervised and unsupervised pixel-wise segmentation of ",
    "arxiv_url": "https://arxiv.org/abs/2303.12533v2",
    "pdf_url": "https://arxiv.org/pdf/2303.12533v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2303.12533",
    "arxiv_authors": [
      "Elliot Vincent",
      "Jean Ponce",
      "Mathieu Aubry"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Pixel-wise+Agricultural+Image+Time+Series+Classification%3A+Comparisons+and+a+Deformable+Prototype-based+Approach+Elliot+Vincent+Jean+Ponce+Mathieu+Aubry",
    "gs_search_success": true,
    "gs_authors": [
      "vC2vywcAAAAJ",
      "0MiPsosAAAAJ",
      "AUz-y5wAAAAJ"
    ],
    "citation_count": 8
  },
  {
    "arxiv_id": "2401.07567",
    "title": "Bias-Conflict Sample Synthesis and Adversarial Removal Debias Strategy for Temporal Sentence Grounding in Video",
    "year": 2024,
    "published": "2024-01-15T09:59:43Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Temporal Sentence Grounding in Video (TSGV) is troubled by dataset bias issue, which is caused by the uneven temporal distribution of the target moments for samples with similar semantic components in input videos or query texts. Existing methods resort to utilizing prior knowledge about bias to artificially break this uneven distribution, which only removes a limited amount of significant language biases. In this work, we propose the bias-conflict sample synthesis and adversarial removal debias",
    "arxiv_url": "https://arxiv.org/abs/2401.07567v2",
    "pdf_url": "https://arxiv.org/pdf/2401.07567v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2401.07567",
    "arxiv_authors": [
      "Zhaobo Qi",
      "Yibo Yuan",
      "Xiaowen Ruan",
      "Shuhui Wang",
      "Weigang Zhang",
      "Qingming Huang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Bias-Conflict+Sample+Synthesis+and+Adversarial+Removal+Debias+Strategy+for+Temporal+Sentence+Grounding+in+Video+Zhaobo+Qi+Yibo+Yuan+Xiaowen+Ruan+Shuhui+Wang+Weigang+Zhang",
    "gs_search_success": true,
    "gs_authors": [
      "J1vMnRgAAAAJ",
      "QZ8URKAAAAAJ",
      "h-JxBSYAAAAJ",
      "4bm5wYUAAAAJ"
    ],
    "citation_count": 12
  },
  {
    "arxiv_id": "2310.06291",
    "title": "Three-Dimensional Medical Image Fusion with Deformable Cross-Attention",
    "year": 2023,
    "published": "2023-10-10T04:10:56Z",
    "categories": [
      "eess.IV",
      "cs.CV",
      "physics.med-ph"
    ],
    "abstract": "Multimodal medical image fusion plays an instrumental role in several areas of medical image processing, particularly in disease recognition and tumor detection. Traditional fusion methods tend to process each modality independently before combining the features and reconstructing the fusion image. However, this approach often neglects the fundamental commonalities and disparities between multimodal information. Furthermore, the prevailing methodologies are largely confined to fusing two-dimensi",
    "arxiv_url": "https://arxiv.org/abs/2310.06291v1",
    "pdf_url": "https://arxiv.org/pdf/2310.06291v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2310.06291",
    "arxiv_authors": [
      "Lin Liu",
      "Xinxin Fan",
      "Chulong Zhang",
      "Jingjing Dai",
      "Yaoqin Xie",
      "Xiaokun Liang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Three-Dimensional+Medical+Image+Fusion+with+Deformable+Cross-Attention+Lin+Liu+Xinxin+Fan+Chulong+Zhang+Jingjing+Dai+Yaoqin+Xie",
    "gs_search_success": true,
    "gs_authors": [
      "z1E45JUAAAAJ",
      "KfrVZewAAAAJ",
      "BAcuUS4AAAAJ",
      "ACrSrHkAAAAJ",
      "ndrM4NcAAAAJ",
      "ALkaB9YAAAAJ"
    ],
    "citation_count": 3
  },
  {
    "arxiv_id": "2503.12947",
    "title": "DivCon-NeRF: Diverse and Consistent Ray Augmentation for Few-Shot NeRF",
    "year": 2025,
    "published": "2025-03-17T08:59:34Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Neural Radiance Field (NeRF) has shown remarkable performance in novel view synthesis but requires numerous multi-view images, limiting its practicality in few-shot scenarios. Ray augmentation has been proposed to alleviate overfitting caused by sparse training data by generating additional rays. However, existing methods, which generate augmented rays only near the original rays, exhibit pronounced floaters and appearance distortions due to limited viewpoints and inconsistent rays obstructed by",
    "arxiv_url": "https://arxiv.org/abs/2503.12947v2",
    "pdf_url": "https://arxiv.org/pdf/2503.12947v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2503.12947",
    "arxiv_authors": [
      "Ingyun Lee",
      "Jae Won Jang",
      "Seunghyeon Seo",
      "Nojun Kwak"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=DivCon-NeRF%3A+Diverse+and+Consistent+Ray+Augmentation+for+Few-Shot+NeRF+Ingyun+Lee+Jae+Won+Jang+Seunghyeon+Seo+Nojun+Kwak",
    "gs_search_success": true,
    "gs_authors": [
      "ChN2UUkAAAAJ",
      "h_8-1M0AAAAJ",
      "LL9u-5IAAAAJ"
    ],
    "citation_count": null
  },
  {
    "arxiv_id": "2404.00672",
    "title": "A General and Efficient Training for Transformer via Token Expansion",
    "year": 2024,
    "published": "2024-03-31T12:44:24Z",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.CV"
    ],
    "abstract": "The remarkable performance of Vision Transformers (ViTs) typically requires an extremely large training cost. Existing methods have attempted to accelerate the training of ViTs, yet typically disregard method universality with accuracy dropping. Meanwhile, they break the training consistency of the original transformers, including the consistency of hyper-parameters, architecture, and strategy, which prevents them from being widely applied to different Transformer networks. In this paper, we pro",
    "arxiv_url": "https://arxiv.org/abs/2404.00672v1",
    "pdf_url": "https://arxiv.org/pdf/2404.00672v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2404.00672",
    "arxiv_authors": [
      "Wenxuan Huang",
      "Yunhang Shen",
      "Jiao Xie",
      "Baochang Zhang",
      "Gaoqi He",
      "Ke Li",
      "Xing Sun",
      "Shaohui Lin"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=A+General+and+Efficient+Training+for+Transformer+via+Token+Expansion+Wenxuan+Huang+Yunhang+Shen+Jiao+Xie+Baochang+Zhang+Gaoqi+He",
    "gs_search_success": true,
    "gs_authors": [
      "6Ys6HgsAAAAJ",
      "IUtix9IAAAAJ",
      "5Nt_2DYAAAAJ",
      "k8AMa1kAAAAJ",
      "29teR74AAAAJ",
      "ImJz6MsAAAAJ",
      "mfWsFM0AAAAJ"
    ],
    "citation_count": 8
  },
  {
    "arxiv_id": "2307.09520",
    "title": "Adversarial Bayesian Augmentation for Single-Source Domain Generalization",
    "year": 2023,
    "published": "2023-07-18T18:01:30Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Generalizing to unseen image domains is a challenging problem primarily due to the lack of diverse training data, inaccessible target data, and the large domain shift that may exist in many real-world settings. As such data augmentation is a critical component of domain generalization methods that seek to address this problem. We present Adversarial Bayesian Augmentation (ABA), a novel algorithm that learns to generate image augmentations in the challenging single-source domain generalization se",
    "arxiv_url": "https://arxiv.org/abs/2307.09520v2",
    "pdf_url": "https://arxiv.org/pdf/2307.09520v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2307.09520",
    "arxiv_authors": [
      "Sheng Cheng",
      "Tejas Gokhale",
      "Yezhou Yang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Adversarial+Bayesian+Augmentation+for+Single-Source+Domain+Generalization+Sheng+Cheng+Tejas+Gokhale+Yezhou+Yang",
    "gs_search_success": true,
    "gs_authors": [
      "_ILTlEwAAAAJ",
      "k2suuZgAAAAJ",
      "TWAwdYsAAAAJ"
    ],
    "citation_count": 28
  },
  {
    "arxiv_id": "2412.01254",
    "title": "EmojiDiff: Advanced Facial Expression Control with High Identity Preservation in Portrait Generation",
    "year": 2024,
    "published": "2024-12-02T08:24:11Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "This paper aims to bring fine-grained expression control while maintaining high-fidelity identity in portrait generation. This is challenging due to the mutual interference between expression and identity: (i) fine expression control signals inevitably introduce appearance-related semantics (e.g., facial contours, and ratio), which impact the identity of the generated portrait; (ii) even coarse-grained expression control can cause facial changes that compromise identity, since they all act on th",
    "arxiv_url": "https://arxiv.org/abs/2412.01254v2",
    "pdf_url": "https://arxiv.org/pdf/2412.01254v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2412.01254",
    "arxiv_authors": [
      "Liangwei Jiang",
      "Ruida Li",
      "Zhifeng Zhang",
      "Shuo Fang",
      "Chenguang Ma"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=EmojiDiff%3A+Advanced+Facial+Expression+Control+with+High+Identity+Preservation+in+Portrait+Generation+Liangwei+Jiang+Ruida+Li+Zhifeng+Zhang+Shuo+Fang+Chenguang+Ma",
    "gs_search_success": true,
    "gs_authors": [],
    "citation_count": 4
  },
  {
    "arxiv_id": "2505.02025",
    "title": "A Birotation Solution for Relative Pose Problems",
    "year": 2025,
    "published": "2025-05-04T08:24:14Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Relative pose estimation, a fundamental computer vision problem, has been extensively studied for decades. Existing methods either estimate and decompose the essential matrix or directly estimate the rotation and translation to obtain the solution. In this article, we break the mold by tackling this traditional problem with a novel birotation solution. We first introduce three basis transformations, each associated with a geometric metric to quantify the distance between the relative pose to be ",
    "arxiv_url": "https://arxiv.org/abs/2505.02025v1",
    "pdf_url": "https://arxiv.org/pdf/2505.02025v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2505.02025",
    "arxiv_authors": [
      "Hongbo Zhao",
      "Ziwei Long",
      "Mengtan Zhang",
      "Hanli Wang",
      "Qijun Chen",
      "Rui Fan"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=A+Birotation+Solution+for+Relative+Pose+Problems+Hongbo+Zhao+Ziwei+Long+Mengtan+Zhang+Hanli+Wang+Qijun+Chen",
    "gs_search_success": true,
    "gs_authors": [
      "uanS4aIAAAAJ",
      "WioFu64AAAAJ",
      "P5AJTXcAAAAJ"
    ],
    "citation_count": null
  },
  {
    "arxiv_id": "2302.06353",
    "title": "Contour-based Interactive Segmentation",
    "year": 2023,
    "published": "2023-02-13T13:35:26Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Recent advances in interactive segmentation (IS) allow speeding up and simplifying image editing and labeling greatly. The majority of modern IS approaches accept user input in the form of clicks. However, using clicks may require too many user interactions, especially when selecting small objects, minor parts of an object, or a group of objects of the same type. In this paper, we consider such a natural form of user interaction as a loose contour, and introduce a contour-based IS method. We eva",
    "arxiv_url": "https://arxiv.org/abs/2302.06353v2",
    "pdf_url": "https://arxiv.org/pdf/2302.06353v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2302.06353",
    "arxiv_authors": [
      "Danil Galeev",
      "Polina Popenova",
      "Anna Vorontsova",
      "Anton Konushin"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Contour-based+Interactive+Segmentation+Danil+Galeev+Polina+Popenova+Anna+Vorontsova+Anton+Konushin",
    "gs_search_success": true,
    "gs_authors": [
      "HiVoQCIAAAAJ",
      "HCj54P8AAAAJ",
      "ZT_k-wMAAAAJ"
    ],
    "citation_count": 1
  },
  {
    "arxiv_id": "2410.13598",
    "title": "Let Me Finish My Sentence: Video Temporal Grounding with Holistic Text Understanding",
    "year": 2024,
    "published": "2024-10-17T14:31:02Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Video Temporal Grounding (VTG) aims to identify visual frames in a video clip that match text queries. Recent studies in VTG employ cross-attention to correlate visual frames and text queries as individual token sequences. However, these approaches overlook a crucial aspect of the problem: a holistic understanding of the query sentence. A model may capture correlations between individual word tokens and arbitrary visual frames while possibly missing out on the global meaning. To address this, we",
    "arxiv_url": "https://arxiv.org/abs/2410.13598v1",
    "pdf_url": "https://arxiv.org/pdf/2410.13598v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2410.13598",
    "arxiv_authors": [
      "Jongbhin Woo",
      "Hyeonggon Ryu",
      "Youngjoon Jang",
      "Jae Won Cho",
      "Joon Son Chung"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Let+Me+Finish+My+Sentence%3A+Video+Temporal+Grounding+with+Holistic+Text+Understanding+Jongbhin+Woo+Hyeonggon+Ryu+Youngjoon+Jang+Jae+Won+Cho+Joon+Son+Chung",
    "gs_search_success": true,
    "gs_authors": [
      "v9BcFogAAAAJ",
      "oB5AOQQAAAAJ",
      "O_l7ShIAAAAJ",
      "th4fvfIAAAAJ",
      "JJ_LQ0YAAAAJ"
    ],
    "citation_count": 1
  },
  {
    "arxiv_id": "2411.07765",
    "title": "Novel View Synthesis with Pixel-Space Diffusion Models",
    "year": 2024,
    "published": "2024-11-12T12:58:33Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Synthesizing a novel view from a single input image is a challenging task. Traditionally, this task was approached by estimating scene depth, warping, and inpainting, with machine learning models enabling parts of the pipeline. More recently, generative models are being increasingly employed in novel view synthesis (NVS), often encompassing the entire end-to-end system. In this work, we adapt a modern diffusion model architecture for end-to-end NVS in the pixel space, substantially outperforming",
    "arxiv_url": "https://arxiv.org/abs/2411.07765v1",
    "pdf_url": "https://arxiv.org/pdf/2411.07765v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2411.07765",
    "arxiv_authors": [
      "Noam Elata",
      "Bahjat Kawar",
      "Yaron Ostrovsky-Berman",
      "Miriam Farber",
      "Ron Sokolovsky"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Novel+View+Synthesis+with+Pixel-Space+Diffusion+Models+Noam+Elata+Bahjat+Kawar+Yaron+Ostrovsky-Berman+Miriam+Farber+Ron+Sokolovsky",
    "gs_search_success": true,
    "gs_authors": [
      "36gR46QAAAAJ",
      "88l-2DcAAAAJ",
      "z005TlIAAAAJ"
    ],
    "citation_count": 5
  },
  {
    "arxiv_id": "2408.07825",
    "title": "SSRFlow: Semantic-aware Fusion with Spatial Temporal Re-embedding for Real-world Scene Flow",
    "year": 2024,
    "published": "2024-07-31T02:28:40Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Scene flow, which provides the 3D motion field of the first frame from two consecutive point clouds, is vital for dynamic scene perception. However, contemporary scene flow methods face three major challenges. Firstly, they lack global flow embedding or only consider the context of individual point clouds before embedding, leading to embedded points struggling to perceive the consistent semantic relationship of another frame. To address this issue, we propose a novel approach called Dual Cross A",
    "arxiv_url": "https://arxiv.org/abs/2408.07825v1",
    "pdf_url": "https://arxiv.org/pdf/2408.07825v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2408.07825",
    "arxiv_authors": [
      "Zhiyang Lu",
      "Qinghan Chen",
      "Zhimin Yuan",
      "Ming Cheng"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=SSRFlow%3A+Semantic-aware+Fusion+with+Spatial+Temporal+Re-embedding+for+Real-world+Scene+Flow+Zhiyang+Lu+Qinghan+Chen+Zhimin+Yuan+Ming+Cheng",
    "gs_search_success": true,
    "gs_authors": [
      "JOoZUmUAAAAJ",
      "Ii6P4MkAAAAJ",
      "kAnv3SkAAAAJ",
      "477EMiUAAAAJ",
      "L214A8gAAAAJ"
    ],
    "citation_count": null
  },
  {
    "arxiv_id": "2401.07061",
    "title": "Dual-View Data Hallucination with Semantic Relation Guidance for Few-Shot Image Recognition",
    "year": 2024,
    "published": "2024-01-13T12:32:29Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Learning to recognize novel concepts from just a few image samples is very challenging as the learned model is easily overfitted on the few data and results in poor generalizability. One promising but underexplored solution is to compensate the novel classes by generating plausible samples. However, most existing works of this line exploit visual information only, rendering the generated data easy to be distracted by some challenging factors contained in the few available samples. Being aware of",
    "arxiv_url": "https://arxiv.org/abs/2401.07061v2",
    "pdf_url": "https://arxiv.org/pdf/2401.07061v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2401.07061",
    "arxiv_authors": [
      "Hefeng Wu",
      "Guangzhi Ye",
      "Ziyang Zhou",
      "Ling Tian",
      "Qing Wang",
      "Liang Lin"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Dual-View+Data+Hallucination+with+Semantic+Relation+Guidance+for+Few-Shot+Image+Recognition+Hefeng+Wu+Guangzhi+Ye+Ziyang+Zhou+Ling+Tian+Qing+Wang",
    "gs_search_success": true,
    "gs_authors": [
      "Nav8m8gAAAAJ",
      "gX2pNewAAAAJ",
      "yg7JdVQAAAAJ"
    ],
    "citation_count": 7
  },
  {
    "arxiv_id": "2407.04003",
    "title": "Fully Fine-tuned CLIP Models are Efficient Few-Shot Learners",
    "year": 2024,
    "published": "2024-07-04T15:22:54Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Prompt tuning, which involves training a small set of parameters, effectively enhances the pre-trained Vision-Language Models (VLMs) to downstream tasks. However, they often come at the cost of flexibility and adaptability when the tuned models are applied to different datasets or domains. In this paper, we explore capturing the task-specific information via meticulous refinement of entire VLMs, with minimal parameter adjustments. When fine-tuning the entire VLMs for specific tasks under limited",
    "arxiv_url": "https://arxiv.org/abs/2407.04003v1",
    "pdf_url": "https://arxiv.org/pdf/2407.04003v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2407.04003",
    "arxiv_authors": [
      "Mushui Liu",
      "Bozheng Li",
      "Yunlong Yu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Fully+Fine-tuned+CLIP+Models+are+Efficient+Few-Shot+Learners+Mushui+Liu+Bozheng+Li+Yunlong+Yu",
    "gs_search_success": true,
    "gs_authors": [
      "NB9Mn5MAAAAJ",
      "-WUyWpMAAAAJ",
      "cbC26HkAAAAJ",
      "qx1yRVEAAAAJ",
      "mCaOnp4AAAAJ"
    ],
    "citation_count": 10
  },
  {
    "arxiv_id": "2403.05124",
    "title": "CLIP-Gaze: Towards General Gaze Estimation via Visual-Linguistic Model",
    "year": 2024,
    "published": "2024-03-08T07:37:21Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Gaze estimation methods often experience significant performance degradation when evaluated across different domains, due to the domain gap between the testing and training data. Existing methods try to address this issue using various domain generalization approaches, but with little success because of the limited diversity of gaze datasets, such as appearance, wearable, and image quality. To overcome these limitations, we propose a novel framework called CLIP-Gaze that utilizes a pre-trained v",
    "arxiv_url": "https://arxiv.org/abs/2403.05124v1",
    "pdf_url": "https://arxiv.org/pdf/2403.05124v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2403.05124",
    "arxiv_authors": [
      "Pengwei Yin",
      "Guanzhong Zeng",
      "Jingjing Wang",
      "Di Xie"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=CLIP-Gaze%3A+Towards+General+Gaze+Estimation+via+Visual-Linguistic+Model+Pengwei+Yin+Guanzhong+Zeng+Jingjing+Wang+Di+Xie",
    "gs_search_success": true,
    "gs_authors": [
      "7sxVnykAAAAJ",
      "Eyx-kX8AAAAJ",
      "muYYWO4AAAAJ"
    ],
    "citation_count": 20
  },
  {
    "arxiv_id": "2304.01227",
    "title": "Resolution-Invariant Image Classification based on Fourier Neural Operators",
    "year": 2023,
    "published": "2023-04-02T10:23:36Z",
    "categories": [
      "cs.CV",
      "cs.LG",
      "math.NA"
    ],
    "abstract": "In this paper we investigate the use of Fourier Neural Operators (FNOs) for image classification in comparison to standard Convolutional Neural Networks (CNNs). Neural operators are a discretization-invariant generalization of neural networks to approximate operators between infinite dimensional function spaces. FNOs - which are neural operators with a specific parametrization - have been applied successfully in the context of parametric PDEs. We derive the FNO architecture as an example for con",
    "arxiv_url": "https://arxiv.org/abs/2304.01227v1",
    "pdf_url": "https://arxiv.org/pdf/2304.01227v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2304.01227",
    "arxiv_authors": [
      "Samira Kabri",
      "Tim Roith",
      "Daniel Tenbrinck",
      "Martin Burger"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Resolution-Invariant+Image+Classification+based+on+Fourier+Neural+Operators+Samira+Kabri+Tim+Roith+Daniel+Tenbrinck+Martin+Burger",
    "gs_search_success": true,
    "gs_authors": [
      "tbAlI9kAAAAJ",
      "kVi18K0AAAAJ",
      "cDDMZv4AAAAJ",
      "BKlbQTAAAAAJ"
    ],
    "citation_count": 16
  },
  {
    "arxiv_id": "2404.12680",
    "title": "VoxAtnNet: A 3D Point Clouds Convolutional Neural Network for Generalizable Face Presentation Attack Detection",
    "year": 2024,
    "published": "2024-04-19T07:30:36Z",
    "categories": [
      "cs.CV",
      "cs.CR"
    ],
    "abstract": "Facial biometrics are an essential components of smartphones to ensure reliable and trustworthy authentication. However, face biometric systems are vulnerable to Presentation Attacks (PAs), and the availability of more sophisticated presentation attack instruments such as 3D silicone face masks will allow attackers to deceive face recognition systems easily. In this work, we propose a novel Presentation Attack Detection (PAD) algorithm based on 3D point clouds captured using the frontal camera o",
    "arxiv_url": "https://arxiv.org/abs/2404.12680v1",
    "pdf_url": "https://arxiv.org/pdf/2404.12680v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2404.12680",
    "arxiv_authors": [
      "Raghavendra Ramachandra",
      "Narayan Vetrekar",
      "Sushma Venkatesh",
      "Savita Nageshker",
      "Jag Mohan Singh",
      "R. S. Gad"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=VoxAtnNet%3A+A+3D+Point+Clouds+Convolutional+Neural+Network+for+Generalizable+Face+Presentation+Attack+Detection+Raghavendra+Ramachandra+Narayan+Vetrekar+Sushma+Venkatesh+Savita+Nageshker+Jag+Mohan+Singh",
    "gs_search_success": true,
    "gs_authors": [
      "qD0Uzq0AAAAJ",
      "OIYIrmIAAAAJ",
      "ZWjmZDcAAAAJ",
      "6c6gvqAAAAAJ"
    ],
    "citation_count": 5
  },
  {
    "arxiv_id": "2412.13610",
    "title": "Faster and Stronger: When ANN-SNN Conversion Meets Parallel Spiking Calculation",
    "year": 2024,
    "published": "2024-12-18T08:37:13Z",
    "categories": [
      "cs.NE",
      "cs.AI",
      "cs.CV"
    ],
    "abstract": "Spiking Neural Network (SNN), as a brain-inspired and energy-efficient network, is currently facing the pivotal challenge of exploring a suitable and efficient learning framework. The predominant training methodologies, namely Spatial-Temporal Back-propagation (STBP) and ANN-SNN Conversion, are encumbered by substantial training overhead or pronounced inference latency, which impedes the advancement of SNNs in scaling to larger networks and navigating intricate application domains. In this work,",
    "arxiv_url": "https://arxiv.org/abs/2412.13610v2",
    "pdf_url": "https://arxiv.org/pdf/2412.13610v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2412.13610",
    "arxiv_authors": [
      "Zecheng Hao",
      "Qichao Ma",
      "Kang Chen",
      "Yi Zhang",
      "Zhaofei Yu",
      "Tiejun Huang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Faster+and+Stronger%3A+When+ANN-SNN+Conversion+Meets+Parallel+Spiking+Calculation+Zecheng+Hao+Qichao+Ma+Kang+Chen+Yi+Zhang+Zhaofei+Yu",
    "gs_search_success": true,
    "gs_authors": [
      "mX2poY0AAAAJ",
      "qaUgD50AAAAJ",
      "txTkX7YAAAAJ",
      "knvEK4AAAAAJ",
      "F5feBP4AAAAJ"
    ],
    "citation_count": null
  },
  {
    "arxiv_id": "2303.11831",
    "title": "CLADE: Cycle Loss Augmented Degradation Enhancement for Unpaired Super-Resolution of Anisotropic Medical Images",
    "year": 2023,
    "published": "2023-03-21T13:19:51Z",
    "categories": [
      "cs.CV",
      "cs.LG",
      "eess.IV",
      "physics.med-ph"
    ],
    "abstract": "Three-dimensional (3D) imaging is popular in medical applications, however, anisotropic 3D volumes with thick, low-spatial-resolution slices are often acquired to reduce scan times. Deep learning (DL) offers a solution to recover high-resolution features through super-resolution reconstruction (SRR). Unfortunately, paired training data is unavailable in many 3D medical applications and therefore we propose a novel unpaired approach; CLADE (Cycle Loss Augmented Degradation Enhancement). CLADE use",
    "arxiv_url": "https://arxiv.org/abs/2303.11831v3",
    "pdf_url": "https://arxiv.org/pdf/2303.11831v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2303.11831",
    "arxiv_authors": [
      "Michele Pascale",
      "Vivek Muthurangu",
      "Javier Montalt Tordera",
      "Heather E Fitzke",
      "Gauraang Bhatnagar",
      "Stuart Taylor",
      "Jennifer Steeden"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=CLADE%3A+Cycle+Loss+Augmented+Degradation+Enhancement+for+Unpaired+Super-Resolution+of+Anisotropic+Medical+Images+Michele+Pascale+Vivek+Muthurangu+Javier+Montalt+Tordera+Heather+E+Fitzke+Gauraang+Bhatnagar",
    "gs_search_success": true,
    "gs_authors": [
      "0dxDjuIAAAAJ",
      "hEfovLsAAAAJ",
      "GRHqMf0AAAAJ",
      "LXpgKbMAAAAJ",
      "rXjDFPYAAAAJ"
    ],
    "citation_count": 1
  },
  {
    "arxiv_id": "2303.09068",
    "title": "Vortex Feature Positioning: Bridging Tabular IIoT Data and Image-Based Deep Learning",
    "year": 2023,
    "published": "2023-03-16T04:02:17Z",
    "categories": [
      "cs.CV",
      "cs.LG",
      "eess.IV"
    ],
    "abstract": "Tabular data from IIoT devices are typically analyzed using decision tree-based machine learning techniques, which struggle with high-dimensional and numeric data. To overcome these limitations, techniques converting tabular data into images have been developed, leveraging the strengths of image-based deep learning approaches such as Convolutional Neural Networks. These methods cluster similar features into distinct image areas with fixed sizes, regardless of the number of features, resembling a",
    "arxiv_url": "https://arxiv.org/abs/2303.09068v2",
    "pdf_url": "https://arxiv.org/pdf/2303.09068v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2303.09068",
    "arxiv_authors": [
      "Jong-Ik Park",
      "Sihoon Seong",
      "JunKyu Lee",
      "Cheol-Ho Hong"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Vortex+Feature+Positioning%3A+Bridging+Tabular+IIoT+Data+and+Image-Based+Deep+Learning+Jong-Ik+Park+Sihoon+Seong+JunKyu+Lee+Cheol-Ho+Hong",
    "gs_search_success": true,
    "gs_authors": [
      "r0505CgAAAAJ",
      "4kco1LYAAAAJ",
      "iChrFn8AAAAJ",
      "lJtZwsMAAAAJ"
    ],
    "citation_count": 1
  },
  {
    "arxiv_id": "2411.17606",
    "title": "HyperSeg: Towards Universal Visual Segmentation with Large Language Model",
    "year": 2024,
    "published": "2024-11-26T17:18:20Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "This paper aims to address universal segmentation for image and video perception with the strong reasoning ability empowered by Visual Large Language Models (VLLMs). Despite significant progress in current unified segmentation methods, limitations in adaptation to both image and video scenarios, as well as the complex reasoning segmentation, make it difficult for them to handle various challenging instructions and achieve an accurate understanding of fine-grained vision-language correlations. We",
    "arxiv_url": "https://arxiv.org/abs/2411.17606v2",
    "pdf_url": "https://arxiv.org/pdf/2411.17606v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2411.17606",
    "arxiv_authors": [
      "Cong Wei",
      "Yujie Zhong",
      "Haoxian Tan",
      "Yong Liu",
      "Zheng Zhao",
      "Jie Hu",
      "Yujiu Yang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=HyperSeg%3A+Towards+Universal+Visual+Segmentation+with+Large+Language+Model+Cong+Wei+Yujie+Zhong+Haoxian+Tan+Yong+Liu+Zheng+Zhao",
    "gs_search_success": true,
    "gs_authors": [
      "9rY7WqAAAAAJ",
      "4gH3sxsAAAAJ",
      "DAJdHnkAAAAJ",
      "i9keb3IAAAAJ"
    ],
    "citation_count": 14
  },
  {
    "arxiv_id": "2303.04346",
    "title": "Semi-Supervised 2D Human Pose Estimation Driven by Position Inconsistency Pseudo Label Correction Module",
    "year": 2023,
    "published": "2023-03-08T02:57:05Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "In this paper, we delve into semi-supervised 2D human pose estimation. The previous method ignored two problems: (i) When conducting interactive training between large model and lightweight model, the pseudo label of lightweight model will be used to guide large models. (ii) The negative impact of noise pseudo labels on training. Moreover, the labels used for 2D human pose estimation are relatively complex: keypoint category and keypoint position. To solve the problems mentioned above, we propos",
    "arxiv_url": "https://arxiv.org/abs/2303.04346v1",
    "pdf_url": "https://arxiv.org/pdf/2303.04346v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2303.04346",
    "arxiv_authors": [
      "Linzhi Huang",
      "Yulong Li",
      "Hongbo Tian",
      "Yue Yang",
      "Xiangang Li",
      "Weihong Deng",
      "Jieping Ye"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Semi-Supervised+2D+Human+Pose+Estimation+Driven+by+Position+Inconsistency+Pseudo+Label+Correction+Module+Linzhi+Huang+Yulong+Li+Hongbo+Tian+Yue+Yang+Xiangang+Li",
    "gs_search_success": true,
    "gs_authors": [
      "T9AzhwcAAAAJ",
      "x8CoMv4AAAAJ",
      "hYQb9goAAAAJ",
      "oC75phkAAAAJ",
      "1rhBlUEAAAAJ"
    ],
    "citation_count": 25
  },
  {
    "arxiv_id": "2409.07307",
    "title": "Data Augmentation via Latent Diffusion for Saliency Prediction",
    "year": 2024,
    "published": "2024-09-11T14:36:24Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Saliency prediction models are constrained by the limited diversity and quantity of labeled data. Standard data augmentation techniques such as rotating and cropping alter scene composition, affecting saliency. We propose a novel data augmentation method for deep saliency prediction that edits natural images while preserving the complexity and variability of real-world scenes. Since saliency depends on high-level and low-level features, our approach involves learning both by incorporating photom",
    "arxiv_url": "https://arxiv.org/abs/2409.07307v1",
    "pdf_url": "https://arxiv.org/pdf/2409.07307v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2409.07307",
    "arxiv_authors": [
      "Bahar Aydemir",
      "Deblina Bhattacharjee",
      "Tong Zhang",
      "Mathieu Salzmann",
      "Sabine Süsstrunk"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Data+Augmentation+via+Latent+Diffusion+for+Saliency+Prediction+Bahar+Aydemir+Deblina+Bhattacharjee+Tong+Zhang+Mathieu+Salzmann+Sabine+S%C3%BCsstrunk",
    "gs_search_success": true,
    "gs_authors": [
      "LurWtuYAAAAJ",
      "n-B0jr4AAAAJ",
      "EX3OYP4AAAAJ",
      "F3YYEmMAAAAJ",
      "fWVzdOcAAAAJ"
    ],
    "citation_count": 6
  },
  {
    "arxiv_id": "2407.02004",
    "title": "SAVE: Segment Audio-Visual Easy way using Segment Anything Model",
    "year": 2024,
    "published": "2024-07-02T07:22:28Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.SD",
      "eess.AS"
    ],
    "abstract": "The primary aim of Audio-Visual Segmentation (AVS) is to precisely identify and locate auditory elements within visual scenes by accurately predicting segmentation masks at the pixel level. Achieving this involves comprehensively considering data and model aspects to address this task effectively. This study presents a lightweight approach, SAVE, which efficiently adapts the pre-trained segment anything model (SAM) to the AVS task. By incorporating an image encoder adapter into the transformer b",
    "arxiv_url": "https://arxiv.org/abs/2407.02004v2",
    "pdf_url": "https://arxiv.org/pdf/2407.02004v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2407.02004",
    "arxiv_authors": [
      "Khanh-Binh Nguyen",
      "Chae Jung Park"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=SAVE%3A+Segment+Audio-Visual+Easy+way+using+Segment+Anything+Model+Khanh-Binh+Nguyen+Chae+Jung+Park",
    "gs_search_success": true,
    "gs_authors": [
      "LoOglv4AAAAJ"
    ],
    "citation_count": 4
  },
  {
    "arxiv_id": "2312.09365",
    "title": "SAR image segmentation algorithms based on I-divergence-TV model",
    "year": 2023,
    "published": "2023-12-09T04:14:46Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "In this paper, we propose a novel variational active contour model based on I-divergence-TV model to segment Synthetic aperture radar (SAR) images with multiplicative gamma noise, which hybrides edge-based model with region-based model. The proposed model can efficiently stop the contours at weak or blurred edges, and can automatically detect the exterior and interior boundaries of images. We incorporate the global convex segmentation method and split Bregman technique into the proposed model, a",
    "arxiv_url": "https://arxiv.org/abs/2312.09365v2",
    "pdf_url": "https://arxiv.org/pdf/2312.09365v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2312.09365",
    "arxiv_authors": [
      "Guangming Liu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=SAR+image+segmentation+algorithms+based+on+I-divergence-TV+model+Guangming+Liu",
    "gs_search_success": true,
    "gs_authors": [],
    "citation_count": 2
  },
  {
    "arxiv_id": "2405.14014",
    "title": "RadarOcc: Robust 3D Occupancy Prediction with 4D Imaging Radar",
    "year": 2024,
    "published": "2024-05-22T21:48:17Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "cs.RO"
    ],
    "abstract": "3D occupancy-based perception pipeline has significantly advanced autonomous driving by capturing detailed scene descriptions and demonstrating strong generalizability across various object categories and shapes. Current methods predominantly rely on LiDAR or camera inputs for 3D occupancy prediction. These methods are susceptible to adverse weather conditions, limiting the all-weather deployment of self-driving cars. To improve perception robustness, we leverage the recent advances in automotiv",
    "arxiv_url": "https://arxiv.org/abs/2405.14014v4",
    "pdf_url": "https://arxiv.org/pdf/2405.14014v4",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2405.14014",
    "arxiv_authors": [
      "Fangqiang Ding",
      "Xiangyu Wen",
      "Yunzhou Zhu",
      "Yiming Li",
      "Chris Xiaoxuan Lu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=RadarOcc%3A+Robust+3D+Occupancy+Prediction+with+4D+Imaging+Radar+Fangqiang+Ding+Xiangyu+Wen+Yunzhou+Zhu+Yiming+Li+Chris+Xiaoxuan+Lu",
    "gs_search_success": true,
    "gs_authors": [
      "i_aajNoAAAAJ",
      "6SVBFSwAAAAJ",
      "Ja8dgh8AAAAJ",
      "idu78-EAAAAJ",
      "WxgdNyAAAAAJ"
    ],
    "citation_count": 32
  },
  {
    "arxiv_id": "2303.13913",
    "title": "GarmentTracking: Category-Level Garment Pose Tracking",
    "year": 2023,
    "published": "2023-03-24T10:59:17Z",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "abstract": "Garments are important to humans. A visual system that can estimate and track the complete garment pose can be useful for many downstream tasks and real-world applications. In this work, we present a complete package to address the category-level garment pose tracking task: (1) A recording system VR-Garment, with which users can manipulate virtual garment models in simulation through a VR interface. (2) A large-scale dataset VR-Folding, with complex garment pose configurations in manipulation li",
    "arxiv_url": "https://arxiv.org/abs/2303.13913v2",
    "pdf_url": "https://arxiv.org/pdf/2303.13913v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2303.13913",
    "arxiv_authors": [
      "Han Xue",
      "Wenqiang Xu",
      "Jieyi Zhang",
      "Tutian Tang",
      "Yutong Li",
      "Wenxin Du",
      "Ruolin Ye",
      "Cewu Lu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=GarmentTracking%3A+Category-Level+Garment+Pose+Tracking+Han+Xue+Wenqiang+Xu+Jieyi+Zhang+Tutian+Tang+Yutong+Li",
    "gs_search_success": true,
    "gs_authors": [
      "kQMFnUkAAAAJ",
      "fCy8NOUAAAAJ",
      "PdzO-4YAAAAJ",
      "xyanDrMAAAAJ",
      "QZVQEWAAAAAJ",
      "KSv942gAAAAJ",
      "j861KLoAAAAJ"
    ],
    "citation_count": 20
  },
  {
    "arxiv_id": "2402.18213",
    "title": "Multi-objective Differentiable Neural Architecture Search",
    "year": 2024,
    "published": "2024-02-28T10:09:04Z",
    "categories": [
      "cs.LG",
      "cs.CV",
      "stat.ML"
    ],
    "abstract": "Pareto front profiling in multi-objective optimization (MOO), i.e., finding a diverse set of Pareto optimal solutions, is challenging, especially with expensive objectives that require training a neural network. Typically, in MOO for neural architecture search (NAS), we aim to balance performance and hardware metrics across devices. Prior NAS approaches simplify this task by incorporating hardware constraints into the objective function, but profiling the Pareto front necessitates a computationa",
    "arxiv_url": "https://arxiv.org/abs/2402.18213v3",
    "pdf_url": "https://arxiv.org/pdf/2402.18213v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2402.18213",
    "arxiv_authors": [
      "Rhea Sanjay Sukthanker",
      "Arber Zela",
      "Benedikt Staffler",
      "Samuel Dooley",
      "Josif Grabocka",
      "Frank Hutter"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Multi-objective+Differentiable+Neural+Architecture+Search+Rhea+Sanjay+Sukthanker+Arber+Zela+Benedikt+Staffler+Samuel+Dooley+Josif+Grabocka",
    "gs_search_success": true,
    "gs_authors": [
      "hD_6YioAAAAJ",
      "OsamqmMAAAAJ",
      "kByZ7WYAAAAJ",
      "YUrxwrkAAAAJ",
      "KRy27XcAAAAJ",
      "BNZlRMsAAAAJ"
    ],
    "citation_count": 10
  },
  {
    "arxiv_id": "2402.05917",
    "title": "Point-VOS: Pointing Up Video Object Segmentation",
    "year": 2024,
    "published": "2024-02-08T18:52:23Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Current state-of-the-art Video Object Segmentation (VOS) methods rely on dense per-object mask annotations both during training and testing. This requires time-consuming and costly video annotation mechanisms. We propose a novel Point-VOS task with a spatio-temporally sparse point-wise annotation scheme that substantially reduces the annotation effort. We apply our annotation scheme to two large-scale video datasets with text descriptions and annotate over 19M points across 133K objects in 32K v",
    "arxiv_url": "https://arxiv.org/abs/2402.05917v2",
    "pdf_url": "https://arxiv.org/pdf/2402.05917v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2402.05917",
    "arxiv_authors": [
      "Idil Esen Zulfikar",
      "Sabarinath Mahadevan",
      "Paul Voigtlaender",
      "Bastian Leibe"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Point-VOS%3A+Pointing+Up+Video+Object+Segmentation+Idil+Esen+Zulfikar+Sabarinath+Mahadevan+Paul+Voigtlaender+Bastian+Leibe",
    "gs_search_success": true,
    "gs_authors": [
      "MqI866QAAAAJ",
      "taUv_MUAAAAJ",
      "89vcmSoAAAAJ"
    ],
    "citation_count": 3
  },
  {
    "arxiv_id": "2405.01230",
    "title": "Evaluation of Video-Based rPPG in Challenging Environments: Artifact Mitigation and Network Resilience",
    "year": 2024,
    "published": "2024-05-02T12:21:51Z",
    "categories": [
      "cs.CV",
      "eess.SP"
    ],
    "abstract": "Video-based remote photoplethysmography (rPPG) has emerged as a promising technology for non-contact vital sign monitoring, especially under controlled conditions. However, the accurate measurement of vital signs in real-world scenarios faces several challenges, including artifacts induced by videocodecs, low-light noise, degradation, low dynamic range, occlusions, and hardware and network constraints. In this article, we systematically investigate comprehensive investigate these issues, measuri",
    "arxiv_url": "https://arxiv.org/abs/2405.01230v1",
    "pdf_url": "https://arxiv.org/pdf/2405.01230v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2405.01230",
    "arxiv_authors": [
      "Nhi Nguyen",
      "Le Nguyen",
      "Honghan Li",
      "Miguel Bordallo López",
      "Constantino Álvarez Casado"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Evaluation+of+Video-Based+rPPG+in+Challenging+Environments%3A+Artifact+Mitigation+and+Network+Resilience+Nhi+Nguyen+Le+Nguyen+Honghan+Li+Miguel+Bordallo+L%C3%B3pez+Constantino+%C3%81lvarez+Casado",
    "gs_search_success": true,
    "gs_authors": [
      "tow0oVoAAAAJ",
      "vLYIwqUAAAAJ",
      "7_t4zCIAAAAJ",
      "rSu541oAAAAJ",
      "j_lXNWMAAAAJ"
    ],
    "citation_count": 13
  },
  {
    "arxiv_id": "2501.19047",
    "title": "Understanding Model Calibration -- A gentle introduction and visual exploration of calibration and the expected calibration error (ECE)",
    "year": 2025,
    "published": "2025-01-31T11:18:45Z",
    "categories": [
      "stat.ME",
      "cs.AI",
      "cs.CV",
      "cs.LG",
      "stat.ML"
    ],
    "abstract": "To be considered reliable, a model must be calibrated so that its confidence in each decision closely reflects its true outcome. In this blogpost we'll take a look at the most commonly used definition for calibration and then dive into a frequently used evaluation measure for model calibration. We'll then cover some of the drawbacks of this measure and how these surfaced the need for additional notions of calibration, which require their own new evaluation measures. This post is not intended to ",
    "arxiv_url": "https://arxiv.org/abs/2501.19047v5",
    "pdf_url": "https://arxiv.org/pdf/2501.19047v5",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2501.19047",
    "arxiv_authors": [
      "Maja Pavlovic"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Understanding+Model+Calibration+--+A+gentle+introduction+and+visual+exploration+of+calibration+and+the+expected+calibration+error+%28ECE%29+Maja+Pavlovic",
    "gs_search_success": true,
    "gs_authors": [
      "NEa-wIIAAAAJ"
    ],
    "citation_count": 8
  },
  {
    "arxiv_id": "2406.02559",
    "title": "ShadowRefiner: Towards Mask-free Shadow Removal via Fast Fourier Transformer",
    "year": 2024,
    "published": "2024-04-18T03:53:33Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Shadow-affected images often exhibit pronounced spatial discrepancies in color and illumination, consequently degrading various vision applications including object detection and segmentation systems. To effectively eliminate shadows in real-world images while preserving intricate details and producing visually compelling outcomes, we introduce a mask-free Shadow Removal and Refinement network (ShadowRefiner) via Fast Fourier Transformer. Specifically, the Shadow Removal module in our method aim",
    "arxiv_url": "https://arxiv.org/abs/2406.02559v2",
    "pdf_url": "https://arxiv.org/pdf/2406.02559v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2406.02559",
    "arxiv_authors": [
      "Wei Dong",
      "Han Zhou",
      "Yuqiong Tian",
      "Jingke Sun",
      "Xiaohong Liu",
      "Guangtao Zhai",
      "Jun Chen"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=ShadowRefiner%3A+Towards+Mask-free+Shadow+Removal+via+Fast+Fourier+Transformer+Wei+Dong+Han+Zhou+Yuqiong+Tian+Jingke+Sun+Xiaohong+Liu",
    "gs_search_success": true,
    "gs_authors": [
      "E6zbSYgAAAAJ",
      "XI79Mw0AAAAJ",
      "dlcOwwkAAAAJ",
      "Tq2hoMQAAAAJ",
      "tkTl3BMAAAAJ"
    ],
    "citation_count": 26
  },
  {
    "arxiv_id": "2302.12393",
    "title": "Blind Omnidirectional Image Quality Assessment: Integrating Local Statistics and Global Semantics",
    "year": 2023,
    "published": "2023-02-24T01:47:13Z",
    "categories": [
      "cs.MM",
      "cs.CV"
    ],
    "abstract": "Omnidirectional image quality assessment (OIQA) aims to predict the perceptual quality of omnidirectional images that cover the whole 180$\\times$360$^{\\circ}$ viewing range of the visual environment. Here we propose a blind/no-reference OIQA method named S$^2$ that bridges the gap between low-level statistics and high-level semantics of omnidirectional images. Specifically, statistic and semantic features are extracted in separate paths from multiple local viewports and the hallucinated global o",
    "arxiv_url": "https://arxiv.org/abs/2302.12393v1",
    "pdf_url": "https://arxiv.org/pdf/2302.12393v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2302.12393",
    "arxiv_authors": [
      "Wei Zhou",
      "Zhou Wang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Blind+Omnidirectional+Image+Quality+Assessment%3A+Integrating+Local+Statistics+and+Global+Semantics+Wei+Zhou+Zhou+Wang",
    "gs_search_success": true,
    "gs_authors": [
      "O0PjM4QAAAAJ",
      "D8SDfHQAAAAJ"
    ],
    "citation_count": 15
  },
  {
    "arxiv_id": "2503.09361",
    "title": "Deep Learning for Climate Action: Computer Vision Analysis of Visual Narratives on X",
    "year": 2025,
    "published": "2025-03-12T13:03:49Z",
    "categories": [
      "cs.CV",
      "cs.SI"
    ],
    "abstract": "Climate change is one of the most pressing challenges of the 21st century, sparking widespread discourse across social media platforms. Activists, policymakers, and researchers seek to understand public sentiment and narratives while access to social media data has become increasingly restricted in the post-API era. In this study, we analyze a dataset of climate change-related tweets from X (formerly Twitter) shared in 2019, containing 730k tweets along with the shared images. Our approach integ",
    "arxiv_url": "https://arxiv.org/abs/2503.09361v1",
    "pdf_url": "https://arxiv.org/pdf/2503.09361v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2503.09361",
    "arxiv_authors": [
      "Katharina Prasse",
      "Marcel Kleinmann",
      "Inken Adam",
      "Kerstin Beckersjuergen",
      "Andreas Edte",
      "Jona Frroku",
      "Timotheus Gumpp",
      "Steffen Jung",
      "Isaac Bravo",
      "Stefanie Walter",
      "Margret Keuper"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Deep+Learning+for+Climate+Action%3A+Computer+Vision+Analysis+of+Visual+Narratives+on+X+Katharina+Prasse+Marcel+Kleinmann+Inken+Adam+Kerstin+Beckersjuergen+Andreas+Edte",
    "gs_search_success": true,
    "gs_authors": [
      "PaVhYr4AAAAJ",
      "x5ovaJcAAAAJ"
    ],
    "citation_count": null
  },
  {
    "arxiv_id": "2307.00040",
    "title": "DisCo: Disentangled Control for Realistic Human Dance Generation",
    "year": 2023,
    "published": "2023-06-30T17:37:48Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Generative AI has made significant strides in computer vision, particularly in text-driven image/video synthesis (T2I/T2V). Despite the notable advancements, it remains challenging in human-centric content synthesis such as realistic dance generation. Current methodologies, primarily tailored for human motion transfer, encounter difficulties when confronted with real-world dance scenarios (e.g., social media dance), which require to generalize across a wide spectrum of poses and intricate human ",
    "arxiv_url": "https://arxiv.org/abs/2307.00040v3",
    "pdf_url": "https://arxiv.org/pdf/2307.00040v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2307.00040",
    "arxiv_authors": [
      "Tan Wang",
      "Linjie Li",
      "Kevin Lin",
      "Yuanhao Zhai",
      "Chung-Ching Lin",
      "Zhengyuan Yang",
      "Hanwang Zhang",
      "Zicheng Liu",
      "Lijuan Wang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=DisCo%3A+Disentangled+Control+for+Realistic+Human+Dance+Generation+Tan+Wang+Linjie+Li+Kevin+Lin+Yuanhao+Zhai+Chung-Ching+Lin",
    "gs_search_success": true,
    "gs_authors": [
      "YG0DFyYAAAAJ",
      "LKSy1kwAAAAJ",
      "rP02ve8AAAAJ",
      "bkALdvsAAAAJ",
      "legkbM0AAAAJ",
      "WR875gYAAAAJ",
      "wFduC9EAAAAJ",
      "PKgt2LgAAAAJ",
      "cDcWXuIAAAAJ"
    ],
    "citation_count": 149
  },
  {
    "arxiv_id": "2409.13251",
    "title": "T2M-X: Learning Expressive Text-to-Motion Generation from Partially Annotated Data",
    "year": 2024,
    "published": "2024-09-20T06:20:00Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "The generation of humanoid animation from text prompts can profoundly impact animation production and AR/VR experiences. However, existing methods only generate body motion data, excluding facial expressions and hand movements. This limitation, primarily due to a lack of a comprehensive whole-body motion dataset, inhibits their readiness for production use. Recent attempts to create such a dataset have resulted in either motion inconsistency among different body parts in the artificially augment",
    "arxiv_url": "https://arxiv.org/abs/2409.13251v1",
    "pdf_url": "https://arxiv.org/pdf/2409.13251v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2409.13251",
    "arxiv_authors": [
      "Mingdian Liu",
      "Yilin Liu",
      "Gurunandan Krishnan",
      "Karl S Bayer",
      "Bing Zhou"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=T2M-X%3A+Learning+Expressive+Text-to-Motion+Generation+from+Partially+Annotated+Data+Mingdian+Liu+Yilin+Liu+Gurunandan+Krishnan+Karl+S+Bayer+Bing+Zhou",
    "gs_search_success": false,
    "gs_authors": [],
    "citation_count": null,
    "error_type": "fetch_failed"
  },
  {
    "arxiv_id": "2505.15545",
    "title": "Multi-View Projection for Unsupervised Domain Adaptation in 3D Semantic Segmentation",
    "year": 2025,
    "published": "2025-05-21T14:08:42Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "3D semantic segmentation is essential for autonomous driving and road infrastructure analysis, but state-of-the-art 3D models suffer from severe domain shift when applied across datasets. We propose a multi-view projection framework for unsupervised domain adaptation (UDA). Our method aligns LiDAR scans into coherent 3D scenes and renders them from multiple virtual camera poses to generate large-scale synthetic 2D datasets (PC2D) in various modalities. An ensemble of 2D segmentation models is tr",
    "arxiv_url": "https://arxiv.org/abs/2505.15545v2",
    "pdf_url": "https://arxiv.org/pdf/2505.15545v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2505.15545",
    "arxiv_authors": [
      "Andrew Caunes",
      "Thierry Chateau",
      "Vincent Fremont"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Multi-View+Projection+for+Unsupervised+Domain+Adaptation+in+3D+Semantic+Segmentation+Andrew+Caunes+Thierry+Chateau+Vincent+Fremont",
    "gs_search_success": true,
    "gs_authors": [
      "dq-S95wAAAAJ",
      "DW-OBZYAAAAJ",
      "0BzH_AEAAAAJ"
    ],
    "citation_count": 2
  },
  {
    "arxiv_id": "2411.14723",
    "title": "Effective SAM Combination for Open-Vocabulary Semantic Segmentation",
    "year": 2024,
    "published": "2024-11-22T04:36:12Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Open-vocabulary semantic segmentation aims to assign pixel-level labels to images across an unlimited range of classes. Traditional methods address this by sequentially connecting a powerful mask proposal generator, such as the Segment Anything Model (SAM), with a pre-trained vision-language model like CLIP. But these two-stage approaches often suffer from high computational costs, memory inefficiencies. In this paper, we propose ESC-Net, a novel one-stage open-vocabulary segmentation model that",
    "arxiv_url": "https://arxiv.org/abs/2411.14723v2",
    "pdf_url": "https://arxiv.org/pdf/2411.14723v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2411.14723",
    "arxiv_authors": [
      "Minhyeok Lee",
      "Suhwan Cho",
      "Jungho Lee",
      "Sunghun Yang",
      "Heeseung Choi",
      "Ig-Jae Kim",
      "Sangyoun Lee"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Effective+SAM+Combination+for+Open-Vocabulary+Semantic+Segmentation+Minhyeok+Lee+Suhwan+Cho+Jungho+Lee+Sunghun+Yang+Heeseung+Choi",
    "gs_search_success": true,
    "gs_authors": [
      "GbgofCEAAAAJ",
      "NAj3cTcAAAAJ",
      "pTueCQ8AAAAJ",
      "WGchT7cAAAAJ",
      "H647yqgAAAAJ"
    ],
    "citation_count": 6
  },
  {
    "arxiv_id": "2412.15341",
    "title": "Efficient Fine-Tuning and Concept Suppression for Pruned Diffusion Models",
    "year": 2024,
    "published": "2024-12-19T19:13:18Z",
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "abstract": "Recent advances in diffusion generative models have yielded remarkable progress. While the quality of generated content continues to improve, these models have grown considerably in size and complexity. This increasing computational burden poses significant challenges, particularly in resource-constrained deployment scenarios such as mobile devices. The combination of model pruning and knowledge distillation has emerged as a promising solution to reduce computational demands while preserving gen",
    "arxiv_url": "https://arxiv.org/abs/2412.15341v2",
    "pdf_url": "https://arxiv.org/pdf/2412.15341v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2412.15341",
    "arxiv_authors": [
      "Reza Shirkavand",
      "Peiran Yu",
      "Shangqian Gao",
      "Gowthami Somepalli",
      "Tom Goldstein",
      "Heng Huang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Efficient+Fine-Tuning+and+Concept+Suppression+for+Pruned+Diffusion+Models+Reza+Shirkavand+Peiran+Yu+Shangqian+Gao+Gowthami+Somepalli+Tom+Goldstein",
    "gs_search_success": true,
    "gs_authors": [
      "4OqLaDwAAAAJ",
      "uS4nCAMAAAAJ",
      "KmSuVtgAAAAJ",
      "9mNI83oAAAAJ",
      "T2ezBDsAAAAJ",
      "SXJ4R24AAAAJ"
    ],
    "citation_count": 5
  },
  {
    "arxiv_id": "2306.16544",
    "title": "Multi-Scale Deformable Alignment and Content-Adaptive Inference for Flexible-Rate Bi-Directional Video Compression",
    "year": 2023,
    "published": "2023-06-28T20:32:16Z",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "abstract": "The lack of ability to adapt the motion compensation model to video content is an important limitation of current end-to-end learned video compression models. This paper advances the state-of-the-art by proposing an adaptive motion-compensation model for end-to-end rate-distortion optimized hierarchical bi-directional video compression. In particular, we propose two novelties: i) a multi-scale deformable alignment scheme at the feature level combined with multi-scale conditional coding, ii) moti",
    "arxiv_url": "https://arxiv.org/abs/2306.16544v1",
    "pdf_url": "https://arxiv.org/pdf/2306.16544v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2306.16544",
    "arxiv_authors": [
      "M. Akın Yılmaz",
      "O. Ugur Ulas",
      "A. Murat Tekalp"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Multi-Scale+Deformable+Alignment+and+Content-Adaptive+Inference+for+Flexible-Rate+Bi-Directional+Video+Compression+M.+Ak%C4%B1n+Y%C4%B1lmaz+O.+Ugur+Ulas+A.+Murat+Tekalp",
    "gs_search_success": true,
    "gs_authors": [
      "kHZOZzkAAAAJ",
      "GzwcDjUAAAAJ",
      "gRgNm7oAAAAJ"
    ],
    "citation_count": 13
  },
  {
    "arxiv_id": "2505.15814",
    "title": "A Taxonomy of Structure from Motion Methods",
    "year": 2025,
    "published": "2025-05-21T17:59:44Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Structure from Motion (SfM) refers to the problem of recovering both structure (i.e., 3D coordinates of points in the scene) and motion (i.e., camera matrices) starting from point correspondences in multiple images. It has attracted significant attention over the years, counting practical reconstruction pipelines as well as theoretical results. This paper is conceived as a conceptual review of SfM methods, which are grouped into three main categories, according to which part of the problem - bet",
    "arxiv_url": "https://arxiv.org/abs/2505.15814v1",
    "pdf_url": "https://arxiv.org/pdf/2505.15814v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2505.15814",
    "arxiv_authors": [
      "Federica Arrigoni"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=A+Taxonomy+of+Structure+from+Motion+Methods+Federica+Arrigoni",
    "gs_search_success": true,
    "gs_authors": [
      "bzBtqfQAAAAJ"
    ],
    "citation_count": 1
  },
  {
    "arxiv_id": "2504.19115",
    "title": "Towards Latency-Aware 3D Streaming Perception for Autonomous Driving",
    "year": 2025,
    "published": "2025-04-27T05:49:52Z",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "abstract": "Although existing 3D perception algorithms have demonstrated significant improvements in performance, their deployment on edge devices continues to encounter critical challenges due to substantial runtime latency. We propose a new benchmark tailored for online evaluation by considering runtime latency. Based on the benchmark, we build a Latency-Aware 3D Streaming Perception (LASP) framework that addresses the latency issue through two primary components: 1) latency-aware history integration, whi",
    "arxiv_url": "https://arxiv.org/abs/2504.19115v1",
    "pdf_url": "https://arxiv.org/pdf/2504.19115v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2504.19115",
    "arxiv_authors": [
      "Jiaqi Peng",
      "Tai Wang",
      "Jiangmiao Pang",
      "Yuan Shen"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Towards+Latency-Aware+3D+Streaming+Perception+for+Autonomous+Driving+Jiaqi+Peng+Tai+Wang+Jiangmiao+Pang+Yuan+Shen",
    "gs_search_success": true,
    "gs_authors": [
      "5Mtc0ZoAAAAJ",
      "JmbbZWIAAAAJ",
      "ssSfKpAAAAAJ"
    ],
    "citation_count": null
  },
  {
    "arxiv_id": "2311.04588",
    "title": "Army of Thieves: Enhancing Black-Box Model Extraction via Ensemble based sample selection",
    "year": 2023,
    "published": "2023-11-08T10:31:29Z",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR",
      "cs.CV"
    ],
    "abstract": "Machine Learning (ML) models become vulnerable to Model Stealing Attacks (MSA) when they are deployed as a service. In such attacks, the deployed model is queried repeatedly to build a labelled dataset. This dataset allows the attacker to train a thief model that mimics the original model. To maximize query efficiency, the attacker has to select the most informative subset of data points from the pool of available data. Existing attack strategies utilize approaches like Active Learning and Semi-",
    "arxiv_url": "https://arxiv.org/abs/2311.04588v1",
    "pdf_url": "https://arxiv.org/pdf/2311.04588v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2311.04588",
    "arxiv_authors": [
      "Akshit Jindal",
      "Vikram Goyal",
      "Saket Anand",
      "Chetan Arora"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Army+of+Thieves%3A+Enhancing+Black-Box+Model+Extraction+via+Ensemble+based+sample+selection+Akshit+Jindal+Vikram+Goyal+Saket+Anand+Chetan+Arora",
    "gs_search_success": true,
    "gs_authors": [
      "c-DznCMAAAAJ",
      "YmYvVEQAAAAJ",
      "YJaVmSwAAAAJ",
      "Q8cTLNMAAAAJ"
    ],
    "citation_count": 10
  },
  {
    "arxiv_id": "2504.16276",
    "title": "An Automated Pipeline for Few-Shot Bird Call Classification: A Case Study with the Tooth-Billed Pigeon",
    "year": 2025,
    "published": "2025-04-22T21:21:41Z",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "cs.SD"
    ],
    "abstract": "This paper presents an automated one-shot bird call classification pipeline designed for rare species absent from large publicly available classifiers like BirdNET and Perch. While these models excel at detecting common birds with abundant training data, they lack options for species with only 1-3 known recordings-a critical limitation for conservationists monitoring the last remaining individuals of endangered birds. To address this, we leverage the embedding space of large bird classification ",
    "arxiv_url": "https://arxiv.org/abs/2504.16276v2",
    "pdf_url": "https://arxiv.org/pdf/2504.16276v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2504.16276",
    "arxiv_authors": [
      "Abhishek Jana",
      "Moeumu Uili",
      "James Atherton",
      "Mark O'Brien",
      "Joe Wood",
      "Leandra Brickson"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=An+Automated+Pipeline+for+Few-Shot+Bird+Call+Classification%3A+A+Case+Study+with+the+Tooth-Billed+Pigeon+Abhishek+Jana+Moeumu+Uili+James+Atherton+Mark+O%27Brien+Joe+Wood",
    "gs_search_success": true,
    "gs_authors": [
      "DCN3neAAAAAJ",
      "IxEIY0gAAAAJ"
    ],
    "citation_count": null
  },
  {
    "arxiv_id": "2412.19062",
    "title": "DAPoinTr: Domain Adaptive Point Transformer for Point Cloud Completion",
    "year": 2024,
    "published": "2024-12-26T05:16:54Z",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "Point Transformers (PoinTr) have shown great potential in point cloud completion recently. Nevertheless, effective domain adaptation that improves transferability toward target domains remains unexplored. In this paper, we delve into this topic and empirically discover that direct feature alignment on point Transformer's CNN backbone only brings limited improvements since it cannot guarantee sequence-wise domain-invariant features in the Transformer. To this end, we propose a pioneering Domain A",
    "arxiv_url": "https://arxiv.org/abs/2412.19062v1",
    "pdf_url": "https://arxiv.org/pdf/2412.19062v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2412.19062",
    "arxiv_authors": [
      "Yinghui Li",
      "Qianyu Zhou",
      "Jingyu Gong",
      "Ye Zhu",
      "Richard Dazeley",
      "Xinkui Zhao",
      "Xuequan Lu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=DAPoinTr%3A+Domain+Adaptive+Point+Transformer+for+Point+Cloud+Completion+Yinghui+Li+Qianyu+Zhou+Jingyu+Gong+Ye+Zhu+Richard+Dazeley",
    "gs_search_success": true,
    "gs_authors": [
      "QxRHA48AAAAJ",
      "KHg04fkAAAAJ",
      "Y_kHeQwAAAAJ",
      "w4YcdsoAAAAJ",
      "cG_WXywAAAAJ",
      "Tp8Sx6AAAAAJ"
    ],
    "citation_count": 5
  },
  {
    "arxiv_id": "2503.05051",
    "title": "Accelerated Patient-specific Non-Cartesian MRI Reconstruction using Implicit Neural Representations",
    "year": 2025,
    "published": "2025-03-07T00:05:43Z",
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "abstract": "The scanning time for a fully sampled MRI can be undesirably lengthy. Compressed sensing has been developed to minimize image artifacts in accelerated scans, but the required iterative reconstruction is computationally complex and difficult to generalize on new cases. Image-domain-based deep learning methods (e.g., convolutional neural networks) emerged as a faster alternative but face challenges in modeling continuous k-space, a problem amplified with non-Cartesian sampling commonly used in acc",
    "arxiv_url": "https://arxiv.org/abs/2503.05051v1",
    "pdf_url": "https://arxiv.org/pdf/2503.05051v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2503.05051",
    "arxiv_authors": [
      "Di Xu",
      "Hengjie Liu",
      "Xin Miao",
      "Daniel O'Connor",
      "Jessica E. Scholey",
      "Wensha Yang",
      "Mary Feng",
      "Michael Ohliger",
      "Hui Lin",
      "Dan Ruan",
      "Yang Yang",
      "Ke Sheng"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Accelerated+Patient-specific+Non-Cartesian+MRI+Reconstruction+using+Implicit+Neural+Representations+Di+Xu+Hengjie+Liu+Xin+Miao+Daniel+O%27Connor+Jessica+E.+Scholey",
    "gs_search_success": true,
    "gs_authors": [
      "LbXcdmsAAAAJ",
      "2jrPdDgAAAAJ",
      "gwSqxwEAAAAJ",
      "gK_sUR8AAAAJ"
    ],
    "citation_count": null
  },
  {
    "arxiv_id": "2305.15975",
    "title": "Triplet Knowledge Distillation",
    "year": 2023,
    "published": "2023-05-25T12:12:31Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "In Knowledge Distillation, the teacher is generally much larger than the student, making the solution of the teacher likely to be difficult for the student to learn. To ease the mimicking difficulty, we introduce a triplet knowledge distillation mechanism named TriKD. Besides teacher and student, TriKD employs a third role called anchor model. Before distillation begins, the pre-trained anchor model delimits a subspace within the full solution space of the target problem. Solutions within the su",
    "arxiv_url": "https://arxiv.org/abs/2305.15975v1",
    "pdf_url": "https://arxiv.org/pdf/2305.15975v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2305.15975",
    "arxiv_authors": [
      "Xijun Wang",
      "Dongyang Liu",
      "Meina Kan",
      "Chunrui Han",
      "Zhongqin Wu",
      "Shiguang Shan"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Triplet+Knowledge+Distillation+Xijun+Wang+Dongyang+Liu+Meina+Kan+Chunrui+Han+Zhongqin+Wu",
    "gs_search_success": true,
    "gs_authors": [
      "4AKCKKEAAAAJ",
      "KlqOhiUAAAAJ",
      "pHnJPmcAAAAJ",
      "VxQGEOcAAAAJ",
      "D6tWz44AAAAJ",
      "Vkzd7MIAAAAJ"
    ],
    "citation_count": 3
  },
  {
    "arxiv_id": "2503.01085",
    "title": "Identity documents recognition and detection using semantic segmentation with convolutional neural network",
    "year": 2025,
    "published": "2025-03-03T01:13:28Z",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "Object recognition and detection are well-studied problems with a developed set of almost standard solutions. Identity documents recognition, classification, detection, and localization are the tasks required in a number of applications, particularly, in physical access control security systems at critical infrastructure premises. In this paper, we propose the new original architecture of a model based on an artificial convolutional neural network and semantic segmentation approach for the recog",
    "arxiv_url": "https://arxiv.org/abs/2503.01085v1",
    "pdf_url": "https://arxiv.org/pdf/2503.01085v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2503.01085",
    "arxiv_authors": [
      "Mykola Kozlenko",
      "Volodymyr Sendetskyi",
      "Oleksiy Simkiv",
      "Nazar Savchenko",
      "Andy Bosyi"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Identity+documents+recognition+and+detection+using+semantic+segmentation+with+convolutional+neural+network+Mykola+Kozlenko+Volodymyr+Sendetskyi+Oleksiy+Simkiv+Nazar+Savchenko+Andy+Bosyi",
    "gs_search_success": true,
    "gs_authors": [
      "KYT7J_8AAAAJ"
    ],
    "citation_count": 9
  },
  {
    "arxiv_id": "2504.13406",
    "title": "LangCoop: Collaborative Driving with Language",
    "year": 2025,
    "published": "2025-04-18T02:03:14Z",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CL",
      "cs.CV"
    ],
    "abstract": "Multi-agent collaboration holds great promise for enhancing the safety, reliability, and mobility of autonomous driving systems by enabling information sharing among multiple connected agents. However, existing multi-agent communication approaches are hindered by limitations of existing communication media, including high bandwidth demands, agent heterogeneity, and information loss. To address these challenges, we introduce LangCoop, a new paradigm for collaborative autonomous driving that lever",
    "arxiv_url": "https://arxiv.org/abs/2504.13406v2",
    "pdf_url": "https://arxiv.org/pdf/2504.13406v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2504.13406",
    "arxiv_authors": [
      "Xiangbo Gao",
      "Yuheng Wu",
      "Rujia Wang",
      "Chenxi Liu",
      "Yang Zhou",
      "Zhengzhong Tu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=LangCoop%3A+Collaborative+Driving+with+Language+Xiangbo+Gao+Yuheng+Wu+Rujia+Wang+Chenxi+Liu+Yang+Zhou",
    "gs_search_success": true,
    "gs_authors": [
      "0xCRTw0AAAAJ",
      "9ajdZaEAAAAJ",
      "bSpZc84AAAAJ",
      "bAhIqYwAAAAJ",
      "UFvM01gAAAAJ"
    ],
    "citation_count": 15
  },
  {
    "arxiv_id": "2404.01367",
    "title": "Bigger is not Always Better: Scaling Properties of Latent Diffusion Models",
    "year": 2024,
    "published": "2024-04-01T17:59:48Z",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "We study the scaling properties of latent diffusion models (LDMs) with an emphasis on their sampling efficiency. While improved network architecture and inference algorithms have shown to effectively boost sampling efficiency of diffusion models, the role of model size -- a critical determinant of sampling efficiency -- has not been thoroughly examined. Through empirical analysis of established text-to-image diffusion models, we conduct an in-depth investigation into how model size influences sa",
    "arxiv_url": "https://arxiv.org/abs/2404.01367v2",
    "pdf_url": "https://arxiv.org/pdf/2404.01367v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2404.01367",
    "arxiv_authors": [
      "Kangfu Mei",
      "Zhengzhong Tu",
      "Mauricio Delbracio",
      "Hossein Talebi",
      "Vishal M. Patel",
      "Peyman Milanfar"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Bigger+is+not+Always+Better%3A+Scaling+Properties+of+Latent+Diffusion+Models+Kangfu+Mei+Zhengzhong+Tu+Mauricio+Delbracio+Hossein+Talebi+Vishal+M.+Patel",
    "gs_search_success": true,
    "gs_authors": [
      "lDDm920AAAAJ",
      "9ajdZaEAAAAJ",
      "kqXlLzYAAAAJ",
      "AkEXTbIAAAAJ",
      "e_nu_TIAAAAJ",
      "iGzDl8IAAAAJ"
    ],
    "citation_count": 20
  },
  {
    "arxiv_id": "2304.09498",
    "title": "Learning Robust Visual-Semantic Embedding for Generalizable Person Re-identification",
    "year": 2023,
    "published": "2023-04-19T08:37:25Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Generalizable person re-identification (Re-ID) is a very hot research topic in machine learning and computer vision, which plays a significant role in realistic scenarios due to its various applications in public security and video surveillance. However, previous methods mainly focus on the visual representation learning, while neglect to explore the potential of semantic features during training, which easily leads to poor generalization capability when adapted to the new domain. In this paper,",
    "arxiv_url": "https://arxiv.org/abs/2304.09498v1",
    "pdf_url": "https://arxiv.org/pdf/2304.09498v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2304.09498",
    "arxiv_authors": [
      "Suncheng Xiang",
      "Jingsheng Gao",
      "Mengyuan Guan",
      "Jiacheng Ruan",
      "Chengfeng Zhou",
      "Ting Liu",
      "Dahong Qian",
      "Yuzhuo Fu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Learning+Robust+Visual-Semantic+Embedding+for+Generalizable+Person+Re-identification+Suncheng+Xiang+Jingsheng+Gao+Mengyuan+Guan+Jiacheng+Ruan+Chengfeng+Zhou",
    "gs_search_success": true,
    "gs_authors": [
      "IyrSeG8AAAAJ",
      "O4o2aQcAAAAJ",
      "QDlaRJkAAAAJ",
      "oymfXUIAAAAJ",
      "_PdbsmAAAAAJ",
      "EQEPRKAAAAAJ"
    ],
    "citation_count": 17
  },
  {
    "arxiv_id": "2402.12536",
    "title": "Designing High-Performing Networks for Multi-Scale Computer Vision",
    "year": 2024,
    "published": "2024-02-19T20:50:55Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Since the emergence of deep learning, the computer vision field has flourished with models improving at a rapid pace on more and more complex tasks. We distinguish three main ways to improve a computer vision model: (1) improving the data aspect by for example training on a large, more diverse dataset, (2) improving the training aspect by for example designing a better optimizer, and (3) improving the network architecture (or network for short). In this thesis, we chose to improve the latter, i.",
    "arxiv_url": "https://arxiv.org/abs/2402.12536v1",
    "pdf_url": "https://arxiv.org/pdf/2402.12536v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2402.12536",
    "arxiv_authors": [
      "Cédric Picron"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Designing+High-Performing+Networks+for+Multi-Scale+Computer+Vision+C%C3%A9dric+Picron",
    "gs_search_success": true,
    "gs_authors": [
      "EuFF9kUAAAAJ",
      "nzXOc4QAAAAJ",
      "AyXW9gYAAAAJ"
    ],
    "citation_count": 2
  },
  {
    "arxiv_id": "2405.17698",
    "title": "BaboonLand Dataset: Tracking Primates in the Wild and Automating Behaviour Recognition from Drone Videos",
    "year": 2024,
    "published": "2024-05-27T23:09:37Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Using drones to track multiple individuals simultaneously in their natural environment is a powerful approach for better understanding group primate behavior. Previous studies have demonstrated that it is possible to automate the classification of primate behavior from video data, but these studies have been carried out in captivity or from ground-based cameras. To understand group behavior and the self-organization of a collective, the whole troop needs to be seen at a scale where behavior can ",
    "arxiv_url": "https://arxiv.org/abs/2405.17698v3",
    "pdf_url": "https://arxiv.org/pdf/2405.17698v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2405.17698",
    "arxiv_authors": [
      "Isla Duporge",
      "Maksim Kholiavchenko",
      "Roi Harel",
      "Scott Wolf",
      "Dan Rubenstein",
      "Meg Crofoot",
      "Tanya Berger-Wolf",
      "Stephen Lee",
      "Julie Barreau",
      "Jenna Kline",
      "Michelle Ramirez",
      "Charles Stewart"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=BaboonLand+Dataset%3A+Tracking+Primates+in+the+Wild+and+Automating+Behaviour+Recognition+from+Drone+Videos+Isla+Duporge+Maksim+Kholiavchenko+Roi+Harel+Scott+Wolf+Dan+Rubenstein",
    "gs_search_success": true,
    "gs_authors": [
      "FiaKXNMAAAAJ",
      "JeflQ3IAAAAJ",
      "Hq28JM0AAAAJ",
      "iO-ruAkAAAAJ",
      "oD4QN6wAAAAJ",
      "QcA4Z_QAAAAJ",
      "oJ_KilcAAAAJ"
    ],
    "citation_count": 31
  },
  {
    "arxiv_id": "2405.14419",
    "title": "A motion-based compression algorithm for resource-constrained video camera traps",
    "year": 2024,
    "published": "2024-05-23T10:39:33Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "q-bio.QM"
    ],
    "abstract": "Field-captured video facilitates detailed studies of spatio-temporal aspects of animal locomotion, decision-making and environmental interactions including predator-prey relationships and habitat utilisation. But even though data capture is cheap with mass-produced hardware, storage, processing and transmission overheads provide a hurdle to acquisition of high resolution video from field-situated edge computing devices. Efficient compression algorithms are therefore essential if monitoring is to",
    "arxiv_url": "https://arxiv.org/abs/2405.14419v3",
    "pdf_url": "https://arxiv.org/pdf/2405.14419v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2405.14419",
    "arxiv_authors": [
      "Malika Nisal Ratnayake",
      "Lex Gallon",
      "Adel N. Toosi",
      "Alan Dorin"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=A+motion-based+compression+algorithm+for+resource-constrained+video+camera+traps+Malika+Nisal+Ratnayake+Lex+Gallon+Adel+N.+Toosi+Alan+Dorin",
    "gs_search_success": true,
    "gs_authors": [
      "akGqM2IAAAAJ",
      "0QydmJAAAAAJ",
      "qIh_I-gAAAAJ"
    ],
    "citation_count": null
  },
  {
    "arxiv_id": "2404.10838",
    "title": "Dynamic Self-adaptive Multiscale Distillation from Pre-trained Multimodal Large Model for Efficient Cross-modal Representation Learning",
    "year": 2024,
    "published": "2024-04-16T18:22:49Z",
    "categories": [
      "cs.CV",
      "cs.CL",
      "cs.MM"
    ],
    "abstract": "In recent years, pre-trained multimodal large models have attracted widespread attention due to their outstanding performance in various multimodal applications. Nonetheless, the extensive computational resources and vast datasets required for their training present significant hurdles for deployment in environments with limited computational resources. To address this challenge, we propose a novel dynamic self-adaptive multiscale distillation from pre-trained multimodal large model for efficien",
    "arxiv_url": "https://arxiv.org/abs/2404.10838v1",
    "pdf_url": "https://arxiv.org/pdf/2404.10838v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2404.10838",
    "arxiv_authors": [
      "Zhengyang Liang",
      "Meiyu Liang",
      "Wei Huang",
      "Yawen Li",
      "Zhe Xue"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Dynamic+Self-adaptive+Multiscale+Distillation+from+Pre-trained+Multimodal+Large+Model+for+Efficient+Cross-modal+Representation+Learning+Zhengyang+Liang+Meiyu+Liang+Wei+Huang+Yawen+Li+Zhe+Xue",
    "gs_search_success": true,
    "gs_authors": [
      "rQpizr0AAAAJ",
      "9IC8FBQAAAAJ",
      "h6gVF8YAAAAJ",
      "RzNY-o4AAAAJ"
    ],
    "citation_count": 4
  },
  {
    "arxiv_id": "2304.08709",
    "title": "You Only Need Two Detectors to Achieve Multi-Modal 3D Multi-Object Tracking",
    "year": 2023,
    "published": "2023-04-18T02:45:18Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "In the classical tracking-by-detection (TBD) paradigm, detection and tracking are separately and sequentially conducted, and data association must be properly performed to achieve satisfactory tracking performance. In this paper, a new end-to-end multi-object tracking framework is proposed, which integrates object detection and multi-object tracking into a single model. The proposed tracking framework eliminates the complex data association process in the classical TBD paradigm, and requires no ",
    "arxiv_url": "https://arxiv.org/abs/2304.08709v2",
    "pdf_url": "https://arxiv.org/pdf/2304.08709v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2304.08709",
    "arxiv_authors": [
      "Xiyang Wang",
      "Chunyun Fu",
      "Jiawei He",
      "Mingguang Huang",
      "Ting Meng",
      "Siyu Zhang",
      "Hangning Zhou",
      "Ziyao Xu",
      "Chi Zhang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=You+Only+Need+Two+Detectors+to+Achieve+Multi-Modal+3D+Multi-Object+Tracking+Xiyang+Wang+Chunyun+Fu+Jiawei+He+Mingguang+Huang+Ting+Meng",
    "gs_search_success": true,
    "gs_authors": [
      "EKtNV5sAAAAJ",
      "b5Le6AEAAAAJ"
    ],
    "citation_count": 33
  },
  {
    "arxiv_id": "2308.09515",
    "title": "Learnt Contrastive Concept Embeddings for Sign Recognition",
    "year": 2023,
    "published": "2023-08-18T12:47:18Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "In natural language processing (NLP) of spoken languages, word embeddings have been shown to be a useful method to encode the meaning of words. Sign languages are visual languages, which require sign embeddings to capture the visual and linguistic semantics of sign. Unlike many common approaches to Sign Recognition, we focus on explicitly creating sign embeddings that bridge the gap between sign language and spoken language. We propose a learning framework to derive LCC (Learnt Contrastive Conce",
    "arxiv_url": "https://arxiv.org/abs/2308.09515v1",
    "pdf_url": "https://arxiv.org/pdf/2308.09515v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2308.09515",
    "arxiv_authors": [
      "Ryan Wong",
      "Necati Cihan Camgoz",
      "Richard Bowden"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Learnt+Contrastive+Concept+Embeddings+for+Sign+Recognition+Ryan+Wong+Necati+Cihan+Camgoz+Richard+Bowden",
    "gs_search_success": true,
    "gs_authors": [
      "mvvgDvcAAAAJ",
      "T3C1SW0AAAAJ",
      "Tk5Egv8AAAAJ"
    ],
    "citation_count": 10
  },
  {
    "arxiv_id": "2408.16809",
    "title": "See or Guess: Counterfactually Regularized Image Captioning",
    "year": 2024,
    "published": "2024-08-29T17:59:57Z",
    "categories": [
      "cs.CV",
      "cs.CL",
      "cs.MM"
    ],
    "abstract": "Image captioning, which generates natural language descriptions of the visual information in an image, is a crucial task in vision-language research. Previous models have typically addressed this task by aligning the generative capabilities of machines with human intelligence through statistical fitting of existing datasets. While effective for normal images, they may struggle to accurately describe those where certain parts of the image are obscured or edited, unlike humans who excel in such ca",
    "arxiv_url": "https://arxiv.org/abs/2408.16809v1",
    "pdf_url": "https://arxiv.org/pdf/2408.16809v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2408.16809",
    "arxiv_authors": [
      "Qian Cao",
      "Xu Chen",
      "Ruihua Song",
      "Xiting Wang",
      "Xinting Huang",
      "Yuchen Ren"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=See+or+Guess%3A+Counterfactually+Regularized+Image+Captioning+Qian+Cao+Xu+Chen+Ruihua+Song+Xiting+Wang+Xinting+Huang",
    "gs_search_success": true,
    "gs_authors": [
      "urC8meQAAAAJ",
      "v5LctN8AAAAJ",
      "2OTi2RUAAAAJ",
      "loPoqy0AAAAJ",
      "QmyPDWQAAAAJ",
      "U4EnCRYAAAAJ"
    ],
    "citation_count": 6
  },
  {
    "arxiv_id": "2406.13445",
    "title": "Lost in UNet: Improving Infrared Small Target Detection by Underappreciated Local Features",
    "year": 2024,
    "published": "2024-06-19T11:11:38Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Many targets are often very small in infrared images due to the long-distance imaging meachnism. UNet and its variants, as popular detection backbone networks, downsample the local features early and cause the irreversible loss of these local features, leading to both the missed and false detection of small targets in infrared images. We propose HintU, a novel network to recover the local features lost by various UNet-based methods for effective infrared small target detection. HintU has two key",
    "arxiv_url": "https://arxiv.org/abs/2406.13445v1",
    "pdf_url": "https://arxiv.org/pdf/2406.13445v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2406.13445",
    "arxiv_authors": [
      "Wuzhou Quan",
      "Wei Zhao",
      "Weiming Wang",
      "Haoran Xie",
      "Fu Lee Wang",
      "Mingqiang Wei"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Lost+in+UNet%3A+Improving+Infrared+Small+Target+Detection+by+Underappreciated+Local+Features+Wuzhou+Quan+Wei+Zhao+Weiming+Wang+Haoran+Xie+Fu+Lee+Wang",
    "gs_search_success": true,
    "gs_authors": [
      "bxz8Pn8AAAAJ",
      "O4lGUj8AAAAJ",
      "EN0PM30AAAAJ",
      "MEBjvKsAAAAJ",
      "OyzZmeAAAAAJ",
      "TdrJj8MAAAAJ"
    ],
    "citation_count": 12
  },
  {
    "arxiv_id": "2409.13648",
    "title": "V^3: Viewing Volumetric Videos on Mobiles via Streamable 2D Dynamic Gaussians",
    "year": 2024,
    "published": "2024-09-20T16:54:27Z",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "abstract": "Experiencing high-fidelity volumetric video as seamlessly as 2D videos is a long-held dream. However, current dynamic 3DGS methods, despite their high rendering quality, face challenges in streaming on mobile devices due to computational and bandwidth constraints. In this paper, we introduce V^3 (Viewing Volumetric Videos), a novel approach that enables high-quality mobile rendering through the streaming of dynamic Gaussians. Our key innovation is to view dynamic 3DGS as 2D videos, facilitating ",
    "arxiv_url": "https://arxiv.org/abs/2409.13648v2",
    "pdf_url": "https://arxiv.org/pdf/2409.13648v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2409.13648",
    "arxiv_authors": [
      "Penghao Wang",
      "Zhirui Zhang",
      "Liao Wang",
      "Kaixin Yao",
      "Siyuan Xie",
      "Jingyi Yu",
      "Minye Wu",
      "Lan Xu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=V%5E3%3A+Viewing+Volumetric+Videos+on+Mobiles+via+Streamable+2D+Dynamic+Gaussians+Penghao+Wang+Zhirui+Zhang+Liao+Wang+Kaixin+Yao+Siyuan+Xie",
    "gs_search_success": true,
    "gs_authors": [
      "aPS5pJkAAAAJ",
      "UCO39HkAAAAJ",
      "3J9Xq68AAAAJ",
      "aUAnceQAAAAJ",
      "cTpteaQAAAAJ"
    ],
    "citation_count": 31
  },
  {
    "arxiv_id": "2310.01881",
    "title": "Adaptive Multi-NeRF: Exploit Efficient Parallelism in Adaptive Multiple Scale Neural Radiance Field Rendering",
    "year": 2023,
    "published": "2023-10-03T08:34:49Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Recent advances in Neural Radiance Fields (NeRF) have demonstrated significant potential for representing 3D scene appearances as implicit neural networks, enabling the synthesis of high-fidelity novel views. However, the lengthy training and rendering process hinders the widespread adoption of this promising technique for real-time rendering applications. To address this issue, we present an effective adaptive multi-NeRF method designed to accelerate the neural rendering process for large scene",
    "arxiv_url": "https://arxiv.org/abs/2310.01881v1",
    "pdf_url": "https://arxiv.org/pdf/2310.01881v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2310.01881",
    "arxiv_authors": [
      "Tong Wang",
      "Shuichi Kurabayashi"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Adaptive+Multi-NeRF%3A+Exploit+Efficient+Parallelism+in+Adaptive+Multiple+Scale+Neural+Radiance+Field+Rendering+Tong+Wang+Shuichi+Kurabayashi",
    "gs_search_success": true,
    "gs_authors": [
      "thXGrVYAAAAJ"
    ],
    "citation_count": null
  },
  {
    "arxiv_id": "2407.13771",
    "title": "Training-Free Model Merging for Multi-target Domain Adaptation",
    "year": 2024,
    "published": "2024-07-18T17:59:57Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "In this paper, we study multi-target domain adaptation of scene understanding models. While previous methods achieved commendable results through inter-domain consistency losses, they often assumed unrealistic simultaneous access to images from all target domains, overlooking constraints such as data transfer bandwidth limitations and data privacy concerns. Given these challenges, we pose the question: How to merge models adapted independently on distinct domains while bypassing the need for dir",
    "arxiv_url": "https://arxiv.org/abs/2407.13771v1",
    "pdf_url": "https://arxiv.org/pdf/2407.13771v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2407.13771",
    "arxiv_authors": [
      "Wenyi Li",
      "Huan-ang Gao",
      "Mingju Gao",
      "Beiwen Tian",
      "Rong Zhi",
      "Hao Zhao"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Training-Free+Model+Merging+for+Multi-target+Domain+Adaptation+Wenyi+Li+Huan-ang+Gao+Mingju+Gao+Beiwen+Tian+Rong+Zhi",
    "gs_search_success": true,
    "gs_authors": [
      "ygQznUQAAAAJ",
      "WvbKfLgAAAAJ",
      "PMuXXWQAAAAJ",
      "y_jNFVgAAAAJ"
    ],
    "citation_count": 15
  },
  {
    "arxiv_id": "2311.12070",
    "title": "FDDM: Unsupervised Medical Image Translation with a Frequency-Decoupled Diffusion Model",
    "year": 2023,
    "published": "2023-11-19T19:44:44Z",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "abstract": "Diffusion models have demonstrated significant potential in producing high-quality images in medical image translation to aid disease diagnosis, localization, and treatment. Nevertheless, current diffusion models have limited success in achieving faithful image translations that can accurately preserve the anatomical structures of medical images, especially for unpaired datasets. The preservation of structural and anatomical details is essential to reliable medical diagnosis and treatment planni",
    "arxiv_url": "https://arxiv.org/abs/2311.12070v3",
    "pdf_url": "https://arxiv.org/pdf/2311.12070v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2311.12070",
    "arxiv_authors": [
      "Yunxiang Li",
      "Hua-Chieh Shao",
      "Xiaoxue Qian",
      "You Zhang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=FDDM%3A+Unsupervised+Medical+Image+Translation+with+a+Frequency-Decoupled+Diffusion+Model+Yunxiang+Li+Hua-Chieh+Shao+Xiaoxue+Qian+You+Zhang",
    "gs_search_success": true,
    "gs_authors": [
      "evbcKz8AAAAJ",
      "VzdJe_4AAAAJ",
      "0vAGzmMAAAAJ"
    ],
    "citation_count": 8
  },
  {
    "arxiv_id": "2412.11435",
    "title": "Learning Implicit Features with Flow Infused Attention for Realistic Virtual Try-On",
    "year": 2024,
    "published": "2024-12-16T04:23:33Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Image-based virtual try-on is challenging since the generated image should fit the garment to model images in various poses and keep the characteristics and details of the garment simultaneously. A popular research stream warps the garment image firstly to reduce the burden of the generation stage, which relies highly on the performance of the warping module. Other methods without explicit warping often lack sufficient guidance to fit the garment to the model images. In this paper, we propose FI",
    "arxiv_url": "https://arxiv.org/abs/2412.11435v1",
    "pdf_url": "https://arxiv.org/pdf/2412.11435v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2412.11435",
    "arxiv_authors": [
      "Delong Zhang",
      "Qiwei Huang",
      "Yuanliu Liu",
      "Yang Sun",
      "Wei-Shi Zheng",
      "Pengfei Xiong",
      "Wei Zhang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Learning+Implicit+Features+with+Flow+Infused+Attention+for+Realistic+Virtual+Try-On+Delong+Zhang+Qiwei+Huang+Yuanliu+Liu+Yang+Sun+Wei-Shi+Zheng",
    "gs_search_success": false,
    "gs_authors": [],
    "citation_count": null,
    "error_type": "fetch_failed"
  },
  {
    "arxiv_id": "2403.09363",
    "title": "Sentinel-Guided Zero-Shot Learning: A Collaborative Paradigm without Real Data Exposure",
    "year": 2024,
    "published": "2024-03-14T13:12:49Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "With increasing concerns over data privacy and model copyrights, especially in the context of collaborations between AI service providers and data owners, an innovative SG-ZSL paradigm is proposed in this work. SG-ZSL is designed to foster efficient collaboration without the need to exchange models or sensitive data. It consists of a teacher model, a student model and a generator that links both model entities. The teacher model serves as a sentinel on behalf of the data owner, replacing real da",
    "arxiv_url": "https://arxiv.org/abs/2403.09363v1",
    "pdf_url": "https://arxiv.org/pdf/2403.09363v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2403.09363",
    "arxiv_authors": [
      "Fan Wan",
      "Xingyu Miao",
      "Haoran Duan",
      "Jingjing Deng",
      "Rui Gao",
      "Yang Long"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Sentinel-Guided+Zero-Shot+Learning%3A+A+Collaborative+Paradigm+without+Real+Data+Exposure+Fan+Wan+Xingyu+Miao+Haoran+Duan+Jingjing+Deng+Rui+Gao",
    "gs_search_success": true,
    "gs_authors": [
      "Ft_70SQAAAAJ",
      "AVemHcIAAAAJ",
      "2fIQOBsAAAAJ",
      "QSY2OkIAAAAJ",
      "2NRO1eoAAAAJ",
      "IrkuknEAAAAJ"
    ],
    "citation_count": 7
  },
  {
    "arxiv_id": "2403.12035",
    "title": "CoCoCo: Improving Text-Guided Video Inpainting for Better Consistency, Controllability and Compatibility",
    "year": 2024,
    "published": "2024-03-18T17:59:27Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Recent advancements in video generation have been remarkable, yet many existing methods struggle with issues of consistency and poor text-video alignment. Moreover, the field lacks effective techniques for text-guided video inpainting, a stark contrast to the well-explored domain of text-guided image inpainting. To this end, this paper proposes a novel text-guided video inpainting model that achieves better consistency, controllability and compatibility. Specifically, we introduce a simple but e",
    "arxiv_url": "https://arxiv.org/abs/2403.12035v1",
    "pdf_url": "https://arxiv.org/pdf/2403.12035v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2403.12035",
    "arxiv_authors": [
      "Bojia Zi",
      "Shihao Zhao",
      "Xianbiao Qi",
      "Jianan Wang",
      "Yukai Shi",
      "Qianyu Chen",
      "Bin Liang",
      "Kam-Fai Wong",
      "Lei Zhang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=CoCoCo%3A+Improving+Text-Guided+Video+Inpainting+for+Better+Consistency%2C+Controllability+and+Compatibility+Bojia+Zi+Shihao+Zhao+Xianbiao+Qi+Jianan+Wang+Yukai+Shi",
    "gs_search_success": true,
    "gs_authors": [
      "mt5mvZ8AAAAJ",
      "dNQiLDQAAAAJ",
      "Zb5wT08AAAAJ",
      "QrMKIkEAAAAJ",
      "Kh8FoLQAAAAJ",
      "oQXfkSQAAAAJ",
      "fyMni2cAAAAJ",
      "djpQeLEAAAAJ"
    ],
    "citation_count": 28
  },
  {
    "arxiv_id": "2306.04021",
    "title": "Energy-Based Models for Cross-Modal Localization using Convolutional Transformers",
    "year": 2023,
    "published": "2023-06-06T21:27:08Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "cs.RO"
    ],
    "abstract": "We present a novel framework using Energy-Based Models (EBMs) for localizing a ground vehicle mounted with a range sensor against satellite imagery in the absence of GPS. Lidar sensors have become ubiquitous on autonomous vehicles for describing its surrounding environment. Map priors are typically built using the same sensor modality for localization purposes. However, these map building endeavors using range sensors are often expensive and time-consuming. Alternatively, we leverage the use of ",
    "arxiv_url": "https://arxiv.org/abs/2306.04021v1",
    "pdf_url": "https://arxiv.org/pdf/2306.04021v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2306.04021",
    "arxiv_authors": [
      "Alan Wu",
      "Michael S. Ryoo"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Energy-Based+Models+for+Cross-Modal+Localization+using+Convolutional+Transformers+Alan+Wu+Michael+S.+Ryoo",
    "gs_search_success": true,
    "gs_authors": [
      "vcw0TJIAAAAJ"
    ],
    "citation_count": 3
  },
  {
    "arxiv_id": "2405.18021",
    "title": "MULi-Ev: Maintaining Unperturbed LiDAR-Event Calibration",
    "year": 2024,
    "published": "2024-05-28T10:09:49Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Despite the increasing interest in enhancing perception systems for autonomous vehicles, the online calibration between event cameras and LiDAR - two sensors pivotal in capturing comprehensive environmental information - remains unexplored. We introduce MULi-Ev, the first online, deep learning-based framework tailored for the extrinsic calibration of event cameras with LiDAR. This advancement is instrumental for the seamless integration of LiDAR and event cameras, enabling dynamic, real-time cal",
    "arxiv_url": "https://arxiv.org/abs/2405.18021v1",
    "pdf_url": "https://arxiv.org/pdf/2405.18021v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2405.18021",
    "arxiv_authors": [
      "Mathieu Cocheteux",
      "Julien Moreau",
      "Franck Davoine"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=MULi-Ev%3A+Maintaining+Unperturbed+LiDAR-Event+Calibration+Mathieu+Cocheteux+Julien+Moreau+Franck+Davoine",
    "gs_search_success": true,
    "gs_authors": [
      "_wJgcSUAAAAJ",
      "QWX91zcAAAAJ"
    ],
    "citation_count": 9
  },
  {
    "arxiv_id": "2404.04421",
    "title": "PhysAvatar: Learning the Physics of Dressed 3D Avatars from Visual Observations",
    "year": 2024,
    "published": "2024-04-05T21:44:57Z",
    "categories": [
      "cs.GR",
      "cs.CV"
    ],
    "abstract": "Modeling and rendering photorealistic avatars is of crucial importance in many applications. Existing methods that build a 3D avatar from visual observations, however, struggle to reconstruct clothed humans. We introduce PhysAvatar, a novel framework that combines inverse rendering with inverse physics to automatically estimate the shape and appearance of a human from multi-view video data along with the physical parameters of the fabric of their clothes. For this purpose, we adopt a mesh-aligne",
    "arxiv_url": "https://arxiv.org/abs/2404.04421v2",
    "pdf_url": "https://arxiv.org/pdf/2404.04421v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2404.04421",
    "arxiv_authors": [
      "Yang Zheng",
      "Qingqing Zhao",
      "Guandao Yang",
      "Wang Yifan",
      "Donglai Xiang",
      "Florian Dubost",
      "Dmitry Lagun",
      "Thabo Beeler",
      "Federico Tombari",
      "Leonidas Guibas",
      "Gordon Wetzstein"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=PhysAvatar%3A+Learning+the+Physics+of+Dressed+3D+Avatars+from+Visual+Observations+Yang+Zheng+Qingqing+Zhao+Guandao+Yang+Wang+Yifan+Donglai+Xiang",
    "gs_search_success": true,
    "gs_authors": [
      "Q7Ouk0QAAAAJ",
      "sY8lt7AAAAAJ",
      "UVMgJvYAAAAJ",
      "_kElCmMAAAAJ",
      "TFsE4BIAAAAJ",
      "_yNBmx8AAAAJ",
      "tjT-DfsAAAAJ",
      "q5y44h8AAAAJ",
      "4zyT8SYAAAAJ"
    ],
    "citation_count": 45
  },
  {
    "arxiv_id": "2311.00562",
    "title": "MNN: Mixed Nearest-Neighbors for Self-Supervised Learning",
    "year": 2023,
    "published": "2023-11-01T14:59:41Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "In contrastive self-supervised learning, positive samples are typically drawn from the same image but in different augmented views, resulting in a relatively limited source of positive samples. An effective way to alleviate this problem is to incorporate the relationship between samples, which involves including the top-K nearest neighbors of positive samples. However, the problem of false neighbors (i.e., neighbors that do not belong to the same category as the positive sample) is an objective ",
    "arxiv_url": "https://arxiv.org/abs/2311.00562v2",
    "pdf_url": "https://arxiv.org/pdf/2311.00562v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2311.00562",
    "arxiv_authors": [
      "Xianzhong Long",
      "Chen Peng",
      "Yun Li"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=MNN%3A+Mixed+Nearest-Neighbors+for+Self-Supervised+Learning+Xianzhong+Long+Chen+Peng+Yun+Li",
    "gs_search_success": true,
    "gs_authors": [
      "2BK_EXgAAAAJ"
    ],
    "citation_count": 1
  },
  {
    "arxiv_id": "2409.13591",
    "title": "Portrait Video Editing Empowered by Multimodal Generative Priors",
    "year": 2024,
    "published": "2024-09-20T15:45:13Z",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "abstract": "We introduce PortraitGen, a powerful portrait video editing method that achieves consistent and expressive stylization with multimodal prompts. Traditional portrait video editing methods often struggle with 3D and temporal consistency, and typically lack in rendering quality and efficiency. To address these issues, we lift the portrait video frames to a unified dynamic 3D Gaussian field, which ensures structural and temporal coherence across frames. Furthermore, we design a novel Neural Gaussian",
    "arxiv_url": "https://arxiv.org/abs/2409.13591v1",
    "pdf_url": "https://arxiv.org/pdf/2409.13591v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2409.13591",
    "arxiv_authors": [
      "Xuan Gao",
      "Haiyao Xiao",
      "Chenglai Zhong",
      "Shimin Hu",
      "Yudong Guo",
      "Juyong Zhang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Portrait+Video+Editing+Empowered+by+Multimodal+Generative+Priors+Xuan+Gao+Haiyao+Xiao+Chenglai+Zhong+Shimin+Hu+Yudong+Guo",
    "gs_search_success": true,
    "gs_authors": [
      "1vLVI9IAAAAJ",
      "cxF_-i4AAAAJ",
      "_D9aUrgAAAAJ"
    ],
    "citation_count": 8
  },
  {
    "arxiv_id": "2305.12250",
    "title": "DAC: Detector-Agnostic Spatial Covariances for Deep Local Features",
    "year": 2023,
    "published": "2023-05-20T17:43:09Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Current deep visual local feature detectors do not model the spatial uncertainty of detected features, producing suboptimal results in downstream applications. In this work, we propose two post-hoc covariance estimates that can be plugged into any pretrained deep feature detector: a simple, isotropic covariance estimate that uses the predicted score at a given pixel location, and a full covariance estimate via the local structure tensor of the learned score maps. Both methods are easy to impleme",
    "arxiv_url": "https://arxiv.org/abs/2305.12250v2",
    "pdf_url": "https://arxiv.org/pdf/2305.12250v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2305.12250",
    "arxiv_authors": [
      "Javier Tirado-Garín",
      "Frederik Warburg",
      "Javier Civera"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=DAC%3A+Detector-Agnostic+Spatial+Covariances+for+Deep+Local+Features+Javier+Tirado-Gar%C3%ADn+Frederik+Warburg+Javier+Civera",
    "gs_search_success": true,
    "gs_authors": [
      "j_sMzokAAAAJ",
      "0Ozzy4IAAAAJ",
      "Bh77DXYAAAAJ"
    ],
    "citation_count": 2
  },
  {
    "arxiv_id": "2407.13677",
    "title": "PASTA: Controllable Part-Aware Shape Generation with Autoregressive Transformers",
    "year": 2024,
    "published": "2024-07-18T16:52:45Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR",
      "cs.LG"
    ],
    "abstract": "The increased demand for tools that automate the 3D content creation process led to tremendous progress in deep generative models that can generate diverse 3D objects of high fidelity. In this paper, we present PASTA, an autoregressive transformer architecture for generating high quality 3D shapes. PASTA comprises two main components: An autoregressive transformer that generates objects as a sequence of cuboidal primitives and a blending network, implemented with a transformer decoder that compo",
    "arxiv_url": "https://arxiv.org/abs/2407.13677v1",
    "pdf_url": "https://arxiv.org/pdf/2407.13677v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2407.13677",
    "arxiv_authors": [
      "Songlin Li",
      "Despoina Paschalidou",
      "Leonidas Guibas"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=PASTA%3A+Controllable+Part-Aware+Shape+Generation+with+Autoregressive+Transformers+Songlin+Li+Despoina+Paschalidou+Leonidas+Guibas",
    "gs_search_success": true,
    "gs_authors": [
      "5JlEyTAAAAAJ",
      "zxFlR6sAAAAJ",
      "WXw-gLgAAAAJ"
    ],
    "citation_count": 8
  },
  {
    "arxiv_id": "2401.06442",
    "title": "RotationDrag: Point-based Image Editing with Rotated Diffusion Features",
    "year": 2024,
    "published": "2024-01-12T08:24:35Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "A precise and user-friendly manipulation of image content while preserving image fidelity has always been crucial to the field of image editing. Thanks to the power of generative models, recent point-based image editing methods allow users to interactively change the image content with high generalizability by clicking several control points. But the above mentioned editing process is usually based on the assumption that features stay constant in the motion supervision step from initial to targe",
    "arxiv_url": "https://arxiv.org/abs/2401.06442v1",
    "pdf_url": "https://arxiv.org/pdf/2401.06442v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2401.06442",
    "arxiv_authors": [
      "Minxing Luo",
      "Wentao Cheng",
      "Jian Yang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=RotationDrag%3A+Point-based+Image+Editing+with+Rotated+Diffusion+Features+Minxing+Luo+Wentao+Cheng+Jian+Yang",
    "gs_search_success": true,
    "gs_authors": [
      "cpIySUoAAAAJ",
      "6CIDtZQAAAAJ"
    ],
    "citation_count": 3
  },
  {
    "arxiv_id": "2309.13516",
    "title": "InSpaceType: Reconsider Space Type in Indoor Monocular Depth Estimation",
    "year": 2023,
    "published": "2023-09-24T00:39:41Z",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "abstract": "Indoor monocular depth estimation has attracted increasing research interest. Most previous works have been focusing on methodology, primarily experimenting with NYU-Depth-V2 (NYUv2) Dataset, and only concentrated on the overall performance over the test set. However, little is known regarding robustness and generalization when it comes to applying monocular depth estimation methods to real-world scenarios where highly varying and diverse functional \\textit{space types} are present such as libra",
    "arxiv_url": "https://arxiv.org/abs/2309.13516v2",
    "pdf_url": "https://arxiv.org/pdf/2309.13516v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2309.13516",
    "arxiv_authors": [
      "Cho-Ying Wu",
      "Quankai Gao",
      "Chin-Cheng Hsu",
      "Te-Lin Wu",
      "Jing-Wen Chen",
      "Ulrich Neumann"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=InSpaceType%3A+Reconsider+Space+Type+in+Indoor+Monocular+Depth+Estimation+Cho-Ying+Wu+Quankai+Gao+Chin-Cheng+Hsu+Te-Lin+Wu+Jing-Wen+Chen",
    "gs_search_success": true,
    "gs_authors": [
      "0bXl62UAAAAJ",
      "TTDb6YMAAAAJ",
      "5i7k1RoAAAAJ",
      "UyWUO9IAAAAJ",
      "tIdThSIAAAAJ",
      "MHet2VoAAAAJ"
    ],
    "citation_count": 10
  },
  {
    "arxiv_id": "2405.11129",
    "title": "MotionGS : Compact Gaussian Splatting SLAM by Motion Filter",
    "year": 2024,
    "published": "2024-05-18T00:47:29Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "With their high-fidelity scene representation capability, the attention of SLAM field is deeply attracted by the Neural Radiation Field (NeRF) and 3D Gaussian Splatting (3DGS). Recently, there has been a surge in NeRF-based SLAM, while 3DGS-based SLAM is sparse. A novel 3DGS-based SLAM approach with a fusion of deep visual feature, dual keyframe selection and 3DGS is presented in this paper. Compared with the existing methods, the proposed tracking is achieved by feature extraction and motion fi",
    "arxiv_url": "https://arxiv.org/abs/2405.11129v2",
    "pdf_url": "https://arxiv.org/pdf/2405.11129v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2405.11129",
    "arxiv_authors": [
      "Xinli Guo",
      "Weidong Zhang",
      "Ruonan Liu",
      "Peng Han",
      "Hongtian Chen"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=MotionGS+%3A+Compact+Gaussian+Splatting+SLAM+by+Motion+Filter+Xinli+Guo+Weidong+Zhang+Ruonan+Liu+Peng+Han+Hongtian+Chen",
    "gs_search_success": true,
    "gs_authors": [],
    "citation_count": 11
  },
  {
    "arxiv_id": "2504.08823",
    "title": "FM-LoRA: Factorized Low-Rank Meta-Prompting for Continual Learning",
    "year": 2025,
    "published": "2025-04-09T19:36:18Z",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "abstract": "How to adapt a pre-trained model continuously for sequential tasks with different prediction class labels and domains and finally learn a generalizable model across diverse tasks is a long-lasting challenge. Continual learning (CL) has emerged as a promising approach to leverage pre-trained models (e.g., Transformers) for sequential tasks. While many existing CL methods incrementally store additional learned structures, such as Low-Rank Adaptation (LoRA) adapters or prompts and sometimes even pr",
    "arxiv_url": "https://arxiv.org/abs/2504.08823v1",
    "pdf_url": "https://arxiv.org/pdf/2504.08823v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2504.08823",
    "arxiv_authors": [
      "Xiaobing Yu",
      "Jin Yang",
      "Xiao Wu",
      "Peijie Qiu",
      "Xiaofeng Liu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=FM-LoRA%3A+Factorized+Low-Rank+Meta-Prompting+for+Continual+Learning+Xiaobing+Yu+Jin+Yang+Xiao+Wu+Peijie+Qiu+Xiaofeng+Liu",
    "gs_search_success": true,
    "gs_authors": [
      "VighnTUAAAAJ",
      "7HLmlHMAAAAJ",
      "53LJfHAAAAAJ",
      "ymNbC1YAAAAJ"
    ],
    "citation_count": 4
  },
  {
    "arxiv_id": "2505.24866",
    "title": "TalkingHeadBench: A Multi-Modal Benchmark & Analysis of Talking-Head DeepFake Detection",
    "year": 2025,
    "published": "2025-05-30T17:59:08Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "The rapid advancement of talking-head deepfake generation fueled by advanced generative models has elevated the realism of synthetic videos to a level that poses substantial risks in domains such as media, politics, and finance. However, current benchmarks for deepfake talking-head detection fail to reflect this progress, relying on outdated generators and offering limited insight into model robustness and generalization. We introduce TalkingHeadBench, a comprehensive multi-model multi-generator",
    "arxiv_url": "https://arxiv.org/abs/2505.24866v1",
    "pdf_url": "https://arxiv.org/pdf/2505.24866v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2505.24866",
    "arxiv_authors": [
      "Xinqi Xiong",
      "Prakrut Patel",
      "Qingyuan Fan",
      "Amisha Wadhwa",
      "Sarathy Selvam",
      "Xiao Guo",
      "Luchao Qi",
      "Xiaoming Liu",
      "Roni Sengupta"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=TalkingHeadBench%3A+A+Multi-Modal+Benchmark+%26+Analysis+of+Talking-Head+DeepFake+Detection+Xinqi+Xiong+Prakrut+Patel+Qingyuan+Fan+Amisha+Wadhwa+Sarathy+Selvam",
    "gs_search_success": true,
    "gs_authors": [
      "Id8SJl8AAAAJ",
      "H93xhggAAAAJ",
      "j1k9a_QAAAAJ",
      "xrArE5YAAAAJ",
      "kcJpqKgAAAAJ"
    ],
    "citation_count": 3
  },
  {
    "arxiv_id": "2403.19589",
    "title": "TOD3Cap: Towards 3D Dense Captioning in Outdoor Scenes",
    "year": 2024,
    "published": "2024-03-28T17:12:55Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "3D dense captioning stands as a cornerstone in achieving a comprehensive understanding of 3D scenes through natural language. It has recently witnessed remarkable achievements, particularly in indoor settings. However, the exploration of 3D dense captioning in outdoor scenes is hindered by two major challenges: 1) the domain gap between indoor and outdoor scenes, such as dynamics and sparse visual inputs, makes it difficult to directly adapt existing indoor methods; 2) the lack of data with comp",
    "arxiv_url": "https://arxiv.org/abs/2403.19589v2",
    "pdf_url": "https://arxiv.org/pdf/2403.19589v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2403.19589",
    "arxiv_authors": [
      "Bu Jin",
      "Yupeng Zheng",
      "Pengfei Li",
      "Weize Li",
      "Yuhang Zheng",
      "Sujie Hu",
      "Xinyu Liu",
      "Jinwei Zhu",
      "Zhijie Yan",
      "Haiyang Sun",
      "Kun Zhan",
      "Peng Jia",
      "Xiaoxiao Long",
      "Yilun Chen",
      "Hao Zhao"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=TOD3Cap%3A+Towards+3D+Dense+Captioning+in+Outdoor+Scenes+Bu+Jin+Yupeng+Zheng+Pengfei+Li+Weize+Li+Yuhang+Zheng",
    "gs_search_success": true,
    "gs_authors": [
      "CyPiUucAAAAJ",
      "4PXGeaYAAAAJ",
      "8YJvY9cAAAAJ",
      "Z_QY_VwAAAAJ",
      "anGhGdYAAAAJ",
      "kgRjFN8AAAAJ",
      "Wn2Aic0AAAAJ",
      "W3G5kZEAAAAJ",
      "SYbFNsIAAAAJ",
      "uUd5v2cAAAAJ",
      "1J061HIAAAAJ"
    ],
    "citation_count": 30
  },
  {
    "arxiv_id": "2501.13418",
    "title": "Rethinking the Sample Relations for Few-Shot Classification",
    "year": 2025,
    "published": "2025-01-23T06:45:17Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Feature quality is paramount for classification performance, particularly in few-shot scenarios. Contrastive learning, a widely adopted technique for enhancing feature quality, leverages sample relations to extract intrinsic features that capture semantic information and has achieved remarkable success in Few-Shot Learning (FSL). Nevertheless, current few-shot contrastive learning approaches often overlook the semantic similarity discrepancies at different granularities when employing the same m",
    "arxiv_url": "https://arxiv.org/abs/2501.13418v1",
    "pdf_url": "https://arxiv.org/pdf/2501.13418v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2501.13418",
    "arxiv_authors": [
      "Guowei Yin",
      "Sheng Huang",
      "Luwen Huangfu",
      "Yi Zhang",
      "Xiaohong Zhang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Rethinking+the+Sample+Relations+for+Few-Shot+Classification+Guowei+Yin+Sheng+Huang+Luwen+Huangfu+Yi+Zhang+Xiaohong+Zhang",
    "gs_search_success": true,
    "gs_authors": [
      "BMo9cZ4AAAAJ",
      "JL-pDFYAAAAJ"
    ],
    "citation_count": 1
  },
  {
    "arxiv_id": "2502.10794",
    "title": "Distraction is All You Need for Multimodal Large Language Model Jailbreaking",
    "year": 2025,
    "published": "2025-02-15T13:25:12Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Multimodal Large Language Models (MLLMs) bridge the gap between visual and textual data, enabling a range of advanced applications. However, complex internal interactions among visual elements and their alignment with text can introduce vulnerabilities, which may be exploited to bypass safety mechanisms. To address this, we analyze the relationship between image content and task and find that the complexity of subimages, rather than their content, is key. Building on this insight, we propose the",
    "arxiv_url": "https://arxiv.org/abs/2502.10794v2",
    "pdf_url": "https://arxiv.org/pdf/2502.10794v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2502.10794",
    "arxiv_authors": [
      "Zuopeng Yang",
      "Jiluan Fan",
      "Anli Yan",
      "Erdun Gao",
      "Xin Lin",
      "Tao Li",
      "Kanghua Mo",
      "Changyu Dong"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Distraction+is+All+You+Need+for+Multimodal+Large+Language+Model+Jailbreaking+Zuopeng+Yang+Jiluan+Fan+Anli+Yan+Erdun+Gao+Xin+Lin",
    "gs_search_success": true,
    "gs_authors": [
      "UKR9sNQAAAAJ",
      "qbuqD10AAAAJ",
      "SilAzvwAAAAJ",
      "usKTO3wAAAAJ",
      "Mj2CqyUAAAAJ",
      "pyHD7UQAAAAJ",
      "qtMWgYsAAAAJ"
    ],
    "citation_count": 15
  },
  {
    "arxiv_id": "2411.15355",
    "title": "UniGaussian: Driving Scene Reconstruction from Multiple Camera Models via Unified Gaussian Representations",
    "year": 2024,
    "published": "2024-11-22T21:59:46Z",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "abstract": "Urban scene reconstruction is crucial for real-world autonomous driving simulators. Although existing methods have achieved photorealistic reconstruction, they mostly focus on pinhole cameras and neglect fisheye cameras. In fact, how to effectively simulate fisheye cameras in driving scene remains an unsolved problem. In this work, we propose UniGaussian, a novel approach that learns a unified 3D Gaussian representation from multiple camera models for urban scene reconstruction in autonomous dri",
    "arxiv_url": "https://arxiv.org/abs/2411.15355v2",
    "pdf_url": "https://arxiv.org/pdf/2411.15355v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2411.15355",
    "arxiv_authors": [
      "Yuan Ren",
      "Guile Wu",
      "Runhao Li",
      "Zheyuan Yang",
      "Yibo Liu",
      "Xingxin Chen",
      "Tongtong Cao",
      "Bingbing Liu"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=UniGaussian%3A+Driving+Scene+Reconstruction+from+Multiple+Camera+Models+via+Unified+Gaussian+Representations+Yuan+Ren+Guile+Wu+Runhao+Li+Zheyuan+Yang+Yibo+Liu",
    "gs_search_success": true,
    "gs_authors": [
      "P4Rp5uAAAAAJ",
      "zqDgDQ4AAAAJ",
      "-rCulKwAAAAJ",
      "ZtaFLE0AAAAJ",
      "SkzzXSYAAAAJ",
      "NdOL5w0AAAAJ"
    ],
    "citation_count": 8
  },
  {
    "arxiv_id": "2408.09181",
    "title": "PADetBench: Towards Benchmarking Physical Attacks against Object Detection",
    "year": 2024,
    "published": "2024-08-17T12:11:22Z",
    "categories": [
      "cs.CV",
      "cs.CR",
      "cs.LG"
    ],
    "abstract": "Physical attacks against object detection have gained increasing attention due to their significant practical implications. However, conducting physical experiments is extremely time-consuming and labor-intensive. Moreover, physical dynamics and cross-domain transformation are challenging to strictly regulate in the real world, leading to unaligned evaluation and comparison, severely hindering the development of physically robust models. To accommodate these challenges, we explore utilizing real",
    "arxiv_url": "https://arxiv.org/abs/2408.09181v3",
    "pdf_url": "https://arxiv.org/pdf/2408.09181v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2408.09181",
    "arxiv_authors": [
      "Jiawei Lian",
      "Jianhong Pan",
      "Lefan Wang",
      "Yi Wang",
      "Lap-Pui Chau",
      "Shaohui Mei"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=PADetBench%3A+Towards+Benchmarking+Physical+Attacks+against+Object+Detection+Jiawei+Lian+Jianhong+Pan+Lefan+Wang+Yi+Wang+Lap-Pui+Chau",
    "gs_search_success": true,
    "gs_authors": [
      "rAPPIPQAAAAJ",
      "MYREIH0AAAAJ",
      "84agyF4AAAAJ",
      "MAG909MAAAAJ",
      "K_51OTcAAAAJ"
    ],
    "citation_count": 3
  },
  {
    "arxiv_id": "2502.09779",
    "title": "Automated Muscle and Fat Segmentation in Computed Tomography for Comprehensive Body Composition Analysis",
    "year": 2025,
    "published": "2025-02-13T21:27:10Z",
    "categories": [
      "eess.IV",
      "cs.CV"
    ],
    "abstract": "Body composition assessment using CT images can potentially be used for a number of clinical applications, including the prognostication of cardiovascular outcomes, evaluation of metabolic health, monitoring of disease progression, assessment of nutritional status, prediction of treatment response in oncology, and risk stratification for surgical and critical care outcomes. While multiple groups have developed in-house segmentation tools for this analysis, there are very limited publicly availab",
    "arxiv_url": "https://arxiv.org/abs/2502.09779v4",
    "pdf_url": "https://arxiv.org/pdf/2502.09779v4",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2502.09779",
    "arxiv_authors": [
      "Yaqian Chen",
      "Hanxue Gu",
      "Yuwen Chen",
      "Jichen Yang",
      "Haoyu Dong",
      "Joseph Y. Cao",
      "Adrian Camarena",
      "Christopher Mantyh",
      "Roy Colglazier",
      "Maciej A. Mazurowski"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Automated+Muscle+and+Fat+Segmentation+in+Computed+Tomography+for+Comprehensive+Body+Composition+Analysis+Yaqian+Chen+Hanxue+Gu+Yuwen+Chen+Jichen+Yang+Haoyu+Dong",
    "gs_search_success": true,
    "gs_authors": [
      "eZVEUCIAAAAJ",
      "jGv3bRUAAAAJ",
      "aGjCpQUAAAAJ",
      "61s49p0AAAAJ",
      "iegKFuQAAAAJ"
    ],
    "citation_count": 2
  },
  {
    "arxiv_id": "2410.02705",
    "title": "ControlAR: Controllable Image Generation with Autoregressive Models",
    "year": 2024,
    "published": "2024-10-03T17:28:07Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Autoregressive (AR) models have reformulated image generation as next-token prediction, demonstrating remarkable potential and emerging as strong competitors to diffusion models. However, control-to-image generation, akin to ControlNet, remains largely unexplored within AR models. Although a natural approach, inspired by advancements in Large Language Models, is to tokenize control images into tokens and prefill them into the autoregressive model before decoding image tokens, it still falls shor",
    "arxiv_url": "https://arxiv.org/abs/2410.02705v3",
    "pdf_url": "https://arxiv.org/pdf/2410.02705v3",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2410.02705",
    "arxiv_authors": [
      "Zongming Li",
      "Tianheng Cheng",
      "Shoufa Chen",
      "Peize Sun",
      "Haocheng Shen",
      "Longjin Ran",
      "Xiaoxin Chen",
      "Wenyu Liu",
      "Xinggang Wang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=ControlAR%3A+Controllable+Image+Generation+with+Autoregressive+Models+Zongming+Li+Tianheng+Cheng+Shoufa+Chen+Peize+Sun+Haocheng+Shen",
    "gs_search_success": true,
    "gs_authors": [
      "PH8rJHYAAAAJ",
      "AfC_R58AAAAJ",
      "s697AYIAAAAJ",
      "SmHr4W4AAAAJ",
      "ogoCvHEAAAAJ",
      "Grkp5AQAAAAJ",
      "qNCTLV0AAAAJ"
    ],
    "citation_count": 41
  },
  {
    "arxiv_id": "2312.13277",
    "title": "Deep Learning on Object-centric 3D Neural Fields",
    "year": 2023,
    "published": "2023-12-20T18:56:45Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "In recent years, Neural Fields (NFs) have emerged as an effective tool for encoding diverse continuous signals such as images, videos, audio, and 3D shapes. When applied to 3D data, NFs offer a solution to the fragmentation and limitations associated with prevalent discrete representations. However, given that NFs are essentially neural networks, it remains unclear whether and how they can be seamlessly integrated into deep learning pipelines for solving downstream tasks. This paper addresses th",
    "arxiv_url": "https://arxiv.org/abs/2312.13277v2",
    "pdf_url": "https://arxiv.org/pdf/2312.13277v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2312.13277",
    "arxiv_authors": [
      "Pierluigi Zama Ramirez",
      "Luca De Luigi",
      "Daniele Sirocchi",
      "Adriano Cardace",
      "Riccardo Spezialetti",
      "Francesco Ballerini",
      "Samuele Salti",
      "Luigi Di Stefano"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Deep+Learning+on+Object-centric+3D+Neural+Fields+Pierluigi+Zama+Ramirez+Luca+De+Luigi+Daniele+Sirocchi+Adriano+Cardace+Riccardo+Spezialetti",
    "gs_search_success": true,
    "gs_authors": [
      "PpHLOpQAAAAJ",
      "OAYAmKYAAAAJ"
    ],
    "citation_count": 9
  },
  {
    "arxiv_id": "2506.08020",
    "title": "Bi-level Unbalanced Optimal Transport for Partial Domain Adaptation",
    "year": 2025,
    "published": "2025-05-19T13:40:40Z",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "abstract": "Partial domain adaptation (PDA) problem requires aligning cross-domain samples while distinguishing the outlier classes for accurate knowledge transfer. The widely used weighting framework tries to address the outlier classes by introducing the reweighed source domain with a similar label distribution to the target domain. However, the empirical modeling of weights can only characterize the sample-wise relations, which leads to insufficient exploration of cluster structures, and the weights coul",
    "arxiv_url": "https://arxiv.org/abs/2506.08020v1",
    "pdf_url": "https://arxiv.org/pdf/2506.08020v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2506.08020",
    "arxiv_authors": [
      "Zi-Ying Chen",
      "Chuan-Xian Ren",
      "Hong Yan"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Bi-level+Unbalanced+Optimal+Transport+for+Partial+Domain+Adaptation+Zi-Ying+Chen+Chuan-Xian+Ren+Hong+Yan",
    "gs_search_success": true,
    "gs_authors": [
      "nWsPNkQAAAAJ"
    ],
    "citation_count": null
  },
  {
    "arxiv_id": "2410.15472",
    "title": "Multi-Layer Feature Fusion with Cross-Channel Attention-Based U-Net for Kidney Tumor Segmentation",
    "year": 2024,
    "published": "2024-10-20T19:02:41Z",
    "categories": [
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "abstract": "Renal tumors, especially renal cell carcinoma (RCC), show significant heterogeneity, posing challenges for diagnosis using radiology images such as MRI, echocardiograms, and CT scans. U-Net based deep learning techniques are emerging as a promising approach for automated medical image segmentation for minimally invasive diagnosis of renal tumors. However, current techniques need further improvements in accuracy to become clinically useful to radiologists. In this study, we present an improved U-",
    "arxiv_url": "https://arxiv.org/abs/2410.15472v2",
    "pdf_url": "https://arxiv.org/pdf/2410.15472v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2410.15472",
    "arxiv_authors": [
      "Fnu Neha",
      "Arvind K. Bansal"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Multi-Layer+Feature+Fusion+with+Cross-Channel+Attention-Based+U-Net+for+Kidney+Tumor+Segmentation+Fnu+Neha+Arvind+K.+Bansal",
    "gs_search_success": true,
    "gs_authors": [
      "Fkzk4oAAAAAJ"
    ],
    "citation_count": 8
  },
  {
    "arxiv_id": "2504.14708",
    "title": "Time Frequency Analysis of EMG Signal for Gesture Recognition using Fine grained Features",
    "year": 2025,
    "published": "2025-04-20T18:51:10Z",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "abstract": "Electromyography (EMG) based hand gesture recognition converts forearm muscle activity into control commands for prosthetics, rehabilitation, and human computer interaction. This paper proposes a novel approach to EMG-based hand gesture recognition that uses fine-grained classification and presents XMANet, which unifies low-level local and high level semantic cues through cross layer mutual attention among shallow to deep CNN experts. Using stacked spectrograms and scalograms derived from the Sh",
    "arxiv_url": "https://arxiv.org/abs/2504.14708v1",
    "pdf_url": "https://arxiv.org/pdf/2504.14708v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2504.14708",
    "arxiv_authors": [
      "Parshuram N. Aarotale",
      "Ajita Rattani"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=Time+Frequency+Analysis+of+EMG+Signal+for+Gesture+Recognition+using+Fine+grained+Features+Parshuram+N.+Aarotale+Ajita+Rattani",
    "gs_search_success": true,
    "gs_authors": [
      "9esyU2EAAAAJ",
      "rr7lycwAAAAJ"
    ],
    "citation_count": 2
  },
  {
    "arxiv_id": "2411.13291",
    "title": "DATAP-SfM: Dynamic-Aware Tracking Any Point for Robust Structure from Motion in the Wild",
    "year": 2024,
    "published": "2024-11-20T13:01:16Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "This paper proposes a concise, elegant, and robust pipeline to estimate smooth camera trajectories and obtain dense point clouds for casual videos in the wild. Traditional frameworks, such as ParticleSfM~\\cite{zhao2022particlesfm}, address this problem by sequentially computing the optical flow between adjacent frames to obtain point trajectories. They then remove dynamic trajectories through motion segmentation and perform global bundle adjustment. However, the process of estimating optical flo",
    "arxiv_url": "https://arxiv.org/abs/2411.13291v1",
    "pdf_url": "https://arxiv.org/pdf/2411.13291v1",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2411.13291",
    "arxiv_authors": [
      "Weicai Ye",
      "Xinyu Chen",
      "Ruohao Zhan",
      "Di Huang",
      "Xiaoshui Huang",
      "Haoyi Zhu",
      "Hujun Bao",
      "Wanli Ouyang",
      "Tong He",
      "Guofeng Zhang"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=DATAP-SfM%3A+Dynamic-Aware+Tracking+Any+Point+for+Robust+Structure+from+Motion+in+the+Wild+Weicai+Ye+Xinyu+Chen+Ruohao+Zhan+Di+Huang+Xiaoshui+Huang",
    "gs_search_success": true,
    "gs_authors": [
      "CTqD_VwAAAAJ",
      "pD1NOyUAAAAJ",
      "pw_0Z_UAAAAJ",
      "F0xfpXAAAAAJ"
    ],
    "citation_count": 2
  },
  {
    "arxiv_id": "2412.17098",
    "title": "DreamOmni: Unified Image Generation and Editing",
    "year": 2024,
    "published": "2024-12-22T17:17:28Z",
    "categories": [
      "cs.CV"
    ],
    "abstract": "Currently, the success of large language models (LLMs) illustrates that a unified multitasking approach can significantly enhance model usability, streamline deployment, and foster synergistic benefits across different tasks. However, in computer vision, while text-to-image (T2I) models have significantly improved generation quality through scaling up, their framework design did not initially consider how to unify with downstream tasks, such as various types of editing. To address this, we intro",
    "arxiv_url": "https://arxiv.org/abs/2412.17098v2",
    "pdf_url": "https://arxiv.org/pdf/2412.17098v2",
    "google_scholar_url": "https://scholar.google.com/scholar_lookup?arxiv_id=2412.17098",
    "arxiv_authors": [
      "Bin Xia",
      "Yuechen Zhang",
      "Jingyao Li",
      "Chengyao Wang",
      "Yitong Wang",
      "Xinglong Wu",
      "Bei Yu",
      "Jiaya Jia"
    ],
    "gs_search_url": "https://scholar.google.com/scholar?q=DreamOmni%3A+Unified+Image+Generation+and+Editing+Bin+Xia+Yuechen+Zhang+Jingyao+Li+Chengyao+Wang+Yitong+Wang",
    "gs_search_success": true,
    "gs_authors": [
      "rh2fID8AAAAJ",
      "XPAkzTEAAAAJ",
      "mqrKmvcAAAAJ",
      "NfFTKfYAAAAJ",
      "tGneTm4AAAAJ",
      "1pZcoqgAAAAJ",
      "8OijNgkAAAAJ",
      "LVsp9RQAAAAJ"
    ],
    "citation_count": 11
  }
]