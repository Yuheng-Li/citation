#!/usr/bin/env python3
"""
‰ªé1-5ÊúàÁöÑËÆ∫Êñá‰∏≠ÈöèÊú∫ÈááÊ†∑nÁØáÔºåËé∑ÂèñGoogle ScholarÊï∞ÊçÆÔºàÂè™ÊèêÂèñIDÔºâ
"""
import json
import random
import time
import re
import requests
import os
from urllib.parse import quote_plus
from archived_code.old_approach.parse_name_and_id_from_gs_for_arxiv.gs_utils import extract_authors_from_gs_html

# Bright Data configuration
headers = {
    "Authorization": "Bearer 95f1e5b4c7ef79702fb77666561a234104bba6d008706c56d95f27dcae3daa9a",
    "Content-Type": "application/json"
}

def build_google_scholar_search_url(title, authors):
    """ÊûÑÂª∫ Google Scholar ÊêúÁ¥¢ URL"""
    query_parts = [title]
    top_authors = authors[:5] if len(authors) > 5 else authors
    query_parts.extend(top_authors)
    search_query = " ".join(query_parts)
    encoded_query = quote_plus(search_query)
    url = f"https://scholar.google.com/scholar?q={encoded_query}"
    return url

def fetch_google_scholar_page(url, timeout=30):
    """‰ΩøÁî® Bright Data API Ëé∑Âèñ Google Scholar È°µÈù¢"""
    data = {
        "zone": "yuheng_serp",
        "url": url,
        "format": "raw"
    }
    
    try:
        response = requests.post(
            "https://api.brightdata.com/request",
            json=data,
            headers=headers,
            timeout=timeout
        )
        
        if response.status_code == 200:
            if "not supported" in response.text.lower():
                return None
            if len(response.text) < 100:
                return None
            return response.text
        else:
            return None
    except Exception as e:
        print(f"  ‚ö†Ô∏è  ÈîôËØØ: {e}")
        return None

def extract_scholar_ids_from_html(html):
    """‰ªé HTML ‰∏≠ÊèêÂèñÊâÄÊúâ Google Scholar IDs"""
    pattern = r'/citations\?user=([^&"\']+)'
    scholar_ids = list(set(re.findall(pattern, html)))
    return scholar_ids

def extract_citation_count_from_html(html):
    """‰ªé HTML ‰∏≠ÊèêÂèñÂºïÁî®Êï∞Èáè"""
    # ÁÆÄÂçïÁöÑÊ≠£ÂàôË°®ËææÂºèÊèêÂèñ "Cited by X"
    match = re.search(r'Cited by\s+(\d+)', html, re.IGNORECASE)
    if match:
        try:
            return int(match.group(1))
        except:
            pass
    return None

def safe_save_json(data, output_file):
    """ÂÆâÂÖ®Âú∞‰øùÂ≠ò JSON Êñá‰ª∂ÔºàÂéüÂ≠êÂÜôÂÖ•Ôºâ"""
    temp_file = output_file + '.tmp'
    try:
        with open(temp_file, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        os.replace(temp_file, output_file)  # ÂéüÂ≠êÊõøÊç¢
        return True
    except Exception as e:
        print(f"  ‚ö†Ô∏è  ‰øùÂ≠òÂ§±Ë¥•: {e}")
        if os.path.exists(temp_file):
            os.remove(temp_file)
        return False

def main():
    # ÈÖçÁΩÆÂèÇÊï∞
    sample_size = 4000  # ÈöèÊú∫ÈááÊ†∑ÁöÑËÆ∫ÊñáÊï∞Èáè
    output_file = "gs_data_collection.json"
    raw_html_dir = "/mnt/localssd/raw_html"  # raw HTML ‰øùÂ≠òÁõÆÂΩï
    
    # ÂàõÂª∫ raw HTML ‰øùÂ≠òÁõÆÂΩï
    os.makedirs(raw_html_dir, exist_ok=True)
    print(f"raw HTML ‰øùÂ≠òÁõÆÂΩï: {raw_html_dir}\n")
    
    # Âä†ËΩΩËÆ∫ÊñáÊï∞ÊçÆ
    json_file = "cv_papers_20230101_to_20250531.json"
    print(f"Âä†ËΩΩËÆ∫ÊñáÊï∞ÊçÆ: {json_file}")
    
    with open(json_file, 'r', encoding='utf-8') as f:
        all_papers = json.load(f)
    
    print(f"ÊÄªËÆ∫ÊñáÊï∞: {len(all_papers)}")
    
    # Ê£ÄÊü•ÊòØÂê¶ÊúâÂ∑≤Â§ÑÁêÜÁöÑÁªìÊûúÊñá‰ª∂
    processed_arxiv_ids = set()
    existing_results = []
    
    try:
        with open(output_file, 'r', encoding='utf-8') as f:
            existing_results = json.load(f)
            processed_arxiv_ids = {paper.get('arxiv_id') for paper in existing_results if paper.get('arxiv_id')}
            print(f"ÂèëÁé∞Â∑≤Â§ÑÁêÜÁöÑÁªìÊûúÊñá‰ª∂: {output_file}")
            print(f"Â∑≤Â§ÑÁêÜÁöÑËÆ∫ÊñáÊï∞: {len(processed_arxiv_ids)}")
    except FileNotFoundError:
        print(f"Êú™ÊâæÂà∞Â∑≤Â§ÑÁêÜÁöÑÁªìÊûúÊñá‰ª∂ÔºåÂ∞Ü‰ªéÂ§¥ÂºÄÂßã")
    except Exception as e:
        print(f"‚ö†Ô∏è  ËØªÂèñÂ∑≤Â§ÑÁêÜÁªìÊûúÊñá‰ª∂Êó∂Âá∫Èîô: {e}ÔºåÂ∞Ü‰ªéÂ§¥ÂºÄÂßã")
    
    # ‰ªéÊâÄÊúâËÆ∫Êñá‰∏≠ÁßªÈô§Â∑≤Â§ÑÁêÜÁöÑËÆ∫Êñá
    remaining_papers = [paper for paper in all_papers if paper.get('arxiv_id') not in processed_arxiv_ids]
    
    print(f"Ââ©‰ΩôÊú™Â§ÑÁêÜÁöÑËÆ∫ÊñáÊï∞: {len(remaining_papers)}")
    print(f"ÈöèÊú∫ÈááÊ†∑: {sample_size} ÁØáËÆ∫Êñá\n")
    
    if len(remaining_papers) == 0:
        print("‚ö†Ô∏è  ÊâÄÊúâËÆ∫ÊñáÈÉΩÂ∑≤Â§ÑÁêÜÂÆåÊàêÔºÅ")
        return
    
    if len(remaining_papers) < sample_size:
        print(f"‚ö†Ô∏è  Ââ©‰ΩôËÆ∫ÊñáÊï∞ ({len(remaining_papers)}) Â∞ë‰∫éÈááÊ†∑Êï∞Èáè ({sample_size})ÔºåÂ∞ÜÂ§ÑÁêÜÊâÄÊúâÂâ©‰ΩôËÆ∫Êñá")
        sample_size = len(remaining_papers)
    
    # ÈöèÊú∫ÈááÊ†∑ n ÁØáËÆ∫Êñá
    sampled_papers = random.sample(remaining_papers, sample_size)
    
    results = existing_results.copy()  # ‰øùÁïôÂ∑≤ÊúâÁªìÊûú
    
    # ‰øùÂ≠òÈó¥Èöî
    save_interval = 100
    initial_count = len(existing_results)  # ËÆ∞ÂΩïÂàùÂßãÊï∞Èáè
    
    for idx, paper in enumerate(sampled_papers, 1):
        arxiv_id = paper.get('arxiv_id', '')
        arxiv_authors = paper.get('authors', [])
        title = paper.get('title', '')
        
        print(f"[{idx}/{len(sampled_papers)}] arXiv ID: {arxiv_id}")
        print(f"  Ê†áÈ¢ò: {title[:60]}...")
        
        # ÊûÑÂª∫ÊêúÁ¥¢ URL
        gs_search_url = build_google_scholar_search_url(title, arxiv_authors)
        
        # ÊûÑÂª∫ÁªìÊûúÔºö‰øùÁïôÂéüÂßãËÆ∫ÊñáÁöÑÊâÄÊúâ‰ø°ÊÅØ
        result = paper.copy()
        # ÁßªÈô§ authors Â≠óÊÆµÔºå‰ΩøÁî® arxiv_authors Êõø‰ª£
        if 'authors' in result:
            result['arxiv_authors'] = result.pop('authors')
        else:
            result['arxiv_authors'] = arxiv_authors
        
        # Ê∑ªÂä† Google Scholar Áõ∏ÂÖ≥Â≠óÊÆµ
        result['gs_search_url'] = gs_search_url
        
        # Ëé∑Âèñ Google Scholar È°µÈù¢
        try:
            html = fetch_google_scholar_page(gs_search_url)
            
            if html:
                # ‰øùÂ≠ò raw HTML Âà∞Êñá‰ª∂
                html_filename = f"{arxiv_id.replace('/', '_')}.html"
                html_filepath = os.path.join(raw_html_dir, html_filename)
                try:
                    with open(html_filepath, 'w', encoding='utf-8') as html_file:
                        html_file.write(html)
                    print(f"  üíæ Â∑≤‰øùÂ≠ò HTML: {html_filename}")
                except Exception as e:
                    print(f"  ‚ö†Ô∏è  ‰øùÂ≠ò HTML Â§±Ë¥•: {e}")
                
                # ÊèêÂèñ Google Scholar IDs
                scholar_ids = extract_scholar_ids_from_html(html)
                
                # ÊèêÂèñÂºïÁî®Êï∞Èáè
                citation_count = extract_citation_count_from_html(html)
                
                # ÊèêÂèñ‰ΩúËÄÖÊï∞ÈáèÔºà‰ΩøÁî®ÊóßÁâàÊú¨ÁöÑÂáΩÊï∞Ôºâ
                gs_author_count = None
                try:
                    authors_list, is_truncated, raw_text = extract_authors_from_gs_html(html, title=title)
                    if authors_list is not None:  # None Ë°®Á§∫Â§ö‰∏™ÂåπÈÖçÁªìÊûú
                        gs_author_count = len(authors_list)
                except Exception as e:
                    # Â¶ÇÊûúÊèêÂèñ‰ΩúËÄÖÊï∞ÈáèÂ§±Ë¥•Ôºå‰∏çÂΩ±ÂìçÊï¥‰ΩìÊµÅÁ®ã
                    print(f"  ‚ö†Ô∏è  ÊèêÂèñ‰ΩúËÄÖÊï∞ÈáèÂ§±Ë¥•: {e}")
                
                result['gs_search_success'] = True
                result['gs_authors'] = scholar_ids  # Âè™‰øùÂ≠ò ID ÂàóË°®
                result['citation_count'] = citation_count
                result['gs_author_count'] = gs_author_count
                result['raw_html_filename'] = html_filename  # Ê∑ªÂä† HTML Êñá‰ª∂Âêç
                
                print(f"  ‚úÖ ÊàêÂäüÊèêÂèñ {len(scholar_ids)} ‰∏™ IDÔºå{gs_author_count} ‰∏™‰ΩúËÄÖ")
            else:
                print(f"  ‚ùå Êó†Ê≥ïËé∑Âèñ Google Scholar È°µÈù¢")
                result['gs_search_success'] = False
                result['gs_authors'] = []
                result['citation_count'] = None
                result['gs_author_count'] = None
                result['raw_html_filename'] = None
                result['error_type'] = "fetch_failed"
        except Exception as e:
            # Ê£ÄÊü•ÊòØÂê¶ÊòØË∂ÖÊó∂ÈîôËØØ
            error_str = str(e).lower()
            if 'timeout' in error_str or 'timed out' in error_str:
                error_type = "timeout"
                print(f"  ‚ùå ËØ∑Ê±ÇË∂ÖÊó∂")
            else:
                error_type = "fetch_failed"
                print(f"  ‚ùå Ëé∑ÂèñÂ§±Ë¥•: {e}")
            
            result['gs_search_success'] = False
            result['gs_authors'] = []
            result['citation_count'] = None
            result['gs_author_count'] = None
            result['raw_html_filename'] = None
            result['error_type'] = error_type
        
        results.append(result)
        print()
        
        # ÊØèÂ§ÑÁêÜ10ÁØá‰øùÂ≠ò‰∏ÄÊ¨°Ôºà‰ΩøÁî®ÂéüÂ≠êÂÜôÂÖ•Ôºâ
        if idx % save_interval == 0:
            if safe_save_json(results, output_file):
                print(f"  üíæ Â∑≤‰øùÂ≠ò {len(results)} ÁØáËÆ∫ÊñáÔºàÊØè {save_interval} ÁØáËá™Âä®‰øùÂ≠òÔºâ\n")
        
        # ÈÅøÂÖçËØ∑Ê±ÇËøáÂø´
        time.sleep(0.05)
    
    # ÊúÄÂêé‰øùÂ≠ò‰∏ÄÊ¨°ÔºàÁ°Æ‰øùÊâÄÊúâÁªìÊûúÈÉΩ‰øùÂ≠ò‰∫ÜÔºå‰ΩøÁî®ÂéüÂ≠êÂÜôÂÖ•Ôºâ
    if not safe_save_json(results, output_file):
        raise Exception("ÊúÄÁªà‰øùÂ≠òÂ§±Ë¥•")
    
    print(f"\nÁªìÊûúÂ∑≤‰øùÂ≠òÂà∞: {output_file}")
    print(f"ÊÄªÂÖ±Â§ÑÁêÜ: {len(results)} ÁØáËÆ∫ÊñáÔºàÂåÖÂê´‰πãÂâçÂ∑≤Â§ÑÁêÜÁöÑ {initial_count} ÁØáÔºâ")
    success_count = sum(1 for r in results if r.get('gs_search_success', False))
    print(f"ÊàêÂäüËé∑Âèñ: {success_count} ÁØá")
    print(f"Êú¨Ê¨°Êñ∞Â¢û: {len(results) - initial_count} ÁØá")

if __name__ == "__main__":
    main()

