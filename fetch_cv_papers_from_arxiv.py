#!/usr/bin/env python3
"""
ä½¿ç”¨ arXiv API æŠ“å– cs.CV (è®¡ç®—æœºè§†è§‰) ç±»åˆ«çš„è®ºæ–‡
"""
import requests
import xml.etree.ElementTree as ET
import time
import json
from pathlib import Path
from datetime import datetime, timedelta


def fetch_cv_papers(max_results=100, start=0, sort_by='submittedDate', sort_order='descending', 
                    start_date=None, end_date=None):
    """
    ä» arXiv æŠ“å– cs.CV ç±»åˆ«çš„è®ºæ–‡
    
    Args:
        max_results: æ¯æ¬¡è¯·æ±‚æœ€å¤šè¿”å›çš„è®ºæ–‡æ•°é‡ï¼ˆarXiv API é™åˆ¶æœ€å¤š2000ï¼‰
        start: èµ·å§‹ä½ç½®ï¼ˆç”¨äºåˆ†é¡µï¼‰
        sort_by: æ’åºæ–¹å¼ ('relevance', 'lastUpdatedDate', 'submittedDate')
        sort_order: æ’åºé¡ºåº ('ascending', 'descending')
        start_date: å¼€å§‹æ—¥æœŸï¼Œæ ¼å¼ 'YYYYMMDD' æˆ– 'YYYYMMDDHHMMSS'ï¼Œä¾‹å¦‚ '20250501'
        end_date: ç»“æŸæ—¥æœŸï¼Œæ ¼å¼ 'YYYYMMDD' æˆ– 'YYYYMMDDHHMMSS'ï¼Œä¾‹å¦‚ '20250531'
    
    Returns:
        è®ºæ–‡åˆ—è¡¨ï¼Œæ¯ä¸ªè®ºæ–‡åŒ…å«æ ‡é¢˜ã€ä½œè€…ã€arXiv IDã€æ‘˜è¦ã€å‘å¸ƒæ—¥æœŸã€Google Scholar é“¾æ¥ç­‰
    """
    base_url = "http://export.arxiv.org/api/query"
    
    # æ„å»ºæŸ¥è¯¢å­—ç¬¦ä¸²
    query = "cat:cs.CV"
    
    # å¦‚æœæŒ‡å®šäº†æ—¥æœŸèŒƒå›´ï¼Œæ·»åŠ åˆ°æŸ¥è¯¢ä¸­
    if start_date and end_date:
        # arXiv API æ—¥æœŸæ ¼å¼ï¼šYYYYMMDDHHMMSS
        # å¦‚æœåªæä¾›äº†æ—¥æœŸï¼Œè¡¥å……æ—¶é—´éƒ¨åˆ†
        if len(start_date) == 8:
            start_date_full = f"{start_date}000000"
        else:
            start_date_full = start_date
        
        if len(end_date) == 8:
            end_date_full = f"{end_date}235959"
        else:
            end_date_full = end_date
        
        query = f"{query} AND submittedDate:[{start_date_full} TO {end_date_full}]"
    
    params = {
        'search_query': query,
        'start': start,
        'max_results': max_results,
        'sortBy': sort_by,
        'sortOrder': sort_order
    }
    
    headers = {
        "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36"
    }
    
    # åªåœ¨ç¬¬ä¸€æ‰¹æ—¶æ‰“å°è¯¦ç»†ä¿¡æ¯
    if start == 0:
        print(f"æ­£åœ¨æŠ“å– arXiv cs.CV è®ºæ–‡...")
        print(f"å‚æ•°: max_results={max_results}, sort_by={sort_by}")
        if start_date and end_date:
            print(f"æ—¥æœŸèŒƒå›´: {start_date} åˆ° {end_date}")
    
    try:
        response = requests.get(base_url, params=params, headers=headers)
        
        if response.status_code == 200:
            # è§£æ XML å“åº”
            root = ET.fromstring(response.content)
            
            # arXiv API ä½¿ç”¨ Atom å‘½åç©ºé—´
            ns = {'atom': 'http://www.w3.org/2005/Atom',
                  'arxiv': 'http://arxiv.org/schemas/atom'}
            
            entries = root.findall('atom:entry', ns)
            
            if not entries:
                print("æœªæ‰¾åˆ°ä»»ä½•è®ºæ–‡")
                return []
            
            papers = []
            for entry in entries:
                # æå– arXiv ID
                arxiv_id = None
                id_text = entry.find('atom:id', ns).text if entry.find('atom:id', ns) is not None else None
                if id_text:
                    # arXiv ID æ ¼å¼: http://arxiv.org/abs/1234.5678v1
                    arxiv_id = id_text.split('/')[-1].split('v')[0]
                
                # æå–æ ‡é¢˜
                title_elem = entry.find('atom:title', ns)
                title = title_elem.text.strip().replace('\n', ' ') if title_elem is not None else None
                
                # æå–ä½œè€…
                authors = []
                for author_elem in entry.findall('atom:author', ns):
                    name_elem = author_elem.find('atom:name', ns)
                    if name_elem is not None:
                        authors.append(name_elem.text.strip())
                
                # æå–æ‘˜è¦
                summary_elem = entry.find('atom:summary', ns)
                abstract = summary_elem.text.strip().replace('\n', ' ') if summary_elem is not None else None
                
                # æå–å‘å¸ƒæ—¥æœŸ
                published_elem = entry.find('atom:published', ns)
                published = published_elem.text.strip() if published_elem is not None else None
                year = None
                if published:
                    try:
                        year = int(published.split('-')[0])
                    except:
                        pass
                
                # æå–åˆ†ç±»
                categories = []
                for category_elem in entry.findall('atom:category', ns):
                    term = category_elem.get('term')
                    if term:
                        categories.append(term)
                
                # ä» API å“åº”ä¸­æå–é“¾æ¥
                # arXiv API è¿”å›çš„é“¾æ¥ç±»å‹ï¼š
                # - rel='alternate': arXiv é¡µé¢é“¾æ¥
                # - rel='related' title='pdf': PDF é“¾æ¥
                arxiv_url = None
                pdf_url = None
                for link_elem in entry.findall('atom:link', ns):
                    rel = link_elem.get('rel')
                    title_attr = link_elem.get('title')
                    href = link_elem.get('href')
                    
                    if rel == 'alternate':
                        arxiv_url = href
                    elif rel == 'related' and title_attr == 'pdf':
                        pdf_url = href
                
                # å¦‚æœæ²¡æœ‰ä» API è·å–åˆ°é“¾æ¥ï¼Œåˆ™æ‰‹åŠ¨æ„å»ºï¼ˆå¤‡ç”¨æ–¹æ¡ˆï¼‰
                if not arxiv_url and arxiv_id:
                    arxiv_url = f"https://arxiv.org/abs/{arxiv_id}"
                if not pdf_url and arxiv_id:
                    pdf_url = f"https://arxiv.org/pdf/{arxiv_id}.pdf"
                
                # æ³¨æ„ï¼šGoogle Scholar é“¾æ¥ä¸æ˜¯ arXiv API è¿”å›çš„
                # arXiv API åªè¿”å› arXiv é¡µé¢å’Œ PDF é“¾æ¥
                # ä½†æ˜¯ arXiv ç½‘é¡µä¸Šæœ‰ Google Scholar é“¾æ¥ï¼Œæ ¼å¼ä¸ºï¼š
                # https://scholar.google.com/scholar_lookup?arxiv_id={arxiv_id}
                # è¿™æ¯”é€šè¿‡æ ‡é¢˜æœç´¢æ›´ç²¾ç¡®
                google_scholar_url = None
                if arxiv_id:
                    google_scholar_url = f"https://scholar.google.com/scholar_lookup?arxiv_id={arxiv_id}"
                
                paper = {
                    'arxiv_id': arxiv_id,
                    'title': title,
                    'authors': authors,
                    'year': year,
                    'published': published,
                    'categories': categories,
                    'abstract': abstract[:500] if abstract else None,  # é™åˆ¶æ‘˜è¦é•¿åº¦
                    'arxiv_url': arxiv_url,
                    'pdf_url': pdf_url,
                    'google_scholar_url': google_scholar_url
                }
                papers.append(paper)
            
            print(f"æˆåŠŸæŠ“å– {len(papers)} ç¯‡è®ºæ–‡")
            return papers
            
        elif response.status_code == 429:
            print(f"âš ï¸  è¯·æ±‚é¢‘ç‡è¿‡é«˜ï¼Œç­‰å¾… 60 ç§’åé‡è¯•...")
            time.sleep(60)
            return fetch_cv_papers(max_results, start, sort_by, sort_order, start_date, end_date)
        else:
            print(f"âš ï¸  HTTP é”™è¯¯: {response.status_code}")
            print(f"å“åº”å†…å®¹: {response.text[:200]}")
            return []
            
    except ET.ParseError as e:
        print(f"âš ï¸  XML è§£æé”™è¯¯: {e}")
        return []
    except Exception as e:
        print(f"âš ï¸  é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return []


def get_month_ranges(start_date_str, end_date_str):
    """
    å°†æ—¥æœŸèŒƒå›´åˆ†æˆæ¯ä¸ªæœˆ
    
    Args:
        start_date_str: å¼€å§‹æ—¥æœŸï¼Œæ ¼å¼ 'YYYYMMDD'ï¼Œä¾‹å¦‚ '20230101'
        end_date_str: ç»“æŸæ—¥æœŸï¼Œæ ¼å¼ 'YYYYMMDD'ï¼Œä¾‹å¦‚ '20250531'
    
    Returns:
        æœˆä»½èŒƒå›´åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ ä¸º (start_date, end_date) å…ƒç»„
    """
    start_date = datetime.strptime(start_date_str, '%Y%m%d')
    end_date = datetime.strptime(end_date_str, '%Y%m%d')
    
    ranges = []
    current = start_date
    
    while current <= end_date:
        # è®¡ç®—å½“å‰æœˆçš„æœ€åä¸€å¤©
        if current.month == 12:
            next_month = current.replace(year=current.year + 1, month=1, day=1)
        else:
            next_month = current.replace(month=current.month + 1, day=1)
        
        month_end = next_month - timedelta(days=1)
        
        # å¦‚æœç»“æŸæ—¥æœŸåœ¨å½“å‰æœˆå†…ï¼Œä½¿ç”¨ç»“æŸæ—¥æœŸ
        if month_end > end_date:
            month_end = end_date
        
        # æ ¼å¼åŒ–æ—¥æœŸ
        month_start_str = current.strftime('%Y%m%d')
        month_end_str = month_end.strftime('%Y%m%d')
        
        ranges.append((month_start_str, month_end_str))
        
        # ç§»åŠ¨åˆ°ä¸‹ä¸ªæœˆçš„ç¬¬ä¸€å¤©
        current = next_month
    
    return ranges


def fetch_all_cv_papers_by_date(start_date, end_date, batch_size=2000):
    """
    åˆ†é¡µæŠ“å–æŒ‡å®šæ—¥æœŸèŒƒå›´å†…çš„æ‰€æœ‰ cs.CV è®ºæ–‡
    
    Args:
        start_date: å¼€å§‹æ—¥æœŸï¼Œæ ¼å¼ 'YYYYMMDD'ï¼Œä¾‹å¦‚ '20250501'
        end_date: ç»“æŸæ—¥æœŸï¼Œæ ¼å¼ 'YYYYMMDD'ï¼Œä¾‹å¦‚ '20250531'
        batch_size: æ¯æ‰¹æŠ“å–çš„æ•°é‡ï¼ˆarXiv API é™åˆ¶æœ€å¤š2000ï¼‰
    
    Returns:
        æ‰€æœ‰è®ºæ–‡çš„åˆ—è¡¨
    """
    all_papers = []
    start = 0
    
    print(f"\nå¼€å§‹æŠ“å– {start_date} åˆ° {end_date} çš„æ‰€æœ‰ cs.CV è®ºæ–‡...")
    print("=" * 80)
    
    while True:
        print(f"\næ­£åœ¨æŠ“å–ç¬¬ {start // batch_size + 1} æ‰¹ (start={start})...")
        
        papers = fetch_cv_papers(
            max_results=batch_size,
            start=start,
            sort_by='submittedDate',
            sort_order='ascending',
            start_date=start_date,
            end_date=end_date
        )
        
        if not papers:
            print("æ²¡æœ‰æ›´å¤šè®ºæ–‡äº†")
            break
        
        all_papers.extend(papers)
        print(f"æœ¬æ‰¹æŠ“å– {len(papers)} ç¯‡ï¼Œç´¯è®¡ {len(all_papers)} ç¯‡")
        
        # å¦‚æœè¿”å›çš„è®ºæ–‡æ•°å°‘äº batch_sizeï¼Œè¯´æ˜å·²ç»æŠ“å–å®Œäº†
        if len(papers) < batch_size:
            print("å·²æŠ“å–æ‰€æœ‰è®ºæ–‡")
            break
        
        start += batch_size
        
        # é¿å…è¯·æ±‚è¿‡å¿«ï¼ŒarXiv API å»ºè®®æ¯æ¬¡è¯·æ±‚é—´éš” 3 ç§’
        time.sleep(3.1)
    
    print(f"\næ€»å…±æŠ“å– {len(all_papers)} ç¯‡è®ºæ–‡")
    return all_papers


def fetch_cv_papers_by_date_range(start_date, end_date, batch_size=2000):
    """
    æŠ“å–æŒ‡å®šæ—¥æœŸèŒƒå›´å†…çš„æ‰€æœ‰ cs.CV è®ºæ–‡ï¼Œå¦‚æœèŒƒå›´è¶…è¿‡1ä¸ªæœˆåˆ™è‡ªåŠ¨æŒ‰æœˆåˆ†å—
    
    Args:
        start_date: å¼€å§‹æ—¥æœŸï¼Œæ ¼å¼ 'YYYYMMDD'ï¼Œä¾‹å¦‚ '20230101'
        end_date: ç»“æŸæ—¥æœŸï¼Œæ ¼å¼ 'YYYYMMDD'ï¼Œä¾‹å¦‚ '20250531'
        batch_size: æ¯æ‰¹æŠ“å–çš„æ•°é‡ï¼ˆarXiv API é™åˆ¶æœ€å¤š2000ï¼‰
    
    Returns:
        æ‰€æœ‰è®ºæ–‡çš„åˆ—è¡¨
    """
    # è®¡ç®—æ—¥æœŸèŒƒå›´æ˜¯å¦è¶…è¿‡1ä¸ªæœˆ
    start_dt = datetime.strptime(start_date, '%Y%m%d')
    end_dt = datetime.strptime(end_date, '%Y%m%d')
    
    # è®¡ç®—æœˆä»½å·®
    months_diff = (end_dt.year - start_dt.year) * 12 + (end_dt.month - start_dt.month)
    
    # å¦‚æœè¶…è¿‡1ä¸ªæœˆï¼ŒæŒ‰æœˆåˆ†å—æŸ¥è¯¢
    if months_diff > 1 or (months_diff == 1 and end_dt.day > start_dt.day):
        print(f"\næ—¥æœŸèŒƒå›´è¶…è¿‡1ä¸ªæœˆï¼Œå°†æŒ‰æœˆåˆ†å—æŸ¥è¯¢...")
        month_ranges = get_month_ranges(start_date, end_date)
        print(f"å…±åˆ†æˆ {len(month_ranges)} ä¸ªæœˆä»½å—\n")
        
        all_papers = []
        total_months = len(month_ranges)
        start_time = time.time()
        
        for idx, (month_start, month_end) in enumerate(month_ranges, 1):
            print(f"\n{'='*80}")
            progress_pct = (idx / total_months) * 100
            print(f"å¤„ç†ç¬¬ {idx}/{total_months} ä¸ªæœˆ ({progress_pct:.1f}%): {month_start} åˆ° {month_end}")
            print(f"{'='*80}")
            
            month_start_time = time.time()
            month_papers = fetch_all_cv_papers_by_date(month_start, month_end, batch_size)
            month_elapsed = time.time() - month_start_time
            
            all_papers.extend(month_papers)
            
            # è®¡ç®—é¢„è®¡å‰©ä½™æ—¶é—´
            elapsed_total = time.time() - start_time
            avg_time_per_month = elapsed_total / idx
            remaining_months = total_months - idx
            estimated_remaining = avg_time_per_month * remaining_months
            
            print(f"\nâœ… æœ¬æœˆå®Œæˆ: æŠ“å– {len(month_papers)} ç¯‡ï¼Œè€—æ—¶ {month_elapsed:.1f} ç§’")
            print(f"ğŸ“Š ç´¯è®¡: {len(all_papers)} ç¯‡è®ºæ–‡")
            if remaining_months > 0:
                print(f"â±ï¸  é¢„è®¡å‰©ä½™æ—¶é—´: {estimated_remaining/60:.1f} åˆ†é’Ÿ ({estimated_remaining:.0f} ç§’)")
            
            # æœˆä»½ä¹‹é—´ç¨ä½œå»¶è¿Ÿ
            if idx < len(month_ranges):
                time.sleep(2)
        
        print(f"\n{'='*80}")
        print(f"æ‰€æœ‰æœˆä»½æŠ“å–å®Œæˆï¼Œæ€»è®¡ {len(all_papers)} ç¯‡è®ºæ–‡")
        print(f"{'='*80}")
        return all_papers
    else:
        # å•ä¸ªæœˆä»½ï¼Œç›´æ¥æŸ¥è¯¢
        return fetch_all_cv_papers_by_date(start_date, end_date, batch_size)


def save_papers_to_json(papers, output_file):
    """å°†è®ºæ–‡æ•°æ®ä¿å­˜ä¸º JSON æ–‡ä»¶"""
    output_path = Path(output_file)
    output_path.parent.mkdir(parents=True, exist_ok=True)
    
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(papers, f, ensure_ascii=False, indent=2)
    
    print(f"\nè®ºæ–‡æ•°æ®å·²ä¿å­˜åˆ°: {output_path}")
    print(f"å…± {len(papers)} ç¯‡è®ºæ–‡")


def print_papers_summary(papers, num_to_show=10):
    """æ‰“å°è®ºæ–‡æ‘˜è¦ä¿¡æ¯"""
    print("\n" + "=" * 80)
    print(f"è®ºæ–‡æ‘˜è¦ (æ˜¾ç¤ºå‰ {min(num_to_show, len(papers))} ç¯‡)")
    print("=" * 80)
    
    for idx, paper in enumerate(papers[:num_to_show], 1):
        print(f"\n[{idx}] {paper['title']}")
        print(f"    arXiv ID: {paper['arxiv_id']}")
        print(f"    ä½œè€…: {', '.join(paper['authors'][:3])}{'...' if len(paper['authors']) > 3 else ''}")
        print(f"    å‘å¸ƒæ—¥æœŸ: {paper['published']}")
        print(f"    åˆ†ç±»: {', '.join(paper['categories'])}")
        print(f"    arXiv URL: {paper['arxiv_url']}")
        print(f"    Google Scholar: {paper['google_scholar_url']}")
        if paper['abstract']:
            print(f"    æ‘˜è¦: {paper['abstract'][:150]}...")


if __name__ == "__main__":
    # é…ç½®å‚æ•°ï¼šæŠ“å–2023å¹´1æœˆ1æ—¥åˆ°2025å¹´5æœˆ31æ—¥çš„æ‰€æœ‰è®ºæ–‡
    START_DATE = "20230101"  # 2023å¹´1æœˆ1æ—¥
    END_DATE = "20250531"    # 2025å¹´5æœˆ31æ—¥
    BATCH_SIZE = 2000        # æ¯æ‰¹æœ€å¤šæŠ“å–2000ç¯‡ï¼ˆarXiv APIé™åˆ¶ï¼‰
    
    # æ³¨æ„ï¼šarXiv API å¯¹å•æ¬¡æŸ¥è¯¢çš„åç§»é‡æœ‰é™åˆ¶ï¼ˆé€šå¸¸æœ€å¤š10000ï¼‰
    # å¦‚æœæ—¥æœŸèŒƒå›´è¶…è¿‡1ä¸ªæœˆï¼Œä¼šè‡ªåŠ¨æŒ‰æœˆåˆ†å—æŸ¥è¯¢ä»¥é¿å…è¶…è¿‡é™åˆ¶
    # ä½¿ç”¨åˆ†é¡µåŠŸèƒ½æŠ“å–æ‰€æœ‰è®ºæ–‡
    papers = fetch_cv_papers_by_date_range(
        start_date=START_DATE,
        end_date=END_DATE,
        batch_size=BATCH_SIZE
    )
    
    if papers:
        # æ‰“å°æ‘˜è¦
        print_papers_summary(papers, num_to_show=10)
        
        # ä¿å­˜åˆ° JSON æ–‡ä»¶
        output_file = f"cv_papers_{START_DATE}_to_{END_DATE}.json"
        save_papers_to_json(papers, output_file)
        
        # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
        print("\n" + "=" * 80)
        print("ç»Ÿè®¡ä¿¡æ¯")
        print("=" * 80)
        print(f"æ€»è®ºæ–‡æ•°: {len(papers)}")
        print(f"æœ‰ Google Scholar é“¾æ¥: {sum(1 for p in papers if p['google_scholar_url'])}")
        print(f"æœ‰æ‘˜è¦: {sum(1 for p in papers if p['abstract'])}")
        
        # æŒ‰å¹´ä»½ç»Ÿè®¡
        years = {}
        for paper in papers:
            year = paper.get('year')
            if year:
                years[year] = years.get(year, 0) + 1
        
        if years:
            print("\næŒ‰å¹´ä»½åˆ†å¸ƒ:")
            for year in sorted(years.keys(), reverse=True):
                print(f"  {year}: {years[year]} ç¯‡")
    else:
        print("æœªæŠ“å–åˆ°ä»»ä½•è®ºæ–‡")

